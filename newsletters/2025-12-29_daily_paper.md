
# <img src="https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo.png" width="30"/> Hugging Face 2025-12-29 论文日报

## 📊 今日论文统计
- 总论文数：15
- 热门领域：Vision, RL

## 📝 论文详情


### 1. InsertAnywhere：融合4D场景几何与扩散模型实现逼真视频物体插入

**原文标题：** InsertAnywhere: Bridging 4D Scene Geometry and Diffusion Models for Realistic Video Object Insertion

**摘要：**
基于扩散模型的视频生成技术的最新进展为可控视频编辑开辟了新的可能性，然而，由于对4D场景理解的局限性以及对遮挡和光照效果处理不足，实现逼真的视频物体插入仍然面临挑战。本文提出InsertAnywhere，一种新的视频物体插入框架，能够实现几何一致的物体放置和外观保真的视频合成。我们的方法始于一个4D感知掩码生成模块，该模块重建场景几何结构，并在保持时间连贯性和遮挡一致性的同时，将用户指定的物体放置跨帧传播。在此空间基础上，我们扩展了一种基于扩散的视频生成模型，以联合合成插入的物体及其周围的局部变化（如光照和阴影）。为了支持有监督训练，我们引入了ROSE++，这是一个光照感知的合成数据集，通过将ROSE物体移除数据集转换为包含物体移除视频、物体存在视频以及视觉语言模型生成的参考图像的三元组而构建。通过大量实验，我们证明该框架能够在多样化的真实世界场景中生成几何合理且视觉连贯的物体插入效果，显著优于现有研究和商业模型。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.17504) | [arXiv](https://arxiv.org/abs/2512.17504)



---

### 2. 基于心智图景感知的检索增强生成技术提升长文本理解能力

**原文标题：** Mindscape-Aware Retrieval Augmented Generation for Improved Long Context Understanding

**摘要：**
人类通过构建内容的整体语义表征来理解长而复杂的文本。心理学研究揭示的人类心智图景感知能力表明，这种全局视角有助于组织先验知识、解读新信息，并整合分散在文档各处的证据。当前检索增强生成系统缺乏此类全局引导机制，因此在处理长文本任务时面临挑战。本文提出心智图景感知检索增强生成方法，首次为基于大语言模型的检索增强生成系统赋予显式的全局上下文感知能力。该方法通过分层摘要构建心智图景，并基于该全局语义表征同步优化检索与生成过程。这一机制使检索器能够构建增强型查询嵌入，同时使生成器能够在连贯的全局语境中对检索证据进行推理。我们在多类长文本及双语基准测试中，针对证据理解与全局语义构建任务对方法进行评估。实验表明该方法持续超越基线模型，进一步分析显示其能将局部细节与连贯的全局表征相融合，从而实现更趋近人类认知模式的长文本检索与推理。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.17220) | [arXiv](https://arxiv.org/abs/2512.17220)



---

### 3. MAI-UI技术报告：以现实世界为中心的基础图形用户界面智能体

**原文标题：** MAI-UI Technical Report: Real-World Centric Foundation GUI Agents

**摘要：**
图形用户界面（GUI）智能体的发展有望彻底革新下一代人机交互。基于这一愿景，我们提出了MAI-UI，这是一个覆盖全尺寸谱系的基础GUI智能体家族，包括2B、8B、32B以及235B-A22B等多种变体。我们识别出现实部署面临的四个关键挑战：缺乏原生的智能体-用户交互、仅依赖UI操作的限制、实用部署架构的缺失，以及在动态环境中的脆弱性。MAI-UI通过一套统一的方法论应对这些问题：一个自演进的数据管道，将导航数据扩展至包含用户交互与MCP工具调用；一个原生的设备-云端协作系统，根据任务状态路由执行；以及一个具备先进优化能力的在线强化学习框架，用于扩展并行环境与上下文长度。MAI-UI在GUI基础任务与移动导航任务上均取得了新的最先进性能。在基础任务基准测试中，其在ScreenSpot-Pro上达到73.5%，在MMBench GUI L2上达到91.3%，在OSWorld-G上达到70.9%，在UI-Vision上达到49.2%，并在ScreenSpot-Pro上超越了Gemini-3-Pro和Seed1.8。在移动GUI导航任务上，其在AndroidWorld上创造了76.7%的新SOTA，超越了UI-Tars-2、Gemini-2.5-Pro和Seed1.8。在MobileWorld上，MAI-UI获得了41.7%的成功率，显著优于端到端GUI模型，并与基于Gemini-3-Pro的智能体框架表现相当。我们的在线强化学习实验表明，将并行环境从32个扩展到512个带来了+5.2个百分点的显著提升，将环境步数预算从15步增加到50步则带来了+4.3个百分点的提升。最后，原生的设备-云端协作系统将设备端性能提升了33%，减少了超过40%的云端模型调用，并有效保护了用户隐私。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.22047) | [arXiv](https://arxiv.org/abs/2512.22047)



---

### 4. UniPercept：迈向跨美学、质量、结构与纹理的统一感知级图像理解

**原文标题：** UniPercept: Towards Unified Perceptual-Level Image Understanding across Aesthetics, Quality, Structure, and Texture

**摘要：**
多模态大语言模型在视觉定位、分割和描述等视觉理解任务中取得了显著进展，但其感知图像底层特征的能力仍存在局限。本研究提出UniPercept-Bench——一个跨美学、质量、结构与纹理三大关键领域的统一感知级图像理解框架。我们建立了层次化定义体系并构建大规模数据集以评估感知级图像理解能力。在此基础上，通过领域自适应预训练与任务对齐强化学习，开发出具有强泛化能力的基线模型UniPercept，该模型在视觉评分与视觉问答任务中均表现出色。UniPercept在感知级图像理解任务上超越现有多模态大语言模型，并能作为即插即用的奖励模型服务于文本到图像生成任务。本研究在多模态大语言模型时代明确了感知级图像理解的定义，通过构建综合性基准与强基线模型，为推进感知级多模态图像理解研究奠定了坚实基础。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.21675) | [arXiv](https://arxiv.org/abs/2512.21675)



---

### 5. ProEdit：基于反演技术的提示驱动编辑方法优化

**原文标题：** ProEdit: Inversion-based Editing From Prompts Done Right

**摘要：**
基于反演技术的视觉编辑方法为用户提供了一种无需训练即可根据指令编辑图像或视频的有效途径。现有方法通常在采样过程中注入源图像信息以保持编辑一致性，但这种采样策略过度依赖源信息，会对目标图像的编辑效果产生负面影响（例如无法按指令改变主体的姿态、数量或颜色等属性）。本研究提出ProEdit方法，从注意力机制和潜在空间两个维度解决该问题。在注意力机制方面，我们引入KV混合技术，在编辑区域混合源图像与目标图像的键值特征，既降低源图像对编辑区域的影响，又保持背景一致性。在潜在空间方面，我们提出潜在偏移技术，通过对源潜在空间的编辑区域施加扰动，消除反演潜在向量对采样过程的影响。在多个图像与视频编辑基准测试上的大量实验表明，本方法达到了当前最优性能。此外，本设计具备即插即用特性，可无缝集成至现有反演与编辑方法（如RF-Solver、FireFlow和UniEdit）中。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.22118) | [arXiv](https://arxiv.org/abs/2512.22118)



---

### 6. TimeBill：面向大语言模型的时间预算推理框架

**原文标题：** TimeBill: Time-Budgeted Inference for Large Language Models

**摘要：**
大语言模型正日益应用于时间敏感系统，如机器人、自动驾驶、具身智能和工业自动化等领域。在这些场景中，在给定时间预算内生成准确响应对于决策、控制或安全关键任务至关重要。然而，大语言模型的自回归生成特性使其端到端执行时间的建模与估计面临挑战。此外，现有基于固定键值缓存淘汰比例的高效推理方法难以适应具有不同时间预算的多样化任务，不恰当的淘汰比例可能导致推理无法完成或响应性能下降。本文提出TimeBill，一种新颖的面向大语言模型的时间预算推理框架，旨在平衡推理效率与响应性能。具体而言，我们设计了细粒度响应长度预测器与执行时间估计器，以精准预测大语言模型的端到端执行时间。在此基础上，我们开发了一种时间预算高效推理方法，能够根据执行时间预测与给定时间预算自适应调整键值缓存淘汰比例。最后，通过大量实验，我们验证了TimeBill在多种超时策略下提升任务完成率并保持响应性能的优势。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.21859) | [arXiv](https://arxiv.org/abs/2512.21859)



---

### 7. Omni-Weather：面向天气生成与理解的多模态统一基础模型

**原文标题：** Omni-Weather: Unified Multimodal Foundation Model for Weather Generation and Understanding

**摘要：**
天气建模既需要精确的预测，也需要机制性的解释，然而现有方法往往将这两个目标割裂处理，将生成与理解分离开来。为弥补这一不足，我们提出了Omni-Weather——首个将天气生成与理解统一于单一架构的多模态基础模型。该模型通过雷达编码器处理天气生成任务，并利用共享的自注意力机制进行统一处理。此外，我们构建了一个用于天气生成因果推理的思维链数据集，以实现可解释的输出并提升感知质量。大量实验表明，Omni-Weather在天气生成与理解任务上均达到了最先进的性能。我们的研究进一步表明，天气领域的生成任务与理解任务能够相互促进。Omni-Weather也证明了统一天气生成与理解的可行性与重要价值。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.21643) | [arXiv](https://arxiv.org/abs/2512.21643)



---

### 8. 少看而精看：双向感知塑形在多模态推理中的应用

**原文标题：** See Less, See Right: Bi-directional Perceptual Shaping For Multimodal Reasoning

**摘要：**
大型视觉语言模型（VLMs）通常受益于中间视觉线索的辅助，这些线索可通过外部工具注入或在推理过程中生成为潜在视觉标记。然而，现有机制仍存在以下局限：忽略细粒度视觉证据（如图表中的折线）、跨领域泛化能力较弱，且推理成本较高。本文提出双向感知塑形方法，该方法将问题导向的掩码视图转化为双向的“何处关注”信号，从而在训练过程中塑造模型的感知能力。BiPS首先在原始图像与仅保留问题相关区域的证据保留视图之间施加KL一致性约束，以鼓励模型对支持性像素进行粗略但完整的覆盖。随后，在原始图像与关键像素被掩码的证据消除视图之间施加KL分离约束，使得图像不再支持原始答案，从而抑制仅依赖文本的捷径策略（即仅从文本中获取答案），并强化模型对细粒度视觉信息的依赖。在八个基准测试中，BiPS将Qwen2.5-VL-7B模型的平均性能提升了8.2%，并在未见过的数据集和图像类型上展现出强大的跨领域泛化能力。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.22120) | [arXiv](https://arxiv.org/abs/2512.22120)



---

### 9. InSight-o3：以广义视觉搜索增强多模态基础模型能力

**原文标题：** InSight-o3: Empowering Multimodal Foundation Models with Generalized Visual Search

**摘要：**
人工智能代理“以图像思考”的能力需要推理与感知的深度融合。然而，当前开源的多数多模态代理在推理能力方面仍显不足，而这种能力对于分析包含密集图表/图形的文档、地图导航等现实任务至关重要。为弥补这一差距，我们提出了O3-Bench——一个旨在评估多模态推理能力、并强调对视觉细节交错关注的新基准。O3-Bench包含一系列具有挑战性的问题，要求代理通过多步推理，从图像的不同区域整合细微的视觉信息。即使对于OpenAI o3等前沿系统，这些问题也极具挑战性，其在O3-Bench上的准确率仅为40.8%。为推进进展，我们提出了InSight-o3，这是一个由视觉推理代理（vReasoner）和视觉搜索代理（vSearcher）组成的多代理框架。我们为vSearcher引入了广义视觉搜索任务——即超越自然图像中简单物体或图形的定位，能够根据自由形式语言描述，定位关系性、模糊性或概念性的图像区域。随后，我们提出了一种通过强化学习专门为此任务训练的多模态大语言模型。作为一个即插即用代理，我们的vSearcher能够增强前沿多模态模型（作为vReasoner），显著提升其在广泛基准测试上的性能。这标志着向构建强大的类o3开源系统迈出了坚实一步。我们的代码与数据集可在 https://github.com/m-Just/InSight-o3 获取。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.18745) | [arXiv](https://arxiv.org/abs/2512.18745)



---

### 10. SWE-RM：面向软件工程智能体的免执行反馈机制

**原文标题：** SWE-RM: Execution-free Feedback For Software Engineering Agents

**摘要：**
基于执行的反馈（如单元测试）通过测试时扩展（TTS）和强化学习（RL）被广泛用于编码智能体的开发。该范式需要可扩展且可靠的单元测试用例收集以提供准确反馈，但由此产生的反馈往往较为稀疏，且难以有效区分同为成功或同为失败的执行轨迹。相比之下，来自奖励模型的免执行反馈能够在不依赖单元测试用例的情况下提供更细粒度的信号。尽管具有潜力，面向实际软件工程（SWE）智能体的免执行反馈机制仍未得到充分探索。为开发在TTS与RL场景下均有效的通用奖励模型，我们观察到两个在TTS性能上几乎相同的验证器在RL中可能产生截然不同的结果。直观而言，TTS主要反映模型选择最优轨迹的能力，但该能力未必能泛化至RL场景。为突破此局限，我们识别出对RL训练至关重要的两个额外维度：分类准确性与校准性。通过开展系统性对照实验，我们深入探究如何训练能够在这类指标上均表现稳健的奖励模型，具体分析了训练数据规模、策略混合方式及数据源构成等多重因素的影响。基于上述研究，我们提出SWE-RM——一个采用专家混合架构的精准稳健奖励模型，其总参数量为300亿，推理时激活参数量为30亿。SWE-RM显著提升了SWE智能体在TTS与RL场景下的性能表现。例如，在SWE-Bench Verified基准测试中，通过TTS方法，该模型将Qwen3-Coder-Flash的准确率从51.6%提升至62.0%，将Qwen3-Coder-Max的准确率从67.0%提升至74.6%，在开源模型中实现了新的最优性能。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.21919) | [arXiv](https://arxiv.org/abs/2512.21919)



---

### 11. SlideTailor：面向科研论文的个性化演示文稿幻灯片生成系统

**原文标题：** SlideTailor: Personalized Presentation Slide Generation for Scientific Papers

**摘要：**
自动演示文稿幻灯片生成技术能够显著简化内容创作流程。然而，由于不同用户的偏好存在差异，现有研究中未充分明确的生成框架往往导致生成结果难以满足用户的个性化需求。本文提出一种基于用户偏好条件的论文至幻灯片生成新任务，并设计了一种受人类行为启发的智能体框架SlideTailor，该框架能够以渐进式、适应用户偏好的方式生成可编辑的幻灯片。与要求用户以详细文本形式描述偏好的传统方法不同，本系统仅需用户提供一个论文-幻灯片示例对和一个视觉模板——这些自然且易于提供的素材隐式地编码了用户在内容组织与视觉风格方面的丰富偏好。尽管这些输入信息具有隐式且无标注的特性，本框架仍能有效提炼并泛化用户偏好，从而指导定制化幻灯片的生成。此外，我们提出一种新颖的语音链式机制，使幻灯片内容与预设的口头讲述规划保持协同。这一设计显著提升了生成幻灯片的质量，并支持视频演示等下游应用。为支撑该新任务的研究，我们构建了一个涵盖多样化用户偏好的基准数据集，并设计了具有可解释性的评估指标以进行鲁棒性验证。大量实验结果表明，本框架在个性化幻灯片生成方面具有显著优势。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.20292) | [arXiv](https://arxiv.org/abs/2512.20292)



---

### 12. SVBench：视频生成模型在社会推理能力上的评估

**原文标题：** SVBench: Evaluation of Video Generation Models on Social Reasoning

**摘要：**
近期文本到视频生成模型在视觉真实性、运动连贯性及文本-视频对齐方面展现出显著进展，但其生成社会一致性行为的能力仍存在根本性局限。与人类能够轻松从简短视觉线索中推断意图、信念、情感及社会规范不同，当前模型往往仅呈现字面场景，未能捕捉深层的因果或心理逻辑。为系统评估这一差距，我们提出了首个面向视频生成的社会推理基准测试。基于发展心理学与社会心理学的研究成果，该基准将三十个经典社会认知范式归纳为七个核心维度，包括心理状态推断、目标导向行为、共同注意、社会协调、亲社会行为、社会规范及多智能体策略。为实现这些范式的可操作化，我们开发了一套完全无需训练的基于智能体的流程，该流程能够：（1）提炼每个实验的推理机制，（2）合成多样化的视频适用场景，（3）通过基于线索的批判机制确保概念中立性与难度控制，（4）利用高性能视觉语言模型作为评估者，在社会推理的五个可解释维度上对生成视频进行评价。基于此框架，我们对七种前沿视频生成系统进行了首次大规模研究。结果显示存在显著的性能差距：尽管现代模型在表层合理性方面表现优异，但在意图识别、信念推理、共同注意及亲社会行为推断等维度上普遍存在系统性缺陷。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.21507) | [arXiv](https://arxiv.org/abs/2512.21507)



---

### 13. 基于可验证奖励的强化学习中样本极性再思考

**原文标题：** Rethinking Sample Polarity in Reinforcement Learning with Verifiable Rewards

**摘要：**
大型推理模型通常采用基于可验证奖励的强化学习方法进行训练，以提升其推理能力。在该范式下，策略更新同时使用正负两种自生成轨迹样本，这两种样本对应着不同的极性特征。本文系统研究了样本极性对可验证奖励强化学习训练动态与行为模式的影响机制。研究发现：正样本能够强化已有的正确推理模式，而负样本则有助于探索新的推理路径。我们进一步探究了在样本层面与词元层面对正负样本优势值进行调整如何影响训练过程。基于这些发现，本文提出一种面向策略优化的自适应非对称词元级优势塑造方法（A3PO），该方法能够根据不同极性特征，将优势信号更精确地分配到关键词元上。在五个推理基准测试上的实验验证了该方法的有效性。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.21625) | [arXiv](https://arxiv.org/abs/2512.21625)



---

### 14. 一种适用于通用3×3矩阵乘法的58次加法、秩23算法

**原文标题：** A 58-Addition, Rank-23 Scheme for General 3x3 Matrix Multiplication

**摘要：**
本文提出了一种针对一般非交换环上精确3×3矩阵乘法的最新算法，该算法实现了仅需58次标量加法的秩23计算方案。这一结果在不改变基的前提下，将先前最佳的加法复杂度从60次进一步降低。该算法是通过结合三元受限翻转图探索与贪婪交集约简的自动搜索方法发现的，该方法能有效消除公共子表达式。所得计算方案仅使用{-1, 0, 1}范围内的系数，确保了算法在任意域上的高效性与可移植性。标量运算总量从83次减少至81次。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.21980) | [arXiv](https://arxiv.org/abs/2512.21980)



---

### 15. 遮蔽教师与强化学生：视觉语言模型的知识蒸馏方法

**原文标题：** Masking Teacher and Reinforcing Student for Distilling Vision-Language Models

**摘要：**
大规模视觉语言模型（VLMs）近期在多模态理解方面取得了显著进展，但其庞大的参数量使其难以部署于移动或边缘设备。这催生了对于紧凑且高性能VLMs的需求，此类模型需要能够从强大的大型教师模型中高效学习知识。然而，由于师生模型之间存在巨大的规模差距，将知识从大型教师模型蒸馏到小型学生模型仍面临挑战：学生模型往往难以复现教师模型复杂的高维表征，导致学习过程不稳定且性能下降。为解决这一问题，我们提出Masters（遮蔽教师与强化学生）框架——一种基于掩码渐进强化学习（RL）的蒸馏方法。该框架首先遮蔽教师模型中非主导权重以降低不必要的复杂度，随后通过在训练过程中逐步恢复教师模型容量，实现渐进式知识传递。这种策略使学生模型能够以平稳、稳定的方式从教师模型中学习更丰富的表征。为进一步优化知识迁移，Masters框架整合了离线强化学习阶段，该阶段包含两种互补奖励机制：衡量生成响应正确性的精度奖励，以及量化从教师到学生响应迁移难易程度的蒸馏奖励。相较于计算成本高昂且生成冗长响应的在线“思考-回答”强化学习范式，本方法利用遮蔽教师模型预生成的响应进行离线强化学习。这些响应提供了丰富而高效的指导，使学生模型无需经过“思考-回答”过程即可实现强劲性能。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.22238) | [arXiv](https://arxiv.org/abs/2512.22238)



---


## 🔍 关键词云图
![关键词云图](../images/keywords_wordcloud.png)

## 📈 近期论文趋势
![论文趋势](../images/daily_papers.png)

## 🎙️ 语音播报
- [收听今日论文解读](../audio/2025-12-29_daily_papers.mp3)

## 📱 订阅渠道
- GitHub: [hf-daily-paper-newsletter-chinese](https://github.com/2404589803/hf-daily-paper-newsletter-chinese)