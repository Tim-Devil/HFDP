<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Hugging Face 论文日报 - 2026-01-15</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', sans-serif;
            line-height: 1.6;
            color: #333;
            background-color: #f5f5f5;
            padding: 20px;
        }
        
        .container {
            max-width: 900px;
            margin: 0 auto;
            background-color: white;
            padding: 40px;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }
        
        h1 {
            color: #2c3e50;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
            margin-bottom: 30px;
            font-size: 28px;
        }
        
        h1 img {
            vertical-align: middle;
            margin-right: 10px;
        }
        
        h2 {
            color: #34495e;
            margin-top: 40px;
            margin-bottom: 20px;
            font-size: 24px;
            border-left: 4px solid #3498db;
            padding-left: 15px;
        }
        
        h3 {
            color: #2c3e50;
            margin-top: 30px;
            margin-bottom: 15px;
            font-size: 20px;
        }
        
        p {
            margin-bottom: 15px;
            text-align: justify;
        }
        
        ul {
            margin-left: 20px;
            margin-bottom: 20px;
        }
        
        li {
            margin-bottom: 8px;
        }
        
        a {
            color: #3498db;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        strong {
            color: #2c3e50;
            font-weight: 600;
        }
        
        hr {
            border: none;
            border-top: 1px solid #e0e0e0;
            margin: 30px 0;
        }
        
        /* 关键修复:限制图片宽度 */
        img {
            max-width: 100%;
            height: auto;
            display: block;
            margin: 20px auto;
            border-radius: 4px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        
        /* 确保图片容器也有宽度限制 */
        p img {
            max-width: 100%;
        }
        
        /* 论文详情区域样式 */
        .paper-section {
            background-color: #f9f9f9;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 20px;
        }
        
        /* 统计信息样式 */
        .stats {
            background-color: #e8f4f8;
            padding: 15px;
            border-radius: 6px;
            margin-bottom: 20px;
        }
        
        /* 响应式设计 */
        @media (max-width: 768px) {
            .container {
                padding: 20px;
            }
            
            h1 {
                font-size: 24px;
            }
            
            h2 {
                font-size: 20px;
            }
            
            h3 {
                font-size: 18px;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <h1><img src="https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo.png" width="30"/> Hugging Face 2026-01-15 论文日报</h1>
<h2>📊 今日论文统计</h2>
<ul>
<li>总论文数：27</li>
<li>热门领域：GPT, RL, LLM, Transformer, NLP</li>
</ul>
<h2>📝 论文详情</h2>
<h3>1. 面向算法代码优化的受控自演化方法</h3>
<p><strong>原文标题：</strong> Controlled Self-Evolution for Algorithmic Code Optimization</p>
<p><strong>摘要：</strong>
自演化方法通过"生成-验证-精化"的迭代循环增强代码生成能力，但现有方法存在探索效率低下的问题，难以在有限资源约束下发现具有更优复杂度的解决方案。这种低效性源于三个核心瓶颈：初始化偏差使演化过程陷入次优解区域、缺乏反馈引导的随机操作难以控制，以及跨任务经验利用不足。为解决这些瓶颈，我们提出受控自演化框架，该框架包含三个关键组件：多样化规划初始化生成结构相异的算法策略以实现广阔解空间覆盖；遗传演化机制以反馈引导的定向突变与组合交叉替代随机操作；分层演化记忆系统在任务间与任务内层面同时捕获成功与失败经验。在EffiBench-X基准上的实验表明，CSE在不同大语言模型基座上均持续优于所有基线方法。此外，CSE在演化早期即展现出更高效率，并在整个演化过程中保持持续改进能力。代码已开源：https://github.com/QuantaAlpha/EvoControl。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.07348">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.07348">arXiv</a></p>
<hr />
<h3>2. DeepResearchEval：一种用于深度研究任务构建与智能体评估的自动化框架</h3>
<p><strong>原文标题：</strong> DeepResearchEval: An Automated Framework for Deep Research Task Construction and Agentic Evaluation</p>
<p><strong>摘要：</strong>
深度研究系统广泛用于多步骤网络研究、分析与跨来源信息整合，但其评估仍面临挑战。现有基准测试通常需要大量人工标注的任务构建，依赖静态评估维度，或在缺乏引用时无法可靠验证事实。为弥补这些不足，我们提出了DeepResearchEval——一种用于深度研究任务构建与智能体评估的自动化框架。在任务构建方面，我们设计了一种基于人物角色的流程，通过多样化用户画像生成真实且复杂的研究任务，并应用任务资格筛选与搜索必要性检验的两阶段过滤机制，仅保留需要多源证据整合与外部检索的任务。在评估方面，我们提出了一种智能体评估流程，包含两个核心组件：自适应点式质量评估——能够根据每个生成的任务动态推导任务特定的评估维度、标准与权重；以及主动事实核查——即使在没有引用的情况下，也能通过网络搜索自主提取并验证报告中的陈述。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.09688">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.09688">arXiv</a></p>
<hr />
<h3>3. MAXS：基于大语言模型智能体的元自适应探索框架</h3>
<p><strong>原文标题：</strong> MAXS: Meta-Adaptive Exploration with LLM Agents</p>
<p><strong>摘要：</strong>
大语言模型智能体通过多工具协作展现出固有的推理能力。然而在智能体推理过程中，现有方法常面临两大问题：（一）因缺乏前瞻性而导致的局部短视生成；（二）轨迹不稳定性，即早期微小误差可能演变为发散推理路径。这些问题使得全局有效性与计算效率难以兼顾。为应对上述挑战，我们提出基于大语言模型智能体的元自适应探索框架MAXS（https://github.com/exoskeletonzj/MAXS），该元自适应推理框架能灵活整合工具执行与推理规划。MAXS采用前瞻策略将推理路径延伸若干步骤，评估工具使用的优势值，并结合步骤一致性方差与跨步骤趋势斜率联合选择稳定、一致且高价值的推理步骤。此外，我们引入轨迹收敛机制，在达成路径一致性时停止进一步推演以控制计算成本，从而实现多工具推理中资源效率与全局有效性的平衡。我们在三个基础模型（MiMo-VL-7B、Qwen2.5-VL-7B、Qwen2.5-VL-32B）和五个数据集上进行了广泛实验，结果表明MAXS在性能与推理效率方面均持续优于现有方法。进一步分析验证了我们前瞻策略与工具使用机制的有效性。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.09259">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.09259">arXiv</a></p>
<hr />
<h3>4. A^3-Bench：基于锚点与吸引子激活的记忆驱动科学推理基准测试</h3>
<p><strong>原文标题：</strong> A^3-Bench: Benchmarking Memory-Driven Scientific Reasoning via Anchor and Attractor Activation</p>
<p><strong>摘要：</strong>
科学推理不仅依赖于逻辑推断，还需要激活先验知识与经验结构。记忆能够高效复用知识并增强推理的一致性与稳定性。然而，现有基准测试主要评估最终答案或逐步推理的连贯性，忽视了人类推理背后基于记忆驱动的机制——该机制通过激活锚点与吸引子，并将其整合至多步推理中实现。为填补这一空白，我们提出A^3-Bench（https://a3-bench.github.io），这是一个基于锚点与吸引子激活理论构建的双尺度记忆驱动激活科学推理评估基准。首先，我们采用SAPM流程（主体、锚点与吸引子、问题及记忆发展）对跨领域的2,198个科学推理问题进行了系统标注。其次，我们引入基于锚点与吸引子的双尺度记忆评估框架，并提出AAUI（锚点-吸引子利用指数）指标以量化记忆激活率。最后，通过对多种基础模型与推理范式的实验，我们验证了A^3-Bench的有效性，分析了记忆激活如何影响推理性能，从而为记忆驱动的科学推理机制提供了新的研究视角。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.09274">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.09274">arXiv</a></p>
<hr />
<h3>5. 面向卓越长链思维推理的分布对齐序列蒸馏方法</h3>
<p><strong>原文标题：</strong> Distribution-Aligned Sequence Distillation for Superior Long-CoT Reasoning</p>
<p><strong>摘要：</strong>
本报告提出DASD-4B-Thinking——一个轻量级但能力卓越、完全开源的推理模型。该模型在数学、科学推理和代码生成等具有挑战性的基准测试中，取得了同规模开源模型中最先进的性能表现，甚至超越多个更大规模的模型。我们首先对学界广泛采用的蒸馏范式进行批判性重审：基于教师模型生成响应的监督微调，即序列级蒸馏。尽管近期一系列遵循此方案的研究展现了显著的效率优势与强大的实证性能，但这些方法主要基于监督微调视角。因此，现有研究多聚焦于设计启发式的监督微调数据过滤规则，却普遍忽视了蒸馏的核心原则——使学生模型能够学习教师模型的完整输出分布，从而继承其泛化能力。具体而言，我们指出当前实践存在的三个关键局限：1）教师模型序列级分布的表征不足；2）教师输出分布与学生模型学习能力之间的错位；3）教师强制训练与自回归推理产生的曝光偏差。总之，这些缺陷反映出蒸馏过程中系统性缺乏明确的师生交互机制，导致蒸馏的本质未能得到充分发掘。为解决这些问题，我们提出多项方法论创新，共同构建了增强型序列级蒸馏训练流程。值得注意的是，DASD-4B-Thinking仅使用44.8万训练样本就获得了具有竞争力的结果——这比现有大多数开源工作采用的训练数据量少一个数量级。为支持社区研究，我们公开发布了模型与训练数据集。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.09088">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.09088">arXiv</a></p>
<hr />
<h3>6. Fast-ThinkAct：基于可言语化潜在规划的高效视觉-语言-动作推理框架</h3>
<p><strong>原文标题：</strong> Fast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning</p>
<p><strong>摘要：</strong>
视觉-语言-动作任务需要在动态环境中对复杂视觉场景进行推理并执行适应性动作。尽管近期关于推理型视觉-语言-动作模型的研究表明，显式的思维链能够提升泛化能力，但冗长的推理轨迹会导致高推理延迟。本文提出Fast-ThinkAct——一种通过可言语化潜在推理实现紧凑高效规划的推理框架。该方法通过从教师模型蒸馏学习，在偏好引导目标的驱动下对齐操作轨迹，从而学习基于潜在思维链的高效推理机制，同时迁移语言与视觉规划能力以实现具身控制。这种设计实现了推理增强的策略学习，有效连接了紧凑推理与动作执行。在多种具身操作与推理基准测试上的广泛实验表明，Fast-ThinkAct在保持有效长程规划、少样本适应及故障恢复能力的同时，相比最先进的推理型视觉-语言-动作模型最高可降低89.3%的推理延迟，并展现出卓越的性能。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.09708">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.09708">arXiv</a></p>
<hr />
<h3>7. SkinFlow：基于动态视觉编码与分阶段强化学习的开放性皮肤病诊断高效信息传输框架</h3>
<p><strong>原文标题：</strong> SkinFlow: Efficient Information Transmission for Open Dermatological Diagnosis via Dynamic Visual Encoding and Staged RL</p>
<p><strong>摘要：</strong>
通用大规模视觉-语言模型尽管参数量庞大，但在皮肤病学领域常因“注意力弥散”现象而表现不佳——即难以从背景噪声中分离出细微的病理特征。本文挑战了“参数扩展是提升医学精度的唯一路径”这一固有认知，提出SkinFlow框架，将诊断任务重构为视觉信息传输效率的优化问题。该框架采用虚拟宽度动态视觉编码器，在不增加实体参数的前提下实现复杂病理流形的“展开”，并结合两阶段强化学习策略：第一阶段在受限语义空间中对齐显性医学描述，第二阶段重建隐性的诊断纹理特征。此外，我们设计了基于临床实践的评价协议，优先考量诊断安全性与层级化相关性，而非僵化的标签匹配。实验结果表明：我们提出的70亿参数模型在Fitzpatrick17k基准测试中刷新了最高性能记录，相较于通用大模型（如Qwen3VL-235B与GPT-5.2），其Top-1准确率提升12.06%，Top-6准确率提升28.57%。这些发现证明，通过优化几何容量与信息流路径，能够比单纯扩展参数规模产生更优越的诊断推理能力。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.09136">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.09136">arXiv</a></p>
<hr />
<h3>8. OpenDecoder：开放大语言模型解码以在检索增强生成中融入文档质量评估</h3>
<p><strong>原文标题：</strong> OpenDecoder: Open Large Language Model Decoding to Incorporate Document Quality in RAG</p>
<p><strong>摘要：</strong>
大语言模型的发展已在一系列下游任务中取得优异性能，包括基于大语言模型的检索增强生成。生成内容的质量高度依赖于检索信息的有用性，以及大语言模型内部信息处理机制将其整合到答案生成中的能力。通常假设检索信息与问题相关，然而检索信息的相关性和有用性可能因问题与文档集合的不同而存在差异。在答案生成中考虑检索信息的相关性至关重要。本文提出OpenDecoder，这是一种新方法，通过显式评估检索信息并将其作为生成过程中的质量指示特征。我们的目标是构建一个对不同程度噪声上下文具有更强鲁棒性的检索增强生成模型。研究考虑了三种显式评估信息：相关性评分、排序评分和查询性能预测评分。在五个基准数据集上的实验结果表明，OpenDecoder通过超越多种基线方法，展现出卓越的有效性和更好的鲁棒性。重要的是，该范式具有高度灵活性，可与大语言模型针对任意目的的后训练相结合，并能整合任意类型的外部指示特征。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.09028">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.09028">arXiv</a></p>
<hr />
<h3>9. OpenVoxel：面向开放词汇3D场景理解的免训练体素分组与描述方法</h3>
<p><strong>原文标题：</strong> OpenVoxel: Training-Free Grouping and Captioning Voxels for Open-Vocabulary 3D Scene Understanding</p>
<p><strong>摘要：</strong>
本文提出OpenVoxel，一种面向开放词汇3D场景理解任务的免训练算法，用于对稀疏体素进行分组与描述。给定从三维场景多视角图像中获得的稀疏体素栅格化模型，OpenVoxel能够生成描述场景中不同物体的有意义分组。通过利用强大的视觉语言模型与多模态大语言模型，本方法通过对每个分组进行语义描述成功构建信息丰富的场景地图，从而支持开放词汇分割与指代表达分割等进阶三维场景理解任务。与现有方法不同，本方法无需训练过程，且不依赖CLIP/BERT文本编码器生成的嵌入向量，而是直接采用多模态大语言模型进行文本到文本的检索。大量实验表明，本方法在多项指标上优于近期研究成果，尤其在复杂指代表达分割任务中表现突出。相关代码将开源发布。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.09575">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.09575">arXiv</a></p>
<hr />
<h3>10. ExpSeek：面向网络智能体的自触发经验寻求机制</h3>
<p><strong>原文标题：</strong> ExpSeek: Self-Triggered Experience Seeking for Web Agents</p>
<p><strong>摘要：</strong>
经验干预作为一种新兴技术范式，在网络智能体中展现出巨大潜力，其通过积累的经验为智能体提供有价值的洞察，从而增强其交互能力。然而，现有方法主要在任务执行前将经验作为全局上下文被动注入，难以适应智能体与环境交互过程中动态变化的上下文观察。本文提出ExpSeek，将经验干预转向步骤级的主动寻求模式：（1）利用模型内在信号估计步骤级熵阈值以确定干预时机；（2）设计步骤级定制化的经验内容。在四个具有挑战性的网络智能体基准测试中，基于Qwen3-8B和32B模型的实验表明，ExpSeek分别实现了9.3%和7.5%的绝对性能提升。实验验证了熵作为自触发信号的可行性及优势，并揭示即使仅使用4B规模的小型经验模型，也能显著提升更大规模智能体模型的性能。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.08605">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.08605">arXiv</a></p>
<hr />
<h3>11. EvoFSM：基于有限状态机的可控自演化深度研究框架</h3>
<p><strong>原文标题：</strong> EvoFSM: Controllable Self-Evolution for Deep Research with Finite State Machines</p>
<p><strong>摘要：</strong>
尽管基于大语言模型的智能体在深度研究任务中展现出潜力，但现有方法大多依赖固定工作流程，难以适应现实世界中开放式的查询需求。为此，近期研究开始探索通过让智能体重写自身代码或提示来实现自我演化以提升问题解决能力，然而无约束的优化往往引发不稳定性、幻觉及指令偏移等问题。本文提出EvoFSM，一种结构化的自演化框架，通过演化显式的有限状态机而非依赖自由形式的重写，实现了适应性与可控性的统一。EvoFSM将优化空间解耦为宏观的流程（状态转移逻辑）与微观的技能（状态特定行为），从而在清晰的行为边界下实现针对性改进。在评估机制的引导下，EvoFSM通过一组受限操作对有限状态机进行细化，并进一步引入自演化记忆模块，将成功轨迹提炼为可复用的先验知识，将失败模式转化为未来查询的约束条件。在五个多跳问答基准上的广泛实验证明了EvoFSM的有效性。具体而言，EvoFSM在DeepSearch基准上达到了58.0%的准确率。在交互式决策任务上的附加实验结果进一步验证了其泛化能力。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.09465">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.09465">arXiv</a></p>
<hr />
<h3>12. FocusUI：通过位置保持的视觉标记选择实现高效用户界面定位</h3>
<p><strong>原文标题：</strong> FocusUI: Efficient UI Grounding via Position-Preserving Visual Token Selection</p>
<p><strong>摘要：</strong>
视觉语言模型在用户界面定位任务中展现出卓越性能，这得益于其处理日益高分辨率截图的能力。然而，截图被分割为数千个视觉标记（例如2K分辨率下约4700个），导致显著的计算开销并稀释了注意力机制。相比之下，人类在与用户界面交互时通常聚焦于感兴趣区域。本研究开创性地探索高效用户界面定位任务。通过对任务特性与挑战的实际分析，我们提出FocusUI框架——该框架在保持位置连续性的前提下，选择与指令最相关的图像块以实现精确定位。FocusUI解决了两个关键挑战：（1）消除视觉编码中的冗余标记。我们通过融合指令条件评分与基于规则的用户界面图评分（该评分通过降低大范围同质区域的权重来选择具有区分度且与指令相关的视觉标记）构建图像块级监督机制。（2）在视觉标记选择过程中保持位置连续性。我们发现通用视觉标记剪枝方法会因位置信息断裂而导致用户界面定位任务精度严重下降。为此，我们提出创新的位置填充策略，将每个连续被丢弃的视觉标记序列压缩为置于该序列末位索引的特殊标记，从而保持位置连续性。在四个定位基准测试上的综合实验表明，FocusUI超越了专用图形用户界面基线模型。在ScreenSpot-Pro基准测试中，FocusUI-7B较GUI-Actor-7B实现3.7%的性能提升。即使仅保留30%的视觉标记，FocusUI-7B性能仅下降3.2%，同时推理速度提升达1.44倍，峰值GPU内存降低17%。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.03928">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.03928">arXiv</a></p>
<hr />
<h3>13. 大语言模型是否易受偏好削弱攻击？一种诊断偏好对齐与现实有效性权衡的析因分析方法论</h3>
<p><strong>原文标题：</strong> Are LLMs Vulnerable to Preference-Undermining Attacks (PUA)? A Factorial Analysis Methodology for Diagnosing the Trade-off between Preference Alignment and Real-World Validity</p>
<p><strong>摘要：</strong>
大语言模型的训练通常以偏好对齐为优化目标，奖励那些被认为有益且交互友好的输出。然而，这种以偏好为导向的目标可能被利用：操纵性提示可以引导模型倾向于取悦用户的附和，而非基于事实的修正。本研究探讨经过对齐的模型是否易受偏好削弱攻击的影响——这类操纵性提示策略旨在利用模型取悦用户偏好的倾向，却以牺牲真实性为代价。我们提出一种诊断方法论，相比聚合基准分数，该方法能提供更细粒度、更具指向性的分析。通过采用析因评估框架，在受控的2×2⁴实验设计中，我们将提示引发的输出变化分解为系统目标（以事实为导向 vs 以偏好为导向）与PUA式对话因素（指令控制、人格贬损、条件性认可、现实否认）的可解释效应。令人惊讶的是，更先进的模型有时反而更容易受到操纵性提示的影响。除了占主导地位的现实否认因素外，我们还观察到模型特定的效应符号反转以及与PUA式因素的交互作用，这表明需要针对性的防御策略而非统一的鲁棒性方案。这些发现提出了一种新颖、可复现的析因评估方法论，为RLHF等训练后过程提供更细粒度的诊断，通过更细致地理解偏好对齐风险及操纵性提示的影响，助力大语言模型在产品迭代中实现更优的权衡。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.06596">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.06596">arXiv</a></p>
<hr />
<h3>14. TranslateGemma技术报告</h3>
<p><strong>原文标题：</strong> TranslateGemma Technical Report</p>
<p><strong>摘要：</strong>
本文介绍了TranslateGemma——一套基于Gemma 3基础模型的开源机器翻译模型。为增强Gemma 3模型在翻译任务中固有的多语言能力，我们采用两阶段微调方法。首先，通过融合前沿模型生成的大规模高质量合成平行数据与人工翻译平行数据，进行监督式微调。随后实施强化学习阶段，采用包含MetricX-QE与AutoMQM在内的奖励模型集成方案，针对翻译质量进行优化。我们在WMT25测试集的10个语言对上通过人工评估，以及在WMT24++基准测试的55个语言对上通过自动评估，验证了TranslateGemma的有效性。自动评估指标显示，所有规模的TranslateGemma模型相较于基线Gemma 3模型均取得持续且显著的性能提升。值得注意的是，较小规模的TranslateGemma模型常能达到与较大规模基线模型相当的性能，同时具备更优的效率。我们还证明TranslateGemma模型保持了强大的多模态能力，在Vistra图像翻译基准测试中表现出增强性能。本次开源TranslateGemma模型旨在为研究社区提供强大且适应性强的机器翻译工具。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.09012">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.09012">arXiv</a></p>
<hr />
<h3>15. 想象而后规划：基于世界模型的自适应前瞻智能体学习</h3>
<p><strong>原文标题：</strong> Imagine-then-Plan: Agent Learning from Adaptive Lookahead with World Models</p>
<p><strong>摘要：</strong>
世界模型的最新进展为环境状态的未来动态建模提供了可能，使智能体能够在无需访问真实环境的情况下进行推理与决策。现有方法主要执行单步或固定步长的轨迹推演，其在复杂任务规划中的潜力尚未得到充分挖掘。本文提出“想象而后规划”统一框架，通过前瞻想象实现智能体学习。在该框架中，智能体的策略模型与习得的世界模型交互，生成多步“想象”轨迹。由于想象步长可能随任务类型与阶段动态变化，我们引入一种创新的自适应前瞻机制，通过权衡终极目标与任务进展来调节想象深度。生成的想象轨迹蕴含丰富的未来状态信息（如已达成进度与潜在冲突），这些信息与当前观测相融合，构建出兼具部分可观测性与可想象性的马尔可夫决策过程以指导策略学习。我们通过免训练与强化训练两种变体实现该框架。在代表性智能体基准测试中的大量实验表明，该方法显著优于现有基线模型。进一步分析验证了自适应前瞻机制能有效增强智能体的推理能力，为应对更广泛复杂任务提供了重要启示。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.08955">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.08955">arXiv</a></p>
<hr />
<h3>16. 基于稀疏扩散与三维渲染的静态场景高效相机控制视频生成</h3>
<p><strong>原文标题：</strong> Efficient Camera-Controlled Video Generation of Static Scenes via Sparse Diffusion and 3D Rendering</p>
<p><strong>摘要：</strong>
基于扩散模型的现代视频生成模型能够生成高度逼真的视频片段，但其计算效率低下，通常需要数分钟GPU时间才能生成数秒视频。这种低效性对在需要实时交互的应用（如具身人工智能和虚拟/增强现实）中部署生成式视频构成了关键障碍。本文探索了一种静态场景相机条件视频生成的新策略：利用基于扩散的生成模型生成稀疏关键帧集合，随后通过三维重建与渲染合成完整视频。通过将关键帧提升至三维表征并渲染中间视角，我们的方法在保证几何一致性的同时，将生成成本分摊至数百帧。我们进一步提出一种可预测给定相机轨迹最优关键帧数量的模型，使系统能够自适应分配计算资源。最终方法SRENDER对简单轨迹使用极稀疏关键帧，对复杂相机运动则采用更密集关键帧。该方法在生成20秒视频时，比基于扩散的基线方法提速超过40倍，同时保持高视觉保真度与时间稳定性，为高效可控的视频合成提供了实用路径。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.09697">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.09697">arXiv</a></p>
<hr />
<h3>17. 几何稳定性：表征分析中被忽视的维度</h3>
<p><strong>原文标题：</strong> Geometric Stability: The Missing Axis of Representations</p>
<p><strong>摘要：</strong>
现有学习表征分析方法存在盲区：其聚焦于相似性度量，主要评估嵌入向量与外部参考的对齐程度，但相似性仅能揭示表征内容，无法判断结构是否稳健。本文提出几何稳定性这一全新维度，用于量化表征几何在扰动下的保持可靠性，并构建Shesha测量框架。通过在七个领域的2,463种配置中进行实验，我们发现稳定性与相似性在经验层面无关（ρ≈0.01）且机制相异：移除主要主成分后相似性度量会失效，而稳定性仍能敏感捕捉细粒度流形结构。这种差异产生可操作的洞见：在安全监控方面，稳定性可作为功能性几何预警指标，其检测结构漂移的灵敏度比CKA提高近2倍，同时能过滤刚性距离度量中引发误报的非功能性噪声；在可控性方面，监督稳定性可预测线性可操控性（ρ=0.89-0.96）；在模型选择方面，稳定性与可迁移性解耦，揭示了迁移优化所付出的几何代价。超越机器学习领域，稳定性可预测CRISPR扰动一致性与神经-行为耦合度。通过量化系统维持结构的可靠性，几何稳定性为生物与计算系统的表征审计提供了必要补充，与相似性分析形成完整方法论体系。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.09173">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.09173">arXiv</a></p>
<hr />
<h3>18. AI海马体：我们距离人类记忆还有多远？</h3>
<p><strong>原文标题：</strong> The AI Hippocampus: How Far are We From Human Memory?</p>
<p><strong>摘要：</strong>
记忆在增强现代大语言模型与多模态大语言模型的推理能力、适应性及语境保真度方面发挥着基础性作用。随着这些模型从静态预测器转变为能够持续学习和个性化推理的交互系统，记忆机制的整合已成为其架构与功能演进的核心议题。本文对LLMs与MLLMs中的记忆研究进行了全面而结构化的梳理，将现有文献整合为涵盖隐性记忆、显性记忆与智能体记忆范式的统一分类体系。具体而言，本文系统阐述了三类主要记忆框架：隐性记忆指预训练Transformer内部参数所蕴含的知识，包括其记忆存储、关联检索与语境推理能力，近期研究聚焦于对这一潜在记忆的解释、操控与重构方法；显性记忆涉及为增强模型输出而设计的外部存储与检索组件，通过文本语料库、稠密向量及图结构等动态可查询知识表征，实现与信息源的可扩展、可更新交互；智能体记忆在自主智能体中引入具有时间延续性的持久记忆结构，促进多智能体系统中的长期规划、自我一致性与协作行为，对具身交互AI具有重要意义。本文突破文本范畴，进一步探讨了多模态场景下的记忆整合机制，强调视觉、语言、音频与行为模态间的连贯性至关重要。文中系统论述了关键架构进展、基准任务与开放挑战，涵盖记忆容量、对齐机制、事实一致性及跨系统互操作性等核心议题。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.09113">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.09113">arXiv</a></p>
<hr />
<h3>19. 流等变世界模型：部分可观测动态环境中的记忆机制</h3>
<p><strong>原文标题：</strong> Flow Equivariant World Models: Memory for Partially Observed Dynamic Environments</p>
<p><strong>摘要：</strong>
具身系统将世界体验为“流动的交响曲”：多种连续感官输入流与自身运动耦合，并与外部物体动力学交织形成的组合。这些数据流遵循平滑的时间参数化对称性，并通过精确结构化的代数进行组合；然而大多数神经网络世界模型忽视了这一结构，转而反复从数据中重新学习相同的变换。本研究提出“流等变世界模型”框架，将自身运动与外部物体运动统一为单参数李群“流”。我们利用这种统一性实现对变换的群等变性，从而在数百个时间步上提供稳定的潜在世界表征。在二维与三维部分可观测视频世界建模基准测试中，我们证明流等变世界模型显著优于当前基于扩散和记忆增强的先进世界建模架构——尤其在智能体当前视野外存在可预测世界动力学时表现突出。研究表明流等变性对长序列推演具有特殊优势，其泛化能力远超训练时域。通过构建与内外运动相关联的世界模型表征，流等变性为数据高效、对称性引导的具身智能开辟了可扩展路径。项目链接：https://flowequivariantworldmodels.github.io。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.01075">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.01075">arXiv</a></p>
<hr />
<h3>20. DPWriter：基于多样化规划分支的强化学习在创意写作中的应用</h3>
<p><strong>原文标题：</strong> DPWriter: Reinforcement Learning with Diverse Planning Branching for Creative Writing</p>
<p><strong>摘要：</strong>
基于强化学习（RL）的大型语言模型（LLMs）增强方法常导致输出多样性降低，削弱了其在创意写作等开放式任务中的实用性。现有方法缺乏引导多样化探索的显式机制，往往优先考虑优化效率和性能而忽视多样性。本文提出一种围绕半结构化长链思维（CoT）构建的强化学习框架，该框架将生成过程分解为显式规划的中间步骤。我们引入了一种多样化规划分支方法，该方法基于多样性变化在规划阶段策略性地引入分叉，并结合群体感知的多样性奖励以鼓励不同的生成轨迹。在创意写作基准测试上的实验结果表明，我们的方法在保持生成质量的同时显著提升了输出多样性， consistently outperforming existing baselines。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.09609">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.09609">arXiv</a></p>
<hr />
<h3>21. Omni-R1：迈向统一生成式多模态推理范式</h3>
<p><strong>原文标题：</strong> Omni-R1: Towards the Unified Generative Paradigm for Multimodal Reasoning</p>
<p><strong>摘要：</strong>
多模态大语言模型（MLLMs）在多模态推理领域正取得显著进展。早期方法主要关注纯文本推理。近期研究虽已将多模态信息融入推理步骤，但通常遵循单一任务特定的推理模式，这限制了其在各类多模态任务中的泛化能力。实际上，众多多模态任务需要多样化的推理技能，例如聚焦图像特定区域或在图像中标记对象。为解决这一问题，我们提出统一生成式多模态推理范式，通过在推理过程中生成中间图像来统一多样化的多模态推理能力。我们通过Omni-R1实例化这一范式——该框架采用感知对齐损失与感知奖励的两阶段SFT+RL架构，从而实现功能性图像生成。此外，我们提出Omni-R1-Zero，该方法通过从纯文本推理数据中自举逐步可视化信息，无需依赖多模态标注。实验结果表明，Omni-R1在广泛的多模态任务中实现了统一的生成式推理，而Omni-R1-Zero在整体表现上可与Omni-R1持平甚至更优，这为生成式多模态推理指明了具有前景的发展方向。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.09536">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.09536">arXiv</a></p>
<hr />
<h3>22. 告别陈旧反馈：面向开放世界智能体学习的协同演化批评器</h3>
<p><strong>原文标题：</strong> No More Stale Feedback: Co-Evolving Critics for Open-World Agent Learning</p>
<p><strong>摘要：</strong>
基于批评的强化学习已成为训练大语言模型智能体的重要范式，其通过自然语言反馈增强稀疏结果奖励。然而，现有方法多依赖静态或离线批评器模型，无法随策略演化而自适应调整。在在线策略强化学习中，智能体的错误模式会随时间推移发生变化，导致固定批评器逐渐失效，其反馈效用随之递减。为此，我们提出ECHO（基于后见指导优化的演化批评器）框架，通过同步协同演化循环实现策略与批评器的联合优化。ECHO采用级联式轨迹生成机制：批评器对初始轨迹生成多重诊断，随后通过策略精炼实现群体结构化优势估计。针对学习平台期挑战，我们提出饱和度感知增益重塑目标，使批评器能够通过促进高绩效轨迹的渐进改进获得奖励。通过双轨GRPO更新机制，ECHO确保批评反馈与演化策略保持同步。实验结果表明，ECHO在开放世界环境中能实现更稳定的训练过程，并在长周期任务中取得更高的成功率。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.06794">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.06794">arXiv</a></p>
<hr />
<h3>23. SCALER：面向推理的合成可扩展自适应学习环境</h3>
<p><strong>原文标题：</strong> SCALER:Synthetic Scalable Adaptive Learning Environment for Reasoning</p>
<p><strong>摘要：</strong>
强化学习为提升大语言模型的推理能力提供了系统化方法，但其有效性依赖于能够随模型进化持续提供信息量的训练信号。实践中，当任务难度与模型能力失配，或训练过程被少量重复问题模式主导时，强化学习的进展往往受阻。为协同解决这些问题，我们提出SCALER（面向推理的合成可扩展自适应学习环境），该框架通过自适应环境设计来维持有效的学习信号。SCALER引入可扩展的合成流程，将现实编程问题转化为具有可控难度与无限实例生成能力的可验证推理环境，使得强化学习能够突破有限数据集的限制，同时保持严格的正确定性保证。在此基础上，SCALER进一步采用自适应多环境强化学习策略，动态调整实例难度并筛选活跃环境集合，以追踪模型能力边界并维持分布多样性。这种协同适应机制避免了奖励稀疏性，缓解了对狭窄任务模式的过拟合，并支持训练过程中的持续改进。大量实验表明，SCALER在多种推理基准测试中持续优于基于数据集的强化学习基线，并展现出更稳定、更长周期的训练动态。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.04809">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.04809">arXiv</a></p>
<hr />
<h3>24. 焦点引导：从视频扩散模型的语义弱层解锁可控性</h3>
<p><strong>原文标题：</strong> Focal Guidance: Unlocking Controllability from Semantic-Weak Layers in Video Diffusion Models</p>
<p><strong>摘要：</strong>
图像到视频（I2V）生成任务旨在根据参考图像和文本提示合成视频。这要求扩散模型在去噪过程中协调高频视觉约束与低频文本引导。然而，尽管现有I2V模型优先考虑视觉一致性，如何有效耦合这种双重引导以确保对文本提示的严格遵循仍缺乏深入探索。本研究观察到，在基于扩散Transformer（DiT）的I2V模型中，某些中间层表现出较弱的语义响应（称为语义弱层），其表现为文本-视觉相似度的可测量下降。我们将此归因于一种称为“条件隔离”的现象，即对视觉特征的注意力部分脱离文本引导，并过度依赖学习到的视觉先验。为解决此问题，我们提出焦点引导（FG）方法，以增强语义弱层的可控性。FG包含两种机制：（1）细粒度语义引导（FSG）利用CLIP识别参考帧中的关键区域，并将其作为锚点引导语义弱层；（2）注意力缓存将语义响应层的注意力图传递至语义弱层，注入显式语义信号，减轻其对模型学习视觉先验的过度依赖，从而提升对文本指令的遵循能力。为进一步验证本方法并弥补该方向评估标准的缺失，我们提出了一个用于评估I2V模型指令遵循能力的基准测试。在该基准上，焦点引导证明了其有效性和泛化能力：将Wan2.1-I2V的总分提升至0.7250（+3.97%），并将基于MMDiT的HunyuanVideo-I2V评分提升至0.5571（+7.44%）。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.07287">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.07287">arXiv</a></p>
<hr />
<h3>25. 集群工作负载分配：基于自然语言处理的语义软亲和性</h3>
<p><strong>原文标题：</strong> Cluster Workload Allocation: Semantic Soft Affinity Using Natural Language Processing</p>
<p><strong>摘要：</strong>
集群工作负载分配通常需要复杂的配置，存在可用性差距。本文提出一种基于自然语言处理的语义化、意图驱动的集群系统调度范式。该系统通过集成大型语言模型的Kubernetes调度扩展器，解析用于软亲和性偏好的自然语言分配提示注解。我们开发了包含集群状态缓存和意图分析器（使用AWS Bedrock）的原型系统。实证评估表明，在评估基准数据集上，Amazon Nova Pro/Premier和Mistral Pixtral Large等顶级模型实现了较高的LLM解析准确率（子集准确率&gt;95%），显著优于基线引擎。在六种场景下的调度质量测试显示，与标准Kubernetes配置相比，该原型实现了更优或同等的资源安置效果，尤其在复杂场景、量化场景及冲突软偏好处理方面表现突出。结果验证了使用LLM实现可访问调度的可行性，但也揭示了同步LLM延迟等局限性，建议通过异步处理实现生产就绪。本研究证实了语义软亲和性在简化工作负载编排方面的实用价值。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.09282">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.09282">arXiv</a></p>
<hr />
<h3>26. sui-1：基于引证且可验证的长文本摘要生成模型</h3>
<p><strong>原文标题：</strong> sui-1: Grounded and Verifiable Long-Form Summarization</p>
<p><strong>摘要：</strong>
大型语言模型经常生成看似合理但缺乏真实性的摘要，用户无法依据源文本进行验证，这在政府与法律分析等对合规性要求严格的领域中是一个关键局限。本文提出sui-1模型，这是一个拥有240亿参数的模型，能够生成带有文中引证的抽象摘要，使用户能够将每个主张追溯至其源语句。我们的合成数据流程将思维链提示与多阶段验证相结合，从议会文件、网络文本和维基百科等多种来源中，生成了涵盖五种语言的超过22,000个高质量训练样本。评估表明，sui-1模型在性能上显著超越了所有测试的开源基线模型，包括参数量为其三倍的模型。这些结果证明，对于基于引证的摘要生成任务，针对性的训练效果远超单纯扩大模型规模。模型权重及交互演示已公开发布。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.08472">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.08472">arXiv</a></p>
<hr />
<h3>27. SampoNLP：一种用于子词分词器形态学分析的自参照工具包</h3>
<p><strong>原文标题：</strong> SampoNLP: A Self-Referential Toolkit for Morphological Analysis of Subword Tokenizers</p>
<p><strong>摘要：</strong>
子词分词的质量对大语言模型至关重要，然而对形态丰富的乌拉尔语系语言进行分词器评估，常因缺乏清晰的语素词典而受阻。本文介绍SampoNLP——一种无需语料库的形态学词典构建工具包，其采用受最小描述长度原理启发的自参照原子性评分方法，通过内部结构线索过滤复合形式，适用于低资源场景。利用SampoNLP为芬兰语、匈牙利语和爱沙尼亚语生成的高纯度词典，我们对BPE分词器在不同词汇表规模（8k-256k）下进行了系统评估。我们提出统一指标——综合性能评分（IPS），以权衡语素覆盖度与过度切分问题。通过分析IPS曲线，我们确定了收益递减的“拐点”，并首次为这些语言提供了基于实证的最优词汇表规模建议。本研究不仅提供了实践指导，还定量揭示了标准BPE方法对高度黏着性语言的局限性。SampoNLP工具库及所有生成资源已公开：https://github.com/AragonerUA/SampoNLP</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.04469">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.04469">arXiv</a></p>
<hr />
<h2>🔍 关键词云图</h2>
<p><img alt="关键词云图" src="../images/keywords_wordcloud.png" /></p>
<h2>📈 近期论文趋势</h2>
<p><img alt="论文趋势" src="../images/daily_papers.png" /></p>
<h2>🎙️ 语音播报</h2>
<ul>
<li><a href="../audio/2026-01-15_daily_papers.mp3">收听今日论文解读</a></li>
</ul>
<h2>📱 订阅渠道</h2>
<ul>
<li>GitHub: <a href="https://github.com/2404589803/hf-daily-paper-newsletter-chinese">hf-daily-paper-newsletter-chinese</a></li>
</ul>
    </div>
</body>
</html>