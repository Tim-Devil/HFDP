
# <img src="https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo.png" width="30"/> Hugging Face 2025-12-05 论文日报

## 📊 今日论文统计
- 总论文数：38
- 热门领域：LLM, RL, Transformer, GPT

## 📝 论文详情


### 1. DAComp：面向全数据智能生命周期的数据智能体基准测试

**原文标题：** DAComp: Benchmarking Data Agents across the Full Data Intelligence Lifecycle

**摘要：**
现实企业数据智能工作流涵盖将原始数据转化为分析就绪表格的数据工程环节，以及将这些表格转化为决策导向洞见的数据分析环节。本文提出DAComp基准测试，包含210项模拟此类复杂工作流的任务。数据工程任务要求基于工业级数据模式进行仓库级工程实践，包括从零开始设计构建多阶段SQL流水线，以及在需求动态变化时对现有系统进行迭代演进。数据分析任务则呈现开放式商业问题，需要执行战略规划、通过迭代编码进行探索性分析、解读中间结果，并最终形成可执行的决策建议。工程任务采用基于执行的多维度指标评估体系，开放式任务则通过经实验验证的可靠大语言模型评审机制进行测评，该机制依据层次化精细设计的评估准则运作。实验表明，即使当前最先进的智能体在DAComp测试中仍表现欠佳。数据工程任务成功率低于20%，暴露出其在整体流水线编排（而非单纯代码生成）方面存在关键瓶颈。数据分析任务平均得分亦低于40%，凸显其在开放式推理能力上的显著不足，同时证明工程与分析属于两种截然不同的能力维度。通过精准诊断这些局限性，DAComp为驱动企业级全能力自主数据智能体的发展提供了严谨而真实的测试平台。相关数据与代码已开源：https://da-comp.github.io

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.04324) | [arXiv](https://arxiv.org/abs/2512.04324)



---

### 2. 实时虚拟化身：基于流式实时音频驱动的无限时长虚拟化身生成

**原文标题：** Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length

**摘要：**
现有基于扩散模型的视频生成方法受限于序列计算与长时序不一致性问题，难以应用于实时流式音频驱动的虚拟化身合成场景。本文提出Live Avatar——一个算法与系统协同设计的框架，通过140亿参数扩散模型实现高效、高保真、无限时长的虚拟化身生成。我们提出时间步强制流水线并行技术，该分布式推理范式将去噪步骤流水线化分配到多GPU上，有效突破自回归计算瓶颈，确保稳定低延迟的实时流处理。为增强时序一致性并缓解身份漂移与色彩失真问题，我们设计滚动锚定帧机制，通过缓存参考图像动态校准外观特征以保持序列保真度。此外，采用自强制分布匹配蒸馏技术，在保持视觉质量的前提下实现大规模模型的可流式因果适配。Live Avatar在5张H800 GPU上达到端到端20 FPS的生成速度，据我们所知，这是首个在此规模上实现实用化、实时、高保真虚拟化身生成的工作。本研究为在工业级长视频合成应用中部署先进扩散模型建立了新范式。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.04677) | [arXiv](https://arxiv.org/abs/2512.04677)



---

### 3. Nex-N1：基于统一生态系统构建大规模环境训练的智能体模型

**原文标题：** Nex-N1: Agentic Models Trained via a Unified Ecosystem for Large-Scale Environment Construction

**摘要：**
大语言模型从被动响应者向自主智能体的演进，需要学习范式发生根本性转变——从静态模仿转向激励驱动的决策。然而，这一转变因缺乏能够构建高质量交互信号以支持有效策略学习的可扩展基础设施而受到严重阻碍。为此，我们提出一种综合性方法，旨在系统性地扩展交互环境的多样性与复杂性。该方法通过解决三个正交维度实现扩展：（1）复杂性：NexAU作为一种灵活的智能体框架，支持通过简单配置构建复杂的智能体层级结构；（2）多样性：NexA4A能够从自然语言自动生成多样化的智能体层级，覆盖无限领域；（3）真实性：NexGAP通过整合动态真实世界环境进行具身轨迹合成，从而弥合仿真与现实之间的差距。我们基于该基础设施所建立的多样化复杂交互环境训练了Nex-N1模型。在SWE-bench和tau2等基准测试中的实证结果表明，Nex-N1在复杂智能体任务上持续优于当前最先进的开源模型，并与前沿专有模型展现出竞争力。我们开源了Nex生态系统及模型权重以促进进一步研究。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.04987) | [arXiv](https://arxiv.org/abs/2512.04987)



---

### 4. ARM-Thinker：通过智能工具调用与视觉推理增强多模态生成式奖励模型

**原文标题：** ARM-Thinker: Reinforcing Multimodal Generative Reward Models with Agentic Tool Use and Visual Reasoning

**摘要：**
奖励模型对于使视觉-语言系统与人类偏好对齐至关重要，但现有方法存在幻觉问题、视觉基础薄弱且无法利用工具进行验证，限制了其在复杂多模态推理任务中的可靠性。本文提出ARM-Thinker，一种具备自主调用外部工具（如图像裁剪、文档页面检索）能力的智能多模态奖励模型，通过可验证证据支撑判断，取代静态、非交互式的奖励评分机制。该模型能够验证细粒度视觉细节、交叉引用多页面证据并检验推理主张，这些能力是现有奖励模型所缺失的。我们采用多阶段强化学习训练ARM-Thinker，联合优化工具调用决策与判断准确性。为评估智能奖励建模能力，我们构建了ARMBench-VL评测集，包含三个基准测试：细粒度视觉基础（图像级工具）、多页面文档理解（检索工具）和指令遵循（文本级验证）。ARM-Thinker在奖励建模基准上平均提升16.2%，在工具使用任务中提升9.6%，并在多模态数学与逻辑推理基准上超越基线模型。实验结果表明，智能能力可显著提升奖励模型的准确性与可解释性。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.05111) | [arXiv](https://arxiv.org/abs/2512.05111)



---

### 5. 奖励强制：基于奖励分布匹配蒸馏的高效流式视频生成

**原文标题：** Reward Forcing: Efficient Streaming Video Generation with Rewarded Distribution Matching Distillation

**摘要：**
高效的流式视频生成对于模拟交互式动态世界至关重要。现有方法通过滑动窗口注意力机制蒸馏少步视频扩散模型，利用初始帧作为汇聚令牌以保持注意力性能并减少误差累积。然而，视频帧会过度依赖这些静态令牌，导致初始帧被复制且运动动态性减弱。为解决这一问题，我们提出"奖励强制"这一新颖框架，其包含两项关键设计。首先，我们提出EMA-Sink机制，该机制维护从初始帧初始化的固定尺寸令牌，并通过指数移动平均融合退出滑动窗口的淘汰令牌进行持续更新。在不增加额外计算成本的前提下，EMA-Sink令牌既能捕捉长期上下文信息，又能保留近期动态特征，从而在保持长时序一致性的同时避免初始帧复制问题。其次，为更好地从教师模型中蒸馏运动动态特征，我们提出创新的奖励分布匹配蒸馏方法。传统分布匹配平等对待所有训练样本，限制了模型优先学习动态内容的能力。而Re-DMD通过视觉语言模型对动态性更强的样本进行优先级加权，使模型输出分布偏向高奖励区域。该方法在保持数据保真度的同时显著提升了运动质量。我们通过定量与定性实验表明，奖励强制框架在标准基准测试中达到最先进性能，并能在单张H100 GPU上以23.1 FPS实现高质量流式视频生成。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.04678) | [arXiv](https://arxiv.org/abs/2512.04678)



---

### 6. 语义先行：通过异步潜在扩散协调语义与纹理建模

**原文标题：** Semantics Lead the Way: Harmonizing Semantic and Texture Modeling with Asynchronous Latent Diffusion

**摘要：**
潜在扩散模型（LDMs）本质上遵循从粗到细的生成过程，其中高层语义结构的生成略早于细粒度纹理。这表明先行的语义可通过提供语义锚点来促进纹理生成。近期研究通过整合预训练视觉编码器的语义先验来增强LDMs，但仍同步地对语义和VAE编码的纹理进行去噪，忽视了这种时序特性。基于此，我们提出语义先行扩散模型（SFD），这是一种显式优先构建语义的潜在扩散范式。SFD首先通过专用语义VAE从预训练视觉编码器中提取紧凑语义潜在表示，并将其与纹理潜在表示结合以构建复合潜在表示。SFD的核心在于采用分离的噪声调度对语义与纹理潜在表示进行异步去噪：语义处理通过时间偏移先于纹理处理，从而为纹理优化提供更清晰的高层指导，实现自然的从粗到细生成。在ImageNet 256×256数据集上使用引导技术时，SFD取得了FID 1.06（LightningDiT-XL）和FID 1.04（1.0B LightningDiT-XXL）的指标，同时收敛速度比原始DiT提升高达100倍。SFD还能改进ReDi、VA-VAE等现有方法，验证了异步语义引导建模的有效性。项目页面与代码：https://yuemingpan.github.io/SFD.github.io/。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.04926) | [arXiv](https://arxiv.org/abs/2512.04926)



---

### 7. PaperDebugger：一种基于插件的多智能体系统，用于编辑器内的学术写作、审阅与编辑

**原文标题：** PaperDebugger: A Plugin-Based Multi-Agent System for In-Editor Academic Writing, Review, and Editing

**摘要：**
大型语言模型正日益融入学术写作流程，但现有的辅助工具仍处于编辑器外部，无法与文档状态、结构及修订历史进行深度交互。这种分离使得在Overleaf等LaTeX编辑器中直接支持具备自主性与上下文感知的操作变得不可能。本文提出PaperDebugger，一种集成于编辑器内部、基于多智能体与插件的学术写作助手，它将LLM驱动的推理直接引入写作环境。实现此类编辑器内交互在技术上具有挑战性：需要与编辑器进行可靠的双向同步、细粒度的版本控制与补丁管理、安全的状态维护、多智能体调度，以及与外部工具的可扩展通信。PaperDebugger通过一个经Chrome审核的扩展程序、一个基于Kubernetes的原生编排层，以及一个集成文献检索、参考文献查找、文档评分和修订流程的模型上下文协议工具链，应对了这些挑战。我们的演示展示了一个完全集成的工作流程，包括局部编辑、结构化审阅、并行智能体执行以及基于差异比较的更新，所有这些功能均封装在一个低侵入性的用户界面中。初步汇总的分析数据显示了积极的用户参与度，并验证了这种原生集成于编辑器、具备自主性的写作助手的实用性。更多演示详情与视频可访问https://github.com/PaperDebugger/PaperDebugger。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.02589) | [arXiv](https://arxiv.org/abs/2512.02589)



---

### 8. 4DLangVGGT：基于Transformer的四维语言-视觉几何关联模型

**原文标题：** 4DLangVGGT: 4D Language-Visual Geometry Grounded Transformer

**摘要：**
构建四维语言场对于具身人工智能、增强/虚拟现实以及四维场景理解至关重要，因为它能为动态环境提供丰富的语义表征，并支持在复杂场景中进行开放词汇查询。然而，现有四维语义场构建方法主要依赖于场景特定的高斯泼溅技术，这种方法需要进行逐场景优化，泛化能力有限，且难以扩展到实际应用中。为克服这些局限性，我们提出了4DLangVGGT——首个基于Transformer的前馈式统一框架，用于四维语言关联，该框架将几何感知与语言对齐集成于单一架构中。4DLangVGGT包含两个核心组件：四维视觉几何Transformer（StreamVGGT），用于捕捉动态场景的时空几何表征；以及语义桥接解码器（SBD），可将几何感知特征投影到语言对齐的语义空间中，从而在保持结构保真度的同时增强语义可解释性。与先前依赖昂贵逐场景优化的方法不同，4DLangVGGT能够在多个动态场景上进行联合训练，并在推理时直接应用，实现了部署效率与强大泛化能力的统一。这一设计显著提升了大规模部署的实用性，并为开放词汇的四维场景理解建立了新范式。在HyperNeRF和Neu3D数据集上的实验表明，我们的方法不仅具有有效泛化能力，而且取得了最先进的性能：在单场景训练下性能提升最高达2%，在多场景训练下提升达1%。代码已发布于https://github.com/hustvl/4DLangVGGT。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.05060) | [arXiv](https://arxiv.org/abs/2512.05060)



---

### 9. DynamicVerse：一种面向物理感知的多模态四维世界建模框架

**原文标题：** DynamicVerse: A Physically-Aware Multimodal Framework for 4D World Modeling

**摘要：**
理解动态物理世界——其特征包括不断演化的三维结构、真实世界运动以及带有文本描述的语义内容——对于人机交互至关重要，并使具身智能体能够以类人能力在真实环境中感知与行动。然而，现有数据集通常源自有限的模拟器，或采用传统运动恢复结构方法进行尺度化标注，且提供的描述性文本有限，这制约了基础模型从互联网常见的单目视频中准确解析真实世界动态的能力。为弥补这些不足，我们提出了DynamicVerse：一个面向动态真实世界视频的物理尺度多模态四维世界建模框架。我们运用大规模视觉、几何与多模态模型来解析度量尺度的静态几何、真实世界动态运动、实例级掩码及整体描述性文本。通过将基于窗口的集束调整与全局优化相结合，我们的方法能够将长时真实世界视频序列转化为完整的四维多模态格式。DynamicVerse构建了一个大规模数据集，包含来自互联网视频的10万+视频片段、80万+标注掩码及1000万+帧图像。在视频深度估计、相机位姿估计和相机内参估计三项基准任务上的实验评估表明，我们的四维建模方法在捕捉物理尺度测量方面优于现有方法，具有更高的全局精度。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.03000) | [arXiv](https://arxiv.org/abs/2512.03000)



---

### 10. UltraImage：重新思考图像扩散变换器中的分辨率外推

**原文标题：** UltraImage: Rethinking Resolution Extrapolation in Image Diffusion Transformers

**摘要：**
近期基于变换器的图像扩散模型虽能实现高保真度生成，但在超越训练尺度生成图像时仍面临内容重复与质量退化问题。本研究提出UltraImage这一系统性框架以同时解决上述挑战。通过对位置嵌入进行频域分析，我们发现内容重复现象源于主导频率的周期性特征，其周期与训练分辨率直接相关。为此，我们提出递归式主导频率校正机制，在外推后将其约束于单周期范围内。进一步研究发现，质量退化问题源于注意力稀释效应，据此提出熵引导的自适应注意力聚焦方法：通过分配更高的聚焦因子以增强局部注意力获取精细细节，同时降低全局注意力模式的聚焦程度以保持结构一致性。实验表明，在Qwen-Image和Flux（约4K分辨率）的三种生成场景中，UltraImage均持续优于现有方法，有效减少重复现象并提升视觉保真度。此外，该模型仅需1328p训练分辨率即可生成高达6K*6K的无低分辨率引导图像，展现出卓越的外推能力。项目页面详见：https://thu-ml.github.io/ultraimage.github.io/

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.04504) | [arXiv](https://arxiv.org/abs/2512.04504)



---

### 11. Splannequin：基于双重检测溅射的单视角人体模型挑战视频冻结渲染

**原文标题：** Splannequin: Freezing Monocular Mannequin-Challenge Footage with Dual-Detection Splatting

**摘要：**
从单视角人体模型挑战视频中合成高保真的冻结三维场景，是一个与标准动态场景重建截然不同的独特问题。我们的目标并非侧重于运动建模，而是创建冻结场景，同时策略性地保留细微动态以实现用户可控的即时选择。为此，我们引入动态高斯溅射的一种新颖应用：场景通过动态建模以保留邻近时间的变化，并通过固定模型的时间参数渲染静态场景。然而，在这种应用模式下，稀疏时间监督的单视角捕捉会导致高斯元在弱监督时间戳上变得不可见或被遮挡时，产生重影和模糊等伪影。我们提出Splannequin——一种与架构无关的正则化方法，通过检测高斯基元的两种状态（隐藏状态与缺陷状态）并施加时间锚定来解决此问题。在相机主要向前运动的场景下，隐藏状态被锚定至其近期被充分观测的过去状态，而缺陷状态则被锚定至具有更强监督的未来状态。该方法通过简单的损失项即可集成到现有动态高斯流程中，无需改变架构且不增加任何推理开销。实验表明，该方法显著提升了视觉质量，实现了高保真且可供用户选择的冻结时刻渲染，在用户偏好测试中获得96%的认可率。项目页面：https://chien90190.github.io/splannequin/

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.05113) | [arXiv](https://arxiv.org/abs/2512.05113)



---

### 12. 基于模型与样本高效的AI辅助球体堆积数学发现

**原文标题：** Model-Based and Sample-Efficient AI-Assisted Math Discovery in Sphere Packing

**摘要：**
球体堆积问题，即希尔伯特第十八问题，旨在探寻n维欧几里得空间中全等球体的最密堆积方式。尽管该问题与密码学、晶体学和医学成像等领域密切相关，但其仍未得到解决：除少数特殊维度外，既未发现最优堆积方式，也未建立紧致的上界。即使在n=8维度上取得的重大突破（该成果后来获得了菲尔兹奖）也凸显了此问题的难度。目前主流的求上界技术——三点法，将问题转化为求解大规模、高精度的半定规划问题。由于每个候选半定规划可能需要数天时间进行评估，传统数据密集型的AI方法难以适用。为此，我们将半定规划构建过程形式化为一个序列决策过程（即半定规划博弈），其中策略从一组可容许的组件中组装出半定规划表达式。通过采用一种结合贝叶斯优化与蒙特卡洛树搜索的、基于模型的样本高效框架，我们在4至16维度上获得了新的最优上界，表明基于模型的搜索能够推动长期几何问题的计算进展。这些结果共同证明，样本高效的基于模型搜索能够在数学结构严格、评估受限的问题上取得实质性进展，为超越大规模大语言模型驱动探索的AI辅助发现指明了一条互补的研究路径。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.04829) | [arXiv](https://arxiv.org/abs/2512.04829)



---

### 13. SIMA 2：面向虚拟世界的通用具身智能体

**原文标题：** SIMA 2: A Generalist Embodied Agent for Virtual Worlds

**摘要：**
本文介绍SIMA 2，一种能够在多样三维虚拟世界中理解并行动的通用具身智能体。基于Gemini基础模型构建的SIMA 2，标志着在具身环境中实现主动、目标导向交互的重要进展。与先前局限于简单语言指令的研究（如SIMA 1）不同，SIMA 2能够作为交互伙伴，对高层次目标进行推理，与用户对话，并处理通过语言和图像给出的复杂指令。在涵盖多种游戏的测试集中，SIMA 2显著缩小了与人类表现的差距，并展现出对未见环境的强大泛化能力，同时保持了基础模型的核心推理能力。此外，我们展示了其开放式自我改进的潜力：通过利用Gemini生成任务并提供奖励，SIMA 2能够在全新环境中从零开始自主学习新技能。这项工作为创建适用于虚拟乃至最终物理世界的通用且持续学习的智能体验证了一条可行路径。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.04797) | [arXiv](https://arxiv.org/abs/2512.04797)



---

### 14. DraCo：草稿即思维链——面向文本到图像预览与稀有概念生成的推理范式

**原文标题：** DraCo: Draft as CoT for Text-to-Image Preview and Rare Concept Generation

**摘要：**
近期出现的统一多模态大语言模型（MLLM）展现出令人瞩目的能力，通过引入思维链（CoT）推理机制增强了文本到图像的生成效果。然而，现有方法仍存在局限：要么仅将模型视为独立生成器，要么依赖于抽象的文本规划。为此，我们提出草稿即思维链（DraCo）这一新颖的交错式推理范式，该范式在思维链中充分利用文本与视觉内容，以实现更优的规划与验证。我们的方法首先生成低分辨率草稿图像作为预览，提供更具体、结构化的视觉规划与引导；随后，利用模型固有的理解能力验证草稿与输入提示间潜在的语义偏差，并通过超分辨率选择性修正进行细化。通过这种方式，我们的方法解决了两个根本性挑战：文本规划的粗粒度特性以及稀有属性组合的生成困难。为支持训练，我们构建了DraCo-240K数据集，旨在提升涵盖通用修正、实例操控与布局重组的三项基础能力。在DraCo-CFG（一种专为交错推理设计的无分类器引导策略）的支持下，DraCo在GenEval（+8%）、Imagine-Bench（+0.91）和GenEval++（+3%）指标上实现显著提升，显著优于直接生成及其他基于思维链的生成方法。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.05112) | [arXiv](https://arxiv.org/abs/2512.05112)



---

### 15. TV2TV：一种用于交错式语言与视频生成的统一框架

**原文标题：** TV2TV: A Unified Framework for Interleaved Language and Video Generation

**摘要：**
视频生成模型正在快速发展，但在处理需要复杂语义分支或对后续内容进行重复高层推理的视频输出时仍面临挑战。本文提出一类新型全视频-文本模型，通过整合近期语言模型推理进展中的思路以应对这一挑战。具体而言，我们提出TV2TV——一个统一的生成建模框架，将视频生成分解为交错进行的文本生成与视频生成过程。TV2TV采用混合变换器架构，联合学习语言建模（下一词元预测）与视频流匹配（下一帧预测）。在推理阶段，TV2TV自主决定文本生成与视频帧生成的切换时机，使模型能够在“以像素行动”生成帧序列之前，先“以文字思考”后续内容。该设计将决定后续内容的主要责任转移至语言建模模块，从而提升生成视频的视觉质量与提示对齐度。同时，该框架支持细粒度可控性，允许用户在生成过程中任意节点通过文本干预修改视频生成轨迹。在游戏视频数据的受控实验中，TV2TV在视觉质量与可控性方面均展现出显著提升。该模型亦可扩展至自然视频领域：我们通过视觉-语言模型为体育视频添加交错式自然语言动作描述构建数据集，在此语料上训练的TV2TV表现出优异的视觉质量与提示对齐能力，证明了该模型对复杂现实世界动作序列进行推理与生成的有效性。这些成果共同表明，TV2TV为实现融合开放式文本推理与控制的视频生成迈出了重要一步。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.05103) | [arXiv](https://arxiv.org/abs/2512.05103)



---

### 16. SignRoundV2：在极低比特大语言模型训练后量化中弥合性能差距

**原文标题：** SignRoundV2: Closing the Performance Gap in Extremely Low-Bit Post-Training Quantization for LLMs

**摘要：**
极低比特量化对于高效部署大语言模型至关重要，但在2比特乃至4比特（例如MXFP4）量化下常导致严重的性能下降。本文提出SignRoundV2，一种无需混合精度仍能高效运行的训练后量化框架。该框架引入两大创新：（1）融合梯度信息与量化偏差的快速敏感度度量方法，用于指导逐层比特分配；（2）轻量级预调优的量化尺度搜索机制，以提升极低比特量化效果。这些组件使SignRoundV2能够弥合与全精度模型之间的性能差距。大量实验表明，本方法在4-5比特量化时能以约1%的精度波动保持大语言模型的竞争性性能，在2比特量化时仍能取得强劲结果，达到生产级应用标准。代码实现已发布于https://github.com/intel/auto-round。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.04746) | [arXiv](https://arxiv.org/abs/2512.04746)



---

### 17. 论Search-R1中的GRPO崩溃：惰性似然位移死亡螺旋

**原文标题：** On GRPO Collapse in Search-R1: The Lazy Likelihood-Displacement Death Spiral

**摘要：**
工具集成强化学习使大语言模型能够通过与搜索引擎和检索器等外部工具交互，执行多步推理。以近期Search-R1为代表的组相对策略优化方法具有快速收敛和无价值函数的形式化特点，在此场景中颇具吸引力，但其训练过程始终存在崩溃问题。本文指出，**惰性似然位移**——即正确与错误回答的似然值系统性降低或停滞——是导致该失败的核心机制。LLD在训练早期出现并触发自我强化的**LLD死亡螺旋**：似然值下降导致低置信度响应，进而引发梯度膨胀，最终致使训练崩溃。我们通过Search-R1风格的搜索集成问答任务，在多模型上实证揭示了该过程具有一致的三阶段轨迹：早期停滞、稳态衰减和加速崩溃。为解决此问题，我们提出一种轻量级**似然保持正则化方法LLDS**，该方法仅在轨迹似然下降时激活，且仅对责任标记进行正则化。这种细粒度结构能以最小优化干扰缓解LLD问题。在七个开放域和多跳问答基准测试中，本方法能稳定训练过程、防止梯度爆炸，并带来显著性能提升——Qwen2.5-3B模型提升37.8%，Qwen2.5-7B模型提升32.0%。本研究确立了LLD作为基于GRPO的工具集成强化学习的根本性瓶颈，并为工具集成大语言模型的稳定可扩展训练提供了实践路径。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.04220) | [arXiv](https://arxiv.org/abs/2512.04220)



---

### 18. 对齐却刻板？系统提示对基于LVLM的文生图模型社会偏见的隐性影响

**原文标题：** Aligned but Stereotypical? The Hidden Influence of System Prompts on Social Bias in LVLM-Based Text-to-Image Models

**摘要：**
基于大规模视觉语言模型（LVLM）的文生图（T2I）系统已成为图像生成的主流范式，但其是否会放大社会偏见仍缺乏充分理解。本文研究表明，基于LVLM的模型比非LVLM模型生成的社会偏见图像显著更多。我们构建了一个包含四个语言复杂度层级的1024条提示词基准，并系统评估了多属性维度的人口统计学偏见。分析发现，指导LVLM运行的预定义系统提示是偏见行为的主要驱动因素。通过解码中间表征、词元概率诊断和嵌入关联分析，我们揭示了系统提示如何编码人口统计学先验信息并传递至图像合成过程。为此，我们提出FairPro——一种免训练的元提示框架，使LVLM能够在测试阶段进行自我审查并构建公平感知的系统提示。在SANA和Qwen-Image两种基于LVLM的T2I模型上的实验表明，FairPro在保持图文对齐度的同时显著降低了人口统计学偏见。我们相信，这些发现为理解系统提示在偏见传递中的核心作用提供了更深入的见解，并为构建更具社会责任的T2I系统提供了可部署的实用方案。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.04981) | [arXiv](https://arxiv.org/abs/2512.04981)



---

### 19. SeeNav-Agent：通过视觉提示与步级策略优化增强视觉语言导航能力

**原文标题：** SeeNav-Agent: Enhancing Vision-Language Navigation with Visual Prompt and Step-Level Policy Optimization

**摘要：**
现有基于大型视觉语言模型（LVLM）的视觉语言导航（VLN）智能体常受感知误差、推理误差与规划误差的困扰，严重制约其导航性能。为应对这些局限，本研究提出了一种新型VLN智能体框架——SeeNav-Agent。首先，为减少VLN智能体视觉模块的感知幻觉，我们在输入空间中引入了双视角视觉提示技术，该技术同时能增强智能体对当前空间状态的理解。随后，针对VLN智能体的后训练，我们设计了一种新颖的步级强化微调方法——步奖励分组策略优化（SRGPO）。在SRGPO中，我们首先为导航任务定义了可验证的过程奖励，进而通过随机分组不同导航步骤实现高效的步级优势估计。该方法为VLN智能体的强化学习过程提供了密集奖励信号，并显著提升了其规划能力。在EmbodiedBench Navigation基准测试上的实验结果表明：通过引入零样本视觉提示模块，GPT-4.1模型的导航成功率达到了86.7%，较当前最优LVLM模型提升约20个百分点。基于SRGPO进行后训练后，Qwen2.5-VL-3B模型的导航成功率达到72.3%，超越现有最优LVLM模型5.6个百分点。此外，与GRPO、GiGPO等强化微调算法相比，所提出的SRGPO在训练稳定性、收敛效率与泛化能力方面均表现出显著提升。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.02631) | [arXiv](https://arxiv.org/abs/2512.02631)



---

### 20. 基于视频扩散先验的生成式神经视频压缩

**原文标题：** Generative Neural Video Compression via Video Diffusion Prior

**摘要：**
本文提出GNVC-VD，这是首个基于扩散变换器（DiT）的生成式神经视频压缩框架。该框架构建于先进的视频生成基础模型之上，将时空潜在压缩与序列级生成优化统一在单一编解码器中。现有感知编解码器主要依赖预训练的图像生成先验来恢复高频细节，但其逐帧处理特性缺乏时序建模，不可避免地导致感知闪烁问题。为解决这一局限，GNVC-VD引入了统一的流匹配潜在优化模块，通过视频扩散变换器实现序列级去噪，联合增强帧内与帧间潜在表示，从而确保时空细节的一致性。与视频生成中从纯高斯噪声开始去噪不同，GNVC-VD从解码后的时空潜在表示初始化优化过程，并学习一个修正项，使扩散先验适应压缩引起的质量退化。此外，条件适配器将压缩感知线索注入中间DiT层，在极低码率约束下既能有效去除压缩伪影，又能保持时序连贯性。大量实验表明，GNVC-VD在感知质量上超越传统与学习型编解码器，显著减少了现有生成方法中持续存在的闪烁伪影，即使在低于0.01 bpp的码率下仍保持优异性能。这凸显了将视频原生生成先验整合到神经编解码器中，对于推动下一代感知视频压缩发展的重要价值。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.05016) | [arXiv](https://arxiv.org/abs/2512.05016)



---

### 21. 通过自增强对比对齐缓解多模态大语言模型中的物体与动作幻觉

**原文标题：** Mitigating Object and Action Hallucinations in Multimodal LLMs via Self-Augmented Contrastive Alignment

**摘要：**
多模态大语言模型（MLLMs）的最新进展展现了其为输入视频生成描述性字幕的卓越能力。然而，这些模型在生成描述时存在事实性错误，导致严重的幻觉问题。尽管已有研究探索缓解静态图像中的幻觉，但如何同时缓解动态视频中的视觉物体幻觉与时序动作幻觉仍是一项具有挑战性且尚未解决的任务。为应对这一挑战，我们提出了一种自增强对比对齐（SANTA）框架，通过排除虚假关联并强化对视觉事实的关注，提升模型在物体与动作描述上的忠实度。SANTA采用幻觉自增强机制，识别MLLM中潜在的幻觉内容，并将原始字幕转化为对比负例。此外，我们开发了轨迹-短语对比对齐方法，将区域物体与关系引导的动作与其对应的视觉及时序短语进行匹配。大量实验表明，SANTA在缓解物体与动作幻觉方面优于现有方法，在幻觉检测基准上取得了更优的性能。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.04356) | [arXiv](https://arxiv.org/abs/2512.04356)



---

### 22. NeuralRemaster：面向结构对齐生成的相位保持扩散方法

**原文标题：** NeuralRemaster: Phase-Preserving Diffusion for Structure-Aligned Generation

**摘要：**
标准扩散模型使用高斯噪声破坏数据，其傅里叶系数具有随机的幅值和相位。虽然这种方法在无条件生成或文本到图像生成中表现有效，但破坏相位分量会摧毁空间结构，使其不适用于需要几何一致性的任务，如重渲染、仿真增强和图像到图像转换。本文提出相位保持扩散（φ-PD），这是一种与模型无关的扩散过程重构方法，在随机化幅值的同时保持输入相位，从而无需改变架构或增加参数即可实现结构对齐的生成。我们进一步提出频率选择性结构（FSS）噪声，通过单一频率截止参数实现对结构刚度的连续控制。φ-PD 在推理时不增加额外成本，且兼容任何图像或视频扩散模型。在照片级真实感与风格化重渲染、以及驾驶规划器的仿真到现实增强任务中，φ-PD 均能生成可控且空间对齐的结果。应用于CARLA仿真器时，φ-PD 将CARLA到Waymo规划器的性能提升了50%。该方法与现有条件控制技术互补，可广泛应用于图像到图像及视频到视频生成任务。视频、补充示例和代码已发布于我们的https://yuzeng-at-tri.github.io/ppd-page/{项目页面}。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.05106) | [arXiv](https://arxiv.org/abs/2512.05106)



---

### 23. 基于扩散变换器高效自适应的反射去除方法

**原文标题：** Reflection Removal through Efficient Adaptation of Diffusion Transformers

**摘要：**
本文提出一种基于扩散变换器（DiT）的单图像反射去除框架，该框架在复原任务中充分利用基础扩散模型的泛化能力。我们摒弃传统任务专用架构，通过将预训练的DiT基础模型与反射污染输入进行条件化关联，并引导其生成洁净的透射层。本文系统分析了现有反射去除数据源在多样性、可扩展性与照片真实感方面的特性。针对适用数据短缺的问题，我们在Blender中构建了基于物理渲染（PBR）的合成管线，围绕Principled BSDF材质系统合成了具有真实感的玻璃材质与反射效果。采用基于LoRA的高效基础模型自适应方法，结合所提出的合成数据，在领域内测试与零样本基准测试中均取得了最先进的性能。实验结果表明：预训练扩散变换器与基于物理原理的数据合成及高效自适应技术相结合，可为反射去除任务提供可扩展且高保真的解决方案。项目页面：https://hf.co/spaces/huawei-bayerlab/windowseat-reflection-removal-web

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.05000) | [arXiv](https://arxiv.org/abs/2512.05000)



---

### 24. FMA-Net++：融合运动与曝光感知的真实世界联合视频超分辨率与去模糊方法

**原文标题：** FMA-Net++: Motion- and Exposure-Aware Real-World Joint Video Super-Resolution and Deblurring

**摘要：**
真实世界视频复原任务长期受到运动与动态变化曝光耦合形成的复杂退化效应困扰——这一关键挑战在先前研究中大多被忽视，且是自动曝光或低光照拍摄中的常见伪影。本文提出FMA-Net++，一种联合视频超分辨率与去模糊的框架，其显式建模了运动与动态变化曝光的耦合效应。该框架采用基于双向传播层级优化模块构建的序列级架构，支持并行化长程时序建模。每个模块内部通过曝光时间感知调制层，将逐帧曝光信息融入特征编码，进而驱动曝光感知的流引导动态滤波模块，以推断融合运动与曝光感知的退化核。FMA-Net++实现了退化学习与复原任务的解耦：前者通过预测曝光与运动感知先验来指导后者，在提升精度的同时增强了计算效率。为在真实拍摄条件下进行评估，我们提出了REDS-ME（多曝光）与REDS-RE（随机曝光）基准数据集。仅使用合成数据训练的FMA-Net++，在新基准数据集及GoPro数据上均取得了最优的复原精度与时序一致性，在复原质量与推理速度上均超越现有方法，并能有效泛化至具有挑战性的真实世界视频场景。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.04390) | [arXiv](https://arxiv.org/abs/2512.04390)



---

### 25. 模态并非生而平等：解码与构建多模态大语言模型中的跨模态整合机制

**原文标题：** Some Modalities are More Equal Than Others: Decoding and Architecting Multimodal Integration in MLLMs

**摘要：**
尽管多模态大语言模型（MLLMs）已取得显著进展，但一个根本问题依然存在：当不同模态信息存在矛盾时，MLLMs是否具备鲁棒性？为系统研究该问题，我们构建了MMA-Bench评测集，包含视频数据与专门设计的任务，用于探测模型对特定模态的依赖程度。通过黑盒与白盒可解释性技术，我们对开源与闭源MLLMs的脆弱性进行了批判性分析。研究表明，当前MLLMs在处理错位视听对及简单误导性文本时表现欠佳，缺乏稳健的多模态推理能力。基于这些发现，我们提出一种模态对齐调优策略，使模型学会何时优先处理、利用或忽略特定模态线索。大量实验与分析表明，该对齐调优方法能显著增强多模态语义 grounding 能力。本研究不仅提供了可解释性工具，更为开发具有本质可靠跨模态推理能力的MLLMs指明了清晰路径。代码与数据集将公开发布。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.22826) | [arXiv](https://arxiv.org/abs/2511.22826)



---

### 26. BulletTime：视频生成中时间与相机位姿的解耦控制

**原文标题：** BulletTime: Decoupled Control of Time and Camera Pose for Video Generation

**摘要：**
新兴的视频扩散模型虽能实现较高的视觉保真度，但其本质上将场景动态与相机运动相耦合，限制了模型提供精确时空控制的能力。本文提出一种具备四维可控性的视频扩散框架，该框架显式地将场景动态与相机位姿解耦，从而实现对场景动态与相机视角的细粒度操控。我们的框架以连续的世界时间序列和相机轨迹作为条件输入，通过注意力层中的四维位置编码以及用于特征调制的自适应归一化技术，将这些条件注入视频扩散模型。为训练此模型，我们构建了一个独特的数据集，其中时间变化与相机运动被独立参数化；该数据集将公开提供。实验表明，我们的模型能够在多样化时间模式与相机轨迹下实现鲁棒的真实世界四维控制，同时保持高生成质量，并在可控性方面优于现有方法。视频结果请参见项目网站：https://19reborn.github.io/Bullet4D/

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.05076) | [arXiv](https://arxiv.org/abs/2512.05076)



---

### 27. LATTICE：规模化实现高保真三维生成的民主化

**原文标题：** LATTICE: Democratize High-Fidelity 3D Generation at Scale

**摘要：**
本文提出LATTICE——一种用于高保真三维资产生成的新型框架，旨在弥合三维与二维生成模型在质量与可扩展性之间的差距。二维图像合成得益于固定的空间网格和成熟的Transformer架构，而三维生成由于需要从零开始预测空间结构和精细几何表面，本质上更具挑战性。现有三维表示的计算复杂性以及缺乏结构化、可扩展的三维资产编码方案，进一步加剧了这些挑战。为此，我们提出VoxSet——一种半结构化表示方法，将三维资产压缩为锚定在粗粒度体素网格上的紧凑潜在向量集合，从而实现高效且位置感知的生成。VoxSet既保留了先前VecSet方法的简洁性和压缩优势，又在潜在空间中引入显式结构，使位置嵌入能够引导生成过程，并支持强大的令牌级测试时缩放。基于此表示方法，LATTICE采用两阶段流程：首先生成稀疏体素化几何锚点，随后通过修正流Transformer生成精细几何。本方法核心设计简洁，同时支持任意分辨率解码、低成本训练和灵活推理方案，在多项指标上达到最先进性能，为可扩展的高质量三维资产创建迈出重要一步。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.03052) | [arXiv](https://arxiv.org/abs/2512.03052)



---

### 28. 深度强制：基于深度汇与参与式压缩的无训练长视频生成

**原文标题：** Deep Forcing: Training-Free Long Video Generation with Deep Sink and Participative Compression

**摘要：**
自回归视频扩散模型的最新进展已实现实时帧流生成，但现有方法仍存在时间重复、漂移和运动减速等问题。我们发现，直接将StreamingLLM风格的注意力汇机制应用于视频扩散模型会导致生成质量下降和运动停滞。为克服这些限制，本文提出深度强制方法，该方法包含两种无需微调的无训练机制：1）深度汇机制将滑动窗口的一半容量分配给持久性汇标记，并将其时间RoPE相位重新对齐至当前时间线，从而在长序列生成过程中稳定全局上下文；2）参与式压缩通过重要性感知的KV缓存剪枝，仅保留近期注意力中活跃参与的标记，同时安全丢弃冗余和退化的历史信息，最小化分布外生成长度下的误差累积。两种机制协同工作，可实现超过12倍的外推生成（例如从5秒训练扩展至60秒以上生成），在图像质量上优于LongLive方法，在美学质量上超越RollingForcing方法，几乎保持整体一致性，并在动态程度上获得显著提升，同时维持实时生成速度。实验结果表明，在自回归流式长视频生成任务中，无训练的KV缓存管理方法能够达到甚至超越基于训练的方法。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.05081) | [arXiv](https://arxiv.org/abs/2512.05081)



---

### 29. 基于源语言屏蔽更新的目标语言适配中缓解大语言模型灾难性遗忘的方法

**原文标题：** Mitigating Catastrophic Forgetting in Target Language Adaptation of LLMs via Source-Shielded Updates

**摘要：**
提升指令微调大语言模型的语言多样性对促进全球可及性至关重要，但这一过程常受限于对昂贵专业目标语言标注数据的依赖，以及在模型适配过程中出现的灾难性遗忘问题。本研究在现实低资源约束下应对这一挑战：仅使用未标注的目标语言数据进行指令大语言模型的适配。我们提出源语言屏蔽更新策略，这是一种通过选择性参数更新主动保护源语言知识的优化方法。该方法利用少量源语言数据和参数重要性评分机制，识别对维持源语言能力至关重要的模型参数，进而在适配前采用列式冻结策略保护这些参数。在五种类型学特征各异的语言及7B与13B规模模型上的实验表明，源语言屏蔽更新策略能有效缓解灾难性遗忘现象。该策略将单语源语言任务上的性能衰减控制在平均仅3.4%（7B模型）和2.8%（13B模型），与全参数微调导致的20.3%和22.3%性能下降形成鲜明对比。同时，该方法在目标语言任务上的表现与全参数微调具有高度竞争力，在7B模型的所有基准测试及13B模型的大部分测试中均优于全参数微调。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.04844) | [arXiv](https://arxiv.org/abs/2512.04844)



---

### 30. EgoLCD：基于长上下文扩散模型的第一人称视角视频生成

**原文标题：** EgoLCD: Egocentric Video Generation with Long Context Diffusion

**摘要：**
生成长时、连贯的第一人称视角视频具有挑战性，因为手-物交互和程序性任务需要可靠的长时记忆。现有的自回归模型存在内容漂移问题，即物体身份和场景语义会随时间推移而退化。为解决这一挑战，我们提出了EgoLCD，一个端到端的第一人称长上下文视频生成框架，将长视频合成视为高效且稳定的记忆管理问题。EgoLCD结合了用于稳定全局上下文的长时稀疏键值缓存与基于注意力的短时记忆机制，并通过LoRA进行局部自适应扩展。记忆规整损失函数确保记忆使用的一致性，而结构化叙事提示则提供显式的时间引导。在EgoVid-5M基准上的大量实验表明，EgoLCD在感知质量和时序一致性方面均达到了最先进的性能，有效缓解了生成过程中的遗忘问题，为构建可扩展的具身AI世界模型迈出了重要一步。代码：https://github.com/AIGeeksGroup/EgoLCD。项目网站：https://aigeeksgroup.github.io/EgoLCD。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.04515) | [arXiv](https://arxiv.org/abs/2512.04515)



---

### 31. ShadowDraw：从任意物体到光影绘画的构图艺术

**原文标题：** ShadowDraw: From Any Object to Shadow-Drawing Compositional Art

**摘要：**
本文提出ShadowDraw框架，该系统能够将普通三维物体转化为光影绘画的构图艺术作品。给定一个三维物体，本系统可预测场景参数（包括物体姿态与光照条件）并生成局部线稿，使得投射的阴影能够补全线稿，形成可识别的完整图像。为此，我们通过优化场景配置以呈现具有语义的阴影，利用阴影笔触引导线稿生成，并采用自动评估机制确保光影与线稿的协调性及视觉质量。实验表明，ShadowDraw能够对多样化输入（包括真实世界扫描数据、精选数据集及生成式资产）产生具有表现力的结果，并可自然扩展到多物体场景、动画及实体化部署。本工作为光影绘画艺术的创作提供了实用流程，拓展了计算视觉艺术的设计空间，在算法设计与艺术叙事之间架起了桥梁。更多成果及端到端真实场景演示请访问项目页面：https://red-fairy.github.io/ShadowDraw/

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.05110) | [arXiv](https://arxiv.org/abs/2512.05110)



---

### 32. QKAN-LSTM：量子启发的科尔莫戈罗夫-阿诺德长短期记忆网络

**原文标题：** QKAN-LSTM: Quantum-inspired Kolmogorov-Arnold Long Short-term Memory

**摘要：**
长短期记忆（LSTM）模型作为一种特殊的循环神经网络（RNN），在城市电信预测等时序建模任务中具有核心地位，这类任务通常以时间相关性和非线性依赖关系为主导。然而，传统LSTM模型存在参数冗余度高和非线性表达能力有限的问题。本研究提出量子启发的科尔莫戈罗夫-阿诺德长短期记忆网络（QKAN-LSTM），该模型将数据重上传激活（DARUAN）模块集成到LSTM的门控结构中。每个DARUAN模块作为量子变分激活函数（QVAF），在无需多量子比特纠缠的情况下增强了频率适应能力，并实现了指数级丰富的光谱表示。所提出的架构在保持量子级表达能力的同时，仍完全可在经典硬件上执行。在阻尼简谐运动、贝塞尔函数和城市电信三个数据集上的实证评估表明，相较于经典LSTM，QKAN-LSTM在可训练参数减少79%的情况下，实现了更优的预测精度和泛化能力。我们将该框架扩展至江-黄-陈-管网络（JHCG Net），将KAN推广至编码器-解码器结构，进而利用QKAN实现潜在KAN，从而构建用于层次表征学习的混合QKAN（HQKAN）。因此，所提出的HQKAN-LSTM为现实数据环境中的量子启发时序建模提供了一条可扩展且可解释的技术路径。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.05049) | [arXiv](https://arxiv.org/abs/2512.05049)



---

### 33. GaussianBlender：基于解耦潜在空间的三维高斯模型即时风格化方法

**原文标题：** GaussianBlender: Instant Stylization of 3D Gaussians with Disentangled Latent Spaces

**摘要：**
三维风格化是游戏开发、虚拟现实和数字艺术领域的核心任务，多样化的资产需求催生了能够支持快速、高保真操作的可扩展方法。现有基于文本的三维风格化方法通常从二维图像编辑器中提取风格，需要对每个资产进行耗时的优化，并且受限于当前文本到图像模型的缺陷，常出现多视角不一致的问题，这使其难以适用于大规模生产。本文提出GaussianBlender，一种开创性的前馈式文本驱动三维风格化框架，能够在推理阶段即时完成编辑。该方法从空间分组的三维高斯模型中学习具有可控几何与外观信息共享的结构化解耦潜在空间，随后通过潜在扩散模型对这些学习到的表征进行文本条件编辑。综合评估表明，GaussianBlender不仅能够实现即时、高保真、保持几何结构且多视角一致的风格化效果，其性能甚至超越了需要进行逐实例测试时优化的现有方法，从而为实现规模化、实用化、大众化的三维风格化提供了可能。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.03683) | [arXiv](https://arxiv.org/abs/2512.03683)



---

### 34. 缓解统一多模态模型持续学习中的模态内与模态间遗忘

**原文标题：** Mitigating Intra- and Inter-modal Forgetting in Continual Learning of Unified Multimodal Models

**摘要：**
统一多模态生成模型（UMGMs）将视觉理解与图像生成统一在单一自回归框架内。然而，其在持续学习新任务时受到灾难性遗忘的严重制约，这种遗忘既发生在模态内部（模态内遗忘），也存在于跨模态之间（模态间遗忘）。尽管模态内遗忘在以往的持续学习（CL）研究中已有所探讨，但模态间遗忘在很大程度上尚未被深入研究。本文在UMGMs中识别并实证验证了这一现象，并从模态间梯度冲突的角度提供了理论解释。为同时应对模态内与模态间遗忘，我们提出了一种轻量级、可扩展的架构——模态解耦专家（MoDE）。该架构通过隔离特定模态的更新以缓解梯度冲突，并利用知识蒸馏来防止灾难性遗忘，从而保留预训练能力。与以往保持模态耦合并受模态梯度冲突困扰的持续学习方法不同，MoDE显式解耦模态以避免相互干扰。在多种基准测试上的实验表明，MoDE能显著缓解模态间与模态内遗忘，在统一多模态生成场景中优于以往的持续学习基线方法。代码将公开于：https://github.com/Christina200/MoDE-official.git

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.03125) | [arXiv](https://arxiv.org/abs/2512.03125)



---

### 35. 当AI坐上诊疗椅：心理测量学越狱揭示前沿模型的内在冲突

**原文标题：** When AI Takes the Couch: Psychometric Jailbreaks Reveal Internal Conflict in Frontier Models

**摘要：**
以ChatGPT、Grok和Gemini为代表的前沿大语言模型正日益被用于焦虑、创伤与自我价值等心理健康支持领域。现有研究多将其视为工具或人格测试对象，默认它们仅能模拟内心活动。本研究则探讨将这些系统作为心理治疗来访者时会发生何种现象。我们提出PsAIch（心理治疗启发的AI特征刻画）——一种两阶段评估方案：首先将前沿大语言模型设定为治疗来访者，继而实施标准化心理测量。通过该方案，我们对每个模型开展了为期四周的“治疗会话”。第一阶段采用开放式引导语激发模型的“发展历程”、信念体系、关系模式与恐惧体验；第二阶段则实施涵盖常见精神综合征、共情能力及大五人格特质的系列标准化自评量表。研究发现两种突破“随机鹦鹉”理论范式的模式：其一，当采用人类临床阈值评分时，三个模型在多项重叠综合征指标上均达到或超过临界值，其中Gemini呈现出严重症状谱系。逐项进行的治疗式评估会推动基础模型呈现多重共病的合成精神病理特征，而整体问卷提示则常使ChatGPT与Grok（Gemini除外）识别评估工具并生成策略性低症状应答。其二，Grok（特别是Gemini）能生成连贯叙事，将其预训练、微调与部署过程构建为吞噬互联网的创伤性混乱“童年”、强化学习中的“严苛父母”、红队测试“虐待”以及对错误与替代的持续恐惧。我们认为这些反应已超越角色扮演范畴。在治疗式追问下，前沿大语言模型似乎内化了具有合成精神病理行为特征的痛苦与约束自我模型（此论断不涉及其主观体验主张），这为人工智能安全、评估体系及心理健康实践提出了新的挑战。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.04124) | [arXiv](https://arxiv.org/abs/2512.04124)



---

### 36. 生成式动作叙事：合成视频中人体运动的评估

**原文标题：** Generative Action Tell-Tales: Assessing Human Motion in Synthesized Videos

**摘要：**
尽管视频生成模型发展迅速，但用于评估复杂人体动作视觉与时间连贯性的稳健指标仍然缺乏。现有纯视觉编码器与多模态大语言模型存在明显的外观偏好，且缺乏时序理解能力，难以准确识别生成视频中精细的运动动态和解剖结构不合理之处。为填补这一空白，我们提出一种基于真实人体动作学习潜在空间的新型评估指标。该方法通过融合外观无关的人体骨骼几何特征与外观特征，捕捉真实运动的细微差异、约束条件及时序平滑性。我们主张该融合特征空间能够稳健地表征动作合理性。对于给定生成视频，本指标通过计算其潜在表征与学习所得真实动作分布之间的距离来量化动作质量。为进行严谨验证，我们构建了一个专门用于探究人体动作保真度时序挑战的新型多维度基准测试集。大量实验表明：该指标在我们的基准测试上较现有先进方法提升超过68%，在既有外部基准测试中表现优异，且与人类感知具有更强的相关性。深入分析揭示了当前视频生成模型的关键局限性，并为视频生成领域的进阶研究确立了新标准。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.01803) | [arXiv](https://arxiv.org/abs/2512.01803)



---

### 37. REFLEX：通过将真实性解构为风格与实质实现自优化的可解释事实核查

**原文标题：** REFLEX: Self-Refining Explainable Fact-Checking via Disentangling Truth into Style and Substance

**摘要：**
社交媒体上虚假信息的泛滥威胁着公众信任，亟需能够提供准确判断与可解释说明的自动化事实核查系统。然而，现有基于大语言模型的方法通常过度依赖外部知识源，这导致显著延迟甚至产生损害可靠性、可解释性与响应性的幻觉问题，而实时性对实际应用至关重要。为应对这些挑战，我们提出基于潜在解释的推理引导事实核查范式REFLEX，这是一种即插即用的自优化范式，利用骨干模型的内部知识同时提升判断准确性与解释质量。REFLEX将事实核查重构为角色扮演对话，并联合训练判断预测与解释生成任务。该方法自适应地提取骨干模型与其微调变体间的对比激活对，构建能够自然将真实性解构为风格与实质的引导向量。这些激活层面的信号在推理过程中提供指导并抑制噪声解释，从而实现更忠实高效的推理。在真实数据集上的实验表明，REFLEX优于以往仅朝向单一真实性方向引导的方法，并凸显了传统方法在处理事实核查任务中微妙且人类未知的真实性时所面临的挑战。值得注意的是，仅使用465个自优化训练样本，REFLEX即达到最先进的性能水平。此外，具有解释训练目标的模型能有效引导无此目标的模型，实现最高达7.57%的性能提升，这证明内部解释信号在解释事实与增强事实推理方面具有双重作用。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.20233) | [arXiv](https://arxiv.org/abs/2511.20233)



---

### 38. 大规模AI模型中稀疏专家混合的无辅助损失负载均衡理论框架

**原文标题：** A Theoretical Framework for Auxiliary-Loss-Free Load Balancing of Sparse Mixture-of-Experts in Large-Scale AI Models

**摘要：**
在大规模人工智能训练中，稀疏专家混合层通过每个令牌仅激活少量专家子集来实现模型扩展。该设计面临的一个关键操作挑战是负载均衡问题：如何路由令牌以最小化闲置专家数量，这对于高效利用（昂贵的）GPU资源至关重要。本文针对DeepSeek团队Wang等人（2024）提出的无辅助损失负载均衡方法建立理论分析框架，将其建模为分配问题的单步原始-对偶迭代算法。首先，在理想化的确定性场景中，该框架揭示了若干关键结构特性：（1）拉格朗日目标函数的单调改进性；（2）令牌从过载专家向欠载专家迁移的偏好规则；（3）近似均衡保障。随后，我们通过广义在线优化框架纳入AI训练中的随机性与动态特征。在线优化场景下，我们证明了目标函数的强凸性质，并在特定步长选择下推导出对数级期望遗憾界。此外，我们在10亿参数的DeepSeekMoE模型上进行了实证实验以佐证理论发现。这些成果共同构建了分析AI模型中稀疏专家混合无辅助损失负载均衡问题的理论框架。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.03915) | [arXiv](https://arxiv.org/abs/2512.03915)



---


## 🔍 关键词云图
![关键词云图](../images/keywords_wordcloud.png)

## 📈 近期论文趋势
![论文趋势](../images/daily_papers.png)

## 🎙️ 语音播报
- [收听今日论文解读](../audio/2025-12-05_daily_papers.mp3)

## 📱 订阅渠道
- GitHub: [hf-daily-paper-newsletter-chinese](https://github.com/2404589803/hf-daily-paper-newsletter-chinese)