<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Hugging Face 论文日报 - 2025-12-08</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', sans-serif;
            line-height: 1.6;
            color: #333;
            background-color: #f5f5f5;
            padding: 20px;
        }
        
        .container {
            max-width: 900px;
            margin: 0 auto;
            background-color: white;
            padding: 40px;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }
        
        h1 {
            color: #2c3e50;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
            margin-bottom: 30px;
            font-size: 28px;
        }
        
        h1 img {
            vertical-align: middle;
            margin-right: 10px;
        }
        
        h2 {
            color: #34495e;
            margin-top: 40px;
            margin-bottom: 20px;
            font-size: 24px;
            border-left: 4px solid #3498db;
            padding-left: 15px;
        }
        
        h3 {
            color: #2c3e50;
            margin-top: 30px;
            margin-bottom: 15px;
            font-size: 20px;
        }
        
        p {
            margin-bottom: 15px;
            text-align: justify;
        }
        
        ul {
            margin-left: 20px;
            margin-bottom: 20px;
        }
        
        li {
            margin-bottom: 8px;
        }
        
        a {
            color: #3498db;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        strong {
            color: #2c3e50;
            font-weight: 600;
        }
        
        hr {
            border: none;
            border-top: 1px solid #e0e0e0;
            margin: 30px 0;
        }
        
        /* 关键修复:限制图片宽度 */
        img {
            max-width: 100%;
            height: auto;
            display: block;
            margin: 20px auto;
            border-radius: 4px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        
        /* 确保图片容器也有宽度限制 */
        p img {
            max-width: 100%;
        }
        
        /* 论文详情区域样式 */
        .paper-section {
            background-color: #f9f9f9;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 20px;
        }
        
        /* 统计信息样式 */
        .stats {
            background-color: #e8f4f8;
            padding: 15px;
            border-radius: 6px;
            margin-bottom: 20px;
        }
        
        /* 响应式设计 */
        @media (max-width: 768px) {
            .container {
                padding: 20px;
            }
            
            h1 {
                font-size: 24px;
            }
            
            h2 {
                font-size: 20px;
            }
            
            h3 {
                font-size: 18px;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <h1><img src="https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo.png" width="30"/> Hugging Face 2025-12-08 论文日报</h1>
<h2>📊 今日论文统计</h2>
<ul>
<li>总论文数：24</li>
<li>热门领域：RL, LLM, GPT</li>
</ul>
<h2>📝 论文详情</h2>
<h3>1. TwinFlow：基于自对抗流实现大模型一步生成</h3>
<p><strong>原文标题：</strong> TwinFlow: Realizing One-step Generation on Large Models with Self-adversarial Flows</p>
<p><strong>摘要：</strong>
近年来，大规模多模态生成模型在多模态生成（包括图像与视频生成）领域展现出卓越能力。这类模型通常基于扩散模型或流匹配等多步生成框架构建，其固有的迭代特性限制了推理效率（通常需要40-100次函数评估）。尽管已有多种少步生成方法致力于加速推理，但现有方案存在明显局限：基于蒸馏的主流方法（如渐进蒸馏与一致性蒸馏）需要复杂的迭代蒸馏流程，或在极低步数（&lt;4步）下出现显著性能衰退；而将对抗训练融入蒸馏过程的方法（如DMD/DMD2和SANA-Sprint）虽能提升性能，却因引入额外训练模型导致训练不稳定、复杂度增加及显存开销巨大。为此，我们提出TwinFlow——一种简单高效的训练框架，能够构建仅需单步推理的生成模型。该框架无需依赖固定的预训练教师模型，在训练过程中避免了传统对抗网络的使用，特别适合构建大规模高效生成模型。在文生图任务中，本方法在单步推理下取得0.83的GenEval分数，显著优于SANA-Sprint（基于GAN损失的框架）和RCGM（基于一致性的框架）等基线模型。值得注意的是，我们通过对Qwen-Image-20B进行全参数训练，验证了TwinFlow框架的可扩展性，并将其转化为高效的少步生成器。实验表明：在仅使用单步推理时，该方法在GenEval和DPG-Bench基准测试中达到原100步模型的性能水平，在几乎保持生成质量的同时将计算成本降低至百分之一。项目页面详见：https://zhenglin-cheng.com/twinflow。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.05150">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.05150">arXiv</a></p>
<hr />
<h3>2. EditThinker：解锁任意图像编辑器的迭代推理能力</h3>
<p><strong>原文标题：</strong> EditThinker: Unlocking Iterative Reasoning for Any Image Editor</p>
<p><strong>摘要：</strong>
基于指令的图像编辑已成为一个重要的研究领域，得益于图像生成基础模型的发展，该领域已能实现较高的美学质量，这使得指令跟随能力成为当前的主要挑战。现有方法通过监督学习或强化学习来提升指令遵循度，但由于固有的随机性及缺乏深思熟虑的过程，单轮编辑的成功率仍然有限。本研究提出了一种深思式编辑框架，使模型在编辑过程中进行“思考”，通过迭代执行“边编辑边思考”的认知循环来模拟人类认知过程：即批判结果、优化指令，并重复生成直至满意。具体而言，我们训练了一个单一的多模态大语言模型EditThinker，作为该框架的推理引擎，联合生成批判评分、推理过程及优化后的指令。我们采用强化学习方法，将EditThinker的思考过程与其编辑行为对齐，从而产生更具针对性的指令改进。在四个基准测试上的大量实验表明，我们的方法能够显著提升任意图像编辑模型的指令跟随能力，且提升幅度显著。我们将公开数据构建框架、数据集及模型，以促进相关领域的发展。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.05965">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.05965">arXiv</a></p>
<hr />
<h3>3. 从模仿到判别：面向跨域推理任务增强的广义课程优势机制</h3>
<p><strong>原文标题：</strong> From Imitation to Discrimination: Toward A Generalized Curriculum Advantage Mechanism Enhancing Cross-Domain Reasoning Tasks</p>
<p><strong>摘要：</strong>
强化学习已成为大语言模型后训练的一种范式，显著提升了其推理能力。此类方法为每个样本计算优势值，反映其表现优于或劣于预期水平，从而为训练提供正向与负向双重信号。然而，现有方法中两种信号的无差别混合（尤其在早期阶段）可能导致指导意义模糊且性能提升有限。为解决这一问题，我们提出<strong>CAPO</strong>（<strong>C</strong>urriculum <strong>A</strong>dvantage <strong>P</strong>olicy <strong>O</strong>ptimization，课程优势策略优化）——一种基于优势信号的自适应课程机制。该机制首先利用纯正向优势样本引导模仿学习以建立坚实基础，随后逐步引入负向信号以培养判别能力，从而提升模型在复杂场景中的泛化性能。本方法兼容GRPO、PPO、RLOO、Reinforce++等多种优化算法，在数学推理任务中持续取得稳定且显著的性能提升，并能有效泛化至多模态图形用户界面（GUI）推理场景，展现出其作为通用且鲁棒的优化框架的潜力。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.02580">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.02580">arXiv</a></p>
<hr />
<h3>4. EMMA：一种高效的多模态理解、生成与编辑统一架构</h3>
<p><strong>原文标题：</strong> EMMA: Efficient Multimodal Understanding, Generation, and Editing with a Unified Architecture</p>
<p><strong>摘要：</strong>
本文提出EMMA，一种高效统一的多模态理解、生成与编辑架构。该架构主要包括：1）具有32倍压缩率的高效自编码器，显著减少了生成任务所需的标记数量。通过对图像采用相同压缩率，该设计同时保证了理解与生成任务间的训练平衡；2）在视觉理解与生成标记之间采用通道级拼接而非标记级拼接，进一步减少了统一架构中的视觉标记数量；3）共享解耦网络在满足任务特定建模需求的同时，实现了跨任务的协同优化；4）视觉理解编码器采用专家混合机制，在仅少量增加参数的条件下显著提升了感知能力。大量实验表明，EMMA-4B在效率与性能上均显著优于当前最先进的多模态统一方法（如BAGEL-7B），同时相较于近期专业多模态理解与生成模型（如Qwen3-VL与Qwen-Image）也展现出竞争优势。我们相信EMMA为未来统一多模态架构的发展奠定了坚实基础。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.04810">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.04810">arXiv</a></p>
<hr />
<h3>5. PaCo-RL：基于成对奖励建模推进强化学习在一致性图像生成中的应用</h3>
<p><strong>原文标题：</strong> PaCo-RL: Advancing Reinforcement Learning for Consistent Image Generation with Pairwise Reward Modeling</p>
<p><strong>摘要：</strong>
一致性图像生成要求在多个图像中忠实保持身份、风格和逻辑连贯性，这对于故事叙述和角色设计等应用至关重要。由于缺乏捕捉视觉一致性的大规模数据集以及建模人类感知偏好的复杂性，监督训练方法在此任务上面临挑战。本文认为，强化学习通过使模型能够以无需数据的方式学习复杂且主观的视觉标准，提供了一种有前景的替代方案。为实现这一目标，我们提出了PaCo-RL，这是一个将专用一致性奖励模型与高效强化学习算法相结合的综合框架。其第一个组成部分PaCo-Reward是一个基于自动化子图配对构建的大规模数据集训练的成对一致性评估器，它通过生成式自回归评分机制（辅以任务感知指令和思维链推理）来评估一致性。第二个组成部分PaCo-GRPO采用了一种新颖的解耦分辨率优化策略，显著降低了强化学习成本，同时结合了对数平滑的多奖励聚合机制，确保奖励优化的平衡与稳定。在两个代表性子任务上的大量实验表明，PaCo-Reward显著提升了与人类视觉一致性感知的对齐程度，而PaCo-GRPO在提高训练效率和稳定性的同时，实现了最先进的一致性生成性能。这些结果共同凸显了PaCo-RL作为一种实用且可扩展的一致性图像生成解决方案的潜力。项目页面详见：https://x-gengroup.github.io/HomePage_PaCo-RL/。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.04784">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.04784">arXiv</a></p>
<hr />
<h3>6. SCAIL：通过三维一致姿态表征的上下文学习实现影视级角色动画</h3>
<p><strong>原文标题：</strong> SCAIL: Towards Studio-Grade Character Animation via In-Context Learning of 3D-Consistent Pose Representations</p>
<p><strong>摘要：</strong>
尽管近期取得进展，实现符合影视级制作标准的角色动画仍具挑战。现有方法可将驱动视频中的动作迁移至参考图像，但在涉及复杂运动和跨身份动画的开放场景中，常难以保持结构保真度与时间一致性。本研究提出SCAIL（基于上下文学习的影视级角色动画框架），该框架通过两项关键创新应对这些挑战：首先，我们提出一种新颖的三维姿态表征方法，提供更鲁棒灵活的运动信号；其次，我们在扩散-变换器架构中引入全上下文姿态注入机制，实现对完整运动序列的有效时空推理。为满足影视级标准，我们构建了兼顾多样性与质量的精选数据流程，并建立系统性评估的综合基准。实验表明，SCAIL实现了最先进的性能表现，将角色动画向影视级可靠性与真实感推进。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.05905">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.05905">arXiv</a></p>
<hr />
<h3>7. 熵比裁剪作为一种软全局约束用于稳定强化学习</h3>
<p><strong>原文标题：</strong> Entropy Ratio Clipping as a Soft Global Constraint for Stable Reinforcement Learning</p>
<p><strong>摘要：</strong>
大语言模型的后训练依赖于强化学习来提升模型能力与对齐质量。然而，离策略训练范式会引入分布偏移，这往往使策略超出可信区域，导致训练不稳定，具体表现为策略熵值的波动与梯度不稳定。尽管PPO-Clip通过重要性采样裁剪缓解了这一问题，但其仍忽略了动作的全局分布偏移。为应对这些挑战，我们提出使用当前策略与先前策略之间的熵比作为新的全局度量指标，该指标能有效量化策略在更新过程中探索性的相对变化。基于此度量，我们引入了熵比裁剪机制，对熵比施加双向约束。这能在全局分布层面稳定策略更新，并弥补PPO-clip无法调节未采样动作概率偏移的不足。我们将ERC机制集成至DAPO与GPPO两种强化学习算法中。在多个基准测试上的实验表明，ERC能够持续提升算法性能。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.05591">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.05591">arXiv</a></p>
<hr />
<h3>8. 基于单张图像的4D合成：联合三维几何重建与运动生成</h3>
<p><strong>原文标题：</strong> Joint 3D Geometry Reconstruction and Motion Generation for 4D Synthesis from a Single Image</p>
<p><strong>摘要：</strong>
从单张静态图像生成具有交互性与动态性的4D场景仍是核心挑战。现有"先生成后重建"与"先重建后生成"方法大多将几何与运动解耦，导致时空不一致性与泛化能力不足。为解决这些问题，我们扩展了"先重建后生成"框架，提出面向4D合成的运动生成与几何重建联合方法（MoRe4D）。首先构建包含6万条密集点轨迹视频样本的大规模数据集TrajScene-60K，以缓解高质量4D场景数据稀缺问题。在此基础上，提出基于扩散模型的4D场景轨迹生成器（4D-STraG），能够联合生成几何一致且运动合理的4D点轨迹。为利用单视角先验信息，设计了深度引导的运动归一化策略与运动感知模块，实现几何与动态特征的有效融合。进一步提出4D视角合成模块（4D-ViSM），可从4D点轨迹表征渲染任意相机轨迹的视频。实验表明，MoRe4D能够基于单张图像生成具有多视角一致性与丰富动态细节的高质量4D场景。代码地址：https://github.com/Zhangyr2022/MoRe4D。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.05044">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.05044">arXiv</a></p>
<hr />
<h3>9. COOPER：空间智能中协同感知与推理的统一模型</h3>
<p><strong>原文标题：</strong> COOPER: A Unified Model for Cooperative Perception and Reasoning in Spatial Intelligence</p>
<p><strong>摘要：</strong>
视觉空间推理对于使多模态大语言模型（MLLMs）理解物体属性与空间关系至关重要，然而现有模型在三维感知推理方面仍面临挑战。当前方法通常通过两种孤立路径进行增强：一是在感知层面，通过为RGB输入添加深度与分割等辅助模态；二是在推理层面，通过空间视觉问答数据集训练并结合强化学习。本研究探讨了统一MLLM是否能够通过自适应交错推理机制，发展出增强空间感知的内在能力，从而实现更强大的空间智能。我们提出COOPER模型，该统一MLLM以深度与分割作为辅助模态，通过两阶段训练获得辅助模态生成能力与自适应交错推理能力。实验表明，COOPER在保持通用性能的同时，空间推理任务平均提升6.91%。值得注意的是，仅接受辅助模态生成训练的变体模型在距离与尺寸估计任务上亦获得7.92%的性能增益，这表明学习生成辅助模态有助于模型内化空间知识并强化空间理解能力。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.04563">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.04563">arXiv</a></p>
<hr />
<h3>10. RealGen：基于检测器引导奖励的逼真文本到图像生成</h3>
<p><strong>原文标题：</strong> RealGen: Photorealistic Text-to-Image Generation via Detector-Guided Rewards</p>
<p><strong>摘要：</strong>
随着图像生成技术的持续发展，GPT-Image-1与Qwen-Image等先进模型已在文本-图像一致性与世界知识理解方面取得显著成果，但在生成逼真图像方面仍存在不足。即使在简单的文本到图像生成任务中，这些模型也倾向于生成带有明显人工智能痕迹的“虚假”图像，常表现为“皮肤过度光滑”与“面部油亮反光”等特征。为重新实现“以假乱真”的生成目标，本文提出RealGen——一种逼真文本到图像生成框架。该框架整合了用于提示词优化的大语言模型组件与用于生成逼真图像的扩散模型。受对抗生成思想启发，RealGen创新性地引入“检测器奖励”机制，通过语义级与特征级合成图像检测器量化人工痕迹并评估图像真实感。我们结合GRPO算法利用该奖励信号优化整个生成流程，显著提升了图像的真实感与细节表现。此外，本文提出自动化评估基准RealBench，采用检测器评分与竞技场评分机制，实现了无需人工参与的逼真度评估，其评估结果更精准且符合真实用户体验。实验表明，在真实感、细节呈现与美学质量方面，RealGen显著优于GPT-Image-1、Qwen-Image等通用模型以及FLUX-Krea等专业逼真生成模型。代码已开源：https://github.com/yejy53/RealGen。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.00473">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.00473">arXiv</a></p>
<hr />
<h3>11. 无需人工标注的自改进视觉语言模型评判器</h3>
<p><strong>原文标题：</strong> Self-Improving VLM Judges Without Human Annotations</p>
<p><strong>摘要：</strong>
高效的视觉语言模型评判器对模型发展至关重要。当前训练VLM评判器的方法主要依赖大规模人工偏好标注，但这种方法成本高昂，且随着模型快速迭代，标注数据极易过时。本研究提出一种无需人工偏好标注、仅使用自合成数据的VLM评判器自训练框架。该方法采用迭代式三阶段流程：（1）生成具有不同质量层次的多模态指令-响应对；（2）为每对数据生成推理轨迹与评判结果，并筛除不符合预期质量水平的数据；（3）基于正确的评判答案及其推理轨迹进行训练。我们在多模态奖励基准和VL奖励基准上对所得评判器进行跨领域评估，涵盖正确性、偏好性、推理能力、安全性和视觉问答等维度。实验表明，该方法将Llama-3.2-11B多模态评判器在VL奖励基准上的整体准确率从0.38提升至0.51，在通用性、幻觉识别和推理维度表现尤为突出，其性能常优于包括Llama-3.2-90B、GPT-4o和Claude 3.5 Sonnet在内的更大规模模型。这些无需人工标注的成果整体上展现了未来评判器与快速进化的VLM能力同步演进的可能性。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.05145">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.05145">arXiv</a></p>
<hr />
<h3>12. 具备自知之明能力的世界模型：基于校准不确定性的可控视频生成</h3>
<p><strong>原文标题：</strong> World Models That Know When They Don't Know: Controllable Video Generation with Calibrated Uncertainty</p>
<p><strong>摘要：</strong>
生成式视频模型的最新进展已推动高保真视频合成领域取得重大突破，特别是在可控视频生成方面——生成的视频能够以文本和动作输入为条件，例如在指令引导的视频编辑和机器人世界建模中。尽管具备这些卓越能力，可控视频模型仍常产生幻觉现象，即生成与物理现实不符的未来视频帧，这在机器人策略评估与规划等任务中引发严重关切。然而，现有最先进的视频模型缺乏评估和表达自身置信度的能力，阻碍了幻觉缓解的实现。为系统应对这一挑战，我们提出C3方法：一种用于训练连续尺度校准可控视频模型的不确定性量化方法，可在子块级别实现密集置信度估计，精准定位每帧生成视频中的不确定性区域。我们的不确定性量化方法通过三项核心创新赋能视频模型进行不确定性估计：首先，该方法构建了基于严格恰当评分规则的训练框架，使视频模型同时学习正确性与校准性；其次，我们在潜在空间估计视频模型的不确定性，避免了像素空间方法存在的训练不稳定性和过高计算成本；第三，我们将密集的潜在空间不确定性映射至RGB空间的可解释像素级不确定性，通过高分辨率不确定性热力图直观标识不可信区域，实现可视化分析。基于大规模机器人学习数据集（Bridge与DROID）的广泛实验及实际场景评估表明，该方法不仅能在训练分布内提供校准的不确定性估计，还能实现有效的分布外检测。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.05927">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.05927">arXiv</a></p>
<hr />
<h3>13. SpaceControl：在三维生成建模中引入测试时空间控制</h3>
<p><strong>原文标题：</strong> SpaceControl: Introducing Test-Time Spatial Control to 3D Generative Modeling</p>
<p><strong>摘要：</strong>
三维资产的生成方法近年来取得了显著进展，然而如何对物体几何形态提供直观且精确的控制仍是关键挑战。现有方法主要依赖文本或图像提示，但这些方式在几何特异性上往往存在不足：语言描述可能具有模糊性，而图像编辑则较为繁琐。本研究提出SpaceControl，一种无需训练的测试时方法，用于实现三维生成的显式空间控制。该方法能够接受从粗略几何基元到精细网格的多种几何输入，并可无缝集成于现代预训练生成模型，无需任何额外训练。通过可控参数，用户可在几何保真度与输出真实感之间进行权衡。大量定量评估与用户研究表明，SpaceControl在保持高视觉质量的同时，其几何忠实度优于基于训练和基于优化的基线方法。最后，我们开发了交互式用户界面，支持在线编辑超二次曲面并直接转换为带纹理的三维资产，为创意工作流程的实际应用提供便利。项目页面详见：https://spacecontrol3d.github.io/</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.05343">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.05343">arXiv</a></p>
<hr />
<h3>14. ReVSeg：利用强化学习激励视频分割中的推理链</h3>
<p><strong>原文标题：</strong> ReVSeg: Incentivizing the Reasoning Chain for Video Segmentation with Reinforcement Learning</p>
<p><strong>摘要：</strong>
以推理为中心的视频对象分割本质上是一项复杂任务：查询通常涉及动态变化、因果关系和时间交互，而非静态外观。然而现有解决方案通常将这些因素简化为潜在嵌入的推理过程，使得推理链变得不透明且本质上难以处理。因此，我们采用显式分解视角，提出ReVSeg模型，该模型在预训练视觉语言模型的原生接口中通过序列化决策执行推理。ReVSeg并非将所有推理折叠为单步预测，而是执行三个显式操作——语义解析、时序证据选择和空间定位——以此对齐预训练模型的固有能力。我们进一步采用强化学习优化多步推理链，使模型能够根据结果驱动的信号自我优化决策质量。实验结果表明，ReVSeg在标准视频对象分割基准测试中达到最先进性能，并生成可解释的推理轨迹。项目页面详见https://clementine24.github.io/ReVSeg/。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.02835">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.02835">arXiv</a></p>
<hr />
<h3>15. 人工智能与人类协同进化以实现更安全的协同超级智能</h3>
<p><strong>原文标题：</strong> AI &amp; Human Co-Improvement for Safer Co-Superintelligence</p>
<p><strong>摘要：</strong>
自我改进是当前人工智能领域备受关注的目标，但其过程充满风险，且可能需要较长时间才能完全实现。我们认为对人类而言，更可实现且更优的目标是实现协同改进的最大化：即人类研究者与人工智能通过协作达成协同超级智能。具体而言，应着力提升人工智能系统与人类研究者协同开展人工智能研究的能力——从构思到实验的全过程——从而既加速人工智能研究进展，又通过人机共生机制普遍赋予人工智能与人类更安全的超级智能。将人类研究能力的提升纳入这一循环体系，将使我们以更快、更安全的方式实现该目标。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.05356">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.05356">arXiv</a></p>
<hr />
<h3>16. M3DR：迈向通用多语言多模态文档检索</h3>
<p><strong>原文标题：</strong> M3DR: Towards Universal Multilingual Multimodal Document Retrieval</p>
<p><strong>摘要：</strong>
多模态文档检索系统在视觉与文本内容的语义对齐搜索方面已取得显著进展。然而，现有方法大多仍以英语为中心，限制了其在多语言环境中的有效性。本研究提出M3DR（多语言多模态文档检索）框架，旨在跨越语言鸿沟，使其能够适应不同语言文化背景。M3DR通过合成多语言文档数据，兼容多种视觉-语言架构与模型规模，实现了鲁棒的跨语言与跨模态对齐。基于对比训练方法，我们的模型学习了文本与文档图像的统一表征，并能有效跨语言迁移。我们在22种类型各异的语言上验证了该能力，证明其在语言和文字变体间具有稳定的性能与适应性。此外，我们构建了一个涵盖真实多语言场景的综合基准，在单语、多语及混合语言设置下评估模型性能。M3DR兼容单稠密向量与ColBERT风格的词元级多向量两种检索范式。我们提出的NetraEmbed与ColNetraEmbed模型实现了最先进的性能，在跨语言检索任务上相对性能提升约150%。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.03514">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.03514">arXiv</a></p>
<hr />
<h3>17. 主动视频感知：面向智能体长视频理解的迭代式证据搜寻</h3>
<p><strong>原文标题：</strong> Active Video Perception: Iterative Evidence Seeking for Agentic Long Video Understanding</p>
<p><strong>摘要：</strong>
长视频理解（LVU）面临巨大挑战，因为回答现实世界中的查询通常依赖于隐藏在数小时冗余且无关内容中的稀疏、时间分散的线索。尽管智能体流程提升了视频推理能力，但主流框架依赖于与查询无关的视频描述器来感知视频信息，这既浪费了计算资源处理无关内容，也模糊了细粒度的时间与空间信息。受主动感知理论启发，我们认为LVU智能体应主动决定观察的内容、时机与位置，并持续评估当前观察是否足以回答查询。本文提出主动视频感知（AVP），这是一种证据搜寻框架，它将视频视为交互式环境，直接从像素中获取紧凑且与查询相关的证据。具体而言，AVP通过多模态大语言模型（MLLM）智能体运行一个“规划-观察-反思”的迭代过程：在每一轮中，规划器提出有针对性的视频交互指令，观察器执行指令以提取带时间戳的证据，反思器则评估证据对回答查询的充分性，从而决定是终止流程并给出答案，还是触发进一步观察。在五个LVU基准测试中，AVP均取得了最高性能，且提升显著。值得注意的是，AVP在平均准确率上以仅需18.4%的推理时间和12.4%的输入令牌量，超越了现有最佳智能体方法5.7%。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.05774">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.05774">arXiv</a></p>
<hr />
<h3>18. 从片段到场景：基于视觉语言模型的自动驾驶时序理解研究</h3>
<p><strong>原文标题：</strong> From Segments to Scenes: Temporal Understanding in Autonomous Driving via Vision-Language Model</p>
<p><strong>摘要：</strong>
自动驾驶领域的时序理解仍是一项重大挑战，即使对当前最先进的视觉语言模型而言亦是如此。先前研究虽已引入旨在提升时序推理能力的数据集与基准测试，但其重点关注体育、烹饪、电影等其他视频内容，尚无专门针对以自我为中心视角的自动驾驶视频时序理解特有问题构建的基准体系。为填补这一空白，本研究提出自动驾驶时序理解基准测试，用于评估视觉语言模型捕捉自动驾驶场景中动作间动态关系的能力。该基准包含近6000组问答对，涵盖7项人工设计的任务。此外，我们对9个开源与闭源的通用模型以及最先进的自动驾驶专用模型进行了系统性评估。实验表明，当前最先进模型在该基准测试中表现欠佳，主要归因于其细粒度运动理解能力存在缺陷。为提升运动理解能力及整体测试准确率，我们提出两种无需训练的创新解决方案：基于思维链的场景推理方法，以及融合自我中心时序认知地图的时序认知映射方法。将所提方法与现有视觉语言模型结合后，在自动驾驶时序理解基准上的平均准确率最高提升17.72%。通过构建该基准体系、评估多类先进模型并提出有效增强方法，本研究旨在推动自动驾驶时序理解领域的后续研究。基准数据集与评估代码已分别发布于https://huggingface.co/datasets/vbdai/TAD与https://github.com/vbdi/tad_bench。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.05277">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.05277">arXiv</a></p>
<hr />
<h3>19. ProPhy：面向动态世界模拟的渐进式物理对齐框架</h3>
<p><strong>原文标题：</strong> ProPhy: Progressive Physical Alignment for Dynamic World Simulation</p>
<p><strong>摘要：</strong>
视频生成领域的最新进展在构建世界模拟器方面展现出显著潜力。然而，当前模型在生成物理一致性结果方面仍存在困难，尤其在处理大规模或复杂动态场景时更为明显。这一局限主要源于现有方法对物理提示的响应呈现各向同性特征，且忽视了生成内容与局部物理线索之间的细粒度对齐。为解决这些挑战，我们提出ProPhy——一种渐进式物理对齐框架，能够实现显式的物理感知条件化与各向异性生成。ProPhy采用两阶段物理专家混合机制进行判别式物理先验提取：语义专家从文本描述中推断语义级物理原理，而细化专家则捕获标记级的物理动态特征。该机制使模型能够学习细粒度的物理感知视频表征，从而更准确地反映底层物理规律。此外，我们提出一种物理对齐策略，将视觉语言模型的物理推理能力迁移至细化专家模块，以提升动态物理现象的表征精度。在物理感知视频生成基准测试上的大量实验表明，ProPhy相较于现有先进方法能够生成更具真实感、动态性和物理一致性的结果。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.05564">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.05564">arXiv</a></p>
<hr />
<h3>20. SQ格式：面向大语言模型的统一稀疏量化硬件友好数据格式</h3>
<p><strong>原文标题：</strong> SQ-format: A Unified Sparse-Quantized Hardware-friendly Data Format for LLMs</p>
<p><strong>摘要：</strong>
训练后量化（PTQ）在大语言模型（LLMs）的普及中起着至关重要的作用。然而，由于硬件支持有限，现有的低位量化和稀疏化技术难以平衡精度与效率。例如，W4A8配置仅能实现与W8A8相同的峰值TOPS，而GPU支持的稀疏数据格式（2:4半结构化稀疏）因精度损失问题鲜少被采用。为弥补这一差距，本文提出稀疏量化格式（SQ格式），这是一种统一的量化和稀疏化数据格式，有望被新型硬件及现有GPU轻松支持。SQ格式利用稀疏矩阵可在高精度下加速、而低精度矩阵乘法亦可相应加速的特性，旨在实现性能与吞吐量之间的帕累托改进。该格式尤其适用于具有离群值非均衡分布的激活数据，并使其静态压缩成为可能。我们展示了SQ格式在训练后量化方面的先进性能，提出了支持该格式所需的硬件方案，并进一步为下一代AI加速器提供了设计探索与洞见。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.05409">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.05409">arXiv</a></p>
<hr />
<h3>21. TimesNet-Gen：基于深度学习的场地特定强震动生成方法</h3>
<p><strong>原文标题：</strong> TimesNet-Gen: Deep Learning-based Site Specific Strong Motion Generation</p>
<p><strong>摘要：</strong>
有效减轻地震风险依赖于准确的场地特定评估，这需要能够表征局部场地条件对地震动特征影响的模型。在此背景下，从记录的地震动中学习场地控制特征的数据驱动方法提供了一个有前景的研究方向。本文针对时域加速度计记录进行强震动生成，提出了TimesNet-Gen——一种时域条件生成器。该方法采用站点特定的潜在瓶颈结构进行建模。我们通过比较各台站真实记录与生成记录的HVSR曲线及场地基本频率f_0分布来评估生成效果，并基于f_0分布的混淆矩阵计算评分以综合量化台站特异性。TimesNet-Gen在台站级别的特征对齐方面表现优异，在场地特定强震动合成任务中优于基于频谱图的条件变分自编码器基线模型。相关代码已发布于https://github.com/brsylmz23/TimesNet-Gen。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.04694">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.04694">arXiv</a></p>
<hr />
<h3>22. Colon-X：从多模态理解迈向临床推理的智能结肠镜技术新进展</h3>
<p><strong>原文标题：</strong> Colon-X: Advancing Intelligent Colonoscopy from Multimodal Understanding to Clinical Reasoning</p>
<p><strong>摘要：</strong>
本研究提出Colon-X开放计划，旨在推动结肠镜多模态智能技术的发展。我们首先构建了ColonVQA数据集——目前结肠镜领域最全面的多模态数据集，涵盖76种临床发现和18类多模态任务，包含超过110万条视觉问答条目。该数据集不仅为学界提供了基础数据资源，我们更进一步探究了结肠镜技术中一个关键但尚未充分探索的转型方向——从多模态理解向临床推理的演进：（a）为评估当前多模态理解能力的发展现状，我们系统测试了22个多模态大语言模型的泛化性能，并考察其在人为干扰条件下的可靠性。结果表明，当前主流MLLM模型的临床输出结果在鲁棒性与可信度方面仍有显著不足。（b）为弥合这一差距，我们深入探索了面向结肠镜的推理导向智能技术。具体而言，我们构建了基于临床实践的推理数据集ColonReason（采用多专家辩论流程进行标注），并开发了首个体现实时推理能力的R1架构模型ColonR1。该模型融合任务自适应奖励机制与梯度稳定优化技术，在数据稀缺条件下实现了56.61%的综合准确率，较监督微调方法提升25.22%，为多模态结肠镜分析建立了全新的推理性能基准。所有数据与模型资源已通过https://github.com/ai4colonoscopy/Colon-X开源发布。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.03667">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.03667">arXiv</a></p>
<hr />
<h3>23. 从浮点运算到资源足迹：人工智能的资源成本</h3>
<p><strong>原文标题：</strong> From FLOPs to Footprints: The Resource Cost of Artificial Intelligence</p>
<p><strong>摘要：</strong>
随着计算需求持续攀升，评估人工智能的环境足迹需要超越能耗与用水量，涵盖专用硬件的材料需求。本研究通过将计算工作量与物理硬件需求相关联，量化了人工智能训练的材料足迹。采用电感耦合等离子体发射光谱法对英伟达A100 SXM 40 GB图形处理器（GPU）的元素组成进行分析，共识别出32种元素。结果表明，人工智能硬件约90%由重金属构成，贵金属含量仅为痕量。以质量计，铜、铁、锡、硅和镍是GPU的主要组成元素。通过多步骤研究方法，我们将这些测量数据与不同使用周期内单GPU的计算吞吐量相结合，并纳入不同训练效率模式下特定人工智能模型训练的计算需求。基于情景的分析显示：根据模型浮点运算利用率（MFU）和硬件使用年限，训练GPT-4需要1,174至8,800块A100 GPU，对应最多7吨有毒元素的开采与最终处置。软件与硬件的协同优化策略可降低材料需求：将MFU从20%提升至60%可使GPU需求减少67%，而将使用年限从1年延长至3年可实现相近的节约效果；同时实施这两项措施最高可降低93%的GPU需求。我们的研究结果表明，诸如GPT-3.5到GPT-4之间的渐进性能提升，伴随着不成比例的高昂材料成本。本研究强调必须将材料资源考量纳入人工智能可扩展性的讨论，并指出未来人工智能的发展必须符合资源效率与环境责任原则。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.04142">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.04142">arXiv</a></p>
<hr />
<h3>24. 具有鲁棒护栏的大型语言模型分类自适应审核模型</h3>
<p><strong>原文标题：</strong> Taxonomy-Adaptive Moderation Model with Robust Guardrails for Large Language Models</p>
<p><strong>摘要：</strong>
大型语言模型通常在训练后阶段进行安全性对齐；然而，其仍可能生成不当输出，对用户构成潜在风险。这一挑战凸显了在模型输入与输出两端均需部署鲁棒安全机制的必要性。本研究提出Roblox Guard 1.0——一种基于指令微调的前沿大型语言模型，通过构建多模型串联流水线增强审核能力，实现覆盖输入输出的全方位安全管控。该模型以Llama-3.1-8B-Instruct为基座，经指令微调后能够泛化至未见过的安全分类体系，并在跨领域安全基准测试中表现出卓越性能。指令微调过程融合合成与开源安全数据集，辅以思维链推理与输入反转技术，以增强上下文理解与决策能力。为支持系统性评估，我们同步发布RobloxGuard-Eval基准测试集，该数据集配备可扩展的安全分类体系，专门用于评估大型语言模型护栏机制与审核框架的有效性。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.05339">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.05339">arXiv</a></p>
<hr />
<h2>🔍 关键词云图</h2>
<p><img alt="关键词云图" src="../images/keywords_wordcloud.png" /></p>
<h2>📈 近期论文趋势</h2>
<p><img alt="论文趋势" src="../images/daily_papers.png" /></p>
<h2>🎙️ 语音播报</h2>
<ul>
<li><a href="../audio/2025-12-08_daily_papers.mp3">收听今日论文解读</a></li>
</ul>
<h2>📱 订阅渠道</h2>
<ul>
<li>GitHub: <a href="https://github.com/2404589803/hf-daily-paper-newsletter-chinese">hf-daily-paper-newsletter-chinese</a></li>
</ul>
    </div>
</body>
</html>