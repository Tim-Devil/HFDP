<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Hugging Face 论文日报 - 2025-11-07</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', sans-serif;
            line-height: 1.6;
            color: #333;
            background-color: #f5f5f5;
            padding: 20px;
        }
        
        .container {
            max-width: 900px;
            margin: 0 auto;
            background-color: white;
            padding: 40px;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }
        
        h1 {
            color: #2c3e50;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
            margin-bottom: 30px;
            font-size: 28px;
        }
        
        h1 img {
            vertical-align: middle;
            margin-right: 10px;
        }
        
        h2 {
            color: #34495e;
            margin-top: 40px;
            margin-bottom: 20px;
            font-size: 24px;
            border-left: 4px solid #3498db;
            padding-left: 15px;
        }
        
        h3 {
            color: #2c3e50;
            margin-top: 30px;
            margin-bottom: 15px;
            font-size: 20px;
        }
        
        p {
            margin-bottom: 15px;
            text-align: justify;
        }
        
        ul {
            margin-left: 20px;
            margin-bottom: 20px;
        }
        
        li {
            margin-bottom: 8px;
        }
        
        a {
            color: #3498db;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        strong {
            color: #2c3e50;
            font-weight: 600;
        }
        
        hr {
            border: none;
            border-top: 1px solid #e0e0e0;
            margin: 30px 0;
        }
        
        /* 关键修复:限制图片宽度 */
        img {
            max-width: 100%;
            height: auto;
            display: block;
            margin: 20px auto;
            border-radius: 4px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        
        /* 确保图片容器也有宽度限制 */
        p img {
            max-width: 100%;
        }
        
        /* 论文详情区域样式 */
        .paper-section {
            background-color: #f9f9f9;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 20px;
        }
        
        /* 统计信息样式 */
        .stats {
            background-color: #e8f4f8;
            padding: 15px;
            border-radius: 6px;
            margin-bottom: 20px;
        }
        
        /* 响应式设计 */
        @media (max-width: 768px) {
            .container {
                padding: 20px;
            }
            
            h1 {
                font-size: 24px;
            }
            
            h2 {
                font-size: 20px;
            }
            
            h3 {
                font-size: 18px;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <h1><img src="https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo.png" width="30"/> Hugging Face 2025-11-07 论文日报</h1>
<h2>📊 今日论文统计</h2>
<ul>
<li>总论文数：15</li>
<li>热门领域：LLM, Transformer, RL, GPT</li>
</ul>
<h2>📝 论文详情</h2>
<h3>1. 以视频思考：视频生成作为一种前景广阔的多模态推理范式</h3>
<p><strong>原文标题：</strong> Thinking with Video: Video Generation as a Promising Multimodal
  Reasoning Paradigm</p>
<p><strong>摘要：</strong>
“以文本思考”和“以图像思考”范式显著提升了大型语言模型（LLMs）与视觉语言模型（VLMs）的推理能力。然而这些范式存在固有局限：（1）图像仅能捕捉瞬时状态而无法表征动态过程或连续变化；（2）文本与视觉作为独立模态的割裂阻碍了统一的多模态理解与生成。为突破这些限制，我们提出“以视频思考”新范式，通过视频生成模型（如Sora-2）在统一时序框架中桥接视觉与文本推理。为支持该研究，我们开发了视频思维基准（VideoThinkBench），涵盖两大任务类别：（1）视觉中心任务（如目测谜题）；（2）文本中心任务（如GSM8K、MMMU子集）。评估结果表明Sora-2具备卓越推理能力：在视觉中心任务中整体媲美最先进VLMs，且在目测游戏等任务中表现更优；在文本中心任务中，MATH准确率达92%，MMMU准确率达75.53%。我们系统分析了能力来源，发现自洽性与上下文学习能提升Sora-2性能。本研究证实视频生成模型有望成为统一的多模态理解与生成载体，使“以视频思考”成为统一的多模态推理范式。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.04570">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.04570">arXiv</a></p>
<hr />
<h3>2. V-Thinker：基于图像交互的思维框架</h3>
<p><strong>原文标题：</strong> V-Thinker: Interactive Thinking with Images</p>
<p><strong>摘要：</strong>
如何使大型多模态模型深度整合图像交互与长周期推理能力，始终是该领域长期存在的挑战。近期以视觉为中心的推理研究探索了"基于图像的思维"这一新兴范式，标志着从图像辅助推理到图像交互思维的范式转变。尽管这一里程碑进展使模型能够聚焦细粒度图像区域，但现有研究仍受限于狭窄的视觉工具空间和特定任务的工作流设计。为突破这些限制，我们提出V-Thinker——一个通过端到端强化学习实现交互式视觉思维的通用的多模态推理助手。该框架包含两大核心组件：（1）数据进化飞轮机制，通过多样性、质量与难度三维度自动合成、演进并验证交互式推理数据集；（2）视觉渐进训练课程，首先通过点级监督实现感知对齐，继而通过两阶段强化学习框架融合交互推理。此外，我们构建了VTBench专家验证基准，专门针对视觉中心交互推理任务进行评估。大量实验表明，V-Thinker在通用推理和交互推理场景中均持续优于基于大型多模态模型的基线方法，为推进图像交互推理应用提供了重要洞见。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.04460">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.04460">arXiv</a></p>
<hr />
<h3>3. 基于经验合成的智能体规模化学习</h3>
<p><strong>原文标题：</strong> Scaling Agent Learning via Experience Synthesis</p>
<p><strong>摘要：</strong>
尽管强化学习能够通过交互式自我优化赋能大语言模型智能体，但其实际应用仍面临诸多挑战：昂贵的环境交互成本、有限的任务多样性、不可靠的奖励信号以及复杂的基础设施要求，这些因素共同阻碍了可扩展经验数据的收集。为应对这些挑战，我们提出DreamGym——首个以可扩展性为核心目标、通过合成多样化经验来实现自主智能体高效在线强化学习的统一框架。该框架摒弃成本高昂的真实环境交互，将环境动态蒸馏为基于推理的经验模型，通过逐步推导生成连贯的状态转移与反馈信号，从而为强化学习提供可扩展的智能体交互数据收集方案。为提升状态转移的稳定性和质量，DreamGym采用由离线真实数据初始化、并通过持续注入新型交互不断扩展的经验回放缓冲区，以动态支撑智能体训练。在知识获取方面，DreamGym自适应生成挑战当前策略的新任务，实现更高效的在线课程学习。跨环境与智能体架构的实验表明，DreamGym在完全合成环境与仿真到现实迁移场景中均能显著提升强化学习效果。在WebArena等非强化学习适配任务中，DreamGym以超过30%的优势超越所有基线方法；在强化学习适配但成本高昂的场景中，仅通过合成交互即可达到GRPO和PPO的性能水平。当将纯合成经验训练的策略迁移至真实环境时，DreamGym在显著减少现实交互次数的同时带来额外性能提升，为通用强化学习提供了可扩展的预热启动策略。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.03773">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.03773">arXiv</a></p>
<hr />
<h3>4. 寒武纪-S：迈向视频空间超感知之路</h3>
<p><strong>原文标题：</strong> Cambrian-S: Towards Spatial Supersensing in Video</p>
<p><strong>摘要：</strong>
我们认为，实现真正多模态智能的进展需要从被动式任务驱动系统和暴力长上下文处理转向更广阔的超感知范式。我们将空间超感知定义为超越纯语言理解的四个阶段：语义感知（识别所见事物的名称）、流式事件认知（在连续体验中维持记忆）、隐式三维空间认知（推断像素背后的世界结构）以及预测性世界建模（建立筛选和组织信息的内部模型）。现有基准大多仅测试初级阶段，对空间认知的覆盖范围有限，且鲜少以需要真实世界建模的方式挑战模型。为推进空间超感知研究，我们提出VSI-SUPER双模块基准：VSR（长程视觉空间回溯）与VSC（持续视觉空间计数）。这些任务需要处理任意长度的视频输入，且能有效抵御暴力上下文扩展。我们通过构建VSI-590K数据集并训练寒武纪-S模型测试数据扩展极限，在VSI基准上实现30%的绝对性能提升且未牺牲通用能力。然而模型在VSI-SUPER上的表现仍存在局限，表明仅靠规模扩展不足以实现空间超感知。我们提出预测性感知作为发展路径，并通过概念验证展示自监督的潜在帧预测器如何利用预测误差驱动记忆与事件分割。在VSI-SUPER基准上，该方法显著超越主流闭源基线模型，证明空间超感知需要模型不仅能够观察，更要具备对经验的预测、筛选和组织能力。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.04670">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.04670">arXiv</a></p>
<hr />
<h3>5. GUI-360：面向计算机使用智能体的综合数据集与基准测试框架</h3>
<p><strong>原文标题：</strong> GUI-360: A Comprehensive Dataset and Benchmark for Computer-Using Agents</p>
<p><strong>摘要：</strong>
本文提出GUI-360^circ——一个大规模综合性数据集与基准测试套件，旨在推动计算机使用智能体（CUAs）的发展。当前CUAs面临三大持续性挑战：真实场景任务稀缺、多模态轨迹自动采集与标注流程缺失、以及缺乏统一评估GUI定位、屏幕解析与行为预测的基准框架。GUI-360^circ通过基于大语言模型的增强型自动化流程（包含查询溯源、环境模板构建、任务实例化、批量执行与质量过滤）解决了这些难题。该开放语料库涵盖主流Windows办公应用的数千条轨迹，包含超过120万次执行动作步骤，集成全分辨率屏幕截图、可访问性元数据（若可用）、实例化目标、中间推理轨迹以及成功/失败行为轨迹。本数据集支持GUI定位、屏幕解析和行为预测三大核心任务，并提供反映现代智能体设计的混合GUI+API行为空间。在GUI-360^circ上对前沿视觉-语言模型的基准测试显示，现有模型在定位与行为预测方面存在显著不足；监督微调与强化学习虽能提升性能，但尚未达到人类水平可靠性。我们公开GUI-360^circ数据集及配套代码，以促进可复现研究并加速稳健桌面CUAs的发展。完整数据集已发布于：https://huggingface.co/datasets/vyokky/GUI-360。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.04307">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.04307">arXiv</a></p>
<hr />
<h3>6. 英伟达Nemotron Nano V2 VL模型</h3>
<p><strong>原文标题：</strong> NVIDIA Nemotron Nano V2 VL</p>
<p><strong>摘要：</strong>
我们推出Nemotron视觉语言系列最新模型Nemotron Nano V2 VL，该模型专为强化现实场景文档理解、长视频解析与推理任务而设计。通过对模型架构、数据集和训练方案的重大改进，Nemotron Nano V2 VL在视觉与文本全领域性能均显著超越前代模型Llama-3.1-Nemotron-Nano-VL-8B。本模型基于混合Mamba-Transformer架构的Nemotron Nano V2大型语言模型，结合创新性令牌精简技术，在长文档和长视频场景中实现了更高的推理吞吐量。我们将发布BF16、FP8和FP4三种格式的模型检查点，并公开大部分数据集、训练方案及训练代码。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.03929">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.03929">arXiv</a></p>
<hr />
<h3>7. 基于多模态语义扰动的视觉语言模型污染检测</h3>
<p><strong>原文标题：</strong> Contamination Detection for VLMs using Multi-Modal Semantic Perturbation</p>
<p><strong>摘要：</strong>
视觉语言模型（VLM）的最新进展已在众多基准任务中实现最先进的性能。然而，使用互联网规模且通常具有专有属性的预训练语料库引发了从业者和用户共同关注的关键问题：由于测试集泄露导致的性能虚高。尽管已有研究针对大型语言模型提出了预训练数据净化与基准重构等缓解策略，但针对受污染VLM开发检测方法的互补方向仍待深入探索。为填补这一空白，我们通过故意污染开源VLM在主流基准测试上的数据，发现现有检测方法要么完全失效，要么表现出不一致的行为。进而提出一种基于多模态语义扰动的新型检测方法，该方法兼具简洁性与高效性，实验证明受污染模型在受控扰动下无法保持泛化能力。最后，我们在多种现实污染场景中验证该方法的有效性，确认其具有强鲁棒性和实用价值。相关代码与扰动数据集将公开发布。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.03774">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.03774">arXiv</a></p>
<hr />
<h3>8. 多头注意力机制的强彩票假设</h3>
<p><strong>原文标题：</strong> The Strong Lottery Ticket Hypothesis for Multi-Head Attention Mechanisms</p>
<p><strong>摘要：</strong>
强彩票假设认为，在随机初始化的神经网络中隐藏着高性能子网络（称为强彩票）。尽管近期理论研究已在多种神经架构中证实了该假设，但针对Transformer架构的强彩票假设仍缺乏理论支撑。特别是现有理论尚未涵盖Transformer的核心组件——多头注意力机制。为填补这一空白，我们提出了对多头注意力机制中存在强彩票的理论分析。研究证明：若具有H个头、输入维度为d的随机初始化多头注意力机制中键与值的隐藏维度为O(dlog(Hd^{3/2}))，则该机制极大概率包含可逼近任意具有相同输入维度多头注意力机制的强彩票。进一步地，通过将该理论拓展至无归一化层的Transformer架构，我们实证验证了理论发现：当增加源模型（多头注意力机制与Transformer）的隐藏维度时，其内部强彩票与近似目标模型之间的逼近误差呈指数级下降。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.04217">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.04217">arXiv</a></p>
<hr />
<h3>9. 基准设计者应“在测试集上训练”以揭示可被利用的非视觉捷径</h3>
<p><strong>原文标题：</strong> Benchmark Designers Should "Train on the Test Set" to Expose Exploitable
  Non-Visual Shortcuts</p>
<p><strong>摘要：</strong>
稳健的基准对于评估多模态大语言模型至关重要。然而我们发现，许多模型无需具备强大的视觉理解能力即可在多项多模态基准测试中取得优异成绩，这实际上是通过利用数据偏差、语言先验和表面模式实现的。对于本需依赖视觉输入的以视觉为中心的基准而言，这一问题尤为严重。我们提出一项基准设计的诊断原则：若基准存在被钻空子的可能，则必将被钻空子。因此设计者应率先尝试“破解”自身构建的基准，通过诊断与去偏差流程系统性地识别并消除非视觉偏差。有效的诊断需要直接“在测试集上训练”——通过探测已发布测试集固有的可被利用模式来实现。我们将这一标准具体化为两个组成部分：首先采用“测试集压力测试”方法诊断基准的脆弱性，主要诊断工具涉及对强大语言模型进行k折交叉验证的微调（仅使用测试集的非视觉文本输入），以揭示捷径性能并为每个样本分配偏差分数s(x)；同时辅以基于手工特征的轻量级随机森林诊断法，实现快速可解释的审计。其次通过“迭代偏差剪枝”程序过滤高偏差样本以实现基准去偏差。将该框架应用于VSI-Bench、CV-Bench、MMMU和VideoMME四个基准后，我们发现了普遍存在的非视觉偏差。作为案例研究，我们应用完整框架创建了VSI-Bench-Debiased，结果显示其非视觉可解性显著降低，且视觉盲区性能差距较原始基准更为明显。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.04655">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.04655">arXiv</a></p>
<hr />
<h3>10. 面向仿人机器人的视觉驱动反应式足球技能学习</h3>
<p><strong>原文标题：</strong> Learning Vision-Driven Reactive Soccer Skills for Humanoid Robots</p>
<p><strong>摘要：</strong>
仿人足球运动对具身智能提出了代表性挑战，要求机器人在紧密耦合的感知-行动循环中运作。然而，现有系统通常依赖解耦模块，导致动态环境中出现响应延迟与行为失调，而现实世界的感知局限进一步加剧了这些问题。本研究提出一种基于强化学习的统一控制器，通过视觉感知与运动控制的直接集成，使仿人机器人获得反应式足球技能。我们的方法将对抗运动先验扩展至现实动态环境中的感知场景，搭建起运动模仿与视觉基础动态控制之间的桥梁。我们引入结合虚拟感知系统的编码器-解码器架构，该系统可建模真实世界的视觉特性，使策略能够从不完美观测中恢复特权状态，并建立感知与行动的主动协同。最终实现的控制器展现出强大的反应能力，在包括真实RoboCup比赛在内的多种场景中，持续执行协调一致且鲁棒的足球行为。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.03996">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.03996">arXiv</a></p>
<hr />
<h3>11. 如何利用源语言感知神经机器翻译指标评估语音翻译系统</h3>
<p><strong>原文标题：</strong> How to Evaluate Speech Translation with Source-Aware Neural MT Metrics</p>
<p><strong>摘要：</strong>
语音到文本翻译系统的自动评估通常通过比较翻译假设与一个或多个参考译文来实现。尽管这种方法具有一定效果，但它继承了基于参考评估的固有局限——忽略了源语言输入中的有价值信息。在机器翻译领域，最新研究表明融入源文本的神经评估指标能获得与人工评判更强的一致性。然而将这一思路延伸至语音翻译领域面临特殊挑战：源输入是音频而非文本，且通常无法获得可靠的源语言转录文本或源语与参考译文的对齐信息。本研究首次系统探讨了语音翻译的源语言感知评估方法，特别关注源语言文本不可用的实际应用场景。我们提出了两种互补的文本代理生成策略：自动语音识别转录文本和参考译文的反向翻译，并引入一种新颖的两步跨语言重对齐算法来解决合成源文本与参考译文之间的对齐失配问题。我们在涵盖79个语言对的两个语音翻译基准测试集上展开实验，评估了六种不同架构与性能水平的语音翻译系统。实验结果表明：当词错误率低于20%时，自动语音识别转录文本比反向翻译更能提供可靠的合成源语言信息，而反向翻译始终是计算成本更低且仍具效力的替代方案。此外，我们提出的跨语言重对齐算法能够确保源语言感知机器翻译指标在语音翻译评估中的稳健应用，为建立更精准、更系统的语音翻译评估方法开辟了新途径。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.03295">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.03295">arXiv</a></p>
<hr />
<h3>12. SIMS-V：面向空间视频理解的模拟指令调优框架</h3>
<p><strong>原文标题：</strong> SIMS-V: Simulated Instruction-Tuning for Spatial Video Understanding</p>
<p><strong>摘要：</strong>
尽管多模态语言模型在高层视频理解方面表现卓越，但其在跨时空空间推理方面仍存在困难。当前空间训练方法主要依赖现实世界视频数据，然而获取具有精确空间标注的多样化视频素材仍是瓶颈问题。为突破此限制，我们提出SIMS-V——一个系统化的数据生成框架，通过利用三维模拟器的特权信息，为多模态语言模型创建富含空间信息的视频训练数据。基于该框架，我们通过系统化消融实验探究问题类型、混合策略与数据规模等模拟数据特性对现实世界迁移效果的影响。研究发现，仅需三类核心问题（度量测算、视角依赖推理与时序追踪）即可最有效地发展可迁移的空间智能，其效果优于全面覆盖式的提问策略。这些发现实现了高效训练：我们基于2.5万模拟样本微调的70亿参数视频大语言模型，不仅超越720亿参数基线模型，更在严谨的现实空间推理基准测试中与专用模型性能相当。该方法展现出强大泛化能力，在保持通用视频理解性能的同时，在具身智能任务和现实空间任务上实现显著提升。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.04668">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.04668">arXiv</a></p>
<hr />
<h3>13. 面向大语言模型系统的RDMA点对点通信技术</h3>
<p><strong>原文标题：</strong> RDMA Point-to-Point Communication for LLM Systems</p>
<p><strong>摘要：</strong>
新兴的大语言模型系统范式，如分离式推理、专家混合路由及异步强化微调等，需要超越传统集合通信的灵活点对点通信支持。现有实现方案受限于特定网络接口控制器，难以融入推理引擎且缺乏跨硬件供应商的移植性。本文提出TransferEngine系统，通过桥接通用网卡功能提供统一接口。该系统采用ImmCounter原语实现单向WriteImm操作完成通知，无需依赖网络传输顺序假设，可透明管理每块GPU对应的多个网卡。实验表明在NVIDIA ConnectX-7与AWS弹性光纤适配器上均实现400 Gbps峰值吞吐量。通过三个生产系统验证其效能：(1)支持动态扩展的分离式推理KvCache传输；(2)万亿参数模型的强化学习权重更新仅需1.3秒；(3)在ConnectX-7上实现超越DeepEP解码延迟的专家混合调度/聚合方案，并在EFA上获得首批可行延迟数据。研究表明，该可移植点对点通信方案可与集合通信形成互补，同时规避供应商锁定风险。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2510.27656">HuggingFace</a> | <a href="https://arxiv.org/abs/2510.27656">arXiv</a></p>
<hr />
<h3>14. SAIL-RL：基于双奖励强化学习的多模态大语言模型思考时机与方式引导框架</h3>
<p><strong>原文标题：</strong> SAIL-RL: Guiding MLLMs in When and How to Think via Dual-Reward RL
  Tuning</p>
<p><strong>摘要：</strong>
本文提出SAIL-RL强化学习后训练框架，通过指导多模态大语言模型掌握思考时机与思考方式，显著增强其推理能力。现有方法存在双重局限：仅依赖结果监督的机制虽奖励正确答案却无法确保推理过程的合理性；采用统一思考策略常导致简单任务过度思考而复杂任务思考不足。SAIL-RL通过双奖励机制突破这些限制：思考奖励从事实依据、逻辑连贯性及答案一致性三维度评估推理质量，判断奖励则自适应决策应进行深度推理或直接作答。在最新SAIL-VL2模型上的实验表明，该框架在4B与8B参数规模下均能提升推理与多模态理解基准性能，相较于GPT-4o等商业闭源模型展现出竞争优势，并显著降低幻觉现象，为构建更可靠、自适应的多模态大语言模型建立了理论框架。代码已开源：https://github.com/BytedanceDouyinContent/SAIL-RL。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.02280">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.02280">arXiv</a></p>
<hr />
<h3>15. EVTAR：基于额外非配对视觉参考的端到端虚拟试衣系统</h3>
<p><strong>原文标题：</strong> EVTAR: End-to-End Try on with Additional Unpaired Visual Reference</p>
<p><strong>摘要：</strong>
本文提出EVTAR模型——一种融合附加参考的端到端虚拟试衣系统，该模型在将目标服装直接拟合至人体图像的同时，通过引入参考图像提升试衣精度。当前主流虚拟试衣方法需依赖复杂输入，如不可知人体图像、人体姿态、密集姿态或身体关键点等，导致实施过程繁琐且难以应用于实际场景。相较之下，EVTAR采用两阶段训练策略，仅需源图像与目标服装即可完成简易推理。本模型无需掩码、密集姿态或分割图即可生成试衣效果，更通过引入其他穿着同款服装的参考者图像，显著提升服装纹理与细粒度细节的保持能力。这种机制模拟了人类挑选服装时参考模特展示的决策过程，从而实现更逼真高质量的着装效果。为支撑这些功能，我们通过补充参考图像与非配对人体图像丰富了训练数据。在两大常用基准数据集及多样化任务上的实验结果表明，本方法持续展现出卓越性能。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.00956">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.00956">arXiv</a></p>
<hr />
<h2>🔍 关键词云图</h2>
<p><img alt="关键词云图" src="../images/keywords_wordcloud.png" /></p>
<h2>📈 近期论文趋势</h2>
<p><img alt="论文趋势" src="../images/daily_papers.png" /></p>
<h2>🎙️ 语音播报</h2>
<ul>
<li><a href="../audio/2025-11-07_daily_papers.mp3">收听今日论文解读</a></li>
</ul>
<h2>📱 订阅渠道</h2>
<ul>
<li>GitHub: <a href="https://github.com/2404589803/hf-daily-paper-newsletter-chinese">hf-daily-paper-newsletter-chinese</a></li>
</ul>
    </div>
</body>
</html>