<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Hugging Face 论文日报 - 2025-11-07</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', sans-serif;
            line-height: 1.6;
            color: #333;
            background-color: #f5f5f5;
            padding: 20px;
        }
        
        .container {
            max-width: 900px;
            margin: 0 auto;
            background-color: white;
            padding: 40px;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }
        
        h1 {
            color: #2c3e50;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
            margin-bottom: 30px;
            font-size: 28px;
        }
        
        h1 img {
            vertical-align: middle;
            margin-right: 10px;
        }
        
        h2 {
            color: #34495e;
            margin-top: 40px;
            margin-bottom: 20px;
            font-size: 24px;
            border-left: 4px solid #3498db;
            padding-left: 15px;
        }
        
        h3 {
            color: #2c3e50;
            margin-top: 30px;
            margin-bottom: 15px;
            font-size: 20px;
        }
        
        p {
            margin-bottom: 15px;
            text-align: justify;
        }
        
        ul {
            margin-left: 20px;
            margin-bottom: 20px;
        }
        
        li {
            margin-bottom: 8px;
        }
        
        a {
            color: #3498db;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        strong {
            color: #2c3e50;
            font-weight: 600;
        }
        
        hr {
            border: none;
            border-top: 1px solid #e0e0e0;
            margin: 30px 0;
        }
        
        /* 关键修复:限制图片宽度 */
        img {
            max-width: 100%;
            height: auto;
            display: block;
            margin: 20px auto;
            border-radius: 4px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        
        /* 确保图片容器也有宽度限制 */
        p img {
            max-width: 100%;
        }
        
        /* 论文详情区域样式 */
        .paper-section {
            background-color: #f9f9f9;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 20px;
        }
        
        /* 统计信息样式 */
        .stats {
            background-color: #e8f4f8;
            padding: 15px;
            border-radius: 6px;
            margin-bottom: 20px;
        }
        
        /* 响应式设计 */
        @media (max-width: 768px) {
            .container {
                padding: 20px;
            }
            
            h1 {
                font-size: 24px;
            }
            
            h2 {
                font-size: 20px;
            }
            
            h3 {
                font-size: 18px;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <h1><img src="https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo.png" width="30"/> Hugging Face 2025-11-07 论文日报</h1>
<h2>📊 今日论文统计</h2>
<ul>
<li>总论文数：15</li>
<li>热门领域：LLM, Transformer, RL, GPT</li>
</ul>
<h2>📝 论文详情</h2>
<h3>1. 视频思维：视频生成作为一种前景广阔的多模态推理范式</h3>
<p><strong>原文标题：</strong> Thinking with Video: Video Generation as a Promising Multimodal
  Reasoning Paradigm</p>
<p><strong>摘要：</strong>
“文本思维”与“图像思维”范式显著提升了大型语言模型（LLMs）和视觉语言模型（VLMs）的推理能力。然而这些范式存在固有局限：（1）图像仅能捕捉瞬时状态，无法呈现动态过程或连续变化；（2）文本与视觉作为独立模态的割裂状态，阻碍了统一的多模态理解与生成。为突破这些限制，我们提出“视频思维”新范式，通过Sora-2等视频生成模型在统一时序框架中桥接视觉与文本推理。为支持该研究，我们开发了视频思维基准测试集（VideoThinkBench），包含两大任务类别：（1）以视觉为核心的任务（如目测谜题）；（2）以文本为核心的任务（如GSM8K、MMMU子集）。评估结果表明Sora-2具备卓越推理能力：在视觉核心任务中，其整体表现与最先进VLMs相当，且在目测游戏等任务中超越VLMs；在文本核心任务中，Sora-2在MATH数据集准确率达92%，在MMMU数据集达75.53%。我们系统分析了这些能力的来源，并发现自洽性与上下文学习能提升Sora-2表现。本研究证实视频生成模型有望成为统一的多模态理解与生成模型，使“视频思维”成为统一的多模态推理范式。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.04570">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.04570">arXiv</a></p>
<hr />
<h3>2. V-Thinker：基于图像的交互式思维框架</h3>
<p><strong>原文标题：</strong> V-Thinker: Interactive Thinking with Images</p>
<p><strong>摘要：</strong>
如何使大型多模态模型深度整合图像交互能力与长程推理机制，始终是该领域长期存在的关键挑战。近期以视觉为核心的推理研究探索出"基于图像的思维"新范式，标志着从图像辅助推理向图像交互式思维的范式转变。尽管这一里程碑进展使模型能够聚焦细粒度图像区域，但现有研究仍受限于狭窄的视觉工具空间和特定任务的工作流设计。为突破这些限制，我们提出V-Thinker——通过端到端强化学习实现以视觉为核心的交互式思维通用多模态推理助手。该框架包含两大核心组件：(1) 数据演化飞轮机制，通过多样性、质量与难度三维度自动合成、演进并验证交互式推理数据集；(2) 视觉渐进训练课程体系，首先通过点级监督实现感知对齐，继而通过两阶段强化学习框架融合交互推理能力。此外，我们构建了专家验证的VTBench基准测试集，专门针对视觉核心交互推理任务。大量实验表明，V-Thinker在通用推理与交互推理场景中均持续超越基于大型多模态模型的强基线方法，为推进图像交互式推理应用提供了重要洞见。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.04460">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.04460">arXiv</a></p>
<hr />
<h3>3. 基于经验合成的智能体规模化学习</h3>
<p><strong>原文标题：</strong> Scaling Agent Learning via Experience Synthesis</p>
<p><strong>摘要：</strong>
尽管强化学习能够通过交互式自我改进赋能大语言模型智能体，但其实际应用仍面临 rollout 成本高昂、任务多样性有限、奖励信号不可靠及基础设施复杂等挑战，这些问题共同阻碍了可扩展经验数据的采集。为应对这些挑战，我们提出 DreamGym——首个以可扩展性为核心目标，通过合成多样化经验来支撑自主智能体在线强化学习训练的统一框架。该框架摒弃昂贵的真实环境 rollout，通过将环境动态蒸馏至基于推理的经验模型，借助逐步推理推导出连贯的状态转移与反馈信号，从而实现可扩展的强化学习智能体 rollout 采集。为提升状态转移的稳定性和质量，DreamGym 采用经离线真实数据初始化的经验回放缓冲区，并通过持续注入新交互数据来动态支撑智能体训练。在知识获取方面，DreamGym 自适应生成挑战当前策略的新任务，实现更高效的在线课程学习。跨环境与智能体架构的实验表明，DreamGym 在完全合成环境与仿真到现实迁移场景中均显著提升强化学习训练效果。在 WebArena 等非强化学习适配任务上，DreamGym 以超过 30% 的优势超越所有基线方法；在强化学习适配但成本高昂的场景中，仅通过合成交互即可达到 GRPO 与 PPO 的性能水平。当将纯合成经验训练的策略迁移至真实环境强化学习时，DreamGym 在大幅减少现实交互次数的同时带来显著性能提升，为通用强化学习提供了可扩展的热启动策略。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.03773">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.03773">arXiv</a></p>
<hr />
<h3>4. 寒武纪-S：迈向视频空间超感知之路</h3>
<p><strong>原文标题：</strong> Cambrian-S: Towards Spatial Supersensing in Video</p>
<p><strong>摘要：</strong>
我们认为实现真正多模态智能的进展需要从被动式、任务驱动的系统和暴力长上下文处理转向更广阔的超感知范式。我们将空间超感知定义为超越纯语言理解的四个阶段：语义感知（识别所见之物）、流式事件认知（在连续体验中维持记忆）、隐式三维空间认知（推断像素背后的世界）以及预测性世界建模（创建用于筛选和组织信息的内部模型）。现有基准大多仅测试初级阶段，对空间认知的覆盖范围有限，且鲜少以需要真实世界建模的方式挑战模型。为推动空间超感知的发展，我们提出VSI-SUPER双模块基准：VSR（长程视觉空间回溯）与VSC（持续视觉空间计数）。这些任务需要任意长度的视频输入，却能有效规避暴力上下文扩展的缺陷。通过构建VSI-590K数据集并训练寒武纪-S模型，我们在不牺牲通用能力的前提下于VSI基准上实现了30%的绝对性能提升。然而模型在VSI-SUPER上的表现仍存在局限，表明仅靠规模扩展无法实现空间超感知。我们提出预测性感知作为发展路径，并通过概念验证展示自监督的潜在帧预测器如何利用预测误差驱动记忆与事件分割。该方法在VSI-SUPER基准上显著超越主流商业基线，证明空间超感知需要模型不仅能够观察，更应具备对经验的预测、筛选和组织能力。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.04670">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.04670">arXiv</a></p>
<hr />
<h3>5. GUI-360：面向计算机使用智能体的综合数据集与基准测试框架</h3>
<p><strong>原文标题：</strong> GUI-360: A Comprehensive Dataset and Benchmark for Computer-Using Agents</p>
<p><strong>摘要：</strong>
本文推出GUI-360^circ——一个大规模综合性数据集与基准测试套件，旨在推动计算机使用智能体（CUAs）的发展。当前CUAs面临三大持续性挑战：真实场景任务稀缺、缺乏多模态轨迹的自动化采集标注流程、以及缺失能同步评估GUI定位、屏幕解析与行动预测的统一基准。GUI-360^circ通过采用LLM增强的自动化流程（涵盖查询溯源、环境模板构建、任务实例化、批量执行及LLM驱动的质量过滤）有效解决了这些瓶颈。该开放语料库包含在主流Windows办公应用中采集的数千条交互轨迹，涵盖超120万次执行动作步骤，内含全分辨率屏幕截图、可访问性元数据（若可用）、实例化目标、中间推理轨迹以及成功/失败的行动轨迹。本数据集支持GUI定位、屏幕解析与行动预测三大核心任务，并提供符合现代智能体设计的GUI+API混合行动空间。基于GUI-360^circ对前沿视觉-语言模型的基准测试表明，现有模型在定位与行动预测方面存在显著缺陷；监督微调与强化学习虽能带来明显提升，但仍未达到人类水平可靠性。我们公开GUI-360^circ数据集及配套代码，以促进可复现研究并加速鲁棒性桌面CUAs的发展。完整数据集已发布于https://huggingface.co/datasets/vyokky/GUI-360。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.04307">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.04307">arXiv</a></p>
<hr />
<h3>6. 英伟达Nemotron Nano V2 VL模型</h3>
<p><strong>原文标题：</strong> NVIDIA Nemotron Nano V2 VL</p>
<p><strong>摘要：</strong>
我们推出Nemotron Nano V2 VL——Nemotron视觉语言系列的最新模型，专为强化现实场景文档理解、长视频解析与推理任务而设计。该模型通过架构创新、数据集优化与训练方法的重大升级，在视觉与文本全领域性能上显著超越前代模型Llama-3.1-Nemotron-Nano-VL-8B。本架构基于混合Mamba-Transformer大语言模型Nemotron Nano V2，结合创新性令牌压缩技术，在长文档与长视频场景中实现更高推理吞吐量。我们现发布BF16、FP8及FP4格式的模型检查点，并公开大部分数据集、训练方案与实现代码。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.03929">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.03929">arXiv</a></p>
<hr />
<h3>7. 基于多模态语义扰动的视觉语言模型污染检测</h3>
<p><strong>原文标题：</strong> Contamination Detection for VLMs using Multi-Modal Semantic Perturbation</p>
<p><strong>摘要：</strong>
视觉语言模型（VLM）的最新进展已在众多基准任务中实现最先进的性能。然而，使用互联网规模且通常具有专有属性的预训练语料库给从业者和用户带来了关键隐忧：因测试集泄露导致的性能虚高。虽然已有研究针对大型语言模型提出了预训练数据净化与基准重构等缓解策略，但针对受污染VLM开发检测方法的互补方向仍待深入探索。为填补这一空白，我们通过在主流基准上对开源VLM实施定向污染，发现现有检测方法要么完全失效，要么表现出不一致的行为。继而提出一种基于多模态语义扰动的新型检测方法，该方法简洁而高效，实证表明受污染模型在受控扰动下无法保持泛化能力。最后，我们在多种现实污染场景中验证该方法的有效性，确认其具有稳健性与实用性。相关代码与扰动数据集将公开发布。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.03774">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.03774">arXiv</a></p>
<hr />
<h3>8. 多头注意力机制的强彩票假设</h3>
<p><strong>原文标题：</strong> The Strong Lottery Ticket Hypothesis for Multi-Head Attention Mechanisms</p>
<p><strong>摘要：</strong>
强彩票假设(SLTH)提出，在随机初始化的神经网络中隐藏着高性能子网络，即强彩票(SLT)。尽管近期理论研究已证实SLTH适用于多种神经网络架构，但针对Transformer架构的SLTH仍缺乏理论支撑。特别是当前SLTH理论尚未涵盖多头注意力(MHA)机制——Transformer的核心组件。为填补这一空白，我们提出了MHA中SLT存在性的理论分析。我们证明，若具有H个头、输入维度为d的随机初始化MHA其键值隐藏维度为O(dlog(Hd^{3/2}))，则该模型大概率包含可近似任意同输入维度MHA的SLT。进一步地，通过运用该MHA理论，我们将SLTH扩展至无归一化层的Transformer架构。我们通过实验验证了理论发现，证明当增加源模型（MHA和Transformer）的隐藏维度时，源模型内SLT与近似目标模型之间的逼近误差呈指数级下降。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.04217">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.04217">arXiv</a></p>
<hr />
<h3>9. 基准设计者应“在测试集上训练”以揭示可被利用的非视觉捷径</h3>
<p><strong>原文标题：</strong> Benchmark Designers Should "Train on the Test Set" to Expose Exploitable
  Non-Visual Shortcuts</p>
<p><strong>摘要：</strong>
稳健的基准对于评估多模态大语言模型至关重要。然而我们发现，许多模型无需具备强大的视觉理解能力即可在多项多模态基准测试中取得优异成绩，这实际上是利用了数据偏差、语言先验和表面模式。对于本需依赖视觉输入的以视觉为核心的基准测试而言，这一问题尤为严重。我们提出一项基准设计的诊断原则：若某个基准存在被取巧的可能，则其终将被利用。因此，设计者应当率先尝试“破解”自身设计的基准，通过诊断与去偏差流程系统性地识别并消除非视觉偏差。有效的诊断需要直接“在测试集上训练”——通过探测已发布测试集固有的可被利用模式来实现。我们将该标准具体化为两个组成部分：首先采用“测试集压力测试”方法诊断基准的脆弱性，主要诊断工具涉及对强大语言模型进行k折交叉验证的微调（仅使用测试集中的非视觉文本输入），以揭示捷径性能并为每个样本分配偏差分数s(x)；同时辅以基于手工特征的轻量级随机森林诊断方法，实现快速可解释的审计。其次，我们通过“迭代偏差剪枝”程序过滤高偏差样本以实现基准去偏差。将该框架应用于VSI-Bench、CV-Bench、MMMU和VideoMME四个基准测试后，我们发现了普遍存在的非视觉偏差。作为案例研究，我们应用完整框架创建了VSI-Bench-Debiased版本，结果显示相较于原版，该版本不仅降低了非视觉可解性，还扩大了视觉盲区性能差距。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.04655">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.04655">arXiv</a></p>
<hr />
<h3>10. 面向仿人机器人的视觉驱动反应式足球技能学习</h3>
<p><strong>原文标题：</strong> Learning Vision-Driven Reactive Soccer Skills for Humanoid Robots</p>
<p><strong>摘要：</strong>
仿人足球运动对具身智能提出了代表性挑战，要求机器人在紧密耦合的感知-行动循环中运作。然而现有系统通常依赖解耦模块，导致动态环境中出现响应延迟与行为失协，而现实世界的感知局限进一步加剧了这些问题。本研究提出基于强化学习的统一控制器，通过视觉感知与运动控制的直接集成，使仿人机器人获得反应式足球技能。我们的方法将对抗运动先验扩展至现实动态环境中的感知场景，搭建起运动模仿与视觉基础动态控制之间的桥梁。我们引入结合虚拟感知系统的编码器-解码器架构，该系统可建模真实视觉特性，使策略能够从不完美观测中恢复特权状态，并建立感知与行动的主动协调机制。最终实现的控制器展现出强大的反应能力，在包括真实RoboCup比赛在内的多种场景中，持续执行协调一致且鲁棒的足球运动行为。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.03996">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.03996">arXiv</a></p>
<hr />
<h3>11. 如何利用源语言感知神经机器翻译指标评估语音翻译质量</h3>
<p><strong>原文标题：</strong> How to Evaluate Speech Translation with Source-Aware Neural MT Metrics</p>
<p><strong>摘要：</strong>
语音到文本翻译系统的自动评估通常通过比较翻译假设与一个或多个参考译文来实现。尽管这种方法具有一定效果，但其继承了基于参考评估的固有局限——忽略了源语言输入中的有价值信息。在机器翻译领域，最新研究表明融入源文本的神经评估指标能获得与人工评判更高的一致性。然而将这一思路延伸至语音翻译领域面临特殊挑战：源输入为音频而非文本，且通常无法获得可靠的源语言转录文本或源语与参考译文的对齐信息。本研究首次系统探讨了面向语音翻译的源语言感知评估方法，重点关注源语言文本不可获取的实际应用场景。我们提出了两种互补的文本代理生成策略：自动语音识别转录文本和参考译文的反向翻译，并引入一种新颖的双语跨语言重对齐算法以解决合成源文本与参考译文之间的对齐失配问题。基于覆盖79个语言对的两个语音翻译基准测试集及六种不同架构与性能水平的系统实验表明：当词错误率低于20%时，自动语音识别转录文本比反向翻译更能提供可靠的合成源语言信息，而反向翻译始终是计算成本更低且仍具效力的替代方案。此外，我们的跨语言重对齐算法能够支持源语言感知机器翻译指标在语音翻译评估中的稳健应用，为建立更精准、更系统的语音翻译评估方法开辟了新途径。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.03295">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.03295">arXiv</a></p>
<hr />
<h3>12. SIMS-V：面向空间视频理解的模拟指令调优框架</h3>
<p><strong>原文标题：</strong> SIMS-V: Simulated Instruction-Tuning for Spatial Video Understanding</p>
<p><strong>摘要：</strong>
尽管多模态语言模型在高层视频理解方面表现卓越，但其在跨时空空间推理方面仍存在困难。当前空间训练方法主要依赖真实世界视频数据，然而获取具有精确空间标注的多样化视频素材仍是瓶颈问题。为突破此限制，我们提出SIMS-V——一个系统化的数据生成框架，通过利用三维模拟器的特权信息为多模态语言模型创建富含空间信息的视频训练数据。基于该框架，我们通过系统化消融实验探究问题类型、混合策略与数据规模等模拟数据特性对现实世界迁移效果的影响。研究发现，仅需三种核心问题类别（度量测算、视角依赖推理与时序追踪）即可最有效地开发可迁移的空间智能，其效果优于全面覆盖的问题类型策略。这些发现实现了高效训练：我们基于仅2.5万模拟样本微调的70亿参数视频大语言模型，不仅超越720亿参数基线模型，更在严谨的真实世界空间推理基准测试中与专有模型性能相当。该方法展现出强大的泛化能力，在保持通用视频理解性能的同时，在具身化任务和真实世界空间任务上实现显著提升。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.04668">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.04668">arXiv</a></p>
<hr />
<h3>13. 面向大语言模型系统的RDMA点对点通信技术</h3>
<p><strong>原文标题：</strong> RDMA Point-to-Point Communication for LLM Systems</p>
<p><strong>摘要：</strong>
新兴的大语言模型系统范式，包括解耦推理、专家混合路由与异步强化微调等场景，亟需突破传统集合通信模式的可扩展点对点通信能力。现有实现方案受限于特定网络接口控制器，难以融入推理引擎且缺乏跨硬件平台的移植性。本文提出TransferEngine系统，通过抽象通用网卡功能提供统一接口。该系统基于ImmCounter原语实现单向WriteImm操作完成通知，无需依赖网络传输的有序性假设，并透明管理每块GPU对应的多块网卡。实验表明，在NVIDIA ConnectX-7与AWS弹性结构适配器上均实现了400 Gbps的峰值吞吐量。我们通过三个生产系统验证TransferEngine：(1)支持动态扩缩容的解耦推理场景下KvCache传输；(2)万亿参数模型的强化学习权重更新仅需1.3秒；(3)在ConnectX-7上超越DeepEP解码延迟的专家混合调度/聚合实现，并在EFA上取得首例可行延迟案例。研究表明，该可移植点对点通信方案在避免供应商锁定的同时，可与集合通信形成有效互补。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2510.27656">HuggingFace</a> | <a href="https://arxiv.org/abs/2510.27656">arXiv</a></p>
<hr />
<h3>14. SAIL-RL：基于双奖励强化学习调优机制指导多模态大语言模型的思考时机与思考方式</h3>
<p><strong>原文标题：</strong> SAIL-RL: Guiding MLLMs in When and How to Think via Dual-Reward RL
  Tuning</p>
<p><strong>摘要：</strong>
本文提出SAIL-RL强化学习后训练框架，通过指导多模态大语言模型掌握思考时机与思考方式，显著增强其推理能力。现有方法存在两大局限：仅关注最终答案正确性的结果监督模式，无法保证推理过程的合理性；采用统一思考策略，常导致简单任务过度思考而复杂任务思考不足。SAIL-RL通过双奖励机制突破这些限制：思考奖励从事实依据、逻辑连贯性及答案一致性三个维度评估推理质量，判断奖励则自适应决策适合深度推理或直接回答的情境。在最新SAIL-VL2模型上的实验表明，该框架在4B与8B参数规模下均能提升推理与多模态理解基准性能，相较GPT-4o等商业闭源模型展现出竞争优势，并显著降低幻觉现象，为构建更可靠、自适应的多模态大语言模型提供了理论框架。代码将发布于https://github.com/BytedanceDouyinContent/SAIL-RL。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.02280">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.02280">arXiv</a></p>
<hr />
<h3>15. EVTAR：基于额外非配对视觉参考的端到端虚拟试衣系统</h3>
<p><strong>原文标题：</strong> EVTAR: End-to-End Try on with Additional Unpaired Visual Reference</p>
<p><strong>摘要：</strong>
本文提出EVTAR模型——一种融合附加参考的端到端虚拟试衣系统，该系统在将目标服装直接适配至人体图像的同时，通过引入参考图像提升试衣精度。现有虚拟试衣方法大多依赖复杂输入，如不可知人体图像、人体姿态、密集姿态或身体关键点，导致流程繁琐且难以实际应用。相较之下，EVTAR采用两阶段训练策略，仅需源图像与目标服装即可完成简易推理。本模型无需掩码、密集姿态或分割图即可生成试衣效果，更通过引入其他穿着同款服装的参考图像，有效保持衣物纹理与细粒度细节。这种机制模拟人类挑选服装时参考模特展示的认知过程，从而实现更逼真高质量的着装效果。为支撑这些功能，我们通过补充参考数据与非配对人体图像扩展了训练数据集。在两大常用基准测试及多样化任务上的实验结果表明，该方法持续展现出卓越性能。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.00956">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.00956">arXiv</a></p>
<hr />
<h2>🔍 关键词云图</h2>
<p><img alt="关键词云图" src="../images/keywords_wordcloud.png" /></p>
<h2>📈 近期论文趋势</h2>
<p><img alt="论文趋势" src="../images/daily_papers.png" /></p>
<h2>🎙️ 语音播报</h2>
<ul>
<li><a href="../audio/2025-11-07_daily_papers.mp3">收听今日论文解读</a></li>
</ul>
<h2>📱 订阅渠道</h2>
<ul>
<li>GitHub: <a href="https://github.com/2404589803/hf-daily-paper-newsletter-chinese">hf-daily-paper-newsletter-chinese</a></li>
</ul>
    </div>
</body>
</html>