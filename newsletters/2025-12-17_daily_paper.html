<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Hugging Face 论文日报 - 2025-12-17</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', sans-serif;
            line-height: 1.6;
            color: #333;
            background-color: #f5f5f5;
            padding: 20px;
        }
        
        .container {
            max-width: 900px;
            margin: 0 auto;
            background-color: white;
            padding: 40px;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }
        
        h1 {
            color: #2c3e50;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
            margin-bottom: 30px;
            font-size: 28px;
        }
        
        h1 img {
            vertical-align: middle;
            margin-right: 10px;
        }
        
        h2 {
            color: #34495e;
            margin-top: 40px;
            margin-bottom: 20px;
            font-size: 24px;
            border-left: 4px solid #3498db;
            padding-left: 15px;
        }
        
        h3 {
            color: #2c3e50;
            margin-top: 30px;
            margin-bottom: 15px;
            font-size: 20px;
        }
        
        p {
            margin-bottom: 15px;
            text-align: justify;
        }
        
        ul {
            margin-left: 20px;
            margin-bottom: 20px;
        }
        
        li {
            margin-bottom: 8px;
        }
        
        a {
            color: #3498db;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        strong {
            color: #2c3e50;
            font-weight: 600;
        }
        
        hr {
            border: none;
            border-top: 1px solid #e0e0e0;
            margin: 30px 0;
        }
        
        /* 关键修复:限制图片宽度 */
        img {
            max-width: 100%;
            height: auto;
            display: block;
            margin: 20px auto;
            border-radius: 4px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        
        /* 确保图片容器也有宽度限制 */
        p img {
            max-width: 100%;
        }
        
        /* 论文详情区域样式 */
        .paper-section {
            background-color: #f9f9f9;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 20px;
        }
        
        /* 统计信息样式 */
        .stats {
            background-color: #e8f4f8;
            padding: 15px;
            border-radius: 6px;
            margin-bottom: 20px;
        }
        
        /* 响应式设计 */
        @media (max-width: 768px) {
            .container {
                padding: 20px;
            }
            
            h1 {
                font-size: 24px;
            }
            
            h2 {
                font-size: 20px;
            }
            
            h3 {
                font-size: 18px;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <h1><img src="https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo.png" width="30"/> Hugging Face 2025-12-17 论文日报</h1>
<h2>📊 今日论文统计</h2>
<ul>
<li>总论文数：39</li>
<li>热门领域：GPT, AIGC, RL, LLM, Transformer</li>
</ul>
<h2>📝 论文详情</h2>
<h3>1. MMGR：多模态生成式推理评估框架</h3>
<p><strong>原文标题：</strong> MMGR: Multi-Modal Generative Reasoning</p>
<p><strong>摘要：</strong>
视频基础模型能够生成视觉逼真且时序连贯的内容，但其作为世界模拟器的可靠性取决于是否能够捕捉物理、逻辑与空间约束。现有评估指标（如弗雷歇视频距离）侧重于感知质量，却忽略了推理缺陷，包括对因果性、物理规律及全局一致性的违背。本文提出MMGR（多模态生成式推理评估与基准框架），这是一个基于五大推理能力构建的原则性评估框架：物理推理、逻辑推理、三维空间推理、二维空间推理及时序推理。MMGR在三大领域评估生成式推理能力：抽象推理（ARC-AGI、数独）、具身导航（真实三维环境导航与定位）以及物理常识（运动场景与组合交互）。该框架采用细粒度评估指标，要求视频与图像生成在整体上均具备正确性。我们对主流视频模型（Veo-3、Sora-2、Wan-2.2）和图像模型（Nano-banana、Nano-banana Pro、GPT-4o-image、Qwen-image）进行基准测试，揭示了各领域存在的显著性能差距。模型在物理常识任务中表现尚可，但在抽象推理领域表现不佳（ARC-AGI准确率低于10%），且在具身环境中的长程空间规划任务上存在明显不足。我们的分析指出了当前模型的核心局限，包括过度依赖感知数据、全局状态一致性薄弱，以及优化目标偏向视觉合理性而忽视因果正确性。MMGR提供了一个统一的诊断性基准，并为构建具备推理意识的生成式世界模型指明了发展方向。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.14691">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.14691">arXiv</a></p>
<hr />
<h3>2. 视频真实性测试：AI生成的ASMR视频能否欺骗视觉语言模型与人类？</h3>
<p><strong>原文标题：</strong> Video Reality Test: Can AI-Generated ASMR Videos fool VLMs and Humans?</p>
<p><strong>摘要：</strong>
视频生成技术的最新进展已能产出与真实视频难以区分的生动内容，这使得AI生成视频检测成为新兴的社会挑战。现有AIGC检测基准大多针对无音频视频进行评估，覆盖宽泛的叙事领域，且仅聚焦于分类任务。然而，当前最先进的视频生成模型能否制作出具有沉浸感、音画同步且能可靠欺骗人类与视觉语言模型的视频，仍不明确。为此，我们提出“视频真实性测试”——一个基于ASMR音视频源的基准测试套件，用于评估严格音画耦合下的感知真实性，其特点包括：（一）沉浸式ASMR音视频源。基于精心筛选的真实ASMR视频构建，该基准针对细粒度的动作-物体交互，涵盖多样化的物体、动作与背景。（二）同行评审式评估。采用对抗性创作者-评审者协议：视频生成模型作为试图欺骗评审者的创作者，而视觉语言模型则作为识别伪造内容的评审者。实验结果表明：最佳创作者模型Veo3.1-Fast甚至能欺骗多数视觉语言模型——最强评审模型（Gemini 2.5-Pro）仅达到56%的准确率（随机基准为50%），远低于人类专家水平（81.25%）。音频的加入提升了真伪判别能力，但水印等表面线索仍会显著误导模型。这些发现界定了当前视频生成真实性的边界，并揭示了视觉语言模型在感知保真度与音画一致性方面的局限。代码已开源：https://github.com/video-reality-test/video-reality-test。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.13281">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.13281">arXiv</a></p>
<hr />
<h3>3. WorldPlay：面向实时交互式世界建模的长期几何一致性研究</h3>
<p><strong>原文标题：</strong> WorldPlay: Towards Long-Term Geometric Consistency for Real-Time Interactive World Modeling</p>
<p><strong>摘要：</strong>
本文提出WorldPlay，一种流式视频扩散模型，能够实现具有长期几何一致性的实时交互式世界建模，解决了当前方法在速度与内存之间的权衡限制。WorldPlay的创新性主要体现在三个方面：1）采用双重动作表征技术，实现对用户键盘与鼠标输入的鲁棒动作控制；2）通过重构上下文记忆机制动态重建历史帧的上下文信息，并利用时序重构技术保持几何重要性高但时间久远的帧的可访问性，有效缓解记忆衰减问题；3）提出面向记忆感知模型的上下文强制蒸馏方法，通过对齐师生模型间的记忆上下文，保持学生模型利用长程信息的能力，在实现实时生成速度的同时避免误差漂移。综合实验表明，WorldPlay能以24帧/秒的速率生成720p长序列流式视频，在保持优异一致性的同时优于现有技术，并在多样化场景中展现出强大的泛化能力。项目页面与在线演示详见：https://3d-models.hunyuan.tencent.com/world/ 与 https://3d.hunyuan.tencent.com/sceneTo3D。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.14614">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.14614">arXiv</a></p>
<hr />
<h3>4. Scone：通过统一的理解-生成建模在主体驱动图像生成中融合构图与区分能力</h3>
<p><strong>原文标题：</strong> Scone: Bridging Composition and Distinction in Subject-Driven Image Generation via Unified Understanding-Generation Modeling</p>
<p><strong>摘要：</strong>
主体驱动图像生成已从单一主体构图发展到多主体构图，但普遍忽视了区分能力——即在输入包含多个候选主体时准确识别并生成正确主体的能力。这一局限影响了模型在复杂真实视觉场景中的有效性。我们提出Scone，一种统一的理解-生成方法，将构图与区分能力相结合。Scone通过理解专家模块充当语义桥梁，传递语义信息并引导生成专家模块在保持主体身份的同时最小化干扰。我们采用两阶段训练策略：第一阶段学习构图能力，第二阶段通过语义对齐和基于注意力的掩码机制增强区分能力。同时，我们提出了SconeEval评估基准，用于衡量模型在多样化场景中的构图与区分性能。实验表明，在两个基准测试中，Scone在构图与区分任务上均优于现有开源模型。我们的模型、评估基准及训练数据已开源：https://github.com/Ryann-Ran/Scone。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.12675">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.12675">arXiv</a></p>
<hr />
<h3>5. RoboTracer：面向机器人学的视觉语言模型空间轨迹推理技术</h3>
<p><strong>原文标题：</strong> RoboTracer: Mastering Spatial Trace with Reasoning in Vision-Language Models for Robotics</p>
<p><strong>摘要：</strong>
空间轨迹追踪作为机器人具身交互的基础能力，其实现面临本质性挑战，这要求系统在复杂空间指代与现实世界度量测量的基础上进行多步骤度量锚定推理。然而，现有方法难以应对此类组合式任务。为此，我们提出RoboTracer——一种具有三维空间感知能力的视觉语言模型，首次通过通用空间编码器与回归监督解码器，在监督微调阶段实现三维空间指代与测量能力的同步提升，从而增强模型的尺度感知能力。进一步地，RoboTracer通过引入度量敏感过程奖励的强化微调机制，推进多步骤度量锚定推理，监督关键中间感知线索以精准生成空间轨迹。为支持监督微调与强化微调训练，我们构建了包含3000万问答对的大规模数据集TraceSpatial，涵盖室外/室内/桌面场景，并支持多达9步的复杂推理流程。同时，我们提出评测基准TraceSpatial-Bench，填补了空间轨迹追踪系统性评估的空白。实验结果表明，RoboTracer在空间理解、测量与指代能力上均超越基线模型，平均成功率达79.1%，并在TraceSpatial-Bench基准上以显著优势取得最先进性能，较Gemini-2.5-Pro准确率提升36%。值得注意的是，RoboTracer可与多种控制策略集成，在杂乱的真实场景中跨机器人平台（UR5、G1人形机器人）执行长时程动态任务。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.13660">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.13660">arXiv</a></p>
<hr />
<h3>6. OpenDataArena：一个用于基准测试后训练数据集价值的公平开放平台</h3>
<p><strong>原文标题：</strong> OpenDataArena: A Fair and Open Arena for Benchmarking Post-Training Dataset Value</p>
<p><strong>摘要：</strong>
大语言模型的快速发展依赖于后训练数据集的质量与多样性。然而，一个关键矛盾持续存在：尽管模型经过严格基准测试，但支撑它们的数据却仍是一个“黑箱”——其构成不透明、来源不确定且缺乏系统性评估。这种不透明性阻碍了研究的可复现性，并模糊了数据特性与模型行为之间的因果关系。为弥合这一差距，我们提出了OpenDataArena（ODA），这是一个旨在对后训练数据内在价值进行基准测试的全面开放平台。ODA构建了一个包含四大支柱的综合生态系统：（一）统一的训练-评估流程，确保在不同模型（如Llama、Qwen）和领域间进行公平、开放的比较；（二）多维评分框架，从数十个不同维度刻画数据质量；（三）交互式数据谱系探索器，用于可视化数据集谱系并解析其组成来源；（四）完全开源的工具包，支持训练、评估与评分，以促进数据研究。基于ODA开展的大规模实验——涵盖多个领域的超过120个训练数据集、22项基准测试，并通过超过600次训练运行和4000万个已处理数据点进行验证——揭示了若干重要发现。我们的分析揭示了数据复杂性与任务性能之间的固有权衡，通过谱系追踪识别了流行基准中的冗余，并绘制了数据集间的谱系关系图。我们公开所有结果、工具与配置，以推动高质量数据评估的普及。ODA不仅旨在扩展排行榜，更期望推动从试错式的数据整理转向以数据为中心的人工智能的规范科学，从而为数据混合规律与基础模型战略构建的严谨研究铺平道路。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.14051">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.14051">arXiv</a></p>
<hr />
<h3>7. 矢量棱镜：通过分层语义结构实现矢量图形动画化</h3>
<p><strong>原文标题：</strong> Vector Prism: Animating Vector Graphics by Stratifying Semantic Structure</p>
<p><strong>摘要：</strong>
可缩放矢量图形（SVG）是现代网页设计的核心要素，随着网络环境日益动态化，对其动画化的需求持续增长。尽管代码生成与运动规划领域近期取得进展，但视觉语言模型在自动化生成矢量图形动画方面仍面临挑战。视觉语言模型常错误处理SVG文件，因为视觉连贯的部件常被拆解为底层图形元素，这些元素难以提示哪些组件应协同运动。本文提出一种框架，通过恢复SVG动画化所需的语义结构，揭示当前视觉语言模型系统所忽略的关键层面。该框架通过对多个弱部件预测结果进行统计聚合，使系统能够从噪声预测中稳定推断语义信息。通过将SVG重组为语义群组，本方法使视觉语言模型能够生成连贯性显著提升的动画。实验结果表明，相较于现有方法，本框架取得实质性突破，证明语义重建是实现稳健SVG动画化的关键步骤，并为视觉语言模型与矢量图形之间建立更具可解释性的交互机制提供支持。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.14336">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.14336">arXiv</a></p>
<hr />
<h3>8. 从任务中心视角揭示向量相似性搜索的潜在缺陷并引领下一代发展方向</h3>
<p><strong>原文标题：</strong> Reveal Hidden Pitfalls and Navigate Next Generation of Vector Similarity Search from Task-Centric Views</p>
<p><strong>摘要：</strong>
高维空间中的向量相似性搜索正迅速成为新一代数据库系统的核心功能，支撑着从大语言模型中的嵌入检索到语义信息检索与推荐引擎等众多数据密集型服务。然而，现有基准测试主要基于召回率-延迟权衡来评估向量相似性搜索方法，其真值仅由距离度量定义，忽略了检索质量如何最终影响下游任务。这种脱节可能误导学术研究与工业实践。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.12980">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.12980">arXiv</a></p>
<hr />
<h3>9. MemFlow：用于一致高效长视频叙事的自适应流动记忆</h3>
<p><strong>原文标题：</strong> MemFlow: Flowing Adaptive Memory for Consistent and Efficient Long Video Narratives</p>
<p><strong>摘要：</strong>
流式视频生成的核心挑战在于保持长上下文中的内容一致性，这对记忆设计提出了较高要求。现有方案大多通过预定义策略压缩历史帧来维护记忆。然而，不同待生成视频片段应参考不同的历史线索，固定策略难以满足这一需求。本研究提出MemFlow以解决该问题：在生成新片段前，我们通过当前片段文本提示检索最相关的历史帧，动态更新记忆库。该设计即使后续帧出现新事件或场景切换，仍能保持叙事连贯性。此外，在生成过程中，我们仅针对注意力层中的每个查询激活记忆库中最相关的标记，有效保障了生成效率。MemFlow由此以可忽略的计算负担（相比无记忆基线仅降低7.9%速度）实现了卓越的长上下文一致性，并保持了对所有支持KV缓存的流式视频生成模型的兼容性。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.14699">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.14699">arXiv</a></p>
<hr />
<h3>10. RecGPT-V2技术报告</h3>
<p><strong>原文标题：</strong> RecGPT-V2 Technical Report</p>
<p><strong>摘要：</strong>
大语言模型（LLMs）在将推荐系统从隐式行为模式匹配转变为显式意图推理方面展现出显著潜力。尽管RecGPT-V1通过将基于LLM的推理融入用户兴趣挖掘与物品标签预测，成功开创了这一范式，但其存在四个根本性局限：（1）多推理路径的计算效率低下与认知冗余；（2）固定模板生成中解释多样性的不足；（3）监督学习范式下泛化能力有限；（4）结果导向的评估方式过于简化，难以匹配人类标准。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.14503">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.14503">arXiv</a></p>
<hr />
<h3>11. ShowTable：通过协作反思与精炼解锁创意表格可视化</h3>
<p><strong>原文标题：</strong> ShowTable: Unlocking Creative Table Visualization with Collaborative Reflection and Refinement</p>
<p><strong>摘要：</strong>
尽管现有的生成模型和统一模型在通用图像生成方面表现出色，但在需要超越一般场景的深度推理、规划以及精确数据到视觉映射能力的任务上仍存在不足。为了突破现有局限，我们引入了一项新颖且具有挑战性的任务：创意表格可视化，要求模型根据给定表格数据生成既忠实于原始信息又具备美学效果的信息图表。为应对这一挑战，我们提出了ShowTable，一个通过渐进式自我校正过程将多模态大语言模型与扩散模型协同结合的流程。该流程以多模态大语言模型为核心协调器，负责推理视觉规划并判断视觉误差以提供精细化指令，扩散模型则执行这些指令，从而实现高保真度的可视化结果。为支持此任务及流程，我们设计了三种自动化数据构建流程，用于训练不同模块。此外，我们引入了TableVisBench——一个包含800个跨5个评估维度的挑战性实例的新基准，以系统评估该任务的性能。实验表明，基于不同模型实例化的流程均显著优于基线方法，突显了其在多模态推理、生成及错误校正方面的有效能力。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.13303">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.13303">arXiv</a></p>
<hr />
<h3>12. 基于文本可导向图像到三维的前馈式编辑方法</h3>
<p><strong>原文标题：</strong> Feedforward 3D Editing via Text-Steerable Image-to-3D</p>
<p><strong>摘要：</strong>
图像到三维生成技术的最新进展为设计、增强现实/虚拟现实及机器人领域开辟了广阔前景。然而，要在实际应用中有效使用人工智能生成的三维资产，关键需求在于能够便捷地对其进行编辑。本文提出一种前馈式方法Steer3D，通过为图像到三维模型增加文本导向能力，实现用语言指令编辑生成的三维资产。该方法受ControlNet启发，将其适配至图像到三维生成任务，从而在单次前向传播中实现直接文本导向。我们构建了可扩展的自动数据生成引擎，并开发了基于流匹配训练与直接偏好优化的两阶段训练方案。与现有方法相比，Steer3D能更精准地遵循语言指令，同时保持与原始三维资产更好的一致性，且处理速度提升2.4至28.5倍。实验表明，仅需十万量级数据即可为预训练的图像到三维生成模型增加文本模态的导向能力。项目网站：https://glab-caltech.github.io/steer3d/</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.13678">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.13678">arXiv</a></p>
<hr />
<h3>13. Nemotron-Cascade：面向通用推理模型的级联强化学习规模化方法</h3>
<p><strong>原文标题：</strong> Nemotron-Cascade: Scaling Cascaded Reinforcement Learning for General-Purpose Reasoning Models</p>
<p><strong>摘要：</strong>
利用强化学习构建通用推理模型面临显著的跨领域异质性挑战，包括推理时响应长度与验证延迟的巨大差异。这种差异性使得强化学习基础设施复杂化，拖慢训练进程，并为训练课程设计（如响应长度扩展）和超参数选择带来困难。本研究提出级联分域强化学习方法，用以开发具备指令执行与深度思考双模式的通用推理模型Nemotron-Cascade。相较于传统混合多领域异构提示的方法，级联强化学习通过按领域顺序执行强化学习来降低工程复杂度，在广泛基准测试中实现了最先进的性能。值得注意的是，作为前置步骤的对齐性RLHF能大幅提升模型推理能力，其效果远超单纯的偏好优化；后续分域RLVR阶段几乎不会降低模型在已训练领域取得的基准性能，甚至可能带来提升（如图1示例所示）。经过强化学习的140亿参数模型在LiveCodeBench v5/v6/Pro基准上超越其监督微调教师模型DeepSeek-R1-0528，并在2025年国际信息学奥林匹克竞赛中达到银牌水平。我们公开分享了完整的训练方案与数据构建方法。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.13607">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.13607">arXiv</a></p>
<hr />
<h3>14. Olmo 3</h3>
<p><strong>原文标题：</strong> Olmo 3</p>
<p><strong>摘要：</strong>
我们推出Olmo 3系列模型，这是一组参数规模分别为70亿和320亿的顶尖全开源语言模型。Olmo 3模型的构建目标涵盖长上下文推理、函数调用、代码生成、指令遵循、通用对话及知识检索。本次发布完整公开了模型构建全流程，即该系列模型从数据采集、训练检查点到依赖组件的完整生命周期。我们的旗舰模型Olmo 3 Think 32B，是迄今为止发布的最强全开源思维推理模型。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.13961">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.13961">arXiv</a></p>
<hr />
<h3>15. 可微分进化强化学习</h3>
<p><strong>原文标题：</strong> Differentiable Evolutionary Reinforcement Learning</p>
<p><strong>摘要：</strong>
在强化学习（RL）中，设计有效的奖励函数是一个核心且往往艰巨的挑战，尤其是在为复杂推理任务开发自主智能体时。尽管存在自动化的奖励优化方法，但它们通常依赖于将奖励函数视为黑盒的无导数进化启发式算法，未能捕捉奖励结构与任务性能之间的因果关系。为弥合这一差距，我们提出了可微分进化强化学习（DERL），这是一个双层框架，能够自主发现最优奖励信号。在DERL中，元优化器通过组合结构化的原子基元来进化奖励函数（即元奖励），从而指导内层策略的训练。关键的是，与以往的进化方法不同，DERL在其元优化过程中是可微分的：它将内层验证性能视为信号，通过强化学习来更新元优化器。这使得DERL能够近似任务成功的“元梯度”，逐步学习生成更密集、更具可操作性的反馈。我们在三个不同领域验证了DERL：机器人智能体（ALFWorld）、科学模拟（ScienceWorld）和数学推理（GSM8k, MATH）。实验结果表明，DERL在ALFWorld和ScienceWorld上达到了最先进的性能，显著优于依赖启发式奖励的方法，尤其是在分布外场景中。对进化轨迹的分析表明，DERL成功捕捉了任务的内在结构，实现了无需人工干预的自我改进智能体对齐。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.13399">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.13399">arXiv</a></p>
<hr />
<h3>16. VersatileFFN：通过自适应宽深复用实现大语言模型的参数高效性</h3>
<p><strong>原文标题：</strong> VersatileFFN: Achieving Parameter Efficiency in LLMs via Adaptive Wide-and-Deep Reuse</p>
<p><strong>摘要：</strong>
大语言模型（LLMs）的快速扩展虽取得了显著性能，但也导致了高昂的内存成本。现有的参数高效方法（如剪枝和量化）主要对预训练模型进行压缩，并未增强其架构能力，因此受限于基础模型的表示上限。本文提出VersatileFFN，一种新颖的前馈网络（FFN），能够在固定参数预算下灵活复用宽度和深度维度的参数。受认知双过程理论启发，VersatileFFN包含两条自适应路径：一条是宽度自适应路径，通过单一共享FFN生成混合子专家，在不增加参数的情况下模拟稀疏专家路由；另一条是深度自适应路径，递归应用同一FFN以模拟对复杂令牌的更深层处理。一个难度感知门控机制动态平衡两条路径，引导“简单”令牌通过高效的宽度路径，并为“困难”令牌分配更深层的迭代细化处理。关键的是，两条路径复用相同参数，因此所有额外能力均来自计算而非内存开销。在不同基准测试和模型规模上的实验验证了该方法的有效性。代码发布于https://github.com/huawei-noah/noah-research/tree/master/VersatileFFN。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.14531">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.14531">arXiv</a></p>
<hr />
<h3>17. A4-Agent：一种用于零样本可供性推理的智能体框架</h3>
<p><strong>原文标题：</strong> A4-Agent: An Agentic Framework for Zero-Shot Affordance Reasoning</p>
<p><strong>摘要：</strong>
可供性预测旨在根据语言指令识别物体上的交互区域，对于具身人工智能至关重要。当前主流的端到端模型将高层推理与低层定位耦合在单一整体流程中，并依赖于对标注数据集的训练，这导致其在新颖物体和未见环境中的泛化能力较差。本文超越这一范式，提出了A4-Agent——一种免训练的智能体框架，将可供性预测解耦为一个三阶段流程。该框架在测试时协调多个专用基础模型：(1) “梦想家”，利用生成模型可视化交互过程；(2) “思考者”，运用大型视觉语言模型确定需交互的物体部件；(3) “定位者”，协调视觉基础模型精确定位交互区域。通过无需任何任务特定微调即可利用预训练模型的互补优势，我们的零样本框架在多个基准测试中显著优于当前最先进的监督方法，并展现出对真实场景的强健泛化能力。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.14442">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.14442">arXiv</a></p>
<hr />
<h3>18. SS4D：基于结构化时空隐变量的原生4D生成模型</h3>
<p><strong>原文标题：</strong> SS4D: Native 4D Generative Model via Structured Spacetime Latents</p>
<p><strong>摘要：</strong>
本文提出SS4D，一种原生4D生成模型，能够直接从单目视频合成动态3D物体。与现有通过优化3D或视频生成模型构建4D表示的方法不同，我们直接在4D数据上训练生成器，实现了高保真度、时间连贯性和结构一致性。本方法的核心是一组压缩的结构化时空隐变量。具体而言：（1）针对4D训练数据稀缺的问题，我们基于预训练的单图像转3D模型构建框架，保持强大的空间一致性；（2）通过引入跨帧推理的专用时序层来保证时间连贯性；（3）为支持长视频序列的高效训练与推理，我们采用因子化4D卷积和时序下采样模块沿时间轴压缩隐变量序列。此外，我们设计了精心优化的训练策略，以增强模型对遮挡场景的鲁棒性。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.14284">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.14284">arXiv</a></p>
<hr />
<h3>19. Sparse-LaViDa：稀疏多模态离散扩散语言模型</h3>
<p><strong>原文标题：</strong> Sparse-LaViDa: Sparse Multimodal Discrete Diffusion Language Models</p>
<p><strong>摘要：</strong>
掩码离散扩散模型（MDMs）在图像理解、生成与编辑等多模态任务中已展现出卓越性能。然而，由于每个采样步骤均需重复处理冗余的掩码标记，其推理速度仍存在优化空间。本研究提出Sparse-LaViDa——一种新颖的建模框架，通过在每一步推理中动态截断非必要的掩码标记来加速MDM采样。为保持生成质量，我们引入专用寄存器标记作为被截断标记的紧凑表征。此外，为确保训练与推理的一致性，我们设计了特殊的注意力掩码机制，使训练过程能精确模拟截断采样流程。基于当前最先进的统一MDM框架LaViDa-O构建的Sparse-LaViDa，在文本到图像生成、图像编辑和数学推理等多样化任务中实现了最高达2倍的加速，同时保持了原有的生成质量。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.14008">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.14008">arXiv</a></p>
<hr />
<h3>20. 用于视觉标记化与生成的球面Leech量化</h3>
<p><strong>原文标题：</strong> Spherical Leech Quantization for Visual Tokenization and Generation</p>
<p><strong>摘要：</strong>
非参数量化因其参数高效性和对大码本的良好可扩展性而备受关注。本文通过格编码的视角，提出了不同非参数量化方法的统一表述。格编码的几何特性解释了在训练自编码器时，为何需要为某些现有的免查表量化变体（如BSQ）引入辅助损失项。在此基础上，我们探索了若干可能的候选方案，包括随机格、广义斐波那契格以及最密球堆积格。研究发现，基于Leech格的量化方法（称为球面Leech量化，Λ_{24}-SQ）凭借其高对称性和超球面上的均匀分布特性，既能简化训练流程，又能改善重建与压缩的权衡关系。在图像标记化与压缩任务中，该方法在所有评估指标上均优于当前最佳基准BSQ，同时消耗更少的比特数。该改进效果同样适用于当前最先进的自回归图像生成框架。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.14697">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.14697">arXiv</a></p>
<hr />
<h3>21. CRISP：基于平面场景基元的单目视频接触引导式实景转仿真方法</h3>
<p><strong>原文标题：</strong> CRISP: Contact-Guided Real2Sim from Monocular Video with Planar Scene Primitives</p>
<p><strong>摘要：</strong>
本文提出CRISP方法，该技术能够从单目视频中重建可仿真的运动人体与场景几何结构。现有的人-场景联合重建研究主要依赖数据驱动的先验知识以及缺乏物理约束的联合优化方法，或重建出存在噪声与伪影的几何结构，导致包含场景交互的运动追踪策略失效。与此不同，我们的核心思路是通过对场景点云进行深度、法向量和光流的多维度聚类，拟合平面几何基元，从而重建出凸面化、洁净且可直接用于仿真的几何结构。为重建在交互过程中可能被遮挡的场景几何，我们利用人-场景接触建模技术（例如通过人体姿态重建被遮挡的椅面）。最后，我们通过强化学习驱动的人形控制器验证人与场景重建结果的物理合理性。在以人为本的视频基准测试（EMDB、PROX）中，本方法将运动追踪失败率从55.2%降低至6.9%，同时使强化学习仿真吞吐量提升43%。我们进一步在真实场景视频（包括日常拍摄视频、网络视频乃至Sora生成视频）中验证了方法的有效性。实验表明CRISP能够大规模生成物理有效的人体运动与交互环境，显著推动了机器人及AR/VR领域的实景转仿真应用发展。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.14696">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.14696">arXiv</a></p>
<hr />
<h3>22. TimeLens：基于多模态大语言模型重新思考视频时序定位</h3>
<p><strong>原文标题：</strong> TimeLens: Rethinking Video Temporal Grounding with Multimodal LLMs</p>
<p><strong>摘要：</strong>
本文并未提出一种新方法，而是为视频理解的核心能力——视频时序定位（VTG）建立了一个直接、渐进但至关重要的基线。尽管多模态大语言模型（MLLMs）在多种视频理解任务中表现出色，但针对VTG任务优化这些模型的方案仍待深入探索。本文提出TimeLens，围绕数据质量和算法设计两个主要维度，系统性地研究了如何构建具有强大VTG能力的MLLMs。我们首先揭示了现有VTG基准数据集中存在的关键质量问题，并引入了TimeLens-Bench——该基准包含三个经过严格质量标准重新精细标注的流行数据集版本。分析表明，与传统基准相比，模型评估排名发生了显著变化，证实了先前评估标准的不可靠性。同时，我们通过自动化重标注流程处理噪声训练数据，构建了TimeLens-100K这一大规模高质量训练数据集。基于数据基础，我们深入探索了算法设计原则，获得了一系列有意义的洞见及高效可行的实践方案，包括：用于时间表征的交错文本编码、采用可验证奖励的免思考强化学习（RLVR）作为训练范式，以及精心设计的RLVR训练方案。这些努力最终催生了TimeLens模型系列——该系列MLLMs在开源模型中实现了最先进的VTG性能，甚至超越了GPT-5和Gemini-2.5-Flash等专有模型。所有代码、数据及模型将公开发布以促进后续研究。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.14698">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.14698">arXiv</a></p>
<hr />
<h3>23. EVOLVE-VLA：基于环境反馈的视觉-语言-动作模型测试时训练框架</h3>
<p><strong>原文标题：</strong> EVOLVE-VLA: Test-Time Training from Environment Feedback for Vision-Language-Action Models</p>
<p><strong>摘要：</strong>
实现真正自适应的具身智能需要智能体不仅通过模仿静态演示来学习，更要通过持续的环境交互进行改进，这类似于人类通过实践掌握技能的方式。视觉-语言-动作模型通过利用大语言模型推动了机器人操作的发展，但其根本上仍受限于监督微调范式：每个任务需要数百次演示、僵化地记忆轨迹，且在部署条件偏离训练时无法适应。本文提出EVOLVE-VLA，一种测试时训练框架，使视觉-语言-动作模型能够通过环境交互持续适应，仅需极少甚至无需任务特定演示。其核心技术挑战在于用自主反馈替代测试时无法获取的预设奖励信号。我们通过设计可学习的进度估计器提供密集反馈来解决此问题，并特别通过两种机制“驯服”这种固有噪声信号：（1）累积进度估计机制平滑噪声点估计；（2）渐进式规划范围扩展策略实现策略逐步演化。EVOLVE-VLA取得显著性能提升：长时序任务提升8.6%，单样本学习提升22.0%，并实现跨任务泛化——在未见任务上无需任务特定演示训练即可达到20.8%的成功率（纯监督微调方法为0%）。定性分析揭示了演示中未出现的新兴能力，包括错误恢复和新策略生成。这项工作标志着视觉-语言-动作模型向真正学习与适应迈出关键一步，从静态模仿转向持续自我改进。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.14666">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.14666">arXiv</a></p>
<hr />
<h3>24. TAT：面向一体化医学图像复原的任务自适应Transformer</h3>
<p><strong>原文标题：</strong> TAT: Task-Adaptive Transformer for All-in-One Medical Image Restoration</p>
<p><strong>摘要：</strong>
医学图像复原（MedIR）旨在从低质量医学图像中恢复出高质量图像。当前MedIR领域的研究进展主要集中于能够同时处理多种不同复原任务的一体化模型。然而，由于模态类型与退化类型均存在显著差异，使用共享模型处理这些多样化任务时，必须审慎考虑两种关键的任务间关系：一是任务干扰，即不同任务对同一参数产生冲突的梯度更新方向；二是任务失衡，即各任务固有学习难度差异导致的不均衡优化问题。为应对这些挑战，我们提出了一种任务自适应Transformer（TAT）框架，该创新框架通过两项关键技术实现动态任务适应。首先，我们设计了任务自适应权重生成策略，通过为每个任务生成特定的权重参数，消除共享权重参数上潜在的梯度冲突，从而缓解任务干扰。其次，我们提出了任务自适应损失平衡策略，根据任务特定学习难度动态调整损失权重，防止某些任务主导训练或训练不足。大量实验表明，我们提出的TAT模型在PET合成、CT去噪和MRI超分辨率三项MedIR任务中，无论是针对特定任务还是一体化设置，均取得了最先进的性能。代码已开源：https://github.com/Yaziwel/TAT。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.14550">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.14550">arXiv</a></p>
<hr />
<h3>25. Zoom-Zero：通过时序局部放大实现从粗到细的强化视频理解</h3>
<p><strong>原文标题：</strong> Zoom-Zero: Reinforced Coarse-to-Fine Video Understanding via Temporal Zoom-in</p>
<p><strong>摘要：</strong>
基于视频的问答任务旨在定位视频中的相关时序片段并针对给定问题生成准确答案；然而，现有的大规模视频语言模型在时序感知能力上存在局限。尽管基于群体相对策略优化的方法尝试提升时序定位性能，但其答案仍难以忠实于视频证据，导致时序定位错误与事实幻觉。本研究提出Zoom-Zero框架，采用从粗到细的处理流程：首先定位查询相关的视频片段，继而通过时序局部放大机制聚焦于关键帧进行细粒度视觉验证。本方法通过两项关键创新突破群体相对策略优化在基于视频的问答任务中的局限：（1）引入局部放大精度奖励机制，用于验证时序定位预测的可靠性，并促进对定位帧的细粒度视觉验证；（2）提出基于令牌的选择性奖励分配策略，将奖励精准关联至负责时序定位或答案生成的关键令牌，从而缓解群体相对策略优化在处理多维度奖励信号时的不足。实验表明，所提方法显著提升了基于视频的问答性能：在NExT-GQA数据集上时序定位精度提升5.2%，在ReXTime数据集上提升4.6%，同时平均答案准确率提高2.4%。此外，推理过程中采用的从粗到细局部放大机制，能在保持全局上下文的同时保留关键视觉细节，使长视频理解任务在基准测试中获得6.4%的平均性能提升。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.14273">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.14273">arXiv</a></p>
<hr />
<h3>26. Efficient-DLM：从自回归到扩散语言模型，以及更快的速度</h3>
<p><strong>原文标题：</strong> Efficient-DLM: From Autoregressive to Diffusion Language Models, and Beyond in Speed</p>
<p><strong>摘要：</strong>
扩散语言模型（dLM）作为一种支持并行、非自回归生成的新兴范式展现出巨大潜力，但其从零开始训练时的学习效率仍落后于自回归（AR）语言模型。为此，我们研究AR到dLM的转换方法，旨在将预训练的AR模型转化为高效的dLM，在保持AR模型任务精度的同时显著提升生成速度。我们通过分析现有AR-to-dLM方法在注意力模式与训练目标上的局限性，提出了更有效的转换原则与方法论。具体而言，我们首先系统比较了不同注意力模式，发现保持预训练AR模型的权重分布对有效转换至关重要。因此，我们提出一种采用分块注意力模式的持续预训练方案：该方案在块间保持因果性，同时在块内实现双向建模。我们发现，相较于完全双向建模，此方法不仅能更好地保留预训练AR模型的权重分布，还具备已知的KV缓存优势，从而在精度与效率上实现双赢。其次，为缓解掩码词分布（训练时的均匀分布与推理时的高度从左到右分布）之间的训练-测试差距，我们提出一种位置相关的词掩码策略：在训练中对靠后词元赋予更高掩码概率，以更好地模拟测试阶段的行为。基于此框架，我们对dLM的注意力模式、训练动态及其他设计选择进行了广泛研究，为可扩展的AR-to-dLM转换提供了实用见解。这些研究最终形成了Efficient-DLM系列模型，其性能超越当前最先进的AR模型与dLM。例如，我们的Efficient-DLM 8B模型相较于Dream 7B和Qwen3 4B，在准确率上分别提升5.4%和2.7%，同时吞吐量提高至4.5倍和2.7倍。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.14067">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.14067">arXiv</a></p>
<hr />
<h3>27. Janus：面向可扩展MoE推理的注意力与专家模块解耦系统</h3>
<p><strong>原文标题：</strong> Janus: Disaggregating Attention and Experts for Scalable MoE Inference</p>
<p><strong>摘要：</strong>
大型专家混合模型（MoE）的推理因资源需求高且工作负载动态变化而面临挑战。现有解决方案通常将整个模型部署为单一整体单元，对注意力模块和专家模块采用统一的资源配置，忽视了二者不同的需求，导致可扩展性有限和资源效率低下。本文提出Janus——一个可扩展的MoE推理系统，通过将注意力模块与专家模块解耦至独立的GPU子集群，实现各模块的独立管理与弹性扩展。Janus包含三项关键设计以实现高效解耦的MoE推理：首先，提出自适应两阶段通信机制，利用节点内与节点间的带宽层级进行低延迟数据交换；其次，针对MoE模块的内存受限特性，设计轻量级调度器并以GPU内核形式实现，以最小开销平衡跨GPU的激活专家数量，从而降低推理延迟；最后，实施细粒度资源管理，动态调整专家分布并独立扩展注意力与MoE资源，提升整体效率。实验表明，Janus在满足单令牌延迟要求的同时，可实现比现有先进系统最高3.9倍的每GPU吞吐量提升。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.13525">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.13525">arXiv</a></p>
<hr />
<h3>28. RePo：基于上下文重定位的语言模型</h3>
<p><strong>原文标题：</strong> RePo: Language Models with Context Re-Positioning</p>
<p><strong>摘要：</strong>
上下文学习是现代大语言模型（LLM）的核心能力，然而主流架构通过分配线性或固定位置索引，强加了一种僵化且固定的上下文结构。基于认知负荷理论（CLT），我们认为这种缺乏信息量的结构会增加外部认知负荷，消耗本应用于深度推理与注意力分配的有限工作记忆容量。为解决此问题，我们提出RePo，一种通过上下文重定位来降低外部负荷的新机制。与标准方法不同，RePo采用可微分模块 f_φ 来分配能够捕捉上下文依赖关系的词元位置，而非依赖预定义的整数范围。通过在OLMo-2 1B骨干模型上进行持续预训练，我们证明RePo在处理含噪声上下文、结构化数据及长上下文任务时性能显著提升，同时在通用短上下文任务上保持竞争力。详细分析表明，RePo能成功为遥远但相关的信息分配更高注意力，在稠密非线性空间中分配位置，并有效捕捉输入上下文的内在结构。代码已开源：https://github.com/SakanaAI/repo。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.14391">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.14391">arXiv</a></p>
<hr />
<h3>29. JMMMU-Pro：基于图像的日本多学科多模态理解基准及其Vibe基准构建方法</h3>
<p><strong>原文标题：</strong> JMMMU-Pro: Image-based Japanese Multi-discipline Multimodal Understanding Benchmark via Vibe Benchmark Construction</p>
<p><strong>摘要：</strong>
本文提出了JMMMU-Pro——一个基于图像的日本多学科多模态理解基准，以及Vibe Benchmark Construction——一种可扩展的构建方法。遵循从MMMU到MMMU-Pro的演进路径，JMMMU-Pro在JMMMU基础上将问题图像与问题文本整合为单一图像，从而构建出需要通过视觉感知实现图文融合理解的评测基准。为构建JMMMU-Pro，我们提出Vibe Benchmark Construction方法：通过图像生成模型（如Nano Banana Pro）生成候选视觉问题，由人工验证输出结果并在必要时调整提示词重新生成，以确保数据质量。借助Nano Banana Pro高度逼真的图像生成能力及其嵌入清晰日文文本的特性，我们以较低成本构建了涵盖多样化背景与版式设计的高质量基准。实验结果表明，所有开源大型多模态模型在JMMMU-Pro上都面临显著挑战，这凸显了JMMMU-Pro作为指导开源社区未来发展的重要基准价值。我们相信JMMMU-Pro为评估大型多模态模型的日语能力提供了更严格的评测工具，同时Vibe Benchmark Construction方法也为未来基于图像的视觉问答基准开发提供了高效的建设指南。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.14620">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.14620">arXiv</a></p>
<hr />
<h3>30. MobileWorldBench：面向移动智能体的语义世界建模</h3>
<p><strong>原文标题：</strong> MobileWorldBench: Towards Semantic World Modeling For Mobile Agents</p>
<p><strong>摘要：</strong>
世界模型在提升具身智能体任务性能方面展现出巨大潜力。现有研究主要集中于像素空间世界模型，但此类方法在图形用户界面（GUI）场景中存在实际局限性——预测未来状态中的复杂视觉元素往往较为困难。本研究探索了适用于GUI智能体的世界建模新范式，即通过自然语言而非原始像素预测来描述状态转移。首先，我们提出MobileWorldBench基准测试，用于评估视觉语言模型（VLMs）作为移动GUI智能体世界模型的性能表现。其次，我们发布了包含140万样本的大规模数据集MobileWorld，该数据集显著提升了VLMs的世界建模能力。最后，我们提出一种创新框架，将VLM世界模型集成到移动智能体的规划框架中，证明语义世界模型可通过提高任务成功率直接赋能移动智能体。代码与数据集已发布于https://github.com/jacklishufan/MobileWorld</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.14014">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.14014">arXiv</a></p>
<hr />
<h3>31. 大语言模型能力消除方法比较分析：跨架构评估</h3>
<p><strong>原文标题：</strong> Comparative Analysis of LLM Abliteration Methods: A Cross-Architecture Evaluation</p>
<p><strong>摘要：</strong>
大语言模型中的安全对齐机制通过习得的拒绝行为防止对有害查询作出响应，但这些机制同时阻碍了包括认知建模、对抗测试与安全分析在内的合法研究应用。尽管能力消除技术能够通过定向正交化实现拒绝表征的精准移除，现有实施方案的相对有效性尚未得到系统评估。本研究在十六个指令微调模型（7B-140亿参数）上评估四种能力消除工具（Heretic、DECCP、ErisForge、FailSpy），报告了全部16个模型的工具兼容性及工具支持子集的量化指标。在基准测试子集中，单次消除方法展现出更优的能力保持性（三个模型的GSM8K平均变化：ErisForge -0.28个百分点；DECCP -0.13个百分点），而贝叶斯优化消除则产生可变的分布偏移（KL散度：0.043-1.646）及模型依赖的能力影响。这些发现为研究人员提供了跨模型架构部署能力消除工具的实证选择依据。核心结果表明，数学推理能力对消除干预表现出最高敏感性，GSM8K分数变化范围达+1.51至-18.81个百分点（相对变化-26.5%），具体取决于工具选择与模型架构。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.13655">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.13655">arXiv</a></p>
<hr />
<h3>32. TraPO：一种提升大语言模型推理能力的半监督强化学习框架</h3>
<p><strong>原文标题：</strong> TraPO: A Semi-Supervised Reinforcement Learning Framework for Boosting LLM Reasoning</p>
<p><strong>摘要：</strong>
基于可验证奖励的强化学习（RLVR）通过利用答案可验证信号来指导策略优化，已被证明在训练大型推理模型（LRMs）方面具有良好效果，但该方法存在标注成本高昂的问题。为缓解此问题，近期研究探索了仅从模型内部一致性（如通过熵和多数投票）推导奖励的无监督RLVR方法。尽管这些方法看似前景良好，但其在训练后期常出现模型崩溃现象，这可能源于缺乏外部监督时错误推理模式被强化。本研究提出一种新颖的半监督RLVR范式，利用少量标注样本指导未标注样本的RLVR训练。我们的核心洞见是：监督奖励对于稳定基于一致性的未标注样本训练至关重要，可确保仅将在标注实例上验证过的推理模式纳入强化学习训练。技术上，我们提出一种有效的策略优化算法TraPO，通过匹配未标注样本与标注样本的学习轨迹相似性来识别可靠的未标注样本。在此基础上，TraPO在六个广泛使用的数学推理基准测试（AIME24/25、AMC、MATH-500、Minerva和Olympiad）和三个分布外任务（ARC-c、GPQA-diamond和MMLU-pro）上实现了显著的数据效率和强大泛化能力。仅使用1K标注样本和3K未标注样本，TraPO即达到42.6%的平均准确率，超越在45K未标注样本上训练的最佳无监督方法（38.3%）。值得注意的是，当使用4K标注样本和12K未标注样本时，TraPO在所有基准测试中甚至优于使用全部45K标注样本训练的完全监督模型，而标注数据使用量仅为其10%。代码可通过https://github.com/ShenzhiYang2000/TRAPO获取。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.13106">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.13106">arXiv</a></p>
<hr />
<h3>33. UAGLNet：基于CNN-Transformer协同机制与不确定性聚合的全局-局部融合网络建筑物提取方法</h3>
<p><strong>原文标题：</strong> UAGLNet: Uncertainty-Aggregated Global-Local Fusion Network with Cooperative CNN-Transformer for Building Extraction</p>
<p><strong>摘要：</strong>
遥感影像中的建筑物提取因目标结构复杂多变而极具挑战性。现有方法通常在分割模型中采用卷积或自注意力模块以捕获多尺度特征，但特征金字塔间的固有差异以及全局与局部特征融合不充分等问题，常导致提取结果存在误差与模糊性。为此，本文提出一种不确定性聚合的全局-局部融合网络（UAGLNet），该网络能够在不确定性建模的引导下有效利用高质量的全局与局部视觉语义特征。具体而言，我们设计了一种协同编码器，在不同层级分别采用混合CNN与Transformer模块以分别捕获局部与全局视觉语义特征，并通过引入中间协同交互模块（CIB）以缓解网络加深时局部与全局特征间的表征差异。进一步，我们提出全局-局部融合模块（GLF），以互补方式融合全局与局部特征表达。此外，为降低不确定区域的分割模糊性，本文构建了不确定性聚合解码器（UAD），通过显式估计像素级不确定性以提升分割精度。大量实验表明，本方法在多个数据集上均优于现有先进方法。代码已开源：https://github.com/Dstate/UAGLNet</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.12941">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.12941">arXiv</a></p>
<hr />
<h3>34. S2D：基于稀疏到稠密关键掩码蒸馏的无监督视频实例分割方法</h3>
<p><strong>原文标题：</strong> S2D: Sparse-To-Dense Keymask Distillation for Unsupervised Video Instance Segmentation</p>
<p><strong>摘要：</strong>
近年来，无监督视频实例分割领域的最先进方法严重依赖从以物体为中心的图像数据集（如ImageNet）生成的合成视频数据。然而，通过人工平移和缩放图像实例掩码生成的视频数据，难以准确模拟真实视频中的复杂运动模式，例如视角变化、单个或多个实例部件的独立运动或相机运动。为解决这一问题，我们提出了一种完全基于真实视频数据训练的无监督视频实例分割模型。该方法从单帧视频的无监督实例分割掩码出发，但这些单帧分割结果存在时序噪声，且其质量在视频序列中波动显著。为此，我们通过利用深度运动先验知识识别视频中的高质量关键掩码，从而建立时序一致性。这些稀疏的关键掩码伪标注随后用于训练一个隐式掩码传播的分割模型，为此我们提出了一种结合时序丢弃损失函数的稀疏到稠密蒸馏方法。在使用生成的稠密标签集训练最终模型后，本方法在多个基准测试中均超越了当前最先进的技术水平。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.14440">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.14440">arXiv</a></p>
<hr />
<h3>35. 生成式AI时代用户感知的揭示：基于情感分析的AI教育应用在数字教学转型中的作用评估</h3>
<p><strong>原文标题：</strong> Unveiling User Perceptions in the Generative AI Era: A Sentiment-Driven Evaluation of AI Educational Apps' Role in Digital Transformation of e-Teaching</p>
<p><strong>摘要：</strong>
生成式人工智能在教育领域的快速融合推动了数字教学的转型，然而用户对AI教育应用的感知仍待深入探究。本研究通过对Google Play商店热门AI教育应用的用户评论进行情感驱动评估，以考察其效能、挑战及教学意义。研究流程包括爬取应用数据与评论、使用RoBERTa进行二元情感分类、GPT-4o提取关键观点，以及GPT-5合成核心积极/消极主题。应用被分为七类（如作业助手、数学解题工具、语言学习工具等），功能重叠反映了多功能设计趋势。结果显示用户情感以积极为主，其中作业类应用（如Edu AI积极率95.9%、Answer.AI积极率92.7%）在准确性、响应速度与个性化方面表现突出，而语言学习/学习管理系统类应用（如Teacher AI积极率仅21.8%）因系统不稳定和功能有限而落后。积极评价聚焦于头脑风暴、问题解决和学习参与度的效率提升；消极评价则集中于付费门槛、内容错误、广告干扰和技术故障。趋势表明，作业助手类应用优于专业化工具，凸显了AI在促进教育普惠的同时也带来依赖性与公平性风险。讨论部分提出未来生态系统的构建方向：人机协同混合模型、VR/AR沉浸式学习技术，并为开发者（适应性个性化设计）和政策制定者（促进包容性的商业化监管）提供发展路线图。本研究强调生成式AI可通过伦理化改进推动数字教学发展，构建公平创新的教育环境。完整数据集详见：https://github.com/erfan-nourbakhsh/GenAI-EdSent。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.11934">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.11934">arXiv</a></p>
<hr />
<h3>36. 面向高质量数据共享的层次化数据集选择方法</h3>
<p><strong>原文标题：</strong> Hierarchical Dataset Selection for High-Quality Data Sharing</p>
<p><strong>摘要：</strong>
现代机器学习的成功依赖于高质量训练数据的获取。在许多实际场景中，例如从公共存储库获取数据或跨机构共享数据时，数据通常以离散数据集的形式组织，这些数据集在相关性、质量和效用上存在差异。因此，选择搜索哪些存储库或机构以获取有用数据集，以及选择哪些数据集纳入模型训练，成为关键决策。然而，现有方法大多仅针对单个样本进行选择，并将所有数据视为同等相关，忽略了数据集及其来源之间的差异。本研究形式化定义了数据集选择任务：从大规模异构数据池中选择完整数据集，以在资源约束下提升下游任务性能。我们提出基于层次结构的数据集选择方法（DaSH），该方法在数据集和群组（如数据集合、机构）两个层面建模数据效用，从而能够从有限观测中实现高效泛化。在两个公共基准测试（Digit-Five 和 DomainNet）中，DaSH 在准确率上优于现有数据选择基线方法最高达 26.2%，同时所需探索步骤显著减少。消融实验表明，DaSH 在低资源场景和相关数据集缺乏的情况下仍保持稳健性，适用于实际多源学习工作流程中可扩展且自适应的数据集选择。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.10952">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.10952">arXiv</a></p>
<hr />
<h3>37. MeViS：面向指代运动表达视频分割的多模态数据集</h3>
<p><strong>原文标题：</strong> MeViS: A Multi-Modal Dataset for Referring Motion Expression Video Segmentation</p>
<p><strong>摘要：</strong>
本文提出一个面向指代运动表达视频分割的大规模多模态数据集，其核心在于依据对物体运动的语言描述实现视频中目标物体的分割与追踪。现有指代视频分割数据集多聚焦于显著物体，且使用的语言表达富含静态属性，使得目标物体可能在单帧图像中即可被识别。此类数据集未能充分强调运动在视频与语言中的作用。为探索利用运动表达与运动推理线索实现像素级视频理解的可行性，我们提出MeViS数据集，该数据集包含33,072条人工标注的文本与音频形式运动表达，涵盖2,006个复杂场景视频中的8,171个物体。我们在MeViS支持的4项任务中对15种现有方法进行基准测试，包括6种指代视频目标分割方法、3种音频引导视频目标分割方法、2种指代多目标追踪方法，以及针对新提出的指代运动表达生成任务的4种视频描述生成方法。测试结果揭示了现有方法在处理运动表达引导的视频理解任务时存在的不足与局限。我们进一步分析了相关挑战，并提出一种面向指代视频目标分割/音频引导视频目标分割/指代多目标追踪任务的LMPM++方法，该方法取得了当前最优性能。本数据集为复杂视频场景中运动表达引导的视频理解算法开发提供了平台。所提出的MeViS数据集及相关方法源代码已公开于https://henghuiding.com/MeViS/。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.10945">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.10945">arXiv</a></p>
<hr />
<h3>38. CoSPlan：基于场景图增量更新的纠错式序列规划</h3>
<p><strong>原文标题：</strong> CoSPlan: Corrective Sequential Planning via Scene Graph Incremental Updates</p>
<p><strong>摘要：</strong>
大规模视觉语言模型展现出令人瞩目的复杂推理能力，但在视觉序列规划领域——即执行多步动作以实现目标——仍未得到充分探索。此外，实际序列规划常包含非最优（错误）步骤，这对模型检测与纠正此类步骤的能力提出了挑战。我们提出纠错式序列规划基准，用于评估视觉语言模型在四大领域易出错的视觉序列规划任务中的表现：迷宫导航、积木重排、图像重建和物体重组。该基准评估两项关键能力：错误检测（识别非最优动作）与步骤补全（纠正并完成动作序列以实现目标）。尽管采用了思维链和场景图等先进推理技术，现有视觉语言模型（如Intern-VLM与Qwen2）在该基准上表现仍不理想，未能有效利用上下文线索达成目标。为此，我们提出一种无需训练的新方法——场景图增量更新，该方法在初始状态与目标状态之间引入中间推理步骤。该方法能帮助视觉语言模型进行序列推理，平均性能提升达5.2%。除了增强纠错式序列规划的可靠性外，该方法还可泛化至传统规划任务（如Plan-Bench和视觉问答）。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.10342">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.10342">arXiv</a></p>
<hr />
<h3>39. ContextAnyone：面向角色一致性的文本到视频生成中的上下文感知扩散方法</h3>
<p><strong>原文标题：</strong> ContextAnyone: Context-Aware Diffusion for Character-Consistent Text-to-Video Generation</p>
<p><strong>摘要：</strong>
文本到视频（T2V）生成技术发展迅速，但在不同场景中保持角色身份一致性仍是一个重大挑战。现有的个性化方法通常侧重于面部特征，却难以保留发型、服装和体型等更广泛的上下文线索，而这些线索对于视觉连贯性至关重要。本文提出ContextAnyone，一种上下文感知的扩散框架，能够通过文本描述和单张参考图像实现角色一致性的视频生成。该方法联合重建参考图像并生成新的视频帧，使模型能够充分感知并利用参考信息。通过一种新颖的“强调-注意力”模块，参考信息被有效整合到基于DiT的扩散主干网络中，该模块有选择性地强化参考感知特征，并防止跨帧的身份漂移。双重引导损失结合了扩散和参考重建目标，以增强外观保真度；同时，提出的Gap-RoPE位置嵌入技术将参考标记与视频标记分离，以稳定时序建模。实验表明，ContextAnyone在身份一致性和视觉质量上优于现有的参考到视频生成方法，能够在不同动作和场景中生成连贯且保持上下文信息的角色视频。项目页面：https://github.com/ziyang1106/ContextAnyone。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.07328">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.07328">arXiv</a></p>
<hr />
<h2>🔍 关键词云图</h2>
<p><img alt="关键词云图" src="../images/keywords_wordcloud.png" /></p>
<h2>📈 近期论文趋势</h2>
<p><img alt="论文趋势" src="../images/daily_papers.png" /></p>
<h2>🎙️ 语音播报</h2>
<ul>
<li><a href="../audio/2025-12-17_daily_papers.mp3">收听今日论文解读</a></li>
</ul>
<h2>📱 订阅渠道</h2>
<ul>
<li>GitHub: <a href="https://github.com/2404589803/hf-daily-paper-newsletter-chinese">hf-daily-paper-newsletter-chinese</a></li>
</ul>
    </div>
</body>
</html>