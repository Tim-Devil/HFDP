<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Hugging Face 论文日报 - 2025-12-17</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', sans-serif;
            line-height: 1.6;
            color: #333;
            background-color: #f5f5f5;
            padding: 20px;
        }
        
        .container {
            max-width: 900px;
            margin: 0 auto;
            background-color: white;
            padding: 40px;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }
        
        h1 {
            color: #2c3e50;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
            margin-bottom: 30px;
            font-size: 28px;
        }
        
        h1 img {
            vertical-align: middle;
            margin-right: 10px;
        }
        
        h2 {
            color: #34495e;
            margin-top: 40px;
            margin-bottom: 20px;
            font-size: 24px;
            border-left: 4px solid #3498db;
            padding-left: 15px;
        }
        
        h3 {
            color: #2c3e50;
            margin-top: 30px;
            margin-bottom: 15px;
            font-size: 20px;
        }
        
        p {
            margin-bottom: 15px;
            text-align: justify;
        }
        
        ul {
            margin-left: 20px;
            margin-bottom: 20px;
        }
        
        li {
            margin-bottom: 8px;
        }
        
        a {
            color: #3498db;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        strong {
            color: #2c3e50;
            font-weight: 600;
        }
        
        hr {
            border: none;
            border-top: 1px solid #e0e0e0;
            margin: 30px 0;
        }
        
        /* 关键修复:限制图片宽度 */
        img {
            max-width: 100%;
            height: auto;
            display: block;
            margin: 20px auto;
            border-radius: 4px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        
        /* 确保图片容器也有宽度限制 */
        p img {
            max-width: 100%;
        }
        
        /* 论文详情区域样式 */
        .paper-section {
            background-color: #f9f9f9;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 20px;
        }
        
        /* 统计信息样式 */
        .stats {
            background-color: #e8f4f8;
            padding: 15px;
            border-radius: 6px;
            margin-bottom: 20px;
        }
        
        /* 响应式设计 */
        @media (max-width: 768px) {
            .container {
                padding: 20px;
            }
            
            h1 {
                font-size: 24px;
            }
            
            h2 {
                font-size: 20px;
            }
            
            h3 {
                font-size: 18px;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <h1><img src="https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo.png" width="30"/> Hugging Face 2025-12-17 论文日报</h1>
<h2>📊 今日论文统计</h2>
<ul>
<li>总论文数：39</li>
<li>热门领域：GPT, AIGC, RL, LLM, Transformer</li>
</ul>
<h2>📝 论文详情</h2>
<h3>1. MMGR：多模态生成式推理评估框架</h3>
<p><strong>原文标题：</strong> MMGR: Multi-Modal Generative Reasoning</p>
<p><strong>摘要：</strong>
视频基础模型能够生成视觉逼真且时序连贯的内容，但其作为世界模拟器的可靠性取决于其是否能够捕捉物理、逻辑与空间约束。现有评估指标（如弗雷歇视频距离）侧重于感知质量，却忽视了推理层面的缺陷，包括对因果性、物理规律及全局一致性的违背。本文提出MMGR（多模态生成式推理评估基准），这是一个基于五大推理能力构建的原则性评估框架：物理推理、逻辑推理、三维空间推理、二维空间推理与时序推理。MMGR在三大领域对生成式推理进行系统评估：抽象推理（ARC-AGI、数独任务）、具身导航（真实世界三维导航与定位）以及物理常识（运动场景与组合交互）。该框架采用细粒度评估指标，要求视频与图像生成在整体上均保持正确性。通过对主流视频模型（Veo-3、Sora-2、Wan-2.2）和图像模型（Nano-banana、Nano-banana Pro、GPT-4o-image、Qwen-image）进行基准测试，本研究揭示了各模型在不同领域存在的显著性能差距。实验表明，模型在物理常识任务上表现尚可，但在抽象推理领域表现欠佳（ARC-AGI任务准确率低于10%），且在具身环境中的长程空间规划任务上存在明显困难。分析进一步指出当前模型的核心局限：过度依赖感知数据、全局状态一致性薄弱，以及优化目标偏向视觉合理性而忽视因果正确性。MMGR为生成式世界模型提供了一个统一的诊断性评估基准，并指明了构建具备推理意识的生成模型的发展路径。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.14691">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.14691">arXiv</a></p>
<hr />
<h3>2. 视频真实性测试：AI生成的ASMR视频能否欺骗视觉语言模型与人类？</h3>
<p><strong>原文标题：</strong> Video Reality Test: Can AI-Generated ASMR Videos fool VLMs and Humans?</p>
<p><strong>摘要：</strong>
近期视频生成技术的进步已能产出与真实视频难以区分的生动内容，使得AI生成视频检测成为新兴的社会挑战。现有AIGC检测基准大多针对无音频视频、面向宽泛叙事领域，且仅聚焦于分类任务。然而，当前最先进的视频生成模型能否产出具有沉浸感、音画同步且足以可靠欺骗人类和视觉语言模型（VLMs）的视频，仍不明确。为此，我们提出“视频真实性测试”——一个基于ASMR音视频源的基准测试套件，用于在紧密音画耦合条件下检验感知真实性，其特点包括：（一）沉浸式ASMR音视频源。基于精心筛选的真实ASMR视频构建，该基准针对细粒度的动作-物体交互，涵盖多样化的物体、动作与背景。（二）同行评审式评估。采用对抗性创作者-评审者协议：视频生成模型作为试图欺骗评审者的创作者，而VLMs则作为旨在识别伪造内容的评审者。实验结果表明：最佳创作者模型Veo3.1-Fast甚至能欺骗大多数VLMs——最强评审模型（Gemini 2.5-Pro）仅达到56%的准确率（随机基准为50%），远低于人类专家水平（81.25%）。音频的加入提升了真假判别能力，但水印等表面线索仍会显著误导模型。这些发现划定了当前视频生成真实性的边界，并揭示了VLMs在感知保真度与音画一致性方面的局限。代码已开源：https://github.com/video-reality-test/video-reality-test。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.13281">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.13281">arXiv</a></p>
<hr />
<h3>3. WorldPlay：面向实时交互式世界建模的长时几何一致性方法</h3>
<p><strong>原文标题：</strong> WorldPlay: Towards Long-Term Geometric Consistency for Real-Time Interactive World Modeling</p>
<p><strong>摘要：</strong>
本文提出WorldPlay，一种流式视频扩散模型，能够实现具有长时几何一致性的实时交互式世界建模，解决了当前方法在速度与内存之间的权衡限制。WorldPlay的创新性主要体现在三个方面：1）采用双重动作表征机制，实现对用户键盘与鼠标输入的鲁棒动作控制；2）为实现长时一致性，提出重构上下文记忆模块，动态重建历史帧的上下文信息，并通过时序重构技术保持几何重要性高但时间久远的帧的可访问性，有效缓解记忆衰减问题；3）提出面向记忆感知模型的上下文强制蒸馏方法，通过对齐师生模型间的记忆上下文，保持学生模型利用长程信息的能力，在实现实时生成速度的同时避免误差漂移。综合以上技术，WorldPlay能够以24帧/秒的速率生成720p长序列流式视频，在保持卓越一致性的同时优于现有技术，并在多样场景中展现出强大的泛化能力。项目页面与在线演示详见：https://3d-models.hunyuan.tencent.com/world/ 与 https://3d.hunyuan.tencent.com/sceneTo3D。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.14614">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.14614">arXiv</a></p>
<hr />
<h3>4. Scone：通过统一理解-生成建模桥接主题驱动图像生成中的组合与区分</h3>
<p><strong>原文标题：</strong> Scone: Bridging Composition and Distinction in Subject-Driven Image Generation via Unified Understanding-Generation Modeling</p>
<p><strong>摘要：</strong>
主题驱动图像生成已从单一主题组合发展到多主题组合，但忽视了区分能力——即在输入包含多个候选主题时准确识别并生成正确主题的能力。这一局限限制了模型在复杂真实视觉场景中的有效性。我们提出Scone，一种统一的理解-生成方法，整合了组合与区分能力。Scone使理解专家充当语义桥梁，传递语义信息并引导生成专家在保持主题身份的同时最小化干扰。采用两阶段训练方案：先学习组合能力，再通过语义对齐和基于注意力的掩码机制增强区分能力。我们还提出了SconeEval基准测试，用于评估多样化场景下的组合与区分性能。实验表明，在两个基准测试中，Scone在组合与区分任务上均优于现有开源模型。我们的模型、基准测试及训练数据已开源：https://github.com/Ryann-Ran/Scone。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.12675">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.12675">arXiv</a></p>
<hr />
<h3>5. RoboTracer：面向机器人学的视觉语言模型空间轨迹推理技术</h3>
<p><strong>原文标题：</strong> RoboTracer: Mastering Spatial Trace with Reasoning in Vision-Language Models for Robotics</p>
<p><strong>摘要：</strong>
空间轨迹追踪作为机器人的基础具身交互能力，其实现面临本质性挑战，需要融合多步骤度量推理、复杂空间指代与现实世界度量测量。然而，现有方法难以应对此类组合式任务。为此，我们提出RoboTracer——一种具有三维感知能力的视觉语言模型，首次通过通用空间编码器与回归监督解码器，在监督微调阶段实现三维空间指代与测量，从而增强模型对尺度信息的感知能力。此外，RoboTracer通过引入度量敏感过程奖励的强化微调机制，监督关键中间感知线索以精准生成空间轨迹，进而推进多步骤度量推理能力。为支撑监督微调与强化微调训练，我们构建了包含3000万问答对的大规模数据集TraceSpatial，涵盖室外/室内/桌面场景，并支持多达9步的复杂推理流程。同时，我们提出评测基准TraceSpatial-Bench，填补了空间轨迹追踪系统性评估的空白。实验结果表明，RoboTracer在空间理解、测量与指代任务中全面超越基线模型，平均成功率达79.1%，并在TraceSpatial-Bench基准上以显著优势取得最先进性能，准确率较Gemini-2.5-Pro提升36%。值得注意的是，RoboTracer可与多种控制策略集成，在杂乱的真实场景中驱动多样化机器人（UR5、G1人形机器人）执行长时程动态任务。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.13660">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.13660">arXiv</a></p>
<hr />
<h3>6. OpenDataArena：一个用于基准测试训练后数据集价值的公平开放平台</h3>
<p><strong>原文标题：</strong> OpenDataArena: A Fair and Open Arena for Benchmarking Post-Training Dataset Value</p>
<p><strong>摘要：</strong>
大语言模型的快速发展依赖于训练后数据集的质量与多样性。然而，一个关键矛盾持续存在：尽管模型经过了严格的基准测试，但支撑这些模型的数据却仍是一个“黑箱”——其构成不透明、来源不明确，且缺乏系统性评估。这种不透明性阻碍了研究的可复现性，并模糊了数据特征与模型行为之间的因果关系。为弥补这一差距，我们推出了OpenDataArena，这是一个旨在对训练后数据的内在价值进行基准测试的全面开放平台。ODA构建了一个包含四大支柱的综合生态系统：（一）统一的训练-评估流程，确保在不同模型（如Llama、Qwen）和领域间进行公平、开放的比较；（二）多维评分框架，从数十个不同维度刻画数据质量；（三）交互式数据谱系探索器，用于可视化数据集谱系并解析其构成来源；（四）完全开源的训练、评估与评分工具包，以促进数据研究。基于ODA开展的大规模实验——涵盖多个领域的120多个训练数据集、22项基准测试，并通过超过600次训练运行和4000万个已处理数据点进行验证——揭示了多项重要发现。我们的分析揭示了数据复杂度与任务性能之间的内在权衡，通过谱系追溯识别了流行基准中的冗余数据，并绘制了数据集间的谱系关系图。我们公开了所有结果、工具与配置，以推动高质量数据评估的普及。ODA不仅旨在扩展排行榜，更期望推动从试错式的数据整理转向以数据为中心的人工智能的规范科学，从而为数据混合规律与基础模型战略构成的严谨研究铺平道路。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.14051">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.14051">arXiv</a></p>
<hr />
<h3>7. 向量棱镜：通过分层语义结构实现矢量图形动画化</h3>
<p><strong>原文标题：</strong> Vector Prism: Animating Vector Graphics by Stratifying Semantic Structure</p>
<p><strong>摘要：</strong>
可缩放矢量图形（SVG）是现代网页设计的核心要素，随着网络环境日益动态化，对其动画化的需求持续增长。尽管在代码生成与运动规划领域已取得进展，但实现矢量图形动画的自动化对于视觉语言模型（VLM）而言仍具挑战性。由于视觉连贯的图形组件常被分割为低层级几何形状，难以提示哪些元素应协同运动，当前VLMs在处理SVG时频繁出现错误。本文提出一种框架，通过恢复SVG动画化所需的语义结构，揭示当前VLM系统所忽视的关键层级。该框架通过对多个弱部件预测结果进行统计聚合，使系统能够从噪声预测中稳定推断语义信息。通过将SVG重组为语义群组，本方法使VLMs能够生成连贯性显著提升的动画。实验结果表明，相较于现有方法，本框架取得实质性突破，证明语义重建是实现稳健SVG动画化的关键步骤，并为VLMs与矢量图形之间建立更可解释的交互机制提供支持。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.14336">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.14336">arXiv</a></p>
<hr />
<h3>8. 从任务中心视角揭示向量相似性搜索的潜在缺陷与下一代导航路径</h3>
<p><strong>原文标题：</strong> Reveal Hidden Pitfalls and Navigate Next Generation of Vector Similarity Search from Task-Centric Views</p>
<p><strong>摘要：</strong>
高维空间中的向量相似性搜索正迅速成为下一代数据库系统的核心功能，支撑着从大语言模型中的嵌入检索到语义信息检索与推荐引擎等众多数据密集型服务。然而，当前基准测试主要围绕基于距离度量定义的召回率-延迟权衡进行评估，忽视了检索质量如何最终影响下游任务，这种脱节可能误导学术研究与工业实践。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.12980">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.12980">arXiv</a></p>
<hr />
<h3>9. MemFlow：用于一致高效长视频叙事的自适应记忆流</h3>
<p><strong>原文标题：</strong> MemFlow: Flowing Adaptive Memory for Consistent and Efficient Long Video Narratives</p>
<p><strong>摘要：</strong>
流式视频生成的核心挑战在于保持长上下文中的内容一致性，这对记忆设计提出了较高要求。现有方案大多通过预定义策略压缩历史帧来维护记忆。然而，不同待生成视频片段应参考不同的历史线索，固定策略难以满足这一需求。本研究提出MemFlow以解决该问题。具体而言，在生成后续片段前，我们通过检索与该片段文本提示最相关的历史帧来动态更新记忆库。这一设计使得即使后续帧中出现新事件或场景切换，叙事仍能保持连贯性。此外，在生成过程中，我们仅激活记忆库中与注意力层每个查询最相关的标记，从而有效保障生成效率。MemFlow通过这种方式实现了卓越的长上下文一致性，其计算开销可忽略不计（与无记忆基线相比仅降低7.9%生成速度），并保持了对所有支持KV缓存的流式视频生成模型的兼容性。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.14699">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.14699">arXiv</a></p>
<hr />
<h3>10. RecGPT-V2技术报告</h3>
<p><strong>原文标题：</strong> RecGPT-V2 Technical Report</p>
<p><strong>摘要：</strong>
大型语言模型在将推荐系统从隐式行为模式匹配转变为显式意图推理方面展现出显著潜力。尽管RecGPT-V1通过将基于大语言模型的推理融入用户兴趣挖掘与物品标签预测，成功开创了这一范式，但其存在四个根本性局限：（1）多推理路径下的计算效率低下与认知冗余；（2）固定模板生成中解释多样性的不足；（3）监督学习范式下泛化能力有限；（4）结果导向的评估方式过于简化，难以匹配人类标准。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.14503">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.14503">arXiv</a></p>
<hr />
<h3>11. ShowTable：通过协同反思与优化解锁创意表格可视化</h3>
<p><strong>原文标题：</strong> ShowTable: Unlocking Creative Table Visualization with Collaborative Reflection and Refinement</p>
<p><strong>摘要：</strong>
尽管现有生成模型与统一模型在通用图像生成方面表现优异，但其在需要深度推理、规划以及超越通用场景的精确数据到视觉映射能力的任务上仍面临挑战。为突破现有局限，我们提出一项新颖且具有挑战性的任务：创意表格可视化，要求模型根据给定表格数据生成既忠实反映数据又具备美学价值的信息图。为应对这一挑战，我们提出ShowTable流程，该流程通过渐进式自我修正过程将多模态大语言模型与扩散模型协同整合。其中，多模态大语言模型作为核心协调者，负责推理视觉方案并判断视觉误差以提供优化指令，扩散模型则执行多模态大语言模型的指令，从而实现高保真度的生成结果。为支持该任务及流程，我们设计了三条自动化数据构建流程用于训练不同模块。此外，我们提出TableVisBench新基准，该基准包含800个涵盖5个评估维度的挑战性实例，用于系统评估模型在此任务上的性能。实验表明，基于不同模型实例化的流程显著优于基线方法，凸显了其在多模态推理、生成与纠错方面的卓越能力。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.13303">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.13303">arXiv</a></p>
<hr />
<h3>12. 基于文本可引导图像到三维的前馈式编辑方法</h3>
<p><strong>原文标题：</strong> Feedforward 3D Editing via Text-Steerable Image-to-3D</p>
<p><strong>摘要：</strong>
图像到三维生成技术的最新进展为设计、增强现实/虚拟现实及机器人领域开辟了广阔前景。然而，要将人工智能生成的三维资产应用于实际场景，关键需求在于能够便捷地对其进行编辑。本文提出一种前馈式方法Steer3D，通过为图像到三维模型增加文本引导能力，实现用自然语言编辑生成的三维资产。该方法受ControlNet架构启发，将其适配于图像到三维生成任务，从而在前向传播中直接实现文本引导。我们构建了可扩展的自动数据生成引擎，并开发了基于流匹配训练与直接偏好优化的两阶段训练方案。与现有方法相比，Steer3D能更准确地遵循语言指令，同时保持与原始三维资产更好的一致性，且处理速度提升2.4至28.5倍。实验表明，仅需十万量级数据即可为预训练的图像到三维生成模型增加新模态（文本）的引导能力。项目网站：https://glab-caltech.github.io/steer3d/</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.13678">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.13678">arXiv</a></p>
<hr />
<h3>13. Nemotron-Cascade：面向通用推理模型的级联强化学习规模化方法</h3>
<p><strong>原文标题：</strong> Nemotron-Cascade: Scaling Cascaded Reinforcement Learning for General-Purpose Reasoning Models</p>
<p><strong>摘要：</strong>
使用强化学习构建通用推理模型面临显著的跨领域异质性挑战，包括推理时响应长度和验证延迟的巨大差异。这种变异性使强化学习基础设施复杂化，延缓训练进程，并为训练课程设计（如响应长度扩展）和超参数选择带来困难。本研究提出级联领域强化学习方法，用于开发具备指令执行与深度思考双模式的通用推理模型Nemotron-Cascade。与传统混合不同领域异构提示的方法不同，级联强化学习通过按领域顺序执行强化学习来协调训练流程，既降低了工程复杂度，又在广泛基准测试中实现了最先进的性能。值得注意的是，作为前置步骤的对齐强化学习从人类反馈不仅超越了单纯的偏好优化，更显著提升了模型的推理能力；后续按领域进行的强化学习验证与修正阶段几乎不会降低模型在早期领域已取得的基准性能，甚至可能带来提升（如图1示例所示）。经过强化学习的140亿参数模型在LiveCodeBench v5/v6/Pro基准上超越其监督微调教师模型DeepSeek-R1-0528，并在2025年国际信息学奥林匹克竞赛中达到银牌水平。我们公开分享了完整的训练方案与数据构建方法。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.13607">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.13607">arXiv</a></p>
<hr />
<h3>14. Olmo 3</h3>
<p><strong>原文标题：</strong> Olmo 3</p>
<p><strong>摘要：</strong>
本文介绍Olmo 3系列模型，这是一组参数规模分别为70亿和320亿的顶尖全开源语言模型。Olmo 3的构建目标涵盖长上下文推理、函数调用、代码生成、指令跟随、通用对话及知识检索等能力。本次发布包含完整的模型构建流程，即该系列模型从数据准备到最终成型的全生命周期，涵盖每个构建阶段、检查点、数据点及所有相关依赖项。我们的旗舰模型Olmo 3 Think 32B，是迄今为止发布的最强大的全开源思维模型。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.13961">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.13961">arXiv</a></p>
<hr />
<h3>15. 可微分进化强化学习</h3>
<p><strong>原文标题：</strong> Differentiable Evolutionary Reinforcement Learning</p>
<p><strong>摘要：</strong>
在强化学习（RL）中，设计有效的奖励函数是一个核心且往往艰巨的挑战，尤其是在为复杂推理任务开发自主智能体时。尽管存在自动化的奖励优化方法，但它们通常依赖于将奖励函数视为黑盒的无导数进化启发式算法，未能捕捉奖励结构与任务性能之间的因果关系。为弥合这一差距，我们提出了可微分进化强化学习（DERL），这是一个双层框架，能够自主发现最优奖励信号。在DERL中，元优化器通过组合结构化的原子基元来演化奖励函数（即元奖励），从而指导内层策略的训练。关键的是，与以往的进化方法不同，DERL在其元优化过程中是可微分的：它将内层验证性能视为信号，通过强化学习来更新元优化器。这使得DERL能够近似任务成功的“元梯度”，逐步学习生成更密集且更具可操作性的反馈。我们在三个不同领域验证了DERL：机器人智能体（ALFWorld）、科学模拟（ScienceWorld）和数学推理（GSM8k, MATH）。实验结果表明，DERL在ALFWorld和ScienceWorld上实现了最先进的性能，显著优于依赖启发式奖励的方法，尤其是在分布外场景中。对进化轨迹的分析表明，DERL成功捕捉了任务的内在结构，使得智能体能够在无需人工干预的情况下实现自我改进的对齐。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.13399">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.13399">arXiv</a></p>
<hr />
<h3>16. VersatileFFN：通过自适应宽深复用实现大语言模型的参数高效性</h3>
<p><strong>原文标题：</strong> VersatileFFN: Achieving Parameter Efficiency in LLMs via Adaptive Wide-and-Deep Reuse</p>
<p><strong>摘要：</strong>
大语言模型（LLMs）规模的快速扩展已取得显著性能，但也导致了高昂的内存成本。现有的参数高效方法（如剪枝和量化）主要对预训练模型进行压缩，而未增强其架构能力，因此触及了基础模型的表示能力上限。本文提出VersatileFFN，一种新颖的前馈网络（FFN），能够在固定参数预算内灵活复用宽度和深度维度的参数。受认知双过程理论启发，VersatileFFN包含两条自适应路径：一条宽度自适应路径，从单个共享FFN生成混合子专家，模拟稀疏专家路由而不增加参数；另一条深度自适应路径，递归应用同一FFN以模拟对复杂标记的更深层处理。一个难度感知门控机制动态平衡两条路径，引导“简单”标记通过高效的宽度路径，并为“困难”标记分配更深层的迭代细化。关键在于两条路径复用相同参数，因此所有额外能力均来自计算而非内存。在不同基准测试和模型规模上的实验验证了该方法的有效性。代码将在https://github.com/huawei-noah/noah-research/tree/master/VersatileFFN 公开。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.14531">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.14531">arXiv</a></p>
<hr />
<h3>17. A4-Agent：一种用于零样本可供性推理的智能体框架</h3>
<p><strong>原文标题：</strong> A4-Agent: An Agentic Framework for Zero-Shot Affordance Reasoning</p>
<p><strong>摘要：</strong>
可供性预测旨在根据语言指令识别物体上的交互区域，是实现具身人工智能的关键技术。当前主流端到端模型将高层推理与低层定位耦合为单一整体流程，并依赖标注数据集进行训练，导致其在新型物体和未见环境中的泛化能力较差。本文突破此范式，提出A4-Agent——一种无需训练的智能体框架，将可供性预测解耦为三阶段流程。该框架在测试时协调多个专用基础模型：(1) 运用生成模型可视化交互场景的“构想者”；(2) 利用大型视觉语言模型确定交互部位的“思考者”；(3) 调度视觉基础模型精确定位交互区域的“定位者”。通过融合预训练模型的互补优势且无需任务特定微调，本零样本框架在多个基准测试中显著超越现有监督方法，并展现出对真实场景的强健泛化能力。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.14442">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.14442">arXiv</a></p>
<hr />
<h3>18. SS4D：基于结构化时空隐变量的原生4D生成模型</h3>
<p><strong>原文标题：</strong> SS4D: Native 4D Generative Model via Structured Spacetime Latents</p>
<p><strong>摘要：</strong>
本文提出SS4D，一种原生4D生成模型，能够直接从单目视频合成动态三维物体。与现有方法通过优化三维或视频生成模型来构建4D表示不同，我们直接在4D数据上训练生成器，实现了高保真度、时间连贯性和结构一致性。本方法的核心是一组经过压缩的结构化时空隐变量。具体而言：（1）针对4D训练数据稀缺的问题，我们在预训练的单图像转三维模型基础上构建，保持了强大的空间一致性；（2）通过引入跨帧推理的专用时间层来保证时间连贯性；（3）为支持长视频序列的高效训练与推理，我们采用分解式4D卷积和时间下采样模块沿时间轴压缩隐变量序列。此外，我们采用精心设计的训练策略以增强对遮挡场景的鲁棒性。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.14284">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.14284">arXiv</a></p>
<hr />
<h3>19. Sparse-LaViDa：稀疏多模态离散扩散语言模型</h3>
<p><strong>原文标题：</strong> Sparse-LaViDa: Sparse Multimodal Discrete Diffusion Language Models</p>
<p><strong>摘要：</strong>
掩码离散扩散模型（MDMs）在图像理解、生成与编辑等广泛多模态任务中展现出卓越性能。然而，由于每个采样步骤均需重复处理冗余的掩码标记，其推理速度仍存在优化空间。本研究提出Sparse-LaViDa——一种新颖的建模框架，通过在每步推理中动态截断非必要的掩码标记以加速MDM采样。为保持生成质量，我们引入专用寄存器标记作为被截断标记的紧凑表征。此外，为确保训练与推理的一致性，我们设计了特殊的注意力掩码机制，使训练过程能精确模拟截断采样流程。基于当前最先进的统一MDM框架LaViDa-O构建的Sparse-LaViDa，在文本到图像生成、图像编辑和数学推理等多样化任务中实现了最高达2倍的加速，同时完全保持生成质量。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.14008">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.14008">arXiv</a></p>
<hr />
<h3>20. 用于视觉标记化与生成的球形里奇量化</h3>
<p><strong>原文标题：</strong> Spherical Leech Quantization for Visual Tokenization and Generation</p>
<p><strong>摘要：</strong>
非参数量化方法因其参数效率高且能扩展至大规模码本而备受关注。本文通过格编码的视角，提出了不同非参数量化方法的统一表述框架。格编码的几何特性揭示了在训练自编码器时，对于某些现有免查表量化变体（如BSQ）引入辅助损失项的必要性。在此基础上，我们探索了若干可能的格结构候选方案，包括随机格、广义斐波那契格以及最密球堆积格。研究发现，基于里奇格的量化方法（称为球形里奇量化Λ_{24}-SQ）凭借其高对称性与超球面上的均匀分布特性，既能简化训练流程，又能改善重建与压缩的权衡关系。在图像标记化与压缩任务中，该方法在所有评估指标上均优于当前最佳基准BSQ，同时消耗更少的比特数。该优势同样延伸至当前最先进的自回归图像生成框架中。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.14697">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.14697">arXiv</a></p>
<hr />
<h3>21. CRISP：基于平面场景基元与接触引导的单目视频真实到仿真转换方法</h3>
<p><strong>原文标题：</strong> CRISP: Contact-Guided Real2Sim from Monocular Video with Planar Scene Primitives</p>
<p><strong>摘要：</strong>
本文提出CRISP方法，该技术能够从单目视频中重建可仿真的运动轨迹与场景几何结构。现有的人-场景联合重建方法通常依赖数据驱动的先验知识，采用无物理约束的联合优化方案，或重建出存在噪声与伪影的几何结构，导致涉及场景交互的运动追踪策略失效。与此不同，我们的核心思路是通过对场景点云进行深度、法向量和光流信息的聚类处理，拟合平面几何基元，从而重建出凸性、清洁且可直接用于仿真的几何模型。为重建可能被交互过程遮挡的场景几何，我们引入人-场景接触建模机制（例如利用人体姿态重建被遮挡的椅面）。最后，通过强化学习驱动的人形控制器验证重建结果，确保人体与场景重建的物理合理性。在以人为中心的视频基准测试集（EMDB、PROX）上，本方法将运动追踪失败率从55.2%降低至6.9%，同时强化学习仿真吞吐量提升43%。我们进一步在多样化真实场景视频（包括日常拍摄视频、网络视频乃至Sora生成视频）中验证了方法的有效性。实验表明，CRISP能够大规模生成物理有效的人体运动与交互环境，显著推进机器人及AR/VR领域的真实到仿真应用。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.14696">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.14696">arXiv</a></p>
<hr />
<h3>22. TimeLens：基于多模态大语言模型重新思考视频时序定位</h3>
<p><strong>原文标题：</strong> TimeLens: Rethinking Video Temporal Grounding with Multimodal LLMs</p>
<p><strong>摘要：</strong>
本文并未提出一种新颖的方法，而是为视频理解的核心能力——视频时序定位（VTG）建立了一个直接、渐进但至关重要的基准。尽管多模态大语言模型（MLLMs）在多种视频理解任务中表现出色，但如何针对VTG任务优化这些模型的策略仍未得到充分探索。本文提出TimeLens，围绕数据质量和算法设计两个主要维度，系统性地研究了如何构建具备强大VTG能力的MLLMs。我们首先揭示了现有VTG基准数据集中存在的关键质量问题，并引入了TimeLens-Bench——一个包含三个流行基准数据集经过严格质量标准重新标注的版本。我们的分析表明，与旧有基准相比，模型评估排名发生了显著变化，证实了先前评估标准的不可靠性。同时，我们通过自动化重新标注流程处理了训练数据中的噪声，构建了TimeLens-100K，这是一个大规模、高质量的训练数据集。基于此数据基础，我们深入探索了算法设计原则，获得了一系列有意义的见解以及高效且有效的实践方法。这些方法包括：用于时间表示的交替文本编码、一种无需复杂推理且具有可验证奖励的强化学习（RLVR）训练范式，以及精心设计的RLVR训练策略。这些努力最终催生了TimeLens模型系列——一组在开源模型中具备最先进VTG性能的MLLMs，其表现甚至超越了GPT-5和Gemini-2.5-Flash等专有模型。所有代码、数据和模型都将公开发布，以促进未来研究。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.14698">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.14698">arXiv</a></p>
<hr />
<h3>23. EVOLVE-VLA：基于环境反馈的视觉-语言-动作模型测试时训练框架</h3>
<p><strong>原文标题：</strong> EVOLVE-VLA: Test-Time Training from Environment Feedback for Vision-Language-Action Models</p>
<p><strong>摘要：</strong>
实现真正自适应的具身智能需要智能体不仅通过模仿静态演示进行学习，更能通过环境交互持续改进，这类似于人类通过实践掌握技能的方式。视觉-语言-动作模型通过利用大型语言模型推动了机器人操作的发展，但其根本上仍受限于监督微调范式：每个任务需要数百次演示、僵化地记忆轨迹，且在部署条件偏离训练时无法适应。本文提出EVOLVE-VLA，一种测试时训练框架，使视觉-语言-动作模型能够通过环境交互持续适应，且仅需极少甚至无需任务特定演示。核心技术挑战在于用自主反馈替代测试时无法获取的预设奖励信号。我们通过设计可提供密集反馈的学习型进度估计器来解决此问题，并特别设计双重机制来“驯服”这种固有噪声信号：（1）累积进度估计机制平滑噪声点估计；（2）渐进式规划范围扩展策略实现策略逐步演化。EVOLVE-VLA取得显著性能提升：长时序任务提升8.6%，单样本学习提升22.0%，并实现跨任务泛化——在未见过且未经任务特定演示训练的任务上达到20.8%成功率（纯监督微调方法为0%）。定性分析揭示了演示中未出现的新兴能力，包括错误恢复和新策略生成。这项研究标志着视觉-语言-动作模型向真正学习与适应能力迈出关键一步，从静态模仿转向持续自我改进。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.14666">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.14666">arXiv</a></p>
<hr />
<h3>24. TAT：面向一体化医学图像复原的任务自适应Transformer</h3>
<p><strong>原文标题：</strong> TAT: Task-Adaptive Transformer for All-in-One Medical Image Restoration</p>
<p><strong>摘要：</strong>
医学图像复原（MedIR）旨在从低质量医学图像中恢复高质量图像。当前MedIR领域的研究进展主要集中于能够同时处理多种不同MedIR任务的一体化模型。然而，由于模态类型与退化类型存在显著差异，使用共享模型处理这些多样化任务时，必须审慎考虑两种关键的任务间关系：其一是任务干扰，即不同任务对同一参数产生冲突的梯度更新方向；其二是任务失衡，即各任务固有学习难度差异导致的优化不均衡问题。为应对这些挑战，我们提出一种任务自适应Transformer（TAT）框架，该创新框架通过两项关键技术实现动态任务适应。首先，我们引入任务自适应权重生成策略，通过为每个任务生成专属权重参数，消除共享权重参数上潜在的梯度冲突，从而缓解任务干扰。其次，我们提出任务自适应损失平衡策略，根据任务特定学习难度动态调整损失权重，防止某些任务主导训练过程或训练不足。大量实验表明，我们提出的TAT模型在PET合成、CT去噪和MRI超分辨率三项MedIR任务中，无论是针对特定任务还是一体化设置，均取得了最先进的性能。代码已发布于https://github.com/Yaziwel/TAT。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.14550">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.14550">arXiv</a></p>
<hr />
<h3>25. Zoom-Zero：通过时序局部放大实现从粗到细的强化视频理解</h3>
<p><strong>原文标题：</strong> Zoom-Zero: Reinforced Coarse-to-Fine Video Understanding via Temporal Zoom-in</p>
<p><strong>摘要：</strong>
基于视频的问答任务旨在定位视频中的相关时序片段并针对给定问题生成准确答案；然而，现有的大规模视频-语言模型在时序感知能力上存在局限。尽管基于群体相对策略优化的现有方法尝试改进时序定位效果，但其答案仍难以忠实锚定于相关视频证据，导致时序定位偏差与幻觉生成。本研究提出Zoom-Zero框架，采用从粗到细的处理流程：首先定位查询相关片段，继而通过时序放大机制聚焦于最具信息量的关键帧进行细粒度视觉验证。本方法通过两项关键创新突破群体相对策略优化在基于视频的问答任务中的局限：（1）引入放大精度奖励机制，验证时序定位预测的可靠性，并促进对定位帧的细粒度视觉验证；（2）设计基于词元的选择性信用分配策略，将奖励精准归因于负责时序定位或答案生成的关键词元，从而缓解群体相对策略优化在处理多维度奖励信号时的固有缺陷。实验表明，所提方法显著推进了基于视频的问答任务的发展：在NExT-GQA数据集上时序定位精度提升5.2%，在ReXTime数据集上提升4.6%，同时平均答案准确率提高2.4%。此外，推理过程中采用的从粗到细放大机制，能在保持全局上下文的前提下保留关键视觉细节，显著提升长视频理解能力，在长视频基准测试中平均获得6.4%的性能提升。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.14273">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.14273">arXiv</a></p>
<hr />
<h3>26. Efficient-DLM：从自回归到扩散语言模型，以及更快的速度</h3>
<p><strong>原文标题：</strong> Efficient-DLM: From Autoregressive to Diffusion Language Models, and Beyond in Speed</p>
<p><strong>摘要：</strong>
扩散语言模型（dLM）作为一种支持并行、非自回归生成的新兴范式展现出巨大潜力，但其从头开始训练时的学习效率仍落后于自回归（AR）语言模型。为此，我们研究AR到dLM的转换方法，旨在将预训练的AR模型转化为高效的dLM，在保持AR模型任务准确性的同时实现更快的生成速度。我们通过分析现有AR-to-dLM方法在注意力模式和训练目标上的局限性，提出了更有效的转换原则与方法。具体而言，我们首先系统比较了不同的注意力模式，发现保持预训练AR模型的权重分布对有效转换至关重要。因此，我们引入了一种采用块状注意力模式的持续预训练方案，该方案在块间保持因果性，同时在块内实现双向建模。我们发现，与完全双向建模相比，这种方法不仅能更好地保留预训练AR模型的权重分布，还具备已知的KV缓存优势，从而在准确性和效率上实现双赢。其次，为缓解掩码标记分布（训练时的均匀分布与推理时的高度从左到右分布）之间的训练-测试差距，我们提出了一种位置相关的标记掩码策略，在训练中对靠后的标记赋予更高的掩码概率，以更好地模拟推理时的行为。基于此框架，我们对dLM的注意力模式、训练动态及其他设计选择进行了广泛研究，为可扩展的AR-to-dLM转换提供了实用见解。这些研究最终形成了Efficient-DLM模型系列，其在性能上超越了当前最先进的AR模型和dLM。例如，我们的Efficient-DLM 8B模型相比Dream 7B和Qwen3 4B，在准确率上分别提升5.4%和2.7%，同时吞吐量提高4.5倍和2.7倍。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.14067">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.14067">arXiv</a></p>
<hr />
<h3>27. Janus：面向可扩展MoE推理的注意力与专家模块解耦系统</h3>
<p><strong>原文标题：</strong> Janus: Disaggregating Attention and Experts for Scalable MoE Inference</p>
<p><strong>摘要：</strong>
大规模专家混合模型推理因高资源需求与动态工作负载而面临挑战。现有方案通常将整个模型部署为单一整体单元，对注意力模块和专家模块采用统一的资源配置，忽视了二者不同的计算特性，导致可扩展性受限与资源利用率低下。本文提出Janus——一种可扩展的MoE推理系统，通过将注意力模块与专家模块解耦部署至独立的GPU子集群，实现各模块的独立管理与弹性扩展。Janus包含三项关键设计以实现高效解耦推理：首先，提出自适应两阶段通信机制，利用节点内与节点间带宽层级实现低延迟数据交换；其次，针对MoE模块的内存访问密集型特性，设计轻量级调度器并以GPU内核形式实现，以最小开销在GPU间均衡激活专家数量，从而降低推理延迟；最后，实施细粒度资源管理，动态调整专家分布并独立扩展注意力与MoE计算资源，提升整体效率。实验表明，在满足单令牌延迟要求的前提下，Janus相比现有最优系统可实现最高3.9倍的每GPU吞吐量提升。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.13525">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.13525">arXiv</a></p>
<hr />
<h3>28. RePo：基于上下文重定位的语言模型</h3>
<p><strong>原文标题：</strong> RePo: Language Models with Context Re-Positioning</p>
<p><strong>摘要：</strong>
上下文学习是现代大语言模型（LLM）的基础能力，然而主流架构通过分配线性或恒定的位置索引，强制设定了僵化且固定的上下文结构。基于认知负荷理论（CLT），我们认为这种缺乏信息量的结构会增加外部认知负荷，消耗本应用于深度推理与注意力分配的有限工作记忆容量。为解决这一问题，我们提出RePo——一种通过上下文重定位来降低外部认知负荷的新机制。与标准方法不同，RePo采用可微分模块 f_φ 来分配能够捕捉上下文依赖关系的词元位置，而非依赖预定义的整数范围。通过在OLMo-2 1B骨干模型上进行持续预训练，我们证明RePo在处理含噪声上下文、结构化数据及长上下文任务时性能显著提升，同时在通用短上下文任务上保持竞争力。深入分析表明，RePo能成功对远距离相关信息分配更高注意力，在稠密非线性空间中定位位置，并有效捕捉输入上下文的内在结构。代码已开源：https://github.com/SakanaAI/repo。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.14391">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.14391">arXiv</a></p>
<hr />
<h3>29. JMMMU-Pro：基于图像的日本多学科多模态理解基准与Vibe基准构建方法</h3>
<p><strong>原文标题：</strong> JMMMU-Pro: Image-based Japanese Multi-discipline Multimodal Understanding Benchmark via Vibe Benchmark Construction</p>
<p><strong>摘要：</strong>
本文提出JMMMU-Pro——一个基于图像的日本多学科多模态理解基准，以及可扩展的构建方法Vibe Benchmark Construction。延续从MMMU到MMMU-Pro的演进路径，JMMMU-Pro通过将问题图像与问题文本融合为单一图像的方式扩展了JMMMU基准，从而构建出需要借助视觉感知实现图文融合理解的评估体系。为构建JMMMU-Pro，我们提出了Vibe Benchmark Construction方法：该方法利用图像生成模型（如Nano Banana Pro）生成候选视觉问题，由人工验证输出结果并在必要时通过调整提示词重新生成，以此保障数据质量。借助Nano Banana Pro高度逼真的图像生成能力及其对日文文本的清晰嵌入特性，我们以较低成本构建了覆盖多样化背景与版式设计的高质量基准。实验结果表明，所有开源大型多模态模型在JMMMU-Pro上均表现欠佳，这凸显了该基准对开源社区未来发展的指导价值。我们相信JMMMU-Pro为评估大型多模态模型的日语能力提供了更严谨的工具，同时Vibe Benchmark Construction方法也为未来基于图像的视觉问答基准开发提供了高效的建设指南。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.14620">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.14620">arXiv</a></p>
<hr />
<h3>30. MobileWorldBench：面向移动智能体的语义世界建模研究</h3>
<p><strong>原文标题：</strong> MobileWorldBench: Towards Semantic World Modeling For Mobile Agents</p>
<p><strong>摘要：</strong>
世界模型在提升具身智能体任务性能方面展现出显著效用。现有研究主要集中于像素空间世界模型，但此类方法在图形用户界面环境中面临实际限制——预测未来状态中的复杂视觉元素往往较为困难。本研究探索了面向图形用户界面智能体的世界建模新范式，通过自然语言描述状态转换而非直接预测原始像素。首先，我们提出MobileWorldBench基准测试，用于评估视觉语言模型作为移动图形用户界面智能体世界模型的性能表现。其次，我们发布了包含140万样本的大规模数据集MobileWorld，该数据集显著提升了视觉语言模型的世界建模能力。最后，我们提出一种创新框架，将视觉语言世界模型集成到移动智能体的规划架构中，证明语义世界模型可通过提高任务成功率直接赋能移动智能体。代码与数据集已公开于https://github.com/jacklishufan/MobileWorld</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.14014">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.14014">arXiv</a></p>
<hr />
<h3>31. 大语言模型拒绝机制消除方法的比较分析：跨架构评估</h3>
<p><strong>原文标题：</strong> Comparative Analysis of LLM Abliteration Methods: A Cross-Architecture Evaluation</p>
<p><strong>摘要：</strong>
大语言模型中的安全对齐机制通过习得的拒绝行为防止对有害查询作出回应，但这些机制同时也阻碍了包括认知建模、对抗性测试与安全分析在内的合法研究应用。虽然消除技术能够通过定向正交化手术式移除拒绝表征，但现有实施方案的相对有效性尚未得到系统评估。本研究在十六个指令微调模型（70亿至140亿参数）上评估了四种消除工具（Heretic、DECCP、ErisForge、FailSpy），报告了全部16个模型的工具兼容性及工具支持子集的量化指标。在基准测试子集中，单次消除方法展现出更优的能力保持性（三个模型的GSM8K平均变化：ErisForge -0.28个百分点；DECCP -0.13个百分点），而贝叶斯优化消除则产生可变的分布偏移（KL散度：0.043-1.646）及模型依赖的能力影响。这些发现为研究者在不同模型架构中部署消除工具提供了基于证据的选择标准。核心结果表明，数学推理能力对消除干预表现出最高敏感性，GSM8K变化范围从+1.51个百分点到-18.81个百分点（相对变化-26.5%），具体取决于工具选择与模型架构。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.13655">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.13655">arXiv</a></p>
<hr />
<h3>32. TraPO：一种提升大语言模型推理能力的半监督强化学习框架</h3>
<p><strong>原文标题：</strong> TraPO: A Semi-Supervised Reinforcement Learning Framework for Boosting LLM Reasoning</p>
<p><strong>摘要：</strong>
基于可验证奖励的强化学习（RLVR）通过利用答案可验证信号指导策略优化，在训练大型推理模型（LRMs）方面已被证明是有效的，但这种方法通常面临高昂的标注成本。为缓解此问题，近期研究探索了无监督RLVR方法，仅通过模型内部一致性（如熵和多数投票）推导奖励。尽管这些方法看似前景良好，但在训练后期常出现模型崩溃现象，这可能源于缺乏外部监督时错误推理模式被强化。本研究提出一种新颖的半监督RLVR范式，利用少量标注样本来指导对未标注样本的RLVR训练。我们的核心观点是：监督奖励对于稳定基于一致性的未标注样本训练至关重要，可确保仅将在标注实例上验证过的推理模式纳入强化学习训练。技术上，我们提出一种高效的策略优化算法TraPO，该算法通过匹配未标注样本与标注样本的学习轨迹相似性来识别可靠的未标注样本。在此基础上，TraPO在六个广泛使用的数学推理基准（AIME24/25、AMC、MATH-500、Minerva和Olympiad）和三个分布外任务（ARC-c、GPQA-diamond和MMLU-pro）上实现了显著的数据效率和强大的泛化能力。仅使用1K标注样本和3K未标注样本，TraPO即达到42.6%的平均准确率，超越了在45K未标注样本上训练的最佳无监督方法（38.3%）。值得注意的是，当使用4K标注样本和12K未标注样本时，TraPO在所有基准测试中甚至优于使用全部45K标注样本训练的完全监督模型，而标注数据用量仅为其10%。代码可通过 https://github.com/ShenzhiYang2000/TRAPO 获取。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.13106">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.13106">arXiv</a></p>
<hr />
<h3>33. UAGLNet：基于CNN-Transformer协同机制的不确定性聚合全局-局部融合网络用于建筑物提取</h3>
<p><strong>原文标题：</strong> UAGLNet: Uncertainty-Aggregated Global-Local Fusion Network with Cooperative CNN-Transformer for Building Extraction</p>
<p><strong>摘要：</strong>
由于建筑物结构复杂多变，从遥感图像中提取建筑物是一项具有挑战性的任务。现有方法通常在分割模型中采用卷积或自注意力模块来捕获多尺度特征，但特征金字塔之间的固有差异以及全局与局部特征融合不足，导致提取结果存在不准确和模糊的问题。为解决这一问题，本文提出一种不确定性聚合的全局-局部融合网络（UAGLNet），该网络能够在不确定性建模的指导下有效利用高质量的全局与局部视觉语义。具体而言，我们设计了一种新型协同编码器，在不同阶段分别采用混合CNN层与Transformer层以捕获局部和全局视觉语义。同时设计了中间协同交互模块（CIB），以在网络加深时缩小局部与全局特征之间的差异。随后，我们提出全局-局部融合模块（GLF），以互补方式融合全局与局部特征表示。此外，为降低不确定区域的分割模糊性，本文提出不确定性聚合解码器（UAD），通过显式估计像素级不确定性来提升分割精度。大量实验表明，本方法相比现有先进方法具有更优越的性能。代码已开源：https://github.com/Dstate/UAGLNet</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.12941">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.12941">arXiv</a></p>
<hr />
<h3>34. S2D：基于稀疏到稠密关键掩码蒸馏的无监督视频实例分割方法</h3>
<p><strong>原文标题：</strong> S2D: Sparse-To-Dense Keymask Distillation for Unsupervised Video Instance Segmentation</p>
<p><strong>摘要：</strong>
近年来，无监督视频实例分割领域的最先进方法严重依赖于从ImageNet等以物体为中心的图像数据集生成的合成视频数据。然而，通过人工平移和缩放图像实例掩码生成的视频合成方法，难以准确模拟视频中真实的运动模式，例如视角变化、单个或多个实例部件的运动或相机运动。为解决这一问题，我们提出了一种完全基于真实视频数据训练的无监督视频实例分割模型。该方法从单帧视频的无监督实例分割掩码出发，但这些单帧分割结果存在时序噪声，且其质量在视频序列中波动较大。为此，我们通过利用深度运动先验识别视频中的高质量关键掩码，从而建立时序一致性。随后，这些稀疏的关键掩码伪标注被用于训练一个隐式掩码传播的分割模型，为此我们提出了一种结合时序丢弃损失（Temporal DropLoss）的稀疏到稠密蒸馏方法。在生成的稠密标签集上训练最终模型后，本方法在多项基准测试中均超越了当前最先进的技术水平。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.14440">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.14440">arXiv</a></p>
<hr />
<h3>35. 生成式AI时代用户感知的揭示：基于情感分析的AI教育应用在数字化教学转型中的作用评估</h3>
<p><strong>原文标题：</strong> Unveiling User Perceptions in the Generative AI Era: A Sentiment-Driven Evaluation of AI Educational Apps' Role in Digital Transformation of e-Teaching</p>
<p><strong>摘要：</strong>
生成式人工智能在教育领域的快速融合推动了数字化教学转型，但用户对AI教育应用的感知仍待深入探究。本研究通过对Google Play商店热门AI教育应用的用户评论进行情感驱动式评估，以考察其效能、挑战及教学启示。研究流程包括爬取应用数据与评论、使用RoBERTa进行二元情感分类、GPT-4o提取关键观点，以及GPT-5综合归纳积极/消极主题。应用被分为七类（如作业助手、数学解题工具、语言学习工具等），类别重叠反映了多功能设计趋势。结果显示用户情感以积极为主，其中作业类应用（如Edu AI积极率95.9%、Answer.AI积极率92.7%）在准确性、响应速度与个性化方面表现突出，而语言学习/LMS类应用（如Teacher AI积极率仅21.8%）因系统不稳定和功能局限评价较低。积极评价集中于激发创意、问题解决效率及学习参与度提升；消极评价则聚焦付费门槛、内容误差、广告干扰及技术故障。趋势表明，作业助手类应用表现优于专项工具，凸显了AI在促进教育普惠性的同时，也伴随着依赖性与公平性风险。讨论部分提出未来应发展人机协同混合模型、结合VR/AR的沉浸式学习生态，并为开发者（适应性个性化设计）和政策制定者（促进包容性的商业化监管）规划行动路径。本研究强调生成式AI可通过伦理化改进推动公平创新的教学环境，从而加速数字化教学转型进程。完整数据集详见：https://github.com/erfan-nourbakhsh/GenAI-EdSent。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.11934">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.11934">arXiv</a></p>
<hr />
<h3>36. 面向高质量数据共享的层次化数据集选择方法</h3>
<p><strong>原文标题：</strong> Hierarchical Dataset Selection for High-Quality Data Sharing</p>
<p><strong>摘要：</strong>
现代机器学习的成功依赖于高质量训练数据的获取。在许多实际场景中，例如从公共存储库获取数据或跨机构共享数据时，数据通常被组织成离散的数据集，这些数据集在相关性、质量和效用上存在差异。因此，选择搜索哪些存储库或机构以获取有用数据集，以及将哪些数据集纳入模型训练，是至关重要的决策。然而，现有方法大多仅选择单个样本，并将所有数据视为同等相关，忽略了数据集及其来源之间的差异。本研究形式化定义了数据集选择任务：从大规模异构数据池中选择完整的数据集，以在资源约束下提升下游任务性能。我们提出了基于层次结构的数据集选择方法，该方法在数据集和群组（如数据集合、机构）两个层面建模数据效用，从而能够从有限观测中实现高效泛化。在两个公开基准测试（Digit-Five 和 DomainNet）上，DaSH 在准确率上优于现有数据选择基线方法最高达 26.2%，同时所需探索步骤显著减少。消融实验表明，DaSH 在低资源环境和相关数据集缺失的情况下仍保持鲁棒性，使其适用于实际多源学习工作流程中可扩展且自适应的数据集选择。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.10952">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.10952">arXiv</a></p>
<hr />
<h3>37. MeViS：面向指代性运动表达视频分割的多模态数据集</h3>
<p><strong>原文标题：</strong> MeViS: A Multi-Modal Dataset for Referring Motion Expression Video Segmentation</p>
<p><strong>摘要：</strong>
本文提出一个面向指代性运动表达视频分割的大规模多模态数据集，其核心在于依据物体运动语言描述实现视频中目标物体的分割与追踪。现有指代视频分割数据集多聚焦于显著物体，且使用的语言表达富含静态属性特征，使得目标物体可能在单帧图像中即可被识别。此类数据集未能充分强调运动信息在视频与语言中的关键作用。为探索利用运动表达与运动推理线索实现像素级视频理解的可行性，我们提出了MeViS数据集，该数据集包含33,072条人工标注的文本与音频双模态运动表达，涵盖2,006个复杂场景视频中的8,171个目标物体。我们在MeViS支持的4项任务中对15种现有方法进行基准测试，包括6种指代视频目标分割（RVOS）方法、3种音频引导视频目标分割（AVOS）方法、2种指代多目标跟踪（RMOT）方法，以及针对新提出的指代性运动表达生成（RMEG）任务的4种视频描述生成方法。实验结果揭示了现有方法在处理运动表达引导的视频理解任务时存在的不足与局限。我们进一步分析了技术挑战，并提出一种面向RVOS/AVOS/RMOT任务的LMPM++方法，该方法取得了当前最优性能。本数据集为复杂视频场景中运动表达引导的视频理解算法研发提供了平台。MeViS数据集及相关方法源代码已公开于https://henghuiding.com/MeViS/。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.10945">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.10945">arXiv</a></p>
<hr />
<h3>38. CoSPlan：基于场景图增量更新的纠错式序列规划</h3>
<p><strong>原文标题：</strong> CoSPlan: Corrective Sequential Planning via Scene Graph Incremental Updates</p>
<p><strong>摘要：</strong>
大规模视觉语言模型（VLMs）展现出令人瞩目的复杂推理能力，但在视觉序列规划领域——即执行多步动作以实现目标——仍鲜有探索。此外，实际的序列规划常包含非最优（错误）步骤，这对VLM检测与纠正此类步骤的能力提出了挑战。我们提出纠错式序列规划基准（CoSPlan），用于评估VLM在四大领域易出错的视觉序列规划任务中的表现：迷宫导航、积木重排、图像重建和物体重组。CoSPlan评估两项关键能力：错误检测（识别非最优动作）与步骤补全（纠正并完成动作序列以实现目标）。尽管采用了思维链和场景图等先进推理技术，现有VLM（如Intern-VLM与Qwen2）在CoSPlan上表现仍不理想，难以有效利用上下文线索达成目标。为此，我们提出一种无需训练的新方法——场景图增量更新（SGI），该方法在初始状态与目标状态之间引入中间推理步骤。SGI能帮助VLM进行序列推理，平均性能提升达5.2%。除提升纠错式序列规划的可靠性外，SGI还可泛化至传统规划任务（如Plan-Bench和视觉问答任务）。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.10342">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.10342">arXiv</a></p>
<hr />
<h3>39. ContextAnyone：面向角色一致性的文本到视频生成中的上下文感知扩散方法</h3>
<p><strong>原文标题：</strong> ContextAnyone: Context-Aware Diffusion for Character-Consistent Text-to-Video Generation</p>
<p><strong>摘要：</strong>
文本到视频（T2V）生成技术发展迅速，但在不同场景中保持角色身份一致性仍是一个主要挑战。现有的个性化方法通常侧重于面部特征，却难以保留发型、服装和体型等更广泛的上下文线索，而这些线索对于视觉连贯性至关重要。本文提出ContextAnyone，一种上下文感知扩散框架，能够基于文本和单张参考图像实现角色一致性的视频生成。我们的方法联合重建参考图像并生成新的视频帧，使模型能够充分感知并利用参考信息。通过一种新颖的“强调-注意力”模块，参考信息被有效整合到基于DiT的扩散主干网络中，该模块选择性地增强参考感知特征并防止跨帧身份漂移。双重引导损失结合了扩散和参考重建目标以提升外观保真度，同时提出的Gap-RoPE位置嵌入技术将参考标记与视频标记分离，以稳定时序建模。实验表明，ContextAnyone在身份一致性和视觉质量上优于现有的参考到视频生成方法，能够在多样化的动作和场景中生成连贯且保持上下文信息的角色视频。项目页面：https://github.com/ziyang1106/ContextAnyone。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.07328">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.07328">arXiv</a></p>
<hr />
<h2>🔍 关键词云图</h2>
<p><img alt="关键词云图" src="../images/keywords_wordcloud.png" /></p>
<h2>📈 近期论文趋势</h2>
<p><img alt="论文趋势" src="../images/daily_papers.png" /></p>
<h2>🎙️ 语音播报</h2>
<ul>
<li><a href="../audio/2025-12-17_daily_papers.mp3">收听今日论文解读</a></li>
</ul>
<h2>📱 订阅渠道</h2>
<ul>
<li>GitHub: <a href="https://github.com/2404589803/hf-daily-paper-newsletter-chinese">hf-daily-paper-newsletter-chinese</a></li>
</ul>
    </div>
</body>
</html>