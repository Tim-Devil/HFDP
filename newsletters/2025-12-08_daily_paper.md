
# <img src="https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo.png" width="30"/> Hugging Face 2025-12-08 论文日报

## 📊 今日论文统计
- 总论文数：24
- 热门领域：RL, GPT, LLM

## 📝 论文详情


### 1. TwinFlow：基于自对抗流实现大模型单步生成

**原文标题：** TwinFlow: Realizing One-step Generation on Large Models with Self-adversarial Flows

**摘要：**
近年来，大规模多模态生成模型在多模态生成（包括图像与视频生成）方面展现出卓越能力。这类模型通常基于扩散模型或流匹配等多步生成框架构建，其固有的多步推理过程限制了生成效率（通常需要40-100次函数评估）。尽管已有多种少步生成方法旨在加速推理，但现有方案存在明显局限：基于蒸馏的主流方法（如渐进蒸馏与一致性蒸馏）需要迭代蒸馏过程，或在极少数步数（<4步）下出现显著性能衰退；而将对抗训练引入蒸馏过程的方法（如DMD/DMD2和SANA-Sprint）虽能提升性能，却因引入额外训练模型导致训练不稳定、复杂度增加及高昂的GPU内存开销。为此，我们提出TwinFlow——一种简单而有效的单步生成模型训练框架。该框架无需依赖固定的预训练教师模型，在训练过程中避免了传统对抗网络的使用，特别适合构建大规模高效生成模型。在文生图任务中，本方法在单步生成条件下取得0.83的GenEval分数，显著优于SANA-Sprint（基于GAN损失的框架）和RCGM（基于一致性的框架）等强基线模型。值得注意的是，我们通过对Qwen-Image-20B模型进行全参数训练，验证了TwinFlow框架的可扩展性，并将其转化为高效少步生成器。实验表明：在仅使用单步生成的情况下，该方法在GenEval和DPG-Bench基准测试中的表现与原始百步生成模型相当，在几乎保持生成质量的同时将计算成本降低至百分之一。项目页面详见：https://zhenglin-cheng.com/twinflow。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.05150) | [arXiv](https://arxiv.org/abs/2512.05150)



---

### 2. EditThinker：为任意图像编辑器解锁迭代推理能力

**原文标题：** EditThinker: Unlocking Iterative Reasoning for Any Image Editor

**摘要：**
基于指令的图像编辑已成为一个重要研究领域，该方法受益于图像生成基础模型，已能实现较高的美学质量，使得指令跟随能力成为当前的主要挑战。现有方法通过监督学习或强化学习来提升指令遵循度，但由于内在的随机性及缺乏深思熟虑的过程，单轮编辑的成功率仍然有限。本研究提出了一种深思型编辑框架，使模型在编辑过程中能够“思考”，通过迭代执行“边编辑边思考”的认知循环来模拟人类认知过程：即批判生成结果并优化指令，随后重复生成直至获得满意输出。具体而言，我们训练了一个单一的多模态大语言模型——EditThinker，作为该框架的推理引擎，联合生成批判评分、推理过程及优化后的指令。我们采用强化学习方法将EditThinker的思考过程与其编辑行为对齐，从而产生更具针对性的指令改进。在四个基准数据集上的大量实验表明，我们的方法能够显著提升任意图像编辑模型的指令跟随能力，且提升幅度显著。我们将公开数据构建框架、数据集及模型，以促进相关领域的发展。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.05965) | [arXiv](https://arxiv.org/abs/2512.05965)



---

### 3. 从模仿到判别：一种增强跨域推理任务的广义课程优势机制

**原文标题：** From Imitation to Discrimination: Toward A Generalized Curriculum Advantage Mechanism Enhancing Cross-Domain Reasoning Tasks

**摘要：**
强化学习已成为大语言模型后训练的一种范式，显著提升了其推理能力。此类方法为每个样本计算优势值，反映其表现优于或劣于预期的程度，从而为训练提供正向与负向信号。然而，现有方法中两种信号的混杂使用，尤其是在训练早期阶段，可能导致指导意义模糊且收益有限。为解决这一问题，我们提出 **CAPO**（**C**urriculum **A**dvantage **P**olicy **O**ptimization，课程优势策略优化），一种基于优势信号的自适应课程机制。该机制首先利用纯正向优势样本进行模仿学习，以建立稳健的基础，随后逐步引入负向信号以培养判别能力，从而提升模型在复杂场景中的泛化性能。本方法与包括GRPO、PPO、RLOO和Reinforce++在内的多种优化方法兼容，在数学推理任务中持续取得稳定且显著的性能提升，并进一步有效泛化至多模态图形用户界面（GUI）推理场景，成为一个通用且鲁棒的优化框架。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.02580) | [arXiv](https://arxiv.org/abs/2512.02580)



---

### 4. EMMA：一种高效多模态理解、生成与编辑的统一架构

**原文标题：** EMMA: Efficient Multimodal Understanding, Generation, and Editing with a Unified Architecture

**摘要：**
本文提出EMMA，一种面向多模态理解、生成与编辑任务的高效统一架构。该架构的核心创新包括：1）设计具有32倍压缩率的高效自编码器，显著降低生成任务所需的标记数量，同时通过对图像施加相同压缩率确保理解与生成任务的训练平衡；2）在视觉理解与生成标记之间采用通道级拼接而非标记级拼接，进一步减少统一架构中的视觉标记数量；3）构建共享解耦网络，在满足任务特定建模需求的同时实现跨任务协同优化；4）在视觉理解编码器中引入专家混合机制，以少量参数增长显著提升感知能力。大量实验表明，EMMA-4B在效率与性能上均显著优于当前先进统一多模态方法（如BAGEL-7B），同时相较于近期专用多模态理解与生成模型（如Qwen3-VL与Qwen-Image）也展现出竞争优势。我们相信EMMA为未来统一多模态架构的发展奠定了坚实基础。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.04810) | [arXiv](https://arxiv.org/abs/2512.04810)



---

### 5. PaCo-RL：通过成对奖励建模推进强化学习在一致性图像生成中的应用

**原文标题：** PaCo-RL: Advancing Reinforcement Learning for Consistent Image Generation with Pairwise Reward Modeling

**摘要：**
一致性图像生成要求在多幅图像中忠实保持身份、风格和逻辑连贯性，这对于故事叙述和角色设计等应用至关重要。由于缺乏捕捉视觉一致性的大规模数据集，以及建模人类感知偏好的复杂性，监督训练方法在此任务上面临困难。本文认为，强化学习通过使模型能够以无数据方式学习复杂且主观的视觉标准，提供了一种有前景的替代方案。为实现这一目标，我们提出了PaCo-RL，这是一个将专用一致性奖励模型与高效强化学习算法相结合的综合框架。其第一个组件PaCo-Reward是一种基于自动化子图配对构建的大规模数据集训练的成对一致性评估器，它通过生成式自回归评分机制进行评估，该机制通过任务感知指令和思维链推理得到增强。第二个组件PaCo-GRPO采用了一种新颖的解耦分辨率优化策略，显著降低了强化学习成本，同时结合了对数调制的多奖励聚合机制，确保了平衡稳定的奖励优化。在两个代表性子任务上的大量实验表明，PaCo-Reward显著提升了与人类视觉一致性感知的对齐程度，而PaCo-GRPO在提升训练效率和稳定性的同时，实现了最先进的一致性生成性能。这些结果共同凸显了PaCo-RL作为一种实用且可扩展的一致性图像生成解决方案的潜力。项目页面详见：https://x-gengroup.github.io/HomePage_PaCo-RL/。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.04784) | [arXiv](https://arxiv.org/abs/2512.04784)



---

### 6. SCAIL：通过三维一致性姿态表征的上下文学习实现影视级角色动画

**原文标题：** SCAIL: Towards Studio-Grade Character Animation via In-Context Learning of 3D-Consistent Pose Representations

**摘要：**
尽管近期取得进展，实现符合影视级制作标准的角色动画仍具挑战。现有方法可将驱动视频中的动作迁移至参考图像，但在涉及复杂运动与跨身份动画的开放场景中，常难以保持结构保真度与时间一致性。本研究提出SCAIL（基于上下文学习的影视级角色动画框架），该框架通过两项关键创新应对这些挑战：首先，我们提出一种新颖的三维姿态表征方法，提供更鲁棒灵活的运动信号；其次，我们在扩散-变换器架构中引入全上下文姿态注入机制，实现对完整运动序列的有效时空推理。为满足影视级标准，我们开发了兼顾多样性与质量的精选数据流程，并建立系统性评估的综合基准。实验表明，SCAIL实现了最先进的性能表现，将角色动画向影视级可靠性与真实感推进。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.05905) | [arXiv](https://arxiv.org/abs/2512.05905)



---

### 7. 熵比裁剪作为一种软性全局约束用于稳定强化学习

**原文标题：** Entropy Ratio Clipping as a Soft Global Constraint for Stable Reinforcement Learning

**摘要：**
大语言模型的后训练依赖于强化学习来提升模型能力与对齐质量。然而，离策略训练范式会引入分布偏移，这往往使策略超出置信区域，导致训练不稳定，表现为策略熵的波动与梯度不稳定。尽管PPO-Clip通过重要性采样裁剪缓解了这一问题，但其仍忽视了动作的全局分布偏移。为应对这些挑战，我们提出使用当前策略与先前策略之间的熵比作为新的全局度量指标，该指标能有效量化策略在更新过程中探索行为的相对变化。基于此度量，我们引入了熵比裁剪机制，对熵比施加双向约束。这能在全局分布层面稳定策略更新，并弥补PPO-clip无法调节未采样动作概率偏移的不足。我们将ERC机制整合至DAPO与GPPO强化学习算法中。在多基准测试上的实验表明，ERC能持续提升算法性能。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.05591) | [arXiv](https://arxiv.org/abs/2512.05591)



---

### 8. 基于单张图像的4D合成：联合三维几何重建与运动生成

**原文标题：** Joint 3D Geometry Reconstruction and Motion Generation for 4D Synthesis from a Single Image

**摘要：**
从单张静态图像生成具有交互性与动态性的4D场景仍是核心挑战。现有"先生成后重建"与"先重建后生成"方法大多将几何结构与运动解耦，导致时空不一致性与泛化能力不足。为解决这些问题，本研究扩展"先重建后生成"框架，提出面向4D合成的运动生成与几何重建联合方法（MoRe4D）。我们首先构建TrajScene-60K大规模数据集，包含6万个具有密集点轨迹的视频样本，以缓解高质量4D场景数据稀缺问题。基于此，提出基于扩散模型的4D场景轨迹生成器（4D-STraG），能够联合生成几何一致且运动合理的4D点轨迹。为利用单视图先验信息，设计了深度引导的运动归一化策略与运动感知模块，实现几何与动态特征的有效融合。进一步提出4D视角合成模块（4D-ViSM），可从4D点轨迹表征渲染任意相机轨迹的视频。实验表明，MoRe4D能够从单张图像生成具有多视角一致性与丰富动态细节的高质量4D场景。代码地址：https://github.com/Zhangyr2022/MoRe4D。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.05044) | [arXiv](https://arxiv.org/abs/2512.05044)



---

### 9. COOPER：空间智能中协同感知与推理的统一模型

**原文标题：** COOPER: A Unified Model for Cooperative Perception and Reasoning in Spatial Intelligence

**摘要：**
视觉空间推理对于使多模态大语言模型理解物体属性与空间关系至关重要，然而现有模型在三维感知推理方面仍面临挑战。当前方法通常通过两种独立路径进行增强：一是在感知层面，通过为RGB输入添加深度与分割等辅助模态；二是在推理层面，通过空间视觉问答数据集训练并结合强化学习。本研究探讨统一的多模态大语言模型能否通过自适应交错推理机制，发展出增强空间感知的内在能力，从而实现更强大的空间智能。我们提出COOPER模型，该统一框架以深度与分割作为辅助模态，通过两阶段训练获得辅助模态生成能力与自适应交错推理能力。实验表明，COOPER在保持通用性能的同时，空间推理任务平均提升6.91%。值得注意的是，仅进行辅助模态生成训练的变体模型在距离与尺寸估计任务上亦获得7.92%的性能增益，这证实学习生成辅助模态有助于模型内化空间知识并强化空间理解能力。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.04563) | [arXiv](https://arxiv.org/abs/2512.04563)



---

### 10. RealGen：基于检测器引导奖励的逼真文本到图像生成

**原文标题：** RealGen: Photorealistic Text-to-Image Generation via Detector-Guided Rewards

**摘要：**
随着图像生成技术的持续进步，GPT-Image-1与Qwen-Image等先进模型已在文本-图像一致性与世界知识掌握方面取得显著成果。然而，这些模型在生成逼真图像方面仍存在不足。即使在简单的文本到图像任务中，它们也倾向于生成带有明显人工智能痕迹的“虚假”图像，常表现为“过度光滑的皮肤”与“油光满面的面部光泽”。为重新实现“以假乱真”的生成目标，我们提出RealGen——一个逼真的文本到图像生成框架。RealGen整合了用于提示词优化的大型语言模型组件与用于逼真图像生成的扩散模型。受对抗生成思想启发，RealGen引入“检测器奖励”机制，通过语义级与特征级的合成图像检测器量化人工痕迹并评估真实感。我们结合GRPO算法利用该奖励信号优化整个生成流程，显著提升了图像的真实感与细节表现。此外，我们提出RealBench自动化评估基准，采用检测器评分与竞技场评分机制，实现了无需人工参与的逼真度评估，其评估结果更精准且符合真实用户体验。实验表明，在真实感、细节与美学维度上，RealGen显著优于GPT-Image-1、Qwen-Image等通用模型以及FLUX-Krea等专业逼真生成模型。代码已开源：https://github.com/yejy53/RealGen。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.00473) | [arXiv](https://arxiv.org/abs/2512.00473)



---

### 11. 无需人工标注的自改进视觉语言模型评判器

**原文标题：** Self-Improving VLM Judges Without Human Annotations

**摘要：**
有效的视觉语言模型评判器对于模型开发至关重要。当前训练VLM评判器的方法主要依赖于大规模人工偏好标注。然而，这种方法成本高昂，且随着模型快速迭代，标注数据极易过时。本研究提出一种无需任何人工偏好标注、仅使用自合成数据的VLM评判器自训练框架。该方法采用迭代式三阶段流程：（1）生成具有不同质量层次的多模态指令-响应对；（2）为每对数据生成推理轨迹与评判结果，并剔除不符合预期质量水平的数据；（3）基于正确的评判答案及其推理轨迹进行训练。我们在多模态奖励基准测试和VL奖励基准测试中，从正确性、偏好性、推理能力、安全性和视觉问答五个领域评估所得评判器。实验表明，该方法将Llama-3.2-11B多模态评判器在VL奖励基准测试中的总体准确率从0.38提升至0.51，在通用性、幻觉抑制和推理维度表现尤为突出，其性能常超越包括Llama-3.2-90B、GPT-4o和Claude 3.5 Sonnet在内的更大规模模型。这些无需人工标注的实验结果整体表明，未来有望构建出能随VLM能力快速进化而同步发展的自演进评判系统。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.05145) | [arXiv](https://arxiv.org/abs/2512.05145)



---

### 12. 自知其不知的世界模型：基于校准不确定性的可控视频生成

**原文标题：** World Models That Know When They Don't Know: Controllable Video Generation with Calibrated Uncertainty

**摘要：**
生成式视频模型的最新进展推动了高保真视频合成领域的重大突破，特别是在可控视频生成方面——生成的视频能够以文本和动作输入为条件，例如在指令引导的视频编辑和机器人世界建模中。尽管具备卓越能力，可控视频模型常出现幻觉现象，即生成的未来视频帧与物理现实不符，这在机器人策略评估与规划等任务中引发严重关切。然而，现有先进视频模型缺乏评估和表达自身置信度的能力，阻碍了幻觉缓解的进程。为系统应对这一挑战，我们提出C3方法：一种不确定性量化技术，用于训练连续尺度校准的可控视频模型，实现亚区块级别的密集置信度估计，精准定位每帧生成视频中的不确定性区域。该方法通过三大核心创新赋能视频模型的不确定性估计：首先，构建基于严格恰当评分规则的新型训练框架，使视频模型同时优化正确性与校准度；其次，在潜在空间估计视频模型的不确定性，规避像素空间方法存在的训练不稳定性和过高计算成本；最后，将密集的潜在空间不确定性映射至RGB空间的可解释像素级不确定性，通过高分辨率不确定性热力图直观标识不可信区域，实现可视化分析。基于大规模机器人学习数据集（Bridge与DROID）的广泛实验及现实场景评估表明，该方法不仅能在训练分布内提供校准的不确定性估计，还能实现有效的分布外检测。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.05927) | [arXiv](https://arxiv.org/abs/2512.05927)



---

### 13. SpaceControl：在三维生成建模中引入测试时空间控制

**原文标题：** SpaceControl: Introducing Test-Time Spatial Control to 3D Generative Modeling

**摘要：**
三维资产的生成方法近年来取得了显著进展，但如何对物体几何形态提供直观且精确的控制仍是关键挑战。现有方法主要依赖文本或图像提示，这些方式在几何特异性上往往存在不足：语言描述可能具有模糊性，而图像编辑则过程繁琐。本研究提出SpaceControl，一种无需训练的测试时方法，用于实现三维生成的显式空间控制。我们的方法能够接受从粗糙几何基元到精细网格的多种几何输入，并可无缝集成于现代预训练生成模型，无需任何额外训练。通过可控参数，用户可在几何保真度与输出真实感之间进行权衡。大量定量评估与用户研究表明，SpaceControl在保持高视觉质量的同时，其几何忠实度优于基于训练和基于优化的基线方法。最后，我们开发了交互式用户界面，支持在线编辑超二次曲面并直接转换为带纹理的三维资产，为创意工作流程的实际应用提供便利。项目页面详见：https://spacecontrol3d.github.io/

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.05343) | [arXiv](https://arxiv.org/abs/2512.05343)



---

### 14. ReVSeg：基于强化学习的推理链激励视频分割方法

**原文标题：** ReVSeg: Incentivizing the Reasoning Chain for Video Segmentation with Reinforcement Learning

**摘要：**
以推理为核心的视频目标分割本质上是一项复杂任务：查询通常涉及动态变化、因果关系与时间交互，而非静态外观特征。然而现有解决方案往往将这些因素压缩为基于隐式嵌入的简化推理，导致推理链不透明且难以追溯。为此，我们采用显式分解视角提出ReVSeg模型，该模型在预训练视觉语言模型的原生交互界面中，将推理过程转化为序列化决策执行。区别于将全部推理折叠为单步预测的传统方法，ReVSeg通过语义解析、时序证据筛选和空间定位三个显式操作步骤，实现对预训练模型能力的系统性对齐。我们进一步采用强化学习优化多步推理链，使模型能够根据结果驱动信号自主优化决策质量。实验结果表明，ReVSeg在标准视频目标分割基准测试中达到最先进性能，并生成可解释的推理轨迹。项目页面详见：https://clementine24.github.io/ReVSeg/。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.02835) | [arXiv](https://arxiv.org/abs/2512.02835)



---

### 15. 人工智能与人类协同进化以实现更安全的共同超级智能

**原文标题：** AI & Human Co-Improvement for Safer Co-Superintelligence

**摘要：**
自我改进是当前人工智能领域备受关注的目标，但其过程充满风险，且可能需要较长时间才能完全实现。我们认为对人类而言，更可实现且更优的目标是实现协同进化的最大化：即人类研究者与人工智能系统通过协作达成共同超级智能。具体而言，应着力提升人工智能系统与人类研究者协同开展人工智能研究的能力——从构想到实验的全过程——从而既加速人工智能研究进展，又通过人机共生机制赋予双方更安全的超级智能。将人类研究能力的提升纳入这一循环体系，将使我们以更快、更安全的方式实现该目标。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.05356) | [arXiv](https://arxiv.org/abs/2512.05356)



---

### 16. M3DR：迈向通用多语言多模态文档检索

**原文标题：** M3DR: Towards Universal Multilingual Multimodal Document Retrieval

**摘要：**
多模态文档检索系统在对齐视觉与文本内容以进行语义搜索方面已取得显著进展。然而，现有方法大多仍以英语为中心，限制了其在多语言环境中的有效性。本研究提出M3DR（多语言多模态文档检索）框架，旨在跨越语言鸿沟，使其能够适用于多样化的语言及文化场景。M3DR利用合成多语言文档数据，可泛化至不同的视觉-语言架构与模型规模，从而实现鲁棒的跨语言与跨模态对齐。通过对比训练，我们的模型能够学习文本与文档图像的统一表征，并有效迁移至不同语言。我们在22种类型各异的语言上验证了这一能力，证明模型在不同语言及文字变体中均具有稳定的性能与适应性。此外，我们引入了一个涵盖真实多语言场景的综合基准，在单语、多语及混合语言设置下评估模型性能。M3DR可同时适用于单稠密向量与ColBERT风格的令牌级多向量检索范式。我们提出的NetraEmbed与ColNetraEmbed模型实现了最先进的性能，在跨语言检索任务上相对性能提升约150%。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.03514) | [arXiv](https://arxiv.org/abs/2512.03514)



---

### 17. 主动视频感知：面向智能体长视频理解的迭代式证据搜寻

**原文标题：** Active Video Perception: Iterative Evidence Seeking for Agentic Long Video Understanding

**摘要：**
长视频理解（LVU）面临巨大挑战，因为回答现实世界中的查询往往依赖于隐藏在数小时冗余且无关内容中的稀疏、时间分散的线索。尽管智能体流程提升了视频推理能力，但主流框架依赖与查询无关的通用描述器来感知视频信息，这既浪费计算资源处理无关内容，又模糊了细粒度的时间与空间信息。受主动感知理论启发，我们认为LVU智能体应主动决定观察的内容、时机与位置，并持续评估当前观察是否足以回答查询。本文提出主动视频感知（AVP），这是一个将视频视为交互环境、直接从像素中获取紧凑且与查询相关证据的搜寻框架。具体而言，AVP通过多模态大语言模型智能体运行迭代式的“计划-观察-反思”流程：每一轮中，规划器提出有针对性的视频交互指令，观察器执行指令以提取带时间戳的证据，反思器则评估证据对回答查询的充分性，从而决定是终止流程并给出答案，还是触发进一步观察。在五个LVU基准测试中，AVP均取得了最高性能，且提升显著。值得注意的是，AVP在平均准确率上优于现有最佳智能体方法5.7%，同时仅需18.4%的推理时间和12.4%的输入令牌量。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.05774) | [arXiv](https://arxiv.org/abs/2512.05774)



---

### 18. 从片段到场景：基于视觉语言模型的自动驾驶时序理解研究

**原文标题：** From Segments to Scenes: Temporal Understanding in Autonomous Driving via Vision-Language Model

**摘要：**
自动驾驶中的时序理解仍然是重大挑战，即使对于当前最先进的视觉语言模型亦是如此。先前研究虽已引入旨在提升时序推理能力的数据集与基准测试，但其重点多集中于体育、烹饪、电影等其他视频内容领域，尚无专门针对以自我为中心视角的自动驾驶视频时序理解特有问题构建的基准测试。为填补这一空白，本研究提出自动驾驶时序理解基准测试，用于评估视觉语言模型捕捉自动驾驶场景中动作间动态关系的能力。该基准包含近6000组问答对，涵盖7项人工设计的任务。此外，我们对9个开源与闭源的通用模型以及最先进的自动驾驶专用模型进行了系统性评估。实验表明，当前最先进模型在该基准测试中表现欠佳，主要归因于其对细粒度运动理解存在不足。为提升运动理解能力及整体测试准确率，本文提出两种无需训练的创新解决方案：基于思维链的场景推理方法，以及融合自我中心时序认知地图的时序认知映射方法。将所提方法与现有视觉语言模型结合后，在自动驾驶时序理解基准上的平均准确率最高提升17.72%。通过构建该基准测试、评估多类先进模型并提出有效增强方法，本研究旨在推动自动驾驶时序理解领域的后续研究。基准测试数据与评估代码已分别发布于https://huggingface.co/datasets/vbdai/TAD与https://github.com/vbdi/tad_bench。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.05277) | [arXiv](https://arxiv.org/abs/2512.05277)



---

### 19. ProPhy：面向动态世界模拟的渐进式物理对齐框架

**原文标题：** ProPhy: Progressive Physical Alignment for Dynamic World Simulation

**摘要：**
视频生成领域的最新进展在构建世界模拟器方面展现出显著潜力。然而，当前模型在生成物理一致性结果方面仍存在困难，特别是在处理大规模或复杂动态场景时。这一局限主要源于现有方法对物理提示的响应呈各向同性，且忽视了生成内容与局部物理线索之间的细粒度对齐。为应对这些挑战，我们提出ProPhy——一种渐进式物理对齐框架，能够实现显式的物理感知条件化与各向异性生成。ProPhy采用两阶段物理专家混合机制进行判别式物理先验提取：语义专家从文本描述中推断语义级物理原理，细化专家则捕捉词元级物理动态。该机制使模型能够学习细粒度的物理感知视频表征，从而更好地反映底层物理规律。此外，我们提出一种物理对齐策略，将视觉语言模型的物理推理能力迁移至细化专家模块，以提升动态物理现象的表征精度。在物理感知视频生成基准上的大量实验表明，ProPhy相比现有先进方法能生成更真实、动态且物理连贯的结果。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.05564) | [arXiv](https://arxiv.org/abs/2512.05564)



---

### 20. SQ-format：一种面向大语言模型的统一稀疏量化硬件友好数据格式

**原文标题：** SQ-format: A Unified Sparse-Quantized Hardware-friendly Data Format for LLMs

**摘要：**
训练后量化（PTQ）在大语言模型（LLM）的普及中起着至关重要的作用。然而，由于硬件支持有限，现有的低位量化和稀疏化技术难以在精度与效率之间取得平衡。例如，W4A8量化仅能实现与W8A8相当的峰值TOPS，而GPU支持的稀疏数据格式（如2:4半结构化稀疏）则因精度损失问题较少被采用。为弥合这一差距，本文提出稀疏量化格式（SQ-format），这是一种统一的量化与稀疏化数据格式，可被新型硬件及现有GPU潜在支持。SQ-format基于稀疏矩阵可在高精度下加速、而低精度矩阵乘法亦可相应加速的特性，旨在实现性能与吞吐量的帕累托改进。该格式特别适用于具有异常值非均衡分布的激活张量，并使其静态压缩成为可能。我们展示了SQ-format在训练后量化中的先进性能，提出了支持该格式所需的硬件架构，并进一步为下一代AI加速器提供了设计探索与前瞻洞见。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.05409) | [arXiv](https://arxiv.org/abs/2512.05409)



---

### 21. TimesNet-Gen：基于深度学习的场地特定强震动生成方法

**原文标题：** TimesNet-Gen: Deep Learning-based Site Specific Strong Motion Generation

**摘要：**
有效的地震风险降低依赖于准确的场地特定评估，这需要能够表征局部场地条件对地震动特征影响的模型。在此背景下，从记录的地震动中学习场地控制特征的数据驱动方法提供了一个有前景的研究方向。本文针对时域加速度计记录进行强震动生成，提出了TimesNet-Gen——一种时域条件生成器。该方法采用站点特定的潜在瓶颈结构进行评估。我们通过比较各站点真实记录与生成记录的HVSR曲线及场地基本频率f_0分布来评估生成效果，并基于f_0分布的混淆矩阵计算得分以量化站点特异性。TimesNet-Gen在站点层面展现出高度一致性，在场地特定强震动合成任务中优于基于频谱图的条件变分自编码器基线模型。相关代码已发布于https://github.com/brsylmz23/TimesNet-Gen。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.04694) | [arXiv](https://arxiv.org/abs/2512.04694)



---

### 22. Colon-X：从多模态理解到临床推理的智能结肠镜技术演进

**原文标题：** Colon-X: Advancing Intelligent Colonoscopy from Multimodal Understanding to Clinical Reasoning

**摘要：**
本研究提出Colon-X开放计划，旨在推动结肠镜多模态智能技术的发展。我们首先构建了ColonVQA——迄今为止最全面的结肠镜多模态数据集，涵盖76种临床发现和18项多模态任务，包含超过110万条视觉问答数据。该数据集不仅为学界提供基础数据资源，我们进一步深入研究了结肠镜领域关键但尚未充分探索的转型路径——从多模态理解向临床推理的演进：（a）为评估当前多模态理解能力的发展现状，我们系统测试了22个多模态大语言模型的泛化性能，并考察其在人工干预扰动下的可靠性。结果表明，当前主流MLLM模型的临床输出仍远未达到稳健可信的标准。（b）为弥合这一差距，我们进一步探索面向结肠镜的推理核心智能技术。具体而言，我们构建了基于临床实践的推理数据集ColonReason（通过多专家辩论流程完成标注），并开发了首款R1架构模型ColonR1——该模型融合任务自适应奖励机制与梯度稳定优化技术。在数据稀缺条件下，ColonR1实现了56.61%的综合准确率，较监督微调方法提升25.22%，为多模态结肠镜分析建立了全新的推理能力基准。所有数据与模型资源已通过https://github.com/ai4colonoscopy/Colon-X开源发布。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.03667) | [arXiv](https://arxiv.org/abs/2512.03667)



---

### 23. 从浮点运算到资源足迹：人工智能的资源成本

**原文标题：** From FLOPs to Footprints: The Resource Cost of Artificial Intelligence

**摘要：**
随着计算需求持续攀升，评估人工智能的环境足迹需要超越能耗与水耗，将专用硬件的材料需求纳入考量。本研究通过关联计算负载与物理硬件需求，量化了人工智能训练的材料足迹。采用电感耦合等离子体发射光谱法对英伟达A100 SXM 40 GB图形处理器（GPU）进行元素分析，共检测出32种元素。结果表明，AI硬件约90%由重金属构成，贵金属含量仅为痕量。按质量计，铜、铁、锡、硅和镍是GPU的主要组成元素。通过多步骤研究方法，我们将这些测量数据与不同使用周期内单GPU的计算吞吐量相结合，并纳入不同训练效率模式下特定AI模型的训练计算需求。基于情景的分析显示，根据模型浮点运算利用率（MFU）与硬件使用寿命，训练GPT-4需要1,174至8,800块A100 GPU，对应最高达7吨有毒元素的开采与最终废弃。软硬件协同优化策略可降低材料需求：将MFU从20%提升至60%可使GPU需求量减少67%，而将使用寿命从1年延长至3年可实现同等幅度的节约；同时实施这两项措施最多可降低93%的GPU需求。我们的研究结果表明，如GPT-3.5到GPT-4所体现的渐进式性能提升，伴随着不成比例的高昂材料成本。本研究强调必须将材料资源考量纳入人工智能可扩展性的讨论，并指出未来AI发展必须符合资源效率与环境责任原则。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.04142) | [arXiv](https://arxiv.org/abs/2512.04142)



---

### 24. 具有鲁棒护栏的适应性分类大语言模型内容审核系统

**原文标题：** Taxonomy-Adaptive Moderation Model with Robust Guardrails for Large Language Models

**摘要：**
大语言模型通常在训练后阶段进行安全性对齐，但仍可能生成不当输出，对用户构成潜在风险。这一挑战凸显了在模型输入和输出两端建立鲁棒安全防护机制的必要性。本研究提出Roblox Guard 1.0模型——一种基于指令微调的前沿大语言模型，通过构建多阶段大语言模型处理流程，实现输入输出的全面内容审核以增强系统安全性。该模型以Llama-3.1-8B-Instruct为基座，通过指令微调使其能够泛化至未见过的安全分类体系，并在领域外安全基准测试中表现出卓越性能。指令微调过程融合了合成与开源安全数据集，并采用思维链推理机制及输入反转技术以增强上下文理解与决策能力。为支持系统性评估，我们同步发布RobloxGuard-Eval基准测试集，该数据集具备可扩展的安全分类体系，专门用于评估大语言模型护栏机制与内容审核框架的有效性。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.05339) | [arXiv](https://arxiv.org/abs/2512.05339)



---


## 🔍 关键词云图
![关键词云图](../images/keywords_wordcloud.png)

## 📈 近期论文趋势
![论文趋势](../images/daily_papers.png)

## 🎙️ 语音播报
- [收听今日论文解读](../audio/2025-12-08_daily_papers.mp3)

## 📱 订阅渠道
- GitHub: [hf-daily-paper-newsletter-chinese](https://github.com/2404589803/hf-daily-paper-newsletter-chinese)