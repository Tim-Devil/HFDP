<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Hugging Face 论文日报 - 2026-01-26</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', sans-serif;
            line-height: 1.6;
            color: #333;
            background-color: #f5f5f5;
            padding: 20px;
        }
        
        .container {
            max-width: 900px;
            margin: 0 auto;
            background-color: white;
            padding: 40px;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }
        
        h1 {
            color: #2c3e50;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
            margin-bottom: 30px;
            font-size: 28px;
        }
        
        h1 img {
            vertical-align: middle;
            margin-right: 10px;
        }
        
        h2 {
            color: #34495e;
            margin-top: 40px;
            margin-bottom: 20px;
            font-size: 24px;
            border-left: 4px solid #3498db;
            padding-left: 15px;
        }
        
        h3 {
            color: #2c3e50;
            margin-top: 30px;
            margin-bottom: 15px;
            font-size: 20px;
        }
        
        p {
            margin-bottom: 15px;
            text-align: justify;
        }
        
        ul {
            margin-left: 20px;
            margin-bottom: 20px;
        }
        
        li {
            margin-bottom: 8px;
        }
        
        a {
            color: #3498db;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        strong {
            color: #2c3e50;
            font-weight: 600;
        }
        
        hr {
            border: none;
            border-top: 1px solid #e0e0e0;
            margin: 30px 0;
        }
        
        /* 关键修复:限制图片宽度 */
        img {
            max-width: 100%;
            height: auto;
            display: block;
            margin: 20px auto;
            border-radius: 4px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        
        /* 确保图片容器也有宽度限制 */
        p img {
            max-width: 100%;
        }
        
        /* 论文详情区域样式 */
        .paper-section {
            background-color: #f9f9f9;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 20px;
        }
        
        /* 统计信息样式 */
        .stats {
            background-color: #e8f4f8;
            padding: 15px;
            border-radius: 6px;
            margin-bottom: 20px;
        }
        
        /* 响应式设计 */
        @media (max-width: 768px) {
            .container {
                padding: 20px;
            }
            
            h1 {
                font-size: 24px;
            }
            
            h2 {
                font-size: 20px;
            }
            
            h3 {
                font-size: 18px;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <h1><img src="https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo.png" width="30"/> Hugging Face 2026-01-26 论文日报</h1>
<h2>📊 今日论文统计</h2>
<ul>
<li>总论文数：18</li>
<li>热门领域：LLM, RL, Transformer, GPT</li>
</ul>
<h2>📝 论文详情</h2>
<h3>1. LongCat-Flash-Thinking-2601技术报告</h3>
<p><strong>原文标题：</strong> LongCat-Flash-Thinking-2601 Technical Report</p>
<p><strong>摘要：</strong>
本文介绍LongCat-Flash-Thinking-2601，这是一个拥有5600亿参数的开源专家混合推理模型，具备卓越的智能体推理能力。该模型在广泛的智能体基准测试中实现了开源模型的领先性能，涵盖智能体搜索、智能体工具使用及工具集成推理等领域。除基准测试表现外，该模型在复杂工具交互场景中展现出强大的泛化能力，并在真实世界的噪声环境中保持稳健性能。其先进能力源于统一的训练框架，该框架将领域并行专家训练与后续融合相结合，并实现了从预训练到后训练阶段数据构建、环境配置、算法设计及基础设施的端到端协同优化。特别地，模型在复杂工具使用场景中的强泛化能力得益于我们对环境扩展和原则性任务构建的深入探索。为优化长尾分布、偏态生成和多轮智能体交互，并在涵盖20余个领域的超10000个环境中实现稳定训练，我们系统性地扩展了异步强化学习框架DORA，以支持稳定高效的大规模多环境训练。此外，针对真实世界任务固有的噪声特性，我们系统分析并解构了现实噪声模式，设计了针对性训练流程，将此类不完美因素显式纳入训练过程，从而显著提升了模型在实际应用中的鲁棒性。为进一步增强复杂推理任务性能，我们引入了深度思考模式，通过密集并行思维协同扩展推理深度与宽度，实现了有效的测试时性能扩展。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.16725">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.16725">arXiv</a></p>
<hr />
<h3>2. SWE-Pruner：面向代码智能体的自适应上下文剪枝框架</h3>
<p><strong>原文标题：</strong> SWE-Pruner: Self-Adaptive Context Pruning for Coding Agents</p>
<p><strong>摘要：</strong>
大语言模型智能体在软件开发中展现出卓越能力，但其性能常受限于冗长的交互上下文，导致高昂的API成本与响应延迟。尽管已有诸如LongLLMLingua等上下文压缩方法应对此挑战，但这些方法通常依赖困惑度等固定指标，忽视了代码理解任务特有的本质，往往破坏代码的语法逻辑结构并丢失关键实现细节。本文提出SWE-Pruner——一个专为代码智能体设计的自适应上下文剪枝框架。该框架借鉴人类程序员在开发调试过程中“选择性浏览”源代码的认知机制，针对长上下文执行任务感知的自适应剪枝。智能体根据当前任务生成明确目标指引（如“聚焦错误处理机制”）作为剪枝导向，通过训练一个轻量级神经浏览模型（0.6B参数）动态筛选与目标相关的上下文代码行。在四个基准测试集和多种模型上的实验表明，SWE-Pruner能在各类场景中保持有效性：在SWE-Bench Verified等智能体任务上实现23-54%的令牌缩减，在LongCodeQA等单轮任务中达到最高14.84倍压缩率，且对性能影响微乎其微。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.16746">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.16746">arXiv</a></p>
<hr />
<h3>3. TwinBrainVLA：通过非对称混合变换器释放通用视觉语言模型在具身任务中的潜力</h3>
<p><strong>原文标题：</strong> TwinBrainVLA: Unleashing the Potential of Generalist VLMs for Embodied Tasks via Asymmetric Mixture-of-Transformers</p>
<p><strong>摘要：</strong>
标准的视觉-语言-动作模型通常通过显式微调一个单一的视觉语言模型主干来适应机器人控制。然而，这种方法在保持高层通用语义理解与学习低层、细粒度感知运动技能之间产生了关键矛盾，常常导致模型对开放世界能力的“灾难性遗忘”。为解决这一冲突，我们提出了TwinBrainVLA，这是一种新颖的架构，它协调了一个保留通用语义理解的通用视觉语言模型和一个专用于具身本体感知的专用视觉语言模型，以实现联合机器人控制。TwinBrainVLA通过一种新颖的非对称混合变换器机制，将保持强大通用视觉推理能力的冻结“左脑”与专精于具身感知的可训练“右脑”协同工作。该设计使得右脑能够动态地从冻结的左脑查询语义知识，并将其与本体感知状态融合，从而为流匹配动作专家提供丰富的条件信息，以生成精确的连续控制。在SimplerEnv和RoboCasa基准测试上的大量实验表明，与现有先进基线相比，TwinBrainVLA实现了更优的操作性能，同时明确保留了预训练视觉语言模型的全面视觉理解能力，为构建同时具备高层语义理解和低层物理灵巧性的通用机器人提供了一个有前景的方向。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.14133">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.14133">arXiv</a></p>
<hr />
<h3>4. VisGym：面向多模态智能体的多样化、可定制、可扩展环境</h3>
<p><strong>原文标题：</strong> VisGym: Diverse, Customizable, Scalable Environments for Multimodal Agents</p>
<p><strong>摘要：</strong>
现代视觉-语言模型在多步骤视觉交互中的表现仍缺乏深入刻画，尤其是在其如何整合长时程的感知、记忆与行动方面。本研究提出VisGym，一个包含17种环境的测试平台，用于评估和训练视觉-语言模型。该套件涵盖符号推理、真实图像理解、导航与操作任务，并提供对任务难度、输入表征、规划时域和反馈机制的灵活控制。我们还提供了可生成结构化演示的多步骤求解器，以支持监督式微调。评估结果表明，所有前沿模型在交互式场景中均表现不佳，在简单配置（46.6%）与困难配置（26.0%）下的成功率均较低。实验揭示了若干显著局限：模型难以有效利用长上下文信息，在无限制历史窗口下的表现反而差于截断窗口；同时，多项基于文本的符号任务在转换为视觉呈现后难度显著增加。然而，在部分可观测或动态未知的场景中，通过显式目标观察、文本反馈以及探索性演示进行监督微调，能够带来稳定性能提升，这为改进多步骤视觉决策指明了具体失效模式与优化路径。代码、数据及模型可通过以下链接获取：https://visgym.github.io/。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.16973">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.16973">arXiv</a></p>
<hr />
<h3>5. Memory-V2V：通过记忆增强视频到视频扩散模型</h3>
<p><strong>原文标题：</strong> Memory-V2V: Augmenting Video-to-Video Diffusion Models with Memory</p>
<p><strong>摘要：</strong>
近期的基础性视频到视频扩散模型在编辑用户提供的视频方面取得了令人瞩目的成果，能够修改视频的外观、运动或摄像机移动。然而，现实中的视频编辑通常是一个迭代过程，用户需要通过多轮交互来优化结果。在这种多轮编辑场景下，当前的视频编辑工具难以保持连续编辑之间的跨轮次一致性。本研究首次针对多轮视频编辑中的跨轮次一致性问题，提出了Memory-V2V——一个简单而有效的框架，通过显式记忆机制增强现有的视频到视频模型。基于先前已编辑视频的外部缓存，Memory-V2V采用精确检索与动态标记化策略，将历史编辑结果作为当前编辑步骤的条件输入。为进一步减少冗余并降低计算开销，我们在DiT骨干网络中引入了一种可学习的标记压缩器，该压缩器能够在保留关键视觉线索的同时压缩冗余的条件标记，从而实现整体30%的加速效果。我们在视频新视角合成和文本条件长视频编辑等挑战性任务上验证了Memory-V2V的性能。大量实验表明，Memory-V2V能够以最小的计算开销生成具有显著更高跨轮次一致性的视频，同时在特定任务性能上保持甚至超越现有最先进基线模型。项目页面：https://dohunlee1.github.io/MemoryV2V</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.16296">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.16296">arXiv</a></p>
<hr />
<h3>6. 推理时验证的规模化：基于测试时准则引导验证的自演进深度研究智能体</h3>
<p><strong>原文标题：</strong> Inference-Time Scaling of Verification: Self-Evolving Deep Research Agents via Test-Time Rubric-Guided Verification</p>
<p><strong>摘要：</strong>
深度研究智能体（DRAs）的最新进展正在改变自动化知识发现与问题解决的范式。现有研究大多通过训练后优化来提升策略能力，本文提出一种替代范式：通过精心设计的评估准则迭代验证策略模型的输出，从而实现智能体能力的自演进。该方法催生了验证过程的推理时规模化，即智能体通过评估自身生成的答案产生迭代反馈与优化，实现自我改进。我们基于自动构建的DRA失败分类法推导评估准则，该系统将智能体失败系统性地归纳为5个主要类别和13个子类别。我们提出DeepVerifier——一种基于准则的结果奖励验证器，其利用验证过程的不对称性，在元评估F1分数上超越原始智能体即裁判和LLM裁判基线12%-48%。为实现实际自演进，DeepVerifier可作为即插即用模块集成于测试时推理流程。该验证器生成基于准则的详细反馈，反馈至智能体进行迭代自举优化，无需额外训练即可精炼响应。当搭载高性能闭源大语言模型时，这种测试时规模化方法在GAIA和XBench-DeepResearch的挑战性子集上实现了8%-11%的准确率提升。最后，为促进开源生态发展，我们开源DeepVerifier-4K数据集，该监督微调数据集包含4,646个聚焦DRA验证的高质量智能体步骤案例，这些案例强调反思与自我批判能力，助力开源模型发展鲁棒的验证能力。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.15808">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.15808">arXiv</a></p>
<hr />
<h3>7. Jet-RL：通过统一的训练与推演精度流实现基于策略的FP8强化学习</h3>
<p><strong>原文标题：</strong> Jet-RL: Enabling On-Policy FP8 Reinforcement Learning with Unified Training and Rollout Precision Flow</p>
<p><strong>摘要：</strong>
强化学习（RL）对于提升大语言模型（LLM）的复杂推理能力至关重要。然而，现有的RL训练流程计算效率低且资源消耗大，其中推演阶段占总训练时间的70%以上。量化RL训练，尤其是使用FP8精度，为解决这一瓶颈提供了可行途径。当前普遍采用的策略是在推演阶段使用FP8精度，同时在训练阶段保持BF16精度。本研究首次对FP8 RL训练进行了全面分析，并证明这种广泛使用的“BF16训练 + FP8推演”策略在长序列推演和复杂任务下存在严重的训练不稳定性，并会导致灾难性的精度崩溃。我们的分析表明，这些失败源于该方法的离策略特性，其在训练与推理之间引入了显著的数值不匹配。基于此发现，我们提出了Jet-RL，一个能够实现稳健、稳定RL优化的FP8训练框架。其核心思想是采用统一的FP8精度流同时覆盖训练与推演阶段，从而最大程度减少数值差异，并避免低效的步间校准。大量实验验证了Jet-RL的有效性：我们的方法在推演阶段实现了最高33%的加速，在训练阶段实现了最高41%的加速，相比BF16训练整体端到端加速达16%，同时在所有实验设置下均保持稳定的收敛性，且精度损失可忽略不计。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.14243">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.14243">arXiv</a></p>
<hr />
<h3>8. SALAD：通过高效线性注意力微调实现视频扩散Transformer的高稀疏化注意力机制</h3>
<p><strong>原文标题：</strong> SALAD: Achieve High-Sparsity Attention via Efficient Linear Attention Tuning for Video Diffusion Transformer</p>
<p><strong>摘要：</strong>
扩散Transformer近期在视频生成领域展现出卓越性能。然而，由于完全注意力机制的二次计算复杂度，长输入序列会导致较高的计算延迟。现有研究提出了多种稀疏注意力机制：免训练的稀疏注意力受限于较低的稀疏度，仅能实现有限的加速效果；而基于训练的方法虽能达到更高稀疏度，却需要大量数据和计算资源进行训练。本研究提出SALAD方法，通过在稀疏注意力旁并行引入轻量级线性注意力分支，并采用输入依赖的门控机制精细平衡两个分支的贡献。该方法在保持生成质量与完全注意力基线相当的前提下，实现了90%的注意力稀疏度和1.72倍的推理加速。此外，我们的微调过程具有高效特性，仅需2,000个视频样本和1,600个训练步数（批次大小为8）即可完成。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.16515">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.16515">arXiv</a></p>
<hr />
<h3>9. GameTalk：面向战略对话的大语言模型训练框架</h3>
<p><strong>原文标题：</strong> GameTalk: Training LLMs for Strategic Conversation</p>
<p><strong>摘要：</strong>
在多智能体环境中进行战略决策是大语言模型面临的核心挑战，尤其在需要通过多轮对话展开协调与谈判的场景中。尽管近期研究已探索大语言模型在独立决策任务中的应用，但如何通过对话优化长期目标的研究仍较为有限。本文提出GameTalk框架，通过多轮交互训练大语言模型进行战略决策。与以往关注单轮目标或静态行为预测的研究不同，我们训练模型在整个对话过程中优化全局目标。通过改进GRPO、DPO和STaR等微调方法，使其能够整合依赖完整交互过程的奖励信号，我们实现了这一目标。在一系列复杂度递增的博弈环境中对该方法进行评估，这些环境专门设计用于检验推理、协调和对手建模等不同维度的能力。实验结果表明，GameTalk显著优于未经训练的基线模型，在奖励塑形条件下表现尤为突出，其中DPO方法持续带来最显著的性能提升。这些发现表明，对话式微调为大语言模型在交互环境中进行推理、谈判与行动提供了可行路径。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.16276">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.16276">arXiv</a></p>
<hr />
<h3>10. MeepleLM：模拟多样化主观体验的虚拟游戏测试员</h3>
<p><strong>原文标题：</strong> MeepleLM: A Virtual Playtester Simulating Diverse Subjective Experiences</p>
<p><strong>摘要：</strong>
近年来，大型语言模型在桌游中的角色已从游戏代理扩展至创意协同设计者。然而，当前系统仍存在一个关键缺陷：缺乏基于涌现用户体验的建构性批判能力。弥补这一缺陷对于协调人机协作至关重要，它使设计者能够借助外部视角完善创作，同时引导模型避免偏见或不可预测的结果。实现桌游批判的自动化面临两大挑战：一是在缺乏显式游戏引擎的情况下，推断连接规则与游戏过程的潜在动态；二是建模多样化玩家群体的主观异质性。为此，我们构建了一个包含1,727份结构校正规则书和15万条通过质量评分与维度感知抽样筛选的评论数据集。我们运用机制-动态-美学（MDA）推理框架增强数据，以显式弥合书面规则与玩家体验之间的因果鸿沟。进一步地，我们提炼玩家角色原型，并提出了MeepleLM——一个内化角色特定推理模式的专用模型，能够精准模拟多样化玩家原型的个性化反馈。实验表明，MeepleLM在社区契合度与批判质量上显著优于最新商业模型（如GPT-5.1、Gemini3-Pro），在评估实用性的用户研究中获得70%的偏好率。MeepleLM可作为通用交互系统的可靠虚拟测试员，标志着面向受众契合、体验感知的人机协作迈出了关键一步。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.07251">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.07251">arXiv</a></p>
<hr />
<h3>11. DSGym：一个用于评估与训练数据科学智能体的整体框架</h3>
<p><strong>原文标题：</strong> DSGym: A Holistic Framework for Evaluating and Training Data Science Agents</p>
<p><strong>摘要：</strong>
数据科学智能体有望通过将数据转化为可执行的分析与发现，加速科学发现与洞察生成。然而，现有数据科学基准测试存在不足，主要体现在评估接口碎片化导致跨基准比较困难、任务覆盖范围狭窄以及缺乏严谨的数据基础。我们特别指出，当前基准测试中相当一部分任务无需使用真实数据即可解决。为应对这些局限，本文提出DSGym——一个在自包含执行环境中评估与训练数据科学智能体的标准化框架。与静态基准不同，DSGym采用模块化架构，可便捷地添加任务、智能体框架及工具，使其成为动态可扩展的测试平台。我们构建了DSGym-Tasks整体任务套件，通过质量筛选与捷径可解性过滤对现有基准进行标准化与优化。进一步通过以下方式拓展任务覆盖范围：（1）DSBio：基于文献构建、专家驱动的生物信息学任务；（2）DSPredict：涵盖计算机视觉、分子预测与单细胞扰动等领域的复杂预测任务。除评估功能外，DSGym通过执行验证的数据合成流程支持智能体训练。作为案例研究，我们构建了包含2000个样本的训练集，在DSGym中训练了一个40亿参数模型，该模型在标准化分析基准测试中表现优于GPT-4o。总体而言，DSGym能够对智能体在真实科学场景中规划、实施与验证数据分析的能力进行严谨的端到端评估。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.16344">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.16344">arXiv</a></p>
<hr />
<h3>12. Mecellem模型：面向法律领域从头训练与持续预训练的土耳其语模型</h3>
<p><strong>原文标题：</strong> Mecellem Models: Turkish Models Trained from Scratch and Continually Pre-trained for the Legal Domain</p>
<p><strong>摘要：</strong>
本文提出Mecellem模型框架，该框架通过领域自适应策略开发面向土耳其法律领域的专用语言模型。我们做出两项贡献：（1）从头预训练的编码器模型：基于ModernBERT架构的双向编码器在包含1127亿词符的土耳其语主导语料库上进行预训练。我们实施了检查点选择策略，通过持续评估训练过程中的下游检索性能，发现最优检查点能在预训练损失达到最小值之前取得最佳检索分数。我们的编码器模型在土耳其语检索排行榜中位列前三，其中较小规模模型（1.55亿参数）的性能可与更大规模参考模型（3.07亿-5.67亿参数）相媲美。相较于前沿模型（embeddinggemma-300m：100.00%、BAAI/bge-m3：99.54%、newmindai/bge-m3-stsb：94.38%），我们的方法实现了92.36%的生产效率，在计算资源需求更少的情况下总体排名第四。当前前沿模型依赖多阶段、计算密集的训练流程，而我们采用单阶段预训练结合高效后训练的方法，提供了更具成本效益的替代方案；（2）持续预训练的解码器模型：通过受控课程学习将Qwen3-1.7B和Qwen3-4B模型适配至土耳其法律领域。采用四阶段持续预训练与最优样本配比，实现了从通用语言知识到专业法律术语及长上下文推理的渐进过渡。该方法在土耳其法律文本上实现了36.2%的困惑度降低，充分证明了领域自适应的有效性。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.16018">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.16018">arXiv</a></p>
<hr />
<h3>13. 无尽终端：面向终端智能体的可扩展强化学习环境构建</h3>
<p><strong>原文标题：</strong> Endless Terminals: Scaling RL Environments for Terminal Agents</p>
<p><strong>摘要：</strong>
环境是自进化智能体的发展瓶颈。现有终端基准测试集为评估而设计，非为训练优化；强化学习需要可扩展的生成流程，而非静态数据集。本文提出“无尽终端”——一个无需人工标注、能自主生成终端任务的全自动流程。该流程包含四个阶段：生成多样化任务描述、构建并验证容器化环境、设计完成度测试、基于可解性进行任务筛选。通过此流程，我们获得了涵盖文件操作、日志管理、数据处理、脚本编写及数据库操作等领域的3255项任务。我们采用标准PPO算法配合二元回合奖励机制进行智能体训练，并采用极简交互循环：无检索机制、多智能体协作或专用工具。尽管设计简洁，在无尽终端上训练的模型仍展现出显著性能提升：在预留开发集上，Llama-3.2-3B模型准确率从4.0%提升至18.2%，Qwen2.5-7B从10.7%提升至53.3%，Qwen3-8B-openthinker-sft从42.6%提升至59.0%。该提升效果可迁移至人工标注基准测试：在TerminalBench 2.0测试中，经无尽终端训练的模型表现全面超越采用复杂智能体框架的对比方案，其中Llama-3.2-3B从0.0%提升至2.2%，Qwen2.5-7B从2.2%提升至3.4%，Qwen3-8B-openthinker-sft从1.1%提升至6.7%。这些结果表明：当环境实现规模化扩展时，简约的强化学习范式即可取得显著成效。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.16443">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.16443">arXiv</a></p>
<hr />
<h3>14. ChartVerse：通过从零开始的可靠程序化合成实现图表推理的规模化</h3>
<p><strong>原文标题：</strong> ChartVerse: Scaling Chart Reasoning via Reliable Programmatic Synthesis from Scratch</p>
<p><strong>摘要：</strong>
图表推理是视觉语言模型（VLM）的关键能力。然而，高质量训练数据的缺乏严重阻碍了开源模型的发展。现有数据集面临双重挑战：合成图表往往过于简单且重复，而相关的问答对则容易出现幻觉，缺乏复杂任务所需的推理深度。为弥补这一差距，我们提出了ChartVerse，一个可扩展的框架，旨在从零开始合成复杂图表与可靠的推理数据。（1）针对简单模式的瓶颈，我们首先引入了Rollout后验熵（RPE），这是一种量化图表复杂度的新指标。在RPE的指导下，我们开发了复杂度感知的图表编码器，通过可执行程序自主合成多样化、高复杂度的图表。（2）为确保推理的严谨性，我们开发了基于真实锚点的逆向问答合成方法。与标准生成方式不同，我们采用答案优先的范式：直接从源代码中提取确定性答案，基于这些锚点生成问题，并强制执行严格的一致性验证。为进一步提升难度与推理深度，我们根据模型失败率筛选样本，并提炼出高质量的思维链（CoT）推理数据。我们使用Qwen3-VL-30B-A3B-Thinking作为教师模型，构建了ChartVerse-SFT-600K和ChartVerse-RL-40K数据集。实验结果表明，ChartVerse-8B模型实现了最先进的性能，显著超越了其教师模型，并与更强的Qwen3-VL-32B-Thinking模型相媲美。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.13606">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.13606">arXiv</a></p>
<hr />
<h3>15. 知识并非万能：注入强化学习技能以实现持续适应</h3>
<p><strong>原文标题：</strong> Knowledge is Not Enough: Injecting RL Skills for Continual Adaptation</p>
<p><strong>摘要：</strong>
大型语言模型（LLM）面临“知识截止”挑战，其冻结的参数化记忆阻碍了新信息的直接内化。尽管监督微调（SFT）常用于更新模型知识，但该方法通常仅更新事实性内容，而无法可靠提升模型运用新信息进行问答或决策的能力。强化学习（RL）对获取推理技能至关重要，但其高昂的计算成本使其难以实现高效的在线适应。我们通过实验观察到，SFT与RL引发的参数更新近乎正交。基于此发现，我们提出参数化技能迁移框架（PaST），该框架支持模块化技能迁移，以实现高效且有效的知识适应。通过从源领域提取领域无关的技能向量，我们可以在目标模型对新数据进行轻量级SFT后，线性注入知识操纵技能。在知识融合问答（SQuAD、LooGLE）与智能体工具使用基准测试（ToolBench）上的实验验证了本方法的有效性。在SQuAD数据集上，PaST相较最先进的自我编辑SFT基线最高提升9.9个点。PaST进一步扩展至LooGLE长上下文问答任务，获得8.0个点的绝对准确率提升，并在ToolBench上实现平均+10.3个点的零样本成功率改进，且在不同工具类别中均保持稳定增益，这证明了技能向量具备强大的可扩展性与跨领域迁移能力。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.11258">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.11258">arXiv</a></p>
<hr />
<h3>16. VISTA-PATH：一种用于计算病理学中病理图像分割与定量分析的交互式基础模型</h3>
<p><strong>原文标题：</strong> VISTA-PATH: An interactive foundation model for pathology image segmentation and quantitative analysis in computational pathology</p>
<p><strong>摘要：</strong>
组织病理学图像的精确语义分割对于定量组织分析和下游临床建模至关重要。现有的分割基础模型通过大规模预训练提升了泛化能力，但由于将分割视为静态视觉预测任务，仍难以与病理学需求充分契合。本文提出VISTA-PATH——一种交互式、类别感知的病理分割基础模型，旨在解析异质性组织结构、整合专家反馈，并生成对临床解读具有直接意义的像素级分割结果。该模型通过联合建模视觉上下文、语义组织描述及可选的专家空间提示来实现分割条件化，从而在异质性病理图像中实现精确的多类别分割。为支撑此范式，我们构建了VISTA-PATH数据集，这是一个包含超过160万个图像-掩码-文本三元组的大规模病理分割语料库，涵盖9个器官和93种组织类别。在大量留出测试集和外部基准数据上的实验表明，VISTA-PATH持续优于现有分割基础模型。值得注意的是，该模型支持动态人机协同优化，能够将稀疏的局部边界框标注反馈传播至全玻片分割。最后，我们证明VISTA-PATH生成的高保真、类别感知分割是计算病理学的优选模型：通过提出的肿瘤相互作用评分（TIS）改进组织微环境分析，该评分与患者生存期呈现显著强相关性。综上，这些成果确立了VISTA-PATH作为基础模型的地位，将病理图像分割从静态预测提升为数字病理学中具有交互性和临床依据的表征方法。源代码与演示可通过https://github.com/zhihuanglab/VISTA-PATH获取。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.16451">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.16451">arXiv</a></p>
<hr />
<h3>17. 镣铐之舞：基于心智理论的学术反驳策略性说服研究</h3>
<p><strong>原文标题：</strong> Dancing in Chains: Strategic Persuasion in Academic Rebuttal via Theory of Mind</p>
<p><strong>摘要：</strong>
尽管人工智能已深度融入研究流程的各个阶段并取得显著进展，学术反驳仍是一个重要且尚未充分探索的挑战。这是因为反驳是在严重信息不对称条件下进行的策略性沟通过程，而非简单的技术辩论。现有方法大多仅模仿表层语言特征，未能把握有效说服所需的核心要素——观点采择能力，因而难以应对这一挑战。本文提出首个基于心智理论的学术反驳框架RebuttalAgent，通过构建"心智状态建模-策略制定-策略响应生成"的三阶段流程，将心智理论操作化为可执行的推理机制。为训练该智能体，我们采用新型批判优化方法构建了大规模数据集RebuttalBench。训练过程包含两个阶段：首先通过监督微调使智能体掌握基于心智理论的分析与策略规划能力，随后利用自奖励机制进行强化学习以实现可扩展的自我优化。为建立可靠高效的自动化评估体系，我们进一步开发了专用评估器Rebuttal-RM，该模型基于超过10万条多源反驳数据进行训练，其评分结果与人类偏好的一致性已超越GPT-4.1等强大评估模型。大量实验表明：RebuttalAgent在自动化指标上平均优于基线模型18.3%，同时在自动化评估与人工评估中均超越先进的专有模型。免责声明：生成的反驳内容仅供作者参考启发与辅助草拟，不可替代作者自身的批判性分析与回应。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.15715">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.15715">arXiv</a></p>
<hr />
<h3>18. 面向代码生成的大语言模型提示指南：一项实证特征研究</h3>
<p><strong>原文标题：</strong> Guidelines to Prompt Large Language Models for Code Generation: An Empirical Characterization</p>
<p><strong>摘要：</strong>
大语言模型（LLMs）当前已广泛应用于各类软件工程任务，其中代码生成是主要应用场景。先前研究表明，恰当的提示工程能够帮助开发者优化代码生成提示。然而，目前尚未形成指导开发者编写有效代码生成提示的专门性指南。本研究提出并评估了一套面向开发场景的提示优化指南。首先，我们采用迭代式、测试驱动的方法自动优化代码生成提示，并通过分析优化过程的结果，识别出能够通过测试的提示改进要素。基于这些要素，我们归纳出10项提示改进指南，涉及输入输出规范、前后置条件说明、示例提供、多维度细节补充以及模糊性澄清等方面。我们邀请了50名从业者进行评估，参与者反馈了他们对所归纳提示改进模式的使用情况及其感知有效性——结果显示，在了解本指南前，实际使用模式与感知有效性并不完全一致。本研究结果不仅对从业者和教育者具有实践意义，也为开发更高效的LLM辅助软件开发工具提供了参考。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.13118">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.13118">arXiv</a></p>
<hr />
<h2>🔍 关键词云图</h2>
<p><img alt="关键词云图" src="../images/keywords_wordcloud.png" /></p>
<h2>📈 近期论文趋势</h2>
<p><img alt="论文趋势" src="../images/daily_papers.png" /></p>
<h2>🎙️ 语音播报</h2>
<ul>
<li><a href="../audio/2026-01-26_daily_papers.mp3">收听今日论文解读</a></li>
</ul>
<h2>📱 订阅渠道</h2>
<ul>
<li>GitHub: <a href="https://github.com/2404589803/hf-daily-paper-newsletter-chinese">hf-daily-paper-newsletter-chinese</a></li>
</ul>
    </div>
</body>
</html>