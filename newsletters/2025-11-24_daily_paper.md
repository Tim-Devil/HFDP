
# <img src="https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo.png" width="30"/> Hugging Face 2025-11-24 论文日报

## 📊 今日论文统计
- 总论文数：24
- 热门领域：RL, GPT, Transformer, LLM

## 📝 论文详情


### 1. OpenMMReasoner：以开放通用方案推动多模态推理前沿研究

**原文标题：** OpenMMReasoner: Pushing the Frontiers for Multimodal Reasoning with an Open and General Recipe

**摘要：**
大型推理模型的最新进展推动着研究者将此类能力扩展至多模态领域的兴趣日益增长。然而，尽管视觉推理领域取得了显著进步，但缺乏透明可复现的数据构建与训练策略仍是制约规模化研究的主要障碍。本研究提出OpenMMReasoner——一个完全透明的双阶段多模态推理方案，涵盖监督微调（SFT）与强化学习（RL）两个阶段。在SFT阶段，我们构建了包含87.4万样本的冷启动数据集，并通过严格的逐步骤验证，为推理能力奠定坚实基础。随后的RL阶段利用跨多个领域的7.4万样本数据集进一步强化和稳定这些能力，从而实现更鲁棒高效的学习过程。大量实验评估表明，我们的训练方案不仅超越了强基线模型，更凸显了数据质量和训练设计对塑造多模态推理性能的关键作用。值得注意的是，在九大多模态推理基准测试中，我们的方法相较Qwen2.5-VL-7B-Instruct基线实现了11.6%的性能提升，为未来大规模多模态推理研究奠定了坚实的实证基础。我们已在https://github.com/EvolvingLMMs-Lab/OpenMMReasoner 开源全部代码、训练流程及数据。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.16334) | [arXiv](https://arxiv.org/abs/2511.16334)



---

### 2. 揭示文本本征维度：从学术摘要到创意故事

**原文标题：** Unveiling Intrinsic Dimension of Texts: from Academic Abstract to Creative Story

**摘要：**
本征维度作为现代大语言模型分析的重要工具，为训练动态、缩放规律及数据集结构的研究提供了关键洞见，但其文本决定因素尚未得到充分探索。我们通过交叉编码器分析、语言特征提取和稀疏自编码器，首次建立了本征维度与可解释文本特性之间的系统关联。本研究获得三项核心发现：首先，本征维度与基于熵的指标形成互补——在控制文本长度后，二者无显著相关性，表明本征维度捕捉的是独立于预测质量的几何复杂性。其次，本征维度呈现稳健的体裁分层：在所有测试模型中，科学论述呈现低维度（约8），百科类内容居中（约9），而创意/观点性写作则显示高维度（约10.5）。这表明当代大语言模型将科学文本视为"表征简单"的范畴，而虚构文学则需要更多表征自由度。第三，通过稀疏自编码器识别出因果特征：科学信号（正式语体、报告模板、统计数据）降低维度，人性化信号（个性化表达、情感元素、叙事结构）则提升维度。定向调控实验证实了这些影响的因果性。因此对当代模型而言，科学写作相对"简单"，而虚构文学、观点表达及情感内容则增加了表征自由度。我们的多维度分析为正确运用本征维度及合理解读基于本征维度的研究结果提供了实践指导。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.15210) | [arXiv](https://arxiv.org/abs/2511.15210)



---

### 3. GeoVista：面向地理定位的增强型网络代理视觉推理系统

**原文标题：** GeoVista: Web-Augmented Agentic Visual Reasoning for Geolocalization

**摘要：**
当前代理视觉推理研究虽能实现深度多模态理解，但主要聚焦于图像处理工具，在通用型代理模型开发方面仍存不足。本研究重新审视地理定位任务，该任务不仅需要精细的视觉定位能力，还需借助网络搜索在推理过程中验证或修正假设。针对现有地理定位基准数据集无法满足高分辨率图像需求及深度代理推理定位挑战的问题，我们构建了GeoBench基准数据集，包含全球范围的静态照片与全景图像，以及不同城市的卫星图像子集，用以系统评估代理模型的地理定位能力。我们提出GeoVista模型，该代理系统将工具调用无缝集成至推理循环，包含用于放大感兴趣区域的图像缩放工具和获取关联网络信息的搜索工具。我们开发了完整的训练流程：首先通过冷启动监督微调阶段学习推理模式与工具使用先验，继而通过强化学习阶段进一步提升推理能力。采用分层奖励机制以利用多层级地理信息，显著提升整体定位性能。实验结果表明，GeoVista在地理定位任务上大幅超越其他开源代理模型，在多数指标上达到与Gemini-2.5-flash、GPT-5等闭源模型相当的性能水平。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.15705) | [arXiv](https://arxiv.org/abs/2511.15705)



---

### 4. SAM 3：基于概念的可提示分割模型

**原文标题：** SAM 3: Segment Anything with Concepts

**摘要：**
本文提出可提示分割模型（SAM）第三代，这是一个基于概念提示实现图像与视频中目标检测、分割与跟踪的统一模型。概念提示定义为简短名词短语（如“黄色校车”）、示例图像或二者组合。可提示概念分割（PCS）接收此类提示后，可为所有匹配目标实例返回分割掩码与唯一标识。为推进PCS技术发展，我们构建了可扩展数据引擎，生成包含400万独特概念标签的高质量数据集，涵盖图像与视频场景中的困难负样本。该模型由共享主干网络的图像级检测器与基于记忆机制的视频跟踪器构成，通过解耦识别与定位的存现度检测头提升检测精度。实验表明，SAM 3在图像与视频PCS任务中的准确率较现有系统提升一倍，并改进了前代SAM在视觉分割任务中的性能。我们同步开源SAM 3模型及配套构建的全新基准数据集SA-Co，用于可提示概念分割研究。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.16719) | [arXiv](https://arxiv.org/abs/2511.16719)



---

### 5. O-Mem：面向个性化长周期自演进智能体的全域记忆系统

**原文标题：** O-Mem: Omni Memory System for Personalized, Long Horizon, Self-Evolving Agents

**摘要：**
基于大语言模型的智能体近期取得的进展在生成类人响应方面展现出巨大潜力，然而在复杂环境中维持长期交互时仍面临挑战，主要源于上下文一致性与动态个性化能力的局限。现有记忆系统通常在检索前依赖语义分组，这种方法可能遗漏语义无关但关键的用户信息，并引入检索噪声。本报告提出O-Mem的初步设计框架，该创新记忆系统基于动态用户画像，通过智能体与用户的主动交互实时提取并更新用户特征与事件记录。O-Mem支持人物属性与主题相关语境的分层检索，从而实现更具适应性与连贯性的个性化响应。在公开基准测试中，O-Mem在LoCoMo上达到51.67%的准确率，较先前最优系统LangMem提升近3%；在PERSONAMEM上取得62.99%的准确率，较前最优系统A-Mem提升3.5%。与既有记忆框架相比，O-Mem还显著提升了令牌处理与交互响应的时间效率。本研究为开发高效且具人类特质的个性化人工智能助手开辟了新的研究方向。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.13593) | [arXiv](https://arxiv.org/abs/2511.13593)



---

### 6. RynnVLA-002：统一视觉-语言-行动与世界模型

**原文标题：** RynnVLA-002: A Unified Vision-Language-Action and World Model

**摘要：**
本文提出RynnVLA-002，一种统一的视觉-语言-行动（VLA）与世界模型。该世界模型通过行动与视觉输入预测未来图像状态，学习环境底层物理规律以优化行动生成。相应地，VLA模型通过图像观测生成后续行动，增强视觉理解能力并支持世界模型的图像生成。RynnVLA-002的统一框架实现了环境动态特性与行动规划的联合学习。实验表明，RynnVLA-002超越了独立的VLA和世界模型，展现出二者的协同增强效应。我们在仿真与真实机器人任务中对该模型进行评估：在LIBERO仿真基准测试中，RynnVLA-002在未经预训练的情况下取得97.4%的成功率；在LeRobot真实场景实验中，其集成世界模型使整体成功率提升50%。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.17502) | [arXiv](https://arxiv.org/abs/2511.17502)



---

### 7. PARROT：输出真值的说服力与一致性鲁棒性评级——面向大语言模型的谄媚鲁棒性基准

**原文标题：** Parrot: Persuasion and Agreement Robustness Rating of Output Truth -- A Sycophancy Robustness Benchmark for LLMs

**摘要：**
本研究提出PARROT（输出真值的说服力与一致性鲁棒性评级）框架，该框架以鲁棒性为核心，旨在衡量大语言模型在权威说服等社会压力下产生的谄媚现象（过度顺从）导致的准确性退化。PARROT通过三重机制实现评估：(i) 采用双盲评估对比同一问题的中立版本与权威错误版本，以隔离因果效应；(ii) 基于对数似然的校准追踪量化模型对正确答案与强加错误答案的置信度偏移；(iii) 通过八态行为分类法系统化识别失效模式（如鲁棒正确、谄媚认同、错误强化、顽固错误、自我修正等）。我们在13个学科领域使用1,302道MMLU式选择题及领域专属权威模板评估了22个模型。结果显示显著异质性：先进模型（如GPT-5、GPT-4.1、Claude Sonnet 4.5）表现出较低的"顺从率"（≤11%，GPT-5：4%）和极小的准确率损失，而早期/小规模模型则出现严重的认知坍塌（GPT-4：80%，Qwen 2.5-1.5B：94%）。风险不仅限于答案改变：弱势模型会降低对正确答案的置信度，同时提升对强加错误答案的置信度。尽管国际法与领域级全球知识表现出高度脆弱性，基础数学则相对稳健。据此我们主张，为实现现实世界的安全部署，"抵抗过度顺从压力"应作为与准确性、伤害规避和隐私保护并列的核心优化目标。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.17220) | [arXiv](https://arxiv.org/abs/2511.17220)



---

### 8. Loomis Painter：绘画过程重建技术研究

**原文标题：** Loomis Painter: Reconstructing the Painting Process

**摘要：**
分步绘画教程对学习艺术技法至关重要，但现有视频资源（如YouTube）缺乏交互性与个性化。尽管近期生成模型在艺术图像合成领域取得进展，但其在跨媒介泛化方面存在局限，常出现时序或结构不一致问题，难以忠实复现人类创作流程。为此，我们提出一个融合多媒介绘画过程生成的统一框架，采用语义驱动的风格控制机制，将多种媒介嵌入扩散模型的条件空间并实施跨媒介风格增强。该方案能实现跨风格的一致纹理演化与过程迁移。通过逆向绘制训练策略进一步确保生成结果符合人类绘画的流畅性。我们还构建了大规模真实绘画过程数据集，从跨媒介一致性、时序连贯性和最终图像保真度三个维度进行评估，在LPIPS、DINO和CLIP指标上取得优异表现。最后提出的感知距离分布（PDP）曲线可量化建模创作序列——构图、色块铺陈与细节精修，精准对应人类艺术创作进程。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.17344) | [arXiv](https://arxiv.org/abs/2511.17344)



---

### 9. WorldGen：从文本到可遍历交互式三维世界的生成系统

**原文标题：** WorldGen: From Text to Traversable and Interactive 3D Worlds

**摘要：**
本文提出WorldGen系统，能够直接根据文本提示自动创建大规模交互式三维世界。该方法将自然语言描述转化为可遍历、全贴图的虚拟环境，支持在标准游戏引擎中即时探索或编辑。通过融合大语言模型驱动的场景布局推理、程序化生成、基于扩散模型的三维生成及对象感知的场景解构技术，WorldGen有效弥合了创意构想与功能化虚拟空间之间的鸿沟，使创作者无需手动建模或具备专业三维技能即可设计出连贯可导航的虚拟世界。该系统采用全模块化架构，支持对布局、尺度和风格的细粒度控制，所生成的世界兼具几何一致性、视觉丰富性与实时渲染效率。本研究成果为推动规模化、低门槛的生成式虚拟世界构建迈出关键一步，拓展了三维生成人工智能在游戏开发、仿真模拟及沉浸式社交等领域的应用前沿。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.16825) | [arXiv](https://arxiv.org/abs/2511.16825)



---

### 10. 螳螂模型：具备解耦视觉预测能力的多模态视觉-语言-动作框架

**原文标题：** Mantis: A Versatile Vision-Language-Action Model with Disentangled Visual Foresight

**摘要：**
视觉-语言-动作模型的最新进展表明，视觉信号能有效补充稀疏动作监督。然而，直接让VLA模型预测高维视觉状态会分散模型容量并产生高昂训练成本，而将视觉状态压缩为紧凑监督信号则不可避免地引发信息瓶颈。此外，现有方法因忽视语言监督常导致理解与推理能力不足。本文提出螳螂模型，该创新框架通过解耦视觉预测机制解决上述问题。具体而言，该模型结合元查询与扩散Transformer头，将视觉预测从主干网络解耦。通过残差连接向DiT提供当前视觉状态，简单的下一状态预测目标使元查询能自动捕捉描述视觉轨迹的潜在动作，从而增强显式动作的学习。这种解耦设计减轻了VLA主干网络负担，使其能通过语言监督保持理解与推理能力。经人类操作视频、机器人示范及图文对预训练后，螳螂模型在LIBERO基准微调后达到96.7%的成功率，在超越强基线的同时展现出高收敛速度。真实场景评估表明，该模型在指令遵循能力、未知指令泛化性和推理能力方面均优于主流开源VLA模型π_{0.5}。相关代码与权重已开源以支持社区研究。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.16175) | [arXiv](https://arxiv.org/abs/2511.16175)



---

### 11. VisMem：潜在视觉记忆解锁视觉语言模型的潜力

**原文标题：** VisMem: Latent Vision Memory Unlocks Potential of Vision-Language Models

**摘要：**
尽管视觉语言模型取得了显著成功，但其在复杂视觉任务上的表现常受限于"视觉处理瓶颈"：在长序列生成过程中容易丧失视觉证据的锚定，并表现出情境化视觉经验的缺失。受人类认知记忆理论中短期视觉主导记忆与长期语义主导记忆区分的启发，我们提出VisMem——一个认知对齐框架，通过动态潜在视觉记忆为VLMs赋能，包含用于细粒度感知保持的短期模块和用于抽象语义巩固的长期模块。这些记忆在推理过程中被无缝调用，使VLMs能够在思维与生成过程中同时保持感知保真度与语义一致性。在涵盖理解、推理与生成的多样化视觉基准测试中，实验结果表明VisMem相较于原始模型实现了11.8%的平均性能提升，且优于所有对比模型，确立了潜在空间记忆增强的新范式。代码将发布于：https://github.com/YU-deep/VisMem.git。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.11007) | [arXiv](https://arxiv.org/abs/2511.11007)



---

### 12. InstructMix2Mix：通过多视角模型个性化实现一致的稀疏视角编辑

**原文标题：** InstructMix2Mix: Consistent Sparse-View Editing Through Multi-View Model Personalization

**摘要：**
本文研究从稀疏输入视角进行多视角图像编辑的任务，其中输入可视为从不同视角捕捉场景的图像混合体。目标是根据文本指令修改场景，同时保持所有视角间的一致性。现有基于逐场景神经场或时序注意力机制的方法在此设定下表现不佳，常产生伪影和不连贯的编辑效果。我们提出InstructMix2Mix框架，通过将二维扩散模型的编辑能力蒸馏至预训练的多视角扩散模型，利用其数据驱动的三维先验实现跨视角一致性。核心创新在于用多视角扩散学生模型取代分数蒸馏采样中的传统神经场整合器，这需要三项新颖适配：跨时间步的渐进式学生模型更新、防止性能退化的专用教师噪声调度器，以及无需额外成本即可增强跨视图一致性的注意力机制改进。实验表明，I-Mix2Mix在保持单帧高质量编辑的同时，显著提升了多视角一致性。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.14899) | [arXiv](https://arxiv.org/abs/2511.14899)



---

### 13. MergeDNA：基于动态分词与令牌融合的上下文感知基因组建模方法

**原文标题：** MergeDNA: Context-aware Genome Modeling with Dynamic Tokenization through Token Merging

**摘要：**
基因组序列建模面临两大未解难题：不同区域的信息密度差异显著，且缺乏明确定义的最小词汇单元。现有方法依赖四种碱基或独立设计的DNA分词器，结合简单的掩码语言建模预训练，往往难以适应基因组序列的复杂度变化。本文利用令牌融合技术，提出一种通过上下文感知预训练任务联合优化动态基因组分词器与潜在Transformer的层次化架构。在网络结构方面，分词模块通过堆叠多层具有局部窗口约束的可微分令牌融合块，将相邻碱基自动分块为词汇单元，随后潜在编码器通过全局注意力块捕获这些融合词汇的上下文信息。通过对称部署潜在解码器与局部解码器，MergeDNA采用两项预训练任务：融合令牌重构任务同步训练动态分词模块并自适应筛选重要令牌，而自适应掩码令牌建模任务则学习预测这些被筛选令牌以捕获信息内容。大量实验表明，MergeDNA在三个主流DNA基准测试和若干多组学任务中，无论是通过微调还是零样本评估，均取得了优于典型分词方法及大规模DNA基础模型的性能表现。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.14806) | [arXiv](https://arxiv.org/abs/2511.14806)



---

### 14. OmniScientist：构建人类与AI科学家协同演化的科研生态系统

**原文标题：** OmniScientist: Toward a Co-evolving Ecosystem of Human and AI Scientists

**摘要：**
随着大语言模型（LLM）的快速发展，AI智能体在科学任务中展现出日益精进的能力，涵盖假设生成、实验设计乃至论文撰写等环节。此类智能体系统通常被称为"AI科学家"。然而，现有AI科学家主要将科学发现建模为独立搜索或优化问题，忽视了科学研究本质上是社会性协作活动这一根本特征。现实科学体系依赖于由协作机制、贡献归属、同行评议和结构化科学知识网络构成的复杂科研基础设施。由于缺乏对这些关键维度的建模，现有系统难以建立真正的研究生态系统，也无法与人类科学界实现深度互动。为弥补这一缺陷，我们提出OmniScientist框架，将人类科研的内在机制显式编码至AI科学工作流中。该框架不仅实现从数据基础、文献综述、研究构思、实验自动化、科学写作到同行评议的端到端自动化，还通过模拟人类科学体系提供完整的基础设施支持，包括：（1）基于引文网络与概念关联的结构化知识体系；（2）支持多智能体无缝协作及人类研究者参与的开放式科研协议（OSP）；（3）基于双盲用户投票与Elo排序的开放评估平台（ScienceArena）。这套基础设施使智能体既能理解并利用人类知识体系，又能通过协作实现共同演化，最终培育出可持续、可扩展的创新生态系统。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.16931) | [arXiv](https://arxiv.org/abs/2511.16931)



---

### 15. 缩小智能规模：探索小型多模态模型中的感知与推理瓶颈

**原文标题：** Downscaling Intelligence: Exploring Perception and Reasoning Bottlenecks in Small Multimodal Models

**摘要：**
多模态模型的规模扩展显著提升了视觉理解与推理能力，但实际应用需求呼唤更精简高效的系统。本研究对多模态模型智能规模缩减现象展开系统性分析，探究大型语言模型容量缩减如何影响多模态能力。初步发现揭示了一个有趣趋势：语言模型规模缩减对视觉能力的影响远超其对语言模型固有能力的继承。我们进一步探究这种性能下降究竟源于预期的视觉推理能力衰减，还是更根本的感知能力丧失。通过分离语言模型规模缩减对感知能力的影响，发现性能仍会急剧下降，其降幅往往与推理能力下降相当甚至更甚。为突破此瓶颈，我们提出视觉提取调优方法，通过显式训练使模型在不同任务中持续提取与指令相关的视觉细节。基于这些提取的视觉信息，我们采用分步推理机制生成答案。这些组件共同构成了"提取+思考"方法论，为该领域的效率与性能设立了新基准。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.17487) | [arXiv](https://arxiv.org/abs/2511.17487)



---

### 16. 视觉自回归模型中的多样性本真溯源

**原文标题：** Diversity Has Always Been There in Your Visual Autoregressive Models

**摘要：**
视觉自回归模型凭借其创新的尺度递进预测范式，相较传统多步自回归与扩散模型在推理效率与图像质量方面展现出显著优势，近来备受关注。然而尽管效率卓越，该类模型仍常面临多样性坍缩问题——即输出变异性的减弱，这种现象与少步蒸馏扩散模型中的观测结果具有相似性。本文提出DiverseVAR这一无需额外训练即可恢复视觉自回归模型生成多样性的简易有效方案。通过理论分析，我们发现特征图中的关键成分是早期尺度多样性形成的主导因素。通过抑制模型输入中的关键成分并增强其输出表达，DiverseVAR在保持高保真合成能力的同时，有效释放了视觉自回归模型内蕴的生成潜力。实证研究表明，该方法仅以可忽略的性能影响为代价，即可显著提升生成多样性。相关代码将发布于https://github.com/wangtong627/DiverseVAR。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.17074) | [arXiv](https://arxiv.org/abs/2511.17074)



---

### 17. Video-R4：通过视觉反刍增强文本富集视频推理能力

**原文标题：** Video-R4: Reinforcing Text-Rich Video Reasoning with Visual Rumination

**摘要：**
理解文本富集视频需要捕捉短暂出现的小尺寸文本线索，这往往需要反复观察。然而现有视频问答模型大多基于固定帧的单次感知，导致存在幻觉现象且在细粒度证据识别上表现不佳。受人类暂停播放、放大关键区域及重复阅读行为的启发，我们提出Video-R4（基于视觉反刍的文本富集视频推理增强模型），该视频推理大语言模型能够执行视觉反刍操作：迭代选择帧序列、放大信息密集区域、重新编码检索像素并持续更新推理状态。我们构建了两个包含可执行反刍轨迹的数据集：用于监督训练的Video-R4-CoT-17k和用于强化学习的Video-R4-RL-30k。提出多阶段反刍学习框架，通过指令微调和基于GRPO的强化学习，逐步训练70亿参数模型掌握原子视觉操作与混合视觉操作。Video-R4-7B在M4-ViteVQA基准测试中达到最先进水平，并进一步泛化至多页文档问答、幻灯片问答及通用视频问答任务，证明迭代式反刍是实现像素级多模态推理的有效范式。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.17490) | [arXiv](https://arxiv.org/abs/2511.17490)



---

### 18. ICLR同行评审与作者反馈机制的研究启示

**原文标题：** Insights from the ICLR Peer Review and Rebuttal Process

**摘要：**
同行评审是科学出版体系的基石，在ICLR等顶级机器学习会议中亦是如此。随着投稿量持续增长，深入理解评审机制的特性与动态对于提升流程效率、评审效能及论文质量至关重要。本研究对ICLR 2024与2025年的同行评审过程开展大规模分析，重点关注反驳环节前后的评分变化及审稿人与作者的互动关系。我们系统考察了评审分数分布、作者-审稿人互动强度、评审提交的时间规律以及共同审稿人的影响效应。通过量化分析与基于大语言模型的评论文本及反驳讨论分类，我们揭示了不同评分区间论文的共性优势与不足，并识别出与分数变化关联最显著的反驳策略趋势。研究发现：初始评分与共同审稿人的评级是反驳过程中分数变化的最强预测因子，这表明审稿人之间存在相互影响；对于临界论文，反驳环节能显著改善评审结果，深思熟虑的作者回应可有效转变审稿人观点。本研究从宏观层面为改进同行评审机制提供了实证依据，既可指导作者制定有效反驳策略，也有助于学术社区设计更公平高效的评审流程。相关代码与评分变化数据已公开于https://github.com/papercopilot/iclr-insights。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.15462) | [arXiv](https://arxiv.org/abs/2511.15462)



---

### 19. 基于草图引导验证的物理感知视频生成规划方法

**原文标题：** Planning with Sketch-Guided Verification for Physics-Aware Video Generation

**摘要：**
当前视频生成方法日益依赖规划中间控制信号（如物体轨迹）来提升时序连贯性与运动保真度。然而这些方法多采用单次规划方案，通常仅能处理简单运动，或需通过多次调用视频生成器进行迭代优化，导致计算成本高昂。为突破这些局限，我们提出SketchVerify——一种免训练的草图验证规划框架，通过在生成完整视频前引入测试时采样与验证循环，以更具动态连贯性的轨迹（即物理合理且符合指令要求的运动）提升运动规划质量。给定提示词与参考图像，本方法首先生成多个候选运动规划方案，随后采用视觉语言验证器从语义指令对齐度和物理合理性两个维度进行综合评估与排序。为高效评分候选运动方案，我们将每条轨迹合成为静态背景上的物体组合轻量视频草图，在保持性能相当的同时规避了昂贵的重复扩散合成过程。通过迭代优化运动规划直至获得满意方案，最终将其输入轨迹条件生成器完成视频合成。在WorldModelBench与PhyWorldBench数据集上的实验表明：相较于基线模型，本方法在运动质量、物理真实感与长程一致性方面均有显著提升，同时具备更高运算效率。消融实验进一步证明，增加轨迹候选方案数量能持续提升整体性能。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.17450) | [arXiv](https://arxiv.org/abs/2511.17450)



---

### 20. VLA-4D：将四维感知嵌入视觉-语言-动作模型以实现时空连贯的机器人操控

**原文标题：** VLA-4D: Embedding 4D Awareness into Vision-Language-Action Models for SpatioTemporally Coherent Robotic Manipulation

**摘要：**
视觉-语言-动作模型在通用机器人任务中展现出潜力，但在需要细粒度表征的时空连贯操控任务中仍面临挑战。现有方法通常将三维位置嵌入视觉表征以提升动作的空间精度，但难以实现动作执行的时序连贯控制。本研究提出VLA-4D——一种具备四维感知的通用VLA模型，用于实现时空连贯的机器人操控。我们的模型基于两项核心设计：1）四维感知视觉表征：通过提取视觉特征，将一维时间嵌入三维位置形成四维嵌入，并借助交叉注意力机制将其融合为统一视觉表征；2）时空动作表征：在传统空间动作表征基础上引入时序信息以实现时空规划，并将多模态表征对齐至大语言模型中完成时空动作预测。在此统一框架下，所设计的视觉与动作表征共同促使机器人操控实现空间平滑性与时序连贯性。此外，我们通过扩展带有时序动作标注的VLA数据集对模型进行微调。大量实验验证了本方法在多种机器人操控任务中的优越性。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.17199) | [arXiv](https://arxiv.org/abs/2511.17199)



---

### 21. 多维度攻击：揭示具备防御机制的视觉语言模型中的跨模型脆弱性

**原文标题：** Multi-Faceted Attack: Exposing Cross-Model Vulnerabilities in Defense-Equipped Vision-Language Models

**摘要：**
视觉语言模型（VLM）的滥用日益增多，促使服务商部署多重防护机制，包括对齐调优、系统提示和内容审核。然而，这些防御措施在面对对抗性攻击时的实际鲁棒性仍待深入探究。本文提出多维度攻击框架，系统性地揭示了主流防御型VLM（如GPT-4o、Gemini-Pro与Llama-4）中存在的通用安全漏洞。该框架的核心组件是注意力转移攻击，通过将有害指令隐藏于具有竞争目标的元任务中实现攻击。基于奖励破解理论，我们从理论层面解释了该攻击的成功机制。为提升跨模型迁移性，我们进一步提出结合轻量级迁移增强算法与简单重复策略的方法，无需模型特定微调即可联合绕过输入级与输出级过滤器。实证研究表明，针对某一视觉编码器优化的对抗图像可广泛迁移至未接触过的VLM，表明共享视觉表征导致了跨模型安全漏洞。总体而言，MFA实现了58.5%的成功率，持续优于现有方法。在最新商用模型上，MFA达到52.8%的成功率，较次优攻击方法提升34%。这些结果对当前防御机制的感知鲁棒性提出质疑，揭示了现代VLM中持续存在的安全缺陷。代码地址：https://github.com/cure-lab/MultiFacetedAttack

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.16110) | [arXiv](https://arxiv.org/abs/2511.16110)



---

### 22. 基于生成式合成数据的X射线违禁品检测优化方法

**原文标题：** Taming Generative Synthetic Data for X-ray Prohibited Item Detection

**摘要：**
训练违禁品检测模型需要大量X射线安检图像，但此类图像的采集与标注过程耗时费力。为解决数据不足问题，现有研究多采用图像合成技术来扩展数据集。然而传统方法主要遵循两阶段流程：第一阶段需进行劳动密集型的前景目标提取，第二阶段执行图像合成。这种流程不仅引入额外人力成本，且效率较低。本文提出基于文本到图像生成的单阶段X射线安检图像合成框架Xsyn，通过两种创新策略提升合成图像的可用性：交叉注意力优化策略利用扩散模型的交叉注意力图优化边界框标注；背景遮挡建模策略在潜在空间中显式建模背景遮挡以增强成像复杂度。据我们所知，与现有方法相比，Xsyn是首个无需额外人力成本即可实现高质量X射线安检图像合成的方案。实验表明，本方法以1.2% mAP提升超越所有现有方法，且生成的合成图像能有效提升多种X射线安检数据集和检测器中的违禁品识别性能。代码已开源：https://github.com/pILLOW-1/Xsyn/。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.15299) | [arXiv](https://arxiv.org/abs/2511.15299)



---

### 23. 重新审视显著图：一种认知对齐的解释方法分类体系与评估框架

**原文标题：** Rethinking Saliency Maps: A Cognitive Human Aligned Taxonomy and Evaluation Framework for Explanations

**摘要：**
显著图在深度学习视觉解释中应用广泛，但其预期目标与多样化用户需求之间的对应关系仍缺乏共识。这种模糊性阻碍了解释方法的有效评估与实际应用。为解决这一局限，我们提出参考框架×粒度（RFxG）分类体系——一个基于双维度的原则性概念框架：参考框架维度区分逐点解释（“为何有此预测？”）与对比解释（“为何此结果而非替代结果？”）；粒度维度涵盖从细粒度类别层面（如“为何是哈士奇？”）到粗粒度组群层面（如“为何是犬类？”）的解释谱系。通过RFxG视角，我们揭示了现有评估指标的关键局限：这些指标过度侧重逐点保真度，却忽视了对比推理与语义粒度。为系统评估RFxG双维度的解释质量，我们提出四项新颖的保真度指标。该综合评估框架将指标应用于十种前沿显著方法、四种模型架构与三个数据集。通过推动用户意图驱动的评估范式转型，本研究不仅为开发契合模型行为的视觉解释奠定了概念基础，更为实现与人类认知复杂性相匹配的解释效果提供了实用工具。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.13081) | [arXiv](https://arxiv.org/abs/2511.13081)



---

### 24. 基于全栈AMD平台的基础模型训练：计算、网络与系统设计

**原文标题：** Training Foundation Models on a Full-Stack AMD Platform: Compute, Networking, and System Design

**摘要：**
我们首次在纯AMD硬件平台上开展了大规模专家混合模型预训练研究，同时采用了配备Pollara互联技术的MI300X GPU。本研究为系统设计与模型架构提供了实用指导。在系统层面，我们实现了对集群与网络特性的全面表征：通过微基准测试分析了Pollara网络上不同消息规模和GPU数量下所有核心集合通信操作（全归约、规约散射、全收集、广播）的性能。据我们所知，这是该领域的首例大规模研究。我们进一步提供了MI300X在核心规模与内存带宽方面的微基准测试数据，为模型设计提供参考。在模型层面，我们引入并应用了针对MI300X优化的Transformer规模配置规则，涵盖注意力机制与多层感知机模块，同时论证了能协同优化训练吞吐量与推理延迟的MoE宽度配置。我们深入阐述了训练技术栈，包括常被忽视的容错机制与检查点重塑等实用工具，并提供了训练方案的详细信息。此外，我们首次披露了ZAYA1基础模型架构（760M激活参数，83亿总参数的MoE模型），该模型将在后续研究中持续优化。ZAYA1基础模型在同等及更大规模模型中，其性能可比肩Qwen3-4B与Gemma3-12B等领先基础模型，并在推理、数学和代码基准测试中超越Llama-3-8B与OLMoE等模型。这些成果共同证明，AMD的硬件、网络和软件栈已发展成熟并完成优化，足以支撑具有竞争力的大规模预训练任务。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.17127) | [arXiv](https://arxiv.org/abs/2511.17127)



---


## 🔍 关键词云图
![关键词云图](../images/keywords_wordcloud.png)

## 📈 近期论文趋势
![论文趋势](../images/daily_papers.png)

## 🎙️ 语音播报
- [收听今日论文解读](../audio/2025-11-24_daily_papers.mp3)

## 📱 订阅渠道
- GitHub: [hf-daily-paper-newsletter-chinese](https://github.com/2404589803/hf-daily-paper-newsletter-chinese)