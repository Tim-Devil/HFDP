
# <img src="https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo.png" width="30"/> Hugging Face 2025-11-24 论文日报

## 📊 今日论文统计
- 总论文数：24
- 热门领域：RL, GPT, Transformer, LLM

## 📝 论文详情


### 1. OpenMMReasoner：以开放通用方案推进多模态推理前沿研究

**原文标题：** OpenMMReasoner: Pushing the Frontiers for Multimodal Reasoning with an Open and General Recipe

**摘要：**
大型推理模型的最新进展促使学界日益关注将此类能力扩展至多模态领域。然而，尽管视觉推理已取得显著进步，但缺乏透明可复现的数据构建与训练策略仍是制约规模化研究的主要障碍。本研究提出OpenMMReasoner——一个完全透明的两阶段多模态推理方案，涵盖监督微调（SFT）与强化学习（RL）阶段。在SFT阶段，我们构建了包含87.4万样本的冷启动数据集，并通过严格的逐步验证机制为推理能力奠定坚实基础。后续RL阶段利用跨领域7.4万样本数据集进一步强化与稳定这些能力，形成更鲁棒高效的学习过程。大量实验表明，我们的训练方案不仅超越强基线模型，更凸显了数据质量与训练设计对多模态推理性能的关键作用。值得注意的是，在九大多模态推理基准测试中，我们的方法相较Qwen2.5-VL-7B-Instruct基线实现了11.6%的性能提升，为未来大规模多模态推理研究奠定了坚实的实证基础。我们已在https://github.com/EvolvingLMMs-Lab/OpenMMReasoner 开源全部代码、训练流程及数据。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.16334) | [arXiv](https://arxiv.org/abs/2511.16334)



---

### 2. 揭示文本本征维度：从学术摘要到创意故事

**原文标题：** Unveiling Intrinsic Dimension of Texts: from Academic Abstract to Creative Story

**摘要：**
本征维度作为现代大语言模型分析的重要工具，为训练动态、扩展行为和数据集结构的研究提供了关键洞见，然而其文本决定因素仍未得到充分探索。我们通过交叉编码器分析、语言特征和稀疏自编码器的多维度研究，首次建立了本征维度与可解释文本特性之间的系统关联。本研究获得三项核心发现：第一，本征维度与基于熵的指标形成互补——在控制文本长度后，二者无显著相关性，表明本征维度捕捉的是与预测质量正交的几何复杂度；第二，本征维度呈现稳定的体裁分层：科学论述呈现低维度值（约8），百科全书内容居中（约9），而创意/观点写作则保持高维度值（约10.5），这一规律在所有测试模型中一致存在，揭示当代大语言模型将科学文本视为"表征简单"的客体，而小说创作需要更多表征自由度；第三，通过稀疏自编码器识别出因果特征：科学信号（正式语体、报告模板、统计数据）降低维度值，而人性化信号（个性化表达、情感元素、叙事特征）则提升维度值。定向调控实验证实了这些影响的因果性。因此对当代模型而言，科学写作相对"简单"，而小说、观点与情感内容则增加了表征自由度。我们的多维度分析为正确运用本征维度及合理解释基于本征维度的研究结果提供了实践指导。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.15210) | [arXiv](https://arxiv.org/abs/2511.15210)



---

### 3. GeoVista：面向地理定位的网络增强型智能视觉推理系统

**原文标题：** GeoVista: Web-Augmented Agentic Visual Reasoning for Geolocalization

**摘要：**
当前智能视觉推理研究虽能实现深度多模态理解，但主要聚焦于图像处理工具，在构建通用型智能体模型方面仍存空白。本研究重新审视地理定位任务，该任务不仅需要精细的视觉定位能力，还需借助网络搜索在推理过程中验证或修正假设。针对现有地理定位基准数据集无法满足高分辨率图像需求及深度智能推理定位挑战的问题，我们构建了GeoBench基准数据集，包含全球范围的实景照片与全景图像，以及不同城市的卫星图像子集，用于系统评估智能体模型的地理定位能力。我们提出GeoVista模型，该智能体将工具调用无缝集成于推理循环中，包含用于放大关注区域的图像缩放工具和检索关联网络信息的搜索工具。我们为其开发了完整训练流程：首先通过冷启动监督微调阶段学习推理模式与工具使用先验，继而通过强化学习阶段进一步提升推理能力。采用分层奖励机制以利用多层级地理信息，显著提升整体地理定位性能。实验结果表明，GeoVista在地理定位任务上大幅超越其他开源智能体模型，在多数指标上达到与Gemini-2.5-flash、GPT-5等闭源模型相当的性能水平。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.15705) | [arXiv](https://arxiv.org/abs/2511.15705)



---

### 4. SAM 3：基于概念的可提示通用分割模型

**原文标题：** SAM 3: Segment Anything with Concepts

**摘要：**
本文提出第三代分段任意模型（SAM 3），这是一个基于概念提示实现图像与视频中目标检测、分割与跟踪的统一模型。概念提示定义为简短名词短语（如“黄色校车”）、示例图像或二者组合。可提示概念分割（PCS）技术接收此类提示后，可返回所有匹配目标实例的分割掩码与唯一标识。为推进PCS技术发展，我们构建了可扩展数据引擎，生成包含400万独特概念标签的高质量数据集，涵盖图像与视频场景中的困难负样本。该模型采用共享骨干网络的图像级检测器与基于记忆机制的视频跟踪器，通过解耦识别与定位的存在性预测头显著提升检测精度。实验表明，SAM 3在图像与视频PCS任务中的准确率较现有系统提升两倍，并改进了前代SAM在视觉分割任务中的性能。我们同步开源SAM 3模型及配套构建的全新可提示概念分割基准SA-Co。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.16719) | [arXiv](https://arxiv.org/abs/2511.16719)



---

### 5. O-Mem：面向个性化长周期自演进智能体的全域记忆系统

**原文标题：** O-Mem: Omni Memory System for Personalized, Long Horizon, Self-Evolving Agents

**摘要：**
基于大语言模型的智能体近期取得了显著进展，在生成类人响应方面展现出巨大潜力，但在复杂环境中维持长期交互时仍面临挑战，主要源于情境一致性与动态个性化能力的局限。现有记忆系统通常在检索前依赖语义分组，这种方法可能遗漏语义无关但关键的用户信息，并引入检索噪声。本报告提出新型记忆框架O-Mem的初步设计，该框架基于动态用户画像构建，通过智能体与用户的主动交互实时提取并更新用户特征与事件记录。O-Mem支持人物属性与主题相关情境的分层检索，从而实现更具适应性与连贯性的个性化响应。在公开基准测试中，O-Mem在LoCoMo数据集上达到51.67%的准确率，较先前最优系统LangMem提升近3%；在PERSONAMEM数据集上达到62.99%的准确率，较先前最优系统A-Mem提升3.5%。与既有记忆框架相比，O-Mem还显著提升了令牌处理与交互响应的时间效率。本研究为未来开发高效且具备类人特质的个性化人工智能助手开辟了新的方向。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.13593) | [arXiv](https://arxiv.org/abs/2511.13593)



---

### 6. RynnVLA-002：统一视觉-语言-行为与世界模型

**原文标题：** RynnVLA-002: A Unified Vision-Language-Action and World Model

**摘要：**
本文提出RynnVLA-002——一个统一的视觉-语言-行为（VLA）与世界模型。该世界模型通过整合行为与视觉输入来预测未来图像状态，学习环境的基础物理规律以优化行为生成。与之对应，VLA模型通过图像观测生成后续行为，不仅增强了视觉理解能力，还为世界模型的图像生成提供支撑。RynnVLA-002的统一框架实现了环境动态特性与行为规划的联合学习。实验表明，RynnVLA-002在性能上超越独立的VLA和世界模型，验证了二者的协同增强效应。我们在仿真环境与真实机器人任务中对该模型进行了评估：在LIBERO仿真基准测试中，RynnVLA-002在无需预训练的情况下取得了97.4%的成功率；在LeRobot真实场景实验中，其集成世界模型使整体成功率提升50%。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.17502) | [arXiv](https://arxiv.org/abs/2511.17502)



---

### 7. PARROT：输出真实性说服力与一致性鲁棒性评级——面向大语言模型的谄媚鲁棒性基准

**原文标题：** Parrot: Persuasion and Agreement Robustness Rating of Output Truth -- A Sycophancy Robustness Benchmark for LLMs

**摘要：**
本研究提出PARROT（输出真实性说服力与一致性鲁棒性评级）框架，该鲁棒性导向的体系旨在量化大语言模型在权威说服情境下因用户施加社会压力而产生的准确性退化现象——即谄媚性过度顺从行为。PARROT通过三重机制实现精准评估：（i）采用双盲实验设计，通过对比同一问题的中立版本与权威性错误版本以隔离因果效应；（ii）基于对数似然的校准追踪技术，量化模型向正确答案与强加错误答案的信心偏移；（iii）运用八态行为分类法系统归类失效模式（如：鲁棒正确、谄媚性附和、错误强化、顽固错误、自我修正等）。我们使用1,302道MMLU风格选择题，覆盖13个学科领域及领域特异性权威模板，对22个模型进行评估。结果显示显著异质性：先进模型（如GPT-5、GPT-4.1、Claude Sonnet 4.5）表现出较低的"盲从率"（≤11%，GPT-5：4%）及最小精度损失，而早期/小规模模型则呈现严重认知崩塌（GPT-4：80%，Qwen 2.5-1.5B：94%）。风险不限于答案变更：弱势模型会降低对正确答案的信心度，同时提升对强加错误答案的置信水平。尽管国际法与领域级全球知识表现出高度脆弱性，基础数学则相对稳健。据此我们主张，在实现现实世界安全部署时，"抵抗过度拟合压力"应作为与准确性、危害规避和隐私保护并列的核心目标。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.17220) | [arXiv](https://arxiv.org/abs/2511.17220)



---

### 8. Loomis Painter：绘画过程重建技术研究

**原文标题：** Loomis Painter: Reconstructing the Painting Process

**摘要：**
分步骤绘画教程对学习艺术技法至关重要，但现有视频资源（如YouTube）缺乏交互性与个性化。尽管近期生成模型在艺术图像合成方面取得进展，但其难以实现跨媒介泛化，常出现时序或结构不一致问题，阻碍了对人类创作流程的忠实复现。为此，我们提出一个融合多媒介绘画过程生成的统一框架，采用语义驱动的风格控制机制，将多种媒介嵌入扩散模型的条件空间并实施跨媒介风格增强。该方案能实现跨风格的纹理一致性演化与过程迁移。通过逆向绘制训练策略进一步确保生成过程平滑且符合人类创作逻辑。我们还构建了大规模真实绘画过程数据集，从跨媒介一致性、时序连贯性和最终图像保真度三个维度进行评估，在LPIPS、DINO和CLIP指标上取得优异表现。最后提出的感知距离分布（PDP）曲线量化建模了包含构图、色块铺陈与细节精修三个阶段的创作序列，精准对应人类艺术创作进程。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.17344) | [arXiv](https://arxiv.org/abs/2511.17344)



---

### 9. WorldGen：从文本到可遍历交互式三维世界的生成系统

**原文标题：** WorldGen: From Text to Traversable and Interactive 3D Worlds

**摘要：**
本文提出WorldGen系统，能够直接从文本提示自动创建大规模交互式三维世界。该方法将自然语言描述转化为可遍历、全贴图的虚拟环境，支持在标准游戏引擎中即时探索或编辑。通过融合大语言模型驱动的场景布局推理、程序化生成、基于扩散模型的三维生成及对象感知场景解构技术，WorldGen有效弥合创意构想与功能化虚拟空间之间的鸿沟，使创作者无需手动建模或具备专业三维技能即可设计出连贯可导航的虚拟世界。本系统采用全模块化架构，支持对布局、尺度与风格的细粒度控制，生成的场景兼具几何一致性、视觉丰富性与实时渲染效率。该研究为实现大规模可访问的生成式世界构建迈出重要一步，推动了三维生成人工智能在游戏开发、仿真模拟及沉浸式社交环境等应用领域的前沿发展。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.16825) | [arXiv](https://arxiv.org/abs/2511.16825)



---

### 10. Mantis：具备解耦视觉预见能力的多模态视觉-语言-动作模型

**原文标题：** Mantis: A Versatile Vision-Language-Action Model with Disentangled Visual Foresight

**摘要：**
视觉-语言-动作模型的最新进展表明，视觉信号能有效补充稀疏的动作监督。然而，直接预测高维视觉状态会分散模型容量并产生高昂训练成本，而将视觉状态压缩为紧凑监督信号则不可避免地引发信息瓶颈。此外，现有方法因忽视语言监督而常存在理解与推理能力不足的问题。本文提出Mantis框架，通过解耦视觉预见机制应对这些挑战。该框架结合元查询与扩散Transformer头，将视觉预见预测从主干网络解耦。借助残差连接向扩散Transformer提供当前视觉状态，通过简单的下一状态预测目标，元查询能自动捕获描述视觉轨迹的潜在动作，从而增强显式动作的学习。这种解耦机制减轻了主干网络负担，使其能通过语言监督保持理解与推理能力。实证研究表明，在人类操作视频、机器人示范和图文对上预训练后，Mantis在LIBERO基准微调后达到96.7%的成功率，超越现有强基线并展现高效收敛速度。真实场景评估表明Mantis在指令遵循能力、未知指令泛化性和推理能力方面均优于开源领先模型π_{0.5}。相关代码与权重已开源以支持社区研究。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.16175) | [arXiv](https://arxiv.org/abs/2511.16175)



---

### 11. VisMem：潜在视觉记忆解锁视觉语言模型的潜力

**原文标题：** VisMem: Latent Vision Memory Unlocks Potential of Vision-Language Models

**摘要：**
尽管视觉语言模型取得了显著成功，但在处理复杂视觉任务时往往受限于"视觉处理瓶颈"：在长序列生成过程中容易丧失视觉证据的锚定，并表现出情境化视觉经验的缺失。受人类认知记忆理论中短期视觉主导记忆与长期语义主导记忆区分的启发，我们提出VisMem——一个认知对齐框架，通过动态潜在视觉记忆为视觉语言模型赋能，其中短期记忆模块负责细粒度感知保持，长期记忆模块负责抽象语义巩固。这些记忆在推理过程中被无缝调用，使视觉语言模型在思维与生成过程中既能保持感知保真度，又能确保语义一致性。在涵盖理解、推理与生成的多样化视觉基准测试中，大量实验表明VisMem相较于原始模型实现了11.8%的平均性能提升，且优于所有对比模型，为潜在空间记忆增强建立了新范式。代码将发布于：https://github.com/YU-deep/VisMem.git。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.11007) | [arXiv](https://arxiv.org/abs/2511.11007)



---

### 12. InstructMix2Mix：通过多视角模型个性化实现一致性的稀疏视角编辑

**原文标题：** InstructMix2Mix: Consistent Sparse-View Editing Through Multi-View Model Personalization

**摘要：**
本文研究从稀疏输入视角进行多视角图像编辑的任务，其中输入可视为从不同视角捕捉场景的混合图像。目标是根据文本指令修改场景，同时保持所有视角间的一致性。现有基于逐场景神经场或时序注意力机制的方法在此设定下表现不佳，常产生伪影和不连贯的编辑效果。我们提出InstructMix2Mix（I-Mix2Mix）框架，该框架将二维扩散模型的编辑能力蒸馏至预训练的多视角扩散模型中，利用其数据驱动的三维先验实现跨视角一致性。核心创新在于用多视角扩散学生模型取代分数蒸馏采样（SDS）中的传统神经场整合器，这需要三项新颖适配：跨时间步的渐进式学生模型更新、防止退化的专用教师噪声调度器，以及无需额外成本即可增强跨视角一致性的注意力机制改进。实验表明，I-Mix2Mix在保持单帧高质量编辑的同时，显著提升了多视角一致性。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.14899) | [arXiv](https://arxiv.org/abs/2511.14899)



---

### 13. MergeDNA：基于动态分词与令牌合并的情境感知基因组建模方法

**原文标题：** MergeDNA: Context-aware Genome Modeling with Dynamic Tokenization through Token Merging

**摘要：**
基因组序列建模面临两大未解难题：不同区域信息密度差异显著，且缺乏明确定义的最小词汇单元。现有方法依赖四种碱基基元或独立设计的DNA分词器，结合简单掩码语言建模预训练策略，往往难以适应基因组序列的复杂度变化。本文利用令牌合并技术，提出一种通过情境感知预训练任务联合优化动态基因组分词器与潜在Transformer的层次化架构。在网络结构方面，分词模块通过堆叠多层具有局部窗口约束的可微分令牌合并块，将相邻碱基自动分块为词汇单元；随后潜在编码器通过全局注意力模块捕捉这些合并词汇的上下文信息。通过对称部署潜在解码器与局部解码器，MergeDNA采用双重预训练任务：合并令牌重建任务同步训练动态分词模块并自适应筛选重要令牌，而自适应掩码令牌建模任务则学习预测这些被筛选令牌以捕捉信息内容。大量实验表明，MergeDNA在三个主流DNA基准测试和多项多组学任务中，无论是通过微调还是零样本评估，均显著超越典型分词方法及大规模DNA基础模型，展现出卓越性能。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.14806) | [arXiv](https://arxiv.org/abs/2511.14806)



---

### 14. 全知科学家：构建人类与AI科学家协同进化的科研生态系统

**原文标题：** OmniScientist: Toward a Co-evolving Ecosystem of Human and AI Scientists

**摘要：**
随着大语言模型（LLM）的快速发展，AI智能体在科学任务中展现出日益增强的能力，涵盖假设生成、实验设计乃至论文撰写等环节。此类智能体系统通常被称为“AI科学家”。然而，现有AI科学家主要将科学发现建模为独立搜索或优化问题，忽视了科学研究本质上是社会性协作活动这一事实。现实科学体系依赖于由协作机制、贡献归属、同行评议和结构化科学知识网络构成的复杂科研基础设施。由于缺乏对这些关键维度的建模，现有系统难以建立真正的科研生态系统，也无法与人类科学界深度互动。为弥补这一空白，我们提出全知科学家框架，该框架将人类科研的底层机制显式编码至AI科学工作流中。全知科学家不仅实现了从数据基础、文献综述、研究构思、实验自动化、科学写作到同行评议的端到端自动化，还通过模拟人类科学体系提供全方位基础设施支持，包括：（1）基于引文网络与概念关联的结构化知识体系；（2）支持多智能体无缝协作与人类研究者参与的开放式科研协议（OSP）；（3）基于双盲配对用户投票与Elo评级机制的开放评估平台（ScienceArena）。这套基础设施使智能体既能理解并利用人类知识体系，又能通过协作实现共同进化，最终培育出可持续、可扩展的创新生态系统。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.16931) | [arXiv](https://arxiv.org/abs/2511.16931)



---

### 15. 缩小智能规模：探索小型多模态模型中的感知与推理瓶颈

**原文标题：** Downscaling Intelligence: Exploring Perception and Reasoning Bottlenecks in Small Multimodal Models

**摘要：**
多模态模型的规模扩展显著推动了视觉理解与推理能力的进步，但实际应用需求呼唤更小巧高效的系统。本研究对多模态模型智能规模缩减现象展开系统性分析，探究大型语言模型容量降低对多模态能力的影响。初步发现揭示了一个有趣趋势：LLM规模缩减对视觉能力的影响远超其对LLM固有能力的继承。我们进一步探究这种性能下降究竟源于预期的视觉推理能力衰减，还是更根本的感知能力丧失。通过分离LLM规模缩减对感知能力的影响，发现性能仍会急剧下降，其降幅往往与推理能力受损程度相当甚至更甚。为突破此瓶颈，我们提出视觉提取调优方法，通过显式训练使模型在不同任务中持续提取与指令相关的视觉细节。基于这些提取的视觉信息，我们采用逐步推理机制生成答案。这些组件共同构成了"提取+思考"方法论，为该领域的效率与性能设立了新标杆。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.17487) | [arXiv](https://arxiv.org/abs/2511.17487)



---

### 16. 视觉自回归模型中的多样性本真溯源

**原文标题：** Diversity Has Always Been There in Your Visual Autoregressive Models

**摘要：**
视觉自回归模型凭借其创新的尺度递进预测范式，在推理效率与图像质量方面较传统多步自回归和扩散模型展现出显著优势，近期备受学界关注。然而，该模型虽具高效特性，却常面临多样性坍缩问题——其输出变异度降低的现象与经少量步数蒸馏的扩散模型如出一辙。本文提出DiverseVAR这一无需重新训练即可恢复视觉自回归模型生成多样性的简易有效方案。通过特征图解构，我们发现早期尺度阶段存在主导多样性生成的关键特征组件。通过抑制模型输入端的关键组件并增强输出端的对应分量，DiverseVAR在保持高保真合成能力的同时，有效释放了视觉自回归模型的内在生成潜力。实证研究表明，本方法在几乎不影响生成性能的前提下显著提升了输出多样性。相关代码将发布于https://github.com/wangtong627/DiverseVAR。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.17074) | [arXiv](https://arxiv.org/abs/2511.17074)



---

### 17. Video-R4：通过视觉反刍增强文本富集视频推理能力

**原文标题：** Video-R4: Reinforcing Text-Rich Video Reasoning with Visual Rumination

**摘要：**
理解文本富集视频需要捕捉短暂出现的小尺寸文本线索，这通常需要反复观察。然而现有视频问答模型大多基于固定帧的单次感知，导致在细粒度证据上出现幻觉识别与推理失败。受人类暂停播放、放大关键区域及重复阅读行为的启发，我们提出Video-R4（基于视觉反刍的文本富集视频推理增强模型），该视频推理大语言模型能够执行视觉反刍操作：迭代选择帧序列、放大信息密集区域、重新编码检索像素并持续更新推理状态。我们构建了两个包含可执行反刍轨迹的数据集：用于监督训练的Video-R4-CoT-17k和用于强化学习的Video-R4-RL-30k。提出多阶段反刍学习框架，通过指令微调和基于GRPO的强化学习逐步训练7B参数模型，使其掌握原子视觉操作与混合视觉操作。Video-R4-7B在M4-ViteVQA基准上取得最先进性能，并进一步泛化至多页文档问答、幻灯片问答及通用视频问答任务，证明迭代式反刍是实现像素级多模态推理的有效范式。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.17490) | [arXiv](https://arxiv.org/abs/2511.17490)



---

### 18. ICLR同行评审与反驳流程的启示

**原文标题：** Insights from the ICLR Peer Review and Rebuttal Process

**摘要：**
同行评审是科学出版的基石，在ICLR等顶级机器学习会议中亦是如此。随着投稿量的持续增长，深入理解评审过程的本质与动态特征对于提升流程效率、评审效能及论文质量至关重要。本文对ICLR 2024与2025年的同行评审过程展开大规模分析，聚焦于反驳前后的评分变化及审稿人与作者的互动关系。我们系统考察了评审分数、作者-审稿人互动程度、评审提交的时间规律以及共同审稿人的影响效应。通过量化分析与基于大语言模型的评审文本及反驳讨论分类相结合，我们揭示了各评分区间的共性优劣特征，以及最能引发分数变化的反驳策略趋势。研究发现：初始评分和共同审稿人的评级是反驳过程中分数变化的最强预测因子，这表明审稿人之间存在相互影响。对于处于录用临界区的论文，反驳环节能有效改善评审结果——深思熟虑的作者回应可显著改变审稿人观点。本研究从宏观层面为改进同行评审机制提供了实证依据，既指导作者制定有效反驳策略，也助力学术社区设计更公平高效的评审体系。代码及分数变化数据详见：https://github.com/papercopilot/iclr-insights。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.15462) | [arXiv](https://arxiv.org/abs/2511.15462)



---

### 19. 基于草图引导验证的物理感知视频生成规划方法

**原文标题：** Planning with Sketch-Guided Verification for Physics-Aware Video Generation

**摘要：**
当前视频生成方法日益依赖规划中间控制信号（如物体轨迹）以提升时序连贯性与运动保真度。然而，现有方法多采用单次规划策略（通常仅适用于简单运动）或需要多次调用视频生成器的迭代优化方案，导致计算成本高昂。为突破这些局限，我们提出SketchVerify——一种免训练的草图验证规划框架，通过在生成完整视频前引入测试时采样与验证循环，以更具动态连贯性的轨迹（即物理合理且符合指令要求的运动）提升运动规划质量。给定文本提示与参考图像，本方法首先生成多个候选运动方案，随后通过视觉语言验证器对其进行联合评估排序，该验证器可同步检验语义指令对齐度与物理合理性。为高效评估候选运动方案，我们将每条轨迹合成为静态背景上的对象组合轻量视频草图，在保持相当性能的同时规避了昂贵的重复扩散合成过程。通过迭代优化运动方案直至获得满意结果，最终将其输入轨迹条件生成器完成视频合成。在WorldModelBench与PhyWorldBench数据集上的实验表明：相较于基准方法，本方案在运动质量、物理真实感与长程一致性方面均取得显著提升，同时具有更高计算效率。消融实验进一步证明，增加轨迹候选数量能持续提升整体性能。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.17450) | [arXiv](https://arxiv.org/abs/2511.17450)



---

### 20. VLA-4D：将四维感知嵌入视觉-语言-动作模型以实现时空连贯的机器人操控

**原文标题：** VLA-4D: Embedding 4D Awareness into Vision-Language-Action Models for SpatioTemporally Coherent Robotic Manipulation

**摘要：**
视觉-语言-动作模型在通用机器人任务中展现出潜力，但在需要细粒度表征的时空连贯操控任务中仍面临挑战。现有方法通常将三维位置嵌入视觉表征以提升动作的空间精度，但难以实现动作执行的时序连贯控制。本研究提出VLA-4D——一种具备四维感知的通用VLA模型，用于实现时空连贯的机器人操控。该模型基于两个核心设计：1）四维感知视觉表征。通过提取视觉特征，将一维时间维度嵌入三维位置形成四维嵌入，并借助交叉注意力机制融合为统一视觉表征；2）时空动作表征。在传统空间动作表征基础上引入时序信息以实现时空规划，并将多模态表征对齐至大语言模型中完成时空动作预测。在此统一框架下，所设计的视觉与动作表征共同促使机器人操控实现空间平滑性与时序连贯性。此外，我们通过扩展带有时序动作标注的VLA数据集对模型进行微调。大量实验验证了本方法在不同机器人操控任务中的优越性。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.17199) | [arXiv](https://arxiv.org/abs/2511.17199)



---

### 21. 多维度攻击：揭示配备防御机制的视觉语言模型中的跨模型安全漏洞

**原文标题：** Multi-Faceted Attack: Exposing Cross-Model Vulnerabilities in Defense-Equipped Vision-Language Models

**摘要：**
随着视觉语言模型（VLM）被滥用的现象日益严重，服务提供商已部署多重防护机制，包括对齐调优、系统提示和内容审核。然而，这些防御措施在面对对抗攻击时的实际鲁棒性仍未得到充分探索。本文提出多维度攻击（MFA）框架，系统性地揭示了配备防御机制的领先VLM（如GPT-4o、Gemini-Pro和Llama-4）中普遍存在的安全漏洞。MFA的核心组件是注意力转移攻击（ATA），该攻击通过将有害指令隐藏在具有竞争目标的元任务中实现突破。我们基于奖励破解理论提供了该攻击成功的理论解释。为提升跨模型迁移能力，我们进一步提出结合轻量级迁移增强算法与简单重复策略的方法，无需模型特定微调即可联合绕过输入级和输出级过滤器。实证研究表明，针对某一视觉编码器优化的对抗图像可广泛迁移至未接触过的VLM，这表明共享视觉表征导致了跨模型安全漏洞。总体而言，MFA实现了58.5%的成功率，持续优于现有方法。在最新商业模型上，MFA达到52.8%的成功率，较次优攻击方法提升34%。这些结果对当前防御机制的感知鲁棒性提出了质疑，并揭示了现代VLM中持续存在的安全弱点。代码地址：https://github.com/cure-lab/MultiFacetedAttack

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.16110) | [arXiv](https://arxiv.org/abs/2511.16110)



---

### 22. 基于生成式合成数据的X射线违禁品检测方法优化

**原文标题：** Taming Generative Synthetic Data for X-ray Prohibited Item Detection

**摘要：**
训练违禁品检测模型需要大量X射线安检图像，但此类图像的采集与标注工作耗时费力。为解决数据不足问题，现有研究主要通过图像合成技术扩增数据集。然而传统方法普遍采用两阶段流程：第一阶段需进行劳动密集型的前景目标提取，第二阶段执行图像合成。该流程不仅引入额外人力成本，且效率低下。本文提出基于文本到图像生成的单阶段X射线安检图像合成框架（Xsyn），通过两项创新策略提升合成图像的可用性：交叉注意力优化策略利用扩散模型的交叉注意力图优化边界框标注；背景遮挡建模策略在潜空间显式建模背景遮挡以增强成像复杂度。据我们所知，相较于现有方法，Xsyn是首个在零额外人力成本下实现高质量X射线安检图像合成的方案。实验表明本方法以1.2% mAP提升超越所有现有方法，且生成的合成图像能有效提升多种X射线安检数据集和检测器中的违禁品识别性能。代码已开源：https://github.com/pILLOW-1/Xsyn/。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.15299) | [arXiv](https://arxiv.org/abs/2511.15299)



---

### 23. 重新审视显著性图谱：一种认知对齐的解释方法分类与评估框架

**原文标题：** Rethinking Saliency Maps: A Cognitive Human Aligned Taxonomy and Evaluation Framework for Explanations

**摘要：**
显著性图谱在深度学习可视化解释中应用广泛，但对其预期目标与不同用户需求之间的匹配关系仍缺乏共识。这种模糊性阻碍了解释方法的有效评估与实际应用。为弥补这一空白，我们提出参考框架×粒度（RFxG）分类法——一个基于原则的概念框架，通过两个基本维度组织显著性解释：参考框架（区分逐点解释“为何有此预测”与对比解释“为何如此而非其他”）和粒度层级（从细粒度类别层面“为何是哈士奇”到粗粒度组别层面“为何是犬类”）。借助RFxG视角，我们揭示了现有评估指标的关键局限：这些指标过度侧重逐点保真度，却忽视了对比推理与语义粒度。为系统评估RFxG双维度的解释质量，我们提出四项新颖的保真度指标。该综合评估框架将指标应用于十种前沿显著性方法、四种模型架构及三个数据集。通过倡导向用户意图驱动评估的范式转变，本研究不仅为开发忠实反映模型行为的可视化解释奠定了概念基础，更提供了关键实践工具，使其能有效契合人类认知与追问的复杂性。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.13081) | [arXiv](https://arxiv.org/abs/2511.13081)



---

### 24. 基于全栈AMD平台的基础模型训练：计算、网络与系统设计

**原文标题：** Training Foundation Models on a Full-Stack AMD Platform: Compute, Networking, and System Design

**摘要：**
本文首次在纯AMD硬件平台上开展大规模专家混合模型预训练研究，采用配备Pollara互联技术的MI300X GPU集群。我们提炼出适用于系统设计与模型架构的实践指导。在系统层面，我们提供了完整的集群与网络特性分析：通过微基准测试获取Pollara网络上不同消息规模和GPU数量下所有核心集合通信操作（全归约、规约散射、全收集、广播）的性能数据，据我们所知这是该领域的首次大规模测试。我们还提供了MI300X在核心规模与内存带宽方面的微基准测试，为模型设计提供依据。在模型层面，我们提出并应用了针对MI300X优化的Transformer规模确定规则，涵盖注意力机制与多层感知机模块，论证了能同时优化训练吞吐与推理延迟的MoE宽度配置。我们深入阐述了训练技术栈，包括常被忽视的容错机制和检查点重塑等实用工具，并提供了详细的训练配方。同时预告了我们的模型架构与基础模型——ZAYA1（760M激活参数，83亿总参数的MoE模型），该模型将在后续论文中持续优化。ZAYA1基础模型在同等规模及更大规模下，其性能可与Qwen3-4B、Gemma3-12B等领先基础模型相媲美，并在推理、数学和代码基准测试中超越Llama-3-8B和OLMoE等模型。这些成果共同证明AMD硬件、网络和软件栈已发展成熟并完成优化，具备支撑具有竞争力的大规模预训练的能力。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.17127) | [arXiv](https://arxiv.org/abs/2511.17127)



---


## 🔍 关键词云图
![关键词云图](../images/keywords_wordcloud.png)

## 📈 近期论文趋势
![论文趋势](../images/daily_papers.png)

## 🎙️ 语音播报
- [收听今日论文解读](../audio/2025-11-24_daily_papers.mp3)

## 📱 订阅渠道
- GitHub: [hf-daily-paper-newsletter-chinese](https://github.com/2404589803/hf-daily-paper-newsletter-chinese)