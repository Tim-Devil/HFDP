
# <img src="https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo.png" width="30"/> Hugging Face 2025-12-18 论文日报

## 📊 今日论文统计
- 总论文数：31
- 热门领域：RL, LLM, Transformer, Diffusion, GPT

## 📝 论文详情


### 1. Step-GUI技术报告

**原文标题：** Step-GUI Technical Report

**摘要：**
多模态大语言模型的最新进展为图形用户界面自动化带来了前所未有的机遇。然而，一个根本性挑战依然存在：如何在保持标注可靠性的同时高效获取高质量训练数据？我们提出了一种由校准步骤奖励系统驱动的自进化训练流程，该系统通过轨迹级校准将模型生成的轨迹转化为可靠的训练信号，以降低10-100倍的成本实现了超过90%的标注准确率。基于此流程，我们推出了Step-GUI系列模型（4B/8B），在保持强大通用能力的同时实现了最先进的GUI性能（8B模型：AndroidWorld 80.2%，OSWorld 48.5%，ScreenShot-Pro 62.6%）。随着GUI智能体能力提升，实际部署需要跨异构设备的标准化接口，同时保护用户隐私。为此，我们提出了GUI-MCP——首个面向GUI自动化的分层架构模型上下文协议，该协议结合底层原子操作与向本地专业模型的高层任务委派机制，实现了敏感数据全程驻留设备的高隐私执行方案。最后，为评估智能体处理真实日常使用场景的能力，我们构建了AndroidDaily基准测试，该基准基于真实移动端使用模式，涵盖高频日常场景中的3146项静态操作与235项端到端任务（8B模型：静态任务89.91%，端到端任务52.50%）。本研究推动了实用化GUI智能体的发展，并展现出在日常数字交互中实际部署的强大潜力。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.15431) | [arXiv](https://arxiv.org/abs/2512.15431)



---

### 2. DEER：基于扩散模型的草稿生成与自回归模型的验证

**原文标题：** DEER: Draft with Diffusion, Verify with Autoregressive Models

**摘要：**
效率作为大语言模型驱动的智能体与推理系统面临的关键实践挑战，正日益受到自回归解码固有延迟的限制。推测式解码通过草稿-验证机制缓解这一成本，但现有方法依赖自回归草稿模型（即草案器），这带来两个根本性问题：（1）逐步累积的不确定性导致目标模型与草案器之间的信任度持续衰减；（2）自回归草案器固有的串行解码特性。这些因素共同导致加速效果有限。本文提出扩散大语言模型草案器能够通过其根本不同的概率建模机制与高效并行解码策略，自然克服上述问题。基于此洞见，我们提出DEER——一种高效的推测式解码框架，采用扩散模型生成草稿，并通过自回归模型进行验证。为实现高质量草稿生成，DEER采用两阶段训练流程使基于扩散大语言模型的草案器与目标自回归模型对齐，并进一步采用单步解码策略生成长片段草稿。实验表明，DEER的草稿接受长度最高可达32个词元，远超EAGLE-3的10个词元。在Qwen3-30B-A3B模型上进行的HumanEval测试中，DEER实现5.54倍加速，而EAGLE-3仅实现2.41倍加速。代码、模型及演示等资源将在https://czc726.github.io/DEER/发布。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.15176) | [arXiv](https://arxiv.org/abs/2512.15176)



---

### 3. 基于雅可比强迫的快速准确因果并行解码方法

**原文标题：** Fast and Accurate Causal Parallel Decoding using Jacobi Forcing

**摘要：**
多令牌生成已成为加速基于Transformer的大模型推理的前沿范式。近期研究主要探索扩散大语言模型（dLLMs）的并行解码能力以降低推理延迟。为达到自回归模型的生成质量，现有技术通常将自回归模型适配为扩散大语言模型以实现并行解码。然而，由于预训练与后训练之间的分布失配问题，这些方法相较于自回归模型的加速效果有限。具体而言，后训练中的掩码数据分布与预训练阶段接触的真实数据分布存在显著偏差，且扩散大语言模型依赖的双向注意力机制与预训练习得的因果先验相冲突，阻碍了精确键值缓存重用的实现。为解决这一问题，我们提出雅可比强迫——一种渐进式蒸馏范式，该范式通过在模型自身生成的并行解码轨迹上进行训练，使自回归模型平滑过渡为高效的并行解码器，同时保持其预训练的因果推理特性。基于此范式训练的雅可比强迫模型在代码与数学基准测试中实现了3.8倍的实际加速，且性能损失极小。依托雅可比强迫模型的轨迹特性，我们进一步提出带拒绝回收机制的多块解码策略，使单次迭代的令牌接受数量提升至4.5倍，实际加速比接近4.0倍，实现了计算资源与推理延迟的有效权衡。代码已开源：https://github.com/hao-ai-lab/JacobiForcing。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.14681) | [arXiv](https://arxiv.org/abs/2512.14681)



---

### 4. HyperVL：面向边缘设备的高效动态多模态大语言模型

**原文标题：** HyperVL: An Efficient and Dynamic Multimodal Large Language Model for Edge Devices

**摘要：**
当前多模态大语言模型虽具备强大的感知与推理能力，但其高昂的计算与内存需求使其难以直接部署在设备端环境中。尽管小参数量模型正逐步被赋予强大的通用能力，标准的视觉Transformer编码器在处理高分辨率输入时仍存在延迟过高和内存消耗过大的瓶颈问题。为应对这些挑战，本文提出HyperVL——一种专为设备端推理设计的高效多模态大语言模型。该模型采用图像分块策略以限制峰值内存使用，并引入两项创新技术：（1）视觉分辨率压缩器，可自适应预测最优编码分辨率以消除冗余计算；（2）双重一致性学习，通过在统一框架中对齐多尺度视觉Transformer编码器，实现在共享大语言模型下视觉分支的动态切换。大量实验表明，HyperVL在多个基准测试中取得了同规模模型的领先性能。此外，其在真实移动设备上显著降低了延迟与功耗，证明了该模型在设备端多模态推理场景中的实用价值。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.14052) | [arXiv](https://arxiv.org/abs/2512.14052)



---

### 5. 面向视觉中心推理的谜题课程GRPO方法

**原文标题：** Puzzle Curriculum GRPO for Vision-Centric Reasoning

**摘要：**
近期如结果监督GRPO等强化学习方法在视觉语言模型的思维链推理方面取得进展，但关键问题依然存在：(i)依赖昂贵且存在噪声的人工标注或外部验证器；(ii)GRPO中奖励机制呈现扁平化与稀疏性特征；(iii)推理链条与最终答案间存在逻辑不一致性。本文提出谜题课程GRPO方法，这是一种基于可验证奖励的强化学习无监督方案，可在无需标注或外部验证器的前提下增强视觉语言模型的视觉推理能力。PC-GRPO通过三个自监督谜题环境替代人工标注：PatchFit、旋转任务（采用二元奖励）和拼图任务（通过分级部分奖励缓解奖励稀疏问题）。为应对扁平化奖励与群体相对优势衰减问题，我们设计了难度感知课程机制，动态调整样本权重并使训练峰值集中于中等难度区间。在训练后阶段，我们持续监控推理-答案一致性指标：与大型语言模型中基础GRPO的观测结果相似，RAC通常呈现先升后降趋势；我们的课程设计延缓了这种衰减，而一致性强化奖励方案进一步提升了RAC指标。实验表明RAC与下游任务准确率具有相关性。在多样化基准测试中，基于Qwen-7B和Qwen-3B架构的PC-GRPO显著提升了推理质量、训练稳定性及终端任务准确率，为视觉语言模型的可扩展、可验证、可解释强化学习后训练提供了实用路径。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.14944) | [arXiv](https://arxiv.org/abs/2512.14944)



---

### 6. 通用推理模型

**原文标题：** Universal Reasoning Model

**摘要：**
通用Transformer（UT）已在ARC-AGI和数独等复杂推理任务中得到广泛应用，但其性能提升的具体来源仍未得到充分探究。本研究系统分析了通用Transformer的变体，发现其在ARC-AGI上的改进主要源于Transformer的循环归纳偏置和强非线性组件，而非复杂的架构设计。基于此发现，我们提出通用推理模型（URM），通过引入短卷积和截断反向传播机制增强通用Transformer。该方法显著提升了推理性能，在ARC-AGI 1上达到53.8% pass@1，在ARC-AGI 2上达到16.0% pass@1的当前最优结果。代码已开源：https://github.com/zitian-gao/URM。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.14693) | [arXiv](https://arxiv.org/abs/2512.14693)



---

### 7. Qwen-Image-Layered：通过图层分解实现内在可编辑性

**原文标题：** Qwen-Image-Layered: Towards Inherent Editability via Layer Decomposition

**摘要：**
当前的可视化生成模型在图像编辑过程中常因栅格图像的纠缠特性而难以保持一致性，即所有视觉内容均被融合至单一画布中。相比之下，专业设计工具采用分层表示法，允许在保持一致性的同时进行独立编辑。受此启发，我们提出了Qwen-Image-Layered——一种端到端的扩散模型，能够将单张RGB图像分解为多个语义解耦的RGBA图层，从而实现内在可编辑性，其中每个RGBA图层均可独立操作而不影响其他内容。为支持可变长度的分解，我们引入了三个关键组件：（1）RGBA-VAE，用于统一RGB与RGBA图像的潜在表示；（2）VLD-MMDiT（可变图层分解MMDiT）架构，能够分解可变数量的图像图层；（3）多阶段训练策略，将预训练的图像生成模型适配为多层图像分解器。此外，针对高质量多层训练图像稀缺的问题，我们构建了一套从Photoshop文档（PSD）中提取并标注多层图像的流程。实验表明，我们的方法在分解质量上显著超越现有方法，并为一致性图像编辑建立了新范式。相关代码与模型已发布于https://github.com/QwenLM/Qwen-Image-Layered。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.15603) | [arXiv](https://arxiv.org/abs/2512.15603)



---

### 8. IC-Effect：基于上下文学习的精准高效视频特效编辑方法

**原文标题：** IC-Effect: Precise and Efficient Video Effects Editing via In-Context Learning

**摘要：**
本文提出IC-Effect，一种基于指令引导与扩散Transformer（DiT）的少样本视频视觉特效编辑框架。该框架能够合成复杂特效（如火焰、粒子与卡通角色），同时严格保持空间与时间一致性。视频特效编辑面临三重挑战：注入的特效需与背景无缝融合、背景必须完全保持不变，且需从有限的配对数据中高效学习特效模式。然而，现有视频编辑模型难以同时满足这些要求。IC-Effect将源视频作为干净的上下文条件，利用DiT模型的上下文学习能力，实现精准的背景保持与自然的特效注入。通过两阶段训练策略——先进行通用编辑适配，再通过特效低秩自适应模块（Effect-LoRA）进行特效专项学习——本框架确保了强大的指令跟随能力与鲁棒的特效建模。为进一步提升效率，我们引入时空稀疏令牌化技术，在显著降低计算量的同时保持高保真度。此外，我们发布了涵盖15种高质量视觉风格的配对特效编辑数据集。大量实验表明，IC-Effect能够实现高质量、可控且时序一致的视觉特效编辑，为视频创作开辟了新的可能性。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.15635) | [arXiv](https://arxiv.org/abs/2512.15635)



---

### 9. Skyra：基于具象化伪影推理的AI生成视频检测方法

**原文标题：** Skyra: AI-Generated Video Detection via Grounded Artifact Reasoning

**摘要：**
AI驱动的视频生成技术滥用已引发严重的社会关切，凸显了对可靠AI生成视频检测器的迫切需求。然而，现有方法大多局限于二元分类，且缺乏可供人类理解的必要解释。本文提出Skyra——一种专门的多模态大语言模型（MLLM），该模型能够识别AI生成视频中人类可感知的视觉伪影，并将其作为检测与解释的具象化证据。为实现这一目标，我们构建了首个包含细粒度人工标注的大规模AI生成视频伪影数据集ViF-CoT-4K，用于监督微调训练。进而开发了一种两阶段训练策略，系统性地提升模型在时空伪影感知、解释能力与检测精度方面的性能。为全面评估Skyra，我们构建了ViF-Bench基准测试集，该数据集包含由十余种前沿视频生成模型产生的3000个高质量样本。大量实验表明，Skyra在多项基准测试中均超越现有方法，同时我们的评估结果为推进可解释AI生成视频检测研究提供了重要启示。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.15693) | [arXiv](https://arxiv.org/abs/2512.15693)



---

### 10. 鲁棒且可校准的真实多媒体内容检测

**原文标题：** Robust and Calibrated Detection of Authentic Multimedia Content

**摘要：**
生成模型能够合成高度逼真的内容（即深度伪造内容），此类技术已被大规模滥用，严重威胁数字媒体的真实性。当前深度伪造检测方法存在两大不可靠因素：（一）事后鉴别非真实内容往往不可行（例如面对记忆样本），导致误报率理论上无界；（二）检测缺乏鲁棒性，攻击者仅需极少计算资源即可针对已知检测器实现近乎完美的规避。为应对这些局限，我们提出一种重合成框架，用于判定样本是否真实或其真实性是否可被合理质疑。本研究聚焦于高效（即计算受限）攻击场景下的高精度、低召回率设定，作出两项核心贡献：首先，我们证明经过校准的重合成方法在保持可控低误报率的同时，成为验证真实样本最可靠的途径；其次，我们验证该方法能有效抵御高效攻击，而现有方法在相同计算预算下极易被规避。该框架支持多模态数据，并融合了前沿的反演技术。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.15182) | [arXiv](https://arxiv.org/abs/2512.15182)



---

### 11. SAGE：基于强化学习的智能任意时长智能体训练及其在长视频推理中的应用

**原文标题：** SAGE: Training Smart Any-Horizon Agents for Long Video Reasoning with Reinforcement Learning

**摘要：**
人类天生具备任意时长的推理能力，即能够根据任务需求灵活选择快速浏览长视频或在必要时完整观看短视频。基于此，视频推理模型理应在不同时长范围内进行灵活推理。然而，现有前沿模型仍采用单轮预测答案的训练方式，需同时处理大量视频帧（类似于完整观看长视频），消耗大量计算资源。这引发了一个关键问题：能否开发出高性能的任意时长视频推理系统？受人类行为启发，我们首先提出SAGE智能体系统，该系统既能对长视频进行多轮推理，也能对简单问题实施单轮处理。其次，我们设计了基于Gemini-2.5-Flash的简易合成数据生成流程，用于训练SAGE的核心协调器SAGE-MM。进一步提出有效的强化学习后训练方案，该方案对培养SAGE-MM的任意时长推理能力至关重要。第三，我们构建了平均时长超过700秒的SAGE-Bench评测集，用于评估现实娱乐场景中的视频推理能力。最后，我们通过实证验证了系统架构、数据生成方法和强化学习方案的有效性：在开放式视频推理任务中取得最高6.1%的性能提升，在超过10分钟的长视频任务中更是获得8.2%的显著改进。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.13874) | [arXiv](https://arxiv.org/abs/2512.13874)



---

### 12. MMSI-Video-Bench：面向视频空间智能的综合基准测试框架

**原文标题：** MMSI-Video-Bench: A Holistic Benchmark for Video-Based Spatial Intelligence

**摘要：**
对连续视觉输入的空间理解能力是多模态大语言模型发展为物理环境中通用助手的关键。然而，目前仍缺乏能够全面评估该目标进展的综合基准。本研究提出MMSI-Video-Bench——首个针对多模态大语言模型视频空间智能的全人工标注基准。该基准通过感知、规划、预测和跨视频推理的四层评估框架，基于来自25个公开数据集及自建视频的1,278个片段构建了1,106个精细化问题。每个问题项均由三维视觉专家精心设计并复核，配备解释性依据以确保精准无歧义的空间 grounding。凭借其多样化的数据来源与全维度任务覆盖，本基准还支持三个领域导向的子基准（室内场景感知基准、机器人操作基准与空间 grounding 基准）以实现针对性能力评估。通过对25个开源与商业多模态大语言模型的系统性评测，我们揭示了显著的人机能力差距：多数模型表现接近随机猜测，最优推理模型仍落后人类水平近60%。进一步研究发现，经过空间微调的模型在本基准上仍存在泛化失效问题。细粒度错误分析揭示了模型在几何推理、运动 grounding、长时程预测及跨视频关联等方面存在系统性缺陷。实验还表明，传统帧采样策略在本推理密集型基准上迁移效果不佳，三维空间线索与思维链提示技术均未带来显著性能提升。我们期望该基准能为推进视频空间智能研究建立坚实的测试平台。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.10863) | [arXiv](https://arxiv.org/abs/2512.10863)



---

### 13. 大语言模型能否引导自身探索？基于梯度引导的强化学习用于大语言模型推理

**原文标题：** Can LLMs Guide Their Own Exploration? Gradient-Guided Reinforcement Learning for LLM Reasoning

**摘要：**
强化学习已成为增强大语言模型推理能力的关键手段，然而当前的探索机制本质上仍与这些模型的实际学习方式存在错配。熵奖励和外部语义比较器虽能鼓励表层多样性，但无法保证采样轨迹在影响优化过程的更新方向上存在差异。本文提出G2RL，一种梯度引导的强化学习框架，其探索驱动力并非来自外部启发式规则，而是源于模型自身的一阶更新几何结构。对于每个响应，G2RL从模型最终层的敏感度中构建序列级特征（该特征可通过标准前向传播以可忽略的成本获取），并通过在采样组内比较这些特征来衡量每条轨迹将如何重塑策略。引入新颖梯度方向的轨迹会获得有界的乘性奖励缩放因子，而冗余或偏离流形的更新则会被弱化，从而产生一种自然对齐PPO式稳定性与KL控制的自参照探索信号。在Qwen3基础版1.7B和4B模型上，针对数学与通用推理基准测试（MATH500、AMC、AIME24、AIME25、GPQA、MMLUpro），G2RL在pass@1、maj@16和pass@k指标上均持续优于基于熵的GRPO及外部嵌入方法。通过对诱导几何结构的分析，我们发现G2RL在保持语义连贯性的同时，将探索范围扩展至显著更多正交且常相对立的梯度方向，这表明策略自身的更新空间为大语言模型强化学习中的探索引导提供了更为可靠且有效的基准。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.15687) | [arXiv](https://arxiv.org/abs/2512.15687)



---

### 14. FiNERweb：面向可扩展多语言命名实体识别的数据集与资源

**原文标题：** FiNERweb: Datasets and Artifacts for Scalable Multilingual Named Entity Recognition

**摘要：**
近期多语言命名实体识别（NER）研究表明，大语言模型（LLMs）能够提供有效的合成监督数据，但此类数据集大多作为广泛实验的副产品出现，而非系统化、可复用的资源。本文提出FiNERweb——一个将师生范式扩展至91种语言和25种文字体系的数据集构建流程。该方法基于FineWeb-Edu，通过训练回归模型识别NER相关文本段落，并利用多语言大语言模型进行标注，最终生成约22.5万条文本段落及23.5万个独立实体标签。实验表明：回归模型F1值超过84；基于FiNERweb训练的模型在英语、泰语和斯瓦希里语的零样本迁移场景中，仅使用基线模型1/19的数据量即可获得相当或更优的性能。此外，我们采用LLM-as-a-judge方法评估标注质量，在忠实度（5分制得3.99分）和完整性（5分制得4.05分）方面均保持稳定高分，表明标注结果可靠且信息丰富。值得注意的是，由于当前最优模型使用目标语言标签评估时F1值会下降0.02至0.09，我们同时发布了包含英文标签及对应目标语言翻译标签的数据集。我们将FiNERweb数据集及其全套资源向研究社区公开，以促进多语言命名实体识别领域更高效的师生训练范式发展。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.13884) | [arXiv](https://arxiv.org/abs/2512.13884)



---

### 15. DiffusionVL：将任意自回归模型转化为扩散视觉语言模型

**原文标题：** DiffusionVL: Translating Any Autoregressive Models into Diffusion Vision Language Models

**摘要：**
近年来在多模态研究中，扩散范式因其独特的解码优势，已成为自回归范式的有前景替代方案。然而，由于基础扩散语言模型的能力限制，扩散视觉语言模型的性能仍显著落后于主流模型。这引出了一个简单而根本的问题：能否基于现有强大的自回归模型构建扩散视觉语言模型？为此，我们提出了DiffusionVL，一个可从任意强大自回归模型转化而来的扩散视觉语言模型系列。通过简单的微调，我们成功将自回归预训练模型适配至扩散范式。该方法带来两个关键发现：（1）从基于自回归的多模态模型向扩散范式的转换效果显著；（2）将自回归语言模型直接转化为扩散视觉语言模型同样可行，其性能可与LLaVA风格的视觉指令微调模型相竞争。此外，我们在扩散视觉语言模型中引入了支持任意长度生成和KV缓存复用的块解码设计，实现了显著的推理加速。我们进行了大量实验：尽管训练数据量不足先前方法所需数据的5%，DiffusionVL仍实现了全面的性能提升——在MMMU-Pro（视觉）基准上提升34.4%，在MME（认知）基准上提升37.5%，同时推理速度提升2倍。模型与代码已发布于https://github.com/hustvl/DiffusionVL。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.15713) | [arXiv](https://arxiv.org/abs/2512.15713)



---

### 16. VOYAGER：一种基于大语言模型的无训练多样化数据集生成方法

**原文标题：** VOYAGER: A Training Free Approach for Generating Diverse Datasets using LLMs

**摘要：**
大语言模型正日益广泛地用于生成合成数据集，以支持下游模型的评估与训练。然而，已有研究指出，此类生成的数据往往缺乏多样性。本文提出Voyager，一种基于原理的新型方法，用于生成多样化数据集。该方法采用迭代策略，并借助行列式点过程的理论工具，直接优化一个旨在提升数据集多样性的数学指标。此外，该方法无需训练、适用于闭源模型，且具有良好的可扩展性。我们不仅从理论上论证了本方法的有效性，还通过综合实验证明，Voyager在多样性指标上显著优于现有主流基线方法，实现了1.5至3倍的提升。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.12072) | [arXiv](https://arxiv.org/abs/2512.12072)



---

### 17. 基于自重采样的自回归视频扩散模型端到端训练方法

**原文标题：** End-to-End Training for Autoregressive Video Diffusion via Self-Resampling

**摘要：**
自回归视频扩散模型在环境模拟方面具有潜力，但容易受到训练-测试不匹配导致的曝光偏差影响。现有研究主要通过后训练方法解决此问题，但通常依赖于双向教师模型或在线判别器。为实现端到端解决方案，本文提出重采样强制训练框架——一种无需教师模型的架构，支持从零开始进行大规模自回归视频模型训练。该方法的核心理念是自重采样机制，通过在训练过程中模拟历史帧在推理阶段可能出现的模型误差。基于这些降质历史帧，稀疏因果掩码在保持时序因果性的同时，实现了帧级扩散损失的并行训练。为提升长序列生成效率，我们进一步提出历史路由机制——这是一种无需参数的动态检索方法，能够为每个查询帧自适应选取最相关的k个历史帧。实验结果表明，本方法在性能上可与基于知识蒸馏的基线模型相媲美，同时得益于原生长度训练策略，在长视频生成中展现出更优越的时序一致性。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.15702) | [arXiv](https://arxiv.org/abs/2512.15702)



---

### 18. VABench：面向音视频生成的综合基准测试框架

**原文标题：** VABench: A Comprehensive Benchmark for Audio-Video Generation

**摘要：**
近期视频生成技术取得了显著进展，使得模型能够生成具有同步音频的视觉吸引力视频。虽然现有视频生成基准测试为视觉质量提供了全面的评估指标，但其缺乏对音视频生成（尤其是旨在生成同步音视频输出的模型）具有说服力的评估体系。为弥补这一空白，我们提出了VABench——一个综合性的多维基准测试框架，旨在系统评估同步音视频生成能力。VABench涵盖三大任务类型：文本到音视频生成、图像到音视频生成以及立体声音视频生成。该框架进一步构建了两大评估模块，覆盖15个评估维度，专门评估文本-视频、文本-音频、视频-音频的成对相似性，音视频同步性，唇语一致性，以及精心设计的音视频问答对等。此外，VABench涵盖七大内容类别：动物、人声、音乐、环境声音、同步物理声音、复杂场景和虚拟世界。我们通过系统化分析与可视化呈现评估结果，旨在为评估具备同步音频能力的视频生成模型建立新标准，并推动该领域的全面进步。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.09299) | [arXiv](https://arxiv.org/abs/2512.09299)



---

### 19. 追求像素监督的视觉预训练

**原文标题：** In Pursuit of Pixel Supervision for Visual Pre-training

**摘要：**
在最基本的层面上，像素是我们感知世界的视觉信息源头。像素包含从低级属性到高级概念的所有层次信息。自编码器代表了从像素或其他原始输入中学习表征的经典且历史悠久的范式。在本研究中，我们证明基于自编码器的自监督学习至今仍具竞争力，能够为下游任务生成强大的表征，同时保持简单、稳定和高效。我们的模型代号为“Pixio”，是一种增强型掩码自编码器（MAE），具有更具挑战性的预训练任务和更强大的架构。该模型通过自筛选策略在20亿张网络爬取图像上进行训练，仅需极少量人工筛选。Pixio在广泛的现实下游任务中表现优异，包括单目深度估计（如Depth Anything）、前馈式三维重建（即MapAnything）、语义分割和机器人学习，其性能优于或匹配相似规模训练的DINOv3模型。我们的研究结果表明，像素空间自监督学习可作为潜在空间方法的有力替代和补充方案。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.15715) | [arXiv](https://arxiv.org/abs/2512.15715)



---

### 20. 自动驾驶中的视觉-语言-动作模型：过去、现在与未来

**原文标题：** Vision-Language-Action Models for Autonomous Driving: Past, Present, and Future

**摘要：**
自动驾驶长期依赖模块化的“感知-决策-动作”流程，其中人工设计的接口与基于规则的组件在复杂或长尾场景中常出现失效。其级联式设计会进一步传播感知误差，导致下游规划与控制性能下降。视觉-动作模型通过从视觉输入到动作的直接映射学习，部分解决了上述局限，但仍存在可解释性差、对分布偏移敏感、缺乏结构化推理或指令跟随能力等问题。近年来，大语言模型与多模态学习的进展推动了视觉-语言-动作框架的兴起，该框架将感知与基于语言的决策相结合。通过统一视觉理解、语言推理与可执行输出，视觉-语言-动作模型为实现更具可解释性、泛化性及符合人类预期的驾驶策略提供了路径。本文对自动驾驶领域新兴的视觉-语言-动作研究体系进行了系统性梳理。我们追溯了从早期视觉-动作方法到现代视觉-语言-动作框架的演进历程，并将现有方法归纳为两大主要范式：端到端视觉-语言-动作模型（在单一模型中整合感知、推理与规划）与双系统视觉-语言-动作模型（通过视觉语言模型进行慢速决策，通过规划器执行快速安全关键任务）。在此分类基础上，我们进一步区分了文本型与数值型动作生成器、显式与隐式引导机制等子类别。同时，本文总结了用于评估基于视觉-语言-动作的驾驶系统的代表性数据集与基准测试，并着重指出了包括鲁棒性、可解释性与指令忠实度在内的关键挑战与未来方向。整体而言，本研究旨在为推动符合人类需求的自动驾驶系统建立清晰的理论基础。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.16760) | [arXiv](https://arxiv.org/abs/2512.16760)



---

### 21. FrontierCS：进化智能面临的演进挑战

**原文标题：** FrontierCS: Evolving Challenges for Evolving Intelligence

**摘要：**
本文提出FrontierCS基准测试集，该集合包含156个跨计算机科学多领域的开放式问题，由计算机科学博士、顶级竞赛编程参与者及命题专家共同设计与评审。与现有聚焦已知最优解任务的基准不同，FrontierCS针对那些最优解未知但解的质量可被客观评估的问题。模型需通过实现可执行程序（而非直接输出答案）来解决这些任务。该基准包含两类问题：一是具有客观部分评分机制的竞赛编程NP难变体算法问题，二是具备相同特性的研究型问题。每个问题均配备专家参考解决方案与自动评估器。通过融合开放式设计、可量化进展与专家评审机制，FrontierCS构建了处于计算机科学难度前沿的评估基准。实证研究表明：前沿推理模型在算法与研究两类任务上仍远落后于人类专家；仅增加推理预算无法弥合该差距；模型常过度优化生成仅可运行的代码，而非探索高质量算法与系统设计。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.15699) | [arXiv](https://arxiv.org/abs/2512.15699)



---

### 22. VTCBench：视觉语言模型能否理解视觉文本压缩下的长上下文？

**原文标题：** VTCBench: Can Vision-Language Models Understand Long Context with Vision-Text Compression?

**摘要：**
扩展大型语言模型（LLM）上下文窗口所带来的计算与内存开销严重限制了其可扩展性。视觉文本压缩（VTC）作为一种值得关注的解决方案，以DeepSeek-OCR和Glyph等框架为代表，通过将长文本转换为密集的二维视觉表示，实现了3倍至20倍的标记压缩率。然而，这种高信息密度对视觉语言模型（VLM）核心长上下文理解能力的影响尚未得到充分研究。为填补这一空白，我们提出了首个针对VTC的基准测试，并系统评估了VLM在三种长上下文理解场景下的性能：VTC-检索，评估模型检索与整合信息的能力；VTC-推理，要求模型通过推断潜在关联来定位词汇重叠度极低的事实；以及VTC-记忆，衡量模型在长期对话记忆中进行综合问答的能力。此外，我们构建了VTCBench-Wild以模拟多样化的输入场景。我们在基准测试中对主流的开源与商业模型进行了全面评估。结果表明，尽管大多数VLM能够较好地解码文本信息（如OCR），但在处理VTC压缩信息时，其长上下文理解能力却表现出令人惊讶的不足，难以捕捉上下文中的长程关联或依赖关系。本研究深化了对VTC的理解，并为设计更高效、可扩展的VLM奠定了基础。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.15649) | [arXiv](https://arxiv.org/abs/2512.15649)



---

### 23. SCOPE：基于提示演化的智能体效能增强方法

**原文标题：** SCOPE: Prompt Evolution for Enhancing Agent Effectiveness

**摘要：**
大语言模型智能体正日益部署于产生海量动态上下文的环境中。然而，一个关键瓶颈依然存在：尽管智能体能够访问这些上下文，但其静态提示缺乏有效管理机制，导致纠正性错误与增强性失效反复出现。为弥补这一能力缺陷，本文提出SCOPE（基于提示演化的自进化上下文优化框架）。该框架将上下文管理构建为在线优化问题，通过综合分析执行轨迹生成指导原则，实现智能体提示的自动化演进。我们提出双流机制，在战术特异性（解决即时错误）与战略通用性（演化长期原则）之间实现动态平衡。此外，我们引入视角驱动探索方法以最大化策略覆盖范围，提升智能体针对任意任务具备正确策略的可能性。在HLE基准测试上的实验表明，SCOPE在无需人工干预的情况下将任务成功率从14.23%提升至38.64%。相关代码已开源：https://github.com/JarvisPei/SCOPE。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.15374) | [arXiv](https://arxiv.org/abs/2512.15374)



---

### 24. Nano Banana Pro是低层次视觉全能手吗？基于14项任务与40个数据集的全方位评估

**原文标题：** Is Nano Banana Pro a Low-Level Vision All-Rounder? A Comprehensive Evaluation on 14 Tasks and 40 Datasets

**摘要：**
文本到图像生成模型的快速发展正在彻底改变视觉内容创作领域。尽管如Nano Banana Pro等商业产品已获得广泛关注，但其作为传统低层次视觉问题通用解决方案的潜力仍未得到充分探索。本研究针对核心问题展开探讨：Nano Banana Pro是否具备低层次视觉全能处理能力？我们通过涵盖40个多样化数据集的14类低层次视觉任务，进行了全面的零样本性能评估。在不进行微调的情况下仅使用简单文本提示，我们将Nano Banana Pro与当前最先进的专用模型进行系统对比。深入分析揭示出显著的性能二分现象：Nano Banana Pro在主观视觉质量方面表现优异，常能生成超越专用模型的合理高频细节，但在传统基于参考图像的定量指标上存在不足。我们认为这种差异源于生成模型固有的随机性特征，使其难以满足传统度量标准对像素级一致性的严格要求。本报告确认Nano Banana Pro在低层次视觉任务中具备零样本竞争潜力，同时指出要达到领域专用模型的高保真度仍面临重大挑战。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.15110) | [arXiv](https://arxiv.org/abs/2512.15110)



---

### 25. WAY：全球AIS轨迹中的船舶目的地估计

**原文标题：** WAY: Estimation of Vessel Destination in Worldwide AIS Trajectory

**摘要：**
自动识别系统（AIS）为数据驱动的海事监控提供了可能，但其存在可靠性问题与数据间隔不规则等局限性。本研究针对全球范围AIS数据提出一种差异化船舶目的地估计方法，通过将长距离港到港轨迹重构为嵌套序列结构，在空间网格框架下缓解时空偏差的同时保持细节分辨率。我们提出名为WAY的新型深度学习架构，专门处理重构后的轨迹以实现提前数天至数周的长时目的地预测。该架构包含轨迹表征层与通道聚合序列处理模块：表征层从运动学与非运动学特征生成多通道向量序列；通道聚合序列处理模块采用多头通道注意力与自注意力机制实现特征聚合与序列信息传递。此外，我们提出任务专用的梯度丢弃技术，通过基于样本长度的随机梯度流阻断机制，在单标签数据上实现多对多训练，避免偏差反馈激增。基于五年AIS数据的实验表明，无论轨迹进展阶段如何，WAY均优于传统空间网格方法；结果同时验证梯度丢弃技术能有效提升模型性能。最后，我们通过到达时间估计的多任务学习框架，探讨了WAY在实际应用中的潜力。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.13190) | [arXiv](https://arxiv.org/abs/2512.13190)



---

### 26. 理解与改进双曲深度强化学习

**原文标题：** Understanding and Improving Hyperbolic Deep Reinforcement Learning

**摘要：**
强化学习智能体的性能关键取决于其底层特征表示的质量。双曲特征空间非常适合这一目的，因为它们能够自然地捕捉复杂强化学习环境中普遍存在的层次化与关系化结构。然而，由于强化学习的非平稳性，利用这些空间通常面临优化挑战。本研究确定了决定双曲深度强化学习智能体训练成败的关键因素。通过分析双曲几何的庞加莱球模型和双曲面模型中核心操作的梯度，我们发现大范数嵌入会破坏基于梯度的训练稳定性，导致近端策略优化算法中的信任域约束被违反。基于这些发现，我们提出了Hyper++——一种新型双曲近端策略优化智能体，它包含三个核心组件：（1）通过分类价值损失替代回归损失实现稳定的评论家训练；（2）特征正则化在保证范数有界的同时，避免了裁剪操作引发的维度灾难；（3）采用优化友好的双曲网络层表达形式。在ProcGen基准测试中，Hyper++能保证稳定的学习过程，其性能超越现有双曲智能体，并将实际训练时间缩短约30%。在基于Double DQN的Atari-5测试中，Hyper++显著优于欧几里得空间和双曲空间的基线方法。代码已发布于https://github.com/Probabilistic-and-Interactive-ML/hyper-rl。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.14202) | [arXiv](https://arxiv.org/abs/2512.14202)



---

### 27. 用于可解释与鲁棒模型训练的混合归因先验

**原文标题：** Hybrid Attribution Priors for Explainable and Robust Model Training

**摘要：**
小型语言模型因其低延迟与轻量部署的优势，在分类等任务中被广泛使用。随着可解释性与鲁棒性日益受到重视，解释引导学习通过引入基于归因的监督机制，已成为一种有效的训练框架；然而，如何获取通用且可靠的归因先验仍面临重大挑战。通过对分类场景中代表性归因方法的分析，我们发现尽管这些方法能可靠地突出类别相关词元，却常聚焦于语义相似类别间共有的通用关键词。由于此类类别在标准训练下本就难以区分，此类归因提供的判别性线索不足，限制了其提升模型区分能力的效果。为突破这一局限，我们提出**类别感知归因先验**——一种新颖的归因先验提取框架，可引导语言模型捕捉细粒度类别差异，并生成更显著、更具判别力的归因先验。基于此，我们进一步提出**CAP混合框架**，将CAP生成的先验与现有归因技术的先验相结合，形成更全面、均衡的监督信号。通过使模型的自归因与这些增强后的先验对齐，我们的方法促进了多样化、决策相关特征的学习。在全数据、少样本及对抗场景中的大量实验表明，该方法能持续提升模型的可解释性与鲁棒性。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.14719) | [arXiv](https://arxiv.org/abs/2512.14719)



---

### 28. SonicMoE：通过IO感知与分块感知优化加速混合专家模型

**原文标题：** SonicMoE: Accelerating MoE with IO and Tile-aware Optimizations

**摘要：**
混合专家模型已成为扩展语言模型规模而不显著增加计算成本的事实标准架构。近期研究显示，MoE模型呈现出高专家粒度化（更小的专家中间维度）与高稀疏化（激活专家数量恒定而专家总数增加）的明显趋势，从而提升单位浮点运算的模型质量。然而，细粒度MoE因更高的IO开销导致激活内存占用增加与硬件效率降低，而稀疏化MoE则因分组通用矩阵乘法内核中的填充操作产生计算浪费。为此，我们提出一种内存高效算法，以前向传播与反向传播过程中最小化激活缓存的方式计算MoE的前向与反向传播。同时，我们设计了可重叠内存IO与计算的GPU内核，使所有MoE架构受益。此外，我们创新性地提出"令牌舍入"方法，以最小化分组通用矩阵乘法内核中填充导致的无效计算。实验结果表明，对于细粒度70亿参数MoE模型，我们的SonicMoE方法在Hopper GPU上相比ScatterMoE的BF16 MoE内核减少45%的激活内存占用，并实现1.86倍的计算吞吐量提升。具体而言，在使用lm-engine代码库配合FSDP-2进行70亿参数MoE模型训练时，SonicMoE在64张H100显卡上达到每日2130亿令牌的训练吞吐量，与ScatterMoE在96张H100显卡上每日2250亿令牌的吞吐量相当。在高MoE稀疏度场景下，我们提出的分块感知令牌舍入算法相比传统Top-K路由机制，在保持相近下游性能的同时可获得额外1.16倍的内核执行加速。我们将所有内核代码开源，以促进更高效的MoE模型训练。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.14080) | [arXiv](https://arxiv.org/abs/2512.14080)



---

### 29. LikeBench：评估大型语言模型个性化中的主观喜好度

**原文标题：** LikeBench: Evaluating Subjective Likability in LLMs for Personalization

**摘要：**
个性化大型语言模型应能记忆用户信息、正确应用这些信息，并随时间推移适应用户偏好以提供更受欢迎的回复。现有的大型语言模型个性化基准主要围绕两个维度展开：准确回忆用户信息以及在后续任务中准确应用已记忆信息。我们认为，第三个维度——喜好度——既是主观的，又是用户体验的核心，却在当前基准中未能得到充分衡量。为全面评估喜好度，我们提出了LikeBench，这是一个多会话动态评估框架，通过衡量大型语言模型随时间推移适应用户偏好以提供更受欢迎回复的能力，从多个维度评估喜好度。在LikeBench中，大型语言模型与模拟用户进行对话，仅通过持续对话学习用户偏好。随着交互的展开，模型尝试调整回复策略，并在每轮对话后由同一模拟用户从七个维度评估其喜好度表现。据我们所知，我们首次将喜好度分解为七个诊断性指标：情感适应性、正式度匹配、知识适应性、指代理解、对话长度适配度、幽默适配度及话题呼应能力，这有助于精准定位模型短板。为使模拟用户更具真实性与区分度，LikeBench采用基于心理学理论的细粒度描述性人物设定，而非以往研究中基于粗粒度高/低特征评级的设定。我们的基准测试表明，强大的记忆性能并不能保证高喜好度：尽管Qwen3的记忆准确率更高（93%，每档案43条信息），但DeepSeek R1在记忆准确率较低（86%，每档案17条信息）的情况下，其喜好度得分仍超出Qwen3达28%。即使是GPT-5等前沿模型，在简短交互中表现良好，但在更长、更嘈杂的交互中仅展现出有限的鲁棒性。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.13077) | [arXiv](https://arxiv.org/abs/2512.13077)



---

### 30. 用于学习多模态机器人操作的同时触觉-视觉感知

**原文标题：** Simultaneous Tactile-Visual Perception for Learning Multimodal Robot Manipulation

**摘要：**
机器人操作需要丰富的多模态感知和有效的学习框架以处理复杂的现实世界任务。结合触觉与视觉感知的透皮（STS）传感器提供了有前景的感知能力，而现代模仿学习为策略获取提供了强大工具。然而，现有STS设计缺乏同步多模态感知能力，且触觉跟踪可靠性不足。此外，如何将这些丰富的多模态信号整合到基于学习的操作流程中仍是开放挑战。本文提出TacThru传感器——一种能实现同步视觉感知与鲁棒触觉信号提取的STS传感器，以及TacThru-UMI模仿学习框架——该框架利用多模态信号进行机械操作。我们的传感器采用全透明弹性体、持久照明、新型关键线标记和高效跟踪技术，学习系统则通过基于Transformer的扩散策略整合多模态信号。在五项具有挑战性的现实任务实验中，TacThru-UMI平均成功率达到85.5%，显著优于交替触觉-视觉感知（66.3%）和纯视觉感知（55.4%）的基线方法。该系统在关键场景中表现优异，包括对细薄柔软物体的接触检测，以及需要多模态协调的精密操作。本研究表明，将同步多模态感知与现代学习框架相结合，能够实现更精确、适应性更强的机器人操作。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.09851) | [arXiv](https://arxiv.org/abs/2512.09851)



---

### 31. 迈向无缝交互：交互式三维会话头部动态的因果轮次建模

**原文标题：** Towards Seamless Interaction: Causal Turn-Level Modeling of Interactive 3D Conversational Head Dynamics

**摘要：**
人类会话涉及言语与非语言线索（如传达注意力和情感的点头、视线转移及面部表情）的持续交换。在三维空间中建模这种双向动态对于构建富有表现力的虚拟化身和交互式机器人至关重要。然而，现有框架常将说话与聆听视为独立过程，或依赖于非因果的全序列建模，导致跨轮次的时间连贯性受限。本文提出TIMAR（轮次交错掩码自回归）框架，这是一种用于三维会话头部生成的因果建模框架，将对话建模为交错的多模态视听上下文。该框架在每轮对话中融合多模态信息，并应用轮次级因果注意力机制以累积会话历史，同时通过轻量级扩散头部预测连续的三维头部动态，从而捕捉协调性与表达变异性。在DualTalk基准测试上的实验表明，TIMAR在测试集上将弗雷歇距离与均方误差降低了15-30%，并在分布外数据上取得相近的增益。源代码将在GitHub仓库 https://github.com/CoderChen01/towards-seamleass-interaction 中公开。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.15340) | [arXiv](https://arxiv.org/abs/2512.15340)



---


## 🔍 关键词云图
![关键词云图](../images/keywords_wordcloud.png)

## 📈 近期论文趋势
![论文趋势](../images/daily_papers.png)

## 🎙️ 语音播报
- [收听今日论文解读](../audio/2025-12-18_daily_papers.mp3)

## 📱 订阅渠道
- GitHub: [hf-daily-paper-newsletter-chinese](https://github.com/2404589803/hf-daily-paper-newsletter-chinese)