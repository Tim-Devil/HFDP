
# <img src="https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo.png" width="30"/> Hugging Face 2025-12-18 论文日报

## 📊 今日论文统计
- 总论文数：31
- 热门领域：RL, LLM, Transformer, Diffusion, GPT

## 📝 论文详情


### 1. Step-GUI技术报告

**原文标题：** Step-GUI Technical Report

**摘要：**
多模态大语言模型的最新进展为图形用户界面自动化带来了前所未有的机遇。然而，一个根本性挑战依然存在：如何在保持标注可靠性的同时高效获取高质量训练数据？我们提出了一种由校准步骤奖励系统驱动的自演化训练流程，该流程通过轨迹级校准将模型生成的轨迹转化为可靠的训练信号，在降低10-100倍成本的同时实现超过90%的标注准确率。基于此流程，我们推出了Step-GUI系列模型（4B/8B参数），在保持强大通用能力的同时实现了业界领先的GUI性能（8B模型：AndroidWorld 80.2%，OSWorld 48.5%，ScreenShot-Pro 62.6%）。随着GUI智能体能力提升，实际部署需要跨异构设备的标准化接口，同时保护用户隐私。为此，我们提出了GUI-MCP——首个面向GUI自动化的模型上下文协议，采用分层架构结合底层原子操作与高层任务委派至本地专业模型，实现敏感数据全程驻留设备的高隐私执行方案。最后，为评估智能体处理真实日常使用场景的能力，我们构建了AndroidDaily基准测试，该测试基于真实移动端使用模式，涵盖高频日常场景中的3146项静态操作与235项端到端任务（8B模型：静态任务89.91%，端到端任务52.50%）。本工作推动了实用化GUI智能体的发展，并在日常数字交互的实际部署中展现出强大潜力。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.15431) | [arXiv](https://arxiv.org/abs/2512.15431)



---

### 2. DEER：基于扩散模型的草稿生成与自回归模型的验证

**原文标题：** DEER: Draft with Diffusion, Verify with Autoregressive Models

**摘要：**
效率作为大语言模型驱动的智能体与推理系统面临的关键实践挑战，日益受到自回归解码固有延迟的限制。推测式解码通过“草稿-验证”机制缓解这一成本，但现有方法依赖自回归草稿模型（即草案器），这带来两个根本性问题：（1）逐步累积的不确定性导致目标模型与草案器之间的信任度持续衰减；（2）自回归草案器固有的串行解码特性。这些因素共同导致加速效果受限。本文提出，扩散大语言模型草案器能通过其根本不同的概率建模机制与高效并行解码策略，自然克服上述问题。基于此洞见，我们提出DEER——一种高效的推测式解码框架，采用扩散模型生成草稿，并通过自回归模型进行验证。为实现高质量草稿生成，DEER采用两阶段训练流程使基于扩散大语言模型的草案器与目标自回归模型对齐，并进一步采用单步解码策略生成长段草稿。实验表明，DEER的草稿接受长度最高可达32个词元，远超EAGLE-3的10个词元。在HumanEval基准测试中，基于Qwen3-30B-A3B模型的DEER实现了5.54倍加速，而EAGLE-3仅达到2.41倍加速。代码、模型及演示等资源将于https://czc726.github.io/DEER/发布。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.15176) | [arXiv](https://arxiv.org/abs/2512.15176)



---

### 3. 基于雅可比强迫的快速准确因果并行解码方法

**原文标题：** Fast and Accurate Causal Parallel Decoding using Jacobi Forcing

**摘要：**
多令牌生成已成为加速基于Transformer的大模型推理的重要范式。近期研究主要探索扩散大语言模型（dLLMs）的并行解码能力以降低推理延迟。为达到自回归模型的生成质量，现有技术通过将自回归模型适配为dLLMs以实现并行解码。然而，由于预训练与后训练之间的不匹配，这些方法相较于自回归模型的加速效果有限。具体而言，后训练中的掩码数据分布与预训练阶段接触的真实数据分布存在显著偏差，且dLLMs依赖的双向注意力机制与预训练习得的因果先验相冲突，阻碍了精确键值缓存重用的实现。针对此问题，本文提出雅可比强迫方法——一种渐进式蒸馏范式，通过在模型自身生成的并行解码轨迹上进行训练，使自回归模型平滑过渡为高效并行解码器，同时保持其预训练的因果推理特性。基于该范式训练的雅可比强迫模型在代码与数学基准测试中实现了3.8倍的实际加速，且性能损失极小。结合雅可比强迫模型的轨迹特性，我们进一步提出带拒绝回收机制的多块解码策略，使单次迭代的令牌接受数量提升至4.5倍，实现近4.0倍的实际加速，有效通过增加计算量换取更低推理延迟。代码已开源：https://github.com/hao-ai-lab/JacobiForcing。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.14681) | [arXiv](https://arxiv.org/abs/2512.14681)



---

### 4. HyperVL：面向边缘设备的高效动态多模态大语言模型

**原文标题：** HyperVL: An Efficient and Dynamic Multimodal Large Language Model for Edge Devices

**摘要：**
当前多模态大语言模型虽具备强大的感知与推理能力，但其高昂的计算与内存需求使其难以直接部署于设备端环境。尽管小参数量模型正逐步被赋予强大的通用能力，标准视觉Transformer编码器在处理高分辨率输入时仍存在延迟过高与内存消耗过大的关键瓶颈。为应对这些挑战，本文提出HyperVL——一种专为设备端推理设计的高效多模态大语言模型。HyperVL采用图像分块策略以控制峰值内存占用，并融合两项创新技术：（1）视觉分辨率压缩器，可自适应预测最优编码分辨率以消除冗余计算；（2）双重一致性学习，将多尺度视觉Transformer编码器对齐至统一框架，实现在共享大语言模型下视觉分支的动态切换。大量实验表明，HyperVL在多个基准测试中取得了同规模模型的领先性能，同时在真实移动设备上显著降低了延迟与功耗，证明了其在设备端多模态推理场景中的实用价值。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.14052) | [arXiv](https://arxiv.org/abs/2512.14052)



---

### 5. 面向视觉中心推理的谜题课程GRPO方法

**原文标题：** Puzzle Curriculum GRPO for Vision-Centric Reasoning

**摘要：**
近期如结果监督GRPO等强化学习方法在视觉语言模型的思维链推理方面取得进展，但关键问题依然存在：(1)依赖昂贵且存在噪声的人工标注或外部验证器；(2)GRPO中扁平稀疏的奖励机制；(3)推理链与最终答案间的逻辑不一致性。我们提出谜题课程GRPO（PC-GRPO），这是一种基于可验证奖励的强化学习无监督方案，可在无需标注或外部验证器的前提下增强视觉语言模型的视觉推理能力。PC-GRPO通过三个自监督谜题环境替代人工标注：PatchFit、旋转任务（采用二元奖励）和拼图任务（通过分级部分奖励缓解奖励稀疏问题）。为应对扁平奖励与消退的组间相对优势，我们引入难度感知课程机制，动态调整样本权重并在中等难度区间达到峰值。我们在后训练阶段持续监测推理-答案一致性：与大型语言模型中原始GRPO的观测结果相似，该一致性通常先升后降；我们的课程设计延缓了这种衰退，而强化一致性的奖励机制能进一步提升该指标。推理-答案一致性与下游任务准确率具有相关性。在多样化基准测试及Qwen-7B/Qwen-3B模型架构上的实验表明，PC-GRPO能显著提升推理质量、训练稳定性和终端任务准确率，为视觉语言模型的可扩展、可验证、可解释强化学习后训练提供了实用路径。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.14944) | [arXiv](https://arxiv.org/abs/2512.14944)



---

### 6. 通用推理模型

**原文标题：** Universal Reasoning Model

**摘要：**
通用Transformer（UT）已广泛应用于ARC-AGI和数独等复杂推理任务，但其性能提升的具体来源仍未得到充分探究。本研究系统分析了通用Transformer的变体，发现其在ARC-AGI上的改进主要源于Transformer的循环归纳偏置和强非线性组件，而非复杂的架构设计。基于这一发现，我们提出通用推理模型（URM），通过引入短卷积和截断反向传播机制增强通用Transformer。该方法显著提升了推理性能，在ARC-AGI 1上达到53.8%的pass@1最佳结果，在ARC-AGI 2上达到16.0%的pass@1最佳结果。代码已开源：https://github.com/zitian-gao/URM。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.14693) | [arXiv](https://arxiv.org/abs/2512.14693)



---

### 7. Qwen-Image-Layered：基于图层分解实现内在可编辑性

**原文标题：** Qwen-Image-Layered: Towards Inherent Editability via Layer Decomposition

**摘要：**
当前视觉生成模型在图像编辑时常因栅格图像的纠缠特性而难以保持一致性——所有视觉内容均融合于单一画布。相比之下，专业设计工具采用分层表示，允许在保持一致性的同时进行独立编辑。受此启发，我们提出Qwen-Image-Layered，一种端到端的扩散模型，能够将单张RGB图像分解为多个语义解耦的RGBA图层，实现内在可编辑性，其中每个RGBA图层均可独立编辑而不影响其他内容。为支持可变长度分解，我们引入三个关键组件：（1）RGBA-VAE统一RGB与RGBA图像的潜在表示；（2）可变层分解MMDiT架构（VLD-MMDiT），能够分解可变数量的图像图层；（3）多阶段训练策略，将预训练图像生成模型适配为多层图像分解器。此外，针对高质量多层训练数据稀缺的问题，我们构建了从Photoshop文档（PSD）中提取并标注多层图像的流程。实验表明，本方法在分解质量上显著超越现有方案，为一致性图像编辑建立了新范式。代码与模型已发布于https://github.com/QwenLM/Qwen-Image-Layered。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.15603) | [arXiv](https://arxiv.org/abs/2512.15603)



---

### 8. IC-Effect：基于上下文学习的精准高效视频特效编辑方法

**原文标题：** IC-Effect: Precise and Efficient Video Effects Editing via In-Context Learning

**摘要：**
本文提出IC-Effect，一种基于指令引导与扩散Transformer（DiT）的少样本视频视觉特效编辑框架。该框架能够合成复杂特效（如火焰、粒子与卡通角色），同时严格保持时空一致性。视频特效编辑面临多重挑战：注入的特效需与背景无缝融合，背景本身必须完全保持不变，且需从有限的配对数据中高效学习特效模式。然而，现有视频编辑模型难以满足这些要求。IC-Effect将源视频作为干净的上下文条件，利用DiT模型的上下文学习能力，实现精准的背景保持与自然的特效注入。通过两阶段训练策略——先进行通用编辑适配，再通过特效低秩自适应模块（Effect-LoRA）进行特效专项学习——本方法确保了强大的指令跟随能力与鲁棒的特效建模。为进一步提升效率，我们引入时空稀疏标记化技术，在显著降低计算量的同时保持高保真度。此外，我们发布了涵盖15种高质量视觉风格的配对特效编辑数据集。大量实验表明，IC-Effect能够实现高质量、可控且时序一致的特效编辑，为视频创作开辟了新的可能性。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.15635) | [arXiv](https://arxiv.org/abs/2512.15635)



---

### 9. Skyra：基于具象化伪影推理的人工智能生成视频检测

**原文标题：** Skyra: AI-Generated Video Detection via Grounded Artifact Reasoning

**摘要：**
人工智能驱动的视频生成技术滥用已引发严重的社会关切，凸显了对可靠AI生成视频检测器的迫切需求。然而，现有方法大多局限于二元分类，且缺乏可供人类解读的必要解释。本文提出Skyra——一种专门的多模态大语言模型（MLLM），该模型能够识别AI生成视频中人类可感知的视觉伪影，并将其作为检测与解释的具象化证据。为实现这一目标，我们构建了首个包含细粒度人工标注的大规模AI生成视频伪影数据集ViF-CoT-4K，用于监督微调训练。进而设计了一种两阶段训练策略，系统性地提升模型在时空伪影感知、解释能力与检测精度方面的性能。为全面评估Skyra，我们构建了ViF-Bench基准测试集，该数据集包含由十余种前沿视频生成模型产生的3000个高质量样本。大量实验表明，Skyra在多项基准测试中超越现有方法，同时我们的评估结果为推进可解释AI生成视频检测研究提供了重要洞见。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.15693) | [arXiv](https://arxiv.org/abs/2512.15693)



---

### 10. 鲁棒且可校准的真实多媒体内容检测

**原文标题：** Robust and Calibrated Detection of Authentic Multimedia Content

**摘要：**
生成模型能够合成高度逼真的内容（即深度伪造内容），此类技术已被大规模滥用，从而破坏数字媒体的真实性。当前深度伪造检测方法存在两大不可靠因素：（一）事后鉴别非真实内容往往不可行（例如面对记忆样本），导致误报率理论上无界；（二）检测机制缺乏鲁棒性，攻击者仅需极少计算资源即可针对已知检测器实现近乎完美的适应性规避。为应对这些局限性，我们提出一种重合成框架，用于判定样本是否真实存在或其真实性是否可被合理质疑。本研究聚焦于高效（即计算受限）攻击场景下的高精度、低召回率设定，并作出两项核心贡献：首先，我们证明经过校准的重合成方法在保持可控低误报率的同时，成为验证真实样本最可靠的途径；其次，我们验证了该方法能有效抵御高效攻击者的对抗性攻击，而现有方法在同等计算预算下极易被规避。该框架支持多模态数据，并融合了前沿的逆向生成技术。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.15182) | [arXiv](https://arxiv.org/abs/2512.15182)



---

### 11. SAGE：基于强化学习的智能任意时长代理训练及其在长视频推理中的应用

**原文标题：** SAGE: Training Smart Any-Horizon Agents for Long Video Reasoning with Reinforcement Learning

**摘要：**
作为人类，我们天生具备任意时长推理能力，即能够根据任务需求灵活选择快速浏览长视频或完整观看短视频。基于此，视频推理模型理应具备跨时长灵活推理的能力。然而，当前最先进的模型仍采用单轮预测模式处理大量视频帧（类似于完整观看长视频），需要消耗大量计算资源。这引发了一个关键问题：能否开发出高性能的任意时长视频推理系统？受人类行为模式启发，我们首先提出SAGE智能代理系统，该系统既能对长视频进行多轮推理，也能对简单问题实施单轮处理。其次，我们设计了基于Gemini-2.5-Flash的轻量级合成数据生成流程，用于训练SAGE系统的核心协调器SAGE-MM。进一步提出高效的强化学习微调方案，该方案对培养SAGE-MM的任意时长推理能力至关重要。第三，我们构建了平均时长超过700秒的SAGE-Bench评测基准，专门针对现实娱乐场景中的视频推理能力进行评估。最后，我们通过实证研究验证了系统架构、数据生成流程与强化学习方案的有效性：在开放式视频推理任务中实现最高6.1%的性能提升，在超过10分钟的长视频任务中更是取得8.2%的显著改进。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.13874) | [arXiv](https://arxiv.org/abs/2512.13874)



---

### 12. MMSI-Video-Bench：面向视频空间智能的综合性基准测试

**原文标题：** MMSI-Video-Bench: A Holistic Benchmark for Video-Based Spatial Intelligence

**摘要：**
对连续视觉输入的空间理解能力是多模态大语言模型发展为物理环境中通用助手的关键。然而，目前仍缺乏能够全面评估该目标进展的综合基准。本研究提出了MMSI-Video-Bench——一个完全由人工标注、面向多模态大语言模型视频空间智能的基准测试。该基准通过来自25个数据集及自建视频的1,278个片段构建的1,106个问题，实现了“感知-规划-预测-跨视频推理”的四层评估框架。每个测试项均由三维视觉专家精心设计并复核，配备解释性依据以确保精准无歧义的 grounding。凭借其多样化的数据源和全面的任务覆盖，MMSI-Video-Bench还支持三个面向特定领域的子基准（室内场景感知基准、机器人基准与 Grounding 基准）以实现针对性能力评估。我们对25个开源及商业多模态大语言模型进行评估，揭示了显著的人机差距：多数模型表现接近随机猜测，最优推理模型仍落后人类近60%。进一步研究发现，经过空间微调的模型在本基准上仍未能有效泛化。细粒度错误分析揭示了模型在几何推理、运动 grounding、长时程预测及跨视频关联等方面存在系统性缺陷。我们还发现，典型的帧采样策略在本推理密集型基准上迁移效果不佳，且三维空间线索与思维链提示均未带来显著性能提升。我们期望本基准能为推进视频空间智能研究建立坚实的测试平台。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.10863) | [arXiv](https://arxiv.org/abs/2512.10863)



---

### 13. 大语言模型能否引导自身探索？面向大语言模型推理的梯度引导强化学习

**原文标题：** Can LLMs Guide Their Own Exploration? Gradient-Guided Reinforcement Learning for LLM Reasoning

**摘要：**
强化学习已成为增强大语言模型推理能力的关键技术，但现有的探索机制本质上仍与这些模型的实际学习方式存在错位。熵奖励和外部语义比较器虽能鼓励表层多样性，却无法保证采样轨迹在影响优化的更新方向上存在差异。本文提出梯度引导强化学习框架G2RL，其探索驱动力并非来自外部启发式规则，而是源于模型自身的一阶更新几何结构。对于每个响应，G2RL通过标准前向传播以可忽略的成本获取模型最终层敏感度，并据此构建序列级特征，通过比较采样组内这些特征来衡量每条轨迹对策略的重塑程度。引入新颖梯度方向的轨迹会获得有界的乘性奖励缩放因子，而冗余或偏离流形的更新则会被弱化，从而产生一种自然对齐PPO式稳定性与KL控制的自我参照探索信号。在Qwen3基础版1.7B和4B模型上进行的数学与通用推理基准测试（MATH500、AMC、AIME24、AIME25、GPQA、MMLUpro）表明，G2RL在pass@1、maj@16和pass@k指标上持续优于基于熵的GRPO及外部嵌入方法。通过分析诱导的几何结构，我们发现G2RL在保持语义连贯性的同时，将探索范围扩展至显著更多正交且常相对立的梯度方向，这揭示出策略自身的更新空间能为大语言模型强化学习的探索引导提供更为可靠且有效的基准。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.15687) | [arXiv](https://arxiv.org/abs/2512.15687)



---

### 14. FiNERweb：面向可扩展多语言命名实体识别的数据集与工具集

**原文标题：** FiNERweb: Datasets and Artifacts for Scalable Multilingual Named Entity Recognition

**摘要：**
近期多语言命名实体识别研究显示，大型语言模型能够提供有效的合成监督数据，但此类数据集多作为广泛实验的副产品出现，而非系统化、可复用的资源。本文提出FiNERweb——一个将师生范式扩展至91种语言和25种文字体系的数据集构建流程。基于FineWeb-Edu框架，我们训练回归模型识别命名实体相关文本段落，并利用多语言大模型进行标注，最终获得约22.5万条文本段落及23.5万个独立实体标签。实验表明：回归模型F1值超过84%；基于FiNERweb训练的模型在英语、泰语和斯瓦希里语的零样本迁移任务中，仅使用基线模型1/19的数据量即可获得相当或更优的性能。通过大模型评估框架对标注质量进行量化分析，结果显示忠实度（3.99/5）与完整性（4.05/5）指标均保持稳定高位，证实了标注的可靠性与信息密度。值得注意的是，当前最优模型使用目标语言标签评估时，其F1值较英语标签评估会下降0.02至0.09。因此我们同步发布包含英语标签及目标语言翻译标签的数据集。现向学术界完整开放FiNERweb数据集及全套工具集，以期推动多语言命名实体识别领域师生训练范式的效能提升。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.13884) | [arXiv](https://arxiv.org/abs/2512.13884)



---

### 15. DiffusionVL：将任意自回归模型转化为扩散式视觉语言模型

**原文标题：** DiffusionVL: Translating Any Autoregressive Models into Diffusion Vision Language Models

**摘要：**
在近期的多模态研究中，扩散范式因其独特的解码优势，已成为自回归范式（AR）的一种有前景的替代方案。然而，由于基础扩散语言模型的能力限制，扩散视觉语言模型（dVLM）的性能仍显著落后于主流模型。这引出了一个简单而根本的问题：能否基于现有强大的AR模型构建dVLM？为此，我们提出了DiffusionVL，这是一个可从任意强大AR模型转化而来的dVLM系列。通过简单的微调，我们成功将AR预训练模型适配至扩散范式。该方法带来了两个关键发现：（1）从基于AR的多模态模型向扩散范式的转换效果显著；（2）将AR语言模型直接转化为dVLM同样可行，其性能可与LLaVA风格的视觉指令调优模型相竞争。此外，我们在dVLM中引入了支持任意长度生成和KV缓存复用的块解码设计，实现了显著的推理加速。我们进行了大量实验，结果表明：尽管训练数据量不足先前方法所需数据的5%，DiffusionVL仍实现了全面的性能提升——在MMMU-Pro（视觉）基准上提升34.4%，在MME（认知）基准上提升37.5%，同时推理速度加快2倍。模型与代码已发布于https://github.com/hustvl/DiffusionVL。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.15713) | [arXiv](https://arxiv.org/abs/2512.15713)



---

### 16. VOYAGER：一种基于大语言模型的无训练多样化数据集生成方法

**原文标题：** VOYAGER: A Training Free Approach for Generating Diverse Datasets using LLMs

**摘要：**
大语言模型正日益广泛地用于生成合成数据集，以支持下游模型的评估与训练。然而，已有研究指出此类生成数据往往缺乏多样性。本文提出Voyager，一种新颖的、基于原理的多样化数据集生成方法。该方法采用迭代策略，并借助行列式点过程的理论工具，直接优化一个旨在提升数据集多样性的数学指标。此外，该方法无需训练、适用于闭源模型，且具有良好的可扩展性。我们不仅从理论上论证了本方法的有效性，还通过全面实验证明，Voyager在多样性指标上显著优于现有主流基线方法，提升幅度达1.5至3倍。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.12072) | [arXiv](https://arxiv.org/abs/2512.12072)



---

### 17. 基于自重采样的自回归视频扩散模型端到端训练方法

**原文标题：** End-to-End Training for Autoregressive Video Diffusion via Self-Resampling

**摘要：**
自回归视频扩散模型在场景仿真方面具有潜力，但容易受到训练-测试失配导致的暴露偏差影响。现有研究主要通过后训练方法解决该问题，但通常依赖双向教师模型或在线判别器。为实现端到端解决方案，本文提出重采样强制训练框架——一种无需教师模型的训练范式，能够实现自回归视频模型从零开始的大规模训练。该方法的核心是自重采样机制，通过在训练过程中模拟历史帧在推理阶段可能出现的模型误差。基于这些降质历史帧，稀疏因果掩码在保持时间因果性的同时，支持通过帧级扩散损失进行并行训练。为提升长序列生成效率，我们进一步提出历史路由机制——这种无参数方法能够为每个查询动态检索前k个最相关的历史帧。实验表明，本方法在达到基于蒸馏的基线模型可比性能的同时，由于采用原生长度训练，在长视频生成中展现出更优的时间一致性。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.15702) | [arXiv](https://arxiv.org/abs/2512.15702)



---

### 18. VABench：面向音视频生成的综合基准测试框架

**原文标题：** VABench: A Comprehensive Benchmark for Audio-Video Generation

**摘要：**
近期视频生成技术取得了显著进展，使得模型能够生成具有同步音频的视觉吸引力视频。尽管现有视频生成基准测试提供了全面的视觉质量评估指标，但其对音视频生成的评估尚不完善，尤其缺乏针对同步音视频输出模型的可靠评估体系。为填补这一空白，我们提出了VABench——一个综合性、多维度的基准测试框架，旨在系统评估同步音视频生成能力。该框架涵盖三大任务类型：文本到音视频生成、图像到音视频生成以及立体声音视频生成，并构建了包含15个维度的两大评估模块。这些维度专门评估文本-视频、文本-音频、视频-音频的成对相似性，音视频同步性，唇语-语音一致性，以及精心设计的音视频问答对等关键指标。此外，VABench覆盖七大内容类别：动物、人声、音乐、环境音、同步物理声效、复杂场景及虚拟世界。我们通过系统化的结果分析与可视化呈现，致力于为具有同步音频能力的视频生成模型建立新的评估标准，从而推动该领域的全面进步。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.09299) | [arXiv](https://arxiv.org/abs/2512.09299)



---

### 19. 追求像素级监督的视觉预训练

**原文标题：** In Pursuit of Pixel Supervision for Visual Pre-training

**摘要：**
在最基础的层面上，像素是我们感知世界时视觉信息的来源。像素包含从低级属性到高级概念等各个层次的信息。自编码器代表了从像素或其他原始输入中学习表征的经典且历史悠久的范式。本研究证明，基于自编码器的自监督学习在当今仍具竞争力，能够为下游任务生成强大的表征，同时保持简洁性、稳定性和高效性。我们开发的代号为"Pixio"的模型，是一种增强型掩码自编码器（MAE），具有更具挑战性的预训练任务和更强大的架构。该模型通过自主筛选策略在20亿张网络爬取图像上进行训练，仅需极少量人工标注。Pixio在野外环境下的多种下游任务中均表现出竞争力，包括单目深度估计（如Depth Anything）、前馈式三维重建（即MapAnything）、语义分割和机器人学习，其性能优于或匹配同等规模训练的DINOv3模型。我们的研究结果表明，像素空间的自监督学习可以成为潜在空间方法的有前景的替代方案和补充。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.15715) | [arXiv](https://arxiv.org/abs/2512.15715)



---

### 20. 自动驾驶中的视觉-语言-动作模型：过去、现在与未来

**原文标题：** Vision-Language-Action Models for Autonomous Driving: Past, Present, and Future

**摘要：**
自动驾驶长期依赖模块化的“感知-决策-动作”流程，其人工设计的接口与基于规则的组件在复杂或长尾场景中常出现失效。这种级联设计会进一步传播感知误差，导致下游规划与控制性能下降。视觉-动作模型通过学习从视觉输入到动作的直接映射，部分解决了上述局限，但其仍存在可解释性差、对分布偏移敏感、缺乏结构化推理或指令跟随能力等问题。近年来，大语言模型与多模态学习的进展推动了视觉-语言-动作框架的兴起，该框架将感知与基于语言的决策相结合。通过统一视觉理解、语言推理与可执行输出，视觉-语言-动作模型为实现更具可解释性、泛化性及符合人类意图的驾驶策略提供了路径。本文对自动驾驶领域新兴的视觉-语言-动作研究体系进行了系统性梳理：我们追溯了从早期视觉-动作方法到现代视觉-语言-动作框架的发展脉络，并将现有方法归纳为两大范式——集成感知、推理与规划于一体的端到端视觉-语言-动作模型，以及将慢速决策（通过视觉语言模型）与快速安全关键执行（通过规划器）分离的双系统视觉-语言-动作架构。在此分类基础上，我们进一步区分了文本型与数值型动作生成器、显式与隐式引导机制等子类。同时，本文总结了用于评估基于视觉-语言-动作的驾驶系统的代表性数据集与基准测试，并指出了包括鲁棒性、可解释性与指令忠实度在内的关键挑战与未来方向。本研究旨在为推进人车协同的自动驾驶系统建立清晰的理论与实践基础。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.16760) | [arXiv](https://arxiv.org/abs/2512.16760)



---

### 21. FrontierCS：演化智能面临的前沿挑战

**原文标题：** FrontierCS: Evolving Challenges for Evolving Intelligence

**摘要：**
本文介绍FrontierCS基准测试集，该数据集包含156个涵盖计算机科学多领域的开放式问题，由计算机科学博士、顶级竞赛编程参与者及命题专家共同设计与评审。与现有聚焦已知最优解任务的基准不同，FrontierCS针对那些最优解未知但解决方案质量可客观评估的问题。模型需通过实现可执行程序（而非直接输出答案）来解决这些任务。该基准包含两类问题：一是具有客观部分评分机制的竞赛编程NP难变体算法问题，二是具备相同特性的研究型问题。每个问题均提供专家参考解决方案和自动评估程序。通过融合开放式设计、可量化进展与专家评审机制，FrontierCS构建了处于计算机科学难度前沿的评估基准。实证研究表明：当前前沿推理模型在算法与研究两类任务上仍大幅落后于人类专家；仅增加推理预算无法弥合该差距；模型常过度优化生成仅可运行的代码，而未能发现高质量的算法与系统设计。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.15699) | [arXiv](https://arxiv.org/abs/2512.15699)



---

### 22. VTCBench：视觉语言模型能否通过视觉文本压缩理解长上下文？

**原文标题：** VTCBench: Can Vision-Language Models Understand Long Context with Vision-Text Compression?

**摘要：**
扩展大语言模型上下文窗口所带来的计算与内存开销严重制约了其可扩展性。视觉文本压缩（VTC）作为一种值得关注的解决方案，通过DeepSeek-OCR、Glyph等框架将长文本转换为密集的二维视觉表征，实现了3至20倍的令牌压缩率。然而，这种高信息密度对视觉语言模型核心长上下文理解能力的影响尚未得到充分研究。为填补这一空白，我们提出了首个VTC专项评测基准，系统评估了视觉语言模型在三种长上下文理解场景中的表现：VTC检索——评估模型检索与聚合信息的能力；VTC推理——要求模型通过推断潜在关联定位词汇重叠度极低的事实；VTC记忆——衡量模型在长期对话记忆中的综合问答能力。此外，我们构建了VTCBench-Wild以模拟多样化输入场景。基于该基准对主流开源与商业模型进行全面评估的结果表明：尽管大多数视觉语言模型能较好解码文本信息（如OCR识别），但在处理VTC压缩信息时却表现出惊人的长上下文理解缺陷，难以捕捉上下文中的长程关联与依赖。本研究深化了对VTC技术的理解，并为设计更高效、可扩展的视觉语言模型奠定了基础。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.15649) | [arXiv](https://arxiv.org/abs/2512.15649)



---

### 23. SCOPE：通过提示演化提升智能体效能

**原文标题：** SCOPE: Prompt Evolution for Enhancing Agent Effectiveness

**摘要：**
大语言模型智能体正日益部署于产生海量动态上下文的环境中。然而，一个关键瓶颈依然存在：尽管智能体能够访问这些上下文，但其静态提示缺乏有效管理机制，导致校正与增强失败反复发生。为弥补这一能力缺口，我们提出了SCOPE（基于提示演化的自演进上下文优化框架）。该框架将上下文管理构建为在线优化问题，通过综合分析执行轨迹生成指导原则，实现智能体提示的自动化演进。我们设计了双流机制，在战术特异性（解决即时错误）与战略通用性（演进长期原则）之间取得平衡。此外，我们引入视角驱动探索机制以最大化策略覆盖范围，提升智能体针对任意任务具备正确策略的可能性。在HLE基准测试中的实验表明，SCOPE将任务成功率从14.23%提升至38.64%且无需人工干预。相关代码已公开于https://github.com/JarvisPei/SCOPE。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.15374) | [arXiv](https://arxiv.org/abs/2512.15374)



---

### 24. Nano Banana Pro是低层视觉全能选手吗？基于14项任务与40个数据集的全方位评估

**原文标题：** Is Nano Banana Pro a Low-Level Vision All-Rounder? A Comprehensive Evaluation on 14 Tasks and 40 Datasets

**摘要：**
文本到图像生成模型的快速发展正在彻底改变视觉内容创作。尽管Nano Banana Pro等商业产品已获得广泛关注，但其作为传统低层视觉挑战通用解决方案的潜力仍未被充分探索。本研究聚焦核心问题：Nano Banana Pro是否堪称低层视觉全能选手？我们通过对涵盖40个异构数据集的14类低层视觉任务进行系统性零样本评估，在未经微调的情况下使用简易文本提示，将Nano Banana Pro与前沿专业模型进行基准比较。综合分析揭示出显著的性能二分现象：Nano Banana Pro虽展现出卓越的主观视觉质量，其生成的高频细节常超越专业模型并呈现合理幻觉，但在传统基于参考图像的定量指标上表现欠佳。我们将此差异归因于生成模型固有的随机性特征，这类模型难以满足传统指标对像素级一致性的严苛要求。本报告确认Nano Banana Pro具备成为低层视觉任务零样本竞争者的潜力，同时指出要达到领域专业模型的高保真度仍面临重大挑战。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.15110) | [arXiv](https://arxiv.org/abs/2512.15110)



---

### 25. WAY：基于全球AIS轨迹的船舶目的地估计方法

**原文标题：** WAY: Estimation of Vessel Destination in Worldwide AIS Trajectory

**摘要：**
自动识别系统（AIS）为数据驱动的海事监控提供了可能，但其存在可靠性不足与数据间隔不规则的问题。本研究针对全球范围AIS数据，提出一种差异化船舶目的地估计方法，通过将长距离港到港轨迹重构为嵌套序列结构进行处理。该方法采用空间网格化策略，在保持精细分辨率的同时有效缓解时空偏差问题。我们设计了一种新型深度学习架构WAY，专门处理重构后的轨迹数据，实现提前数天至数周的长期目的地预测。WAY架构包含轨迹表征层和通道聚合序列处理模块：表征层通过运动学与非运动学特征生成多通道向量序列；通道聚合序列处理模块采用多头通道注意力与自注意力机制实现特征聚合与序列信息传递。此外，本研究提出任务专用的梯度丢弃技术，通过基于样本长度的随机梯度流阻断机制，在单标签训练中实现多对多映射，避免偏差反馈激增。基于五年AIS数据的实验表明，无论轨迹进展阶段如何，WAY均优于传统空间网格方法；结果同时验证梯度丢弃技术能有效提升模型性能。最后，我们通过到港时间估计的多任务学习框架，探讨了WAY在实际应用中的潜力。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.13190) | [arXiv](https://arxiv.org/abs/2512.13190)



---

### 26. 理解与改进双曲深度强化学习

**原文标题：** Understanding and Improving Hyperbolic Deep Reinforcement Learning

**摘要：**
强化学习智能体的性能关键取决于其底层特征表示的质量。双曲特征空间非常适合这一目的，因为它们能自然地捕捉复杂强化学习环境中普遍存在的层次化与关系化结构。然而，由于强化学习的非平稳性，利用这些空间通常面临优化挑战。本研究确定了影响双曲深度强化学习智能体训练成败的关键因素。通过分析双曲几何的庞加莱球模型与双曲面模型中核心运算的梯度，我们发现大范数嵌入会破坏基于梯度的训练稳定性，导致近端策略优化算法中的信任域条件被破坏。基于这些发现，我们提出了Hyper++——一种新型双曲近端策略优化智能体，它包含三个核心组件：（一）通过分类价值损失替代回归损失实现稳定的评论家训练；（二）采用特征正则化技术保证范数有界，同时避免裁剪操作引发的维度灾难；（三）使用优化友好的双曲网络层表达形式。在ProcGen基准测试中，Hyper++能保证稳定学习，性能超越现有双曲智能体，并将实际计算时间减少约30%。在采用双重深度Q网络的Atari-5测试中，Hyper++显著优于欧几里得与双曲基线方法。代码已发布于https://github.com/Probabilistic-and-Interactive-ML/hyper-rl。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.14202) | [arXiv](https://arxiv.org/abs/2512.14202)



---

### 27. 用于可解释与鲁棒模型训练的混合归因先验

**原文标题：** Hybrid Attribution Priors for Explainable and Robust Model Training

**摘要：**
小型语言模型因其低延迟与轻量部署的优势，在分类等任务中得到广泛应用。随着可解释性与鲁棒性日益受到重视，解释引导学习通过引入基于归因的监督信号，已成为一种有效的训练框架；然而，如何获取通用且可靠的归因先验仍面临重大挑战。通过对分类场景中代表性归因方法的分析，我们发现尽管这些方法能可靠地突出类别相关词元，却常聚焦于语义相似类别间共享的通用关键词。由于此类类别在标准训练中本就难以区分，此类归因提供的判别性线索不足，限制了其提升模型区分能力的效果。为突破这一局限，我们提出**类别感知归因先验**——一种新颖的归因先验提取框架，引导语言模型捕捉细粒度的类别差异，并生成更显著、更具判别力的归因先验。基于此，我们进一步提出**混合式类别感知归因先验**，将本框架生成的先验与现有归因技术所得先验相结合，构建更全面、均衡的监督信号。通过使模型的自归因与这些增强后的先验对齐，我们的方法促进了多样化、决策相关特征的学习。在充足数据、少样本及对抗场景中的大量实验表明，本方法能持续提升模型的可解释性与鲁棒性。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.14719) | [arXiv](https://arxiv.org/abs/2512.14719)



---

### 28. SonicMoE：通过IO感知与分块感知优化加速混合专家模型

**原文标题：** SonicMoE: Accelerating MoE with IO and Tile-aware Optimizations

**摘要：**
混合专家模型已成为扩展语言模型规模而不显著增加计算成本的实际架构。近期研究显示，混合专家模型呈现出高专家粒度化（更小的专家中间维度）与高稀疏化（激活专家数量恒定而总专家数量增加）的明确趋势，从而提升了单位浮点运算的模型质量。然而，细粒度混合专家模型因更高的IO开销导致激活内存占用增加与硬件效率降低，而更稀疏的混合专家模型则因分组通用矩阵乘法核函数中的填充操作而产生计算浪费。为此，我们提出一种内存高效算法，以前向传播与反向传播所需的最小激活缓存计算混合专家模型的前向与反向过程。同时，我们设计了能够实现内存IO与计算重叠的GPU核函数，适用于所有混合专家架构。此外，我们提出一种新颖的“令牌舍入”方法，可最大程度减少分组通用矩阵乘法核函数中填充导致的无效计算。实验结果表明，对于细粒度70亿参数混合专家模型，我们的方法SonicMoE相比ScatterMoE的BF16混合专家核函数，在Hopper GPU上将激活内存降低45%，计算吞吐量提升1.86倍。具体而言，在64张H100 GPU上，SonicMoE对使用lm-engine代码库进行FSDP-2训练的70亿参数混合专家模型实现了每日2130亿令牌的训练吞吐量，与ScatterMoE在96张H100上每日2250亿令牌的吞吐量相当。在高混合专家稀疏性设置下，我们提出的分块感知令牌舍入算法相比传统Top-K路由方法，在保持相近下游性能的同时，使核函数执行时间额外加速1.16倍。我们将开源全部核函数代码，以促进混合专家模型的快速训练。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.14080) | [arXiv](https://arxiv.org/abs/2512.14080)



---

### 29. LikeBench：面向个性化的大语言模型主观喜好度评估

**原文标题：** LikeBench: Evaluating Subjective Likability in LLMs for Personalization

**摘要：**
个性化大语言模型应能记忆用户信息、准确应用这些信息，并随时间推移持续适应用户偏好以提供更受青睐的回应。现有的大语言模型个性化基准主要围绕两个维度展开：准确回忆用户信息，以及在后续任务中准确应用已记忆的信息。我们认为，第三个维度——喜好度——既是主观的，又对用户体验至关重要，但当前基准对此维度评估不足。为全面衡量喜好度，我们提出LikeBench：一个多轮次动态评估框架，通过衡量大语言模型随时间推移适应用户偏好以提供更受欢迎回应的能力，从多维度评估喜好度。在LikeBench中，大语言模型与模拟用户进行对话，仅通过持续对话学习用户偏好。随着交互推进，模型尝试调整回应策略；每轮对话后，由同一模拟用户从七个维度评估其喜好度表现。据我们所知，我们首次将喜好度分解为七个可诊断的指标：情感适应性、正式度匹配、知识适应性、指代理解、对话长度适配、幽默适配与话题呼应，从而更精准定位模型短板。为使模拟用户更真实且更具区分度，LikeBench采用基于心理学理论的细粒度描述性人物画像，而非以往研究中基于粗粒度高低特质评分的画像。我们的基准测试表明，强记忆性能并不保证高喜好度：DeepSeek R1的记忆准确率较低（86%，每档案17条事实），但其喜好度得分比记忆准确率更高（93%，每档案43条事实）的Qwen3高出28%。即使是GPT-5等前沿模型，在简短交互中适应良好，但在更长、更嘈杂的交互中仅表现出有限的鲁棒性。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.13077) | [arXiv](https://arxiv.org/abs/2512.13077)



---

### 30. 面向多模态机器人操作学习的触觉-视觉同步感知

**原文标题：** Simultaneous Tactile-Visual Perception for Learning Multimodal Robot Manipulation

**摘要：**
机器人操作需要丰富的多模态感知与高效的学习框架以应对复杂的现实任务。结合触觉与视觉感知的透皮（STS）传感器提供了前景广阔的感知能力，而现代模仿学习为策略获取提供了强大工具。然而，现有STS设计缺乏同步多模态感知能力，且存在触觉追踪不可靠的问题。此外，如何将这些丰富的多模态信号整合至基于学习的操作流程中仍是开放挑战。本文提出TacThru——一种实现同步视觉感知与鲁棒触觉信号提取的STS传感器，以及TacThru-UMI——利用这些多模态信号进行操作的模仿学习框架。该传感器采用全透明弹性体、持久照明、新型键线标记与高效追踪技术，而学习系统通过基于Transformer的扩散策略整合多模态信号。在五项具有挑战性的现实任务实验中，TacThru-UMI平均成功率达到85.5%，显著优于交替触觉-视觉感知基线（66.3%）与纯视觉基线（55.4%）。该系统在关键场景中表现优异，包括对细薄柔软物体的接触检测及需要多模态协调的精细操作。本研究表明，将同步多模态感知与现代学习框架相结合，能够实现更精准、适应性更强的机器人操作。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.09851) | [arXiv](https://arxiv.org/abs/2512.09851)



---

### 31. 迈向无缝交互：交互式三维会话头部动态的因果轮次建模

**原文标题：** Towards Seamless Interaction: Causal Turn-Level Modeling of Interactive 3D Conversational Head Dynamics

**摘要：**
人类会话涉及语音与非语言线索（如传达注意力和情感的点头、视线转移及面部表情）的持续交换。在三维空间中建模这种双向动态对于构建富有表现力的虚拟化身和交互式机器人至关重要。然而，现有框架通常将说话与聆听视为独立过程，或依赖于非因果的全序列建模，这阻碍了跨轮次的时间连贯性。本文提出TIMAR（轮次交错掩码自回归）框架，这是一种用于三维会话头部生成的因果建模框架，将对话建模为交错的多模态视听上下文。该框架在每轮对话中融合多模态信息，并应用轮次级因果注意力机制以累积会话历史，同时通过轻量级扩散头部预测连续的三维头部动态，从而捕捉协调性与表达可变性。在DualTalk基准测试上的实验表明，TIMAR在测试集上将弗雷歇距离与均方误差降低了15-30%，并在分布外数据上取得相近的性能提升。源代码将在GitHub仓库 https://github.com/CoderChen01/towards-seamleass-interaction 中公开。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.15340) | [arXiv](https://arxiv.org/abs/2512.15340)



---


## 🔍 关键词云图
![关键词云图](../images/keywords_wordcloud.png)

## 📈 近期论文趋势
![论文趋势](../images/daily_papers.png)

## 🎙️ 语音播报
- [收听今日论文解读](../audio/2025-12-18_daily_papers.mp3)

## 📱 订阅渠道
- GitHub: [hf-daily-paper-newsletter-chinese](https://github.com/2404589803/hf-daily-paper-newsletter-chinese)