<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Hugging Face 论文日报 - 2025-11-17</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', sans-serif;
            line-height: 1.6;
            color: #333;
            background-color: #f5f5f5;
            padding: 20px;
        }
        
        .container {
            max-width: 900px;
            margin: 0 auto;
            background-color: white;
            padding: 40px;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }
        
        h1 {
            color: #2c3e50;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
            margin-bottom: 30px;
            font-size: 28px;
        }
        
        h1 img {
            vertical-align: middle;
            margin-right: 10px;
        }
        
        h2 {
            color: #34495e;
            margin-top: 40px;
            margin-bottom: 20px;
            font-size: 24px;
            border-left: 4px solid #3498db;
            padding-left: 15px;
        }
        
        h3 {
            color: #2c3e50;
            margin-top: 30px;
            margin-bottom: 15px;
            font-size: 20px;
        }
        
        p {
            margin-bottom: 15px;
            text-align: justify;
        }
        
        ul {
            margin-left: 20px;
            margin-bottom: 20px;
        }
        
        li {
            margin-bottom: 8px;
        }
        
        a {
            color: #3498db;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        strong {
            color: #2c3e50;
            font-weight: 600;
        }
        
        hr {
            border: none;
            border-top: 1px solid #e0e0e0;
            margin: 30px 0;
        }
        
        /* 关键修复:限制图片宽度 */
        img {
            max-width: 100%;
            height: auto;
            display: block;
            margin: 20px auto;
            border-radius: 4px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        
        /* 确保图片容器也有宽度限制 */
        p img {
            max-width: 100%;
        }
        
        /* 论文详情区域样式 */
        .paper-section {
            background-color: #f9f9f9;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 20px;
        }
        
        /* 统计信息样式 */
        .stats {
            background-color: #e8f4f8;
            padding: 15px;
            border-radius: 6px;
            margin-bottom: 20px;
        }
        
        /* 响应式设计 */
        @media (max-width: 768px) {
            .container {
                padding: 20px;
            }
            
            h1 {
                font-size: 24px;
            }
            
            h2 {
                font-size: 20px;
            }
            
            h3 {
                font-size: 18px;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <h1><img src="https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo.png" width="30"/> Hugging Face 2025-11-17 论文日报</h1>
<h2>📊 今日论文统计</h2>
<ul>
<li>总论文数：23</li>
<li>热门领域：LLM, Transformer, GPT, RL</li>
</ul>
<h2>📝 论文详情</h2>
<h3>1. DoPE：旋转位置编码去噪方法</h3>
<p><strong>原文标题：</strong> DoPE: Denoising Rotary Position Embedding</p>
<p><strong>摘要：</strong>
Transformer模型中的旋转位置编码存在固有局限，会削弱长度外推能力。我们将带位置编码的注意力图重新解读为含噪声的特征图，并提出基于截断矩阵熵的无训练去噪位置编码方法DoPE，用于检测特征图中的异常频带。利用特征图的噪声特性，我们进一步通过无参数高斯分布对其进行重参数化，以实现稳健的外推。该方法从理论上揭示了注意力汇聚现象的成因及其与截断矩阵熵的关联。在"大海捞针"和多样本上下文学习任务上的实验表明，DoPE在扩展上下文场景下显著提升了检索精度和推理稳定性。研究结果证明，针对位置嵌入的去噪策略能有效缓解注意力汇聚问题，恢复均衡的注意力模式，为改进长度泛化能力提供了简洁而有效的解决方案。项目页面详见：https://The-physical-picture-of-LLMs.github.io</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.09146">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.09146">arXiv</a></p>
<hr />
<h3>2. WEAVE：解锁与评测上下文交织式多模态理解与生成能力</h3>
<p><strong>原文标题：</strong> WEAVE: Unleashing and Benchmarking the In-context Interleaved Comprehension and Generation</p>
<p><strong>摘要：</strong>
统一多模态模型的最新进展显著推动了视觉理解与生成领域的发展。然而现有数据集和基准主要关注单轮交互，未能体现真实世界图像创作与编辑过程中多轮次、上下文依赖的特性。为弥补这一空白，我们提出WEAVE——首个面向上下文交织式跨模态理解与生成的综合套件。该套件包含两个互补部分：WEAVE-100k作为大规模数据集包含10万个交织样本，覆盖37万次对话轮转和50万张图像，涵盖需要历史上下文推理的理解、编辑与生成任务；WEAVEBench则是基于480张图像构建的含100个任务的人工标注基准，采用融合参考图像及原图与编辑指令的混合式视觉语言模型评判框架，评估模型在多轮生成、视觉记忆和跨领域世界知识推理等方面的能力。实验表明，基于WEAVE-100k的训练能有效提升视觉理解、图像编辑及理解-生成协作能力，更有助于统一多模态模型涌现视觉记忆能力。同时，在WEAVEBench上的广泛评测揭示了当前方法在多轮上下文感知图像生成与编辑方面存在的持续局限与挑战。我们相信WEAVE为多模态社区研究上下文交织式理解与生成提供了新视角与基础平台。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.11434">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.11434">arXiv</a></p>
<hr />
<h3>3. GGBench：面向统一多模态模型的几何生成推理基准</h3>
<p><strong>原文标题：</strong> GGBench: A Geometric Generative Reasoning Benchmark for Unified Multimodal Models</p>
<p><strong>摘要：</strong>
统一多模态模型（UMMs）的出现标志着人工智能领域的范式转变，从被动感知转向主动的跨模态生成。尽管这些模型具有前所未有的信息融合能力，但其评估体系仍存在关键缺陷：现有基准主要分别评估判别式理解或无约束图像生成能力，未能有效衡量生成推理这一整合性认知过程。为弥补这一空白，我们提出几何构造任务可作为理想测试平台，因其本质上要求语言理解与精确视觉生成的深度融合。我们开发的GGBench基准专门用于评估几何生成推理能力，该框架通过系统化诊断模型在理解、推理及主动构建解决方案等方面的综合能力，为新一代智能系统设立了更严谨的评估标准。项目主页：https://opendatalab-raiser.github.io/GGBench/。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.11134">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.11134">arXiv</a></p>
<hr />
<h3>4. UI2Code^N：支持测试时扩展交互的界面到代码生成视觉语言模型</h3>
<p><strong>原文标题：</strong> UI2Code^N: A Visual Language Model for Test-Time Scalable Interactive UI-to-Code Generation</p>
<p><strong>摘要：</strong>
用户界面编程是现代软件开发中核心且高度复杂的环节。视觉语言模型的最新进展凸显了自动界面编程的潜力，但现有方法存在两个关键局限：多模态编程能力尚未充分发展，单轮生成范式难以利用迭代的视觉反馈。我们提出交互式界面到代码生成范式来解决这些挑战，该范式更贴合实际工作流程并提升了性能上限。在此范式下，我们推出UI2Code^N视觉语言模型，通过分阶段预训练、微调与强化学习实现多模态编程能力的根本性提升。该模型统一了三大核心能力：界面到代码生成、界面编辑与界面优化。我们进一步探索测试时扩展的交互生成机制，实现多轮反馈的系统化利用。在界面到代码生成与界面优化基准测试中，UI2Code^N在开源模型中确立了最新技术标杆，其性能可与Claude-4-Sonnet、GPT-5等领先闭源模型相媲美。代码与模型已发布于https://github.com/zai-org/UI2Code_N。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.08195">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.08195">arXiv</a></p>
<hr />
<h3>5. AIonopedia：基于大语言模型的多模态学习协同框架用于离子液体发现</h3>
<p><strong>原文标题：</strong> AIonopedia: an LLM agent orchestrating multimodal learning for ionic liquid discovery</p>
<p><strong>摘要：</strong>
新型离子液体的开发面临物性预测领域的多重挑战，包括数据稀缺、模型精度不足及工作流程碎片化。本研究创新性地运用大语言模型技术，首次提出名为AIonopedia的离子液体发现智能体。该智能体通过融合大语言模型增强的多模态领域基础模型，不仅能实现精准物性预测，更构建了分层搜索架构以支持分子筛选与设计。基于新构建的综合性离子液体数据集进行训练与评估，本模型展现出卓越性能。对文献报道体系的补充评估表明，该智能体可有效执行离子液体结构优化。突破离线测试局限，我们通过真实湿实验证进一步确认其实际效能：该智能体在具有挑战性的分布外任务中展现出卓越的泛化能力，彰显其加速实际离子液体发现进程的潜力。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.11257">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.11257">arXiv</a></p>
<hr />
<h3>6. 虚拟宽度网络</h3>
<p><strong>原文标题：</strong> Virtual Width Networks</p>
<p><strong>摘要：</strong>
本文提出虚拟宽度网络（VWN），该框架能够在避免隐藏层维度增加带来二次计算成本的前提下，实现宽表征的优势。VWN将表征宽度与主干网络宽度解耦，在保持主干计算量基本不变的同时扩展嵌入空间。我们的大规模实验表明：在8倍扩展配置下，下一词元预测的优化速度提升2倍以上，下两词元预测速度提升3倍。随着训练进程推进，损失差距持续扩大且收敛加速比不断增长，证明VWN不仅具有词元效率优势，更随规模扩大持续增强效果。此外，我们发现了虚拟宽度与损失降低之间近似对数线性的缩放规律，这为探索虚拟宽度缩放作为大模型效率新维度提供了初步实证依据和研究动机。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.11238">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.11238">arXiv</a></p>
<hr />
<h3>7. LiteAttention：面向扩散变换器的时间稀疏注意力机制</h3>
<p><strong>原文标题：</strong> LiteAttention: A Temporal Sparse Attention for Diffusion Transformers</p>
<p><strong>摘要：</strong>
扩散变换器在视频生成等领域展现出卓越的质量，但其二次方注意力复杂度导致难以承受的计算延迟。现有加速方法面临根本性权衡：在去噪过程的每一步动态估计稀疏注意力模式会产生高昂计算开销和估计误差，而静态稀疏模式在整个去噪过程中保持固定且往往非最优。我们发现扩散注意力具有关键的结构特性——其稀疏模式在连续去噪步骤间呈现显著的时间连贯性。在步骤t中被判定为非关键的图块，通常在步骤t+δ中仍保持非关键状态。基于这一发现，我们提出LiteAttention方法，利用时间连贯性实现跨去噪序列的进化计算跳跃。通过早期标记非关键图块并向前传播跳跃决策，LiteAttention在无需重复性能分析开销的前提下消除冗余注意力计算，兼具动态方法的自适应性与静态方法的高效性。我们在FlashAttention基础上实现了高度优化的LiteAttention内核，在商用视频扩散模型上实现了显著加速，且未造成质量损失。代码与实现细节将公开发布。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.11062">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.11062">arXiv</a></p>
<hr />
<h3>8. 用人工智能模拟视觉世界：发展路线图</h3>
<p><strong>原文标题：</strong> Simulating the Visual World with Artificial Intelligence: A Roadmap</p>
<p><strong>摘要：</strong>
视频生成领域正在经历重大转型——从关注生成视觉吸引力强的片段，转向构建支持交互并保持物理合理性的虚拟环境。这些进展预示着视频基础模型的出现，它们不仅是视觉生成器，更是隐式世界模型：能够模拟物理动力学、智能体-环境交互以及支配现实或想象世界的任务规划系统。本综述系统梳理了这一演进历程，将现代视频基础模型概念化为两个核心组件的结合：隐式世界模型与视频渲染器。世界模型编码关于世界的结构化知识，包括物理定律、交互动力学和智能体行为，作为潜在模拟引擎实现连贯的视觉推理、长期时间一致性和目标驱动规划；视频渲染器则将这种潜在模拟转化为逼真的视觉观测，使生成的视频成为窥视模拟世界的“窗口”。我们通过四代演进追溯视频生成的发展脉络：核心能力逐步提升，最终形成基于视频生成模型的世界模型，其具备内在物理合理性、实时多模态交互以及跨时空尺度的规划能力。针对每一代，我们界定其核心特征，重点介绍代表性工作，并考察其在机器人、自动驾驶、交互式游戏等领域的应用。最后，我们探讨下一代世界模型的开放挑战与设计原则，包括智能体智能在塑造和评估这些系统中的作用。相关文献动态列表请访问此链接。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.08585">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.08585">arXiv</a></p>
<hr />
<h3>9. SpatialThinker：通过空间奖励增强多模态大语言模型的三维推理能力</h3>
<p><strong>原文标题：</strong> SpatialThinker: Reinforcing 3D Reasoning in Multimodal LLMs via Spatial Rewards</p>
<p><strong>摘要：</strong>
多模态大语言模型在视觉语言任务中取得了显著进展，但在空间理解方面仍存在明显不足。现有空间模型往往依赖显式三维输入或特定架构修改，且受限于大规模数据集或稀疏监督。为突破这些限制，我们提出SpatialThinker——一种通过强化学习训练的3D感知多模态大语言模型，将结构化空间定位与多步推理相融合。该模型通过构建任务相关对象与空间关系的场景图，并借助密集空间奖励进行推理推演，模拟类人空间认知能力。本研究的核心贡献包括：（1）开发数据合成流程，生成包含7K样本的高质量空间视觉问答数据集STVQA-7K；（2）采用多目标密集空间奖励的在线强化学习机制以强化空间定位。实验表明，SpatialThinker-7B在空间理解和真实场景视觉问答基准测试中均优于监督微调与稀疏强化学习基线，其基础模型增益较稀疏强化学习提升近一倍，并超越GPT-4o。这些结果验证了将空间监督与奖励对齐推理相结合的有效性，既能以有限数据实现稳健的三维空间理解，又推动多模态大语言模型向人类水平的视觉推理迈进。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.07403">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.07403">arXiv</a></p>
<hr />
<h3>10. HI-TransPA：听力障碍翻译个人助手</h3>
<p><strong>原文标题：</strong> HI-TransPA: Hearing Impairments Translation Personal Assistant</p>
<p><strong>摘要：</strong>
为给听障人士日常交流提供统一灵活的解决方案，我们将全模态范式引入辅助技术领域，提出指令驱动的视听个人助手HI-TransPA。该模型通过融合模糊语音与高帧率唇部动态，在统一多模态框架内实现翻译与对话双重功能。针对原始数据噪声干扰强、异质性明显，以及现有全模态对听障语音适应性不足的挑战，我们构建了完整的预处理流程：检测面部关键点，分离并稳定唇部区域，定量评估多模态样本质量。基于质量评分设计的课程学习策略，优先训练清晰高置信度样本，逐步引入复杂案例以增强模型鲁棒性。采用SigLIP编码器结合统一3D重采样器，实现对高帧率唇部运动的高效编码。在自建的HI-Dialogue数据集上的实验表明，HI-TransPA在字面准确度与语义保真度方面均达到最优性能。本研究为全模态在辅助通信技术的应用奠定基础，为后续研究提供了端到端建模框架与核心处理工具。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.09915">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.09915">arXiv</a></p>
<hr />
<h3>11. MarsRL：基于智能体流水线并行强化学习的多智能体推理系统进阶研究</h3>
<p><strong>原文标题：</strong> MarsRL: Advancing Multi-Agent Reasoning System via Reinforcement Learning with Agentic Pipeline Parallelism</p>
<p><strong>摘要：</strong>
当前大语言模型的发展主要得益于可验证奖励的强化学习与测试时缩放技术。然而，大语言模型有限的输出长度制约了单次推理过程所能达到的思维深度。多智能体推理系统通过部署求解器、验证器和校正器等多元智能体进行迭代优化，为此提供了颇具前景的替代方案。尽管该体系在Gemini 2.5 Pro等闭源模型中表现优异，但由于开源模型普遍缺乏足够的评判与修正能力，其泛化性能受到限制。为此，我们提出MarsRL——一种融合智能体流水线并行的新型强化学习框架，旨在实现系统中所有智能体的协同优化。该框架通过引入智能体专属奖励机制以降低奖励噪声，并采用流水线式训练策略提升长轨迹处理效率。在Qwen3-30B-A3B-Thinking-2507模型上的实验表明，MarsRL将AIME2025准确率从86.5%提升至93.3%，BeyondAIME准确率从64.9%提升至73.8%，其表现甚至超越Qwen3-235B-A22B-Thinking-2507。这些发现充分证明MarsRL在推动多智能体推理系统发展、拓展其在多样化推理任务中应用边界方面具有重要潜力。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.11373">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.11373">arXiv</a></p>
<hr />
<h3>12. RF-DETR：基于神经架构搜索的实时检测Transformer</h3>
<p><strong>原文标题：</strong> RF-DETR: Neural Architecture Search for Real-Time Detection Transformers</p>
<p><strong>摘要：</strong>
开放词汇检测器在COCO数据集上表现出色，但在面对包含预训练阶段未见的分布外类别的真实数据集时往往泛化能力不足。不同于直接对大型视觉语言模型进行新领域的微调，我们提出RF-DETR——一种轻量级专业检测Transformer，通过权重共享神经架构搜索为任意目标数据集构建精度-延迟帕累托曲线。该方法在目标数据集上微调预训练基础网络，无需重新训练即可评估数千种具有不同精度-延迟权衡的网络配置。此外，我们重新审视神经架构搜索的"可调参数"，以提升DETR架构在多样化目标领域的迁移能力。值得注意的是，RF-DETR在COCO和Roboflow100-VL数据集上显著超越了现有最先进的实时检测方法。RF-DETR（纳米版）在COCO上达到48.0 AP，在相同延迟条件下较D-FINE（纳米版）提升5.3 AP；RF-DETR（2倍大型版）在Roboflow100-VL上以1.2 AP优势超越GroundingDINO（微型版），且推理速度提升20倍。据我们所知，RF-DETR（2倍大型版）是首个在COCO上突破60 AP的实时检测器。代码已开源：https://github.com/roboflow/rf-detr</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.09554">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.09554">arXiv</a></p>
<hr />
<h3>13. 经验引导的推理时策略自适应方法</h3>
<p><strong>原文标题：</strong> Experience-Guided Adaptation of Inference-Time Reasoning Strategies</p>
<p><strong>摘要：</strong>
使智能体AI系统能够基于训练后交互自适应调整问题解决方式仍是一项基础性挑战。现有支持推理时更新维护记忆的系统仅通过修改语言模型或智能体的文本输入进行引导，这意味着无法调整采样参数、移除工具、修改系统提示或在智能体与工作流范式间切换。另一方面，具备更强自适应能力的系统需要离线优化且部署后保持静态。我们提出经验引导推理器（EGuR），该系统基于累积经验在推理时动态生成定制化策略——包含LLM调用、工具使用、采样参数与控制逻辑的完整计算流程。这一突破通过基于LLM的元策略（即输出策略的策略）实现，支持对所有策略组件（提示词、采样参数、工具配置与控制逻辑）进行自适应调整。EGuR通过双组件运作：引导器基于当前问题与结构化经验记忆生成多个候选策略，整合器则通过执行反馈优化后续策略生成。该系统能生成针对特定问题优化的完整可执行策略，支持缓存、检索与按需执行，避免资源浪费。在五个高难度基准测试（AIME 2025、3-SAT及三项Big Bench Extra Hard任务）中，EGuR相较最强基线实现最高14%的准确率提升，同时将计算成本降低达111倍，且两项指标均随系统经验积累持续优化。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.11519">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.11519">arXiv</a></p>
<hr />
<h3>14. DiscoX：专业领域篇章级翻译任务的基准评测体系</h3>
<p><strong>原文标题：</strong> DiscoX: Benchmarking Discourse-Level Translation task in Expert Domains</p>
<p><strong>摘要：</strong>
专业领域的篇章级翻译评估体系尚不完善，尽管其对知识传播与跨语言学术交流具有核心意义。这类翻译任务既要求篇章层面的连贯性，又需要严格的术语准确性，而现有评估方法主要聚焦于句段层面的精确度与流畅性。为突破此局限，我们提出DiscoX——一个面向中英双语的专业级篇章翻译新型基准体系。该体系包含7个专业领域的200篇经专家审校文本，平均长度超过1700词符。为评估系统在DiscoX上的表现，我们同步开发了Metric-S无参考评估系统，可从准确性、流畅度及适配性三个维度提供细粒度自动评测。Metric-S与人工评判呈现高度一致性，显著优于现有评估指标。实验结果显示显著性能差距：即使最先进的大语言模型在这些任务上仍落后于人类专家。该发现验证了DiscoX的挑战难度，揭示了实现专业级机器翻译仍面临的困境。本研究提出的基准体系与评估系统为更严谨的翻译质量评估提供了可靠框架，将推动基于大语言模型的翻译技术持续发展。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.10984">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.10984">arXiv</a></p>
<hr />
<h3>15. EmoVid：面向情感中心视频理解与生成的多模态情感视频数据集</h3>
<p><strong>原文标题：</strong> EmoVid: A Multimodal Emotion Video Dataset for Emotion-Centric Video Understanding and Generation</p>
<p><strong>摘要：</strong>
情感在视频表达中具有关键作用，但现有视频生成系统主要关注低层次视觉指标而忽视情感维度。尽管情感分析在视觉领域已取得进展，视频学界仍缺乏专门资源来衔接情感理解与生成任务，尤其在风格化非写实场景中。为填补这一空白，我们推出EmoVid——首个专为创意媒体设计的多模态情感标注视频数据集，包含卡通动画、影视片段和动态表情包。每个视频均标注有情感标签、视觉属性（亮度、色彩饱和度、色调）及文本描述。通过系统分析，我们揭示了不同视频形式中视觉特征与情感感知关联的时空模式。基于这些发现，我们通过微调Wan2.1模型开发出情感条件视频生成技术。实验结果表明，在文本到视频和图像到视频任务中，生成视频的量化指标与视觉质量均获得显著提升。EmoVid为情感视频计算建立了新基准。本研究不仅为艺术风格化视频的视觉情感分析提供了重要见解，更为增强视频生成中的情感表达提供了实用方法。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.11002">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.11002">arXiv</a></p>
<hr />
<h3>16. 物尽其用：通过多头解码以结构化人类先验引导生成式推荐系统</h3>
<p><strong>原文标题：</strong> Don't Waste It: Guiding Generative Recommenders with Structured Human Priors via Multi-head Decoding</p>
<p><strong>摘要：</strong>
为推荐系统优化除准确性之外的目标（如多样性、新颖性和个性化）对实现长期用户满意度至关重要。为此，工业实践者已积累了海量结构化领域知识，我们称之为人类先验（如物品分类体系、时序模式）。这类知识通常通过排序或后排序阶段的后期调整方式应用，但该方法始终与核心模型学习相分离——这在行业向端到端生成式推荐基础模型转型的背景下尤显不足。另一方面，许多针对超准确性目标的方法往往需要特定架构修改，并以完全无监督的方式学习用户意图，从而丢弃了这些宝贵的人类先验。</p>
<p>我们提出了一种与主干模型无关的框架，将人类先验直接无缝集成到生成式推荐器的端到端训练中，而非摒弃多年实践积累的先验知识。通过借鉴高效大语言模型解码策略设计的轻量级先验条件适配头，我们的方法引导模型沿人类可理解的维度（如交互类型、长短期兴趣）解耦用户意图。同时，我们引入了分层组合策略来建模不同先验类型间的复杂交互。在三个大规模数据集上的实验表明，本方法显著提升了准确性及超准确性目标的表现。我们还证实人类先验能使主干模型更有效地利用更长上下文窗口和更大模型容量。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.10492">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.10492">arXiv</a></p>
<hr />
<h3>17. 工作负载调度器——起源、算法与差异</h3>
<p><strong>原文标题：</strong> Workload Schedulers -- Genesis, Algorithms and Differences</p>
<p><strong>摘要：</strong>
本文提出一种现代工作负载调度器的新型分类方法。我们详细描述了三类调度器：操作系统进程调度器、集群系统作业调度器与大数据调度器。通过考察算法应用与功能特性，系统阐述其从早期雏形到现代实现的演进过程。总结部分通过对比分析各类调度器的差异，探讨其历时性发展规律。最后我们着重指出，适用于本地与分布式系统的调度策略设计在核心关注点上呈现出显著共性。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.10258">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.10258">arXiv</a></p>
<hr />
<h3>18. 面向科学创意生成的大语言模型：以创造力为核心的综述研究</h3>
<p><strong>原文标题：</strong> Large Language Models for Scientific Idea Generation: A Creativity-Centered Survey</p>
<p><strong>摘要：</strong>
科学创意生成是科学发现的核心驱动力，无论是通过解决未解难题还是提出解释未知现象的新颖假说，这一过程始终推动着人类进步。与标准科学推理或通用创意生成不同，科学领域的创意生成具有多目标性和开放性的特点，其贡献的新颖性与实证严谨性同等重要。大语言模型近期展现出作为科学创意生成器的潜力，能够输出兼具连贯性、事实准确性、惊人直觉与合理推理的成果，但其创造能力仍存在不稳定性且缺乏深入理解。本综述对驱动大语言模型科学创意生成的方法进行结构化梳理，重点考察不同方法如何平衡创造力与科学严谨性。我们将现有方法归纳为五个互补体系：外部知识增强、基于提示的分布导向、推理时参数调控、多智能体协作以及参数级适应。为解析其贡献，我们采用两个互补框架：运用博登提出的组合型、探索型与变革型创造力分类法来界定各方法体系预期生成创意的层级，借助罗兹的4P框架（创作者、创作过程、创作环境、创作成果）定位不同方法强调的创造力维度或来源。通过将方法论进展与创造力理论框架相映射，本综述厘清了该领域的发展现状，并为实现大语言模型在科学发现中可靠、系统化且具有变革性应用的关键方向提供了路线指引。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.07448">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.07448">arXiv</a></p>
<hr />
<h3>19. 构建面向智能体的网络：一种声明式的智能体-网络交互框架</h3>
<p><strong>原文标题：</strong> Building the Web for Agents: A Declarative Framework for Agent-Web Interaction</p>
<p><strong>摘要：</strong>
当前自主AI智能体在网络上的部署受到一个根本性错位的阻碍：智能体必须从面向人类的用户界面推断功能可见性，导致交互过程脆弱、低效且不安全。为解决这一问题，我们提出VOIX——一个原生网络框架，通过简单的声明式HTML元素使网站能够为AI智能体提供可靠、可审计且保护隐私的能力。VOIX引入<tool>和<context>标签，允许开发者明确定义可用操作及相关状态，从而为智能体行为建立清晰的机器可读契约。该方法将控制权转移至网站开发者，同时通过将会话交互与网站分离来保护用户隐私。我们通过为期三天的黑客松研究（16名开发者参与）评估了该框架的实用性、易学性和表达能力。结果表明，无论参与者先前经验如何，均能快速构建多样化且功能完整的智能体赋能网络应用。最终，本研究为实现智能体网络提供了基础机制，为未来网络环境中无缝安全的人机协作铺平道路。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.11287">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.11287">arXiv</a></p>
<hr />
<h3>20. CATS-V2V：面向复杂恶劣交通场景的真实世界车车协同感知数据集</h3>
<p><strong>原文标题：</strong> CATS-V2V: A Real-World Vehicle-to-Vehicle Cooperative Perception Dataset with Complex Adverse Traffic Scenarios</p>
<p><strong>摘要：</strong>
车车协同感知技术通过克服复杂恶劣交通场景下的感知局限，在提升自动驾驶性能方面具有巨大潜力。与此同时，数据作为现代自动驾驶人工智能的基础设施至关重要。然而受限于严苛的数据采集要求，现有数据集主要聚焦于常规交通场景，制约了协同感知效益的充分发挥。为应对这一挑战，我们推出CATS-V2V——全球首个面向复杂恶劣交通场景的真实世界车车协同感知数据集。该数据集通过两辆硬件时间同步的车辆采集完成，涵盖10个不同地理位置的10类天气与光照条件。包含100段数据序列的该数据集提供了6万帧10Hz激光雷达点云与126万张多视角30Hz相机图像，并附有75万条经过匿名化处理的高精度RTK固定GNSS/IMU记录。我们同步提供了时序一致的物体三维边界框标注及静态场景数据，用以构建四维鸟瞰图表征。基于此，我们提出基于目标的时序对齐方法，确保所有物体在全传感器模态间实现精准对齐。我们期待CATS-V2V这一迄今同类数据集中规模最大、支持性最强、质量最高的数据集能够推动自动驾驶领域相关研究的发展。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.11168">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.11168">arXiv</a></p>
<hr />
<h3>21. 从证明到程序：大语言模型中工具诱发推理幻觉的特征分析</h3>
<p><strong>原文标题：</strong> From Proof to Program: Characterizing Tool-Induced Reasoning Hallucinations in Large Language Models</p>
<p><strong>摘要：</strong>
工具增强语言模型能够调用外部工具来解决超越其参数能力的问题。然而，这些工具带来的性能提升是否反映可信推理仍不明确。本研究聚焦代码解释器工具，发现即使工具被正确选择和执行，工具增强语言模型仍会将工具输出视为推理替代品，生成看似正确但缺乏连贯论证的解决方案。我们将这种失效模式定义为工具诱发短视，并基于PYMATH基准（包含1,679个竞赛级数学问题，其中Python代码具有辅助作用但非充分解）展开研究。我们进一步开发了多维评估体系，量化工具增强语言模型相较于无工具模型的推理退化现象。研究结果表明：虽然工具增强语言模型的最终答案准确率最高提升19.3个百分点，但其推理行为持续恶化（例如在推理过程的两两比较中，无工具大语言模型胜出率最高提升41.5%）。这种退化随工具使用频次加剧：模型调用工具越频繁，其推理连贯性越差。此外，工具使用使错误类型从算术失误转向全局推理失败（逻辑、假设、创造性错误），约55%的高风险案例中存在工具诱发短视现象。最后，我们提出基于偏好优化的对齐框架，使工具增强语言模型将工具作为辅助证据，在工具使用场景下同步提升最终答案准确率与推理深度。代码与数据详见：https://github.com/megagonlabs/TIM。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.10899">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.10899">arXiv</a></p>
<hr />
<h3>22. 一种面向云计算系统的元启发式负载均衡器</h3>
<p><strong>原文标题：</strong> A Meta-Heuristic Load Balancer for Cloud Computing Systems</p>
<p><strong>摘要：</strong>
本文提出一种云系统服务分配策略，旨在实现节点无过载运行状态下的系统稳定维持与最小成本运维。通过构建云资源利用的抽象模型，该研究综合考虑了多类型资源要素及服务迁移成本因素。实验部分展示了原型元启发式负载均衡器的运行效果，并对测试数据进行了系统性分析与讨论。此外，我们创新性地提出一种混合遗传算法，该算法通过引入其他元启发式算法的输出结果作为初始种群，有效提升了算法性能。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.11721">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.11721">arXiv</a></p>
<hr />
<h3>23. miniF2F-Lean再审视：局限分析与未来路径规划</h3>
<p><strong>原文标题：</strong> miniF2F-Lean Revisited: Reviewing Limitations and Charting a Path Forward</p>
<p><strong>摘要：</strong>
本文从参与数学奥林匹克竞赛的AI系统视角，对miniF2F基准测试中的形式化与非形式化命题展开全面分析。在此场景下，模型需要阅读理解自然语言表述的数学问题，将其形式化为Lean语言，继而完成证明过程。当形式化证明与原始非形式化命题相符时，系统方可获得相应积分。评估结果表明：采用文献中现有最优模型，该流程的最高准确率约为36%，显著低于自动形式化与定理证明领域分别报告的97%与69%的单项最优准确率。通过分析错误模式，我们发现准确率下降的主要根源在于miniF2F中超过半数问题的形式化与非形式化表述存在偏差。我们系统修正了形式化与非形式化陈述中的所有错误、差异及简化问题，最终提出包含完全验证的形式化/非形式化陈述及证明的miniF2F-v2。在新基准上的全流程定理证明测试显示，最佳准确率提升至70%（原miniF2F为40%），但仍反映出自动形式化模型与定理证明器之间存在显著失配。深度分析表明，更高质量的基准测试将有助于学界更精准评估形式推理领域进展，同时更有效诊断自动形式化与定理证明模型的失败与成功模式。本数据集已发布于：https://github.com/roozbeh-yz/miniF2F_v2</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.03108">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.03108">arXiv</a></p>
<hr />
<h2>🔍 关键词云图</h2>
<p><img alt="关键词云图" src="../images/keywords_wordcloud.png" /></p>
<h2>📈 近期论文趋势</h2>
<p><img alt="论文趋势" src="../images/daily_papers.png" /></p>
<h2>🎙️ 语音播报</h2>
<ul>
<li><a href="../audio/2025-11-17_daily_papers.mp3">收听今日论文解读</a></li>
</ul>
<h2>📱 订阅渠道</h2>
<ul>
<li>GitHub: <a href="https://github.com/2404589803/hf-daily-paper-newsletter-chinese">hf-daily-paper-newsletter-chinese</a></li>
</ul>
    </div>
</body>
</html>