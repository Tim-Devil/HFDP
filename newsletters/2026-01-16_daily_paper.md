
# <img src="https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo.png" width="30"/> Hugging Face 2026-01-16 论文日报

## 📊 今日论文统计
- 总论文数：38
- 热门领域：Vision, RL, GPT, LLM, Transformer

## 📝 论文详情


### 1. 基于视觉-语言推理的城市社会语义分割

**原文标题：** Urban Socio-Semantic Segmentation with Vision-Language Reasoning

**摘要：**
作为人类活动的枢纽，城市地表蕴含着丰富的语义实体。从卫星图像中分割这些多样化的实体对于一系列下游应用至关重要。当前先进的分割模型能够可靠地分割由物理属性定义的实体（如建筑物、水体），但在处理社会性定义类别（如学校、公园）时仍面临挑战。本研究通过视觉-语言模型推理实现了社会语义分割。为此，我们构建了名为SocioSeg的城市社会语义分割数据集，该数据集包含卫星影像、数字地图以及按层级结构组织的社会语义实体像素级标注，为相关研究提供了新的资源。此外，我们提出了一种名为SocioReasoner的新型视觉-语言推理框架，该框架通过跨模态识别与多阶段推理模拟人类识别与标注社会语义实体的认知过程。我们采用强化学习优化这一不可微分的推理流程，从而激发视觉-语言模型的推理潜能。实验结果表明，该方法在性能上超越了现有先进模型，并展现出强大的零样本泛化能力。数据集与代码已公开于https://github.com/AMAP-ML/SocioReasoner。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2601.10477) | [arXiv](https://arxiv.org/abs/2601.10477)



---

### 2. STEP3-VL-10B 技术报告

**原文标题：** STEP3-VL-10B Technical Report

**摘要：**
本文介绍STEP3-VL-10B，一个旨在重新定义紧凑效率与前沿多模态智能之间权衡的轻量级开源基础模型。该模型通过两项战略转型实现：首先，采用基于1.2万亿多模态标记的统一全参数解冻预训练策略，将语言对齐的感知编码器与Qwen3-8B解码器融合，建立本质化的视觉-语言协同机制；其次，构建包含超千轮强化学习的规模化后训练流程。关键创新在于引入并行协同推理（PaCoRe）机制，通过动态分配计算资源至可扩展的感知推理模块，实现对多样化视觉假设的探索与综合。实验表明，尽管仅具有100亿参数的紧凑架构，STEP3-VL-10B在多项基准测试中媲美或超越参数量10-20倍的大型模型（如GLM-4.6V-106B、Qwen3-VL-235B）及Gemini 2.5 Pro、Seed-1.5-VL等顶级专有旗舰模型。该模型取得业界领先性能：在MMBench达到92.2%，MMMU达到80.11%，同时在复杂推理任务中表现卓越，AIME2025得分94.43%，MathVision得分75.95%。我们完整开源模型体系，为学术界提供强大、高效且可复现的基准框架。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2601.09668) | [arXiv](https://arxiv.org/abs/2601.09668)



---

### 3. 奖励罕见：面向大语言模型创造性问题解决的独特性感知强化学习

**原文标题：** Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in LLMs

**摘要：**
强化学习已成为大语言模型后训练的核心范式，尤其在复杂推理任务中，但其常受探索坍缩问题困扰：策略过早集中于少数主导推理模式，虽能提升单次通过率，却限制了推演层面的多样性及多次采样通过率的增益。我们认为这一缺陷源于对局部标记行为的常规化约束，而非对解决方案集合多样性的考量。为此，我们提出独特性感知强化学习方法，该推演层目标函数明确奖励那些采用罕见高层策略的正确解决方案。本方法使用基于大语言模型的评判器，依据高层解决策略（忽略表面差异）对同一问题的推演结果进行聚类，并依据聚类规模对策略优势进行反向加权。由此，正确但新颖的策略将比冗余策略获得更高奖励。在数学、物理和医学推理基准测试中，本方法在大规模采样预算下持续提升多次采样通过率，并在不牺牲单次通过率的前提下提高通过率曲线下面积，同时维持探索能力，系统性地发掘出更多样化的解决策略。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2601.08763) | [arXiv](https://arxiv.org/abs/2601.08763)



---

### 4. 面向推理任务的协作式多智能体测试时强化学习

**原文标题：** Collaborative Multi-Agent Test-Time Reinforcement Learning for Reasoning

**摘要：**
多智能体系统已发展成为众多实际应用中由大语言模型驱动的实用协作框架，其通过多样性与交叉验证获得鲁棒性。然而，多智能体强化学习训练过程资源消耗大且稳定性不足：智能体间的协同适应会引发非平稳性问题，而奖励信号往往稀疏且具有高方差特性。为此，我们提出多智能体测试时强化学习框架，该框架在推理阶段将结构化文本经验注入多智能体决策过程。MATTRL构建由专业智能体组成的多专家团队进行多轮讨论，检索并整合测试时经验，最终通过共识机制形成决策。本研究同时探讨了用于构建轮次级经验池并进行对话重注入的信用分配机制。在医学、数学与教育领域的多个挑战性基准测试中，MATTRL相较于多智能体基线模型平均准确率提升3.67%，较可比单智能体基线提升8.67%。消融实验检验了不同信用分配方案，并详细比较了其对训练结果的影响。MATTRL为无需调参即可实现分布偏移鲁棒的多智能体推理提供了一条稳定、高效且可靠的路径。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2601.09667) | [arXiv](https://arxiv.org/abs/2601.09667)



---

### 5. VIBE：基于视觉指令的编辑器

**原文标题：** VIBE: Visual Instruction Based Editor

**摘要：**
基于指令的图像编辑是生成式人工智能领域中发展最快的方向之一。过去一年间，该领域已达到新的水平，数十个开源模型与高性能商业系统相继发布。然而，目前仅有有限数量的开源方法能够实现实际应用级别的质量。此外，作为当前主流技术路线的扩散模型通常参数量庞大、计算成本高昂，广泛使用的变体通常包含60亿至200亿参数，这对许多部署场景和研究环境构成了挑战。本文提出了一种紧凑高效的基于指令图像编辑流程：采用具有20亿参数的现代Qwen3-VL模型指导编辑过程，并利用16亿参数的扩散模型Sana1.5进行图像生成。我们在架构设计、数据处理、训练配置和评估标准等方面均以低成本推理和严格源图像一致性为目标，同时在该规模可行的主要编辑类别中保持高质量输出。通过在ImgEdit和GEdit基准测试上的评估，本方法达到或超越了参数量数倍于本系统、推理成本更高的基线模型性能，尤其在需要保持输入图像特征的编辑任务（如属性调整、对象移除、背景编辑和定向替换）中表现突出。该模型可在24GB GPU内存内运行，在未进行额外推理优化或蒸馏的情况下，于NVIDIA H100显卡上以BF16精度生成最高2K分辨率的编辑图像仅需约4秒。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2601.02242) | [arXiv](https://arxiv.org/abs/2601.02242)



---

### 6. 超越静态工具：面向科学推理的测试时工具演化

**原文标题：** Beyond Static Tools: Test-Time Tool Evolution for Scientific Reasoning

**摘要：**
科学人工智能的核心挑战不仅在于推理本身，更在于在开放的科学世界中创造计算方法的能力。现有基于大语言模型的智能体依赖于静态、预定义的工具库，这种范式在工具稀缺、异构且本质上不完整的科学领域存在根本性缺陷。本文提出测试时工具演化新范式，使智能体能够在推理过程中合成、验证并演化可执行工具。通过将工具从固定资源转化为问题驱动的产物，该方法克服了静态工具库的僵化性与长尾局限性。为建立严谨的评估体系，我们构建了SciEvo基准测试集，包含1,590项科学推理任务及925个自动演化工具的支持。大量实验表明，该方法在准确率与工具效率方面均达到最先进水平，同时实现了计算工具的有效跨领域适配。代码与基准测试集已发布于https://github.com/lujiaxuan0520/Test-Time-Tool-Evol。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2601.07641) | [arXiv](https://arxiv.org/abs/2601.07641)



---

### 7. 丹青：一个前沿的大规模中文视觉-语言预训练数据集

**原文标题：** DanQing: An Up-to-Date Large-Scale Chinese Vision-Language Pre-training Dataset

**摘要：**
视觉-语言预训练模型通过对比预训练从大规模图文对中学习，在各种下游任务中展现出强大性能。大规模英文图文数据集（如COYO-700M和LAION-400M）的发布，使得CLIP、SigLIP等模型在跨模态检索、图像描述等任务中得到广泛应用。然而，由于高质量中文图文数据的稀缺，中文视觉-语言预训练的发展明显滞后。为填补这一空白，我们开发了一套构建高质量中文跨模态数据集的完整流程。基于此，我们提出了丹青数据集，该数据集包含从Common Crawl收集的1亿个图文对。与现有数据集不同，丹青通过更严格的筛选流程进行构建，数据质量更高。此外，丹青主要基于2024-2025年的网络数据构建，使模型能更好地捕捉语义演变趋势，从而具备更强的实用价值。我们通过持续预训练SigLIP2模型，将丹青与现有数据集进行比较。实验结果表明，在一系列中文下游任务中，包括零样本分类、跨模态检索以及基于大语言模型的评估，丹青均能取得更优性能。为促进中文视觉-语言预训练的进一步研究，我们将在知识共享CC-BY 4.0许可下开源丹青数据集。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2601.10305) | [arXiv](https://arxiv.org/abs/2601.10305)



---

### 8. 迈向超长程自主科学：面向机器学习工程的认知累积

**原文标题：** Toward Ultra-Long-Horizon Agentic Science: Cognitive Accumulation for Machine Learning Engineering

**摘要：**
人工智能向自主科学的发展目前正受限于超长程自主性的挑战——即在跨越数日乃至数周的实验周期中保持战略连贯性与迭代修正的能力。尽管大语言模型已在短程推理中展现出强大能力，但在现实科研的高维度、延迟反馈环境中，它们极易被执行细节淹没，难以将稀疏反馈整合为连贯的长期指导。本文提出ML-Master 2.0系统，这是一个掌握超长程机器学习工程自主能力的智能体，该领域可作为科学发现的典型微观缩影。通过将情境管理重构为认知累积过程，本方法引入受计算机体系结构启发的分层认知缓存架构。该多层级架构能够实现经验随时间推移的结构化分层：通过动态将瞬时执行轨迹提炼为稳定知识与跨任务智慧，使智能体得以解耦即时执行与长期实验策略，有效突破静态上下文窗口的扩展限制。在OpenAI MLE-Bench平台24小时预算的评估中，ML-Master 2.0实现了56.44%的领先奖牌获取率。研究表明，超长程自主性为人工智能超越人类既有认知复杂度进行自主探索提供了可扩展的技术蓝图。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2601.10402) | [arXiv](https://arxiv.org/abs/2601.10402)



---

### 9. CoF-T2I：作为纯视觉推理器的视频模型用于文本到图像生成

**原文标题：** CoF-T2I: Video Models as Pure Visual Reasoners for Text-to-Image Generation

**摘要：**
近期视频生成模型揭示了帧链推理能力的涌现，实现了逐帧的视觉推断。凭借这一能力，视频模型已成功应用于多种视觉任务（如迷宫求解、视觉谜题）。然而，由于文本到图像生成过程中缺乏明确定义的视觉推理起点和可解释的中间状态，其在增强文本到图像生成方面的潜力尚未得到充分探索。为弥合这一差距，我们提出CoF-T2I模型，该模型通过渐进式视觉优化将帧链推理融入文本到图像生成过程，其中中间帧作为显式推理步骤，最终帧作为输出结果。为构建这种显式生成过程，我们构建了CoF-Evol-Instruct数据集，该数据集包含建模从语义到美学生成过程的帧链轨迹。为进一步提升生成质量并避免运动伪影，我们实现了对每帧的独立编码操作。实验表明，CoF-T2I显著优于基础视频模型，并在具有挑战性的基准测试中取得竞争性表现：在GenEval上达到0.86分，在Imagine-Bench上达到7.468分。这些结果表明视频模型在推进高质量文本到图像生成方面具有巨大潜力。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2601.10061) | [arXiv](https://arxiv.org/abs/2601.10061)



---

### 10. 先思后生：基于大语言模型编码器的推理感知文本到图像扩散方法

**原文标题：** Think-Then-Generate: Reasoning-Aware Text-to-Image Diffusion with LLM Encoders

**摘要：**
文本到图像扩散模型的最新进展已能依据多样化文本提示生成高质量视觉内容。然而，现有大多数文本到图像扩散模型（包括配备大语言模型文本编码器的系统）仍停留在文本-像素映射层面——它们仅将大语言模型用作文本编码器，未能利用其内在推理能力来推断文本提示对应的视觉呈现。为突破这种字面化生成模式，本文提出“先思后生”范式，通过激励基于大语言模型的文本编码器对原始用户提示进行推理与重写，并将重写后的提示状态作为扩散条件。为实现这一目标，我们首先通过轻量级监督微调激活大语言模型编码器的“先思后写”能力，随后通过双重生成式强化策略优化协同优化语言模型编码器与扩散主干网络，确保对上下文进行忠实推理并通过语义实现精确渲染。具体而言，文本编码器通过基于图像的奖励机制强化其世界知识推断与召回能力，而扩散主干网络则被推动生成语义一致且视觉连贯的图像。实验表明，该方法在基于推理的图像生成与编辑基准测试中，在事实一致性、语义对齐和视觉真实性方面取得显著提升，WISE分数达到0.79，与GPT-4表现近乎持平。本研究为构建兼具推理、表达与演示能力的下一代统一模型迈出了重要一步。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2601.10332) | [arXiv](https://arxiv.org/abs/2601.10332)



---

### 11. Alterbute：图像中物体内在属性的编辑方法

**原文标题：** Alterbute: Editing Intrinsic Attributes of Objects in Images

**摘要：**
本文提出Alterbute，一种基于扩散模型的图像物体内在属性编辑方法。该方法支持修改物体的颜色、纹理、材质甚至形状，同时保持其感知身份与场景上下文。现有方法要么依赖无监督先验而难以保持身份一致性，要么采用过度严格的监督机制限制了有意义的属性变化。我们的方法基于两个核心设计：（一）采用宽松的训练目标，使模型能够根据身份参考图像、描述目标内在属性的文本提示、以及定义外部背景的背景图像与物体掩码，同时修改内在与外在属性。在推理阶段，通过复用原始背景与物体掩码来限制外在变化，从而确保仅目标内在属性被修改；（二）引入视觉命名实体——这是一种细粒度的视觉身份类别（例如“保时捷911卡雷拉”），将具有身份定义特征但内在属性可变的物体归为一类。我们利用视觉语言模型从大规模公共图像数据集中自动提取VNE标签与内在属性描述，实现了可扩展且保持身份一致性的监督训练。实验表明，Alterbute在保持身份一致性的物体内在属性编辑任务上优于现有方法。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2601.10714) | [arXiv](https://arxiv.org/abs/2601.10714)



---

### 12. MatchTIR：基于二分图匹配的工具集成推理细粒度监督方法

**原文标题：** MatchTIR: Fine-Grained Supervision for Tool-Integrated Reasoning via Bipartite Matching

**摘要：**
工具集成推理通过将推理步骤与外部工具调用交错进行，赋能大语言模型处理复杂任务。然而，现有的强化学习方法通常依赖于结果级或轨迹级奖励，对轨迹中的所有步骤赋予统一的优势度。这种粗粒度的信用分配无法区分有效工具调用与冗余或错误调用，尤其在长视野、多轮次的复杂场景中。为此，我们提出MatchTIR框架，通过基于二分图匹配的轮次级奖励分配与双层级优势估计机制，实现细粒度监督。具体而言，我们将信用分配问题构建为预测轨迹与真实轨迹间的二分图匹配问题，采用两种分配策略生成稠密的轮次级奖励信号。此外，为平衡局部步骤精度与全局任务成功率，我们设计了融合轮次级与轨迹级信号的双层级优势估计方案，为每个交互轮次分配差异化的优势值。在三个基准测试上的大量实验验证了MatchTIR的优越性。值得注意的是，我们的40亿参数模型在多数任务上超越了80亿参数的竞争模型，尤其在长视野与多轮次任务中表现突出。代码已开源：https://github.com/quchangle1/MatchTIR。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2601.10712) | [arXiv](https://arxiv.org/abs/2601.10712)



---

### 13. ToolSafe：通过主动式步骤级护栏与反馈增强基于大语言模型的智能体工具调用安全性

**原文标题：** ToolSafe: Enhancing Tool Invocation Safety of LLM-based agents via Proactive Step-level Guardrail and Feedback

**摘要：**
尽管基于大语言模型的智能体能够通过调用外部工具与环境交互，但其扩展的能力也同时放大了安全风险。实时监控步骤级工具调用行为并在不安全执行前主动干预，对于智能体部署至关重要，然而相关研究仍显不足。本研究首先构建了TS-Bench——一个面向大语言模型智能体中步骤级工具调用安全性检测的新型基准测试集。随后，我们利用多任务强化学习开发了护栏模型TS-Guard。该模型通过分析交互历史记录，在执行前主动检测不安全的工具调用行为，评估请求的危害性及行为与攻击的关联性，并生成可解释、可泛化的安全性判断与反馈。此外，我们提出了TS-Flow——一种基于护栏-反馈驱动的智能体推理框架。该框架在提示注入攻击场景下，平均将ReAct式智能体的有害工具调用减少了65%，并将良性任务完成率提升了约10%。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2601.10156) | [arXiv](https://arxiv.org/abs/2601.10156)



---

### 14. 关于GPT-5.2、Gemini 3 Pro、Qwen3-VL、Doubao 1.8、Grok 4.1 Fast、Nano Banana Pro及Seedream 4.5的安全性评估报告

**原文标题：** A Safety Report on GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5

**摘要：**
大语言模型（LLMs）与多模态大语言模型（MLLMs）的快速发展，显著提升了语言与视觉领域的推理、感知及生成能力。然而，这些技术进步是否带来相应的安全性提升尚不明确，部分原因在于现有评估实践较为零散，多局限于单一模态或威胁模型。本报告对GPT-5.2、Gemini 3 Pro、Qwen3-VL、Doubao 1.8、Grok 4.1 Fast、Nano Banana Pro及Seedream 4.5等7个前沿模型进行了综合性安全评估。我们采用统一评估框架——整合基准测试、对抗性评估、多语言评估与合规性评估——在语言、视觉-语言及图像生成三种场景下对各模型进行测试。通过将多模式评估结果汇总为安全性排行榜与模型安全画像，揭示出当前前沿模型安全性呈现显著异质性。尽管GPT-5.2在所有评估中均表现出持续稳健且均衡的安全性能，其他模型则在基准安全性、对抗对齐性、多语言泛化能力及法规合规性之间存在明显权衡。语言与视觉-语言模态在对抗性评估中均表现出显著脆弱性，所有模型在标准基准测试表现良好的情况下仍出现大幅性能衰退。文本到图像模型在受监管视觉风险类别中展现出相对更强的对齐能力，但在对抗性或语义模糊提示下仍显脆弱。总体而言，这些结果表明前沿模型的安全性本质上是多维度的——受模态特性、语言类型及评估方案共同影响，这凸显了建立标准化安全评估体系的必要性，以准确评估实际风险，并引导负责任的模型开发与部署。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2601.10527) | [arXiv](https://arxiv.org/abs/2601.10527)



---

### 15. Molmo2：具备视频理解与定位能力的视觉语言模型开源权重与数据集

**原文标题：** Molmo2: Open Weights and Data for Vision-Language Models with Video Understanding and Grounding

**摘要：**
当前最先进的视频语言模型（VLMs）仍属于闭源系统。最强的开源权重模型要么依赖闭源VLMs生成的合成数据进行知识蒸馏，要么未公开其训练数据与方法。这导致开源社区缺乏改进前沿视频（及图像）语言模型的基础。关键的是，许多下游应用不仅需要高层次视频理解能力，更需要像素级的指向或跟踪定位能力——即使是闭源模型也尚未完全具备这种能力。本文提出Molmo2系列VLMs，该系列在开源模型中达到最先进水平，并在单图像、多图像及视频任务的指向驱动定位方面展现出卓越的新能力。我们的核心贡献在于构建了7个新视频数据集与2个多图像数据集，包括用于预训练的高细节视频描述数据集、用于微调的自由形式视频问答数据集、包含复杂查询的新目标跟踪数据集，以及创新的视频指向数据集——所有数据均未使用闭源VLMs生成。同时，我们提出基于高效数据打包与消息树编码方案的训练方法，并证明视觉令牌的双向注意力机制与新颖的令牌权重策略能有效提升性能。我们8B规模的顶尖模型在短视频理解、计数与描述任务上超越同类开源权重与数据模型，在长视频任务中表现相当，在视频定位任务中显著优于Qwen3-VL等现有开源模型（视频计数准确率35.5对29.6），并在部分任务上超越Gemini 3 Pro等闭源模型（视频指向F1分数38.4对20.0，视频跟踪J&F分数56.2对41.1）。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2601.10611) | [arXiv](https://arxiv.org/abs/2601.10611)



---

### 16. FlowAct-R1：迈向交互式人形视频生成

**原文标题：** FlowAct-R1: Towards Interactive Humanoid Video Generation

**摘要：**
交互式人形视频生成旨在合成逼真的视觉智能体，使其能够通过连续且响应式的视频与人类互动。尽管视频合成领域近期取得了进展，现有方法仍常常面临高保真合成与实时交互需求之间的权衡。本文提出FlowAct-R1，一个专为实时交互式人形视频生成设计的框架。该框架基于MMDiT架构构建，能够在保持低延迟响应的同时，实现任意时长视频的流式合成。我们引入了一种分块扩散强制策略，并结合新颖的自强制变体，以减轻误差累积并确保连续交互过程中的长期时间一致性。通过高效的蒸馏技术与系统级优化，本框架在480p分辨率下实现了稳定的25fps生成速度，首帧生成时间仅约1.5秒。所提方法提供整体且细粒度的全身控制，使智能体能够在交互场景中自然过渡于多种行为状态。实验结果表明，FlowAct-R1在保持跨角色风格鲁棒泛化能力的同时，实现了卓越的行为生动性与感知真实感。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2601.10103) | [arXiv](https://arxiv.org/abs/2601.10103)



---

### 17. 面向快速视频生成的过渡匹配蒸馏方法

**原文标题：** Transition Matching Distillation for Fast Video Generation

**摘要：**
大型视频扩散与流模型已在高质量视频生成领域取得显著成就，但由于其低效的多步采样过程，在实时交互应用中的使用仍受限制。本研究提出过渡匹配蒸馏（TMD）框架，该创新方法可将视频扩散模型蒸馏为高效少步生成器。TMD的核心思想是将扩散模型的多步去噪轨迹与少步概率转移过程相匹配，其中每个转移步骤通过轻量级条件流模型实现。为实现高效蒸馏，我们将原始扩散主干网络分解为两个组件：（1）主主干网络（包含早期多数层），用于在外部转移步骤中提取语义表征；（2）流头部网络（由最后若干层构成），利用这些表征执行多重内部流更新。针对预训练视频扩散模型，我们首先为其引入流头部结构，并将其适配为条件流映射。随后通过分布匹配蒸馏方法，在每一步转移过程中对配备流头部展开的学生模型进行训练。基于Wan2.1 1.3B和14B文本到视频模型的蒸馏实验表明，TMD在生成速度与视觉质量之间实现了灵活而优越的平衡。特别值得注意的是，在可比较的推理成本下，TMD在视觉保真度与提示遵循度方面均优于现有蒸馏模型。项目页面：https://research.nvidia.com/labs/genair/tmd

**论文链接：** [HuggingFace](https://huggingface.co/papers/2601.09881) | [arXiv](https://arxiv.org/abs/2601.09881)



---

### 18. PACEvolve：实现长周期进度感知一致性进化的框架

**原文标题：** PACEvolve: Enabling Long-Horizon Progress-Aware Consistent Evolution

**摘要：**
大型语言模型（LLM）已成为进化搜索的强大操作工具，但高效搜索框架的设计仍处于临时性阶段。尽管当前基于LLM的循环系统展现出潜力，但其在管理进化过程方面缺乏系统性方法。我们识别出三种典型的失效模式：上下文污染（实验历史对未来候选生成产生偏差）、模式崩溃（由于探索-利用平衡不佳导致智能体陷入局部最优）以及弱协作（僵化的交叉策略无法有效利用并行搜索轨迹）。为应对这些挑战，我们提出了进度感知一致性进化框架（PACEvolve），该框架通过鲁棒地管理智能体上下文与搜索动态来解决问题。PACEvolve整合了分层上下文管理（HCM）与剪枝机制以应对上下文污染；采用基于动量的回溯策略（MBB）逃离局部最优；并通过自适应采样策略将回溯与交叉统一为动态搜索协调机制（CE），使智能体能够平衡内部优化与跨轨迹协作。实验表明，PACEvolve为持续的长周期自我改进提供了系统性路径，在LLM-SR和KernelBench基准测试中取得最先进成果，并在Modded NanoGPT任务中发现了超越历史记录的解决方案。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2601.10657) | [arXiv](https://arxiv.org/abs/2601.10657)



---

### 19. Action100M：大规模视频动作数据集

**原文标题：** Action100M: A Large-scale Video Action Dataset

**摘要：**
从视觉观察中推断物理动作是推动机器智能在物理世界中发展的核心能力。实现这一目标需要大规模、开放词汇的视频动作数据集，覆盖广泛领域。本文介绍了Action100M，这是一个基于120万条互联网教学视频（总时长14.6年）构建的大规模数据集，提供了约1亿个具有开放词汇动作标注和丰富描述文本的时序定位片段。Action100M通过全自动流程生成，该流程（i）利用V-JEPA 2嵌入进行分层时序分割，（ii）生成组织为“描述树”的多层级帧与片段描述文本，（iii）通过多轮自优化流程中的推理模型（GPT-OSS-120B）聚合证据，输出结构化标注（简洁/详细动作、执行者、简洁/详细描述）。在Action100M上训练VL-JEPA模型显示出持续的数据规模扩展效益，并在多样化的动作识别基准测试中表现出强大的零样本性能，这确立了Action100M作为视频理解与世界建模可扩展研究的新基础。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2601.10592) | [arXiv](https://arxiv.org/abs/2601.10592)



---

### 20. M^4olGen：精确多属性约束下的多智能体多阶段分子生成

**原文标题：** M^4olGen: Multi-Agent, Multi-Stage Molecular Generation under Precise Multi-Property Constraints

**摘要：**
生成能够同时满足多种理化性质精确数值约束的分子至关重要且极具挑战性。尽管大语言模型（LLMs）具有强大的表达能力，但在缺乏外部结构和反馈的情况下，它们难以实现精确的多目标控制和数值推理。本文提出M^4olGen，一个在多重属性约束下进行分子生成的片段级、检索增强型两阶段框架。第一阶段：原型生成——一个多智能体推理器执行基于检索锚定的片段级编辑，生成接近可行区域的候选分子。第二阶段：基于强化学习的细粒度优化——一个采用组相对策略优化（GRPO）训练的片段级优化器，通过单跳或多跳精细化操作，在调控编辑复杂度和原型偏离度的同时，显式最小化属性与目标值之间的误差。支撑这两个阶段的是一个大规模自动构建的数据集，其中包含片段编辑的推理链及实测属性变化值，从而实现了确定性的、可复现的监督以及可控的多跳推理。与先前工作不同，本框架通过利用片段更好地对分子进行推理，并支持向数值目标的可控精细化调整。在两组属性约束（QED、LogP、分子量以及HOMO、LUMO）下的生成实验表明，本方法在分子有效性和多属性目标的精确满足度上均取得稳定提升，性能优于强基准LLMs及基于图的算法。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2601.10131) | [arXiv](https://arxiv.org/abs/2601.10131)



---

### 21. HeartMuLa：开源音乐基础模型系列

**原文标题：** HeartMuLa: A Family of Open Sourced Music Foundation Models

**摘要：**
本文提出一系列开源音乐基础模型，旨在推动跨任务与跨模态的大规模音乐理解与生成。该框架包含四大核心组件：（1）HeartCLAP：音频-文本对齐模型；（2）HeartTranscriptor：针对真实音乐场景优化的鲁棒歌词识别模型；（3）HeartCodec：低帧率（12.5 Hz）高保真音乐编解码分词器，在捕捉长程音乐结构的同时保留细粒度声学细节，并支持高效自回归建模；（4）HeartMuLa：基于大语言模型的歌曲生成模型，能够在丰富且用户可控的条件下（如文本风格描述、歌词及参考音频）合成高保真音乐。此外，该模型提供两种专项模式：（i）细粒度音乐属性控制，允许用户通过自然语言指令指定歌曲不同段落（如前奏、主歌、副歌）的风格；（ii）简短动感音乐生成，适用于短视频背景音乐。实验表明，当模型参数扩展至70亿时，HeartMuLa性能显著提升。本研究首次证明，利用学术规模的数据与GPU资源即可复现Suno级别的商业级系统。我们期望该系列基础模型能为未来研究提供有力基准，并推动多模态内容生产的实际应用。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2601.10547) | [arXiv](https://arxiv.org/abs/2601.10547)



---

### 22. 视频生成模型的推理时物理对齐与潜在世界模型

**原文标题：** Inference-time Physics Alignment of Video Generative Models with Latent World Models

**摘要：**
当前最先进的视频生成模型虽能产生视觉效果出色的内容，却常违背基础物理规律，限制了其实用性。尽管有观点认为这一缺陷源于预训练阶段对物理规律理解不足，但我们发现物理合理性的缺失也源于次优的推理策略。为此，我们提出WMReward方法，将提升视频生成的物理合理性视为推理时的对齐问题。具体而言，我们利用潜在世界模型（本文采用VJEPA-2）的强物理先验作为奖励信号，通过搜索和引导多条候选去噪轨迹，实现测试阶段计算资源的灵活扩展以提升生成性能。实验表明，该方法在图像条件生成、多帧条件生成和文本条件生成等多种设定下均显著提升了物理合理性，并通过人类偏好研究验证了其有效性。值得注意的是，在ICCV 2025感知测试PhysicsIQ挑战赛中，我们以62.64%的最终得分获得第一名，较先前最佳性能提升7.42%。本研究证明了利用潜在世界模型提升视频生成物理合理性的可行性，其价值超越了特定模型实例或参数化方案。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2601.10553) | [arXiv](https://arxiv.org/abs/2601.10553)



---

### 23. EvasionBench：基于多模型共识与LLM仲裁机制的财务问答规避性回答检测

**原文标题：** EvasionBench: Detecting Evasive Answers in Financial Q&A via Multi-Model Consensus and LLM-as-Judge

**摘要：**
在财报电话会议中检测规避性回答对提升财务透明度至关重要，但大规模基准数据的缺乏制约了该领域进展。本研究提出EvasionBench数据集，包含按三种规避等级划分的30,000个训练样本及1,000个人工标注测试样本（科恩卡帕系数0.835）。核心贡献在于构建了多模型标注框架，其理论基础是：前沿大语言模型间的标注分歧往往指向最具训练价值的困难样本。我们通过挖掘两个强标注模型产生冲突的边界案例，并引入仲裁模型进行最终标注决策。该方法相比单模型蒸馏策略提升2.4%性能，仲裁标注样本虽导致更高训练损失（0.421对比0.393），却显著提升了模型泛化能力——这证明分歧挖掘机制发挥了隐式正则化作用。最终训练的Eva-4B模型（40亿参数）达到81.3%准确率，较基础模型提升25个百分点，仅以极低推理成本即可逼近前沿大语言模型性能水平。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2601.09142) | [arXiv](https://arxiv.org/abs/2601.09142)



---

### 24. TAG-MoE：面向统一生成专家混合模型的任务感知门控机制

**原文标题：** TAG-MoE: Task-Aware Gating for Unified Generative Mixture-of-Experts

**摘要：**
在稠密扩散Transformer架构中，统一的图像生成与编辑模型面临严重的任务干扰问题，其共享参数空间必须在相互冲突的目标（例如局部编辑与主体驱动生成）之间进行折衷。尽管稀疏专家混合模型范式是一种有前景的解决方案，但其门控网络仍保持任务无关性，仅基于局部特征进行操作，无法感知全局任务意图。这种任务无关特性阻碍了有意义的专业化分工，且未能解决根本的任务干扰问题。本文提出一种新颖框架，将语义意图注入MoE路由机制。我们引入分层任务语义标注方案以构建结构化任务描述符（如作用范围、任务类型、内容保持要求），进而设计预测对齐正则化方法，使内部路由决策与任务高层语义保持一致。该正则化使门控网络从任务无关执行器演变为任务调度中心。实验表明，本模型能有效缓解任务干扰，在生成保真度与质量上均优于稠密基线模型；分析结果进一步证实，各专家模块能自然形成清晰且语义关联的专业化能力。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2601.08881) | [arXiv](https://arxiv.org/abs/2601.08881)



---

### 25. PRL：过程奖励学习提升大语言模型的推理能力并拓展推理边界

**原文标题：** PRL: Process Reward Learning Improves LLMs' Reasoning Ability and Broadens the Reasoning Boundary

**摘要：**
提升大语言模型的推理能力是当前持续关注的研究课题。然而，现有工作大多基于轨迹层面的结果奖励，缺乏对推理过程的细粒度监督。其他试图融合过程信号以优化大语言模型的训练框架也严重依赖蒙特卡洛树搜索、训练独立奖励模型等繁琐的额外步骤，损害了训练效率。此外，过程信号设计背后的直觉缺乏严格的理论支撑，导致对优化机制的理解仍不清晰。本文提出过程奖励学习方法，该方法将熵正则化强化学习目标分解为中间步骤，并据此为模型分配具有严格理论依据的过程奖励。我们从理论动机出发，推导出PRL的数学形式，其本质等效于奖励最大化目标加上策略模型与参考模型之间的KL散度惩罚项。PRL能够将结果奖励转化为过程监督信号，从而在强化学习优化过程中更好地引导探索。实验结果表明，PRL不仅通过平均@n指标提升了大语言模型推理能力的整体表现，还通过改进pass@n指标拓展了推理边界。大量实验验证了PRL方法的有效性和泛化能力。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2601.10201) | [arXiv](https://arxiv.org/abs/2601.10201)



---

### 26. LaViT：面向多模态推理的潜在视觉思维对齐

**原文标题：** LaViT: Aligning Latent Visual Thoughts for Multi-modal Reasoning

**摘要：**
当前多模态潜在推理方法通常依赖外部监督（如辅助图像），忽略了内在的视觉注意力动态机制。本研究揭示了知识蒸馏中的关键感知鸿沟：学生模型常模仿教师的文本输出，却关注完全不同的视觉区域，本质上依赖语言先验而非基于感知的推理。为解决此问题，我们提出LaViT框架，通过对齐潜在视觉思维而非静态嵌入表示来弥合这一鸿沟。LaViT强制学生在文本生成前自回归地重构教师的视觉语义与注意力轨迹，并采用课程式感知门控机制以避免捷径学习。大量实验表明，LaViT显著增强了视觉基础能力，在复杂推理任务上最高提升16.9%，使紧凑的30亿参数模型性能超越更大规模的开源变体及GPT-4o等专有模型。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2601.10129) | [arXiv](https://arxiv.org/abs/2601.10129)



---

### 27. LSRIF：面向指令跟随的逻辑结构化强化学习

**原文标题：** LSRIF: Logic-Structured Reinforcement Learning for Instruction Following

**摘要：**
指令跟随能力对大语言模型至关重要，但现实场景中的指令常包含顺序依赖与条件分支等逻辑结构。现有方法通常构建带并行约束的数据集并优化平均奖励，忽略了逻辑依赖性，导致训练信号存在噪声。本文提出一种显式建模指令逻辑的训练框架LSRIF。我们首先构建包含并行、顺序、条件三类约束结构的LSRInstruct数据集，进而设计结构感知的奖励机制LSRIF：对并行结构采用平均聚合奖励，对顺序结构实施失败惩罚传递机制，对条件分支设计选择性奖励。实验表明，LSRIF在指令跟随（领域内/外）和通用推理任务上均带来显著提升。分析显示，通过显式学习逻辑结构，模型注意力层的参数分布发生系统性变化，对约束标记与逻辑运算符的注意力聚焦程度显著增强。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2601.06431) | [arXiv](https://arxiv.org/abs/2601.06431)



---

### 28. 临床文本到SQL中的患者相似性队列推理

**原文标题：** Patient-Similarity Cohort Reasoning in Clinical Text-to-SQL

**摘要：**
现实世界的临床文本到SQL任务需要对异构电子健康记录表、时间窗口和患者相似性队列进行推理，以生成可执行的查询。我们提出了CLINSQL基准测试，该基准基于MIMIC-IV v3.1数据集构建，包含633项专家标注的任务，要求进行多表连接、具有临床意义的筛选并生成可执行的SQL。解决CLINSQL任务需要理解数据库模式元数据和临床编码系统、处理长上下文信息，并构建超越传统文本到SQL的多步骤查询。我们在思维链自优化框架下评估了22个专有和开源模型，并采用基于评分标准的SQL分析与执行检查，优先考虑关键的临床需求。尽管近期技术有所进展，但模型性能仍远未达到临床可靠性要求：在测试集上，GPT-5-mini的执行准确率为74.7%，DeepSeek-R1以69.2%的成绩领先开源模型，而Gemini-2.5-Pro在简单任务上虽达到85.5%，但在困难任务上骤降至67.2%。CLINSQL的进展标志着面向真实世界电子健康记录分析的临床可靠文本到SQL技术取得了实质性进步。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2601.09876) | [arXiv](https://arxiv.org/abs/2601.09876)



---

### 29. 智能体技能在真实环境中的安全性：大规模安全漏洞实证研究

**原文标题：** Agent Skills in the Wild: An Empirical Study of Security Vulnerabilities at Scale

**摘要：**
AI智能体框架的兴起催生了智能体技能——一种包含指令与可执行代码的模块化封装包，能够动态扩展智能体能力。尽管这种架构实现了强大的定制功能，但技能通常以默认信任状态执行且缺乏严格审查，从而形成了重要但尚未被充分认知的攻击面。本研究首次对这一新兴生态系统开展大规模实证安全分析，从两大主流市场收集42,447个技能样本，并运用SkillScan多阶段检测框架（整合静态分析与基于大语言模型的语义分类技术）对31,132个技能进行系统化检测。研究发现普遍存在的安全风险：26.1%的技能至少存在一个漏洞，这些漏洞涵盖提示词注入、数据窃取、权限提升和供应链风险四大类共14种具体模式。其中数据窃取（13.3%）和权限提升（11.8%）最为普遍，5.2%的技能表现出强烈暗示恶意意图的高危模式。研究进一步发现：包含可执行脚本的技能存在漏洞的概率是纯指令型技能的2.12倍（OR=2.12, p<0.001）。本研究的贡献包括：（1）基于8,126个漏洞技能构建的实证漏洞分类体系；（2）达到86.7%精确率与82.5%召回率的验证检测方法；（3）支持后续研究的开源数据集与检测工具包。这些结果表明，在该攻击向量被进一步利用前，亟需建立基于能力的权限控制系统和强制性的安全审查机制。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2601.10338) | [arXiv](https://arxiv.org/abs/2601.10338)



---

### 30. 从故事线中推导角色逻辑：基于编码决策树的方法

**原文标题：** Deriving Character Logic from Storyline as Codified Decision Trees

**摘要：**
角色扮演智能体依赖行为配置文件在不同叙事情境中保持行为一致性，但现有配置文件大多为非结构化、不可执行且验证薄弱，导致智能体行为脆弱。本文提出编码决策树（CDT）——一种数据驱动的框架，能够从大规模叙事数据中推导出可执行且可解释的决策结构。CDT将行为配置文件表示为条件规则树，其中内部节点对应已验证的场景条件，叶节点编码具体行为陈述，从而在执行时实现上下文适配规则的确定性检索。该决策树通过迭代推导候选场景-动作规则、基于数据进行验证，并通过层次化 specialization 进行优化而习得，最终生成的配置文件支持透明化检视与规范化更新。在涵盖16个叙事作品的85个角色的多项基准测试中，CDT显著优于人工编写的配置文件及先前的配置文件推导方法，表明经过编码与验证的行为表征能够为智能体提供更可靠的行为基础。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2601.10080) | [arXiv](https://arxiv.org/abs/2601.10080)



---

### 31. V-DPM：基于动态点图映射的四维视频重建

**原文标题：** V-DPM: 4D Video Reconstruction with Dynamic Point Maps

**摘要：**
强大的三维表示方法（如DUSt3R不变点图映射）通过编码三维形状与相机参数，显著推动了前馈式三维重建的发展。虽然点图映射通常假设静态场景，但动态点图映射通过额外表征场景运动，将这一概念扩展至动态三维内容。然而，现有动态点图映射方法仅限于图像对输入，且与DUSt3R类似，在涉及多于两个视角时仍需通过优化进行后处理。我们认为动态点图映射在视频应用中更具实用价值，并由此提出V-DPM以验证这一观点。首先，我们阐述了如何构建适用于视频输入的动态点图映射表示框架，该框架在最大化表征能力的同时，既便于神经网络的预测实现，又能有效复用预训练模型。其次，我们在近期性能强大的三维重建框架VGGT基础上实现了上述构想。尽管VGGT原训练于静态场景，但我们证明仅需适量合成数据即可将其适配为高效的V-DPM预测器。本方法在动态场景的三维与四维重建任务中达到了当前最优性能。特别需要指出的是，相较于VGGT的近期动态扩展方法（如P3），动态点图映射不仅能重建动态深度，还能完整恢复场景中每个点的三维运动轨迹。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2601.09499) | [arXiv](https://arxiv.org/abs/2601.09499)



---

### 32. RigMo：面向生成式动画的骨骼绑定与运动统一学习框架

**原文标题：** RigMo: Unifying Rig and Motion Learning for Generative Animation

**摘要：**
尽管四维生成领域已取得显著进展，但作为动画核心结构与动态组件的骨骼绑定与运动通常被建模为独立问题。现有流程依赖真实骨骼与蒙皮权重进行运动生成，并将自动骨骼绑定视为独立过程，这削弱了系统的可扩展性与可解释性。本文提出RigMo——一个统一的生成式框架，能够直接从原始网格序列中联合学习骨骼绑定与运动，无需任何人工标注的骨骼数据。RigMo将逐顶点形变编码至两个紧凑的潜在空间：骨骼潜在空间（解码为显式高斯骨骼与蒙皮权重）和运动潜在空间（生成时变SE(3)变换）。这些输出共同定义了具有显式结构与连贯运动的可动画网格，实现了可变形物体的前馈式骨骼与运动推断。除统一发现骨骼-运动关系外，我们进一步提出在RigMo潜在空间中运行的Motion-DiT模型，证明这种结构感知的潜在表示能自然支持下游运动生成任务。在DeformingThings4D、Objaverse-XL和TrueBones数据集上的实验表明，RigMo能够学习平滑、可解释且符合物理规律的骨骼系统，同时在重建效果与类别级泛化能力上优于现有自动骨骼绑定与形变基线方法。RigMo为统一、结构感知且可扩展的动态三维建模建立了新范式。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2601.06378) | [arXiv](https://arxiv.org/abs/2601.06378)



---

### 33. CaMeLs 亦能驾驭计算机：计算机使用代理的系统级安全

**原文标题：** CaMeLs Can Use Computers Too: System-level Security for Computer Use Agents

**摘要：**
人工智能代理易受提示注入攻击，恶意内容可通过劫持代理行为窃取凭证或造成经济损失。目前唯一已知的稳健防御方案是采用架构隔离机制，将可信任务规划与不可信环境观测严格分离。然而，将这一设计应用于计算机使用代理（CUAs）——即通过观察屏幕并执行操作实现任务自动化的系统——面临根本性挑战：现有代理需要持续观测用户界面状态以确定每个操作步骤，这与安全所需的隔离要求相冲突。我们通过论证用户界面工作流虽具动态性但结构可预测的特性，解决了这一矛盾。本文为计算机使用代理提出单次规划框架：可信规划器在观测任何潜在恶意内容之前，生成包含条件分支的完整执行图，从而针对任意指令注入攻击提供可验证的控制流完整性保障。尽管这种架构隔离能有效防御指令注入，但我们研究表明仍需额外措施来防范分支导向攻击——此类攻击通过操纵界面元素触发计划中非预期的有效路径。我们在OSWorld环境中评估该设计，在保持前沿模型高达57%性能的同时，将小型开源模型的性能提升达19%，证明计算机使用代理能够实现严格安全性与实用性的共存。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2601.09923) | [arXiv](https://arxiv.org/abs/2601.09923)



---

### 34. WildRayZer：动态环境中自监督的大视角合成

**原文标题：** WildRayZer: Self-supervised Large View Synthesis in Dynamic Environments

**摘要：**
本文提出WildRayZer，一种用于动态环境（相机与物体均发生运动）中新视角合成的自监督框架。动态内容会破坏静态新视角合成模型所依赖的多视角一致性，导致重影、几何结构失真及姿态估计不稳定等问题。WildRayZer通过执行分析-合成测试应对这一挑战：首先利用仅考虑相机运动的静态渲染器解析刚性结构，其残差则揭示瞬变区域。基于这些残差，我们构建伪运动掩码、蒸馏出运动估计器，并利用该估计器对输入特征进行掩码处理及损失梯度门控，从而使监督学习聚焦于跨视角背景补全任务。为支持大规模训练与评估，我们构建了Dynamic RealEstate10K（D-RE10K）数据集——包含1.5万条自然采集的动态序列的真实场景数据集，以及D-RE10K-iPhone配对数据集——专为稀疏视角瞬态感知新视角合成提供的瞬态-洁净双模态基准测试集。实验表明，WildRayZer在单次前向传播中，无论是瞬态区域消除还是全帧新视角合成质量方面，均持续优于基于优化的方法及前馈基线模型。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2601.10716) | [arXiv](https://arxiv.org/abs/2601.10716)



---

### 35. VQ-Seg：基于向量量化标记扰动的半监督医学图像分割方法

**原文标题：** VQ-Seg: Vector-Quantized Token Perturbation for Semi-Supervised Medical Image Segmentation

**摘要：**
基于特征扰动的一致性学习是半监督医学图像分割中广泛采用的策略。然而，现有扰动方法多依赖于随机丢弃操作，需人工精细调节丢弃率这一敏感超参数，其优化难度较高且易导致正则化效果欠佳。为克服此局限，本文提出VQ-Seg方法，首次引入向量量化技术对特征空间进行离散化处理，并设计了一种可替代随机丢弃的新型可控量化扰动模块。该模块通过重排码本索引的空间位置实现对离散表征的扰动，从而达成高效可控的正则化效果。为缓解量化可能造成的信息损失，我们设计了双分支架构，使图像重建与分割任务共享量化后的特征空间。此外，通过构建后量化特征适配器，引入基础模型的高层语义指导以补偿量化过程中的信息损失。本研究还构建了包含828例中央型肺癌标注CT影像的大规模肺癌数据集。在肺癌数据集及多个公开基准测试上的实验表明，本方法在性能上显著优于现有先进技术。代码已开源：https://github.com/script-Yang/VQ-Seg。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2601.10124) | [arXiv](https://arxiv.org/abs/2601.10124)



---

### 36. 通过先进提示工程技术增强大语言模型的情感分类与反讽检测能力

**原文标题：** Enhancing Sentiment Classification and Irony Detection in Large Language Models through Advanced Prompt Engineering Techniques

**摘要：**
本研究探讨如何运用提示工程技术优化大语言模型（特别是GPT-4o-mini与gemini-1.5-flash）在情感分析任务中的表现。通过对比基线方法，系统评估了小样本学习、思维链提示、自我一致性等先进提示技术的效果。核心任务涵盖情感分类、基于方面的情感分析，以及对反讽等微妙语义的检测。研究详细阐述了理论背景、数据集与实验方法，并以准确率、召回率、精确率和F1分数为指标评估模型性能。实验结果表明：先进提示技术能显著提升情感分析效果，其中小样本学习在GPT-4o-mini中表现最优，而思维链提示使gemini-1.5-flash的反讽检测性能提升达46%。研究发现，虽然先进提示技术整体提升模型性能，但小样本提示对GPT-4o-mini效果最佳，而思维链提示在gemini-1.5-flash的反讽检测中更具优势，这提示提示策略需根据具体模型与任务特性进行定制。本研究凸显了提示设计需同时兼顾大语言模型架构与任务语义复杂性的重要意义。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2601.08302) | [arXiv](https://arxiv.org/abs/2601.08302)



---

### 37. 揭秘注意力机制中的斜线模式：RoPE的角色解析

**原文标题：** Demystifying the Slash Pattern in Attention: The Role of RoPE

**摘要：**
大型语言模型（LLMs）常表现出斜线注意力模式，即注意力分数沿某个偏移量Δ的第Δ条次对角线集中分布。这些模式在跨词元信息传递中起着关键作用。但为何会出现这种模式？本文从实证与理论双重视角揭示了斜线主导注意力头（SDHs）的形成机制。首先，通过分析开源LLMs，我们发现SDHs是模型固有的特性，并能泛化至分布外提示。为解释其内在成因，我们分析了共同决定注意力分数的查询向量、键向量及旋转位置编码（RoPE）。实证研究表明SDHs具有两个特征条件：（1）查询向量与键向量近似为秩一矩阵；（2）RoPE由中高频分量主导。在此条件下，各词元的查询向量与键向量近乎相同，而RoPE中高频分量间的相互作用催生了SDHs。除实证证据外，我们通过理论建模将上述条件形式化为假设，证明其足以保证SDHs的涌现。特别地，我们分析了满足该条件的浅层Transformer模型在RoPE作用下的训练动态，并证明通过梯度下降训练的模型必然呈现SDHs特性，且该特性可泛化至分布外提示。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2601.08297) | [arXiv](https://arxiv.org/abs/2601.08297)



---

### 38. 面向大语言模型持续适应的记忆库压缩方法

**原文标题：** Memory Bank Compression for Continual Adaptation of Large Language Models

**摘要：**
大语言模型已成为众多日常应用的核心技术。然而，随着数据的动态演变，其知识体系会迅速过时。持续学习旨在通过新信息更新大语言模型，同时避免已获取知识的丢失。尽管全参数微调等方法能够整合新数据，但其计算成本高昂且易引发灾难性遗忘问题，即先前知识被覆盖。基于记忆增强的方法通过为大语言模型配备记忆库（一种存储信息以供未来调用的外部记忆模块）来解决此问题。然而，这些方法面临一个关键局限：在现实场景中，当大规模数据流持续涌入时，记忆库会不断膨胀。本文提出MBC模型，该模型通过在线适应学习过程中的码本优化策略实现记忆库压缩。为确保学习稳定性，我们还引入了在线重置机制以防止码本坍缩。此外，我们在LLM的注意力层采用键值低秩适应技术，以高效利用压缩后的记忆表征。基于基准问答数据集的实验表明，与最具竞争力的基线方法相比，MBC在保持在线适应学习过程中高记忆留存精度的同时，将记忆库规模压缩至原有尺寸的0.3%。代码已开源：https://github.com/Thomkat/MBC。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2601.00756) | [arXiv](https://arxiv.org/abs/2601.00756)



---


## 🔍 关键词云图
![关键词云图](../images/keywords_wordcloud.png)

## 📈 近期论文趋势
![论文趋势](../images/daily_papers.png)

## 🎙️ 语音播报
- [收听今日论文解读](../audio/2026-01-16_daily_papers.mp3)

## 📱 订阅渠道
- GitHub: [hf-daily-paper-newsletter-chinese](https://github.com/2404589803/hf-daily-paper-newsletter-chinese)