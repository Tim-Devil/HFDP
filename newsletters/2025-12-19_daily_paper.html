<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Hugging Face 论文日报 - 2025-12-19</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', sans-serif;
            line-height: 1.6;
            color: #333;
            background-color: #f5f5f5;
            padding: 20px;
        }
        
        .container {
            max-width: 900px;
            margin: 0 auto;
            background-color: white;
            padding: 40px;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }
        
        h1 {
            color: #2c3e50;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
            margin-bottom: 30px;
            font-size: 28px;
        }
        
        h1 img {
            vertical-align: middle;
            margin-right: 10px;
        }
        
        h2 {
            color: #34495e;
            margin-top: 40px;
            margin-bottom: 20px;
            font-size: 24px;
            border-left: 4px solid #3498db;
            padding-left: 15px;
        }
        
        h3 {
            color: #2c3e50;
            margin-top: 30px;
            margin-bottom: 15px;
            font-size: 20px;
        }
        
        p {
            margin-bottom: 15px;
            text-align: justify;
        }
        
        ul {
            margin-left: 20px;
            margin-bottom: 20px;
        }
        
        li {
            margin-bottom: 8px;
        }
        
        a {
            color: #3498db;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        strong {
            color: #2c3e50;
            font-weight: 600;
        }
        
        hr {
            border: none;
            border-top: 1px solid #e0e0e0;
            margin: 30px 0;
        }
        
        /* 关键修复:限制图片宽度 */
        img {
            max-width: 100%;
            height: auto;
            display: block;
            margin: 20px auto;
            border-radius: 4px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        
        /* 确保图片容器也有宽度限制 */
        p img {
            max-width: 100%;
        }
        
        /* 论文详情区域样式 */
        .paper-section {
            background-color: #f9f9f9;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 20px;
        }
        
        /* 统计信息样式 */
        .stats {
            background-color: #e8f4f8;
            padding: 15px;
            border-radius: 6px;
            margin-bottom: 20px;
        }
        
        /* 响应式设计 */
        @media (max-width: 768px) {
            .container {
                padding: 20px;
            }
            
            h1 {
                font-size: 24px;
            }
            
            h2 {
                font-size: 20px;
            }
            
            h3 {
                font-size: 18px;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <h1><img src="https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo.png" width="30"/> Hugging Face 2025-12-19 论文日报</h1>
<h2>📊 今日论文统计</h2>
<ul>
<li>总论文数：38</li>
<li>热门领域：Transformer, Vision, Diffusion, RL, LLM, GPT</li>
</ul>
<h2>📝 论文详情</h2>
<h3>1. Kling-Omni 技术报告</h3>
<p><strong>原文标题：</strong> Kling-Omni Technical Report</p>
<p><strong>摘要：</strong>
本文介绍 Kling-Omni，这是一个通用的生成式框架，旨在直接从多模态视觉语言输入中合成高保真度视频。Kling-Omni 采用端到端视角，弥合了多样化视频生成、编辑与智能推理任务之间的功能分离，将其整合为一个完整的系统。与割裂的流水线方法不同，Kling-Omni 支持包括文本指令、参考图像和视频上下文在内的多种用户输入，并将其处理为统一的多模态表示，以提供电影级画质且高度智能的视频内容创作。为支撑这些能力，我们构建了一个全面的数据系统，作为多模态视频创作的基础。该框架还通过高效的大规模预训练策略和针对推理的基础设施优化得到进一步增强。综合评估表明，Kling-Omni 在上下文生成、基于推理的编辑以及多模态指令跟随方面展现出卓越性能。我们相信，Kling-Omni 不仅是一个内容创作工具，更是迈向能够感知、推理、生成并与动态复杂世界交互的多模态世界模拟器的关键进展。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.16776">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.16776">arXiv</a></p>
<hr />
<h3>2. 智能体人工智能的适应性研究</h3>
<p><strong>原文标题：</strong> Adaptation of Agentic AI</p>
<p><strong>摘要：</strong>
前沿的智能体人工智能系统建立在基础模型之上，这些模型可通过调整以实现规划、推理及与外部工具交互，从而执行日益复杂和专门化的任务。随着此类系统在能力和范围上的扩展，适应性已成为提升性能、可靠性和泛化能力的核心机制。本文通过构建一个系统化框架，将快速扩展的研究领域统一为智能体适应性与工具适应性两大维度，并进一步将其分解为工具执行信号驱动型和智能体输出信号驱动型的智能体适应性，以及智能体无关型和智能体监督型的工具适应性。研究表明，该框架有助于厘清智能体人工智能中适应性策略的设计空间，明确其权衡关系，并为系统设计过程中策略的选择与切换提供实践指导。在此基础上，本文综述了各类别中的代表性方法，分析其优势与局限，并指出当前面临的关键开放挑战与未来机遇。总体而言，本文旨在为寻求构建更强大、高效和可靠的智能体人工智能系统的研究者与实践者提供概念基础和实践路线图。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.16301">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.16301">arXiv</a></p>
<hr />
<h3>3. LLaDA2.0：将扩散语言模型扩展至千亿参数规模</h3>
<p><strong>原文标题：</strong> LLaDA2.0: Scaling Up Diffusion Language Models to 100B</p>
<p><strong>摘要：</strong>
本文提出LLaDA2.0——一组通过自回归模型系统化转换构建、总参数量达千亿规模的离散扩散大语言模型，为前沿规模部署建立了新范式。该方法摒弃了成本高昂的从头训练，秉持知识继承、渐进适应与效率优先的设计原则，通过创新的三阶段块级加权序列扩散训练方案，将预训练的自回归模型无缝转换为扩散语言模型：该方案包含块扩散中逐步增加块大小的预热阶段、大规模全序列扩散的稳定阶段，以及回归紧凑块扩散的衰减阶段。结合监督微调与直接偏好优化的训练后对齐，我们获得了LLaDA2.0-mini（160亿参数）和LLaDA2.0-flash（千亿参数）两个经过指令调优的混合专家模型变体，专为实际部署优化。通过保持并行解码的优势，这些模型在前沿规模上实现了卓越的性能与效率。两个模型均已开源发布。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.15745">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.15745">arXiv</a></p>
<hr />
<h3>4. 下一嵌入预测构建强视觉学习器</h3>
<p><strong>原文标题：</strong> Next-Embedding Prediction Makes Strong Vision Learners</p>
<p><strong>摘要：</strong>
受生成式预训练在自然语言领域成功的启发，我们探究相同原理能否构建强大的自监督视觉学习器。不同于训练模型输出特征供下游任务使用，我们训练模型直接生成嵌入以执行预测任务。本研究探索了这种从学习表征到学习模型的范式转变。具体而言，模型通过因果掩码和梯度截断技术，学习基于历史图像块嵌入预测未来嵌入，我们将其称为“下一嵌入预测自回归”（NEPA）。实验证明，在ImageNet-1k数据集上仅以下一嵌入预测为学习目标预训练的简单Transformer模型即具有显著效果——无需像素重建、离散标记、对比损失或任务特定头设计。该方案保持了架构简洁性与可扩展性，无需引入额外设计复杂度。NEPA在多项任务中表现优异：基于ViT-B和ViT-L骨干网络微调后，在ImageNet-1K上分别达到83.8%和85.3%的Top-1准确率，并能有效迁移至ADE20K语义分割任务。我们相信基于嵌入的生成式预训练为视觉自监督学习提供了一种简洁、可扩展且可能模态无关的替代方案。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.16922">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.16922">arXiv</a></p>
<hr />
<h3>5. StereoPilot：基于生成先验学习统一高效立体转换方法</h3>
<p><strong>原文标题：</strong> StereoPilot: Learning Unified and Efficient Stereo Conversion via Generative Priors</p>
<p><strong>摘要：</strong>
随着立体显示设备（如VR头显和3D影院）的快速发展，市场对高质量立体视频内容的需求日益增长。然而，3D视频制作成本高昂且流程复杂，而自动化的单目到立体转换技术受限于多阶段“深度-形变-修复”（DWI）流程的固有缺陷。该范式存在误差传递、深度歧义以及平行与汇聚立体格式不一致等问题。为解决这些挑战，本文首次构建了UniStereo——一个覆盖两种立体格式的大规模统一立体视频转换数据集，以支持公平基准测试与鲁棒的模型训练。基于此数据集，我们提出StereoPilot模型，该高效前馈模型无需依赖显式深度图或迭代扩散采样，可直接合成目标视角视图。通过可学习的域切换模块与循环一致性损失函数，StereoPilot能够自适应不同立体格式并提升视觉一致性。大量实验表明，StereoPilot在视觉保真度与计算效率方面均显著优于现有先进方法。项目页面：https://hit-perfect.github.io/StereoPilot/。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.16915">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.16915">arXiv</a></p>
<hr />
<h3>6. Seedance 1.5 pro：原生音视频联合生成基础模型</h3>
<p><strong>原文标题：</strong> Seedance 1.5 pro: A Native Audio-Visual Joint Generation Foundation Model</p>
<p><strong>摘要：</strong>
近期视频生成领域的进展为统一的音视频生成奠定了基础。本研究提出了Seedance 1.5 pro——一个专为原生音视频联合生成设计的基础模型。该模型采用双分支扩散Transformer架构，通过跨模态联合模块与专门设计的多阶段数据流程相结合，实现了卓越的音画同步效果与生成质量。为提升实用价值，我们实施了精细的后训练优化策略，包括基于高质量数据集的有监督微调，以及结合多维度奖励模型的人类反馈强化学习。此外，我们引入了加速推理框架，将生成速度提升超过10倍。Seedance 1.5 pro在多语言/方言口型同步、动态电影级镜头控制及叙事连贯性增强方面表现突出，使其成为专业级内容创作的强大引擎。该模型现已通过火山引擎平台开放使用，访问地址：https://console.volcengine.com/ark/region:ark+cn-beijing/experience/vision?type=GenVideo。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.13507">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.13507">arXiv</a></p>
<hr />
<h3>7. 深度全景：全景深度估计的基础模型</h3>
<p><strong>原文标题：</strong> Depth Any Panoramas: A Foundation Model for Panoramic Depth Estimation</p>
<p><strong>摘要：</strong>
本研究提出了一种适用于多样化场景距离的全景度量深度基础模型。我们从数据构建与框架设计的双重维度探索了数据闭环范式。通过整合公开数据集、基于UE5模拟器的高质量合成数据、文本到图像生成模型以及网络采集的真实全景图像，我们构建了大规模训练数据集。为缩减室内/室外与合成/真实数据间的域差异，我们设计了包含三阶段的伪标签优化流程，为未标注图像生成可靠真值。模型架构方面，采用具有强预训练泛化能力的DINOv3-Large作为主干网络，并创新性地引入即插即用式距离掩码头模块、以清晰度为核心的优化策略以及以几何一致性为导向的优化方法，从而提升模型对多尺度距离的鲁棒性并强化跨视角几何一致性。在多个基准数据集（如Stanford2D3D、Matterport3D和Deep360）上的实验表明，该模型在保持优异性能与零样本泛化能力的同时，能够在复杂现实场景中实现鲁棒且稳定的度量深度预测。项目页面详见：https://insta360-research-team.github.io/DAP_website/</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.16913">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.16913">arXiv</a></p>
<hr />
<h3>8. 生成式重聚焦：基于单张图像的灵活散焦控制</h3>
<p><strong>原文标题：</strong> Generative Refocusing: Flexible Defocus Control from a Single Image</p>
<p><strong>摘要：</strong>
景深控制在摄影中至关重要，但获得完美对焦通常需要多次尝试或特殊设备。单图像重聚焦技术仍面临挑战，其涉及恢复清晰内容并生成逼真的焦外虚化效果。现有方法存在明显局限：需要全对焦输入图像、依赖仿真器生成的合成数据，且对光圈的控制能力有限。本文提出生成式重聚焦技术，采用包含去模糊网络与虚化网络的两阶段流程，前者可从多样化输入中恢复全对焦图像，后者用于生成可控虚化效果。本研究的核心创新在于半监督训练方法：通过结合合成配对数据与未配对真实虚化图像，并利用EXIF元数据捕捉仿真器无法提供的真实光学特性。实验表明，我们的方法在散焦去模糊、虚化合成和重聚焦基准测试中均达到最优性能。此外，所提出的生成式重聚焦系统支持文本引导的参数调整与自定义光圈形状控制。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.16923">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.16923">arXiv</a></p>
<hr />
<h3>9. DeContext 防御法：扩散变换器中的安全图像编辑</h3>
<p><strong>原文标题：</strong> DeContext as Defense: Safe Image Editing in Diffusion Transformers</p>
<p><strong>摘要：</strong>
上下文扩散模型使用户能够以极高的便捷性和真实感修改图像。然而，这种能力也引发了严重的隐私担忧：个人图像可能被轻易用于身份冒充、虚假信息传播或其他恶意用途，且均未经所有者同意。尽管已有研究探索通过输入扰动来防范个性化文生图模型中的滥用行为，但现代大规模基于上下文扩散变换器（DiT）模型的鲁棒性仍缺乏深入检验。本文提出DeContext方法，旨在保护输入图像免遭未经授权的上下文编辑。我们的核心发现是：源图像的上下文信息主要通过多模态注意力层传播至输出结果。通过注入微小且有针对性的扰动以削弱这些跨注意力路径，DeContext能够阻断信息流，有效解耦输入与输出之间的关联。这种简洁的防御机制兼具高效性与鲁棒性。我们进一步证明，早期去噪步骤和特定变换器模块主导着上下文传播过程，这使得我们可以将扰动集中在最关键的位置。在Flux Kontext和Step1X-Edit数据集上的实验表明，DeContext能持续阻断非预期的图像编辑，同时保持视觉质量。这些结果凸显了基于注意力机制的扰动策略作为抵御图像篡改的有效防御手段。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.16625">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.16625">arXiv</a></p>
<hr />
<h3>10. REGLUE：融合全局与局部语义的隐变量纠缠扩散方法</h3>
<p><strong>原文标题：</strong> REGLUE Your Latents with Global and Local Semantics for Entangled Diffusion</p>
<p><strong>摘要：</strong>
隐扩散模型（LDMs）在图像合成领域取得了最先进的性能，但其重建式去噪目标仅提供间接的语义监督：高级语义特征形成缓慢，导致训练周期延长并限制生成质量。近期研究尝试通过视觉基础模型（VFMs）注入语义信息，或采用外部表征对齐方式，或仅在扩散过程中内部联合建模有限的VFM特征层，未能充分利用VFM中丰富、非线性、多层级的空间语义特征。本文提出REGLUE（全局-局部统一编码的表征纠缠框架），该统一隐扩散框架在单个SiT主干网络中联合建模：（i）VAE图像隐变量，（ii）紧凑的局部（图像块级）VFM语义，以及（iii）全局（图像级）[CLS]标记。通过轻量级卷积语义压缩器对多层VFM特征进行非线性聚合，生成低维且具有空间结构的表征，该表征在扩散过程中与VAE隐变量形成纠缠。外部对齐损失进一步约束内部表征向冻结的VFM目标对齐。在ImageNet 256×256数据集上，REGLUE相较于SiT-B/2和SiT-XL/2基线模型，以及REPA、ReDi和REG方法，在FID指标上持续提升并加速收敛。大量实验表明：（a）空间VFM语义至关重要，（b）非线性压缩是充分发挥其效益的关键，（c）全局标记与外部对齐在本框架的全局-局部-隐变量联合建模中起到互补的轻量化增强作用。代码已开源：https://github.com/giorgospets/reglue。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.16636">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.16636">arXiv</a></p>
<hr />
<h3>11. Alchemist：基于元梯度的数据选择提升文本到图像模型训练效率</h3>
<p><strong>原文标题：</strong> Alchemist: Unlocking Efficiency in Text-to-Image Model Training via Meta-Gradient Data Selection</p>
<p><strong>摘要：</strong>
文本到图像（T2I）生成模型（如Imagen、Stable Diffusion和FLUX）的最新进展显著提升了视觉生成质量。然而，其性能从根本上受限于训练数据的质量。网络爬取和合成图像数据集常包含低质量或冗余样本，导致视觉保真度下降、训练过程不稳定以及计算效率低下。因此，有效的数据选择对于提升数据效率至关重要。现有方法依赖于成本高昂的人工筛选或基于文本到图像数据过滤中单一维度特征的启发式评分。尽管基于元学习的方法已在大型语言模型中得到探索，但尚未适用于图像模态。为此，我们提出<strong>Alchemist</strong>，一个基于元梯度的框架，用于从大规模文本-图像数据对中选择合适的子集。该方法通过以数据为中心的视角迭代优化模型，自动学习评估每个样本的影响。Alchemist包含两个关键阶段：数据评级与数据剪枝。我们训练一个轻量级评级器，基于梯度信息并辅以多粒度感知来估计每个样本的影响，随后采用Shift-G采样策略选择信息丰富的子集以进行高效模型训练。Alchemist是首个面向文本到图像模型训练的自动化、可扩展、基于元梯度的数据选择框架。在合成与网络爬取数据集上的实验表明，Alchemist能持续提升视觉质量与下游任务性能。使用Alchemist选择的50%数据训练，其效果可优于使用完整数据集训练的结果。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.16905">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.16905">arXiv</a></p>
<hr />
<h3>12. 世界即画布：融合参考图像、轨迹与文本的可提示事件绘制</h3>
<p><strong>原文标题：</strong> The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text</p>
<p><strong>摘要：</strong>
本文提出WorldCanvas框架，该框架通过融合文本、轨迹与参考图像，构建了支持丰富用户导向模拟的可提示世界事件系统。相较于纯文本方法及现有轨迹控制的图像转视频技术，我们的多模态方法将编码运动、时序与可见性的轨迹、表达语义意图的自然语言，以及确立物体视觉特征的参考图像相结合，从而能够生成包含多智能体交互、物体进出场、参考图像引导的外观呈现及反直觉事件在内的连贯可控事件。生成的视频不仅具备时序连贯性，更展现出涌现一致性——即使物体暂时消失，其身份特征与场景信息仍得以保持。通过支持富有表现力的世界事件生成，WorldCanvas推动了世界模型从被动预测器向用户可交互式模拟器的演进。项目页面详见：https://worldcanvas.github.io/。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.16924">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.16924">arXiv</a></p>
<hr />
<h3>13. N3D-VLM：原生三维感知赋能视觉语言模型实现精确空间推理</h3>
<p><strong>原文标题：</strong> N3D-VLM: Native 3D Grounding Enables Accurate Spatial Reasoning in Vision-Language Models</p>
<p><strong>摘要：</strong>
当前的多模态模型虽能基于二维图像回答问题，但缺乏内在的三维物体感知能力，限制了其理解三维场景中空间关系与深度信息的能力。本研究提出N3D-VLM——一种将原生三维物体感知与三维视觉推理无缝整合的新型统一框架，既能实现精确的三维定位，又能获得可解释的空间理解。与传统端到端模型直接从RGB/RGB-D输入预测答案不同，我们的方法赋予模型原生三维物体感知能力，使其能够依据文本描述直接在三维空间中定位物体。在实现精确三维物体定位的基础上，模型进一步在三维空间中进行显式推理，从而获得更具可解释性和结构化的空间理解。为支撑这些能力的鲁棒性训练，我们开发了可扩展的数据构建流程：通过深度估计技术将大规模二维标注提升至三维空间，显著增强了三维物体定位数据的多样性与覆盖范围，生成的数据集规模达到现有最大单图像三维检测数据集的六倍以上。该流程还能生成针对三维思维链推理的空间问答数据集，促进三维物体定位与三维空间推理的联合训练。实验结果表明，我们的统一框架不仅在三维定位任务上达到最先进性能，在视觉语言模型的三维空间推理任务中也持续超越现有方法。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.16561">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.16561">arXiv</a></p>
<hr />
<h3>14. JustRL：采用简易强化学习方案扩展15亿参数大语言模型</h3>
<p><strong>原文标题：</strong> JustRL: Scaling a 1.5B LLM with a Simple RL Recipe</p>
<p><strong>摘要：</strong>
近期大语言模型强化学习领域的发展呈现出日益复杂的趋势：多阶段训练流程、动态超参数调度以及课程学习策略。这引发了一个根本性问题：此类复杂性是否必要？本文提出JustRL——一种采用固定超参数的单阶段训练极简方案，在两个15亿参数推理模型上实现了最先进性能（在九项数学基准测试中平均准确率分别达到54.9%和64.3%），同时计算消耗比复杂方案减少2倍。相同超参数可在两个模型间直接迁移而无需调整，且历经4000余步训练仍保持平滑单调的改进趋势，未出现通常需要干预的崩溃或平台期。关键的是，消融实验表明，添加显式长度惩罚和鲁棒验证器等"标准技巧"可能因压缩探索空间而导致性能下降。这些结果表明，当前领域可能正在通过增加复杂性来解决本可通过稳定、规模化基线自然消解的问题。我们公开模型与代码，旨在为学界建立一个经过验证的简易基线。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.16649">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.16649">arXiv</a></p>
<hr />
<h3>15. AdaTooler-V：面向图像与视频的自适应工具使用</h3>
<p><strong>原文标题：</strong> AdaTooler-V: Adaptive Tool-Use for Images and Videos</p>
<p><strong>摘要：</strong>
近期研究表明，多模态大语言模型（MLLMs）能够通过结合视觉工具交互的多模态交错思维链（CoT）获得性能提升。然而，现有开源模型常表现出盲目的工具使用推理模式，即使在无需工具时也会调用视觉工具，这显著增加了推理开销并降低了模型性能。为此，我们提出AdaTooler-V，一种能够通过判断视觉问题是否真正需要工具来实现自适应工具使用的MLLM。首先，我们引入AT-GRPO强化学习算法，该算法根据每个样本的“工具效益评分”自适应调整奖励尺度，鼓励模型仅在工具能带来实质改进时才调用它们。此外，我们构建了两个支持训练的数据集：用于监督微调冷启动的AdaTooler-V-CoT-100k，以及用于覆盖单图像、多图像和视频数据的可验证奖励强化学习的AdaTooler-V-300k。在十二个基准测试上的实验表明，AdaTooler-V具备强大的推理能力，在多样化的视觉推理任务中优于现有方法。值得注意的是，AdaTooler-V-7B在高分辨率基准V*上达到了89.8%的准确率，超越了商业闭源模型GPT-4o和Gemini 1.5 Pro。所有代码、模型和数据均已开源发布。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.16918">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.16918">arXiv</a></p>
<hr />
<h3>16. 探索与利用之辨：通过剪裁、熵与伪奖励重思可验证奖励强化学习</h3>
<p><strong>原文标题：</strong> Exploration v.s. Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward</p>
<p><strong>摘要：</strong>
本文研究了可验证奖励强化学习框架中的探索-利用权衡问题，该框架旨在提升大语言模型的推理能力。近期研究表明，RLVR可通过两种看似矛盾的机制激发大语言模型的数学推理能力：伪奖励机制通过奖励与真实答案无关的结果来抑制模型过度利用既有知识，而熵最小化机制则通过推动模型产生更确定性的输出来抑制探索行为。这揭示了一个令人困惑的动态现象：抑制利用与抑制探索均能提升推理性能，但协调这两种效应的内在原理尚不明确。本研究聚焦两个核心问题：（1）策略熵如何影响模型性能；（2）伪奖励是否通过剪裁偏差与模型污染之间的相互作用产生增益。实验结果表明，伪奖励下的剪裁偏差会降低策略熵，促使模型产生更确定性的输出，而单纯的熵最小化并不足以带来性能提升。我们进一步提出奖励错配模型，阐释了伪奖励为何能在非污染场景下提升模型性能。本研究阐明了伪奖励产生增益的内在机制，并为更有效的RLVR训练提供了理论依据。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.16912">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.16912">arXiv</a></p>
<hr />
<h3>17. 多模态奖励基准2：评估交错文本与图像的全能奖励模型</h3>
<p><strong>原文标题：</strong> Multimodal RewardBench 2: Evaluating Omni Reward Models for Interleaved Text and Image</p>
<p><strong>摘要：</strong>
奖励模型对于训练大语言模型至关重要，但在处理交错图像与文本序列的全能模型领域仍研究不足。本文提出多模态奖励基准2，这是首个针对多模态理解与（交错）生成任务的奖励模型综合基准。该基准涵盖四大任务：文本到图像生成、图像编辑、交错内容生成以及多模态推理（“图像思维”），每个任务提供来自23个模型与智能体在21个源任务中产生的1000组专家标注偏好对。MMRB2的设计具备三大特点：（1）实用且具有挑战性的提示；（2）汇集前沿模型与智能体的响应；（3）通过集成过滤策略构建具有强专家共识的偏好对。基于MMRB2，我们系统评估了各子任务的现有评判器，包括多模态大语言模型作为评判器以及经人类偏好训练的模型。最新Gemini 3 Pro模型达到75-80%的准确率，GPT-5与Gemini 2.5 Pro达到66-75%的准确率（人类专家准确率＞90%），但仍显著超越广泛使用的GPT-4o（59%）。性能最佳的开源模型Qwen3-VL-32B达到与Gemini 2.5 Flash相当的准确率（64%）。研究进一步表明，通过N选一采样策略，MMRB2评估结果与下游任务表现呈强相关性，深入分析揭示了奖励模型未来改进的关键方向。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.16899">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.16899">arXiv</a></p>
<hr />
<h3>18. EasyV2V：一种基于指令的高质量视频编辑框架</h3>
<p><strong>原文标题：</strong> EasyV2V: A High-quality Instruction-based Video Editing Framework</p>
<p><strong>摘要：</strong>
尽管图像编辑技术发展迅速，视频编辑领域仍相对缺乏探索，在一致性、控制能力和泛化性方面面临挑战。本研究系统探讨了数据、架构与控制机制的设计空间，并提出了EasyV2V——一个简洁高效的基于指令的视频编辑框架。在数据层面，我们通过快速逆变换整合现有专家模型构建多样化视频对，借助单帧监督与共享仿射运动的伪配对将图像编辑对提升至视频维度，挖掘密集标注视频片段生成视频训练对，并引入过渡监督以指导编辑过程的动态演化。在模型架构方面，我们发现预训练的文本到视频模型本身具备编辑潜力，从而启发了简化设计思路：仅需通过序列拼接进行条件控制并配合轻量级LoRA微调，即可训练出高性能模型。在控制机制上，我们通过统一的单掩码机制实现时空协同控制，并支持可选参考图像输入。总体而言，EasyV2V支持灵活输入组合（如视频+文本、视频+掩码+文本、视频+掩码+参考图像+文本），在视频编辑效果上达到当前最优水平，超越了同期学术成果与商业系统。项目主页：https://snap-research.github.io/easyv2v/</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.16920">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.16920">arXiv</a></p>
<hr />
<h3>19. FlashPortrait：基于自适应潜在预测实现6倍加速的无限时长肖像动画生成</h3>
<p><strong>原文标题：</strong> FlashPortrait: 6x Faster Infinite Portrait Animation with Adaptive Latent Prediction</p>
<p><strong>摘要：</strong>
当前基于扩散模型的长时长肖像动画加速方法难以确保身份（ID）一致性。本文提出FlashPortrait，一种端到端的视频扩散Transformer模型，能够合成保持身份一致性的无限时长视频，并在推理速度上实现高达6倍的加速。具体而言，FlashPortrait首先利用现成的特征提取器计算与身份无关的面部表情特征。随后，通过引入归一化面部表情模块，将面部特征与扩散潜在变量对齐，即利用各自的均值与方差进行归一化处理，从而提升面部建模中的身份稳定性。在推理阶段，FlashPortrait采用动态滑动窗口策略，并在重叠区域进行加权融合，以确保长动画中的平滑过渡与身份一致性。在每个上下文窗口中，基于特定时间步的潜在变量变化率以及扩散层间的导数幅值比，FlashPortrait利用当前时间步的高阶潜在导数直接预测未来时间步的潜在变量，从而跳过多个去噪步骤，实现6倍速度加速。基准测试实验表明，FlashPortrait在定性与定量评估中均表现出显著有效性。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.16900">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.16900">arXiv</a></p>
<hr />
<h3>20. RePlan：基于推理引导的区域规划用于复杂指令图像编辑</h3>
<p><strong>原文标题：</strong> RePlan: Reasoning-guided Region Planning for Complex Instruction-based Image Editing</p>
<p><strong>摘要：</strong>
基于指令的图像编辑技术实现了对视觉修改的自然语言控制，但现有模型在指令-视觉复杂性（IV-Complexity）场景下表现不佳——即当复杂指令遇到杂乱或模糊的图像内容时。本文提出RePlan（区域对齐规划），一种“先规划后执行”的框架，通过结合视觉语言规划器与扩散编辑模型实现编辑任务。规划器通过逐步推理分解指令，并将其显式定位至目标区域；编辑模型随后采用无需训练的注意力区域注入机制实施修改，从而实现无需迭代修复的精准、并行多区域编辑。为增强规划能力，我们基于GRPO强化学习方法，仅使用1,000条纯指令样本进行训练，显著提升了推理准确性与格式可靠性。我们还提出了IV-Edit基准数据集，专注于细粒度区域定位与知识密集型编辑任务。在多种IV-Complex场景下的实验表明，RePlan在区域精度与整体保真度上均优于使用更大规模数据集训练的基线模型，展现出优越性能。项目页面：https://replan-iv-edit.github.io</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.16864">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.16864">arXiv</a></p>
<hr />
<h3>21. VenusBench-GD：面向多样化落地任务的多平台综合图形用户界面基准测试</h3>
<p><strong>原文标题：</strong> VenusBench-GD: A Comprehensive Multi-Platform GUI Benchmark for Diverse Grounding Tasks</p>
<p><strong>摘要：</strong>
图形用户界面（GUI）落地是构建高效能GUI智能体的关键组成部分。然而，现有的落地基准测试存在显著局限性：它们要么数据量不足且领域覆盖狭窄，要么过度聚焦单一平台并需要高度专业化的领域知识。本研究提出VenusBench-GD——一个跨越多平台的双语GUI落地综合基准测试，支持面向实际应用场景的层次化评估。本基准测试的贡献包括：（一）构建了大规模跨平台基准数据集，涵盖广泛的应用类型、多样化的UI元素及丰富的标注数据；（二）建立了针对落地任务的高质量数据构建流程，其标注精度超越现有基准测试；（三）通过提出层次化任务分类体系拓展了元素落地的评估维度，将落地任务划分为基础与高级两大类别，涵盖六个设计互补的子任务以多角度评估模型性能。实验结果表明：通用多模态模型在基础落地任务上已媲美甚至超越专用GUI模型，而高级任务仍更适配GUI专用模型，但后者存在明显过拟合与鲁棒性不足的问题。这些发现凸显了建立全面多层次评估框架的必要性。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.16501">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.16501">arXiv</a></p>
<hr />
<h3>22. ModelTables：关于模型的表格语料库</h3>
<p><strong>原文标题：</strong> ModelTables: A Corpus of Tables about Models</p>
<p><strong>摘要：</strong>
本文提出ModelTables，这是一个针对模型湖中表格的基准数据集，旨在捕捉性能与配置表格的结构化语义信息，这些信息在纯文本检索中常被忽视。该语料库基于Hugging Face模型卡片、GitHub README文档及相关论文构建，将每个表格与其对应的模型及出版背景进行关联。与开放数据湖中的表格相比，模型表格规模较小但表现出更密集的表格间关联，反映了模型与基准测试紧密耦合的演进过程。当前版本涵盖超过6万个模型和9万个表格。为评估模型与表格的相关性，我们通过三种互补信号构建了多源真实标注数据：（1）论文引用关系，（2）显式的模型卡片链接与继承关系，（3）共享的训练数据集。我们以表格搜索为例展示了该基准数据集的一个综合性实证应用案例，比较了经典数据湖搜索操作（可合并、可连接、关键词搜索）与信息检索基线方法（稠密检索、稀疏检索、混合检索）在该基准上的表现。基于合并操作的语义表格检索整体P@1达到54.8%（引用关系54.6%，继承关系31.3%，共享数据集30.6%）；基于表格的稠密检索达到66.5% P@1；元数据混合检索达到54.1%。评估结果表明现有表格搜索方法仍有明显改进空间。通过发布ModelTables及其构建流程，我们首次提供了描述人工智能模型的大规模结构化数据基准。我们在模型湖中开展的表格发现应用案例，为开发更精确的语义检索、结构化比较及结构化模型知识的系统化组织提供了理论依据与实践证明。相关源代码、数据及其他材料已公开于https://github.com/RJMillerLab/ModelTables。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.16106">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.16106">arXiv</a></p>
<hr />
<h3>23. 听觉翻译：语音模态集成于大语言模型的有效性研究</h3>
<p><strong>原文标题：</strong> Hearing to Translate: The Effectiveness of Speech Modality Integration into LLMs</p>
<p><strong>摘要：</strong>
随着大语言模型（LLMs）的应用范围超越纯文本领域，将语音作为原生模态进行集成催生了语音大语言模型（SpeechLLMs），其旨在直接翻译口语，从而绕过传统的基于转写的处理流程。然而，这种集成是否比成熟的级联架构更能提升语音到文本的翻译质量，仍是一个悬而未决的问题。本研究提出“听觉翻译”评估框架，首次构建了全面的测试集，将5个前沿的SpeechLLMs与16个强大的直接及级联系统（结合领先的语音基础模型与多语言大语言模型）进行严格基准比较。我们的分析涵盖16个基准数据集、13种语言对以及9种具有挑战性的条件（包括不流畅语音、含噪语音和长篇幅语音）。在这项广泛评估中，我们发现级联系统整体上仍是最可靠的方案，而当前的SpeechLLMs仅在特定场景下与级联系统表现相当，语音基础模型则落后于两者。这突出表明，无论是通过模型内部集成还是构建处理流程，融入大语言模型对于实现高质量的语音翻译至关重要。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.16378">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.16378">arXiv</a></p>
<hr />
<h3>24. 关键差异：面向能力差距发现与修正的模型审计方法</h3>
<p><strong>原文标题：</strong> Differences That Matter: Auditing Models for Capability Gap Discovery and Rectification</p>
<p><strong>摘要：</strong>
传统多模态大语言模型（MLLMs）评估方法缺乏可解释性，往往难以充分揭示模型间显著的能力差距。为此，我们提出AuditDM——一种通过主动审计模型分歧来发现并修正MLLM失效模式的自动化框架。该框架通过强化学习微调MLLM作为审计器，使其生成能最大化目标模型间分歧的挑战性问题和反事实图像。训练完成后，审计器可挖掘出多样化的可解释示例，这些示例既能揭示模型缺陷，又可作为无需标注的修正数据。在Gemma-3和PaliGemma-2等前沿模型上的实验表明，AuditDM成功识别出20余种不同的失效类型。基于这些发现进行微调后，所有模型在16个基准测试中均获得持续提升，甚至使30亿参数模型超越其280亿参数的对照模型。研究结果表明，当数据扩展效益递减时，定向模型审计为模型诊断与优化提供了有效路径。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.16921">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.16921">arXiv</a></p>
<hr />
<h3>25. Insight Miner：面向跨领域自然语言对齐的时间序列分析数据集</h3>
<p><strong>原文标题：</strong> Insight Miner: A Time Series Analysis Dataset for Cross-Domain Alignment with Natural Language</p>
<p><strong>摘要：</strong>
时间序列数据在环境分析、农业、交通和金融等诸多科学与工业领域中至关重要。然而，从这类数据中挖掘洞见通常需要深厚的领域专业知识，这一过程既耗时又费力。本文提出Insight Miner，一种旨在生成高质量、综合性时间序列描述的大规模多模态模型，其描述内容融合了领域专业知识。为支持该模型，我们推出了TS-Insights（可通过\href{https://huggingface.co/datasets/zhykoties/time-series-language-alignment}{https://huggingface.co/datasets/zhykoties/time-series-language-alignment}获取），这是首个面向时间序列与语言对齐的通用领域数据集。TS-Insights包含从20个预测数据集中采样的10万个时间序列窗口，其构建采用了一种新型智能体工作流程：先通过统计工具从原始时间序列中提取特征，再使用GPT-4将其合成为连贯的趋势描述。基于TS-Insights进行指令微调后，Insight Miner在生成时间序列描述与洞见方面超越了LLaVA（liu2023llava）和GPT-4等先进多模态模型。我们的研究结果为利用多模态模型进行时间序列分析指明了可行方向，并为使大语言模型将时间序列作为原生输入模态进行解读奠定了重要基础。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.11251">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.11251">arXiv</a></p>
<hr />
<h3>26. Make-It-Poseable：面向三维类人角色动画的前馈式潜在姿态生成模型</h3>
<p><strong>原文标题：</strong> Make-It-Poseable: Feed-forward Latent Posing Model for 3D Humanoid Character Animation</p>
<p><strong>摘要：</strong>
三维角色姿态生成是计算机图形学与视觉领域的一项基础任务。然而，现有方法如自动绑定与姿态条件生成常面临蒙皮权重预测不准确、拓扑结构缺陷及姿态贴合度差等挑战，限制了其鲁棒性与泛化能力。为克服这些局限，本文提出Make-It-Poseable——一种创新的前馈式框架，将角色姿态生成重新定义为潜在空间变换问题。与传统流程中直接变形网格顶点不同，本方法通过操纵角色的潜在表征直接重建新姿态下的角色模型。其核心是一个基于骨骼运动操纵形状标记的潜在姿态变换器，该过程通过密集姿态表征实现精确控制。为确保高保真几何质量并适应拓扑结构变化，我们还引入了潜在空间监督策略与自适应补全模块。实验表明，本方法在姿态生成质量上具有显著优势，并能自然扩展到部件替换与精细化等三维编辑任务中。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.16767">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.16767">arXiv</a></p>
<hr />
<h3>27. FrameDiffuser：基于G-Buffer条件扩散的神经前向帧渲染</h3>
<p><strong>原文标题：</strong> FrameDiffuser: G-Buffer-Conditioned Diffusion for Neural Forward Frame Rendering</p>
<p><strong>摘要：</strong>
面向交互应用的神经渲染需要逐帧将几何与材质属性（G-Buffer）转换为具有真实光照效果的逼真图像。尽管近期基于扩散模型的方法在G-Buffer条件图像合成中展现出潜力，但仍面临关键局限：如RGBX等单帧模型独立生成各帧而缺乏时序一致性；而DiffusionRenderer等视频模型计算开销过大，难以适配多数消费级游戏设备，且需预先获取完整序列，无法适应未来帧依赖用户输入的交互场景。本文提出FrameDiffuser——一种自回归神经渲染框架，通过融合G-Buffer数据与模型自身历史输出来生成时序一致、视觉逼真的帧序列。在生成首帧后，FrameDiffuser仅依赖输入的G-Buffer数据（包含几何结构、材质与表面属性），同时利用已生成的前一帧进行时序引导，从而在数百至数千帧范围内保持稳定且时序一致的生成效果。我们的双条件架构结合了ControlNet的结构引导与ControlLoRA的时序连贯性增强机制，并通过三阶段训练策略实现稳定的自回归生成。该模型针对特定场景环境进行专门化训练，在保持广泛泛化能力的同时，优先保障时序一致性与推理速度。实验表明，相较于通用化方法，场景专用训练能在光照、阴影与反射等视觉要素上实现更优越的逼真度与物理准确性。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.16670">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.16670">arXiv</a></p>
<hr />
<h3>28. 可训练的对数线性稀疏注意力机制用于高效扩散变换器</h3>
<p><strong>原文标题：</strong> Trainable Log-linear Sparse Attention for Efficient Diffusion Transformers</p>
<p><strong>摘要：</strong>
扩散变换器（DiTs）在视觉生成领域取得了最先进的性能，但其二次方的自注意力计算成本从根本上限制了其向长令牌序列的扩展。近期提出的Top-K稀疏注意力方法通过将令牌压缩为块级表示并选择少量相关关键块来减少DiTs的计算量，但仍存在以下问题：（1）在压缩令牌上仍需二次方选择成本；（2）随着序列增长，为保持模型质量需要不断增加K值。我们发现其效率低下的根源在于单层级设计，因为单一粗粒度层级不足以有效表征全局结构。本文提出对数线性稀疏注意力（LLSA），这是一种针对极长令牌序列的可训练稀疏注意力机制，通过利用层次化结构将选择成本和注意力成本从二次方降低至对数线性复杂度。LLSA执行层次化Top-K选择，基于前一层级发现的索引逐步采用稀疏Top-K选择，并引入层次化键值增强机制，在注意力计算过程中使用更少的不同粒度令牌同时保持全局上下文信息。为支持高效训练，我们开发了高性能GPU实现方案，在前向和反向传播中仅使用稀疏索引，无需构建稠密注意力掩码。我们在不使用分块化和VAE编码的高分辨率像素空间图像生成任务上评估LLSA。在256×256像素令牌序列上，LLSA将注意力推理速度提升28.27倍，将DiT训练速度提升6.09倍，同时保持生成质量。实验结果表明，LLSA为高效训练长序列DiTs提供了有前景的技术路径。代码已开源：https://github.com/SingleZombie/LLSA</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.16615">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.16615">arXiv</a></p>
<hr />
<h3>29. 面向语言模型通用推理的耦合变分强化学习方法</h3>
<p><strong>原文标题：</strong> Coupled Variational Reinforcement Learning for Language Model General Reasoning</p>
<p><strong>摘要：</strong>
尽管强化学习在语言模型推理领域取得了显著进展，但其发展仍受限于可验证奖励信号的需求。近期出现的免验证器强化学习方法通过利用大语言模型生成参考答案的内在概率作为奖励信号，缓解了这一限制。然而，这些方法通常仅基于问题条件对推理轨迹进行采样。这种设计导致推理轨迹采样与答案信息解耦，从而引发低效探索以及轨迹与最终答案间的不一致问题。本文提出耦合变分强化学习方法，该方法通过混合采样策略耦合先验分布与后验分布，构建了变分推断与强化学习之间的桥梁。通过构建并优化融合这两种分布的复合分布，该方法在保持强思维-答案一致性的同时实现了高效探索。在数学推理与通用推理基准测试上的大量实验表明，该方法相比基线模型性能提升12.4%，较当前先进的免验证器强化学习基线方法额外提升2.3%，为增强语言模型的通用推理能力提供了理论严谨的框架。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.12576">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.12576">arXiv</a></p>
<hr />
<h3>30. MomaGraph：面向具身任务规划的视觉语言模型状态感知统一场景图</h3>
<p><strong>原文标题：</strong> MomaGraph: State-Aware Unified Scene Graphs with Vision-Language Model for Embodied Task Planning</p>
<p><strong>摘要：</strong>
家庭环境中的移动机械臂需同时具备导航与操作能力，这要求一种紧凑且语义丰富的场景表征，能够捕捉物体位置、功能属性及可操作部件。场景图虽为自然选择，但现有研究常将空间关系与功能关系割裂，将场景视为缺乏物体状态或时序更新的静态快照，并忽视与当前任务最相关的信息。为突破这些局限，我们提出MomaGraph——一种面向具身智能体的统一场景表征，整合了空间-功能关系与部件级交互元素。然而，推进此类表征需要适配的数据集与严谨的评估体系，而这两者长期缺失。为此，我们贡献了MomaGraph-Scenes（首个大规模家庭环境任务驱动精细标注场景图数据集）与MomaGraph-Bench（涵盖从高层规划到细粒度场景理解六类推理能力的系统化评估套件）。基于此基础，我们进一步开发了MomaGraph-R1——一个通过强化学习在MomaGraph-Scenes上训练的70亿参数视觉语言模型。该模型能够预测任务导向场景图，并在“先构图后规划”框架下实现零样本任务规划。大量实验表明，我们的模型在开源模型中达到最先进水平：在基准测试中取得71.6%的准确率（较最佳基线提升11.4%），同时能泛化至公共基准测试，并有效迁移至真实机器人实验。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.16909">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.16909">arXiv</a></p>
<hr />
<h3>31. TabReX：一种无参考、可解释的表格生成评估框架</h3>
<p><strong>原文标题：</strong> TabReX : Tabular Referenceless eXplainable Evaluation</p>
<p><strong>摘要：</strong>
评估大语言模型（LLM）生成的表格质量仍是一个开放挑战：现有方法或将表格扁平化为文本而忽略其结构，或依赖固定参考从而限制泛化能力。本文提出TabReX，一种基于图推理、无需参考且以属性驱动的表格生成评估框架。TabReX将源文本与生成表格均转换为规范化知识图谱，通过LLM引导的匹配过程进行对齐，并计算可解释的、基于评估量规的分数，以量化结构与事实一致性。该评估指标可在敏感度与特异性之间实现可控权衡，提供与人工判断一致的结果及单元格级别的错误追溯。为系统评估指标的鲁棒性，我们构建了TabReX-Bench大规模基准数据集，涵盖六个领域、十二种规划驱动的扰动类型及三个难度层级。实验结果表明，TabReX与专家评价的相关性最高，在强扰动下保持稳定，并能支持细粒度的模型与提示词对比分析，为结构化生成系统的可信可解释评估建立了新范式。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.15907">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.15907">arXiv</a></p>
<hr />
<h3>32. 用于创造性连接与表达视觉概念的“氛围空间”</h3>
<p><strong>原文标题：</strong> Vibe Spaces for Creatively Connecting and Expressing Visual Concepts</p>
<p><strong>摘要：</strong>
创造新的视觉概念通常需要通过最相关的共享属性——即其“氛围”——来连接不同的想法。本文提出“氛围融合”这一新颖任务，旨在生成连贯且有意义的混合图像，以揭示图像间的共享属性。现有方法在实现此类融合时面临挑战，难以识别并遍历潜在空间中连接远距离概念的非线性路径。我们提出“氛围空间”——一种层次化图流形结构，能够在CLIP等特征空间中学习低维测地线，从而实现概念间平滑且语义一致的过渡。为评估创意质量，我们设计了一个融合人类判断、大语言模型推理与基于几何路径的难度评分的认知启发式评估框架。实验表明，相较于现有方法，“氛围空间”生成的融合结果被人类评价者一致认为更具创造性与连贯性。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.14884">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.14884">arXiv</a></p>
<hr />
<h3>33. 心智内推理：潜在空间中的动态多模态交错</h3>
<p><strong>原文标题：</strong> Reasoning Within the Mind: Dynamic Multimodal Interleaving in Latent Space</p>
<p><strong>摘要：</strong>
多模态大语言模型（MLLMs）的最新进展通过在语义空间中引入思维链推理，显著提升了跨模态理解与推理能力。在此基础上，近期研究将思维链机制扩展至视觉模态，使模型能够借助外部工具或显式图像生成在推理过程中整合视觉信息。然而，这些方法仍依赖于显式的分步推理，存在感知-推理交互不稳定及显著计算开销的问题。受人类认知机制启发，我们认为思维并非线性展开，而是通过心智中推理与感知的动态交错进行。基于这一视角，我们提出DMLR——一种测试时动态多模态潜在推理框架，该框架采用置信度引导的潜在策略梯度优化方法，通过精炼潜在思维标记实现深度推理。此外，我们引入了动态视觉注入策略，该策略在每个潜在思维标记处检索最相关的视觉特征并更新最优视觉片段集合，随后将更新后的片段注入潜在思维标记，实现动态的视觉-文本交错。在七个多模态推理基准测试及多种模型架构上的实验表明，DMLR在保持高效推理的同时，显著提升了模型的推理与感知性能。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.12623">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.12623">arXiv</a></p>
<hr />
<h3>34. 双向归一化流：从数据到噪声及其逆过程</h3>
<p><strong>原文标题：</strong> Bidirectional Normalizing Flow: From Data to Noise and Back</p>
<p><strong>摘要：</strong>
归一化流已发展成为生成建模的理论框架。标准归一化流包含前向过程与反向过程：前向过程将数据映射为噪声，反向过程则通过逆向映射生成样本。传统归一化流的前向变换受显式可逆性约束，确保反向过程能作为其精确解析逆。近期TARFlow及其变体通过结合Transformer与自回归流重振了归一化流方法，但也暴露出因果解码成为主要瓶颈。本研究提出双向归一化流框架，该框架无需依赖精确解析逆。双向归一化流通过学习近似底层噪声到数据逆映射的反向模型，实现了更灵活的损失函数与架构设计。在ImageNet数据集上的实验表明，相较于因果解码方法，双向归一化流在将采样速度提升两个数量级的同时提高了生成质量。该框架在基于归一化流的方法中取得了最优结果，并在单次评估方法中展现出具有竞争力的性能。随着归一化流领域近期取得鼓舞性进展，我们希望本研究能进一步激发对这一经典范式的关注。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.10953">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.10953">arXiv</a></p>
<hr />
<h3>35. EmoCaliber：通过置信度言语化与校准推进可靠的视觉情感理解</h3>
<p><strong>原文标题：</strong> EmoCaliber: Advancing Reliable Visual Emotion Comprehension via Confidence Verbalization and Calibration</p>
<p><strong>摘要：</strong>
视觉情感理解旨在从图像中蕴含的情感线索推断情感极性或情绪类别。近年来，多模态大语言模型凭借其泛化能力，成为视觉情感理解的主流范式，能够统一不同情感分类体系下的任务定义。尽管该范式取得了显著成功，但其通常将视觉情感理解视为确定性任务，要求模型为每张图像输出单一、明确的情感标签。这种设定未能充分考虑情感感知固有的主观性，忽视了对于不同观察者而言可能同样合理的其他解释。为应对这一局限，本文提出为多模态大语言模型赋予情感预测置信度言语化的能力。这一附加信号既为用户提供了替代性解释的合理性评估，也揭示了模型对自身能力的评估，从而在实践中增强系统可靠性。基于此洞见，我们提出一个三阶段训练框架：逐步赋予结构化推理能力、教授置信度言语化技巧、校准置信度表达，最终构建出面向视觉情感理解的置信度感知模型EmoCaliber。通过在统一基准测试集VECBench上进行公平全面的评估，EmoCaliber在情感预测与置信度估计两方面均展现出相较于现有方法的整体优越性。这些结果验证了我们方法的有效性，标志着向更可靠的视觉情感理解系统迈出了切实一步。项目页面：https://github.com/wdqqdw/EmoCaliber。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.15528">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.15528">arXiv</a></p>
<hr />
<h3>36. Nemotron-Math：基于多模态监督的高效长上下文数学推理蒸馏</h3>
<p><strong>原文标题：</strong> Nemotron-Math: Efficient Long-Context Distillation of Mathematical Reasoning from Multi-Mode Supervision</p>
<p><strong>摘要：</strong>
高质量的数学推理监督需要多样化的推理风格、长篇幅的解题过程以及有效的工具集成能力，而现有数据集仅能提供有限形式的支持。利用gpt-oss-120b的多模态生成能力，我们提出了Nemotron-Math——一个大规模数学推理数据集，包含750万条涵盖高、中、低三种推理模式的解题过程，每种模式均提供包含与不包含Python工具集成推理（TIR）的版本。该数据集整合了8.5万道精选的AoPS竞赛题与26.2万道社区来源的StackExchange-Math问题，将结构化竞赛任务与多样化的现实数学问题相结合。我们通过受控评估验证了数据集质量：在匹配的AoPS问题上，Nemotron-Math持续优于原始OpenMathReasoning数据集；引入StackExchange-Math数据显著提升了模型的鲁棒性与泛化能力（尤其在HLE-Math基准上），同时保持了数学竞赛基准的准确性。为支持高效的长上下文训练，我们开发了序列分桶策略，使128K上下文长度的微调加速2-3倍且未造成显著精度损失。总体而言，Nemotron-Math实现了最先进的性能表现，在使用Python TIR时于AIME 2024和2025测试中达到100%的maj@16准确率。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.15489">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.15489">arXiv</a></p>
<hr />
<h3>37. 提示与程序间的状态共享</h3>
<p><strong>原文标题：</strong> Sharing State Between Prompts and Programs</p>
<p><strong>摘要：</strong>
大型语言模型（LLM）的兴起催生了一种新型编程范式：自然语言编程。通过编写提示词来引导LLM执行自然语言处理、代码生成、推理等任务，用户实际上是在用自然语言编写代码——即自然语言代码——供LLM执行。</p>
<p>当前新兴研究领域致力于实现自然语言代码与Python等形式化语言之间的互操作性。本文提出一种新颖的编程抽象概念——共享程序状态，该机制消除了实现自然语言代码与程序状态互操作性所需的手动操作。借助共享程序状态，程序员能够编写直接写入程序变量、使用程序对象进行计算，并在程序中实现控制流的自然语言代码。我们提出一种用于规范自然函数接口的架构，该架构可扩展编程系统以支持自然语言代码，并利用此架构将共享程序状态定义为自然函数接口。</p>
<p>我们在Nightjar编程系统中实现了共享程序状态。Nightjar使程序员能够编写包含自然语言代码的Python程序，这些代码可与Python程序状态直接交互。实验表明，Nightjar程序在任务准确率上达到甚至超越手动编写实现的水平（提升4-19%），同时平均减少39.6%的代码量。使用Nightjar的代价是可能产生运行时开销（达到手动实现运行时间的0.4-4.3倍）。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.14805">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.14805">arXiv</a></p>
<hr />
<h3>38. 基于混合LoRA的递归Transformer性能提升研究</h3>
<p><strong>原文标题：</strong> Improving Recursive Transformers with Mixture of LoRAs</p>
<p><strong>摘要：</strong>
递归Transformer中的参数共享机制虽能缩减模型规模，却导致层间表达能力退化。本研究提出混合低秩适配器（MoL）方案，通过在共享前馈网络内部嵌入低秩自适应专家模块，构建轻量级条件计算机制。相较于以往采用固定或外挂适配器的方法，MoL能在保持主干参数绑定的前提下实现基于令牌条件的权重空间调制。我们预训练了现代化递归架构ModernALBERT，集成旋转位置编码、GeGLU激活函数、FlashAttention加速机制及基于知识蒸馏的初始化策略。在GLUE、SQuAD-v2和BEIR基准测试中，参数量为50M-120M的ModernALBERT在紧凑模型中达到最优性能，并超越完全参数化的大型基线模型。同时提出专家融合算法，在推理阶段将MoL压缩为单一适配器且保持精度，实现高效部署。实验结果表明，条件权重空间调制能有效恢复递归Transformer在激进参数共享策略下损失的表达能力。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.12880">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.12880">arXiv</a></p>
<hr />
<h2>🔍 关键词云图</h2>
<p><img alt="关键词云图" src="../images/keywords_wordcloud.png" /></p>
<h2>📈 近期论文趋势</h2>
<p><img alt="论文趋势" src="../images/daily_papers.png" /></p>
<h2>🎙️ 语音播报</h2>
<ul>
<li><a href="../audio/2025-12-19_daily_papers.mp3">收听今日论文解读</a></li>
</ul>
<h2>📱 订阅渠道</h2>
<ul>
<li>GitHub: <a href="https://github.com/2404589803/hf-daily-paper-newsletter-chinese">hf-daily-paper-newsletter-chinese</a></li>
</ul>
    </div>
</body>
</html>