<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Hugging Face 论文日报 - 2025-11-21</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', sans-serif;
            line-height: 1.6;
            color: #333;
            background-color: #f5f5f5;
            padding: 20px;
        }
        
        .container {
            max-width: 900px;
            margin: 0 auto;
            background-color: white;
            padding: 40px;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }
        
        h1 {
            color: #2c3e50;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
            margin-bottom: 30px;
            font-size: 28px;
        }
        
        h1 img {
            vertical-align: middle;
            margin-right: 10px;
        }
        
        h2 {
            color: #34495e;
            margin-top: 40px;
            margin-bottom: 20px;
            font-size: 24px;
            border-left: 4px solid #3498db;
            padding-left: 15px;
        }
        
        h3 {
            color: #2c3e50;
            margin-top: 30px;
            margin-bottom: 15px;
            font-size: 20px;
        }
        
        p {
            margin-bottom: 15px;
            text-align: justify;
        }
        
        ul {
            margin-left: 20px;
            margin-bottom: 20px;
        }
        
        li {
            margin-bottom: 8px;
        }
        
        a {
            color: #3498db;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        strong {
            color: #2c3e50;
            font-weight: 600;
        }
        
        hr {
            border: none;
            border-top: 1px solid #e0e0e0;
            margin: 30px 0;
        }
        
        /* 关键修复:限制图片宽度 */
        img {
            max-width: 100%;
            height: auto;
            display: block;
            margin: 20px auto;
            border-radius: 4px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        
        /* 确保图片容器也有宽度限制 */
        p img {
            max-width: 100%;
        }
        
        /* 论文详情区域样式 */
        .paper-section {
            background-color: #f9f9f9;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 20px;
        }
        
        /* 统计信息样式 */
        .stats {
            background-color: #e8f4f8;
            padding: 15px;
            border-radius: 6px;
            margin-bottom: 20px;
        }
        
        /* 响应式设计 */
        @media (max-width: 768px) {
            .container {
                padding: 20px;
            }
            
            h1 {
                font-size: 24px;
            }
            
            h2 {
                font-size: 20px;
            }
            
            h3 {
                font-size: 18px;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <h1><img src="https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo.png" width="30"/> Hugging Face 2025-11-21 论文日报</h1>
<h2>📊 今日论文统计</h2>
<ul>
<li>总论文数：22</li>
<li>热门领域：Audio, RL, Transformer</li>
</ul>
<h2>📝 论文详情</h2>
<h3>1. 首帧：视频内容定制化的关键所在</h3>
<p><strong>原文标题：</strong> First Frame Is the Place to Go for Video Content Customization</p>
<p><strong>摘要：</strong>
首帧在视频生成模型中究竟扮演着何种角色？传统观点将其视为视频的时空起点，仅是后续动画的生成种子。本研究提出一个根本性不同的视角：视频模型隐式地将首帧作为概念记忆缓冲区，存储视觉实体以供后续生成过程重复利用。基于这一发现，我们证明仅需20-50个训练样本，无需改变模型架构或进行大规模微调，即可在多样化场景中实现鲁棒且泛化性强的视频内容定制。这一发现揭示了视频生成模型在基于参考视频的定制化任务中强大却长期被忽视的能力。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.15700">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.15700">arXiv</a></p>
<hr />
<h3>2. V-ReasonBench：面向视频生成模型的统一推理基准测试套件</h3>
<p><strong>原文标题：</strong> V-ReasonBench: Toward Unified Reasoning Benchmark Suite for Video Generation Models</p>
<p><strong>摘要：</strong>
随着Veo-3等生成式视频模型的最新进展展现出惊人的零样本推理能力，对系统化可靠评估的需求日益增长。我们推出V-ReasonBench基准测试框架，旨在从四个关键维度评估视频推理能力：结构化问题解决、空间认知、基于模式的推理及物理动态理解。该基准集成了合成与真实场景的图像序列，提供多样化且答案可验证的任务，具备可复现、可扩展和明确性等特性。通过对六个前沿视频模型的评估，我们观察到各维度推理能力存在显著差异，尤其在结构化、空间、模式识别及物理推理方面表现参差不齐。我们进一步将视频模型与强图像模型进行对比，分析常见的幻觉生成行为，并研究视频时长对帧序列推理链的影响。总体而言，V-ReasonBench为衡量视频推理能力提供了统一可复现的框架，旨在推动开发具有更可靠、更符合人类思维的推理能力的视频生成模型。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.16668">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.16668">arXiv</a></p>
<hr />
<h3>3. Step-Audio-R1技术报告</h3>
<p><strong>原文标题：</strong> Step-Audio-R1 Technical Report</p>
<p><strong>摘要：</strong>
推理模型通过扩展的思维链推演在文本和视觉领域取得了显著成功。然而音频语言模型领域始终存在一个令人困惑的现象：模型在极少或无需推理的情况下表现更佳，这引发了一个根本性问题——音频智能是否真正受益于深度思考？我们推出Step-Audio-R1，这是首个成功在音频领域解锁推理能力的声音推理模型。通过我们提出的模态锚定推理蒸馏框架，该模型学会了生成与声学特征真正锚定的音频相关推理链，而非产生脱离实际的虚构推演。我们的模型展现出强大的音频推理能力，在涵盖语音、环境声和音乐的综合音频理解与推理基准测试中，不仅超越了Gemini 2.5 Pro，更达到了与最先进模型Gemini 3 Pro相媲美的性能。这些结果表明当推理过程被恰当锚定时，跨模态的推理能力具有可迁移性，从而将扩展推演从音频智能的负担转化为强大优势。通过建立首个成功的音频推理模型，Step-Audio-R1为构建真正跨感官模态深度思考的多模态推理系统开辟了新路径。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.15848">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.15848">arXiv</a></p>
<hr />
<h3>4. 基于多模态基础模型的空间智能规模化研究</h3>
<p><strong>原文标题：</strong> Scaling Spatial Intelligence with Multimodal Foundation Models</p>
<p><strong>摘要：</strong>
尽管取得了显著进展，多模态基础模型在空间智能方面仍存在明显不足。本研究探索通过规模化扩展多模态基础模型来培育SenseNova-SI系列的空间智能，该系列建立在成熟的多模态基础之上，包括视觉理解模型（如Qwen3-VL和InternVL3）以及统一理解与生成模型（如Bagel）。我们采用系统化方法构建了高性能且稳健的空间智能模型：通过严格的空间能力分类体系，精心构建包含八百万个多样化数据样本的SenseNova-SI-8M数据集。SenseNova-SI在广泛的空间智能基准测试中展现出卓越性能：VSI-Bench达68.7%，MMSI达43.3%，MindCube达85.6%，ViewSpatial达54.6%，SITE达50.1%，同时保持强大的通用多模态理解能力（如MMBench-En达84.9%）。更重要的是，我们分析了数据规模化的影响，探讨了通过多样化数据训练实现涌现泛化能力的早期迹象，研究了过拟合和语言捷径风险，提出了空间思维链推理的初步研究，并验证了潜在的下游应用价值。SenseNova-SI是持续发展的项目，本报告将定期更新。所有新训练的多模态基础模型均已公开发布，以推动该领域的深入研究。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.13719">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.13719">arXiv</a></p>
<hr />
<h3>5. SAM 3D：图像三维化通用框架</h3>
<p><strong>原文标题：</strong> SAM 3D: 3Dfy Anything in Images</p>
<p><strong>摘要：</strong>
本文提出SAM 3D——一种基于视觉感知的三维物体重建生成模型，能够通过单张图像预测几何结构、纹理特征和空间布局。该模型在自然图像处理中表现卓越，尤其适用于存在遮挡和场景杂乱的场景，其通过上下文视觉识别线索发挥重要作用。我们采用人机协同标注流程来获取物体形状、纹理及位姿信息，从而构建了规模空前的视觉感知三维重建数据集。通过结合合成预训练与真实场景对齐的现代多阶段训练框架，我们成功突破了三维数据的"数据壁垒"。实验结果表明，本方法相较现有研究取得显著提升，在真实物体与场景的人类偏好测试中获胜率至少达5:1。我们将公开源代码与模型权重、在线演示系统，以及针对真实场景三维重建的新挑战性基准测试集。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.16624">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.16624">arXiv</a></p>
<hr />
<h3>6. 视频即答案：基于联合GRPO的下一视频事件预测与生成方法</h3>
<p><strong>原文标题：</strong> Video-as-Answer: Predict and Generate Next Video Event with Joint-GRPO</p>
<p><strong>摘要：</strong>
尽管语言模型已在众多实际应用中产生重要影响，视频生成领域仍主要局限于娱乐用途。鉴于视频具有展现物理世界信息的天然优势——这些信息往往难以仅通过语言传达（例如仅用文字指导他人打领带）——我们发现了一个尚未充分开发的机遇：将视频扩展为下一代事件预测的新型答案模态，并将其形式化为视频下一代事件预测任务。传统NEP任务以包含流程性或预测性问题的视频作为输入，通过文本来预测下一事件，而VNEP则要求生成动态视频响应。这种从“讲述”到“展示”的转变，为流程化学习和创意探索开启了更直观、可定制的解答方式。然而，该任务对现有模型仍具挑战性，因其需要理解多模态输入、执行指令条件推理，并生成具有视觉与语义一致性的视频。为此，我们提出VANS模型，通过强化学习将视觉语言模型与视频扩散模型对齐以解决VNEP任务。VANS的核心是我们提出的联合GRPO机制，该机制协调VLM和VDM作为整体单元运行。基于对其各自输出的共享奖励驱动，该机制既优化VLM生成兼具准确性和可视化友好度的描述文本，同时指导VDM生成忠实于文本描述及输入视觉语境的视频。为支持此学习过程，我们构建了专用于VNEP任务的VANS-Data-100K数据集。在流程性和预测性基准测试上的实验表明，VANS在视频事件预测与可视化方面均实现了最先进的性能。代码已发布于https://github.com/KlingTeam/VANS。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.16669">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.16669">arXiv</a></p>
<hr />
<h3>7. MiMo-Embodied：跨具身基础模型技术报告</h3>
<p><strong>原文标题：</strong> MiMo-Embodied: X-Embodied Foundation Model Technical Report</p>
<p><strong>摘要：</strong>
我们开源了MiMo-Embodied——首个成功整合自动驾驶与具身人工智能两大领域并实现最先进性能的跨具身基础模型。该模型在任务规划、功能预测与空间理解等17项具身AI基准测试中创下新纪录，同时在环境感知、状态预测与驾驶规划等12项自动驾驶基准测试中表现卓越。在所有任务中，MiMo-Embodied显著超越了现有开源、闭源及专业基线模型。研究表明，通过多阶段学习、精细化数据构建以及思维链/强化学习微调，这两个领域展现出显著的积极迁移效应并形成相互增强。我们详细解析了模型设计与训练方法，以推动后续研究。代码与模型已发布于https://github.com/XiaomiMiMo/MiMo-Embodied。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.16518">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.16518">arXiv</a></p>
<hr />
<h3>8. Agent0：通过工具集成推理实现零数据自进化智能体</h3>
<p><strong>原文标题：</strong> Agent0: Unleashing Self-Evolving Agents from Zero Data via Tool-Integrated Reasoning</p>
<p><strong>摘要：</strong>
基于强化学习训练的大语言模型智能体长期受限于对人类标注数据的依赖，这不仅制约了系统扩展性，更将人工智能发展束缚于人类知识范畴。现有自进化框架虽提供替代方案，但通常受限于模型固有能力和单轮交互机制，难以发展涉及工具使用或动态推理的复杂课程体系。本文提出Agent0——一个完全自主的进化框架，通过多步协同进化与无缝工具集成，无需外部数据即可培育高性能智能体。该框架在相同基础大语言模型上构建两个智能体的共生竞争：课程智能体负责提出日益挑战性的前沿任务，执行智能体则学习解决这些任务。我们集成外部工具以增强执行者的问题解决能力，这种进步反过来推动课程智能体构建更具复杂性、工具感知的新任务。通过这种迭代机制，Agent0建立了自我强化的循环体系，持续生成高质量课程。实验结果表明，Agent0显著提升推理能力，在数学推理基准上使Qwen3-8B-Base模型提升18%，通用推理基准提升24%。代码已开源：https://github.com/aiming-lab/Agent0。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.16043">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.16043">arXiv</a></p>
<hr />
<h3>9. Nemotron Elastic：迈向高效多合一推理大语言模型</h3>
<p><strong>原文标题：</strong> Nemotron Elastic: Towards Efficient Many-in-One Reasoning LLMs</p>
<p><strong>摘要：</strong>
针对多尺度与多部署目标训练大语言模型家族的成本极其高昂，每个不同规模的模型均需独立训练。近期通过剪枝与知识蒸馏实现的模型压缩方法虽降低了成本，但每个压缩模型仍需消耗数千亿标记的训练开销。本文提出Nemotron Elastic框架，用于构建面向推理的混合Mamba-Attention架构大语言模型，该框架可在单一父模型中嵌入多个嵌套子模型，每个子模型针对不同部署配置与预算进行优化。这些子模型与父模型共享权重，无需额外训练或微调即可在部署时零样本提取。我们通过端到端训练的路由器实现此功能，该路由器与专为推理模型设计的两阶段训练课程紧密耦合。我们还提出保留Mamba结构约束的组感知SSM弹性化方法、异构MLP弹性化技术、基于归一化MSE的层重要性评估以改进深度选择，以及支持多预算同步优化的知识蒸馏机制。将Nemotron Elastic应用于Nemotron Nano V2 12B模型，仅用1100亿训练标记即可同步生成90亿与60亿参数模型，相比从头训练模型家族成本降低360倍以上，相较前沿压缩技术缩减约7倍成本。所有嵌套模型在准确度上均达到或超越现有最优水平。此外，与其他压缩方法不同，本方案的嵌套特性可实现“多合一”推理模型，其部署内存占用随模型家族数量增加保持恒定。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.16664">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.16664">arXiv</a></p>
<hr />
<h3>10. 通用基础模型尚不足以满足医院运营的临床需求</h3>
<p><strong>原文标题：</strong> Generalist Foundation Models Are Not Clinical Enough for Hospital Operations</p>
<p><strong>摘要：</strong>
医院与医疗系统的运行依赖于决定患者流、成本及护理质量的操作性决策。尽管通用文本训练的基础模型在医学知识和对话基准测试中表现优异，但其可能缺乏此类操作性决策所需的专业知识。我们推出Lang1模型系列（参数量1亿至70亿），其预训练语料融合了来自纽约大学朗格尼健康中心电子健康记录的800亿临床标记符及互联网来源的6270亿标记符。为在真实场景中严格评估Lang1，我们开发了现实医疗评估基准（ReMedE）——该基准源自668,331份电子健康记录，评估五大关键任务：30天再入院预测、30天死亡率预测、住院时长、共病编码及保险拒赔预测。在零样本设定下，通用模型与专业模型在五项任务中有四项表现不佳（AUROC值36.6%-71.7%），仅死亡率预测例外。经微调后，Lang1-1B模型的表现优于参数量达其70倍的微调通用模型及参数量达其671倍的零样本模型，AUROC指标分别提升3.64%-6.75%和1.66%-23.66%。我们还观察到跨任务扩展效应：对多任务联合微调可提升其他任务表现。Lang1-1B能有效迁移至分布外场景，包括其他临床任务及外部医疗系统。研究结果表明：医院运营的预测能力需要显式监督微调，而基于电子健康记录的领域内预训练可提升该微调过程的效率。我们的发现支持新兴观点——专业大语言模型可在特定任务中与通用模型竞争，并证明构建高效医疗系统人工智能需要结合领域内预训练、监督微调及超越代理基准的真实场景评估。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.13703">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.13703">arXiv</a></p>
<hr />
<h3>11. 生成中思考：视觉生成过程中的文本推理交错机制</h3>
<p><strong>原文标题：</strong> Thinking-while-Generating: Interleaving Textual Reasoning throughout Visual Generation</p>
<p><strong>摘要：</strong>
视觉生成领域的最新进展逐渐探索推理能力的整合。现有方法通常在生成前（作为预规划）或生成后（作为后优化）引入文本推理，但缺乏生成过程中的实时多模态交互。在本初步研究中，我们提出"生成中思考"框架，这是首个在视觉生成全过程中实现文本推理与视觉内容协同演进的交错式架构。随着视觉内容的渐进生成，系统通过交错进行的文本推理既指导后续局部区域的生成，又对已合成内容进行反思。这种动态交互产生了更具上下文感知能力与语义丰富性的视觉输出。为探索该框架潜力，我们研究了三种实现策略：基于零样本提示的方法、基于自建TwiG-50K数据集的有监督微调方法，以及通过定制化TwiG-GRPO策略的强化学习方法，每种策略都为交错推理的动态机制提供了独特见解。我们期待这项工作能推动文本推理交错机制在增强视觉生成方面的深入研究。代码发布地址：https://github.com/ZiyuGuo99/Thinking-while-Generating。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.16671">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.16671">arXiv</a></p>
<hr />
<h3>12. TurkColBERT：土耳其语信息检索的稠密与延迟交互模型基准测试</h3>
<p><strong>原文标题：</strong> TurkColBERT: A Benchmark of Dense and Late-Interaction Models for Turkish Information Retrieval</p>
<p><strong>摘要：</strong>
神经信息检索系统在高资源语言中表现卓越，但对土耳其语这类形态复杂、资源相对匮乏的语言研究仍显不足。当前土耳其语信息检索主要采用稠密双编码器，而保留词元级表示以进行细粒度匹配的延迟交互模型尚未得到系统评估。我们推出TurkColBERT——首个针对土耳其语检索的稠密编码器与延迟交互模型综合基准。通过两阶段适配流程，我们首先在土耳其语NLI/STS任务上微调英语和多语言编码器，随后利用基于MS MARCO-TR训练的PyLate将其转换为ColBERT式检索器。我们在涵盖科学、金融及论证领域的五个土耳其语BEIR数据集上评估了10个模型。结果显示卓越的参数效率：仅含1.0M参数的colbert-hash-nano-tr比600M参数的turkish-e5-large稠密编码器缩小600倍，却能保持其平均mAP值的71%以上。参数量比稠密编码器小3-5倍的延迟交互模型显著优于后者，其中ColmmBERT-base-TR在特定领域任务中mAP提升高达+13.8%。针对生产就绪需求，我们比较了索引算法：MUVERA+重排序比PLAID快3.33倍，并实现+1.7%的相对mAP增益。这使得ColmmBERT-base-TR在MUVERA架构下达到0.54毫秒查询延迟的低延迟检索。我们公开了所有检查点、配置和评估脚本。局限性包括对中等规模数据集（≤5万文档）和翻译基准的依赖，这可能无法完全反映真实场景的土耳其语检索条件；更大规模的MUVERA评估仍有待开展。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.16528">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.16528">arXiv</a></p>
<hr />
<h3>13. SRPO：视觉-语言-动作模型的自参照策略优化方法</h3>
<p><strong>原文标题：</strong> SRPO: Self-Referential Policy Optimization for Vision-Language-Action Models</p>
<p><strong>摘要：</strong>
视觉-语言-动作模型在机器人操作任务中表现卓越，但其性能受限于对专家示范数据的严重依赖，导致存在示范偏差问题。强化学习作为克服这些限制的关键后训练策略，现有VLA-RL方法（包括基于群体的优化方法）却受困于严重的奖励稀疏性。仅依赖二元成功指标会浪费失败轨迹中的宝贵信息，导致训练效率低下。为此，我们提出自参照策略优化方法——一种新型VLA-RL框架。该方法通过利用当前训练批次中模型自身生成的成功轨迹作为自参照基准，无需外部示范数据或人工奖励工程，即可为失败尝试分配渐进式奖励。其核心创新在于利用潜在世界表征来稳健度量行为进展：通过世界模型潜在空间中的压缩可迁移编码，替代原始像素输入或领域特定微调需求，这些表征能自然捕捉跨环境进展模式，实现精准通用的轨迹比较。在LIBERO基准上的实证研究表明，SRPO从监督基线48.9%的成功率起步，仅通过200步强化学习训练即达到99.2%的最新最优成功率，在无额外监督情况下实现103%的相对提升。此外，SRPO在LIBERO-Plus基准上展现出显著鲁棒性，取得167%的性能提升。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.15605">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.15605">arXiv</a></p>
<hr />
<h3>14. SAM2S：通过语义长期跟踪实现手术视频中任意目标分割</h3>
<p><strong>原文标题：</strong> SAM2S: Segment Anything in Surgical Videos via Semantic Long-term Tracking</p>
<p><strong>摘要：</strong>
手术视频分割对计算机辅助手术至关重要，能够实现器械与组织的精确定位与跟踪。基于预定义类别的方法存在局限性，而交互式视频目标分割（iVOS）模型（如SAM2）通过提示机制提供了更灵活的解决方案，但在手术场景下面临领域差异和长期跟踪能力不足的挑战。为突破这些限制，我们构建了SA-SV——目前最大的手术iVOS基准数据集，包含跨越八种手术类型的实例级时空标注（61千帧，1.6千个掩码片段），为长期跟踪与零样本泛化研究提供全面支撑。基于该数据集，我们提出SAM2S基础模型，通过三大创新增强SAM2在手术iVOS中的性能：（1）DiveMem可训练多样性记忆机制，实现鲁棒长期跟踪；（2）面向器械理解的时序语义学习；（3）抗模糊学习策略以缓解多源数据标注不一致问题。大量实验表明，在SA-SV上进行微调可显著提升性能：SAM2的平均J&amp;F指标较原始版本提升12.99。SAM2S进一步将性能推至80.42平均J&amp;F，分别超越原始版与微调版SAM2达17.10和4.11个点，同时保持68 FPS实时推理速度与强大的零样本泛化能力。代码与数据集将在https://jinlab-imvr.github.io/SAM2S 发布。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.16618">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.16618">arXiv</a></p>
<hr />
<h3>15. NaTex：作为潜在颜色扩散的无缝纹理生成方法</h3>
<p><strong>原文标题：</strong> NaTex: Seamless Texture Generation as Latent Color Diffusion</p>
<p><strong>摘要：</strong>
本文提出NaTex——一种在三维空间中直接预测纹理颜色的原生纹理生成框架。与现有基于几何条件多视图扩散模型（MVD）合成二维多视图图像再进行烘焙的方法不同，NaTex规避了MVD流程的固有局限：包括处理遮挡区域需依赖修复、难以实现边界精确的网格-纹理对齐、以及保持跨视图内容与色彩强度的一致性等问题。NaTex采用将纹理视为稠密颜色点云的新范式，通过潜在颜色扩散技术解决上述挑战。该技术包含几何感知的颜色点云变分自编码器（VAE）与多控制扩散变换器（DiT），全部基于三维数据从零开始训练，用于纹理重建与生成。为实现精确对齐，我们引入原生几何控制机制，通过位置嵌入与几何潜在编码将直接三维空间信息作为DiT的条件。我们协同设计VAE-DiT架构，其中几何潜在编码通过专设的几何分支提取，该分支与颜色VAE紧密耦合，提供与纹理保持强对应关系的细粒度表面引导。实验表明，NaTex在纹理一致性与对齐精度上显著优于现有方法。此外，该框架在无需训练或简单调参的情况下，对材质生成、纹理优化、部件分割与纹理化等下游任务展现出强大的泛化能力。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.16317">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.16317">arXiv</a></p>
<hr />
<h3>16. PartUV：基于部件划分的三维网格UV展开方法</h3>
<p><strong>原文标题：</strong> PartUV: Part-Based UV Unwrapping of 3D Meshes</p>
<p><strong>摘要：</strong>
UV展开技术将三维曲面以最小失真度展开为二维平面，通常需要将复杂曲面分解为多个图块。尽管该领域已被广泛研究，现有UV展开方法在处理AI生成网格时仍面临严峻挑战，这类网格通常存在噪声干扰、表面崎岖和条件恶劣等问题。现有方法往往产生高度碎片化的图块和欠优的边界划分，导致伪影产生并影响下游任务。本文提出PartUV——基于部件划分的UV展开流程，在保持低失真度的同时生成数量显著减少且与部件对齐的图块。该方案基于最新基于学习的部件分解方法PartField构建，在自上而下的递归框架中，将高层语义部件分解与新型几何启发式算法相结合。该方法在确保每个图块失真度低于用户设定阈值的同时，最小化图块总数。该流程集成并拓展了参数化与排布算法，包含对非流形与退化网格的专门处理，并采用大规模并行化以提升效率。在涵盖人造物体、CAD模型、AI生成网格和通用形状的四个多样化数据集上的评估表明，PartUV在图块数量和接缝长度指标上优于现有工具与近期神经方法，达到可比拟的失真度，在挑战性网格上呈现高成功率，并支持部件级多贴图排布等新应用。项目主页详见：https://www.zhaoningwang.com/PartUV。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.16659">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.16659">arXiv</a></p>
<hr />
<h3>17. TimeViper：面向高效长视频理解的混合Mamba-Transformer视觉语言模型</h3>
<p><strong>原文标题：</strong> TimeViper: A Hybrid Mamba-Transformer Vision-Language Model for Efficient Long Video Understanding</p>
<p><strong>摘要：</strong>
本文提出TimeViper混合视觉语言模型，旨在解决长视频理解中的关键挑战。处理长视频既需要高效的模型架构，又需要有效的长时序上下文处理机制。为此，TimeViper采用混合Mamba-Transformer主干网络，将状态空间模型的高效性与注意力机制的强表达能力相结合。通过这种混合设计，我们揭示了视觉到文本的信息聚合现象：随着大语言模型层数加深，信息持续从视觉标记向文本标记流动，导致视觉标记出现严重冗余。基于此发现，我们提出TransV模块——一种在保持多模态理解能力的同时，将视觉标记转移并压缩至指令标记的传输机制。该设计使TimeViper能够处理超过10,000帧的时长一小时视频。在多个基准测试上的广泛实验表明，TimeViper在显著扩展处理帧数的同时，性能仍可与最先进模型媲美。我们进一步分析了Mamba与Transformer层的注意力机制，为混合模型的可解释性研究提供了新视角。本工作代表了在开发、解析和压缩混合Mamba-Transformer架构方向上的初步探索。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.16595">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.16595">arXiv</a></p>
<hr />
<h3>18. EntroPIC：基于比例-积分控制的熵稳定方法实现大语言模型的长期稳定训练</h3>
<p><strong>原文标题：</strong> EntroPIC: Towards Stable Long-Term Training of LLMs via Entropy Stabilization with Proportional-Integral Control</p>
<p><strong>摘要：</strong>
大语言模型的长期训练需要保持稳定的探索性，以防止模型坍缩至次优行为。熵在此过程中具有关键作用，它既能控制探索强度，又有助于避免过早收敛到次优解。然而现有强化学习方法难以维持恰当的熵水平，因为训练过程同时包含正负样本，且每类样本在不同训练阶段对熵的影响方式存在差异。为此，我们提出基于比例-积分控制的熵稳定方法（EntroPIC），该创新方法通过动态调整正负样本的损失系数来自适应调节其影响强度，从而在全程训练中稳定熵值，确保持续有效的探索与稳定进展。我们为同策略与异策略学习场景提供了完整的理论分析，证明EntroPIC能够有效控制大规模语言模型训练中的熵变。实验结果表明，本方法可成功将熵值维持在预期水平，为实现大语言模型的稳定最优强化学习训练提供了有效途径。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.15248">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.15248">arXiv</a></p>
<hr />
<h3>19. FinTRec：基于Transformer的金融应用统一上下文广告定向与个性化系统</h3>
<p><strong>原文标题：</strong> FinTRec: Transformer Based Unified Contextual Ads Targeting and Personalization for Financial Applications</p>
<p><strong>摘要：</strong>
基于Transformer的架构虽已广泛应用于序列推荐系统，但其在金融服务领域的实时推荐应用中仍面临独特的实践与建模挑战。这些挑战包括：a) 用户跨数字与实体渠道产生的长周期交互行为（隐式与显式）形成时序异构上下文；b) 多类关联产品并存需协调建模以支持多样化广告位投放与个性化信息流，同时平衡相互竞争的业务目标。我们提出FinTRec这一基于Transformer的框架，旨在应对金融服务领域的这些挑战及运营目标。尽管传统上基于树的模型因可解释性及符合监管要求更受金融服务领域青睐，但本研究证明FinTRec为转向基于Transformer的架构提供了可行有效的解决方案。通过历史模拟与线上A/B测试关联分析，我们证实FinTRec持续优于生产级基于树的基线模型。该统一架构经过产品适配微调后，可实现跨产品信号共享，降低训练成本与技术负债，同时全面提升所有产品的离线性能。据我们所知，这是首个在金融服务领域兼顾技术与业务考量的统一序列推荐建模综合性研究。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.14865">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.14865">arXiv</a></p>
<hr />
<h3>20. BioBench：超越ImageNet的科学机器学习基准蓝图</h3>
<p><strong>原文标题：</strong> BioBench: A Blueprint to Move Beyond ImageNet for Scientific ML Benchmarks</p>
<p><strong>摘要：</strong>
ImageNet-1K线性探针迁移精度虽仍是衡量视觉表征质量的默认指标，但其对科学影像的性能预测已然失效。通过对46个现代视觉模型检查点的测试，ImageNet top-1准确率仅能解释生态学任务中34%的方差差异，且在准确率超过75%的模型中存在30%的误判。我们推出BioBench——一个能捕捉ImageNet缺失维度的开放式生态视觉基准。该基准整合了9项公开的应用驱动型任务，覆盖4个生物分类界域和6种采集模态（无人机RGB影像、网络视频、显微图像、原位与标本照片、相机陷阱帧），总计310万张图像。通过统一的Python接口可实现数据下载、轻量级分类器与冻结主干网络的适配，并输出类别均衡宏F1值（另含FishNet与FungiCLEF的领域特定指标）；在A6000 GPU上ViT-L模型的完整评估仅需6小时。BioBench不仅为生态计算机视觉提供了新的评估标尺，更为构建跨领域可靠的科学人工智能基准建立了可复现的范式模板。代码及预测结果详见https://github.com/samuelstevens/biobench，完整结果可访问https://samuelstevens.me/biobench。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.16315">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.16315">arXiv</a></p>
<hr />
<h3>21. 基于多粒度语言学习的医学视觉理解能力提升</h3>
<p><strong>原文标题：</strong> Boosting Medical Visual Understanding From Multi-Granular Language Learning</p>
<p><strong>摘要：</strong>
图像-文本预训练技术通过对齐视觉与文本表征，显著提升了视觉理解能力。对比语言-图像预训练（CLIP）在多模态学习中发挥了关键作用。然而该方法侧重于单标签、单粒度对齐，在医学影像等复杂领域存在局限性——该类图像通常对应多个高层级标签（如疾病类别）且具有不同注释粒度（如诊断描述、临床解释）。为此，我们提出多粒度语言学习（MGLL）框架，该对比学习框架旨在同时提升多标签与跨粒度对齐能力。MGLL通过以下机制实现优化：利用结构化多标签监督，整合跨粒度文本描述，引入带逐点约束的软标签监督以增强对齐效果。该框架采用平滑KL散度确保跨粒度一致性，同时作为即插即用模块保持视觉语言模型的计算效率。基于我们构建的大规模多粒度数据集进行预训练，并在多个数据集上验证，MGLL在下游任务中超越了现有先进方法。代码已开源：https://github.com/HUANGLIZI/MGLL。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.15943">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.15943">arXiv</a></p>
<hr />
<h3>22. 基于视觉专家协同的草拟-精修框架</h3>
<p><strong>原文标题：</strong> Draft and Refine with Visual Experts</p>
<p><strong>摘要：</strong>
尽管当前的大型视觉语言模型展现出强大的多模态推理能力，但由于过度依赖语言先验而忽视视觉证据，常常产生缺乏依据或虚构的响应。这一局限凸显出现有方法缺乏对模型在推理过程中实际利用视觉信息程度的量化评估。我们提出草拟-精修框架，该智能体框架由问题条件化利用度量驱动：首先通过构建查询条件化关联图来定位问题相关视觉线索，继而通过关联引导的概率掩码测量模型依赖度，从而量化模型对视觉证据的依赖程度。在此度量引导下，DnR智能体通过外部视觉专家提供的定向反馈持续优化初始回答。各视觉专家输出（如检测框或分割掩码）以视觉标记形式呈现在图像上，通过重新查询模型选择能最大程度提升视觉利用率的响应。该流程无需重新训练或改变架构即可增强视觉基础。在视觉问答与图像描述基准测试中的实验表明，该方法持续提升准确率并有效减少虚构内容，证明视觉利用率量化为构建更可解释、证据驱动的多模态智能体系统提供了理论路径。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.11005">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.11005">arXiv</a></p>
<hr />
<h2>🔍 关键词云图</h2>
<p><img alt="关键词云图" src="../images/keywords_wordcloud.png" /></p>
<h2>📈 近期论文趋势</h2>
<p><img alt="论文趋势" src="../images/daily_papers.png" /></p>
<h2>🎙️ 语音播报</h2>
<ul>
<li><a href="../audio/2025-11-21_daily_papers.mp3">收听今日论文解读</a></li>
</ul>
<h2>📱 订阅渠道</h2>
<ul>
<li>GitHub: <a href="https://github.com/2404589803/hf-daily-paper-newsletter-chinese">hf-daily-paper-newsletter-chinese</a></li>
</ul>
    </div>
</body>
</html>