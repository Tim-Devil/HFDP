<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Hugging Face 论文日报 - 2026-01-23</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', sans-serif;
            line-height: 1.6;
            color: #333;
            background-color: #f5f5f5;
            padding: 20px;
        }
        
        .container {
            max-width: 900px;
            margin: 0 auto;
            background-color: white;
            padding: 40px;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }
        
        h1 {
            color: #2c3e50;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
            margin-bottom: 30px;
            font-size: 28px;
        }
        
        h1 img {
            vertical-align: middle;
            margin-right: 10px;
        }
        
        h2 {
            color: #34495e;
            margin-top: 40px;
            margin-bottom: 20px;
            font-size: 24px;
            border-left: 4px solid #3498db;
            padding-left: 15px;
        }
        
        h3 {
            color: #2c3e50;
            margin-top: 30px;
            margin-bottom: 15px;
            font-size: 20px;
        }
        
        p {
            margin-bottom: 15px;
            text-align: justify;
        }
        
        ul {
            margin-left: 20px;
            margin-bottom: 20px;
        }
        
        li {
            margin-bottom: 8px;
        }
        
        a {
            color: #3498db;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        strong {
            color: #2c3e50;
            font-weight: 600;
        }
        
        hr {
            border: none;
            border-top: 1px solid #e0e0e0;
            margin: 30px 0;
        }
        
        /* 关键修复:限制图片宽度 */
        img {
            max-width: 100%;
            height: auto;
            display: block;
            margin: 20px auto;
            border-radius: 4px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        
        /* 确保图片容器也有宽度限制 */
        p img {
            max-width: 100%;
        }
        
        /* 论文详情区域样式 */
        .paper-section {
            background-color: #f9f9f9;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 20px;
        }
        
        /* 统计信息样式 */
        .stats {
            background-color: #e8f4f8;
            padding: 15px;
            border-radius: 6px;
            margin-bottom: 20px;
        }
        
        /* 响应式设计 */
        @media (max-width: 768px) {
            .container {
                padding: 20px;
            }
            
            h1 {
                font-size: 24px;
            }
            
            h2 {
                font-size: 20px;
            }
            
            h3 {
                font-size: 18px;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <h1><img src="https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo.png" width="30"/> Hugging Face 2026-01-23 论文日报</h1>
<h2>📊 今日论文统计</h2>
<ul>
<li>总论文数：27</li>
<li>热门领域：RL, LLM, GPT, Audio, Vision</li>
</ul>
<h2>📝 论文详情</h2>
<h3>1. EvoCUA：通过可扩展合成经验学习进化的计算机使用智能体</h3>
<p><strong>原文标题：</strong> EvoCUA: Evolving Computer Use Agents via Learning from Scalable Synthetic Experience</p>
<p><strong>摘要：</strong>
原生计算机使用智能体（CUA）的发展代表了多模态人工智能领域的重大飞跃。然而，其潜力目前受限于静态数据扩展的约束。现有范式主要依赖对静态数据集的被动模仿，难以捕捉长周期计算机任务中固有的复杂因果动态。本研究提出EvoCUA，一种原生计算机使用智能体模型。与静态模仿不同，EvoCUA将数据生成与策略优化整合为一个自我维持的进化循环。为缓解数据稀缺问题，我们开发了一种可验证的合成引擎，能够自主生成多样化任务并配备可执行的验证器。为实现大规模经验获取，我们设计了可扩展的基础设施，可协调数万个异步沙箱推演。基于这些海量轨迹数据，我们提出一种迭代进化学习策略，以高效内化这些经验。该机制通过识别能力边界动态调节策略更新——强化成功的行为模式，同时通过错误分析与自我修正将失败轨迹转化为丰富的监督信号。在OSWorld基准测试中的实证评估表明，EvoCUA实现了56.7%的成功率，创造了开源模型的新标杆。值得注意的是，EvoCUA显著超越先前最佳开源模型OpenCUA-72B（45.0%），并优于UI-TARS-2（53.1%）等领先的闭源权重模型。关键的是，我们的结果验证了该方法的泛化能力：这种通过经验学习驱动的进化范式，在不同规模的基础模型上均能产生持续的性能提升，为推进原生智能体能力建立了稳健且可扩展的发展路径。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.15876">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.15876">arXiv</a></p>
<hr />
<h3>2. 灵活性陷阱：为何任意顺序生成会限制扩散语言模型的推理潜能</h3>
<p><strong>原文标题：</strong> The Flexibility Trap: Why Arbitrary Order Limits Reasoning Potential in Diffusion Language Models</p>
<p><strong>摘要：</strong>
扩散大语言模型突破了传统大语言模型严格的从左到右生成约束，实现了按任意顺序生成词元。直观而言，这种灵活性意味着其解空间严格包含了固定自回归轨迹，理论上为数学与代码等通用任务解锁了更卓越的推理潜能。因此，众多研究尝试通过强化学习来激发扩散大语言模型的推理能力。本文揭示了一个反直觉的现实：当前形式的任意顺序生成非但未能拓展、反而收窄了扩散大语言模型的推理边界。我们发现扩散大语言模型倾向于利用顺序灵活性来规避对探索至关重要的高不确定性词元，导致解空间过早坍缩。这一发现挑战了现有扩散大语言模型强化学习方法的基本前提——这些方法往往为维持顺序灵活性而付出高昂代价，例如处理组合爆炸的轨迹路径与难以计算的似然概率。我们证明，通过主动放弃任意顺序生成并采用标准的分组相对策略优化方法，能更有效地激发模型的推理能力。我们提出的JustGRPO方法简洁却效果显著（如在GSM8K数据集上达到89.1%准确率），同时完整保留了扩散大语言模型的并行解码能力。项目页面：https://nzl-thu.github.io/the-flexibility-trap</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.15165">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.15165">arXiv</a></p>
<hr />
<h3>3. HERMES：将KV缓存作为层次化内存以实现高效流式视频理解</h3>
<p><strong>原文标题：</strong> HERMES: KV Cache as Hierarchical Memory for Efficient Streaming Video Understanding</p>
<p><strong>摘要：</strong>
多模态大语言模型（MLLMs）的最新进展显著提升了离线视频理解能力。然而，将其扩展至流式视频输入仍面临挑战，现有模型难以同时保持稳定的理解性能、实时响应能力以及较低的GPU内存开销。为应对这一挑战，本文提出HERMES——一种无需训练的新型架构，用于实时、准确地理解视频流。基于对注意力机制的机理研究，我们将KV缓存概念化为一个层次化内存框架，该框架能够在多粒度上封装视频信息。在推理过程中，HERMES通过复用紧凑的KV缓存，实现在资源受限条件下的高效流式理解。值得注意的是，HERMES在用户查询到达时无需额外计算，从而保证了连续视频流交互的实时响应，其首词生成时间较先前最优方法提升了10倍。即使在视频令牌数量相比均匀采样减少高达68%的情况下，HERMES在所有基准测试中仍取得优于或可媲美的准确率，在流式数据集上最高可获得11.4%的性能提升。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.14724">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.14724">arXiv</a></p>
<hr />
<h3>4. BayesianVLA：基于潜在动作查询的视觉-语言-动作模型贝叶斯分解</h3>
<p><strong>原文标题：</strong> BayesianVLA: Bayesian Decomposition of Vision Language Action Models via Latent Action Queries</p>
<p><strong>摘要：</strong>
视觉-语言-动作模型在机器人操作任务中展现出潜力，但其在新指令或复杂多任务场景中的泛化能力往往不足。我们发现当前训练范式存在一个关键缺陷：目标驱动的数据收集会导致数据集偏差。在此类数据集中，仅凭视觉观察即可高度预测语言指令，致使指令与动作之间的条件互信息趋于消失，我们将此现象称为“信息坍缩”。其结果是，模型退化为仅依赖视觉的策略，忽略了语言约束，并在分布外场景中失效。为解决此问题，我们提出BayesianVLA，一种通过贝叶斯分解来强化指令跟随的新颖框架。通过引入可学习的潜在动作查询，我们构建了一个双分支架构，分别估计仅基于视觉的先验分布 p(a|v) 和基于语言条件的后验分布 π(a|v, ℓ)。随后，我们通过优化策略来最大化动作与指令之间的条件点互信息。该目标有效抑制了视觉捷径，并奖励那些能显式解释语言指令的动作。在不需新数据的情况下，BayesianVLA显著提升了泛化性能。在SimplerEnv和RoboCasa上的大量实验证明了其显著优势，其中在具有挑战性的分布外基准测试SimplerEnv上实现了11.3%的性能提升，验证了本方法在动作中稳健地关联语言的能力。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.15197">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.15197">arXiv</a></p>
<hr />
<h3>5. 沙盒环境中的大语言模型激发通用智能体智能</h3>
<p><strong>原文标题：</strong> LLM-in-Sandbox Elicits General Agentic Intelligence</p>
<p><strong>摘要：</strong>
本文提出“沙盒环境中的大语言模型”框架，使大语言模型能够在代码沙盒（即虚拟计算机）中进行探索，从而激发其在非代码领域的通用智能。我们首先证明，未经额外训练的强性能大语言模型展现出利用代码沙盒处理非代码任务的泛化能力。例如，大语言模型能够自主访问外部资源获取新知识，利用文件系统处理长文本语境，并执行脚本以满足格式要求。进一步研究表明，通过“沙盒环境中的大语言模型强化学习”方法，仅使用非智能体数据训练模型进行沙盒探索，即可增强这些智能体能力。实验表明，无论是否经过训练，该框架在数学、物理、化学、生物医学、长文本理解及指令遵循等领域均展现出稳健的泛化性能。最后，我们从计算与系统视角分析了该框架的运行效率，并将其开源为Python工具包以促进实际应用部署。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.16206">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.16206">arXiv</a></p>
<hr />
<h3>6. 基于表征自编码器的文本到图像扩散变换器规模化研究</h3>
<p><strong>原文标题：</strong> Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders</p>
<p><strong>摘要：</strong>
表征自编码器通过在高层语义潜在空间中训练，已在ImageNet的扩散建模中展现出独特优势。本研究探讨该框架能否扩展至大规模自由格式的文本到图像生成任务。我们首先在冻结的表征编码器（SigLIP-2）基础上，通过融合网络数据、合成数据及文本渲染数据进行解码器规模化训练，发现虽然规模扩展能提升整体保真度，但针对文本等特定领域仍需精细化的数据组合策略。随后，我们对原为ImageNet设计的RAE架构选择进行了严格压力测试。分析表明：规模化使框架显著简化——维度依赖的噪声调度机制仍至关重要，而宽扩散头结构、噪声增强解码等复杂设计在大规模场景下收益微乎其微。基于此简化框架，我们在0.5B至9.8B参数规模的扩散变换器上，对RAE与前沿的FLUX VAE进行了系统对比。实验表明：在所有模型规模下，RAE在预训练阶段始终优于VAE；在高质量数据集微调阶段，基于VAE的模型在64轮训练后出现灾难性过拟合，而RAE模型在256轮训练中保持稳定且性能持续更优。所有实验均证明，基于RAE的扩散模型具有更快的收敛速度和更优的生成质量，表明RAE比VAE更适合作为大规模文本到图像生成的简捷而强大的基础架构。此外，由于视觉理解与生成可在共享表征空间中协同运作，多模态模型能直接对生成潜在表示进行推理，这为构建统一模型开辟了新路径。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.16208">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.16208">arXiv</a></p>
<hr />
<h3>7. Stable-DiffCoder：推进代码扩散大语言模型的前沿研究</h3>
<p><strong>原文标题：</strong> Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model</p>
<p><strong>摘要：</strong>
与自回归模型相比，基于扩散的语言模型具有非顺序的块级生成能力和更丰富的数据复用优势，然而在同等计算资源下，现有代码扩散模型仍落后于强大的自回归基线模型。本研究通过受控实验重新审视这一设定，提出了Stable-DiffCoder——一种复用Seed-Coder架构、数据与训练流程的块扩散代码模型。为实现高效知识学习和稳定训练，我们引入了块扩散持续预训练阶段，该阶段通过定制化的预热策略和块级截断噪声调度进行增强。在相同数据与架构条件下，Stable-DiffCoder在广泛的代码基准测试中整体表现优于其自回归对照模型。仅通过持续预训练和监督微调阶段，该模型即能超越多种约80亿参数的自回归与扩散模型，证明基于扩散的训练方法能够单独提升代码建模质量。此外，基于扩散的任意顺序建模能力提升了代码编辑与推理任务中的结构化建模性能，并通过数据增强机制有效促进了低资源编程语言的发展。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.15892">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.15892">arXiv</a></p>
<hr />
<h3>8. SAMTok：用两个词表示任意掩码</h3>
<p><strong>原文标题：</strong> SAMTok: Representing Any Mask with Two Words</p>
<p><strong>摘要：</strong>
像素级能力对于构建交互式智能系统至关重要。然而，由于复杂的区域级编码器、专业化的分割解码器以及相互冲突的训练目标，像素级多模态大语言模型（MLLMs）的扩展仍然面临困难。为应对这些挑战，我们提出了SAMTok——一种离散掩码分词器，它能够将任意区域掩码转换为两个特殊标记，并以高保真度利用这些标记重建掩码。通过将掩码视为新的语言标记，SAMTok使得基础MLLMs（例如QwenVL系列）能够通过标准的下一个标记预测和简单的强化学习来掌握像素级能力，而无需修改模型架构或设计专门的损失函数。SAMTok基于SAM2构建，利用掩码编码器和残差向量量化器在2.09亿个多样化掩码上进行训练，以生成离散、紧凑且信息丰富的标记。通过使用500万个SAMTok格式的掩码理解与生成数据样本，QwenVL-SAMTok在区域描述、区域视觉问答、指代对话、指代分割、场景图解析以及多轮交互式分割任务上取得了最先进或可比的结果。我们进一步引入了一种文本答案匹配奖励机制，使得掩码生成的强化学习能够高效进行，在GRES和GCG基准测试中带来了显著提升。我们的研究结果展示了一种可扩展且简洁的范式，能够为MLLMs赋予强大的像素级能力。代码与模型均已开源。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.16093">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.16093">arXiv</a></p>
<hr />
<h3>9. 测试时学习发现方法研究</h3>
<p><strong>原文标题：</strong> Learning to Discover at Test Time</p>
<p><strong>摘要：</strong>
如何利用人工智能探索科学问题的新最优解？现有测试时扩展研究（如AlphaEvolve）通过调用冻结的大型语言模型进行搜索。本研究在测试时实施强化学习，使大型语言模型能够持续训练，并针对特定测试问题积累经验。这种持续学习形式具有特殊性：其目标在于产生单一优质解而非平均意义上的多个可行解，且专注于解决当前特定问题而非泛化至其他问题。因此，我们的学习目标与搜索子程序均以优先挖掘最具潜力的解决方案为核心设计原则。该方法被命名为"测试时训练发现法"。延续既有研究范式，我们聚焦于具有连续奖励机制的问题领域，并在数学、GPU内核工程、算法设计与生物学四大领域全面报告实验结果。测试时训练发现法在几乎所有领域均实现了当前最优性能：（1）埃尔德什最小重叠问题与自相关不等式证明；（2）GPUMode内核竞赛（较现有最优方案提速达2倍）；（3）历史AtCoder算法竞赛；（4）单细胞分析中的去噪问题。所有解决方案均经领域专家或竞赛组委会审核认证。值得注意的是，本研究全部成果均基于开源模型OpenAI gpt-oss-120b实现，并可通过公开代码复现，而此前最优结果均依赖封闭前沿模型达成。测试时训练过程通过Thinking Machines公司的Tinker API平台运行，单问题解决成本仅数百美元。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.16175">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.16175">arXiv</a></p>
<hr />
<h3>10. Qwen3-TTS技术报告</h3>
<p><strong>原文标题：</strong> Qwen3-TTS Technical Report</p>
<p><strong>摘要：</strong>
本报告介绍了Qwen3-TTS系列模型——一个具备多语言、可控性、鲁棒性与流式合成能力的先进文本转语音模型家族。Qwen3-TTS支持业界领先的3秒语音克隆与基于描述的语音控制技术，既能生成全新的语音身份，也能对输出语音进行细粒度调控。该模型基于超过500万小时、覆盖10种语言的语音数据训练，采用双轨语言模型架构实现实时合成，并配备两种语音分词器：1）Qwen-TTS-Tokenizer-25Hz是专注于语义内容的单码本编解码器，可与Qwen-Audio无缝集成，并通过分块扩散变换器实现流式波形重建；2）Qwen-TTS-Tokenizer-12Hz通过12.5Hz频率的16层多码本设计与轻量因果卷积网络，实现了极致的比特率压缩与超低延迟流式合成，支持首包97毫秒即时响应。大量实验表明，该系列模型在多项主客观评测基准（如TTS多语言测试集、InstructTTSEval及长语音测试集）中均达到业界最优性能。为促进社区研究与发展，我们已依据Apache 2.0协议开源全部分词器与模型。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.15621">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.15621">arXiv</a></p>
<hr />
<h3>11. Terminal-Bench：基于命令行界面的高难度现实任务智能体基准测试</h3>
<p><strong>原文标题：</strong> Terminal-Bench: Benchmarking Agents on Hard, Realistic Tasks in Command Line Interfaces</p>
<p><strong>摘要：</strong>
人工智能智能体有望在不久的将来，在多个领域自主完成具有长期价值的复杂任务。然而，现有基准测试要么未能有效衡量真实世界任务，要么难度不足，难以对前沿模型进行有意义的评估。为此，我们提出了Terminal-Bench 2.0：一个精心构建的高难度基准测试集，包含89个基于真实工作流程设计的计算机终端环境任务。每个任务均设有独立的环境配置、人工编写的解决方案以及用于验证的全面测试集。实验表明，当前前沿模型与智能体在该基准上的得分低于65%，我们进一步通过错误分析指出了模型与智能体需改进的方向。为支持后续研究与开发，我们在https://www.tbench.ai/ 公开了完整数据集及评估工具。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.11868">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.11868">arXiv</a></p>
<hr />
<h3>12. 重新审视组合图像检索评估：基于图像编辑的细粒度基准</h3>
<p><strong>原文标题：</strong> Rethinking Composed Image Retrieval Evaluation: A Fine-Grained Benchmark from Image Editing</p>
<p><strong>摘要：</strong>
组合图像检索是多模态理解领域一项关键且复杂的任务。当前主流的组合图像检索基准通常存在查询类别有限的问题，难以反映现实应用场景的多样化需求。为弥补这一评估缺口，本研究利用图像编辑技术实现对修改类型与内容的精确控制，构建了一个能够跨广泛类别合成查询的自动化流程。基于此流程，我们提出了EDIR——一个新颖的细粒度组合图像检索基准。EDIR包含5,000个高质量查询，这些查询按照5个主类别和15个子类别进行系统化组织。通过对13个多模态嵌入模型的全面评估，我们发现现有模型存在显著的能力缺陷：即使是当前最先进的模型（如RzenEmbed和GME）也难以在所有子类别上保持稳定性能，这凸显了我们基准的严格性。通过对比分析，我们进一步揭示了现有基准存在的固有局限，例如模态偏差与类别覆盖不足等问题。此外，领域内训练实验验证了我们基准的可行性，该实验通过区分“可通过针对性数据解决”与“暴露当前模型架构固有缺陷”的类别，进一步厘清了任务挑战的本质。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.16125">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.16125">arXiv</a></p>
<hr />
<h3>13. OpenVision 3：一个用于理解与生成的统一视觉编码器家族</h3>
<p><strong>原文标题：</strong> OpenVision 3: A Family of Unified Visual Encoder for Both Understanding and Generation</p>
<p><strong>摘要：</strong>
本文提出了一种先进视觉编码器家族，命名为OpenVision 3，其学习单一、统一的视觉表示，可同时服务于图像理解与图像生成任务。我们的核心架构简洁明了：将VAE压缩的图像潜变量输入ViT编码器，并训练其输出以支持两种互补功能。首先，编码器输出被传递至ViT-VAE解码器以重建原始图像，促使表示捕捉生成结构；其次，同一表示通过对比学习与图像描述目标进行优化，以增强语义特征。通过在共享潜空间中联合优化重建驱动与语义驱动的信号，该编码器学习到的表示能够协同作用，并在两种任务范式中均展现出良好的泛化能力。我们通过大量下游评估（编码器参数冻结）验证了这一统一设计的有效性。在多模态理解方面，我们将该编码器嵌入LLaVA-1.5框架：其性能与标准CLIP视觉编码器相当（例如，在SeedBench上得分为62.4对62.2，在POPE上为83.7对82.9）。在生成任务中，我们在RAE框架下进行测试：本方法显著优于基于标准CLIP的编码器（例如，在ImageNet上的gFID指标为1.89对2.54）。我们希望这项工作能够推动统一建模方向的未来研究。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.15369">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.15369">arXiv</a></p>
<hr />
<h3>14. 迈向大语言模型时代的自动化内核生成</h3>
<p><strong>原文标题：</strong> Towards Automated Kernel Generation in the Era of LLMs</p>
<p><strong>摘要：</strong>
现代人工智能系统的性能从根本上受限于其底层内核的质量，这些内核将高级算法语义转化为底层硬件操作。实现接近最优的内核需要对硬件架构和编程模型具备专家级理解，这使得内核工程成为一个关键但众所周知耗时且难以规模化的工作。近期，大语言模型（LLMs）及基于LLM的智能体在自动生成和优化内核方面展现出新的可能性。LLM非常适合压缩那些难以形式化的专家级内核知识，而智能体系统通过将内核开发构建为一个迭代的、反馈驱动的循环，进一步实现了可扩展的优化。该领域已取得快速进展，但目前研究仍较为零散，缺乏针对LLM驱动内核生成的系统性视角。本综述旨在填补这一空白，通过结构化梳理现有方法（涵盖基于LLM的路径与智能体优化工作流），并系统汇编支撑该领域学习与评估的数据集和基准测试，为相关研究提供系统概览。此外，本文进一步概述了关键的开放挑战与未来研究方向，旨在为下一代自动化内核优化建立全面的参考。为持续追踪该领域进展，我们在 https://github.com/flagos-ai/awesome-LLM-driven-kernel-generation 维护了一个开源GitHub仓库。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.15727">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.15727">arXiv</a></p>
<hr />
<h3>15. PROGRESSLM：迈向视觉语言模型中的进度推理</h3>
<p><strong>原文标题：</strong> PROGRESSLM: Towards Progress Reasoning in Vision-Language Models</p>
<p><strong>摘要：</strong>
估计任务进度需要对长时程动态进行推理，而非仅仅识别静态视觉内容。尽管现代视觉语言模型（VLMs）在描述可见内容方面表现出色，但它们能否从部分观察中推断任务已进展到何种程度，目前尚不明确。为此，我们提出了Progress-Bench，这是一个用于系统评估VLMs中进度推理能力的基准。除了基准测试外，我们进一步探索了一种受人类启发的两阶段进度推理范式，包括基于无训练提示的方法和基于精心构建的数据集ProgressLM-45K的有训练方法。在14个VLM上的实验表明，大多数模型尚未准备好进行任务进度估计，它们对演示模态和视角变化表现出敏感性，且在处理不可回答的情况时表现不佳。尽管强制执行结构化进度推理的无训练提示方法带来的提升有限且依赖于模型，而有训练的ProgressLM-3B即使在小规模模型下也实现了持续改进，尽管其训练任务集与评估任务集完全不相交。进一步的分析揭示了典型的错误模式，并阐明了进度推理何时以及为何成功或失败。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.15224">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.15224">arXiv</a></p>
<hr />
<h3>16. VideoMaMa：基于生成先验的掩码引导视频抠图方法</h3>
<p><strong>原文标题：</strong> VideoMaMa: Mask-Guided Video Matting via Generative Prior</p>
<p><strong>摘要：</strong>
由于标注数据稀缺，将视频抠图模型泛化至真实世界视频仍面临重大挑战。为此，我们提出视频掩码转蒙版模型（VideoMaMa），该模型通过利用预训练的视频扩散模型，将粗粒度分割掩码转换为像素级精确的透明度蒙版。尽管仅使用合成数据进行训练，VideoMaMa在真实世界影像上展现出强大的零样本泛化能力。基于此能力，我们开发了可扩展的大规模视频抠图伪标注流程，并构建了“视频任意抠图”（MA-V）数据集。该数据集为超过5万段涵盖多样场景与动态的真实世界视频提供了高质量的抠图标注。为验证该数据集的有效性，我们在MA-V上对SAM2模型进行微调，得到SAM2-Matte模型。相较于基于现有抠图数据集训练的同类模型，该模型在真实场景视频的鲁棒性方面表现更优。这些发现凸显了大规模伪标注视频抠图数据的重要性，并展示了生成先验与易获取的分割线索如何推动视频抠图研究实现可扩展的进展。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.14255">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.14255">arXiv</a></p>
<hr />
<h3>17. 360Anything：基于无几何约束的图像与视频升维至360度全景</h3>
<p><strong>原文标题：</strong> 360Anything: Geometry-Free Lifting of Images and Videos to 360°</p>
<p><strong>摘要：</strong>
将透视图像与视频升维至360度全景可实现沉浸式三维场景生成。现有方法通常依赖于透视图像与等距柱状投影空间之间的显式几何对齐，但这需要已知相机元数据，限制了该方法在缺乏准确校准信息的真实场景数据中的应用。本文提出360Anything——一个基于预训练扩散变换器的无几何约束框架。通过将透视输入与全景目标简化为令牌序列，360Anything以纯数据驱动的方式学习透视到等距柱状投影的映射关系，无需依赖相机信息。我们的方法在图像与视频的透视转360度生成任务中均达到最先进性能，其表现优于使用真实相机信息的现有方法。同时，我们追踪了等距柱状投影边界处接缝伪影的根源，将其归因于VAE编码器中的零填充操作，并引入环形潜在编码以实现无缝生成。最后，我们在零样本相机视场角与方向估计基准测试中展示了具有竞争力的结果，证明了360Anything对几何关系的深度理解及其在计算机视觉任务中的广泛适用性。更多结果详见https://360anything.github.io/。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.16192">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.16192">arXiv</a></p>
<hr />
<h3>18. Cosmos策略：面向视觉运动控制与规划的视觉模型微调方法</h3>
<p><strong>原文标题：</strong> Cosmos Policy: Fine-Tuning Video Models for Visuomotor Control and Planning</p>
<p><strong>摘要：</strong>
近期视频生成模型展现出捕捉复杂物理交互与时序场景演变的卓越能力。为利用其时空先验知识，机器人研究领域已尝试将视频模型应用于策略学习，但传统方法需通过多阶段后训练及新增动作生成架构组件，引入了显著复杂性。本研究提出Cosmos策略，这是一种通过单阶段后训练将大型预训练视频模型（Cosmos-Predict2）适配为高效机器人策略的简洁方法。该方法仅需在目标平台采集的机器人演示数据上进行训练，无需修改模型架构。Cosmos策略通过学习直接生成编码为视频模型潜在扩散过程中隐帧的机器人动作，充分利用模型的预训练先验知识与核心学习算法来捕捉复杂的动作分布。此外，该方法还能生成未来状态图像与价值函数（预期累积奖励），这些信息同样以隐帧形式编码，从而支持在测试阶段规划更高成功概率的动作轨迹。实验评估表明，Cosmos策略在LIBERO与RoboCasa仿真基准测试中分别达到98.5%与67.1%的平均成功率，取得最先进性能；在具有挑战性的真实世界双手操作任务中获得最高平均分，其表现优于从头训练的强扩散策略、基于视频模型的策略，以及在相同机器人演示数据上微调的先进视觉-语言-动作模型。进一步地，基于策略推演数据，Cosmos策略能够通过经验学习优化其世界模型与价值函数，并借助基于模型的规划在复杂任务中实现更高的成功率。相关代码、模型及训练数据已发布于https://research.nvidia.com/labs/dir/cosmos-policy/。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.16163">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.16163">arXiv</a></p>
<hr />
<h3>19. ActionMesh：基于时序三维扩散的动画三维网格生成</h3>
<p><strong>原文标题：</strong> ActionMesh: Animated 3D Mesh Generation with Temporal 3D Diffusion</p>
<p><strong>摘要：</strong>
生成动态三维对象是众多应用的核心任务，然而现有先进方法常因设定局限、计算耗时或质量不足而难以实际应用。本文提出ActionMesh——一种以前馈方式生成可直接投入生产的动态三维网格的生成模型。受早期视频模型启发，我们的核心思路是通过引入时间轴改进现有三维扩散模型，构建出“时序三维扩散”框架。具体而言，首先改进三维扩散阶段以生成表征时序变化且相互独立的三维形状序列隐变量；其次设计时序三维自编码器，将独立形状序列转化为预定义参考形状的对应形变，从而构建完整动画。通过整合这两个组件，ActionMesh能够从单目视频、文本描述甚至结合动画文本提示的三维网格等多种输入生成动态三维网格。相较于既有方法，本方案具有速度快、无需骨骼绑定且保持拓扑一致性的优势，支持快速迭代并兼容纹理映射与重定向等无缝应用。我们在标准视频转4D基准数据集（Consistent4D、Objaverse）上评估模型，在几何精度与时序一致性方面均达到最先进性能，证明本模型能够以前所未有的速度与质量生成动态三维网格。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.16148">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.16148">arXiv</a></p>
<hr />
<h3>20. VIOLA：面向最小标注的视频上下文学习框架</h3>
<p><strong>原文标题：</strong> VIOLA: Towards Video In-Context Learning with Minimal Annotations</p>
<p><strong>摘要：</strong>
将多模态大语言模型（MLLMs）泛化至新兴视频领域对于实际部署至关重要，但由于标注数据稀缺，这一目标仍面临挑战。尽管上下文学习（ICL）提供了一种免训练的适应路径，但传统方法依赖于大规模标注数据池，这在工业或手术等专业场景中往往不切实际，因为这些场景需要专家进行标注。为弥补这一差距，我们提出了VIOLA（基于最小标注的视频上下文学习框架），这是一个标签高效的框架，它将最小化的专家监督与丰富的未标注数据协同结合。首先，为在严格标注预算下最大化效率，我们提出了密度-不确定性加权采样方法。与可能选择视觉异常样本的传统多样性或不确定性策略不同，我们的方法利用密度估计来识别同时具备多样性、代表性和信息量的样本。其次，为在不引入噪声传播的前提下利用剩余未标注数据，我们构建了混合数据池，并引入了置信度感知检索与置信度感知提示机制。这些机制显式建模标签可靠性，基于相似度与置信度的复合分数检索示例，同时使MLLM能够自适应地区分已验证的真实标签与噪声伪标签。通过在九个多样化基准测试中使用四种MLLM进行广泛实验，结果表明我们的框架在低资源设置下显著优于多种基线方法，实现了以最小标注成本达成鲁棒适应的目标。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.15549">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.15549">arXiv</a></p>
<hr />
<h3>21. 智能体置信度校准</h3>
<p><strong>原文标题：</strong> Agentic Confidence Calibration</p>
<p><strong>摘要：</strong>
人工智能智能体正从被动的语言模型快速演变为能够执行复杂多步骤任务的自主系统。然而，其在失败场景中的过度自信仍然是高风险场景部署的根本障碍。现有针对静态单轮输出的校准方法无法应对智能体系统的独特挑战，例如轨迹中的误差累积、外部工具的不确定性以及不透明的故障模式。为应对这些挑战，本文首次提出智能体置信度校准问题，并引入整体轨迹校准框架——一种创新的诊断框架，能够从智能体完整轨迹中提取从宏观动态到微观稳定性的丰富过程级特征。该框架基于简单可解释的模型构建，在八项基准测试、多种大语言模型及不同智能体框架中，其校准性能与判别能力均持续超越现有强基线方法。除性能优势外，该框架实现了三项重要突破：通过揭示故障背后的信号提供可解释性；无需重新训练即可跨领域应用实现可迁移性；通过通用智能体校准器实现泛化能力，该校准器在跨域GAIA基准测试中取得了最佳校准效果（最低预期校准误差）。这些成果共同建立了以过程为中心的置信度校准新范式，为诊断和提升人工智能智能体的可靠性提供了系统性框架。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.15778">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.15778">arXiv</a></p>
<hr />
<h3>22. 能动性不确定性量化</h3>
<p><strong>原文标题：</strong> Agentic Uncertainty Quantification</p>
<p><strong>摘要：</strong>
尽管人工智能智能体在长程推理任务中展现出卓越能力，但其可靠性仍深受“幻觉螺旋”现象的制约——早期认知误差会不可逆地持续扩散。现有方法面临两难困境：不确定性量化方法通常作为被动传感器，仅能诊断风险而无法主动干预；而自我反思机制则易陷入持续或盲目的修正循环。为弥合这一鸿沟，本文提出一种统一的双过程能动性不确定性量化框架，将语言化不确定性转化为主动双向控制信号。该架构包含两个互补机制：系统一（不确定性感知记忆模块）通过隐式传播语言化置信度与语义解释来避免盲目决策；系统二（不确定性感知反思模块）则将这些解释作为理性线索，仅在必要时触发定向推理时解析。这种设计使智能体能够动态平衡高效执行与深度思辨。在闭环基准测试与开放式深度研究任务上的大量实验表明，我们这种无需训练的方法实现了卓越的性能与轨迹级校准效果。我们相信这一原则性框架标志着向可信智能体迈出了重要一步。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.15703">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.15703">arXiv</a></p>
<hr />
<h3>23. 从被动度量到主动信号：不确定性量化在大语言模型中的角色演变</h3>
<p><strong>原文标题：</strong> From Passive Metric to Active Signal: The Evolving Role of Uncertainty Quantification in Large Language Models</p>
<p><strong>摘要：</strong>
尽管大语言模型展现出卓越的能力，但其不可靠性仍是部署于高风险领域的关键障碍。本综述系统阐述了应对这一挑战的功能演进路径：不确定性从被动的诊断度量演变为引导实时模型行为的主动控制信号。我们展示了不确定性如何作为主动控制信号在三大前沿领域发挥作用：在高级推理中优化计算并触发自我修正；在自主智能体中调控工具使用与信息寻求的元认知决策；在强化学习中抑制奖励攻击并通过内在奖励实现自我改进。通过将这些进展锚定于贝叶斯方法和共形预测等新兴理论框架，我们为这一变革性趋势提供了统一视角。本综述提供了全面的概览、批判性分析及实用设计模式，论证了掌握不确定性的新范式对于构建下一代可扩展、可靠且可信的人工智能至关重要。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.15690">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.15690">arXiv</a></p>
<hr />
<h3>24. 教育应用中的大语言模型提示评估研究</h3>
<p><strong>原文标题：</strong> LLM Prompt Evaluation for Educational Applications</p>
<p><strong>摘要：</strong>
随着大语言模型在教育应用中的日益普及，亟需基于实证的方法来设计和评估能够产生个性化且符合教学目标的提示语。本研究提出一种可推广的系统性提示评估方法，并通过分析结构化对话活动中大语言模型生成的后续问题进行实证演示。研究设计并测试了六种提示模板，这些模板融合了成熟的提示工程模式，每种提示侧重不同的教学策略。通过适用于各类教育场景的锦标赛式评估框架对提示模板进行比较，该框架采用Glicko2评分系统，由八位评委从格式规范性、对话支持度和学习者适配性三个维度对问题组进行评价。数据来源于三个独立教育场景中120组真实用户交互记录。结果显示，聚焦策略性阅读的单一提示模板在配对比较中以81%至100%的胜率显著优于其他模板。该提示融合了角色设定与语境管理模块，旨在支持自主学习等元认知学习策略。本方法论为教育技术研究者展示了如何系统评估并优化提示设计，推动教育应用从临时性提示工程向基于证据的提示开发范式转变。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.16134">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.16134">arXiv</a></p>
<hr />
<h3>25. 基于Numba加速的二维扩散限制聚集模拟：实现方法与分形表征</h3>
<p><strong>原文标题：</strong> Numba-Accelerated 2D Diffusion-Limited Aggregation: Implementation and Fractal Characterization</p>
<p><strong>摘要：</strong>
本文提出dla-ideal-solver——一个基于Numba加速Python的高性能二维扩散限制聚集模拟框架。通过即时编译技术，该框架在保持高级编程灵活性的同时，实现了与传统静态编译方案相当的计算吞吐量。我们系统研究了不同注入几何结构与行走粒子浓度下的拉普拉斯生长不稳定性。分析结果证实，在低浓度体系中标准分形维数D_f≈1.71具有稳健性，符合Witten-Sander普适性类别。然而，在高密度环境中我们观察到向类伊甸园致密生长模式（D_f≈1.87）的显著跨越现象，这归因于屏蔽长度的饱和效应。除标准质量-半径标度分析外，本研究采用广义Rényi维数与空隙度度量来量化聚集体的单分形特征与空间异质性。本工作为探索非平衡统计力学中的相变现象建立了一个可复现的开源测试平台。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.15440">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.15440">arXiv</a></p>
<hr />
<h3>26. MirrorBench：一个用于评估拟人化用户代理的可扩展框架</h3>
<p><strong>原文标题：</strong> MirrorBench: An Extensible Framework to Evaluate User-Proxy Agents for Human-Likeness</p>
<p><strong>摘要：</strong>
大语言模型正日益被用作人类模拟器，既用于评估对话系统，也用于生成微调数据。然而，简单的“扮演用户”提示往往会产生冗长、不真实的语句，这凸显了对所谓用户代理进行系统性评估的必要性。我们提出了MIRRORBENCH，这是一个可复现、可扩展的基准测试框架，其评估用户代理的唯一标准是它们在不同对话任务中生成拟人化用户语句的能力，并明确与下游任务的成功解耦。MIRRORBENCH采用模块化执行引擎，具备类型化接口、元数据驱动的注册机制、多后端支持、缓存功能和强大的可观测性。该系统支持可插拔的用户代理、数据集、任务和评估指标，使研究人员能够在统一且考虑方差影响的框架下评估任意模拟器。我们引入了三个词汇多样性指标（MATTR、YULE'S K和HD-D）和三个基于大语言模型评判的指标（GTEval、成对不可区分性和规则推理评估）。在四个开源数据集上的测试表明，MIRRORBENCH能够提供考虑方差的结果，并揭示用户代理与真实人类用户之间的系统性差距。该框架已开源，并包含一个简单的命令行界面，用于运行实验、管理配置与缓存以及生成报告。框架可通过 https://github.com/SAP/mirrorbench 访问。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.08118">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.08118">arXiv</a></p>
<hr />
<h3>27. 作为电路的维格纳之友：超导量子硬件上的分支间通信见证基准测试</h3>
<p><strong>原文标题：</strong> Wigner's Friend as a Circuit: Inter-Branch Communication Witness Benchmarks on Superconducting Quantum Hardware</p>
<p><strong>摘要：</strong>
我们在IBM量子硬件上实现并基准测试了Violaris提出的用于估计操作型分支间通信见证的电路族，该见证定义为由编译的维格纳之友式电路产生的经典测量记录中的关联性。我们将该协议的五量子比特实例实现为单个电路内的寄存器间消息传递模式（而非物理信号传输），并在实际设备噪声和编译约束下评估其行为。该电路编码了观察者子系统的分支条件演化，其动力学依赖于一个控制量子比特，随后通过受控传输操作来探测条件测量上下文之间的关联性。</p>
<p>在ibm_fez后端上执行20000次测量，我们观察到基于布居数的可见度为0.877，沿正交轴的相干性见证值分别为0.840和-0.811，相位敏感幅度约为1.17。虽然可见度度量对某些类型的退相不敏感，但相干性见证提供了对非对角噪声的互补敏感性。</p>
<p>本研究并非对量子力学解释进行检验或区分，而是提供了一个可复现的操作约束流程，用于评估非理想信道相对于校准设备噪声的可检测性。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.16004">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.16004">arXiv</a></p>
<hr />
<h2>🔍 关键词云图</h2>
<p><img alt="关键词云图" src="../images/keywords_wordcloud.png" /></p>
<h2>📈 近期论文趋势</h2>
<p><img alt="论文趋势" src="../images/daily_papers.png" /></p>
<h2>🎙️ 语音播报</h2>
<ul>
<li><a href="../audio/2026-01-23_daily_papers.mp3">收听今日论文解读</a></li>
</ul>
<h2>📱 订阅渠道</h2>
<ul>
<li>GitHub: <a href="https://github.com/2404589803/hf-daily-paper-newsletter-chinese">hf-daily-paper-newsletter-chinese</a></li>
</ul>
    </div>
</body>
</html>