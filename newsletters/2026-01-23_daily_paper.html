<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Hugging Face 论文日报 - 2026-01-23</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', sans-serif;
            line-height: 1.6;
            color: #333;
            background-color: #f5f5f5;
            padding: 20px;
        }
        
        .container {
            max-width: 900px;
            margin: 0 auto;
            background-color: white;
            padding: 40px;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }
        
        h1 {
            color: #2c3e50;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
            margin-bottom: 30px;
            font-size: 28px;
        }
        
        h1 img {
            vertical-align: middle;
            margin-right: 10px;
        }
        
        h2 {
            color: #34495e;
            margin-top: 40px;
            margin-bottom: 20px;
            font-size: 24px;
            border-left: 4px solid #3498db;
            padding-left: 15px;
        }
        
        h3 {
            color: #2c3e50;
            margin-top: 30px;
            margin-bottom: 15px;
            font-size: 20px;
        }
        
        p {
            margin-bottom: 15px;
            text-align: justify;
        }
        
        ul {
            margin-left: 20px;
            margin-bottom: 20px;
        }
        
        li {
            margin-bottom: 8px;
        }
        
        a {
            color: #3498db;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        strong {
            color: #2c3e50;
            font-weight: 600;
        }
        
        hr {
            border: none;
            border-top: 1px solid #e0e0e0;
            margin: 30px 0;
        }
        
        /* 关键修复:限制图片宽度 */
        img {
            max-width: 100%;
            height: auto;
            display: block;
            margin: 20px auto;
            border-radius: 4px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        
        /* 确保图片容器也有宽度限制 */
        p img {
            max-width: 100%;
        }
        
        /* 论文详情区域样式 */
        .paper-section {
            background-color: #f9f9f9;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 20px;
        }
        
        /* 统计信息样式 */
        .stats {
            background-color: #e8f4f8;
            padding: 15px;
            border-radius: 6px;
            margin-bottom: 20px;
        }
        
        /* 响应式设计 */
        @media (max-width: 768px) {
            .container {
                padding: 20px;
            }
            
            h1 {
                font-size: 24px;
            }
            
            h2 {
                font-size: 20px;
            }
            
            h3 {
                font-size: 18px;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <h1><img src="https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo.png" width="30"/> Hugging Face 2026-01-23 论文日报</h1>
<h2>📊 今日论文统计</h2>
<ul>
<li>总论文数：27</li>
<li>热门领域：RL, LLM, GPT, Audio, Vision</li>
</ul>
<h2>📝 论文详情</h2>
<h3>1. EvoCUA：通过可扩展合成经验学习进化的计算机使用智能体</h3>
<p><strong>原文标题：</strong> EvoCUA: Evolving Computer Use Agents via Learning from Scalable Synthetic Experience</p>
<p><strong>摘要：</strong>
原生计算机使用智能体（CUA）的发展代表了多模态人工智能领域的重大飞跃。然而，其潜力目前受限于静态数据扩展的约束。现有范式主要依赖对静态数据集的被动模仿，难以捕捉长周期计算机任务中固有的复杂因果动态。本研究提出EvoCUA，一种原生计算机使用智能体模型。与静态模仿不同，EvoCUA将数据生成与策略优化整合为一个自我维持的进化循环。为缓解数据稀缺问题，我们开发了可验证的合成引擎，能够自主生成多样化任务并配备可执行验证器。为实现大规模经验获取，我们设计了可扩展的基础设施，可协调数万个异步沙箱推演。基于这些海量轨迹，我们提出迭代进化学习策略以高效内化经验。该机制通过识别能力边界动态调节策略更新——强化成功操作流程，同时通过错误分析与自我修正将失败轨迹转化为丰富的监督信号。在OSWorld基准测试中的实证评估表明，EvoCUA实现了56.7%的成功率，创造了开源模型的新标杆。值得注意的是，EvoCUA显著超越此前最佳开源模型OpenCUA-72B（45.0%），并优于UI-TARS-2（53.1%）等领先闭源权重模型。关键的是，我们的结果验证了该方法的泛化能力：这种由经验学习驱动的进化范式在不同规模的基础模型中均能产生持续的性能提升，为推进原生智能体能力建立了稳健且可扩展的发展路径。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.15876">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.15876">arXiv</a></p>
<hr />
<h3>2. 灵活性陷阱：为何任意顺序生成会限制扩散语言模型的推理潜能</h3>
<p><strong>原文标题：</strong> The Flexibility Trap: Why Arbitrary Order Limits Reasoning Potential in Diffusion Language Models</p>
<p><strong>摘要：</strong>
扩散大语言模型（dLLMs）打破了传统大语言模型严格的从左到右生成约束，允许以任意顺序生成标记。直观而言，这种灵活性意味着其解空间严格包含了固定自回归轨迹，理论上为数学和编码等通用任务解锁了更优越的推理潜力。因此，众多研究尝试利用强化学习（RL）来激发dLLMs的推理能力。本文揭示了一个反直觉的现实：在当前形式下，任意顺序生成非但没有扩展dLLMs的推理边界，反而使其收窄。我们发现dLLMs倾向于利用这种顺序灵活性来规避对探索至关重要的高不确定性标记，导致解空间过早坍缩。这一观察挑战了现有dLLMs强化学习方法的前提——这些方法往往投入大量复杂度（如处理组合轨迹和难解似然）以保持顺序灵活性。我们证明，通过主动放弃任意顺序生成并采用标准组相对策略优化（GRPO），能更有效地激发推理能力。我们提出的JustGRPO方法简洁却效果显著（例如在GSM8K数据集上达到89.1%准确率），同时完全保留了dLLMs的并行解码能力。项目页面：https://nzl-thu.github.io/the-flexibility-trap</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.15165">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.15165">arXiv</a></p>
<hr />
<h3>3. HERMES：将KV缓存作为层次化内存以实现高效流式视频理解</h3>
<p><strong>原文标题：</strong> HERMES: KV Cache as Hierarchical Memory for Efficient Streaming Video Understanding</p>
<p><strong>摘要：</strong>
多模态大语言模型（MLLMs）的最新进展显著提升了离线视频理解能力。然而，将其扩展至流式视频输入仍面临挑战，现有模型难以同时保持稳定的理解性能、实时响应能力以及较低的GPU内存开销。为解决这一问题，我们提出了HERMES，一种无需训练的新型架构，用于实时且准确地理解视频流。基于对注意力机制的深入分析，我们将KV缓存概念化为一个层次化内存框架，该框架能够在多粒度上封装视频信息。在推理过程中，HERMES复用紧凑的KV缓存，从而在资源受限条件下实现高效的流式理解。值得注意的是，HERMES在用户查询到达时无需额外计算，确保了连续视频流交互的实时响应，其首词生成时间较先前最优方法提升了10倍。即使与均匀采样相比将视频令牌减少高达68%，HERMES在所有基准测试中仍实现了相当或更优的准确率，在流式数据集上最高可获得11.4%的性能提升。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.14724">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.14724">arXiv</a></p>
<hr />
<h3>4. BayesianVLA：基于潜在动作查询的视觉-语言-动作模型贝叶斯分解</h3>
<p><strong>原文标题：</strong> BayesianVLA: Bayesian Decomposition of Vision Language Action Models via Latent Action Queries</p>
<p><strong>摘要：</strong>
视觉-语言-动作（VLA）模型在机器人操作任务中展现出潜力，但常难以泛化至新指令或复杂的多任务场景。我们发现在当前训练范式中存在一个关键缺陷：目标驱动的数据收集会导致数据集偏差。在此类数据集中，仅凭视觉观察即可高度预测语言指令，导致指令与动作之间的条件互信息趋近于零，我们将此现象称为“信息坍缩”。其结果是模型退化为仅依赖视觉的策略，忽略语言约束，并在分布外（OOD）场景中失效。为解决此问题，我们提出BayesianVLA，一种通过贝叶斯分解强制遵循指令的新框架。通过引入可学习的潜在动作查询，我们构建了一个双分支架构，分别估计仅基于视觉的先验分布 p(a|v) 和基于语言条件的后验分布 π(a|v, ℓ)。随后，我们通过优化策略以最大化动作与指令之间的条件点互信息（PMI）。该目标能有效抑制视觉捷径，并奖励那些显式解释语言命令的动作。在不需新数据的情况下，BayesianVLA显著提升了泛化能力。在SimplerEnv和RoboCasa上的大量实验证明了其显著优势，其中在具有挑战性的OOD SimplerEnv基准上实现了11.3%的性能提升，验证了本方法在动作中稳健关联语言的能力。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.15197">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.15197">arXiv</a></p>
<hr />
<h3>5. 沙盒环境中的大语言模型激发通用智能体智能</h3>
<p><strong>原文标题：</strong> LLM-in-Sandbox Elicits General Agentic Intelligence</p>
<p><strong>摘要：</strong>
本文提出“沙盒环境中的大语言模型”框架，使大语言模型能够在代码沙盒（即虚拟计算机）中进行探索，从而激发其在非代码领域的通用智能。我们首先证明，未经额外训练的强性能大语言模型展现出利用代码沙盒处理非代码任务的泛化能力。例如，大语言模型能够自主访问外部资源以获取新知识，利用文件系统处理长上下文，并执行脚本来满足格式要求。进一步研究表明，通过“沙盒环境中的大语言模型强化学习”方法，仅使用非智能体数据训练模型进行沙盒探索，即可增强这些智能体能力。实验表明，该框架在免训练与后训练两种模式下，均实现了跨越数学、物理、化学、生物医学、长上下文理解及指令遵循等领域的稳健泛化性能。最后，我们从计算与系统视角分析了该框架的运行效率，并将其开源为Python软件包以促进实际应用部署。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.16206">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.16206">arXiv</a></p>
<hr />
<h3>6. 基于表征自编码器的文本到图像扩散变换器规模化研究</h3>
<p><strong>原文标题：</strong> Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders</p>
<p><strong>摘要：</strong>
表征自编码器通过在高层语义潜空间中进行训练，已在ImageNet的扩散建模中展现出独特优势。本研究探讨该框架能否扩展至大规模自由格式的文本到图像生成任务。我们首先基于冻结的表征编码器，通过融合网络数据、合成数据及文本渲染数据进行多维度训练，将RAE解码器规模扩展至超越ImageNet的范围。研究发现：虽然规模扩展能提升整体保真度，但针对文本等特定领域需采用定向数据组合策略。随后，我们对原为ImageNet设计的RAE架构选择进行了严格压力测试。分析表明：规模化使框架显著简化——尽管维度相关的噪声调度机制仍至关重要，但扩散头拓宽、噪声增强解码等复杂结构在大规模场景中收益甚微。基于此简化框架，我们在0.5B至9.8B参数规模的扩散变换器上，对RAE与最先进的FLUX VAE进行了对照实验。结果显示：在所有模型规模下，RAE在预训练阶段始终优于VAE；在高质量数据集微调阶段，基于VAE的模型在64轮训练后出现灾难性过拟合，而RAE模型在256轮训练中保持稳定且性能持续更优。所有实验均表明：基于RAE的扩散模型具有更快的收敛速度和更优的生成质量，证明RAE可作为比VAE更简洁、更强大的大规模文本到图像生成基础架构。此外，由于视觉理解与生成可在共享表征空间中协同运作，多模态模型能直接对生成潜空间进行推理，这为构建统一模型开辟了新路径。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.16208">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.16208">arXiv</a></p>
<hr />
<h3>7. Stable-DiffCoder：拓展代码扩散大语言模型的前沿</h3>
<p><strong>原文标题：</strong> Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model</p>
<p><strong>摘要：</strong>
与自回归模型相比，基于扩散的语言模型具备非顺序、分块生成以及更丰富的数据复用优势，然而在同等计算资源下，现有的代码扩散模型仍落后于强大的自回归基线。本研究通过受控实验重新审视这一设定，提出了Stable-DiffCoder——一种复用Seed-Coder架构、数据与训练流程的分块扩散代码模型。为实现高效知识学习与稳定训练，我们引入了结合定制化预热策略与分块裁剪噪声调度的扩散持续预训练阶段。在相同数据与架构条件下，Stable-DiffCoder在广泛的代码基准测试中整体优于其自回归对照模型。仅通过持续预训练与监督微调阶段，该模型即超越了多种约80亿参数的自回归与扩散模型，证明基于扩散的训练机制能够单独提升代码建模质量。此外，基于扩散的任意顺序建模能力增强了代码编辑与推理任务中的结构化建模效果，并通过数据增强机制提升了低资源编程语言的建模性能。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.15892">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.15892">arXiv</a></p>
<hr />
<h3>8. SAMTok：用两个词表示任意掩码</h3>
<p><strong>原文标题：</strong> SAMTok: Representing Any Mask with Two Words</p>
<p><strong>摘要：</strong>
像素级能力对于构建交互式智能系统至关重要。然而，由于复杂的区域级编码器、专业化的分割解码器以及不兼容的训练目标，像素级多模态大语言模型（MLLMs）的扩展仍然面临困难。为解决这些挑战，我们提出了SAMTok——一种离散掩码标记器，可将任意区域掩码转换为两个特殊标记，并利用这些标记高保真地重建掩码。通过将掩码视为新的语言标记，SAMTok使基础MLLMs（如QwenVL系列）能够通过标准的下一个标记预测和简单的强化学习来掌握像素级能力，而无需修改模型架构或设计专用损失函数。SAMTok基于SAM2构建，利用掩码编码器和残差向量量化器在2.09亿个多样化掩码上进行训练，以生成离散、紧凑且信息丰富的标记。通过使用500万个经SAMTok格式化的掩码理解与生成数据样本，QwenVL-SAMTok在区域描述、区域视觉问答、指代对话、指代分割、场景图解析以及多轮交互式分割任务中取得了领先或可比的结果。我们进一步引入了文本答案匹配奖励机制，实现了掩码生成的高效强化学习，在GRES和GCG基准测试中带来了显著提升。我们的研究结果展示了一种可扩展且简洁的范式，能够为MLLMs赋予强大的像素级能力。代码与模型均已开源。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.16093">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.16093">arXiv</a></p>
<hr />
<h3>9. 测试时学习发现：一种强化学习驱动的探索方法</h3>
<p><strong>原文标题：</strong> Learning to Discover at Test Time</p>
<p><strong>摘要：</strong>
如何利用人工智能为科学问题探索新的最优解？现有测试时扩展方法（如AlphaEvolve）通过提示冻结的大型语言模型进行搜索。本研究提出在测试时进行强化学习，使大型语言模型能够针对特定测试问题持续训练并积累经验。这种持续学习形式具有特殊性：其目标在于生成单一最优解而非平均意义上的多个可行解，且专注于解决当前问题而非泛化至其他问题。因此，我们设计了以最有潜力的解决方案为优先导向的学习目标与搜索子程序，并将该方法命名为“测试时训练发现法”。沿袭前人研究，我们聚焦于具有连续奖励的问题域，并在数学、GPU内核工程、算法设计和生物学领域对所有尝试的问题进行结果报告。测试时训练发现法在几乎所有问题上均实现了当前最优性能：（1）埃尔德什最小重叠问题与自相关不等式；（2）GPUMode内核竞赛（速度较现有技术提升最高达2倍）；（3）历史AtCoder算法竞赛；（4）单细胞分析中的去噪问题。所有解决方案均经过专家或主办方评审。与先前依赖封闭前沿模型的研究不同，本研究全部成果均基于开源模型OpenAI gpt-oss-120b实现，并可通过公开代码复现。测试时训练过程使用Thinking Machines公司的Tinker API完成，每个问题的计算成本仅为数百美元。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.16175">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.16175">arXiv</a></p>
<hr />
<h3>10. Qwen3-TTS技术报告</h3>
<p><strong>原文标题：</strong> Qwen3-TTS Technical Report</p>
<p><strong>摘要：</strong>
本报告介绍了Qwen3-TTS系列模型，这是一组先进的多语言、可控、鲁棒且支持流式合成的文本到语音模型。Qwen3-TTS支持业界领先的3秒语音克隆和基于描述的语音控制，既能生成全新的语音，也能对输出语音进行细粒度调控。该模型基于覆盖10种语言、总时长超过500万小时的语音数据训练而成，采用双轨语言模型架构实现实时合成，并配备两种语音分词器：1）Qwen-TTS-Tokenizer-25Hz是一种强调语义内容的单码本编解码器，可与Qwen-Audio无缝集成，并通过分块DiT实现流式波形重建；2）Qwen-TTS-Tokenizer-12Hz通过12.5Hz频率的16层多码本设计和轻量级因果卷积网络，实现了极致的比特率压缩与超低延迟流式合成，支持首包97毫秒即时发送。大量实验表明，该系列模型在多项客观与主观评测基准（如TTS多语言测试集、InstructTTSEval及长语音测试集）中均达到业界最优性能。为促进社区研究与开发，我们已将全部分词器与模型在Apache 2.0协议下开源发布。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.15621">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.15621">arXiv</a></p>
<hr />
<h3>11. Terminal-Bench：在命令行界面中对智能体执行困难现实任务的基准测试</h3>
<p><strong>原文标题：</strong> Terminal-Bench: Benchmarking Agents on Hard, Realistic Tasks in Command Line Interfaces</p>
<p><strong>摘要：</strong>
人工智能智能体可能很快将能够在不同领域中自主完成具有长期价值的任务。现有基准测试要么未能衡量真实世界任务，要么难度不足以有效评估前沿模型。为此，我们提出了Terminal-Bench 2.0：一个精心构建的困难基准测试集，包含89个受真实工作流程问题启发的计算机终端环境任务。每个任务均设有独立环境、人工编写的解决方案以及用于验证的综合测试。实验表明，前沿模型与智能体在该基准测试中的得分低于65%，我们通过错误分析指出了模型与智能体的改进方向。我们在https://www.tbench.ai/ 发布了完整数据集与评估框架，以助力开发者与研究人员开展后续工作。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.11868">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.11868">arXiv</a></p>
<hr />
<h3>12. 重新审视组合图像检索评估：基于图像编辑的细粒度基准</h3>
<p><strong>原文标题：</strong> Rethinking Composed Image Retrieval Evaluation: A Fine-Grained Benchmark from Image Editing</p>
<p><strong>摘要：</strong>
组合图像检索是多模态理解领域一项关键且复杂的任务。当前组合图像检索基准通常存在查询类别有限的问题，难以反映现实场景的多样化需求。为弥补这一评估缺口，本研究利用图像编辑技术实现对修改类型与内容的精确控制，构建了一个能够跨广泛类别合成查询的自动化流程。基于此流程，我们建立了EDIR——一个新颖的细粒度组合图像检索基准。EDIR包含5,000个高质量查询，其结构划分为5个主类别和15个子类别。通过对13个多模态嵌入模型的系统性评估，我们发现现有模型存在显著的能力缺陷：即使是最先进的模型（如RzenEmbed与GME）也难以在所有子类别中保持稳定性能，这印证了本基准的严格性。通过对比分析，我们进一步揭示了现有基准的内在局限性，包括模态偏见与类别覆盖不足等问题。此外，领域内训练实验验证了本基准的可行性。该实验通过区分“可通过针对性数据解决”与“暴露当前模型架构固有缺陷”的类别，进一步阐明了本任务面临的挑战。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.16125">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.16125">arXiv</a></p>
<hr />
<h3>13. OpenVision 3：兼具理解与生成能力的统一视觉编码器家族</h3>
<p><strong>原文标题：</strong> OpenVision 3: A Family of Unified Visual Encoder for Both Understanding and Generation</p>
<p><strong>摘要：</strong>
本文提出了一系列先进的视觉编码器，命名为OpenVision 3，其学习到一种单一、统一的视觉表征，能够同时服务于图像理解与图像生成任务。我们的核心架构简洁明了：将经过VAE压缩的图像潜在表示输入至ViT编码器，并训练其输出以支持两种互补的功能。首先，编码器的输出被传递至ViT-VAE解码器以重建原始图像，促使该表征捕捉生成式结构。其次，同一表征通过对比学习和图像描述目标进行优化，从而强化语义特征。通过在共享的潜在空间中联合优化重建驱动与语义驱动的信号，该编码器学习到的表征能够在两种任务范式中实现协同并展现出良好的泛化能力。我们通过在下游任务中广泛评估冻结后的编码器性能，验证了这一统一设计的有效性。在多模态理解方面，我们将该编码器嵌入LLaVA-1.5框架：其性能与标准的CLIP视觉编码器相当（例如，在SeedBench上为62.4对62.2，在POPE上为83.7对82.9）。在生成任务方面，我们在RAE框架下进行测试：本模型显著超越了基于标准CLIP的编码器（例如，在ImageNet上的gFID为1.89对2.54）。我们希望这项工作能够推动未来关于统一建模的研究。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.15369">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.15369">arXiv</a></p>
<hr />
<h3>14. 迈向大语言模型时代的自动化内核生成</h3>
<p><strong>原文标题：</strong> Towards Automated Kernel Generation in the Era of LLMs</p>
<p><strong>摘要：</strong>
现代人工智能系统的性能从根本上受限于其底层内核的质量，这些内核将高级算法语义转化为底层硬件操作。实现接近最优的内核需要专家级的硬件架构与编程模型理解能力，这使得内核工程成为关键但极其耗时且难以规模化的工作。近期，大语言模型（LLMs）及基于LLM的智能体技术为自动化内核生成与优化开辟了新路径。LLM能够有效整合难以形式化的专家级内核知识，而智能体系统通过将内核开发构建为迭代式、反馈驱动的循环，进一步实现了可扩展的优化。该领域已取得快速进展，但目前研究仍较为零散，缺乏对大语言模型驱动内核生成的系统性视角。本综述通过结构化梳理现有方法（涵盖基于LLM的路径与智能体优化工作流），系统汇编支撑该领域学习与评估的数据集和基准测试，以填补这一空白。此外，本文进一步阐述了关键开放挑战与未来研究方向，旨在为下一代自动化内核优化建立全面参考。为持续追踪本领域发展，我们在 https://github.com/flagos-ai/awesome-LLM-driven-kernel-generation 维护开源GitHub项目。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.15727">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.15727">arXiv</a></p>
<hr />
<h3>15. PROGRESSLM：迈向视觉语言模型中的进程推理</h3>
<p><strong>原文标题：</strong> PROGRESSLM: Towards Progress Reasoning in Vision-Language Models</p>
<p><strong>摘要：</strong>
估计任务进程需要对长时程动态进行推理，而非仅仅识别静态视觉内容。尽管现代视觉语言模型在描述可见内容方面表现出色，但其能否从部分观察中推断任务已进展到何种程度，目前尚不明确。为此，我们提出了Progress-Bench，这是一个用于系统评估视觉语言模型中进程推理能力的基准。除了基准测试，我们进一步探索了一种受人类启发的两阶段进程推理范式，包括基于无训练提示的方法和基于精心构建的数据集ProgressLM-45K的有训练方法。对14个视觉语言模型的实验表明，大多数模型尚未具备任务进程估计的能力，它们对演示模态和视角变化表现出敏感性，且在处理不可回答的情况时效果不佳。虽然强制结构化进程推理的无训练提示方法带来了有限且依赖于模型的改进，但有训练的ProgressLM-3B模型即使在小规模模型下也能实现一致的性能提升，尽管其训练任务集与评估任务集完全不相交。进一步的分析揭示了典型的错误模式，并阐明了进程推理成功或失败的条件及原因。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.15224">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.15224">arXiv</a></p>
<hr />
<h3>16. VideoMaMa：基于生成先验的掩码引导视频抠图方法</h3>
<p><strong>原文标题：</strong> VideoMaMa: Mask-Guided Video Matting via Generative Prior</p>
<p><strong>摘要：</strong>
由于标注数据稀缺，将视频抠图模型泛化至真实世界视频仍面临重大挑战。为此，我们提出视频掩码转蒙版模型（VideoMaMa），该模型通过利用预训练的视频扩散模型，将粗粒度分割掩码转换为像素级精确的透明度蒙版。尽管仅使用合成数据进行训练，VideoMaMa在真实世界视频素材上展现出强大的零样本泛化能力。基于此能力，我们开发了可扩展的大规模视频抠图伪标注流程，并构建了“视频万物抠图”（MA-V）数据集。该数据集为超过5万段涵盖多样场景与运动的真实世界视频提供了高质量的抠图标注。为验证该数据集的有效性，我们在MA-V上对SAM2模型进行微调，得到SAM2-Matte模型。该模型在真实场景视频的鲁棒性方面优于基于现有抠图数据集训练的同类模型。这些发现凸显了大规模伪标注视频抠图数据的重要性，并展示了生成先验与易获取的分割线索如何推动视频抠图研究的可扩展进展。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.14255">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.14255">arXiv</a></p>
<hr />
<h3>17. 360Anything：无需几何信息的图像与视频升维至360°全景技术</h3>
<p><strong>原文标题：</strong> 360Anything: Geometry-Free Lifting of Images and Videos to 360°</p>
<p><strong>摘要：</strong>
将透视图像与视频升维至360°全景图可实现沉浸式三维场景生成。现有方法通常依赖于透视图像与等距柱状投影空间之间的显式几何对齐，但这需要已知相机元数据，限制了该方法在缺乏准确校准信息的真实场景数据中的应用。本文提出360Anything——一种基于预训练扩散变换器的免几何框架。通过将透视输入与全景目标简单视为令牌序列，360Anything以纯数据驱动的方式学习透视到等距柱状投影的映射关系，无需依赖相机信息。我们的方法在图像与视频的透视转360°生成任务中均达到最先进性能，优于使用真实相机信息的现有方法。同时，我们追踪了等距柱状投影边界接缝伪影的根源，发现其源于VAE编码器中的零填充操作，并引入环形潜在编码以实现无缝生成。最后，我们在零样本相机视场角与方向估计基准测试中展示了具有竞争力的结果，证明了360Anything对几何关系的深度理解及其在计算机视觉任务中的广泛适用性。更多结果详见https://360anything.github.io/。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.16192">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.16192">arXiv</a></p>
<hr />
<h3>18. Cosmos策略：面向视觉运动控制与规划的视觉模型微调</h3>
<p><strong>原文标题：</strong> Cosmos Policy: Fine-Tuning Video Models for Visuomotor Control and Planning</p>
<p><strong>摘要：</strong>
近期视频生成模型展现出捕捉复杂物理交互与场景时序演变的卓越能力。为利用其时空先验知识，机器人研究领域已尝试将视频模型应用于策略学习，但通常需要多阶段后训练及新增动作生成架构组件，从而引入复杂性。本研究提出Cosmos策略，这是一种通过单阶段后训练将大型预训练视频模型（Cosmos-Predict2）适配为高效机器人策略的简洁方法：仅需在目标平台采集的机器人演示数据上进行训练，无需修改模型架构。该策略通过视频模型的潜在扩散过程直接生成以潜在帧形式编码的机器人动作，充分利用模型的预训练先验知识与核心学习算法来捕捉复杂的动作分布。此外，Cosmos策略能同步生成未来状态图像与价值函数（预期累积奖励），这些信息同样以潜在帧编码，从而支持在测试阶段规划更高成功概率的动作轨迹。实验评估表明，Cosmos策略在LIBERO与RoboCasa仿真基准测试中分别达到98.5%与67.1%的平均成功率，取得领先性能；在具挑战性的真实世界双手操作任务中获得最高平均分，其表现优于从头训练的强扩散策略、基于视频模型的策略，以及在相同机器人演示数据上微调的前沿视觉-语言-动作模型。进一步研究表明，基于策略推演数据，Cosmos策略可通过经验学习优化其世界模型与价值函数，并借助基于模型的规划在复杂任务中实现更高的成功率。相关代码、模型及训练数据已发布于https://research.nvidia.com/labs/dir/cosmos-policy/。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.16163">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.16163">arXiv</a></p>
<hr />
<h3>19. ActionMesh：基于时序三维扩散的动态三维网格生成方法</h3>
<p><strong>原文标题：</strong> ActionMesh: Animated 3D Mesh Generation with Temporal 3D Diffusion</p>
<p><strong>摘要：</strong>
动态三维物体的生成是众多应用领域的核心需求，但现有先进方法常因设定局限、运行耗时或质量不足而难以投入实际应用。本文提出ActionMesh——一种以前馈方式直接生成可直接投入生产的动态三维网格的生成模型。受早期视频模型启发，我们的核心思路是通过在现有三维扩散模型中引入时间轴，构建出“时序三维扩散”框架。具体而言，首先改进三维扩散阶段以生成表征时变独立三维形状的同步隐变量序列；其次设计时序三维自编码器，将独立形状序列转换为预定义参考形状的对应形变，从而构建完整动画。通过整合这两个组件，ActionMesh能够从单目视频、文本描述甚至结合动画文本提示的三维网格等多种输入生成动态三维网格。相较于现有方法，本方案具有生成速度快、无需骨骼绑定且保持拓扑一致性的优势，支持快速迭代并兼容纹理贴图与重定向等无缝应用。我们在标准视频转4D基准数据集（Consistent4D、Objaverse）上评估模型，在几何精度与时序一致性方面均达到最先进性能，证明本模型能够以前所未有的速度与质量生成动态三维网格。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.16148">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.16148">arXiv</a></p>
<hr />
<h3>20. VIOLA：面向最小标注的视频上下文学习</h3>
<p><strong>原文标题：</strong> VIOLA: Towards Video In-Context Learning with Minimal Annotations</p>
<p><strong>摘要：</strong>
将多模态大语言模型（MLLMs）泛化至新兴视频领域对于实际部署至关重要，但由于标注数据稀缺，这一目标仍面临挑战。尽管上下文学习（ICL）提供了一种免训练的适应路径，但传统方法依赖于大规模标注数据池，这在工业或手术等专业场景中往往不切实际，因为这些场景需要专家进行标注。为弥补这一差距，我们提出了VIOLA（基于最小标注的视频上下文学习框架），这是一个标签高效的框架，能够将最小化的专家监督与丰富的未标注数据协同利用。首先，为在严格的标注预算下最大化效率，我们提出了密度-不确定性加权采样方法。与可能选择视觉异常样本的传统多样性或不确定性策略不同，我们的方法利用密度估计来识别同时具备多样性、代表性和信息量的样本。其次，为在不引入噪声传播的前提下利用剩余未标注数据，我们构建了一个混合数据池，并引入了置信度感知检索与置信度感知提示机制。这些机制显式地建模标签可靠性，基于相似度与置信度的综合分数检索示例，同时使MLLM能够自适应地区分经过验证的真实标签和带有噪声的伪标签。通过在九个多样化基准测试中使用四种MLLM进行广泛实验，结果表明我们的框架在低资源设置下显著优于多种基线方法，能够以最小标注成本实现鲁棒的适应。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.15549">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.15549">arXiv</a></p>
<hr />
<h3>21. 智能体置信度校准</h3>
<p><strong>原文标题：</strong> Agentic Confidence Calibration</p>
<p><strong>摘要：</strong>
人工智能智能体正从被动的语言模型快速演变为能够执行复杂多步骤任务的自主系统。然而，其在失败场景中的过度自信仍然是高风险场景部署的根本障碍。现有校准方法专为静态单轮输出设计，无法应对智能体系统的独特挑战，例如任务轨迹中的误差累积、外部工具的不确定性以及不透明的故障模式。为应对这些挑战，我们首次提出智能体置信度校准问题，并引入整体轨迹校准——一种创新的诊断框架，能够从宏观动态到微观稳定性等多个维度，提取智能体完整任务轨迹中丰富的流程级特征。基于简单可解释的模型，HTC在八项基准测试、多种大语言模型及不同智能体框架中，于校准度与判别力方面均持续超越现有强基线方法。除性能优势外，HTC还实现三项重要突破：通过揭示故障背后的信号提供可解释性；无需重新训练即可跨领域应用的强迁移能力；以及通过通用智能体校准器实现泛化能力——该校准器在跨域GAIA基准测试中取得了最佳校准效果（最低预期校准误差）。这些成果共同确立了以流程为核心的置信度校准新范式，为诊断和提升人工智能智能体的可靠性提供了系统性框架。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.15778">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.15778">arXiv</a></p>
<hr />
<h3>22. 主体化不确定性量化</h3>
<p><strong>原文标题：</strong> Agentic Uncertainty Quantification</p>
<p><strong>摘要：</strong>
尽管人工智能主体在长程推理任务中展现出卓越能力，但其可靠性仍受“幻觉螺旋”现象的严重制约——早期认知误差会不可逆地持续传播。现有方法面临两难困境：不确定性量化方法通常作为被动传感器，仅能诊断风险而无法解决问题；而自我反思机制则易陷入持续或盲目的修正循环。为弥合这一鸿沟，本文提出统一的双过程主体化不确定性量化框架，将言语化不确定性转化为主动的双向控制信号。该架构包含两个互补机制：系统一（不确定性感知记忆）通过隐式传播言语化置信度与语义解释，避免盲目决策；系统二（不确定性感知反思）则将这些解释作为理性线索，仅在必要时触发针对性推理时解析。这种设计使智能体能够动态平衡高效执行与深度思考。在闭环基准测试与开放式深度研究任务上的大量实验表明，我们这种免训练方法实现了卓越的性能与轨迹级校准。我们相信这一原则性框架为主体可靠性研究迈出了重要一步。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.15703">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.15703">arXiv</a></p>
<hr />
<h3>23. 从被动度量到主动信号：不确定性量化在大语言模型中的角色演变</h3>
<p><strong>原文标题：</strong> From Passive Metric to Active Signal: The Evolving Role of Uncertainty Quantification in Large Language Models</p>
<p><strong>摘要：</strong>
尽管大语言模型展现出卓越的能力，但其不可靠性仍是部署于高风险领域的关键障碍。本综述描绘了应对这一挑战的功能演进路径：不确定性从被动的诊断度量演变为指导实时模型行为的主动控制信号。我们展示了不确定性如何作为主动控制信号在三个前沿领域发挥作用：在高级推理中优化计算并触发自我修正；在自主智能体中管理关于工具使用和信息寻求的元认知决策；在强化学习中缓解奖励破解并通过内在奖励实现自我改进。通过将这些进展锚定于贝叶斯方法和共形预测等新兴理论框架，我们为这一变革性趋势提供了统一视角。本综述提供了全面概述、批判性分析和实用设计模式，论证掌握不确定性的新趋势对于构建下一代可扩展、可靠且可信的人工智能至关重要。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.15690">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.15690">arXiv</a></p>
<hr />
<h3>24. 教育应用中大语言模型提示的评估研究</h3>
<p><strong>原文标题：</strong> LLM Prompt Evaluation for Educational Applications</p>
<p><strong>摘要：</strong>
随着大语言模型在教育应用中的日益普及，亟需基于实证的方法来设计和评估能够产生个性化且符合教学目标的输出提示。本研究提出了一种可推广的系统性提示评估方法，并通过分析结构化对话活动中大语言模型生成的后续问题进行验证。研究设计并测试了六种提示模板，这些模板融合了成熟的提示工程模式，每种提示侧重不同的教学策略。通过适用于各类教育应用的锦标赛式评估框架对提示模板进行比较，该框架采用Glicko2评分系统，由八位评委从格式、对话支持度和学习者适配性三个维度对问题组进行评价。数据来源于三个独立教育场景中的120组真实用户交互记录。结果显示，聚焦策略性阅读的单一提示模板在配对比较中以81%至100%的胜率显著优于其他模板。该提示融合了角色设定与语境管理两种模式，旨在支持自主学习等元认知学习策略。本方法为教育技术研究者展示了如何系统评估并改进提示设计，推动教育应用从临时性提示工程向基于实证的提示开发范式转变。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.16134">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.16134">arXiv</a></p>
<hr />
<h3>25. 基于Numba加速的二维扩散限制凝聚模型：实现方法与分形特征分析</h3>
<p><strong>原文标题：</strong> Numba-Accelerated 2D Diffusion-Limited Aggregation: Implementation and Fractal Characterization</p>
<p><strong>摘要：</strong>
本文提出dla-ideal-solver——一个基于Numba加速Python的高性能二维扩散限制凝聚模拟框架。通过即时编译技术，该框架在保持高层编程灵活性的同时，实现了与传统静态编译方案相当的计算吞吐量。我们系统研究了不同注入几何结构与行走粒子浓度条件下的拉普拉斯生长不稳定性。分析结果证实，在低浓度体系中标准分形维数D_f≈1.71具有稳健性，符合Witten-Sander普适性类别。然而在高密度环境中，我们观察到向类伊甸园致密生长模式（D_f≈1.87）的显著跨越现象，这归因于屏蔽长度的饱和效应。除标准质量-半径标度分析外，本研究采用广义Rényi维数与空隙度指标，定量表征了凝聚体的单分形特性与空间异质性。本工作为探索非平衡统计力学中的相变现象建立了可复现的开源测试平台。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.15440">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.15440">arXiv</a></p>
<hr />
<h3>26. MirrorBench：评估类人用户代理的可扩展框架</h3>
<p><strong>原文标题：</strong> MirrorBench: An Extensible Framework to Evaluate User-Proxy Agents for Human-Likeness</p>
<p><strong>摘要：</strong>
大语言模型（LLM）正日益被用作人类模拟器，既用于评估对话系统，也用于生成微调数据。然而，简单的“扮演用户”提示往往会产生冗长、不真实的语句，这凸显了对所谓用户代理进行系统性评估的必要性。我们提出了MIRRORBENCH，这是一个可复现、可扩展的基准测试框架，其评估用户代理的唯一标准是它们在不同对话任务中产生类人用户语句的能力，并明确与下游任务的成功解耦。MIRRORBENCH采用模块化执行引擎，具备类型化接口、元数据驱动的注册机制、多后端支持、缓存功能和强大的可观测性。该系统支持可插拔的用户代理、数据集、任务和评估指标，使研究人员能够在统一且考虑方差的框架下评估任意模拟器。我们引入了三个词汇多样性指标（MATTR、YULE'S K和HD-D）和三个基于LLM评判的指标（GTEval、成对不可区分性以及规则与推理）。在四个开放数据集上的实验表明，MIRRORBENCH能够提供考虑方差的结果，并揭示了用户代理与真实人类用户之间的系统性差距。该框架已开源，并包含一个简单的命令行界面，用于运行实验、管理配置与缓存以及生成报告。框架可通过 https://github.com/SAP/mirrorbench 访问。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.08118">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.08118">arXiv</a></p>
<hr />
<h3>27. 作为电路的维格纳之友：超导量子硬件上的分支间通信见证基准测试</h3>
<p><strong>原文标题：</strong> Wigner's Friend as a Circuit: Inter-Branch Communication Witness Benchmarks on Superconducting Quantum Hardware</p>
<p><strong>摘要：</strong>
我们在IBM量子硬件上实现并基准测试了Violaris提出的电路族，该电路族用于估计操作性的分支间通信见证，其定义为由编译的维格纳之友式电路产生的经典测量记录中的关联性。我们将该协议的五量子比特实例实现为单个电路内的寄存器间消息传递模式（而非物理信号传递），并在实际设备噪声和编译约束下评估其行为。该电路编码了观察者子系统的分支条件演化，其动力学依赖于一个控制量子比特，随后通过受控传输操作来探测条件测量上下文之间的关联性。</p>
<p>在ibm_fez后端上执行20000次测量，我们观察到基于布居数的可见度为0.877，沿正交轴的相干性见证值分别为0.840和-0.811，相位敏感幅度约为1.17。虽然可见度度量对某些类型的退相不敏感，但相干性见证提供了对非对角噪声的互补敏感性。</p>
<p>本工作并不检验或区分量子力学的不同诠释，而是提供了一个可复现的操作性约束流程，用于评估非理想信道相对于校准设备噪声的可探测性。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.16004">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.16004">arXiv</a></p>
<hr />
<h2>🔍 关键词云图</h2>
<p><img alt="关键词云图" src="../images/keywords_wordcloud.png" /></p>
<h2>📈 近期论文趋势</h2>
<p><img alt="论文趋势" src="../images/daily_papers.png" /></p>
<h2>🎙️ 语音播报</h2>
<ul>
<li><a href="../audio/2026-01-23_daily_papers.mp3">收听今日论文解读</a></li>
</ul>
<h2>📱 订阅渠道</h2>
<ul>
<li>GitHub: <a href="https://github.com/2404589803/hf-daily-paper-newsletter-chinese">hf-daily-paper-newsletter-chinese</a></li>
</ul>
    </div>
</body>
</html>