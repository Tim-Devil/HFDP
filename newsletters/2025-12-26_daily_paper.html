<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Hugging Face 论文日报 - 2025-12-26</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', sans-serif;
            line-height: 1.6;
            color: #333;
            background-color: #f5f5f5;
            padding: 20px;
        }
        
        .container {
            max-width: 900px;
            margin: 0 auto;
            background-color: white;
            padding: 40px;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }
        
        h1 {
            color: #2c3e50;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
            margin-bottom: 30px;
            font-size: 28px;
        }
        
        h1 img {
            vertical-align: middle;
            margin-right: 10px;
        }
        
        h2 {
            color: #34495e;
            margin-top: 40px;
            margin-bottom: 20px;
            font-size: 24px;
            border-left: 4px solid #3498db;
            padding-left: 15px;
        }
        
        h3 {
            color: #2c3e50;
            margin-top: 30px;
            margin-bottom: 15px;
            font-size: 20px;
        }
        
        p {
            margin-bottom: 15px;
            text-align: justify;
        }
        
        ul {
            margin-left: 20px;
            margin-bottom: 20px;
        }
        
        li {
            margin-bottom: 8px;
        }
        
        a {
            color: #3498db;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        strong {
            color: #2c3e50;
            font-weight: 600;
        }
        
        hr {
            border: none;
            border-top: 1px solid #e0e0e0;
            margin: 30px 0;
        }
        
        /* 关键修复:限制图片宽度 */
        img {
            max-width: 100%;
            height: auto;
            display: block;
            margin: 20px auto;
            border-radius: 4px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        
        /* 确保图片容器也有宽度限制 */
        p img {
            max-width: 100%;
        }
        
        /* 论文详情区域样式 */
        .paper-section {
            background-color: #f9f9f9;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 20px;
        }
        
        /* 统计信息样式 */
        .stats {
            background-color: #e8f4f8;
            padding: 15px;
            border-radius: 6px;
            margin-bottom: 20px;
        }
        
        /* 响应式设计 */
        @media (max-width: 768px) {
            .container {
                padding: 20px;
            }
            
            h1 {
                font-size: 24px;
            }
            
            h2 {
                font-size: 20px;
            }
            
            h3 {
                font-size: 18px;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <h1><img src="https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo.png" width="30"/> Hugging Face 2025-12-26 论文日报</h1>
<h2>📊 今日论文统计</h2>
<ul>
<li>总论文数：7</li>
<li>热门领域：综合领域</li>
</ul>
<h2>📝 论文详情</h2>
<h3>1. 潜在隐式视觉推理</h3>
<p><strong>原文标题：</strong> Latent Implicit Visual Reasoning</p>
<p><strong>摘要：</strong>
尽管大型多模态模型已取得显著进展，但其本质上仍以文本为中心，依赖语言作为核心推理模态。这导致其在处理以视觉为主导的推理任务时存在明显局限。近期研究尝试通过辅助图像、深度图或图像裁剪来监督中间视觉步骤以解决该问题，但这些策略对“有效”视觉抽象形式施加了限制性先验，增加了高昂的标注成本，且难以实现跨任务泛化。为突破这一关键局限，我们提出一种任务无关的机制，通过无显式监督的方式训练大型多模态模型自主发现并利用视觉推理标记。这些标记能进行全局注意力计算，并以任务自适应方式对图像进行重编码，使模型无需人工标注监督即可提取相关视觉信息。本方法在多种以视觉为中心的任务上——包括那些难以明确指定中间抽象层的任务——均优于直接微调方法，取得了最先进的性能表现，同时展现出对多任务指令调优的泛化能力。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.21218">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.21218">arXiv</a></p>
<hr />
<h3>2. 自回归模型中的涌现时间抽象实现分层强化学习</h3>
<p><strong>原文标题：</strong> Emergent temporal abstractions in autoregressive models enable hierarchical reinforcement learning</p>
<p><strong>摘要：</strong>
基于下一标记预测进行预训练、并通过强化学习微调的大规模自回归模型已在众多问题领域取得前所未有的成功。在强化学习过程中，这些模型通过逐个生成标记来探索新输出。然而，逐标记采样动作可能导致学习效率低下，尤其在奖励稀疏的情况下。本文证明，通过在自回归模型的内部表征空间中进行决策与探索，可以克服这一问题。具体而言，为发现时间抽象动作，我们引入了一个高阶非因果序列模型，其输出可控制基础自回归模型的残差流激活状态。在具有层次结构的网格世界和基于MuJoCo的任务中，高阶模型学会将长激活序列块压缩至内部控制器。关键的是，每个控制器可执行一系列在长时间尺度上展开、具有行为意义的动作序列，并附带学习得到的终止条件，使得随时间组合多个控制器能够在新型任务中实现高效探索。我们提出的"内部强化学习"方法——即直接对内部控制器进行强化——能够在标准强化学习微调失效的稀疏奖励场景中实现有效学习。本研究结果揭示了自回归模型中潜在动作生成与强化的优势，表明内部强化学习为实现基础模型中的分层强化学习提供了可行路径。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.20605">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.20605">arXiv</a></p>
<hr />
<h3>3. Spatia：基于可更新空间记忆的视频生成方法</h3>
<p><strong>原文标题：</strong> Spatia: Video Generation with Updatable Spatial Memory</p>
<p><strong>摘要：</strong>
现有视频生成模型因视频信号密集且高维的特性，难以保持长期的空间与时间一致性。为克服这一局限，我们提出Spatia——一种空间记忆感知的视频生成框架，其显式地将三维场景点云作为持久化空间记忆进行维护。Spatia基于该空间记忆迭代生成视频片段，并通过视觉SLAM技术持续更新记忆。这种动态-静态解耦的设计增强了生成过程中的空间一致性，同时保留了模型生成逼真动态实体的能力。此外，Spatia支持显式相机控制与三维感知交互编辑等应用，为可扩展、记忆驱动的视频生成提供了几何基础的框架。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.15716">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.15716">arXiv</a></p>
<hr />
<h3>4. 语言模型数学推理的舍恩菲尔德解剖学</h3>
<p><strong>原文标题：</strong> Schoenfeld's Anatomy of Mathematical Reasoning by Language Models</p>
<p><strong>摘要：</strong>
大型语言模型日益展现出推理轨迹，然而其潜在的认知结构与步骤仍难以超越表层统计被识别和分析。本研究采用舍恩菲尔德的情节理论作为归纳性、中观尺度的分析视角，提出ThinkARM（模型推理解剖学）这一可扩展框架，将推理轨迹显式抽象为分析、探索、实施、验证等功能性推理步骤。当应用于多种模型的数学问题求解时，这种抽象揭示了可复现的思维动态，以及推理模型与非推理模型之间的结构性差异——这些差异在词元级视角中并不明显。我们进一步通过两项诊断性案例研究表明：探索步骤作为关键分支节点与解题正确率显著相关；而效率导向的方法会选择性地抑制评估反馈步骤，而非均匀缩短所有响应。综合而言，我们的研究证明情节级表征能使推理步骤显性化，从而系统分析现代语言模型中推理的结构化、稳定化与变异机制。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.19995">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.19995">arXiv</a></p>
<hr />
<h3>5. 视频基础模型编码了多少三维信息？</h3>
<p><strong>原文标题：</strong> How Much 3D Do Video Foundation Models Encode?</p>
<p><strong>摘要：</strong>
视频是三维世界在二维平面上的连续投影。经过大规模视频数据训练后，全局三维理解能力是否会自然涌现？本研究通过量化现有视频基础模型在大量视频数据预训练后获得的三维理解能力来探讨这一问题。我们提出了首个模型无关的评估框架，通过浅层读出器从模型特征中估计多种三维属性，从而系统衡量不同视频基础模型的三维感知能力。研究在多维度上揭示了视频基础模型三维感知的有意义发现：尽管未接受任何三维数据训练，当前最先进的视频生成模型展现出对三维物体与场景的深刻理解能力，其表现甚至可能超越专门针对三维任务训练的大型专家模型。本研究的发现结合对主流视频基础模型的三维基准测试，为构建可扩展的三维模型提供了重要参考。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.19949">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.19949">arXiv</a></p>
<hr />
<h3>6. VA-π：面向像素感知自回归生成的变分策略对齐方法</h3>
<p><strong>原文标题：</strong> VA-π: Variational Policy Alignment for Pixel-Aware Autoregressive Generation</p>
<p><strong>摘要：</strong>
自回归视觉生成模型依赖分词器将图像映射至离散序列及反向重建。然而，分词器的训练目标是从真实标记重建清晰图像，而自回归生成器仅针对标记似然性进行优化。这种目标错位导致生成的标记序列可能解码为低质量图像，且缺乏像素空间的直接监督。本文提出VA-π——一种轻量级的训练后优化框架，通过基于原理的像素空间目标直接优化自回归模型。VA-π将生成器与分词器的对齐问题构建为变分优化，推导出统一像素重建与自回归建模的证据下界。为在离散标记空间中进行优化，VA-π引入基于强化学习的对齐策略：将自回归生成器视为策略网络，以像素空间重建质量作为内在奖励。该奖励通过强制教学条件下预测标记序列对原始图像的重建精度进行度量，使模型获得无需昂贵自由运行采样的像素级直接指导。证据下界中的正则化项作为天然正则器，保持标记的分布一致性。VA-π无需重新训练分词器或依赖外部奖励模型，即可实现现有自回归生成器的快速适配。仅使用1%的ImageNet-1K数据及25分钟微调，即在LlamaGen-XXL模型上将FID从14.36降至7.65，IS从86.55提升至116.70；在GenEval文本到图像任务中，视觉生成模型（LlamaGen：从0.306至0.339）与统一多模态模型（Janus-Pro：从0.725至0.744）均取得显著提升。代码已开源：https://github.com/Lil-Shake/VA-Pi。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.19680">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.19680">arXiv</a></p>
<hr />
<h3>7. GTR-Turbo：融合检查点——智能视觉语言模型训练中隐形的免费导师</h3>
<p><strong>原文标题：</strong> GTR-Turbo: Merged Checkpoint is Secretly a Free Teacher for Agentic VLM Training</p>
<p><strong>摘要：</strong>
基于视觉语言模型构建的多模态智能体在进行多轮强化学习时，常因奖励稀疏与长期信用分配困难而受限。近期研究通过引入提供步骤级反馈的“导师”模型来细化奖励信号，例如引导思维强化与策略蒸馏方法，但这些方法依赖于昂贵且通常具有特权访问权限的模型作为导师，限制了其实用性与可复现性。本文提出GTR-Turbo，作为GTR方法的高效升级版本，它在无需训练或调用昂贵导师模型的情况下实现了同等性能。具体而言，GTR-Turbo将在持续强化学习过程中产生的检查点权重进行融合，并将此融合模型作为“免费”导师，通过监督微调或软逻辑蒸馏来引导后续强化学习。这一设计消除了对特权视觉语言模型的依赖，缓解了先前工作中观察到的“熵崩塌”现象，并保持了训练稳定性。在多种视觉智能任务上的实验表明，相较于GTR方法，GTR-Turbo将基线模型的准确率提升了10-30%，同时将训练时间缩短了50%，计算成本降低了60%。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.13043">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.13043">arXiv</a></p>
<hr />
<h2>🔍 关键词云图</h2>
<p><img alt="关键词云图" src="../images/keywords_wordcloud.png" /></p>
<h2>📈 近期论文趋势</h2>
<p><img alt="论文趋势" src="../images/daily_papers.png" /></p>
<h2>🎙️ 语音播报</h2>
<ul>
<li><a href="../audio/2025-12-26_daily_papers.mp3">收听今日论文解读</a></li>
</ul>
<h2>📱 订阅渠道</h2>
<ul>
<li>GitHub: <a href="https://github.com/2404589803/hf-daily-paper-newsletter-chinese">hf-daily-paper-newsletter-chinese</a></li>
</ul>
    </div>
</body>
</html>