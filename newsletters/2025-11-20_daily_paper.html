<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Hugging Face 论文日报 - 2025-11-20</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', sans-serif;
            line-height: 1.6;
            color: #333;
            background-color: #f5f5f5;
            padding: 20px;
        }
        
        .container {
            max-width: 900px;
            margin: 0 auto;
            background-color: white;
            padding: 40px;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }
        
        h1 {
            color: #2c3e50;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
            margin-bottom: 30px;
            font-size: 28px;
        }
        
        h1 img {
            vertical-align: middle;
            margin-right: 10px;
        }
        
        h2 {
            color: #34495e;
            margin-top: 40px;
            margin-bottom: 20px;
            font-size: 24px;
            border-left: 4px solid #3498db;
            padding-left: 15px;
        }
        
        h3 {
            color: #2c3e50;
            margin-top: 30px;
            margin-bottom: 15px;
            font-size: 20px;
        }
        
        p {
            margin-bottom: 15px;
            text-align: justify;
        }
        
        ul {
            margin-left: 20px;
            margin-bottom: 20px;
        }
        
        li {
            margin-bottom: 8px;
        }
        
        a {
            color: #3498db;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        strong {
            color: #2c3e50;
            font-weight: 600;
        }
        
        hr {
            border: none;
            border-top: 1px solid #e0e0e0;
            margin: 30px 0;
        }
        
        /* 关键修复:限制图片宽度 */
        img {
            max-width: 100%;
            height: auto;
            display: block;
            margin: 20px auto;
            border-radius: 4px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        
        /* 确保图片容器也有宽度限制 */
        p img {
            max-width: 100%;
        }
        
        /* 论文详情区域样式 */
        .paper-section {
            background-color: #f9f9f9;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 20px;
        }
        
        /* 统计信息样式 */
        .stats {
            background-color: #e8f4f8;
            padding: 15px;
            border-radius: 6px;
            margin-bottom: 20px;
        }
        
        /* 响应式设计 */
        @media (max-width: 768px) {
            .container {
                padding: 20px;
            }
            
            h1 {
                font-size: 24px;
            }
            
            h2 {
                font-size: 20px;
            }
            
            h3 {
                font-size: 18px;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <h1><img src="https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo.png" width="30"/> Hugging Face 2025-11-20 论文日报</h1>
<h2>📊 今日论文统计</h2>
<ul>
<li>总论文数：12</li>
<li>热门领域：RL</li>
</ul>
<h2>📝 论文详情</h2>
<h3>1. Kandinsky 5.0：面向图像与视频生成的基座模型系列</h3>
<p><strong>原文标题：</strong> Kandinsky 5.0: A Family of Foundation Models for Image and Video Generation</p>
<p><strong>摘要：</strong>
本报告介绍Kandinsky 5.0——一系列支持高分辨率图像与10秒视频生成的尖端基座模型。该框架包含三大核心模型组合：Kandinsky 5.0图像轻量版（60亿参数图像生成模型）、Kandinsky 5.0视频轻量版（20亿参数的快速轻量文本/图像转视频模型），以及Kandinsky 5.0视频专业版（190亿参数的高质量视频生成模型）。我们系统阐述了多阶段训练流程中的数据治理生命周期（涵盖采集、处理、筛选与聚类），该流程包含大规模预训练阶段，并融合了自监督微调与基于强化学习的训练后优化等质量增强技术。同时，我们提出了创新的架构设计、训练方法与推理优化方案，使人机评估证实Kandinsky 5.0能在各类任务中实现高速生成与顶尖性能。作为开源可用的规模化生成框架，Kandinsky 5.0充分发挥预训练及后续阶段的潜力，可适配多样化生成场景。我们期待通过本报告及同步开源的核心代码与训练检查点，显著推动高质量生成模型在科研领域的发展与普及。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.14993">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.14993">arXiv</a></p>
<hr />
<h3>2. 视频推理：通过迷宫求解任务首次评估视频模型的推理能力</h3>
<p><strong>原文标题：</strong> Reasoning via Video: The First Evaluation of Video Models' Reasoning Abilities through Maze-Solving Tasks</p>
<p><strong>摘要：</strong>
视频模型在具有连贯运动动态的高保真视频生成领域取得了显著成就。与语言建模从文本生成到基于文本推理的发展历程相似，视频模型的发展促使我们思考：视频模型能否通过视频生成进行推理？与离散文本语料相比，视频将推理锚定在明确的空间布局和时间连续性中，这使其成为空间推理的理想载体。本研究探索视频推理范式，并推出VR-Bench——一个系统评估视频模型推理能力的综合基准。该基准以迷宫求解任务为基础，此类任务本质上需要空间规划与多步推理能力，包含5种迷宫类型和多样视觉风格下程序化生成的7,920个视频。实证分析表明，监督微调能有效激发视频模型的推理能力。视频模型在推理过程中展现出更强的空间感知能力，其表现超越主流视觉语言模型，并在多样化场景、任务及复杂度层级中均具有良好的泛化性能。我们进一步发现测试时扩展效应，即推理过程中的多样化采样可将推理可靠性提升10-20%。这些发现凸显了视频推理在空间推理任务中独特的潜力和可扩展性。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.15065">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.15065">arXiv</a></p>
<hr />
<h3>3. 优秀AI研究智能体需具备何种特质？探讨构思多样性的作用</h3>
<p><strong>原文标题：</strong> What Does It Take to Be a Good AI Research Agent? Studying the Role of Ideation Diversity</p>
<p><strong>摘要：</strong>
AI研究智能体通过自动化机器学习模型的设计、实现与训练，为加速科学进展提供了可能。然而该领域仍处于发展初期，驱动智能体轨迹成败的关键因素尚未被完全理解。本研究探讨了构思多样性对智能体表现的影响机制。首先，我们在MLE-bench（评估AI研究智能体的知名基准测试）上分析了不同模型与智能体架构的执行轨迹。分析表明：不同模型与智能体架构会产生不同程度的构思多样性，且表现更优的智能体往往展现出更高的构思多样性。进而通过控制构思多样性程度的实验，我们证实提高构思多样性能够显著增强智能体性能。最后，我们突破MLE-bench传统的奖牌评分体系，通过多维度评估指标验证了研究结论的稳健性，证明该发现在不同智能体性能度量标准下均成立。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.15593">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.15593">arXiv</a></p>
<hr />
<h3>4. VisPlay：基于图像自演进的视觉语言模型</h3>
<p><strong>原文标题：</strong> VisPlay: Self-Evolving Vision-Language Models from Images</p>
<p><strong>摘要：</strong>
强化学习为提升视觉语言模型在复杂推理任务上的性能提供了理论框架。然而现有强化学习方法通常依赖人工标注标签或任务特定启发式规则来定义可验证奖励，这两种方式均成本高昂且难以扩展。我们提出VisPlay——一种自演进的强化学习框架，使视觉语言模型能够利用大量未标注图像数据自主提升推理能力。该框架以单一基础视觉语言模型为起点，将其分配至两个交互角色：图像条件提问器负责构建具有挑战性但可回答的视觉问题，多模态推理器则生成银标答案。通过群体相对策略优化算法对这两个角色进行联合训练，该算法融合了多样性与难度奖励机制，以平衡生成问题的复杂程度与银标答案的质量。VisPlay在Qwen2.5-VL和MiMo-VL两个模型系列中均展现出卓越的扩展效能，在MM-Vet和MMMU等八项基准测试中持续提升视觉推理、组合泛化及幻觉抑制能力，为自演进多模态智能提供了可扩展路径。项目页面详见：https://bruno686.github.io/VisPlay/</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.15661">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.15661">arXiv</a></p>
<hr />
<h3>5. 基于自动生成大规模数据集的指令引导胸部X光病灶分割</h3>
<p><strong>原文标题：</strong> Instruction-Guided Lesion Segmentation for Chest X-rays with Automatically Generated Large-Scale Dataset</p>
<p><strong>摘要：</strong>
当前胸部X光病灶分割模型的应用受限于目标标签数量稀少及依赖冗长的专家级文本输入，这为实际应用设置了障碍。为解决这些局限性，我们提出了一种新范式：指令引导病灶分割，该范式能够基于简洁的用户指令分割多种病灶类型。在此范式下，我们通过全自动多模态流程构建了首个面向CXR病灶分割的大规模指令-答案数据集MIMIC-ILS，该流程可从胸部X光图像及对应报告中自动生成标注。MIMIC-ILS包含源自19.2万张图像和9.1万个独立分割掩码的110万条指令-答案对，涵盖七种主要病灶类型。为实证验证其效用，我们提出了基于MIMIC-ILS微调的视觉语言模型ROSALIA。该模型能够根据用户指令分割多种病灶并生成文本解释。在我们新提出的任务中，该模型实现了优异的病灶分割与文本描述精度，充分证明了我们流程的有效性，也彰显了MIMIC-ILS作为像素级CXR病灶定位基础资源的重大价值。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.15186">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.15186">arXiv</a></p>
<hr />
<h3>6. ARC-Chapter：将小时级视频结构化构建为可导航章节与层级化摘要</h3>
<p><strong>原文标题：</strong> ARC-Chapter: Structuring Hour-Long Videos into Navigable Chapters and Hierarchical Summaries</p>
<p><strong>摘要：</strong>
小时级长视频（如讲座、播客、纪录片）的激增强化了对高效内容结构化的需求。然而现有方法受限于小规模标注训练数据——其标注通常简短粗糙，制约了模型对长视频中细微内容转换的泛化能力。我们提出ARC-Chapter，首个基于百万级长视频章节训练的大规模视频分章模型，其特点在于具备双语、时序锚定和层级化的章节标注体系。为实现这一目标，我们通过结构化流程构建了英汉双语章节数据集，将语音转录文本、场景文字和视觉描述统一整合为从简短标题到详细摘要的多层级标注。实验证明，随着数据规模（数据量和标注密度）的提升，模型性能获得显著改善。此外，我们设计了名为GRACE的新型评估指标，该指标综合考量多对一的片段重叠度与语义相似性，能更准确反映实际分章任务的灵活性要求。大量实验表明，ARC-Chapter以显著优势创下最新性能记录，F1分数较先前最佳方法提升14.0%，SODA分数提升11.3%。更值得注意的是，ARC-Chapter展现出卓越的迁移学习能力，在YouCook2密集视频描述等下游任务中同样提升了现有最佳性能。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.14349">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.14349">arXiv</a></p>
<hr />
<h3>7. MHR：动量人体骨骼系统</h3>
<p><strong>原文标题：</strong> MHR: Momentum Human Rig</p>
<p><strong>摘要：</strong>
本文提出MHR参数化人体模型，该模型融合了ATLAS架构的解耦骨骼/形态设计范式，并采用源自Momentum函数库的柔性现代骨骼绑定与姿态校正系统。我们的模型能够生成具有表现力且符合解剖学原理的人体动画，支持非线性姿态校正功能，其设计目标是为增强现实/虚拟现实及图形处理管线提供稳定可靠的集成方案。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.15586">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.15586">arXiv</a></p>
<hr />
<h3>8. FreeAskWorld：一种以人为中心的具身AI交互式闭环仿真平台</h3>
<p><strong>原文标题：</strong> FreeAskWorld: An Interactive and Closed-Loop Simulator for Human-Centric Embodied AI</p>
<p><strong>摘要：</strong>
随着具身智能成为人工智能研究的核心前沿，仿真平台必须超越低层次物理交互，转向捕捉以人为中心的复杂社会行为。本文提出FreeAskWorld——一个融合大语言模型进行高层行为规划与语义基础交互的仿真框架，其设计基于意图理论与社会认知理论。该框架支持可扩展的逼真人机交互仿真，并包含针对多样化具身任务设计的模块化数据生成流程。为验证框架性能，我们将经典视觉语言导航任务扩展为富含交互的路径问询场景，使智能体能够主动寻求并解析导航指引。我们公开推出FreeAskWorld大规模基准数据集，包含重构环境场景、6种任务类型、16个核心物体类别、63,429帧标注样本帧及超过17小时的交互数据，为具身AI系统的训练与评估提供支持。通过对视觉语言导航模型与人类参与者进行开环与闭环设置的基准测试，实验结果表明：基于FreeAskWorld微调的模型显著优于原始版本，在语义理解与交互能力方面均获得提升。这些发现印证了基于社会认知的仿真框架在推动具身AI系统实现高级规划与自然人机交互方面的有效性。特别需要指出的是，本研究证实交互本身可作为一种附加的信息模态。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.13524">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.13524">arXiv</a></p>
<hr />
<h3>9. RoMa v2：更强大、更精准、更快速、更密集的特征匹配</h3>
<p><strong>原文标题：</strong> RoMa v2: Harder Better Faster Denser Feature Matching</p>
<p><strong>摘要：</strong>
密集特征匹配旨在估计三维场景中两幅图像间的所有对应关系，因其高精度与强鲁棒性已成为当前黄金标准。然而现有密集匹配方法在诸多复杂现实场景中仍存在匹配失败或性能不佳的问题，且高精度模型往往速度缓慢，限制了实际应用。本文通过一系列系统性改进多管齐下攻克这些缺陷，最终构建出性能显著提升的新模型。我们设计了一种新颖的匹配架构与损失函数，结合精心构建的多样化训练数据分布，使模型能够应对多种复杂匹配任务。通过解耦的“先匹配后优化”两阶段流程，我们实现了训练加速，并借助定制CUDA内核显著降低了优化阶段的内存占用。此外，我们利用近期提出的DINOv3基础模型及多项创新洞见，有效提升了模型的鲁棒性与无偏性。在大量实验验证中，本研究所提出的新型匹配器确立了最新技术标杆，其精度显著超越现有方法。代码已发布于https://github.com/Parskatt/romav2</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.15706">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.15706">arXiv</a></p>
<hr />
<h3>10. 生成式音乐人工智能与人类偏好的对齐：方法与挑战</h3>
<p><strong>原文标题：</strong> Aligning Generative Music AI with Human Preferences: Methods and Challenges</p>
<p><strong>摘要：</strong>
尽管生成式音乐人工智能在保真度与风格多样性方面取得了显著进展，但由于其所采用的特定损失函数，这些系统往往难以契合人类细腻的审美偏好。本文主张将偏好对齐技术系统化应用于音乐生成领域，以弥合计算优化与人类音乐审美之间的根本差距。基于包括MusicRL的大规模偏好学习、DiffRhythm+中基于扩散的偏好优化等多偏好对齐框架，以及Text2midi-InferAlign等推理时优化技术在内的最新突破，我们深入探讨了这些技术如何应对音乐领域特有的挑战：时序连贯性、和声一致性及主观质量评估。我们指出了该领域的关键研究挑战，包括长篇幅作品的可扩展性、偏好建模的可靠性等。展望未来，我们预见偏好对齐的音乐生成技术将在交互式作曲工具和个性化音乐服务中催生变革性应用。本研究呼吁持续开展跨学科合作，结合机器学习与音乐理论的前沿进展，共同构建真正服务于人类创作与体验需求的音乐人工智能系统。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.15038">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.15038">arXiv</a></p>
<hr />
<h3>11. 状态混合：面向多模态生成的路由令牌级动态机制</h3>
<p><strong>原文标题：</strong> Mixture of States: Routing Token-Level Dynamics for Multimodal Generation</p>
<p><strong>摘要：</strong>
本文提出状态混合（MoS）——一种创新的多模态扩散模型融合范式，通过基于状态的灵活交互实现模态融合。MoS的核心是可学习的令牌级路由器，该路由器在去噪时间步长和输入依赖条件下建立多模态隐状态间的交互机制，精确地将令牌级特征与扩散轨迹对齐。该路由器通过稀疏选择Top-k隐状态，并采用ε-贪婪策略进行训练，能够以最少可学习参数和可忽略的计算开销高效选择上下文特征。我们通过文本到图像生成（MoS-Image）和编辑（MoS-Editing）任务验证设计，取得了最先进的性能成果。仅使用30亿至50亿参数，我们的模型即可匹配甚至超越参数量达4倍的同类模型。这些发现确立了MoS作为可扩展多模态扩散模型的灵活且计算高效的范式。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.12207">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.12207">arXiv</a></p>
<hr />
<h3>12. Medal S：面向医学分割的空间-文本提示模型</h3>
<p><strong>原文标题：</strong> Medal S: Spatio-Textual Prompt Model for Medical Segmentation</p>
<p><strong>摘要：</strong>
本文提出Medal S医学分割基础模型，该模型在端到端可训练框架内支持原生分辨率空间提示与文本提示。相较于缺乏空间感知的纯文本方法，Medal S实现了三维体积提示与文本嵌入的通道级对齐，有效缓解分辨率失配导致的定位偏差。通过保留完整三维上下文信息，模型能并行处理多个原生分辨率掩码，显著提升多类别分割性能。轻量化三维卷积模块在双提示类型引导下实现精确体素空间优化，支持BiomedSegFM数据集中CT、MRI、PET、超声及显微影像等5种模态的243个分割类别。Medal S提供两种提示模式：纯文本模式（以模型预测结果作为空间提示进行自主优化）和混合模式（结合人工标注提升灵活性）。在24类分割任务中，并行空间提示较顺序提示减少90%以上推理时间。针对目标-图像块比例失衡问题，我们提出动态重采样技术，扩展SAT与nnU-Net的数据增强方法。此外，通过优化文本预处理流程、设计两阶段推理策略及后处理技术，显著提升了内存效率、分割精度与推理速度。在验证集五模态平均指标中，Medal S以DSC 75.44（对比69.83）、NSD 77.34（对比71.06）、F1 38.24（对比24.88）和DSC TP 65.46（对比46.97）全面超越SAT。该模型通过协调空间精度与语义文本指导，在基于顺序提示的多类别医学分割任务中展现出卓越的效能与准确性。Medal S代码已开源：https://github.com/yinghemedical/Medal-S。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.13001">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.13001">arXiv</a></p>
<hr />
<h2>🔍 关键词云图</h2>
<p><img alt="关键词云图" src="../images/keywords_wordcloud.png" /></p>
<h2>📈 近期论文趋势</h2>
<p><img alt="论文趋势" src="../images/daily_papers.png" /></p>
<h2>🎙️ 语音播报</h2>
<ul>
<li><a href="../audio/2025-11-20_daily_papers.mp3">收听今日论文解读</a></li>
</ul>
<h2>📱 订阅渠道</h2>
<ul>
<li>GitHub: <a href="https://github.com/2404589803/hf-daily-paper-newsletter-chinese">hf-daily-paper-newsletter-chinese</a></li>
</ul>
    </div>
</body>
</html>