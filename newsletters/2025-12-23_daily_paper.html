<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Hugging Face 论文日报 - 2025-12-23</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', sans-serif;
            line-height: 1.6;
            color: #333;
            background-color: #f5f5f5;
            padding: 20px;
        }
        
        .container {
            max-width: 900px;
            margin: 0 auto;
            background-color: white;
            padding: 40px;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }
        
        h1 {
            color: #2c3e50;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
            margin-bottom: 30px;
            font-size: 28px;
        }
        
        h1 img {
            vertical-align: middle;
            margin-right: 10px;
        }
        
        h2 {
            color: #34495e;
            margin-top: 40px;
            margin-bottom: 20px;
            font-size: 24px;
            border-left: 4px solid #3498db;
            padding-left: 15px;
        }
        
        h3 {
            color: #2c3e50;
            margin-top: 30px;
            margin-bottom: 15px;
            font-size: 20px;
        }
        
        p {
            margin-bottom: 15px;
            text-align: justify;
        }
        
        ul {
            margin-left: 20px;
            margin-bottom: 20px;
        }
        
        li {
            margin-bottom: 8px;
        }
        
        a {
            color: #3498db;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        strong {
            color: #2c3e50;
            font-weight: 600;
        }
        
        hr {
            border: none;
            border-top: 1px solid #e0e0e0;
            margin: 30px 0;
        }
        
        /* 关键修复:限制图片宽度 */
        img {
            max-width: 100%;
            height: auto;
            display: block;
            margin: 20px auto;
            border-radius: 4px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        
        /* 确保图片容器也有宽度限制 */
        p img {
            max-width: 100%;
        }
        
        /* 论文详情区域样式 */
        .paper-section {
            background-color: #f9f9f9;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 20px;
        }
        
        /* 统计信息样式 */
        .stats {
            background-color: #e8f4f8;
            padding: 15px;
            border-radius: 6px;
            margin-bottom: 20px;
        }
        
        /* 响应式设计 */
        @media (max-width: 768px) {
            .container {
                padding: 20px;
            }
            
            h1 {
                font-size: 24px;
            }
            
            h2 {
                font-size: 20px;
            }
            
            h3 {
                font-size: 18px;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <h1><img src="https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo.png" width="30"/> Hugging Face 2025-12-23 论文日报</h1>
<h2>📊 今日论文统计</h2>
<ul>
<li>总论文数：24</li>
<li>热门领域：LLM, GPT, RL</li>
</ul>
<h2>📝 论文详情</h2>
<h3>1. DataFlow：面向以数据为中心的人工智能时代的LLM驱动统一数据准备与工作流程自动化框架</h3>
<p><strong>原文标题：</strong> DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI</p>
<p><strong>摘要：</strong>
大型语言模型（LLM）对高质量数据需求的快速增长，使得对可扩展、可靠且语义丰富的数据准备流程的需求日益迫切。然而，当前实践仍主要依赖临时脚本和松散定义的工作流程，这些方法缺乏原则性抽象、阻碍可复现性，并且对模型在环数据生成的支持有限。为应对这些挑战，我们提出了DataFlow——一个统一且可扩展的LLM驱动数据准备框架。DataFlow采用系统级抽象设计，支持模块化、可复用和可组合的数据转换，并提供类似PyTorch风格的流程构建API，用于构建可调试和可优化的数据流。该框架包含近200个可复用算子及六类跨领域通用流程，涵盖文本、数学推理、代码、Text-to-SQL、智能体RAG和大规模知识提取。为提升易用性，我们进一步引入DataFlow-Agent，它通过算子合成、流程规划和迭代验证，能够自动将自然语言描述转换为可执行流程。在六个典型应用场景中，DataFlow持续提升了下游LLM性能：我们的数学、代码和文本流程在效果上优于人工精标数据集和专用合成基线——在Text-to-SQL任务中较SynSQL提升高达3%的执行准确率，在代码基准测试中平均提升7%，在MATH、GSM8K和AIME基准上获得1-3个百分点的增益。此外，由DataFlow生成的统一万条样本数据集，使基础模型性能超越基于百万条Infinity-Instruct数据训练的对照模型。这些结果表明，DataFlow为可靠、可复现和可扩展的LLM数据准备提供了实用高效的基础架构，并为未来以数据为中心的人工智能发展奠定了系统级基石。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.16676">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.16676">arXiv</a></p>
<hr />
<h3>2. 棱镜假说：通过统一自编码实现语义与像素表征的和谐统一</h3>
<p><strong>原文标题：</strong> The Prism Hypothesis: Harmonizing Semantic and Pixel Representations via Unified Autoencoding</p>
<p><strong>摘要：</strong>
跨模态的深度表征本质上是相互交织的。本文系统分析了多种语义编码器与像素编码器的频谱特性。有趣的是，我们的研究揭示了一个极具启发性且鲜被探索的对应关系：编码器的特征频谱与其功能角色之间存在明确关联——语义编码器主要捕获编码抽象含义的低频成分，而像素编码器额外保留传达细粒度细节的高频信息。这一启发式发现为理解编码器行为与其底层频谱结构的关系提供了统一视角。我们将其定义为“棱镜假说”：每种数据模态都可视为现实世界在共享特征频谱上的投影，正如棱镜分光原理所示。基于这一洞见，我们提出统一自编码模型，该模型通过创新的频段调制器协调语义结构与像素细节，实现二者的无缝共存。在ImageNet和MS-COCO基准上的大量实验表明，我们的UAE模型以前沿性能将语义抽象与像素级保真度有效统一于单一潜在空间中。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.19693">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.19693">arXiv</a></p>
<hr />
<h3>3. 面向教学视频编辑的区域约束上下文生成方法</h3>
<p><strong>原文标题：</strong> Region-Constraint In-Context Generation for Instructional Video Editing</p>
<p><strong>摘要：</strong>
上下文生成范式近期在教学图像编辑领域展现出强大的数据效率与合成质量优势。然而，将此类上下文学习机制应用于基于指令的视频编辑并非易事。若未明确指定编辑区域，生成结果可能面临编辑区域定位不准的问题，同时在去噪过程中编辑区域与非编辑区域的语义单元会产生相互干扰。为解决上述挑战，本文提出ReCo——一种创新的教学视频编辑范式，该范式在上下文生成过程中深入探索编辑区域与非编辑区域间的约束建模机制。技术层面，ReCo采用宽度拼接策略将源视频与目标视频联合进行去噪处理。为校准视频扩散学习过程，ReCo设计了两项正则化约束：潜在空间正则化与注意力正则化，分别作用于单步反向去噪的潜在特征与注意力图谱。前者通过增大源视频与目标视频间编辑区域的潜在特征差异，同时减小非编辑区域的差异，从而强化对编辑区域的修改聚焦并抑制非编辑区域的异常内容生成；后者通过抑制目标视频编辑区域语义单元对源视频对应区域的注意力关联，有效降低目标视频新对象生成过程中的语义干扰。此外，我们构建了大规模高质量视频编辑数据集ReCo-Data，包含50万条指令-视频对以支撑模型训练。在四大主流指令式视频编辑任务上的大量实验验证了本方法的优越性。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.17650">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.17650">arXiv</a></p>
<hr />
<h3>4. QuCo-RAG：基于预训练语料库量化不确定性以实现动态检索增强生成</h3>
<p><strong>原文标题：</strong> QuCo-RAG: Quantifying Uncertainty from the Pre-training Corpus for Dynamic Retrieval-Augmented Generation</p>
<p><strong>摘要：</strong>
动态检索增强生成技术通过自适应地决定生成过程中的检索时机，以缓解大语言模型中的幻觉问题。然而，现有方法依赖于模型内部信号（如对数概率、熵），这些信号本质上并不可靠，因为大语言模型通常校准不佳，且常在错误输出中表现出高置信度。我们提出QuCo-RAG方法，将依赖主观置信度转向基于预训练数据计算的客观统计量。该方法通过两个阶段量化不确定性：（1）在生成前，识别指示长尾知识缺口的低频实体；（2）在生成过程中，验证实体在预训练语料库中的共现情况，零共现往往意味着幻觉风险。两个阶段均利用Infini-gram引擎对超过4万亿词元的语料进行毫秒级延迟查询，当不确定性较高时触发检索。在多跳问答基准测试中，实验表明QuCo-RAG在使用OLMo-2模型时相比最先进基线实现了5-12个点的精确匹配提升，并能有效迁移至预训练数据未公开的模型（如Llama、Qwen、GPT），最高提升14个点。在生物医学问答领域的泛化实验进一步验证了该范式的鲁棒性。这些结果确立了基于语料库验证作为一种原则性、实际模型无关的动态检索增强生成范式。我们的代码已公开于https://github.com/ZhishanQ/QuCo-RAG。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.19134">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.19134">arXiv</a></p>
<hr />
<h3>5. 无限单应性作为相机控制视频生成的鲁棒条件约束</h3>
<p><strong>原文标题：</strong> Infinite-Homography as Robust Conditioning for Camera-Controlled Video Generation</p>
<p><strong>摘要：</strong>
视频扩散模型的最新进展激发了人们对动态场景相机控制新视角视频生成日益增长的兴趣，旨在为创作者提供后期制作中的电影级摄像机控制能力。相机控制视频生成的核心挑战在于确保生成内容对指定相机位姿的保真度，同时保持视角一致性，并基于有限观测推断被遮挡的几何结构。为此，现有方法要么在轨迹-视频配对数据集上训练轨迹条件视频生成模型，要么从输入视频估计深度以沿目标轨迹重投影并生成未投影区域。然而，现有方法难以生成高保真相机位姿的高质量视频，主要原因有二：（1）基于重投影的方法极易受深度估计误差影响；（2）现有数据集中相机轨迹的有限多样性制约了学习模型的泛化能力。为突破这些局限，我们提出InfCam——一种无需深度估计、具有高相机位姿保真度的相机控制视频到视频生成框架。该框架集成两个核心组件：（1）无限单应性变换，将三维相机旋转直接编码至视频扩散模型的二维隐空间。基于此无噪声旋转信息进行条件约束，通过端到端训练预测残差视差项，从而实现高精度的相机位姿保真；（2）数据增强流程，将现有合成多视角数据集转换为具有多样化轨迹和焦距的序列。实验结果表明，InfCam在相机位姿精度与视觉保真度上均优于基线方法，并能从合成数据良好泛化至真实场景数据。项目页面链接：https://emjay73.github.io/InfCam/</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.17040">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.17040">arXiv</a></p>
<hr />
<h3>6. 大语言模型能否评估学生困境？基于能力模拟的人机难度对齐在试题难度预测中的应用</h3>
<p><strong>原文标题：</strong> Can LLMs Estimate Student Struggles? Human-AI Difficulty Alignment with Proficiency Simulation for Item Difficulty Prediction</p>
<p><strong>摘要：</strong>
试题（问题或任务）难度的准确估计对教育评估至关重要，但常面临冷启动问题。尽管大语言模型展现出超越人类的问题解决能力，但其能否感知人类学习者的认知困境仍存疑问。本研究通过大规模实证分析，在医学知识和数学推理等多个领域对20余个模型进行人机难度对齐研究。研究发现存在系统性错位现象：扩大模型规模并不能可靠提升对齐效果；模型并未与人类认知对齐，而是趋同于机器共识。研究观察到，高性能表现往往阻碍准确的难度估计——即使被明确要求模拟特定能力水平，模型仍难以有效模拟学生的能力局限。此外，模型存在显著的内省能力缺失，无法预测自身局限性。这些结果表明，通用问题解决能力并不等同于对人类认知困境的理解，凸显出现有模型在自动化难度预测应用中的挑战。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.18880">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.18880">arXiv</a></p>
<hr />
<h3>7. WorldWarp：基于异步视频扩散的三维几何传播框架</h3>
<p><strong>原文标题：</strong> WorldWarp: Propagating 3D Geometry with Asynchronous Video Diffusion</p>
<p><strong>摘要：</strong>
生成长时序且几何一致的视频面临一个根本性困境：几何一致性要求在像素空间中严格遵循三维结构，而当前最先进的生成模型在相机条件化的隐空间中运行效果最佳。这种脱节导致现有方法在处理遮挡区域和复杂相机轨迹时存在困难。为弥合这一差距，本文提出WorldWarp框架，该框架将三维结构锚点与二维生成优化器相结合。为实现几何基础，WorldWarp通过高斯溅射（3DGS）技术维护在线三维几何缓存。通过将历史内容显式变形至新视角，该缓存作为结构支架，确保每个新帧都遵循先验几何关系。然而，静态变形不可避免地会因遮挡产生空洞与伪影。我们通过设计面向“填充-修正”目标的时空扩散（ST-Diff）模型解决此问题。本研究的核心创新在于时空动态噪声调度机制：空白区域接受完整噪声以启动生成，而变形区域则接受部分噪声以实现精细化调整。通过每一步动态更新三维缓存，WorldWarp实现了视频片段间的跨帧一致性。实验表明，该框架通过三维逻辑引导结构生成、扩散逻辑优化纹理细节，达到了当前最优的生成保真度。项目页面：https://hyokong.github.io/worldwarp-page/。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.19678">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.19678">arXiv</a></p>
<hr />
<h3>8. LoGoPlanner：基于度量感知视觉几何的定位支撑导航策略</h3>
<p><strong>原文标题：</strong> LoGoPlanner: Localization Grounded Navigation Policy with Metric-aware Visual Geometry</p>
<p><strong>摘要：</strong>
在非结构化环境中进行轨迹规划是移动机器人的一项基础且具有挑战性的能力。传统的模块化流程存在延迟问题，且在感知、定位、建图和规划模块间易产生级联误差。近期端到端学习方法直接将原始视觉观测映射为控制信号或轨迹，有望在开放世界场景中实现更高的性能与效率。然而，大多数现有端到端方法仍依赖独立的定位模块，这些模块需借助精确的传感器外参标定进行自身状态估计，从而限制了其在不同机器人本体与环境间的泛化能力。本文提出LoGoPlanner，一种基于定位支撑的端到端导航框架，通过以下方式解决上述局限：（1）微调长时程视觉几何骨干网络，使其预测基于绝对度量尺度，从而为精确定位提供隐式状态估计；（2）从历史观测中重建周围场景几何结构，为可靠避障提供密集、细粒度的环境感知；（3）将策略建立在前述辅助任务引导的隐式几何信息基础上，从而减少误差传播。我们在仿真和真实场景中对LoGoPlanner进行评估，其完全端到端的设计降低了累积误差，同时度量感知的几何记忆增强了规划一致性与避障能力，相比基于理想定位的基线方法性能提升超过27.3%，并在不同机器人本体与环境中表现出强大的泛化能力。代码与模型已在项目页面https://steinate.github.io/logoplanner.github.io/公开。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.19629">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.19629">arXiv</a></p>
<hr />
<h3>9. UCoder：基于大型语言模型内部探测的无监督代码生成方法</h3>
<p><strong>原文标题：</strong> UCoder: Unsupervised Code Generation by Internal Probing of Large Language Models</p>
<p><strong>摘要：</strong>
大型语言模型在代码生成任务中展现出卓越能力，但其效果严重依赖于使用大规模标注数据（如问答对）或未标注数据集（如代码片段）进行监督训练，这些数据通常成本高昂且难以大规模获取。为突破这一限制，本文提出IPC方法——一种无需任何外部语料（包括未标注代码片段）的无监督框架，通过内部探测大型语言模型实现代码生成。我们引入问题空间探测、测试理解探测、解决方案空间探测以及知识巩固与强化机制，以挖掘大型语言模型内部存在的知识结构与置信度模式。进一步地，IPC通过自洽性机制与基于表示的质量评估来筛选可靠代码候选样本，用以训练UCoder（基于无监督学习的代码生成器）。我们在多个代码基准测试中验证了所提方法，结果表明无监督方法能够达到与监督方法相竞争的性能，同时显著降低对标注数据和计算资源的依赖。分析实验表明，模型内部状态蕴含丰富的代码质量与正确性信号，有效利用这些信号能够为代码生成任务实现高效的无监督学习，从而为资源受限场景下训练代码大型语言模型开辟了新方向。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.17385">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.17385">arXiv</a></p>
<hr />
<h3>10. GenEnv：大语言模型智能体与环境模拟器间的难度对齐协同进化</h3>
<p><strong>原文标题：</strong> GenEnv: Difficulty-Aligned Co-Evolution Between LLM Agents and Environment Simulators</p>
<p><strong>摘要：</strong>
当前，训练高性能大语言模型智能体的关键瓶颈在于现实世界交互数据的高成本与静态特性。为此，我们提出GenEnv框架，通过在智能体与可扩展的生成式环境模拟器之间建立一种难度对齐的协同进化博弈来解决这一问题。与传统基于静态数据集进行模型进化的方法不同，GenEnv实现了数据的动态演化：模拟器充当动态课程策略，持续生成专门适配智能体“最近发展区”的任务。这一过程由一种简单而有效的α-课程奖励机制引导，确保任务难度与智能体当前能力相匹配。我们在五个基准测试（包括API-Bank、ALFWorld、BFCL、Bamboogle和TravelPlanner）上对GenEnv进行评估。实验表明，GenEnv在7B参数基线模型上的性能提升最高达40.3%，其平均表现达到或超越了更大规模模型的水平。与基于Gemini 2.5 Pro的离线数据增强方法相比，GenEnv在使用数据量减少3.3倍的同时实现了更优的性能。通过从静态监督转向自适应模拟，GenEnv为扩展智能体能力提供了一条数据高效的技术路径。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.19682">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.19682">arXiv</a></p>
<hr />
<h3>11. StoryMem：基于记忆机制的多镜头长视频叙事生成</h3>
<p><strong>原文标题：</strong> StoryMem: Multi-shot Long Video Storytelling with Memory</p>
<p><strong>摘要：</strong>
视觉叙事任务需要生成具备电影级画质与长程一致性的多镜头视频。受人类记忆机制启发，本文提出StoryMem范式，该框架将长视频叙事任务重构为基于显式视觉记忆的迭代式镜头合成过程，将预训练的单镜头视频扩散模型转化为多镜头叙事生成器。这一目标通过创新的记忆到视频（M2V）架构实现：该架构维护着由历史生成镜头关键帧构成的紧凑动态记忆库，并通过潜在空间拼接与负向RoPE偏移技术将存储记忆注入单镜头视频扩散模型，仅需LoRA微调即可实现。结合语义关键帧选择策略与美学偏好过滤机制，进一步保障了生成过程中记忆信息的高效性与稳定性。此外，所提框架天然支持平滑镜头转场与定制化叙事生成应用。为推进评估体系建设，我们构建了涵盖多场景的ST-Bench多镜头视频叙事基准测试集。大量实验表明，StoryMem在保持高美学品质与提示词遵循度的同时，相比现有方法实现了更优越的跨镜头一致性，标志着向分钟级连贯视频叙事生成迈出了重要一步。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.19539">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.19539">arXiv</a></p>
<hr />
<h3>12. LoPA：基于前瞻并行解码的大规模扩散语言模型推理加速方法</h3>
<p><strong>原文标题：</strong> LoPA: Scaling dLLM Inference via Lookahead Parallel Decoding</p>
<p><strong>摘要：</strong>
扩散大语言模型（dLLMs）已展现出高速推理的重要潜力。然而，当前基于置信度的解码策略受限于并行度不足，通常每次前向传播仅能生成1-3个词元（TPF）。本研究首次发现dLLM推理的并行度对词元填充顺序（TFO）具有高度敏感性。为此，我们提出无需训练即插即用的前瞻并行解码算法LoPA，通过优化TFO实现推理加速。LoPA通过并行分支同步探索不同候选TFO，并依据分支置信度筛选具有最大未来并行潜力的路径。将LoPA应用于前沿的D2F模型后，解码效率获得显著提升：在GSM8K数据集上，D2F-Dream模型的TPF提升至10.1，同时保持优于Dream基准模型的性能表现。为适配此突破性并行规模，我们进一步开发了支持分支并行（BP）的专用多设备推理系统，在多GPU部署环境下实现单样本1073.9词元/秒的吞吐量。代码已开源：https://github.com/zhijie-group/LoPA。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.16229">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.16229">arXiv</a></p>
<hr />
<h3>13. MobileWorld：在智能体-用户交互与MCP增强环境中的自主移动智能体基准测试</h3>
<p><strong>原文标题：</strong> MobileWorld: Benchmarking Autonomous Mobile Agents in Agent-User Interactive, and MCP-Augmented Environments</p>
<p><strong>摘要：</strong>
在现有的在线移动使用基准测试中，AndroidWorld因其可复现的环境与确定性评估机制已成为主流基准；然而，近期智能体成功率超过90%的表现表明该基准已趋近饱和，亟需更具挑战性的新基准。此外，该环境缺乏电子商务、企业通信等关键应用类别，且未能体现用户指令模糊、工具混合使用等真实移动使用场景特征。为弥补这一差距，我们提出了MobileWorld——一个显著更具挑战性且能更好反映真实移动使用场景的基准测试，涵盖20个应用程序中的201项任务，同时保持与AndroidWorld同等的可复现评估水平。MobileWorld的挑战性主要体现在两方面：其一，它强调跨应用交互的长周期任务——与AndroidWorld相比，MobileWorld平均需要近两倍的任务完成步骤（27.8步对14.3步），且包含更多跨应用任务（62.2%对9.5%）；其二，MobileWorld通过引入智能体-用户交互及MCP增强任务等新型任务类别，突破了传统图形用户界面操作的范畴。为确保评估的稳健性，我们提供了基于快照的容器环境与精确的功能验证机制，包括后端数据库检查与任务回调接口。我们进一步开发了具备扩展动作空间的规划-执行智能体框架，以支持用户交互与MCP调用。实验结果显示，相较于AndroidWorld，各模型性能出现显著下降：最佳智能体框架与端到端模型的成功率分别仅为51.7%和20.9%。分析表明，当前模型在用户交互与MCP调用方面存在明显不足，这为构建更鲁棒的下一代移动智能技术提供了战略发展路径。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.19432">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.19432">arXiv</a></p>
<hr />
<h3>14. 推理调色板：通过潜在情境化调控推理以实现（视觉）语言模型的可控探索</h3>
<p><strong>原文标题：</strong> Reasoning Palette: Modulating Reasoning via Latent Contextualization for Controllable Exploration for (V)LMs</p>
<p><strong>摘要：</strong>
探索能力深刻影响着大型（视觉）语言模型的推理时表现与强化学习训练效果，因为随机采样常产生冗余的推理路径且缺乏高层级多样性。本文提出“推理调色板”——一种新颖的潜在调制框架，通过引入随机潜变量实现策略性情境化，在词元生成前引导模型的内部规划。该潜在上下文通过变分自编码器从问题-答案对的平均池化嵌入中推断得出，每个采样潜变量可能编码独特的推理情境。在推理阶段，采样潜变量被解码为可学习的词元前缀并附加至输入提示前，从而调控模型的内部推理轨迹。通过这种方式，模型在输出生成前对推理策略进行内部采样，进而塑造整个响应序列的风格与结构。简短的监督微调预热阶段使模型适应这种潜在条件调节机制。在强化学习优化中，推理调色板通过按需注入多样化推理模式实现结构化探索，显著提升探索效率与持续学习能力。在多个推理基准测试上的实验表明，本方法能对（视觉）语言模型的策略行为实现可解释且可控的调控，从而相较标准强化学习方法获得持续的性能提升。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.17206">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.17206">arXiv</a></p>
<hr />
<h3>15. 是否存在优于高斯分布的源分布？探索图像流匹配中的源分布选择</h3>
<p><strong>原文标题：</strong> Is There a Better Source Distribution than Gaussian? Exploring Source Distributions for Image Flow Matching</p>
<p><strong>摘要：</strong>
流匹配作为一种强大的生成建模方法，其源分布的选择具有高度灵活性。尽管高斯分布被广泛采用，但在高维数据生成任务中是否存在更优替代方案的问题尚未得到充分探索。本文提出了一种新颖的二维模拟方法，通过在可解释的二维环境中捕捉高维几何特性，使我们能够分析流匹配在训练过程中的学习动态。基于此分析，我们得出关于流匹配行为的若干关键发现：（1）密度近似可能因模态差异而降低生成性能；（2）方向对齐在过度集中时会产生路径纠缠问题；（3）高斯分布的全向覆盖特性可确保稳健学习；（4）范数失准会导致显著的学习成本。基于这些发现，我们提出了一种结合范数对齐训练与方向剪枝采样的实用框架。该方法既保持了稳定流学习所必需的全向监督机制，又在推理阶段消除了数据稀疏区域的初始化问题。值得注意的是，我们的剪枝策略可应用于任何使用高斯源分布训练的流匹配模型，无需重新训练即可获得即时性能提升。实证评估表明，该方法在生成质量和采样效率方面均能实现持续改进。我们的研究为源分布设计提供了实用见解与指导原则，并提出了一种可直接应用于改进现有流匹配模型的技术。代码已开源：https://github.com/kwanseokk/SourceFM。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.18184">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.18184">arXiv</a></p>
<hr />
<h3>16. Real2Edit2Real：通过三维控制界面生成机器人演示数据</h3>
<p><strong>原文标题：</strong> Real2Edit2Real: Generating Robotic Demonstrations via a 3D Control Interface</p>
<p><strong>摘要：</strong>
机器人学习的最新进展得益于大规模数据集和强大的视觉运动策略架构，但策略的鲁棒性仍受限于收集多样化演示数据的高昂成本，尤其是在操作任务的空间泛化方面。为减少重复性数据采集，本文提出Real2Edit2Real框架，该框架通过三维控制界面将三维可编辑性与二维视觉数据相融合，从而生成新的演示数据。我们的方法首先利用度量尺度三维重建模型，从多视角RGB观测中重建场景几何结构。基于重建的几何结构，我们在点云上进行深度可靠的三维编辑以生成新的操作轨迹，同时通过几何校正机器人姿态以恢复物理一致的深度信息，这为合成新演示提供了可靠条件。最后，我们提出一种以深度为主要控制信号，并结合动作、边缘和射线图的多条件视频生成模型，用于合成空间增强的多视角操作视频。在四个真实世界操作任务上的实验表明，仅使用1-5个源演示生成数据训练的策略，其性能可达到甚至超越使用50个真实世界演示训练的策略，将数据效率提升高达10-50倍。此外，在高度和纹理编辑方面的实验结果证明了该框架的灵活性和可扩展性，表明其有潜力成为统一的数据生成框架。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.19402">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.19402">arXiv</a></p>
<hr />
<h3>17. 能否核对无误？风险投资领域自主法律智能体的发展路径</h3>
<p><strong>原文标题：</strong> Does It Tie Out? Towards Autonomous Legal Agents in Venture Capital</p>
<p><strong>摘要：</strong>
在完成风险投资融资轮次前，律师需开展包括股权结构表核验在内的尽职调查工作，即通过大量底层法律文件验证每项证券（如股份、期权、认股权证）及发行条款（如归属时间表、加速触发条件、转让限制）的合规性。尽管大语言模型在法律基准测试中持续进步，但针对股权结构核验等专业法律工作流程，即使当前先进的智能体系统仍难以胜任。该任务需要多文档推理能力、严格的证据可追溯性以及确定性输出，而现有技术方案尚无法稳定实现这些要求。本文将股权结构核验界定为法律人工智能的现实基准测试案例，系统分析与比较现有智能体系统的表现，进而提出面向自动化核验任务的世界模型架构——该架构亦可作为应用型法律智能更广泛发展的基础框架。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.18658">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.18658">arXiv</a></p>
<hr />
<h3>18. CASA：通过自注意力实现跨模态注意力的高效视觉语言融合</h3>
<p><strong>原文标题：</strong> CASA: Cross-Attention via Self-Attention for Efficient Vision-Language Fusion</p>
<p><strong>摘要：</strong>
视觉语言模型通常通过将预训练视觉编码器生成的图像标记插入语言模型的文本流中进行训练。这种方法允许文本与图像信息在模型内部充分交互，但在处理高分辨率图像、长对话或流式视频时，会带来极高的内存与计算成本。采用跨注意力机制的视觉语言模型是标记插入方法的高效替代方案，但其性能存在明显差距，尤其在涉及细粒度视觉细节的任务中。我们发现，提升此类模型性能的关键在于在专用跨注意力层中同时实现局部文本到文本的交互。基于此，我们提出CASA（通过自注意力实现跨模态注意力），这是一种简洁高效的范式。该范式在常见图像理解基准测试中显著缩小了与全标记插入方法的性能差距，同时在处理流式视频描述等长上下文多模态任务时，保持了与跨注意力模型同等的可扩展性。示例与代码请访问项目页面：https://kyutai.org/casa。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.19535">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.19535">arXiv</a></p>
<hr />
<h3>19. MatSpray：将二维材料世界知识融合至三维几何结构</h3>
<p><strong>原文标题：</strong> MatSpray: Fusing 2D Material World Knowledge on 3D Geometry</p>
<p><strong>摘要：</strong>
在游戏与影视行业中，手动建模材质参数与三维几何结构是一项耗时但关键的任务。尽管三维重建技术的最新进展已能实现对场景几何结构与外观的精确近似，但由于缺乏精确且空间变化的材质参数，这些方法在重光照场景中往往表现不足。与此同时，基于二维图像的扩散模型在预测基于物理的渲染（PBR）属性（如反照率、粗糙度与金属度）方面展现出强大性能。然而，将这些二维材质贴图迁移至重建的三维几何结构上仍面临重大挑战。本文提出一种融合学习与投影的创新框架，将二维材质数据融入三维几何结构。我们首先通过高斯泼溅技术重建场景几何，并利用扩散模型从输入图像生成反照率、粗糙度与金属度的二维贴图——任何能够将图像或视频转换为PBR材质的现有扩散模型均可适用。随后，通过优化基于图像的损失函数，或借助高斯光线追踪将材质参数直接投影至高斯单元，将这些预测结果进一步整合到三维表征中。为提升细节精度与多视角一致性，我们进一步引入轻量级神经优化步骤（神经融合器），该模块以光线追踪生成的材质特征为输入，输出精细化调整结果。实验表明，所提方法在量化指标与视觉真实感方面均优于现有技术，能够从重建场景中生成更精确、可重光照且具有照片级真实感的渲染效果，显著提升了内容生产流程中资产创建工作的真实感与效率。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.18314">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.18314">arXiv</a></p>
<hr />
<h3>20. 部件识别：三维部件分割与命名</h3>
<p><strong>原文标题：</strong> Name That Part: 3D Part Segmentation and Naming</p>
<p><strong>摘要：</strong>
本文研究语义三维部件分割问题，即如何将物体分解为具有语义命名的部件。尽管现有数据集包含部件标注信息，但不同数据集间的定义标准不一致，限制了模型的鲁棒性训练。现有方法通常仅生成未标注的分解结果，或在缺乏完整形状标注的情况下检索单一部件。我们提出ALIGN-Parts方法，将部件命名建模为直接集合对齐任务。该方法将三维形状分解为部件单元——一种隐式三维部件表征，并通过二分图匹配与部件描述进行关联。我们融合了三维部件场的几何特征、多视角视觉特征的外观信息，以及语言模型生成的功能描述所包含的语义知识。通过文本对齐损失函数，部件单元与文本共享嵌入空间，在数据充足的条件下实现了理论上的开放词汇匹配框架。我们提出的高效、新颖的单次三维部件分割与命名方法，在多个下游任务中具有应用价值，包括作为可扩展的标注引擎。该模型支持对任意描述的零样本匹配，并对已知类别提供置信度校准预测。经人工验证，我们构建了统一的本体框架，整合了PartNet、3DCoMPaT++和Find3D数据集，涵盖1,794个独立三维部件。同时展示了新构建的Tex-Parts数据集样例，并针对命名三维部件分割任务提出了两种创新性评估指标。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.18003">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.18003">arXiv</a></p>
<hr />
<h3>21. 从形式与自然语言视角理解大语言模型的三段论推理能力</h3>
<p><strong>原文标题：</strong> Understanding Syllogistic Reasoning in LLMs from Formal and Natural Language Perspectives</p>
<p><strong>摘要：</strong>
本研究从逻辑学与自然语言双重视角探讨大语言模型的三段论推理能力。在此过程中，我们深入考察大语言模型的基础推理机制及其研究发展趋势。为支撑研究，我们选取了14个大型语言模型，分别从符号推理与自然语言理解两个维度系统检验其三段论推理性能。研究结果表明，虽然这种推理能力并非所有大语言模型普遍具备的涌现特性，但部分模型在符号推理任务中展现的完美表现促使我们思考：大语言模型是否正在演化为日益形式化的推理机制，而非真正揭示人类推理过程的微妙特征。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.12620">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.12620">arXiv</a></p>
<hr />
<h3>22. Over++：面向图层交互效果的生成式视频合成</h3>
<p><strong>原文标题：</strong> Over++: Generative Video Compositing for Layer Interaction Effects</p>
<p><strong>摘要：</strong>
在专业视频合成工作流程中，艺术家需要手动创建前景主体与背景图层之间的环境交互效果（如阴影、反射、扬尘、飞溅等）。现有视频生成模型难以在添加此类效果的同时保持输入视频内容，而当前视频修复方法要么需要逐帧的高成本掩码标注，要么会产生不符合物理规律的结果。本文提出增强合成这一新任务，其目标是在保持原始场景的基础上，根据文本提示与输入视频图层合成逼真的半透明环境效果。为此，我们提出Over++视频特效生成框架，该框架无需对相机位姿、场景静止性或深度监督进行假设。我们构建了针对该任务的配对特效数据集，并提出保留文本驱动编辑能力的非配对增强策略。该方法还支持可选的掩码控制和关键帧引导，且无需密集标注。尽管在有限数据上训练，Over++仍能生成多样化且逼真的环境效果，在特效生成与场景保持两方面均优于现有基线方法。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.19661">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.19661">arXiv</a></p>
<hr />
<h3>23. 基于大脑的阅读与调控大语言模型状态的坐标轴</h3>
<p><strong>原文标题：</strong> Brain-Grounded Axes for Reading and Steering LLM States</p>
<p><strong>摘要：</strong>
针对大语言模型（LLMs）的可解释性方法通常依赖于文本监督来推导方向向量，但此类方法往往缺乏外部实体基础。本研究提出以人脑活动作为坐标系统（而非训练信号），用于读取与调控大语言模型的内部状态。基于SMN4Lang脑磁图数据集，我们构建了词级别的相位锁定值模式脑图谱，并通过独立成分分析提取潜在坐标轴。这些坐标轴通过独立词典和基于命名实体识别的标签进行验证（词性/对数词频作为有效性检验），随后训练轻量级适配器将大语言模型隐藏状态映射至这些大脑坐标轴，而无需对大语言模型本身进行微调。沿大脑衍生的方向进行调控后，在TinyLlama模型的中间层发现了一个稳健的词汇（与词频相关）坐标轴，该结果在困惑度匹配的对照实验中依然成立；与基于文本的探针相比，大脑坐标轴在实现更低困惑度的同时展现出更大的对数词频偏移量。此外，一个功能/内容坐标轴（第13轴）在TinyLlama、Qwen2-0.5B和GPT-2模型中均表现出一致的调控效果，并得到困惑度匹配的文本层面证据支持。TinyLlama第4层的影响虽显著但不稳定，故我们将其视为次要发现（详见附录）。当使用不含GPT嵌入变化特征的图谱或采用word2vec嵌入重建图谱时，坐标轴结构保持稳定（匹配坐标轴间|r|=0.64-0.95），降低了循环论证的担忧。探索性功能磁共振成像锚定实验提示嵌入变化与对数词频可能存在潜在关联，但该结果对血流动力学模型假设较为敏感，因此仅视为群体层面的参考证据。这些研究结果支持一种新型交互范式：基于神经生理学的坐标轴为大语言模型行为提供了可解释且可控的调控路径。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.19399">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.19399">arXiv</a></p>
<hr />
<h3>24. SecureCode v2.0：用于训练安全感知代码生成模型的生产级数据集</h3>
<p><strong>原文标题：</strong> SecureCode v2.0: A Production-Grade Dataset for Training Security-Aware Code Generation Models</p>
<p><strong>摘要：</strong>
AI助手在45%的安全相关场景中会产生存在漏洞的代码，从而将缺陷大规模引入生产系统。然而，现有的安全编码数据集存在不足：它们缺乏真实事件依据，无法满足现代训练所需的规模，并且缺失开发者在生产部署时所需的操作安全上下文。我们提出了SecureCode v2.0，这是一个包含1,215个聚焦安全性的编码示例的生产级数据集，所有示例均通过了结构验证和专家安全审查。每个示例均关联到具有CVE编号的实际已记录安全事件，提供存在漏洞的实现与安全实现方案，展示具体攻击方式，并包含纵深防御操作指南。该数据集涵盖11个漏洞类别（完整覆盖OWASP Top 10:2025及AI/ML安全威胁）和11种编程语言（Python、JavaScript、Java、Go、PHP、C#、TypeScript、Ruby、Rust、Kotlin以及用于基础设施即代码的YAML）。</p>
<p>我们的质量保障框架确保所有示例均基于真实事件。每个示例包含SIEM集成策略、基础设施加固建议（Docker、AppArmor、WAF配置）以及使用语言适配框架的测试方法。数据集采用4轮对话结构模拟真实开发者与AI的交互过程，从基础实现逐步升级到高级安全考量与纵深防御指导。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.18542">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.18542">arXiv</a></p>
<hr />
<h2>🔍 关键词云图</h2>
<p><img alt="关键词云图" src="../images/keywords_wordcloud.png" /></p>
<h2>📈 近期论文趋势</h2>
<p><img alt="论文趋势" src="../images/daily_papers.png" /></p>
<h2>🎙️ 语音播报</h2>
<ul>
<li><a href="../audio/2025-12-23_daily_papers.mp3">收听今日论文解读</a></li>
</ul>
<h2>📱 订阅渠道</h2>
<ul>
<li>GitHub: <a href="https://github.com/2404589803/hf-daily-paper-newsletter-chinese">hf-daily-paper-newsletter-chinese</a></li>
</ul>
    </div>
</body>
</html>