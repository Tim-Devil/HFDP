<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Hugging Face 论文日报 - 2025-11-18</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', sans-serif;
            line-height: 1.6;
            color: #333;
            background-color: #f5f5f5;
            padding: 20px;
        }
        
        .container {
            max-width: 900px;
            margin: 0 auto;
            background-color: white;
            padding: 40px;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }
        
        h1 {
            color: #2c3e50;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
            margin-bottom: 30px;
            font-size: 28px;
        }
        
        h1 img {
            vertical-align: middle;
            margin-right: 10px;
        }
        
        h2 {
            color: #34495e;
            margin-top: 40px;
            margin-bottom: 20px;
            font-size: 24px;
            border-left: 4px solid #3498db;
            padding-left: 15px;
        }
        
        h3 {
            color: #2c3e50;
            margin-top: 30px;
            margin-bottom: 15px;
            font-size: 20px;
        }
        
        p {
            margin-bottom: 15px;
            text-align: justify;
        }
        
        ul {
            margin-left: 20px;
            margin-bottom: 20px;
        }
        
        li {
            margin-bottom: 8px;
        }
        
        a {
            color: #3498db;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        strong {
            color: #2c3e50;
            font-weight: 600;
        }
        
        hr {
            border: none;
            border-top: 1px solid #e0e0e0;
            margin: 30px 0;
        }
        
        /* 关键修复:限制图片宽度 */
        img {
            max-width: 100%;
            height: auto;
            display: block;
            margin: 20px auto;
            border-radius: 4px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        
        /* 确保图片容器也有宽度限制 */
        p img {
            max-width: 100%;
        }
        
        /* 论文详情区域样式 */
        .paper-section {
            background-color: #f9f9f9;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 20px;
        }
        
        /* 统计信息样式 */
        .stats {
            background-color: #e8f4f8;
            padding: 15px;
            border-radius: 6px;
            margin-bottom: 20px;
        }
        
        /* 响应式设计 */
        @media (max-width: 768px) {
            .container {
                padding: 20px;
            }
            
            h1 {
                font-size: 24px;
            }
            
            h2 {
                font-size: 20px;
            }
            
            h3 {
                font-size: 18px;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <h1><img src="https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo.png" width="30"/> Hugging Face 2025-11-18 论文日报</h1>
<h2>📊 今日论文统计</h2>
<ul>
<li>总论文数：28</li>
<li>热门领域：LLM, GPT, RL, Transformer</li>
</ul>
<h2>📝 论文详情</h2>
<h3>1. P1：基于强化学习的物理奥林匹克竞赛解题系统</h3>
<p><strong>原文标题：</strong> P1: Mastering Physics Olympiads with Reinforcement Learning</p>
<p><strong>摘要：</strong>
大语言模型的最新进展已将其能力边界从谜题求解推进至科学级推理——这类推理要求答案必须经得起自然规律的检验，而非仅符合评分标准。物理学作为检验这一转变的关键领域，以根本性的方式将符号与现实相联结，构成了大多数现代技术的基石。本研究通过开发具备卓越物理推理能力的大语言模型，成功推动了物理研究进展，特别是在解决奥林匹克级别物理问题方面表现突出。我们提出P1系列模型——完全基于强化学习训练的开源物理推理模型族。其中P1-235B-A22B是首个在国际物理奥林匹克竞赛（IPhO 2025）中达到金牌水平的开源模型，并在2024/2025年度的13项国际/地区物理竞赛中斩获12枚金牌。P1-30B-A3B同样在IPhO 2025中超越绝大多数开源模型，获得银牌成绩。进一步结合智能体框架PhysicsMinions后，P1-235B-A22B+PhysicsMinions在IPhO 2025中荣膺总排名第一，并在13项物理竞赛中取得最高平均分。除物理领域外，P1系列模型在数学与编程等推理任务中也展现出卓越性能，体现了该模型家族强大的泛化能力。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.13612">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.13612">arXiv</a></p>
<hr />
<h3>2. MiroThinker：通过模型、上下文与交互式扩展突破开源研究智能体的性能边界</h3>
<p><strong>原文标题：</strong> MiroThinker: Pushing the Performance Boundaries of Open-Source Research Agents via Model, Context, and Interactive Scaling</p>
<p><strong>摘要：</strong>
本文推出MiroThinker v1.0开源研究智能体，旨在推进工具增强推理与信息检索能力的发展。与仅扩大模型规模或上下文长度的传统方案不同，MiroThinker开创性地在模型层面实施交互式扩展，通过系统化训练使模型能够处理更深层次、更高频次的智能体-环境交互，将其作为性能提升的第三维度。相较于孤立运行且长推理链存在性能退化风险的LLM测试时扩展，交互式扩展通过环境反馈与外部信息获取实现错误修正与路径优化。借助强化学习，该模型实现了高效的交互扩展：在256K上下文窗口支持下，单个任务可执行高达600次工具调用，支撑持续多轮推理与复杂现实研究流程。在GAIA、HLE、BrowseComp及BrowseComp-ZH四个代表性基准测试中，72B参数变体分别取得81.9%、37.7%、47.1%与55.6%的准确率，超越既有开源智能体并逼近GPT-5-high等商业系统。分析表明，MiroThinker持续受益于交互式扩展：随着智能体-环境交互深度与频次提升，研究性能呈现可预测的增长，证明交互深度具有与模型规模、上下文长度类似的扩展规律。这些发现确立了交互式扩展作为构建下一代开源研究智能体的第三关键维度，与模型能力及上下文窗口形成有效互补。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.11793">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.11793">arXiv</a></p>
<hr />
<h3>3. Uni-MoE-2.0-Omni：基于先进混合专家架构、训练策略与数据技术的语言核心型全模态大模型扩展</h3>
<p><strong>原文标题：</strong> Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data</p>
<p><strong>摘要：</strong>
我们推出荔枝家族系列新品Uni-MoE 2.0。作为完全开源的全模态大模型，该模型在以语言为核心的多模态理解、推理与生成能力上显著推进了荔枝Uni-MoE系列的发展。基于Qwen2.5-7B稠密架构，我们通过三大核心创新实现从零构建：动态容量混合专家设计、结合迭代强化策略的渐进式训练方法，以及精心构建的多模态数据匹配技术。该模型具备全模态理解能力，并能生成图像、文本与语音。在架构层面，新型混合专家框架通过共享专家、路由专家与空置专家的协同机制，在十种跨模态输入场景中实现计算效率与模型能力的平衡，而全模态三维旋转位置编码技术则确保自注意力层的时空跨模态对齐。训练策略上，在跨模态预训练后采用渐进式监督微调，通过激活模态专属专家并结合均衡数据组合与迭代式GSPO-DPO方法，有效稳定强化学习训练过程并提升推理能力。数据层面，基于约750亿token开源多模态数据训练的基础模型，通过特殊语音与图像生成标记的引入，实现了基于语言线索的条件化生成学习。在85个基准测试的广泛评估中，本模型在76个基准测试中超过50项表现优于采用1.2万亿token训练的Qwen2.5-Omni，在领先全模态大模型中达到顶尖或极具竞争力的性能。核心优势体现在视频理解（8项测试平均提升7%）、全模态理解（4项测试平均提升7%）和视听推理（提升4%）领域，同时显著推进长语音处理（词错误率降低4.2%），并在底层图像处理与可控生成的5项指标中保持领先。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.12609">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.12609">arXiv</a></p>
<hr />
<h3>4. Souper-Model：简单算术如何实现最先进大语言模型性能</h3>
<p><strong>原文标题：</strong> Souper-Model: How Simple Arithmetic Unlocks State-of-the-Art LLM Performance</p>
<p><strong>摘要：</strong>
大语言模型（LLMs）已在多个领域展现出卓越能力，但其训练过程仍需要耗费大量资源和时间，不仅需要强大的计算能力，还需精心设计训练流程。模型融合——即对相同架构的多个模型权重进行平均——已成为一种颇具前景的训练前/后优化技术，无需昂贵重训练即可提升模型性能。本文提出类别专家融合法（SoCE），该方法通过基准测试构成分析筛选最优模型候选集，并采用非均匀加权平均来最大化模型性能。与传统均匀平均方法不同，我们基于模型在不同基准类别间表现相关性较低的观测，为每个弱相关类别集群识别“专家”模型，并通过优化加权而非均匀权重进行组合。实验证明，该方法在多语言能力、工具调用、数学推理等多个领域均能提升模型性能与鲁棒性，并在伯克利函数调用排行榜上取得了最先进的成绩。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.13254">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.13254">arXiv</a></p>
<hr />
<h3>5. Part-X-MLLM：面向部件感知的三维多模态大语言模型</h3>
<p><strong>原文标题：</strong> Part-X-MLLM: Part-aware 3D Multimodal Large Language Model</p>
<p><strong>摘要：</strong>
本文提出Part-X-MLLM——一种原生三维多模态大语言模型，通过将多样化三维任务构建为结构化可执行语法中的程序，实现了任务统一。给定RGB点云和自然语言提示，本模型能自回归生成单一连贯的标记序列，该序列编码了部件级边界框、语义描述及编辑指令。这种结构化输出作为多功能接口，可驱动下游几何感知模块实现基于部件的生成与编辑。通过将符号规划与几何合成解耦，我们的方法允许任何兼容的几何引擎通过单一语言原生前端进行控制。我们预训练了双编码器架构以实现结构与语义解耦，并在大规模以部件为中心的数据集上对模型进行指令微调。实验表明，该模型能出色生成高质量结构化方案，通过统一接口在具身问答、组合生成及局部化编辑任务中实现最先进性能。项目页面：https://chunshi.wang/Part-X-MLLM/</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.13647">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.13647">arXiv</a></p>
<hr />
<h3>6. MMaDA-并行：面向思维感知编辑与生成的多模态扩散大语言模型</h3>
<p><strong>原文标题：</strong> MMaDA-Parallel: Multimodal Large Diffusion Language Models for Thinking-Aware Editing and Generation</p>
<p><strong>摘要：</strong>
尽管思维感知生成技术旨在提升复杂任务的处理性能，但我们发现现有自回归序列方法存在关键缺陷——错误传播效应反而可能导致性能退化。为系统分析该问题，我们提出ParaBench基准测试，专门用于评估文本与图像双模态输出。基于ParaBench的分析表明，这种性能退化与生成推理过程和最终图像间的对齐偏差密切相关。为此，我们提出并行多模态扩散框架MMaDA-Parallel，通过在完整去噪轨迹中实现文本与图像的持续双向交互来解决问题。该框架首先通过监督微调进行训练，继而采用新型并行强化学习策略（ParaRL）进行优化——该策略沿去噪轨迹施加语义奖励以强化跨模态一致性。实验验证表明，我们的模型显著改善了跨模态对齐与语义连贯性，在ParaBench基准上相较最先进的Bagel模型实现了输出对齐指标6.9%的提升，为思维感知图像合成建立了更稳健的范式。代码已开源于：https://github.com/tyfeld/MMaDA-Parallel</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.09611">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.09611">arXiv</a></p>
<hr />
<h3>7. GroupRank：一种强化学习驱动的分组重排序范式</h3>
<p><strong>原文标题：</strong> GroupRank: A Groupwise Reranking Paradigm Driven by Reinforcement Learning</p>
<p><strong>摘要：</strong>
大语言模型展现出作为重排序器的强大潜力，能够有效提升RAG系统的整体性能。然而现有重排序范式面临核心理论与实践的困境：点式排序方法虽然简单灵活，但独立评估文档的特性使其容易陷入"排序短视陷阱"，忽略文档间的相对重要性；而列式排序方法虽能感知全局排序上下文，却存在固有的"列表刚性"缺陷，在处理大规模候选集时面临严重的可扩展性与灵活性挑战。为突破这些限制，我们提出分组式排序这一新型重排序范式。该方法将查询与候选文档组共同输入模型，通过组内比较机制为每个文档分配独立相关性分数。这种设计既保留了点式方法的灵活性，又兼具列式方法的比较能力。我们采用GRPO进行模型训练，并配置融合排序指标与分布奖励的异构奖励函数，以实现跨组分数分布的对齐。针对高质量标注数据稀缺的瓶颈，我们进一步提出创新性的检索与排序数据合成流程，所生成数据不仅可用于训练重排序器，也能用于训练检索器。大量实验验证了我们方法的有效性：在BRIGHT和R2MED两个推理密集型检索基准测试中均取得显著成效。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.11653">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.11653">arXiv</a></p>
<hr />
<h3>8. TiViBench：面向视频生成模型的视觉思维推理基准测试框架</h3>
<p><strong>原文标题：</strong> TiViBench: Benchmarking Think-in-Video Reasoning for Video Generative Models</p>
<p><strong>摘要：</strong>
视频生成模型的快速发展已使其重点从产生视觉合理的输出转向处理需要物理合理性和逻辑一致性的任务。然而，尽管近期出现了如Veo 3的帧序列推理等突破性进展，这些模型是否具备与大型语言模型（LLM）相似的推理能力仍不明确。现有基准测试主要评估视觉保真度和时序连贯性，未能捕捉高阶推理能力。为填补这一空白，我们提出TiViBench——一个专门用于评估图像到视频（I2V）生成模型推理能力的分层基准框架。TiViBench系统性地从四个维度评估推理能力：i）结构推理与搜索，ii）空间与视觉模式推理，iii）符号与逻辑推理，以及iv）行动规划与任务执行，涵盖3个难度级别下的24种多样化任务场景。通过广泛评估，我们发现商业模型（如Sora 2、Veo 3.1）展现出更强的推理潜力，而开源模型虽存在未开发潜力，但仍受限于训练规模和数据多样性的不足。为进一步释放这种潜力，我们提出VideoTPO——一种受偏好优化启发的简易高效测试时策略。该方法通过对生成候选结果进行LLM自分析来识别优劣，无需额外训练、数据或奖励模型即可显著提升推理性能。TiViBench与VideoTPO共同为视频生成模型的推理能力评估与推进铺平道路，为这一新兴领域的未来研究奠定基础。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.13704">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.13704">arXiv</a></p>
<hr />
<h3>9. PhysX-Anything：基于单张图像生成仿真就绪的物理三维资产</h3>
<p><strong>原文标题：</strong> PhysX-Anything: Simulation-Ready Physical 3D Assets from Single Image</p>
<p><strong>摘要：</strong>
三维建模正从静态视觉表示转向可直接用于仿真与交互的物理化可动资产。然而现有三维生成方法大多忽略了关键的物理属性与可动特性，限制了其在具身智能中的应用。为弥补这一差距，我们提出PhysX-Anything——首个仿真就绪的物理三维生成框架，仅需输入单张真实场景图像，即可生成具有显式几何结构、可动属性与物理特征的高质量仿真就绪三维资产。具体而言，我们提出了首个基于视觉语言模型的物理三维生成方法，并创新性地设计了一种高效表征几何结构的三维表示方法。该方法将表征所需标记数量降低193倍，在标准视觉语言模型标记预算内实现显式几何学习，无需在微调阶段引入特殊标记即可显著提升生成质量。此外，为克服现有物理三维数据集多样性不足的问题，我们构建了PhysX-Mobility数据集，将原有物理三维数据集的物体类别扩展2倍以上，涵盖2,000余个具有丰富物理标注的常见现实物体。在PhysX-Mobility数据集与真实场景图像上的大量实验表明，PhysX-Anything具有卓越的生成性能与鲁棒泛化能力。在MuJoCo风格环境中开展的仿真实验进一步验证了本方法生成的仿真就绪资产可直接用于接触密集型的机器人策略学习。我们相信PhysX-Anything将有力推动具身智能与物理仿真等下游应用的全面发展。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.13648">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.13648">arXiv</a></p>
<hr />
<h3>10. UFO^3：编织智能体数字星系</h3>
<p><strong>原文标题：</strong> UFO^3: Weaving the Digital Agent Galaxy</p>
<p><strong>摘要：</strong>
基于大语言模型的智能体正在将数字设备从被动工具转变为主动智能协作者。然而现有框架大多局限于单一操作系统或设备，导致跨设备工作流脆弱且高度依赖人工操作。我们提出UFO^3系统，该系统将异构终端设备（包括桌面端、服务器、移动设备与边缘节点）统一整合为协同编排架构。UFO^3将用户请求建模为可演化的任务星群：通过具有显式控制流与数据依赖关系的原子化子任务构成分布式有向无环图。随着分布式设备持续返回执行结果，任务星群持续动态演化，支持异步执行、自适应恢复与动态优化。星群编排器在实施动态图更新的同时保障任务安全异步执行，而智能体交互协议则通过持久化低延迟通道实现可靠的任务调度与结果流式传输。这些设计打破了设备与平台间的传统壁垒，使智能体能够无缝协作并增强集体智能。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.11332">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.11332">arXiv</a></p>
<hr />
<h3>11. 进化方法而非提示：大语言模型越狱攻击的演化合成</h3>
<p><strong>原文标题：</strong> Evolve the Method, Not the Prompts: Evolutionary Synthesis of Jailbreak Attacks on LLMs</p>
<p><strong>摘要：</strong>
大语言模型自动红队测试框架日益精进，但其仍存在根本性局限：越狱逻辑始终受限于选择、组合或优化既有攻击策略。这种束缚限制了系统创造性，使其无法自主发明全新攻击机制。为突破此限制，我们提出EvoSynth框架，将范式从攻击规划转向越狱方法的演化合成。该框架采用多智能体系统自主设计、进化并执行基于代码的新型攻击算法，其核心突破在于引入代码级自修正循环机制，使系统能够根据失败反馈迭代重写自身攻击逻辑。大量实验表明，EvoSynth不仅在对Claude-Sonnet-4.5等高鲁棒性模型的测试中达到85.5%的攻击成功率，创下最新技术标杆，其生成的攻击方法多样性也显著超越现有技术。我们公开此框架以促进越狱方法演化合成这一新方向的研究。代码地址：https://github.com/dongdongunique/EvoSynth。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.12710">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.12710">arXiv</a></p>
<hr />
<h3>12. 回归本质：让去噪生成模型真正实现去噪</h3>
<p><strong>原文标题：</strong> Back to Basics: Let Denoising Generative Models Denoise</p>
<p><strong>摘要：</strong>
当今的去噪扩散模型并非以经典意义实现"去噪"，即不直接预测干净图像。相反，神经网络预测的是噪声或含噪量值。本文提出，预测干净数据与预测含噪量值存在本质区别。根据流形假设，自然数据应位于低维流形上，而含噪量值则不然。基于此假设，我们主张采用直接预测干净数据的模型，这使得表观容量不足的网络能在高维空间中有效运作。我们证明：无需标记器、预训练或额外损失的简单大尺寸块Transformer在像素层面即可成为强效生成模型。该方法在概念上仅体现为"纯图像Transformer"（简称JiT）。我们在ImageNet数据集256×256和512×512分辨率上使用16×16和32×32大尺寸块进行实验，结果表明JiT能取得具有竞争力的效果——而在相同设置下预测高维含噪量值可能导致灾难性失败。通过让网络回归流形本质，我们的研究既回归基础理论，又致力于在原始自然数据上构建自包含的基于Transformer的扩散范式。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.13720">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.13720">arXiv</a></p>
<hr />
<h3>13. OlmoEarth：面向多模态地球观测的稳定潜在图像建模</h3>
<p><strong>原文标题：</strong> OlmoEarth: Stable Latent Image Modeling for Multimodal Earth Observation</p>
<p><strong>摘要：</strong>
地球观测数据呈现出独特的挑战性：既具备图像的空间特性，又具有视频或文本的时序特征，且呈现高度多模态特性。本文提出OlmoEarth——一个多模态时空基础模型，采用专为地球观测领域设计的新型自监督学习框架、掩码策略与损失函数。在各类研究基准测试和来自外部合作伙伴的实际任务中，OlmoEarth相较于其他12种基础模型实现了最先进的性能表现。在嵌入向量评估中，OlmoEarth在24项任务中的15项取得最佳性能；经过完整微调后，其在29项任务中的19项表现最优。我们将OlmoEarth部署为端到端地球观测模型平台的核心架构，该平台涵盖数据采集、标注、训练及推理全流程。OlmoEarth平台将前沿基础模型与强大数据管理工具赋能致力于解决全球重大问题的非营利组织与非政府机构。OlmoEarth的源代码、训练数据与预训练权重已发布于https://github.com/allenai/olmoearth_pretrain。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.13655">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.13655">arXiv</a></p>
<hr />
<h3>14. Live-SWE-agent：软件工程智能体能否实现实时自主演进？</h3>
<p><strong>原文标题：</strong> Live-SWE-agent: Can Software Engineering Agents Self-Evolve on the Fly?</p>
<p><strong>摘要：</strong>
大型语言模型正在重塑包括软件工程在内的几乎所有行业领域。近年来，已有诸多基于大语言模型的智能体被提出以解决现实世界中的软件问题。此类软件智能体通常配备一套编码工具，能够自主决策后续操作以形成完整任务轨迹，从而解决端到端的软件任务。尽管前景可观，这些智能体往往需要专门设计且仍可能非最优解，因为穷尽整个智能体框架设计空间极具挑战且成本高昂。鉴于软件智能体本质上是可被进一步优化/修改的软件实体，研究者近期提出了若干自改进型软件智能体，包括达尔文-哥德尔机。然而，这类自改进智能体需要在特定基准测试上进行昂贵的离线训练，且在不同大语言模型或基准测试间可能泛化能力有限。本文提出Live-SWE-agent——首个能在解决现实软件问题过程中实时自主持续演进的在线软件智能体。具体而言，该智能体从仅配备基础bash工具的最简框架起步，在解决实际软件问题时自主演进其框架实现。在广泛研究的SWE-bench Verified基准测试中，我们的评估表明Live-SWE-agent无需测试时扩展即可实现75.4%的惊人解决率，超越所有现有开源软件智能体，接近最佳专有解决方案的性能。此外，Live-SWE-agent在最新的SWE-Bench Pro基准测试中优于最先进的人工设计软件智能体，以45.8%的解决率创下当前最佳纪录。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.13646">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.13646">arXiv</a></p>
<hr />
<h3>15. 基因组下一标记预测器具备上下文学习能力</h3>
<p><strong>原文标题：</strong> Genomic Next-Token Predictors are In-Context Learners</p>
<p><strong>摘要：</strong>
上下文学习（ICL）——即模型从输入中提供的示例推断并应用抽象模式的能力——已在基于人类文本进行下一标记预测训练的大语言模型中得到广泛研究。事实上，先前研究常将这种涌现行为归因于人类语言独特的统计特性。这引发了一个根本性问题：在其他序列领域，ICL能否纯粹通过大规模预测训练自然涌现？为探索此问题，我们转向基因组序列这一富含统计结构的替代符号领域。具体而言，我们研究了主要基于下一核苷酸（A/T/C/G）预测训练的Evo2基因组模型，其训练规模与中型LLM相当。我们开发了受控实验框架，包含以语言形式和基因组形式实例化的符号推理任务，实现了基因组模型与语言模型在ICL能力上的直接比较。研究结果表明，与语言模型类似，基因组模型在模式归纳能力上随着上下文示例数量的增加呈现对数线性增长。据我们所知，这是首次在基因组序列中发现自然涌现的ICL能力，支持了“ICL是丰富数据大规模预测建模的产物”这一假说。这些发现将涌现元学习拓展至语言领域之外，为构建统一、模态无关的上下文学习理论指明了方向。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.12797">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.12797">arXiv</a></p>
<hr />
<h3>16. WebCoach：具备跨会话记忆引导的自演进网络智能体</h3>
<p><strong>原文标题：</strong> WebCoach: Self-Evolving Web Agents with Cross-Session Memory Guidance</p>
<p><strong>摘要：</strong>
多模态大语言模型驱动的智能体近期在网络浏览任务中展现出卓越能力，能够完成跨领域的复杂浏览任务。然而，现有智能体存在重复性错误问题，且缺乏跨会话经验学习能力，限制了其长期鲁棒性与样本效率。本文提出WebCoach——一个模型无关的自演进框架，通过赋予网络浏览智能体持续性的跨会话记忆，实现无需重新训练即可提升长期规划、反思与持续学习能力。该框架包含三个核心组件：（1）WebCondenser：将原始导航日志标准化为精简摘要；（2）外部记忆库：将完整操作轨迹组织为情景化经验；（3）教练模块：基于相似度与时效性检索相关经验，并通过运行时钩子决策是否向智能体注入任务特定建议。该设计使网络智能体能够突破原生上下文窗口限制访问长期记忆，显著提升复杂浏览任务的鲁棒性。此外，WebCoach通过持续整合新导航轨迹至情景记忆库实现自我演进，使智能体无需重训练即可持续优化。在WebVoyager基准测试中，WebCoach在三种不同大语言模型基座上均能稳定提升浏览器使用智能体的性能。使用38B参数模型时，任务成功率从47%提升至61%，同时减少或维持平均操作步数。值得注意的是，搭载WebCoach的较小基座模型可实现与使用GPT-4o的同等网络智能体相媲美的性能。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.12997">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.12997">arXiv</a></p>
<hr />
<h3>17. 评估大语言模型在知识图谱中的偶然性发现能力：以药物重定位为例</h3>
<p><strong>原文标题：</strong> Assessing LLMs for Serendipity Discovery in Knowledge Graphs: A Case for Drug Repurposing</p>
<p><strong>摘要：</strong>
大语言模型显著推动了知识图谱问答系统的发展，然而现有系统通常针对返回高度相关但可预测的答案进行优化。目前尚缺乏利用大语言模型提供具有惊喜感和新颖性（"偶然性"）答案的能力。本文正式定义了偶然性感知的知识图谱问答任务，并提出SerenQA框架来评估大语言模型在科学知识图谱问答任务中发现意外洞见的能力。该框架包含基于相关性、新颖性和惊喜度的严谨偶然性度量指标，以及源自临床知识图谱并聚焦药物重定位领域的专家标注基准。此外，该框架还设计了包含三个子任务的结构化评估流程：知识检索、子图推理和偶然性探索。实验结果表明，虽然最先进的大语言模型在检索任务上表现良好，但在识别真正具有惊喜度和价值的发现方面仍存在困难，这凸显了未来改进的重要空间。我们整理的资源及扩展版本发布于：https://cwru-db-group.github.io/serenQA。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.12472">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.12472">arXiv</a></p>
<hr />
<h3>18. 测试时频谱感知潜空间导向在视觉语言模型零样本泛化中的应用</h3>
<p><strong>原文标题：</strong> Test-Time Spectrum-Aware Latent Steering for Zero-Shot Generalization in Vision-Language Models</p>
<p><strong>摘要：</strong>
视觉语言模型在零样本推理方面表现卓越，但常因测试时域偏移而性能下降。为此，近期出现的临时测试时自适应策略已成为将视觉语言模型适配至单张未标注图像的有效技术。然而，现有适配策略（如测试时提示调优）通常需要对大型编码器权重进行反向传播或改变核心模型组件。本研究提出频谱感知测试时导向（STS）——一种轻量级适配框架，该框架从文本嵌入中提取频谱子空间以定义主要语义方向，并通过调整少量样本级偏移参数以最小化增强视图间的信息熵，从而实现频谱感知的潜表征导向。STS完全在推理阶段于潜空间内运行，无需对冻结编码器进行反向传播或结构修改。基于标准评估协议的综合实验表明，STS在多数情况下显著超越或优于最先进的测试时自适应方法，同时仅引入少量额外参数，推理速度提升最高达8倍，内存占用较传统测试时提示调优减少12倍。代码已发布于https://github.com/kdafnis/STS。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.09809">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.09809">arXiv</a></p>
<hr />
<h3>19. UnSAMv2：自监督学习实现任意粒度图像分割</h3>
<p><strong>原文标题：</strong> UnSAMv2: Self-Supervised Learning Enables Segment Anything at Any Granularity</p>
<p><strong>摘要：</strong>
Segment Anything Model（SAM）系列已成为广泛采用的视觉基础模型，但其控制分割粒度的能力仍存在局限。用户常需通过添加更多提示或从预生成掩码中筛选来手动优化结果，以获得理想细节层级。这一过程存在不确定性，因为相同提示可能对应多个合理掩码，且在所有粒度级别收集密集标注的成本过高，使得监督式解决方案难以实施。为解决此局限，我们提出UnSAMv2，无需人工标注即可实现任意粒度级别的图像分割。该模型扩展了UnSAM的分治策略，通过发掘丰富的掩码-粒度配对数据，并引入新型粒度控制嵌入机制，实现对分割尺度的精准连续控制。值得注意的是，仅使用6千张无标注图像和0.02%的附加参数，UnSAMv2就显著增强了SAM-2模型，在交互式分割、全图分割和视频分割任务中实现任意粒度分割。经超过11个基准测试验证，UnSAMv2将NoC_{90}指标从5.69提升至4.75，1-IoU从58.0提升至73.1，AR_{1000}从49.6提升至68.3，证明结合粒度感知自监督学习方法的小规模无标注数据能够释放视觉基础模型的潜力。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.13714">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.13714">arXiv</a></p>
<hr />
<h3>20. MicroVQA++：面向多模态大语言模型的高质量显微图像推理数据集与弱监督图结构</h3>
<p><strong>原文标题：</strong> MicroVQA++: High-Quality Microscopy Reasoning Dataset with Weakly Supervised Graphs for Multimodal Large Language Model</p>
<p><strong>摘要：</strong>
多模态大语言模型在生物医学成像中的应用日益广泛，然而显微图像的科学推理能力仍受限于大规模高质量训练数据的稀缺。我们提出MicroVQA++——一个源自BIOMEDICA档案的三阶段、大规模高质量显微视觉问答语料库。第一阶段通过经专家验证的论文图表-标题对实现监督引导；第二阶段应用HiCQA-Graph新型异质图结构，融合基于自然语言推理的文本蕴含分析、基于CLIP的视觉-语言对齐以及智能体信号，实现对图像、标题与问答三元组的跨模态一致性筛选；第三阶段采用多模态大语言模型智能体生成多项选择题，并经过人工筛查。最终发布的数据集包含大规模训练集和经人工校验的测试集，其布鲁姆分类学难度样本分布超越MicroVQA基准。本研究贡献包括：（1）通过文献专家知识与图结构过滤及人工精修相结合的质量控制数据集；（2）首个联合建模图像-标题-问答三元组以实现跨模态一致性过滤的HiCQA-Graph图结构；（3）实证表明精细的数据构建能使40亿参数级多模态大语言模型达到与GPT-5相当的显微图像推理性能，并在开源模型中实现最优效果。代码与数据集将在评审结束后公开。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.11407">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.11407">arXiv</a></p>
<hr />
<h3>21. 动态反思：通过文本对齐探究视频表征能力</h3>
<p><strong>原文标题：</strong> Dynamic Reflections: Probing Video Representations with Text Alignment</p>
<p><strong>摘要：</strong>
多模态表征对齐技术近年来为揭示不同编码器在跨数据类型中的结构相似性与下游任务能力提供了新的视角。尽管图像与文本对齐已取得显著进展，但视频数据的时间特性在此研究框架下仍待深入探索。本研究首次对视频-文本表征对齐展开系统性分析，深入探究现代视频与语言编码器的能力。我们的研究获得以下关键发现：首先，实验证明跨模态对齐效果高度依赖于测试时提供的视觉数据（静态图像与多帧视频）与文本数据（单一描述与集合描述）的丰富程度，这一特性在使用前沿视频编码器时尤为显著。我们提出参数化测试时扩展定律来刻画该现象，该定律在实证观察中展现出卓越的预测能力。其次，我们探究了语义对齐与语义/非语义下游任务性能间的关联性，初步证据表明与文本编码器的强对齐可能关联着通用视频表征与理解能力。最后，我们将时序推理能力与跨模态对齐建立关联，为视觉语言模型提供了具有挑战性的测试基准。总体而言，本研究提出视频-文本对齐作为一种零样本评估方法，为探究时空数据编码器的表征能力提供了有效途径。项目页面详见：https://video-prh.github.io/</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.02767">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.02767">arXiv</a></p>
<hr />
<h3>22. LoCoBench-Agent：长上下文软件工程中LLM智能体的交互式基准测试框架</h3>
<p><strong>原文标题：</strong> LoCoBench-Agent: An Interactive Benchmark for LLM Agents in Long-Context Software Engineering</p>
<p><strong>摘要：</strong>
随着大语言模型（LLM）逐渐发展成为能够执行复杂软件开发任务的自主智能体，评估其实际能力变得至关重要。现有基准测试（如LoCoBench~qiu2025locobench）虽能评估长上下文代码理解能力，但其采用单轮评估模式，无法捕捉现实编码智能体所需的多轮交互特性、工具使用模式和自适应推理能力。我们提出LoCoBench-Agent——一个专门用于评估LLM智能体在真实长上下文软件工程工作流中表现的综合性评估框架。该框架将LoCoBench的8,000个场景扩展为交互式智能体环境，支持系统化评估多轮对话、工具使用效率、错误恢复能力以及跨时段开发中的架构一致性。我们还提出了包含理解度与效率维度共9项指标的评估方法。本框架为智能体提供8种专业工具（文件操作、搜索、代码分析等），并在10K至1M标记的上下文长度范围内进行评估，实现对长上下文性能的精准度量。通过对前沿模型的系统化评估，我们获得三项关键发现：（1）智能体展现出卓越的长上下文鲁棒性；（2）存在理解度与效率的负相关权衡，即深入探索会提升理解度但降低效率；（3）不同模型的对话效率差异显著，战略性的工具使用模式是高性能智能体的关键区分特征。作为软件工程领域首个长上下文LLM智能体基准测试框架，LoCoBench-Agent为衡量智能体能力、识别性能差距及推进规模化自主软件开发奠定了严谨基础。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.13998">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.13998">arXiv</a></p>
<hr />
<h3>23. SafeGRPO：基于规则约束策略优化的自奖励多模态安全对齐方法</h3>
<p><strong>原文标题：</strong> SafeGRPO: Self-Rewarded Multimodal Safety Alignment via Rule-Governed Policy Optimization</p>
<p><strong>摘要：</strong>
多模态大语言模型虽展现出卓越的推理与指令跟随能力，但其扩展的模态空间引发了由复杂图文交互产生的新型组合式安全风险。这种跨模态耦合即使在单模态输入无害时也可能生成不安全语义，暴露出现有模型脆弱的安全认知。现有研究虽通过引导模型进行风险推理来增强安全性，但未受约束的推理轨迹可能破坏对齐效果；尽管群体相对策略优化（GRPO）提供了无需人工监督的自奖励优化机制，其缺乏可验证的推理安全保障。为此，我们提出SafeGRPO框架，将规则约束的奖励构建机制融入GRPO，实现可解释、可验证的推理安全优化。基于构建的包含显式视觉、文本及组合安全标签的SafeTag-VL-3K数据集，SafeGRPO通过步骤引导的安全思维机制强化结构化推理与行为对齐，在保持通用能力的同时，显著提升了多模态安全认知、组合鲁棒性及跨基准推理稳定性。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.12982">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.12982">arXiv</a></p>
<hr />
<h3>24. AI-Salesman：构建可信赖大语言模型驱动的电话营销系统</h3>
<p><strong>原文标题：</strong> AI-Salesman: Towards Reliable Large Language Model Driven Telemarketing</p>
<p><strong>摘要：</strong>
以电话营销为代表的目标驱动型说服性对话，需要复杂的多轮策略规划和严格的事实准确性，这对当前最先进的大语言模型仍构成重大挑战。既往研究常受限于领域特定数据的缺乏，而直接应用大语言模型则存在策略脆弱性和事实幻觉问题。本文首先构建并发布了业界首个基于真实场景的对话数据集TeleSalesCorpus。继而提出创新框架AI-Salesman，其采用双阶段架构：在训练阶段设计贝叶斯监督强化学习算法，从含噪声的对话数据中学习鲁棒的销售策略；在推理阶段引入动态大纲引导智能体（DOGA），通过预构建的话术库提供动态的逐轮策略指导。此外，我们设计了结合细粒度销售技能指标与“LLM即评判官”范式的综合评估体系。实验结果表明，所提出的AI-Salesman框架在自动评估指标和综合人工评估中均显著优于基线模型，展现了其在复杂说服场景中的卓越效能。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.12133">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.12133">arXiv</a></p>
<hr />
<h3>25. Instella：具备卓越性能的完全开放语言模型</h3>
<p><strong>原文标题：</strong> Instella: Fully Open Language Models with Stellar Performance</p>
<p><strong>摘要：</strong>
大语言模型已在广泛任务中展现出卓越性能，然而多数高性能模型仍保持闭源或部分开放，限制了研究的透明度与可复现性。本研究推出Instella系列——完全开放的三十亿参数语言模型，其训练全程基于公开可获取的数据与代码库。依托AMD Instinct MI300X GPU的算力支持，该模型通过大规模预训练、通用指令微调及人类偏好对齐三个阶段完成开发。尽管使用的预训练标记数量显著少于同期多数模型，Instella在完全开放模型中取得了领先性能，并与同类规模的顶尖开放权重模型表现相当。我们同时发布两个专业变体：支持128K标记上下文长度的Instella-Long，以及通过监督微调与数学任务强化学习增强的推理专用模型Instella-Math。这些成果共同确立了Instella作为透明、高效、多功能的开源替代方案，有力推进了开放可复现语言模型研究的发展目标。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.10628">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.10628">arXiv</a></p>
<hr />
<h3>26. 一种基于区块链保障来源可靠性的去中心化检索增强生成系统</h3>
<p><strong>原文标题：</strong> A Decentralized Retrieval Augmented Generation System with Source Reliabilities Secured on Blockchain</p>
<p><strong>摘要：</strong>
现有检索增强生成系统通常采用集中式架构，导致数据收集、集成和管理成本高昂，并引发隐私担忧。业界亟需一种去中心化的RAG系统，使基础模型能够直接利用数据所有者维护的信息源，同时确保数据所有者对其来源保持完全控制。然而去中心化架构带来新的挑战：众多独立数据源的可靠性差异显著，可能降低检索精度和响应质量。为此，我们提出的去中心化RAG系统创新性地设计了可靠性评分机制，该机制基于各数据源在生成响应过程中的贡献质量进行动态评估，并在检索阶段优先选择高质量数据源。为确保透明度和可信度，评分过程通过基于区块链的智能合约进行安全管理，无需依赖中心化机构即可创建可验证且防篡改的可靠性记录。我们使用两个Llama模型（3B和8B）在两种模拟环境中对系统进行评估，其中六个数据源具有不同等级的可靠性。在真实世界类不可靠数据环境中，本系统相较集中式系统实现了10.7%的性能提升。值得注意的是，在理想可靠数据环境下，本系统性能可逼近集中式系统的理论上限。去中心化基础设施实现了安全可信的评分管理，通过批量更新操作节省约56%的边际成本。我们的代码与系统已在github.com/yining610/Reliable-dRAG开源。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.07577">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.07577">arXiv</a></p>
<hr />
<h3>27. NORA-1.5：基于世界模型与动作偏好奖励训练的视觉-语言-动作模型</h3>
<p><strong>原文标题：</strong> NORA-1.5: A Vision-Language-Action Model Trained using World Model- and Action-based Preference Rewards</p>
<p><strong>摘要：</strong>
视觉-语言-动作模型近期在各类具身任务中展现出良好性能，但其可靠性与泛化能力仍存在不足，尤其是在跨具身系统或真实环境部署时。本研究提出NORA-1.5模型，该模型基于预训练的NORA主干网络，通过引入基于流匹配的动作专家模块实现架构升级。仅此架构改进即带来显著性能提升，使NORA-1.5在仿真与真实场景基准测试中均超越原始NORA模型及多种先进视觉-语言-动作模型。为增强模型鲁棒性与任务成功率，我们开发了一套用于后训练视觉-语言-动作策略的奖励模型。该奖励体系融合了（i）动作条件世界模型——评估生成动作是否导向预期目标，以及（ii）基于真值偏差的启发式准则——区分优质与劣质动作。利用这些奖励信号，我们构建偏好数据集并通过直接偏好优化方法使NORA-1.5适配目标具身系统。大量实验表明，基于奖励的后训练能持续提升模型在仿真与真实机器人环境中的表现，通过简洁高效的奖励模型实现了视觉-语言-动作模型可靠性的显著突破。我们的研究证实NORA-1.5模型与奖励引导的后训练机制共同构成了开发适用于真实场景的可靠具身智能体的有效路径。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.14659">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.14659">arXiv</a></p>
<hr />
<h3>28. OpenUS：基于自适应掩码对比学习的全开源超声图像分析基础模型</h3>
<p><strong>原文标题：</strong> OpenUS: A Fully Open-Source Foundation Model for Ultrasound Image Analysis via Self-Adaptive Masked Contrastive Learning</p>
<p><strong>摘要：</strong>
超声成像因其低成本、便携性、实时反馈和无电离辐射等优势，成为应用最广泛的医学影像模态之一。然而，超声图像解读仍高度依赖操作者经验，且受解剖区域、采集协议和设备类型的影响存在显著差异。这些变异因素，连同散斑噪声、低对比度和标准化标注稀缺等特有挑战，严重制约了泛化性强、标注效率高的超声AI模型的开发。本文提出OpenUS——首个基于大规模公共数据集构建的可复现开源超声基础模型。该模型采用视觉Mamba架构，能够同时捕捉图像中的局部特征与全局长程依赖关系。为在预训练阶段提取丰富特征，我们创新性地提出了结合对比学习与掩码图像建模的自适应掩码框架。该策略将教师模型的注意力图与学生模型的重建损失相融合，通过自适应优化临床相关掩码区域来提升预训练效能。OpenUS还采用动态学习调度机制，逐步调整预训练任务的难度层级。为构建基础模型，我们整合了迄今最大的公共超声数据集，涵盖42个公开数据源的逾30.8万张图像，涉及多解剖部位、多医疗机构、多成像设备及多疾病类型。预训练完成的OpenUS模型可作为高效标注微调任务的骨干网络，快速适配特定下游应用。代码已开源：https://github.com/XZheng0427/OpenUS。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.11510">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.11510">arXiv</a></p>
<hr />
<h2>🔍 关键词云图</h2>
<p><img alt="关键词云图" src="../images/keywords_wordcloud.png" /></p>
<h2>📈 近期论文趋势</h2>
<p><img alt="论文趋势" src="../images/daily_papers.png" /></p>
<h2>🎙️ 语音播报</h2>
<ul>
<li><a href="../audio/2025-11-18_daily_papers.mp3">收听今日论文解读</a></li>
</ul>
<h2>📱 订阅渠道</h2>
<ul>
<li>GitHub: <a href="https://github.com/2404589803/hf-daily-paper-newsletter-chinese">hf-daily-paper-newsletter-chinese</a></li>
</ul>
    </div>
</body>
</html>