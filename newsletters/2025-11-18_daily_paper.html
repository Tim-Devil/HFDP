<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Hugging Face 论文日报 - 2025-11-18</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', sans-serif;
            line-height: 1.6;
            color: #333;
            background-color: #f5f5f5;
            padding: 20px;
        }
        
        .container {
            max-width: 900px;
            margin: 0 auto;
            background-color: white;
            padding: 40px;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }
        
        h1 {
            color: #2c3e50;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
            margin-bottom: 30px;
            font-size: 28px;
        }
        
        h1 img {
            vertical-align: middle;
            margin-right: 10px;
        }
        
        h2 {
            color: #34495e;
            margin-top: 40px;
            margin-bottom: 20px;
            font-size: 24px;
            border-left: 4px solid #3498db;
            padding-left: 15px;
        }
        
        h3 {
            color: #2c3e50;
            margin-top: 30px;
            margin-bottom: 15px;
            font-size: 20px;
        }
        
        p {
            margin-bottom: 15px;
            text-align: justify;
        }
        
        ul {
            margin-left: 20px;
            margin-bottom: 20px;
        }
        
        li {
            margin-bottom: 8px;
        }
        
        a {
            color: #3498db;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        strong {
            color: #2c3e50;
            font-weight: 600;
        }
        
        hr {
            border: none;
            border-top: 1px solid #e0e0e0;
            margin: 30px 0;
        }
        
        /* 关键修复:限制图片宽度 */
        img {
            max-width: 100%;
            height: auto;
            display: block;
            margin: 20px auto;
            border-radius: 4px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        
        /* 确保图片容器也有宽度限制 */
        p img {
            max-width: 100%;
        }
        
        /* 论文详情区域样式 */
        .paper-section {
            background-color: #f9f9f9;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 20px;
        }
        
        /* 统计信息样式 */
        .stats {
            background-color: #e8f4f8;
            padding: 15px;
            border-radius: 6px;
            margin-bottom: 20px;
        }
        
        /* 响应式设计 */
        @media (max-width: 768px) {
            .container {
                padding: 20px;
            }
            
            h1 {
                font-size: 24px;
            }
            
            h2 {
                font-size: 20px;
            }
            
            h3 {
                font-size: 18px;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <h1><img src="https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo.png" width="30"/> Hugging Face 2025-11-18 论文日报</h1>
<h2>📊 今日论文统计</h2>
<ul>
<li>总论文数：28</li>
<li>热门领域：LLM, RL, GPT, Transformer</li>
</ul>
<h2>📝 论文详情</h2>
<h3>1. P1：基于强化学习的物理奥林匹克竞赛解题系统</h3>
<p><strong>原文标题：</strong> P1: Mastering Physics Olympiads with Reinforcement Learning</p>
<p><strong>摘要：</strong>
大型语言模型（LLM）的最新进展已将其能力边界从解决谜题拓展至科学级推理——这类推理要求答案必须经得起自然规律的检验，而非仅符合评分标准。物理学作为这一转变的终极试金石，以根本性的方式将符号与现实相联结，成为大多数现代技术的基石。本研究通过开发具备卓越物理推理能力的大型语言模型推动物理研究进展，尤其擅长解决奥林匹克竞赛级别的物理问题。我们提出P1系列模型——完全通过强化学习（RL）训练的开源物理推理模型家族。其中P1-235B-A22B成为首个在最新国际物理奥林匹克竞赛（IPhO 2025）中达到金牌水平的开源模型，并在2024/2025年度的13项国际/地区物理竞赛中斩获12枚金牌。P1-30B-A3B同样在IPhO 2025中超越几乎所有其他开源模型，获得银牌成绩。进一步结合智能体框架PhysicsMinions后，P1-235B-A22B+PhysicsMinions在IPhO 2025实现总排名第一，并在13项物理竞赛中取得最高平均分。除物理领域外，P1系列模型在数学、编程等推理任务中也展现出卓越性能，证明了该系列模型的强泛化能力。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.13612">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.13612">arXiv</a></p>
<hr />
<h3>2. MiroThinker：通过模型规模、上下文长度与交互深度的协同扩展突破开源研究智能体性能边界</h3>
<p><strong>原文标题：</strong> MiroThinker: Pushing the Performance Boundaries of Open-Source Research Agents via Model, Context, and Interactive Scaling</p>
<p><strong>摘要：</strong>
本文推出开源研究智能体MiroThinker v1.0，旨在推进工具增强推理与信息检索能力的发展。与先前仅扩大模型规模或上下文长度的智能体不同，MiroThinker开创性地在模型层面探索交互深度扩展，将其作为性能提升的第三维度，通过系统化训练使模型能够处理更深层次、更频繁的智能体-环境交互。相较于孤立运行且长推理链存在性能衰减风险的大语言模型测试时扩展，交互式扩展通过环境反馈与外部信息获取实现错误修正与路径优化。借助强化学习，该模型实现了高效的交互扩展：在256K上下文窗口支持下，单任务可执行高达600次工具调用，支撑持续多轮推理与复杂现实研究流程。在GAIA、HLE、BrowseComp及BrowseComp-ZH四个代表性基准测试中，72B参数版本分别取得81.9%、37.7%、47.1%和55.6%的准确率，超越既往开源智能体并逼近GPT-5-high等商业系统。我们的分析表明，MiroThinker始终受益于交互扩展：随着智能体-环境交互深度与频次的增加，研究性能呈现可预测的提升，证明交互深度具有与模型规模、上下文长度类似的扩展规律。这些发现确立了交互扩展作为构建下一代开源研究智能体的第三关键维度，与模型容量和上下文窗口形成有效互补。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.11793">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.11793">arXiv</a></p>
<hr />
<h3>3. Uni-MoE-2.0-Omni：基于先进混合专家系统、训练策略与数据技术的以语言为中心的全模态大模型扩展</h3>
<p><strong>原文标题：</strong> Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data</p>
<p><strong>摘要：</strong>
我们推出荔枝家族系列的全新开源全模态大模型Uni-MoE 2.0。该模型在以语言为核心的多模态理解、推理与生成领域显著推进了荔枝Uni-MoE系列的技术水平。基于Qwen2.5-7B稠密架构，我们通过三大核心创新实现模型从零构建：动态容量混合专家框架设计、结合迭代强化策略的渐进式训练方法，以及精心构建的多模态数据匹配技术。该模型具备全模态理解能力，并可实现图像、文本与语音的跨模态生成。在架构层面，新型混合专家框架通过共享专家、路由专家与空置专家的协同机制，在十种跨模态输入场景中实现计算效率与模型能力的平衡；同时我们提出的全模态三维旋转位置编码技术，有效保障自注意力层中的时空跨模态对齐。训练策略上，在完成跨模态预训练后，我们采用渐进式监督微调方案，通过激活模态特定专家配合均衡数据组合，并结合迭代式GSPO-DPO方法以增强推理能力并稳定强化学习训练过程。数据层面，基于约750亿token开源多模态数据训练的基础模型，通过嵌入专用语音与图像生成标记，实现了基于语言线索的条件化生成能力。在涵盖85个基准测试的全面评估中，本模型在76项测试中超过50项表现优于采用1.2万亿token训练的Qwen2.5-Omni模型，在领先全模态大模型中达到顶尖或极具竞争力的性能水平。核心优势体现在视频理解（8项测试平均提升7%）、全模态理解（4项测试平均提升7%）和视听推理（提升4%）领域，同时显著推进长语音处理（词错误率降低4.2%），并在底层图像处理与可控生成的5项指标中保持领先地位。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.12609">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.12609">arXiv</a></p>
<hr />
<h3>4. Souper-Model：简单算术如何实现前沿大语言模型性能突破</h3>
<p><strong>原文标题：</strong> Souper-Model: How Simple Arithmetic Unlocks State-of-the-Art LLM Performance</p>
<p><strong>摘要：</strong>
大语言模型（LLM）已在多个领域展现出卓越能力，但其训练过程仍需要消耗大量资源和时间，不仅依赖巨额算力支撑，还需精心设计训练流程。模型参数融合技术——即对同架构多个模型的权重参数进行平均处理——已成为一种具有潜力的训练前/后优化方法，可在避免昂贵重复训练的前提下提升模型性能。本文提出类别专家融合模型（SoCE），该方法通过基准测试构成分析确定最优模型候选集，并采用非均匀加权平均策略实现性能最大化。与传统均匀平均方法不同，我们基于模型在不同测试类别中表现相关性较低这一观测结果，为每个弱相关类别集群筛选“专家”模型，并通过优化加权而非均匀权重进行模型融合。实验证明，该方法在多语言能力、工具调用及数学推理等多个领域均能提升模型性能与鲁棒性，并在伯克利函数调用排行榜上取得了领先成果。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.13254">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.13254">arXiv</a></p>
<hr />
<h3>5. Part-X-MLLM：面向部件感知的三维多模态大语言模型</h3>
<p><strong>原文标题：</strong> Part-X-MLLM: Part-aware 3D Multimodal Large Language Model</p>
<p><strong>摘要：</strong>
本文提出Part-X-MLLM——一种原生三维多模态大语言模型，通过将多样化三维任务表述为结构化可执行语法中的程序来实现任务统一。给定RGB点云和自然语言提示，本模型能以自回归方式生成单一连贯的标记序列，该序列编码了部件级边界框、语义描述及编辑指令。这种结构化输出可作为驱动下游几何感知模块的通用接口，实现基于部件的生成与编辑。通过将符号规划与几何合成解耦，我们的方法使得任何兼容的几何引擎都能通过单一语言原生前端进行控制。我们预训练了双编码器架构以实现结构与语义解耦，并在大规模部件中心数据集上对模型进行指令微调。实验表明，本模型在生成高质量结构化规划方面表现卓越，通过统一接口在具身问答、组合生成及局部编辑任务中实现了最先进的性能。项目页面：https://chunshi.wang/Part-X-MLLM/</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.13647">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.13647">arXiv</a></p>
<hr />
<h3>6. MMaDA-并行：面向思维感知编辑与生成的多模态扩散大语言模型</h3>
<p><strong>原文标题：</strong> MMaDA-Parallel: Multimodal Large Diffusion Language Models for Thinking-Aware Editing and Generation</p>
<p><strong>摘要：</strong>
尽管思维感知生成技术旨在提升复杂任务的处理性能，但我们发现现有序列自回归方法存在关键失效模式——由于错误传播效应，反而可能导致性能退化。为系统分析该问题，我们提出ParaBench基准测试集，专门用于评估文本与图像双模态输出。基于ParaBench的分析表明，这种性能退化与生成推理过程和最终图像之间的对齐偏差高度相关。为此，我们提出并行多模态扩散框架MMaDA-Parallel，该框架能在整个去噪轨迹中实现文本与图像的持续双向交互。该模型首先通过监督微调进行训练，随后采用新型并行强化学习策略（ParaRL）进行优化，该策略通过沿轨迹施加语义奖励来增强跨模态一致性。实验验证表明，我们的模型显著改善了跨模态对齐与语义一致性，在ParaBench基准上相较最先进的Bagel模型实现了输出对齐指标6.9%的提升，为思维感知图像合成建立了更稳健的范式。代码已开源于：https://github.com/tyfeld/MMaDA-Parallel</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.09611">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.09611">arXiv</a></p>
<hr />
<h3>7. GroupRank：一种强化学习驱动的分组重排序范式</h3>
<p><strong>原文标题：</strong> GroupRank: A Groupwise Reranking Paradigm Driven by Reinforcement Learning</p>
<p><strong>摘要：</strong>
大语言模型作为重排序器已展现出增强RAG系统整体性能的强大潜力。然而，现有重排序范式受限于核心的理论与实践困境：点式排序方法虽然简单灵活，但独立评估文档的特性使其容易陷入"排序近视陷阱"，忽略文档间的相对重要性；而列式排序方法虽能感知全局排序上下文，却存在固有的"列表刚性缺陷"，在处理大规模候选集时面临严重的可扩展性与灵活性挑战。为此，我们提出分组排序这一新型重排序范式。该方法将查询与候选文档组共同输入模型，通过组内比较为每个文档分配独立相关性分数。该设计既保留了点式方法的灵活性，又实现了列式方法的对比能力。我们采用GRPO进行模型训练，并配备融合排序指标与分布奖励的异构奖励函数，以协调跨组分数分布。针对高质量标注数据稀缺的瓶颈，我们进一步提出合成高质量检索与排序数据的创新流程，所得数据不仅可用于训练重排序器，还可用于训练检索器。在BRIGHT和R2MED两个推理密集型检索基准上的大量实验验证了我们方法的有效性。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.11653">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.11653">arXiv</a></p>
<hr />
<h3>8. TiViBench：视频生成模型中的视觉推理能力基准测试框架</h3>
<p><strong>原文标题：</strong> TiViBench: Benchmarking Think-in-Video Reasoning for Video Generative Models</p>
<p><strong>摘要：</strong>
视频生成模型的快速发展已使其重点从产生视觉合理的输出转向处理需要物理合理性和逻辑一致性的任务。然而，尽管近期出现了如Veo 3的帧序列推理等突破性进展，这些模型是否能展现类似大语言模型的推理能力仍不明确。现有基准主要评估视觉保真度和时序连贯性，未能捕捉高阶推理能力。为填补这一空白，我们提出TiViBench——一个专门用于评估图像到视频生成模型推理能力的分层基准框架。该基准系统地从四个维度评估推理能力：i) 结构推理与检索，ii) 空间与视觉模式推理，iii) 符号与逻辑推理，iv) 行动规划与任务执行，涵盖3个难度级别下的24种任务场景。通过广泛评估，我们发现商业模型（如Sora 2、Veo 3.1）展现出更强的推理潜力，而开源模型虽存在未开发潜力，但仍受限于训练规模和数据多样性的不足。为释放这种潜力，我们提出VideoTPO——一种受偏好优化启发的简易有效测试时策略。通过对生成候选结果进行LLM自分析以识别优劣，VideoTPO在无需额外训练、数据或奖励模型的情况下显著提升推理性能。TiViBench与VideoTPO共同为评估和推进视频生成模型的推理能力开辟道路，为这一新兴领域的未来研究奠定基础。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.13704">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.13704">arXiv</a></p>
<hr />
<h3>9. PhysX-Anything：基于单图像的仿真就绪物理三维资产生成框架</h3>
<p><strong>原文标题：</strong> PhysX-Anything: Simulation-Ready Physical 3D Assets from Single Image</p>
<p><strong>摘要：</strong>
三维建模正从静态视觉表示转向可直接用于仿真与交互的物理化可动资产。然而现有三维生成方法大多忽略关键物理属性与可动结构，限制了其在具身智能中的应用。为此我们提出PhysX-Anything——首个仿真就绪的物理三维生成框架，仅需单张真实场景图像即可生成具有显式几何、可动结构与物理属性的高质量仿真就绪三维资产。具体而言，我们提出首个基于视觉语言模型的物理三维生成方法，并创新性地设计了一种高效几何表征的token化方案，将token数量降低193倍，在标准VLM token预算内实现显式几何学习，且无需微调阶段引入特殊token，显著提升生成质量。此外，为克服现有物理三维数据集多样性不足的问题，我们构建了PhysX-Mobility数据集，将现有物理三维数据集的物体类别扩展2倍以上，包含2000余个常见现实物体并配备丰富物理标注。在PhysX-Mobility与真实场景图像上的大量实验表明，PhysX-Anything具有优异的生成性能与鲁棒泛化能力。基于MuJoCo仿真环境的实验进一步验证，本方法生成的仿真就绪资产可直接用于接触密集型机器人策略学习。我们相信PhysX-Anything将有力推动具身智能与物理仿真等下游应用的发展。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.13648">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.13648">arXiv</a></p>
<hr />
<h3>10. UFO^3：编织智能体数字星系</h3>
<p><strong>原文标题：</strong> UFO^3: Weaving the Digital Agent Galaxy</p>
<p><strong>摘要：</strong>
基于大语言模型的智能体正将数字设备从被动工具转变为主动智能协作者。然而现有框架大多局限于单一操作系统或设备，导致跨设备工作流脆弱且高度依赖人工操作。我们提出UFO^3系统，该系统将异构终端、桌面端、服务器、移动设备与边缘计算节点统一整合为协同编排架构。UFO^3将用户请求建模为可演化的任务星云：即由具有显式控制流与数据依赖关系的原子化子任务构成分布式有向无环图。随着分布式设备持续返回执行结果，任务星云持续动态演化，支持异步执行、自适应恢复与动态优化。星云编排器在实施动态DAG更新的同时保障任务安全异步执行，而智能体交互协议则通过持久化低延迟通道实现可靠任务调度与结果流式传输。这些设计打破了设备与平台间的传统壁垒，使智能体能够无缝协作并增强集体智能。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.11332">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.11332">arXiv</a></p>
<hr />
<h3>11. 进化方法而非提示：大语言模型越狱攻击的演化合成</h3>
<p><strong>原文标题：</strong> Evolve the Method, Not the Prompts: Evolutionary Synthesis of Jailbreak Attacks on LLMs</p>
<p><strong>摘要：</strong>
针对大语言模型的自动化红队测试框架日益精密，但其仍存在根本性局限：越狱逻辑始终局限于选择、组合或优化既有攻击策略。这种约束限制了系统创造性，使其无法自主发明全新攻击机制。为突破此局限，我们提出EvoSynth框架，将范式从攻击规划转向越狱方法的演化合成。该框架采用多智能体系统自主设计、进化并执行基于代码的新型攻击算法，其核心特性在于具备代码级自我修正循环，能够根据失败反馈迭代重写攻击逻辑。通过大量实验验证，EvoSynth不仅在对Claude-Sonnet-4.5等高鲁棒性模型的测试中达到85.5%的攻击成功率，创下最新技术水准，且生成的攻击方式比现有方法具有显著更高的多样性。我们公开此框架以促进越狱方法演化合成这一新方向的研究。代码地址：https://github.com/dongdongunique/EvoSynth。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.12710">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.12710">arXiv</a></p>
<hr />
<h3>12. 回归本源：让去噪生成模型真正执行去噪任务</h3>
<p><strong>原文标题：</strong> Back to Basics: Let Denoising Generative Models Denoise</p>
<p><strong>摘要：</strong>
当今的去噪扩散模型并非经典意义上的"去噪"，即不直接预测洁净图像。相反，神经网络预测的是噪声或含噪量值。本文提出，预测洁净数据与预测含噪量值存在本质区别。根据流形假设，自然数据应位于低维流形上，而含噪量值则不然。基于此假设，我们主张采用直接预测洁净数据的模型，这使得表观容量不足的网络能在高维空间中有效运作。我们证明：无需标记器、预训练或额外损失的简单大尺寸图像块Transformer即可成为强大的生成模型。该方法在概念上仅是"纯图像Transformer"（简称JiT）。我们在ImageNet数据集256×256和512×512分辨率上使用16×16和32×32大尺寸图像块的JiT模型取得了具有竞争力的结果——而在相同场景下，预测高维含噪量值可能导致灾难性失败。通过使网络回归流形的基本原理，我们的研究不仅回归技术本源，更致力于构建基于原始自然数据的Transformer扩散模型的自包含范式。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.13720">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.13720">arXiv</a></p>
<hr />
<h3>13. OlmoEarth：面向多模态地球观测的稳定隐空间图像建模</h3>
<p><strong>原文标题：</strong> OlmoEarth: Stable Latent Image Modeling for Multimodal Earth Observation</p>
<p><strong>摘要：</strong>
地球观测数据具有独特的挑战性：既具备图像的空间特性，又具有视频或文本的时序特征，且呈现高度多模态特性。本文提出OlmoEarth——一个多模态时空基础模型，采用专为地球观测领域设计的新型自监督学习框架、掩码策略与损失函数。在各类研究基准测试和来自外部合作伙伴的实际任务中，OlmoEarth相较于其他12种基础模型实现了最先进的性能表现。在嵌入向量评估中，OlmoEarth在24项任务中的15项取得最佳性能；经过全参数微调后，其在29项任务中的19项表现最优。我们将OlmoEarth部署为端到端地球观测模型平台的核心架构，该平台涵盖数据采集、标注、训练和推理全流程。OlmoEarth平台将前沿基础模型与强大数据管理工具赋能给致力于解决全球重大问题的非营利组织与非政府机构。OlmoEarth源代码、训练数据与预训练权重已在https://github.com/allenai/olmoearth_pretrain 开源。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.13655">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.13655">arXiv</a></p>
<hr />
<h3>14. Live-SWE-agent：软件工程智能体能否实现实时自主演进？</h3>
<p><strong>原文标题：</strong> Live-SWE-agent: Can Software Engineering Agents Self-Evolve on the Fly?</p>
<p><strong>摘要：</strong>
大型语言模型正在重塑包括软件工程在内的几乎所有行业领域。近年来，已有诸多基于大语言模型的智能体被提出以解决现实世界中的软件问题。这类软件智能体通常配备一套编程工具集，能够自主决策后续操作以形成完整任务轨迹，从而解决端到端的软件任务。尽管前景可观，此类智能体往往需要专门设计且仍可能非最优解，因为穷尽整个智能体框架设计空间极具挑战且成本高昂。鉴于软件智能体本质上是可被进一步优化/修改的软件实体，研究者近期提出了若干自演进软件智能体，包括达尔文-哥德尔机。然而，这类自演进智能体需要在特定基准测试上进行昂贵的离线训练，且在不同大语言模型或基准测试间泛化能力有限。本文提出Live-SWE-agent——首个能在解决现实软件问题时于运行时自主持续实时演进的活体软件智能体。具体而言，Live-SWE-agent从仅配备基础bash工具的最简智能体框架起步，在解决实际软件问题的过程中自主演进其框架实现。在广泛研究的SWE-bench Verified基准测试中，我们的评估表明Live-SWE-agent无需测试时扩展即可达到75.4%的惊人解决率，超越所有现有开源软件智能体，逼近最佳专有解决方案的性能。此外，Live-SWE-agent在最新的SWE-Bench Pro基准测试中优于最先进的人工设计软件智能体，以45.8%的解决率创下当前最佳纪录。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.13646">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.13646">arXiv</a></p>
<hr />
<h3>15. 基因组下一标记预测器具备上下文学习能力</h3>
<p><strong>原文标题：</strong> Genomic Next-Token Predictors are In-Context Learners</p>
<p><strong>摘要：</strong>
上下文学习（ICL）——即模型从其输入提供的示例中推断并应用抽象模式的能力——已在基于人类文本进行下一标记预测训练的大语言模型中得到广泛研究。事实上，先前研究常将这种涌现行为归因于人类语言独特的统计特性。这引发了一个根本性问题：在其他序列领域中，ICL能否纯粹通过大规模预测训练自然涌现？为探索此问题，我们转向基因组序列这一富含统计结构的替代符号领域。具体而言，我们研究了主要在下一核苷酸（A/T/C/G）预测任务上训练的Evo2基因组模型，其训练规模与中型LLM相当。我们开发了一个受控实验框架，包含以语言形式和基因组形式实例化的符号推理任务，从而实现对基因组模型与语言模型上下文学习的直接比较。研究结果表明，基因组模型与语言模型类似，随着上下文示例数量的增加，在模式归纳能力上呈现对数线性增益。据我们所知，这是首次在基因组序列中发现自然涌现的上下文学习能力，支持了“ICL是丰富数据大规模预测建模的产物”这一假说。这些发现将涌现元学习拓展至语言领域之外，为构建统一、模态无关的上下文学习理论指明了方向。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.12797">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.12797">arXiv</a></p>
<hr />
<h3>16. WebCoach：具备跨会话记忆引导的自演进网络智能体</h3>
<p><strong>原文标题：</strong> WebCoach: Self-Evolving Web Agents with Cross-Session Memory Guidance</p>
<p><strong>摘要：</strong>
多模态大语言模型驱动的智能体近期在网络导航任务中展现出卓越能力，能够完成跨领域的复杂浏览任务。然而，现有智能体仍面临重复性错误问题，且缺乏跨会话经验学习能力，限制了其长期鲁棒性与样本效率。我们提出WebCoach——一个与模型无关的自演进框架，通过赋予网络浏览智能体持续性的跨会话记忆，实现无需重新训练即可提升长期规划、反思与持续学习能力。该框架包含三个核心组件：（1）WebCondenser：将原始导航日志标准化为精简摘要；（2）外部记忆库：将完整操作轨迹组织为情景化经验；（3）教练模块：基于相似度与时效性检索相关经验，并通过运行时钩子决策是否向智能体注入任务特定建议。该设计使网络智能体能够突破自身上下文窗口限制访问长期记忆，显著增强复杂浏览任务中的鲁棒性。此外，WebCoach通过持续整合新导航轨迹来优化情景记忆，实现无需重新训练的持续性能提升。在WebVoyager基准测试中，WebCoach在三种不同大语言模型基座上均能稳定提升浏览器使用智能体的性能。使用38B参数模型时，任务成功率从47%提升至61%，同时保持或减少了平均操作步数。值得注意的是，搭载WebCoach的较小基座模型可实现与使用GPT-4o的同等网络智能体相媲美的性能。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.12997">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.12997">arXiv</a></p>
<hr />
<h3>17. 评估大语言模型在知识图谱中的偶然性发现能力：以药物重定位为例</h3>
<p><strong>原文标题：</strong> Assessing LLMs for Serendipity Discovery in Knowledge Graphs: A Case for Drug Repurposing</p>
<p><strong>摘要：</strong>
大语言模型显著推动了知识图谱问答系统的发展，然而现有系统通常针对返回高度相关但可预测的答案进行优化。当前系统缺失但亟需的能力是利用大语言模型提供令人惊喜的新颖（"偶然性"）答案。本文正式定义了偶然性感知的知识图谱问答任务，并提出SerenQA框架来评估大语言模型在科学知识图谱问答中发现意外洞见的能力。该框架包含基于相关性、新颖性和惊喜度的严谨偶然性度量指标，以及源自临床知识图谱的专家标注基准数据集，重点关注药物重定位领域。此外，该框架设计了包含三个子任务的结构化评估流程：知识检索、子图推理和偶然性探索。实验结果表明，尽管最先进的大语言模型在检索任务上表现良好，但在识别真正具有惊喜价值的新发现方面仍存在困难，这凸显了未来改进的重要空间。我们整理的资源及扩展版本已发布于：https://cwru-db-group.github.io/serenQA。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.12472">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.12472">arXiv</a></p>
<hr />
<h3>18. 面向视觉语言模型零样本泛化的测试时频谱感知隐空间调控</h3>
<p><strong>原文标题：</strong> Test-Time Spectrum-Aware Latent Steering for Zero-Shot Generalization in Vision-Language Models</p>
<p><strong>摘要：</strong>
视觉语言模型在零样本推理任务中表现卓越，但常因测试时域偏移而性能衰退。为此，基于情景式测试时适应的策略近期成为将视觉语言模型适配至单张未标注图像的有效手段。然而现有适配方法（如测试时提示调优）通常需要对大型编码器权重进行反向传播，或需改动核心模型结构。本研究提出频谱感知测试时调控框架，该轻量化适配框架通过以下方式实现：从文本嵌入中提取频谱子空间以定义核心语义方向，采用频谱感知方式学习隐表征的调控策略，通过适配少量样本级偏移参数来最小化多增强视图下的信息熵。该框架完全在隐空间内进行推理，无需对冻结编码器执行反向传播或结构修改。基于标准评估协议的综合性实验表明，本方法在多数情况下显著超越或优于最先进的测试时适配方法，同时仅引入少量额外参数，推理速度较传统测试时提示调优提升最高达8倍，内存占用减少12倍。代码已开源：https://github.com/kdafnis/STS。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.09809">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.09809">arXiv</a></p>
<hr />
<h3>19. UnSAMv2：自监督学习实现任意粒度图像分割</h3>
<p><strong>原文标题：</strong> UnSAMv2: Self-Supervised Learning Enables Segment Anything at Any Granularity</p>
<p><strong>摘要：</strong>
Segment Anything Model（SAM）系列已成为广泛采用的视觉基础模型，但其分割粒度控制能力仍存在局限。用户常需通过添加更多提示或从预生成掩码中手动筛选来优化结果，以实现所需细节层级。这一过程存在模糊性——相同提示可能对应多个合理掩码，且全粒度密集标注成本极高，使监督式解决方案难以实施。为突破此限制，我们提出UnSAMv2，无需人工标注即可实现任意粒度图像分割。该模型延续UnSAM的分治策略，通过发掘海量掩码-粒度配对数据，引入新型粒度控制嵌入机制，实现对分割尺度的精准连续调控。值得注意的是，仅使用6千张无标注图像和0.02%的附加参数，UnSAMv2即显著增强SAM-2模型，在交互式分割、全图像分割及视频分割任务中实现全粒度分割能力。经11余项基准测试验证，UnSAMv2将NoC_{90}指标从5.69提升至4.75，1-IoU从58.0优化至73.1，AR_{1000}从49.6提升至68.3，证明结合粒度感知自监督方法的少量无标注数据即可释放视觉基础模型的潜在能力。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.13714">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.13714">arXiv</a></p>
<hr />
<h3>20. MicroVQA++：面向多模态大语言模型的高质量显微图像推理数据集与弱监督图结构</h3>
<p><strong>原文标题：</strong> MicroVQA++: High-Quality Microscopy Reasoning Dataset with Weakly Supervised Graphs for Multimodal Large Language Model</p>
<p><strong>摘要：</strong>
多模态大语言模型在生物医学成像领域的应用日益广泛，但显微图像的科学推理仍受限于大规模高质量训练数据的稀缺。我们提出MicroVQA++——一个基于BIOMEDICA档案构建的三阶段、大规模高质量显微视觉问答语料库。第一阶段通过经专家验证的同行评审期刊图注对实现监督信号的初始引导；第二阶段应用HiCQA-Graph新型异质图结构，该图融合基于自然语言推理的文本蕴含关系、基于CLIP的视觉语言对齐以及智能体信号，对图像、图注和问答三元组进行跨模态一致性筛选；第三阶段采用多模态大语言模型智能体生成多项选择题，并经过人工筛查验证。最终发布的数据集包含大规模训练集和经人工校验的测试集，其布鲁姆认知层级难度样本分布超越MicroVQA基准。本研究的贡献在于：（1）构建了融合专家文献知识、图结构过滤与人工精校的质量可控数据集；（2）首创联合建模（图像、图注、问答）三元组的HiCQA-Graph跨模态一致性过滤机制；（3）通过实验证明精细的数据构建能使40亿参数级多模态大语言模型达到与GPT-5相当的显微图像推理性能，并在开源模型中实现最先进水平。代码与数据集将在评审结束后公开发布。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.11407">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.11407">arXiv</a></p>
<hr />
<h3>21. 动态反思：通过文本对齐探究视频表征能力</h3>
<p><strong>原文标题：</strong> Dynamic Reflections: Probing Video Representations with Text Alignment</p>
<p><strong>摘要：</strong>
近年来研究表明，跨模态表征对齐能够揭示不同编码器在多种数据类型上的结构相似性与下游任务处理能力。尽管图像与文本对齐已取得显著进展，但视频数据的时间特性在此背景下仍待深入探索。本研究首次对视频-文本表征对齐展开系统性研究，深入探究现代视频与语言编码器的能力。我们的研究获得若干重要发现：首先，我们证明跨模态对齐高度依赖于测试时提供的视觉数据（静态图像与多帧视频）和文本数据（单一描述与集合描述）的丰富程度，这一特性在使用前沿视频编码器时尤为显著。我们提出参数化测试时缩放定律来刻画这一现象，该定律在实证观察中展现出卓越的预测能力。其次，我们探究了语义对齐与语义/非语义下游任务性能之间的关联，初步证据表明与文本编码器的强对齐可能关联着通用视频表征与理解能力。最后，我们将时序推理与跨模态对齐建立关联，为视觉语言模型提供了具有挑战性的测试平台。总体而言，本研究提出视频-文本对齐作为一种零样本评估方法，为探究时空数据编码器的表征能力提供了新的研究视角。项目页面详见：https://video-prh.github.io/</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.02767">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.02767">arXiv</a></p>
<hr />
<h3>22. LoCoBench-Agent：面向长上下文软件工程场景的大语言模型智能体交互式评测基准</h3>
<p><strong>原文标题：</strong> LoCoBench-Agent: An Interactive Benchmark for LLM Agents in Long-Context Software Engineering</p>
<p><strong>摘要：</strong>
随着大语言模型（LLM）逐步发展为能够执行复杂软件开发任务的自主智能体，对其实际能力的评估变得至关重要。现有基准（如LoCoBench~qiu2025locobench）虽能评估长上下文代码理解能力，但仅关注单轮测试，无法捕捉现实编码智能体所需的多轮交互特性、工具使用模式和自适应推理能力。我们提出LoCoBench-Agent——专为评估长上下文软件工程工作流中LLM智能体表现而构建的综合评测框架。该框架将LoCoBench的8,000个场景扩展为交互式智能体环境，支持系统化评估多轮对话、工具使用效率、错误恢复能力以及跨时段开发的架构一致性。我们同时提出包含理解度与效率维度共9项指标的评测方法，为智能体提供8种专业工具（文件操作、搜索、代码分析等），并在10K至1M令牌的上下文长度范围内进行评测，实现对长上下文性能的精准评估。通过对前沿模型的系统化评测，我们获得三项关键发现：（1）智能体展现出显著的长上下文鲁棒性；（2）理解度与效率存在负相关的权衡关系，深入探索会提升理解度但降低效率；（3）不同模型的对话效率差异显著，战略性的工具使用模式是高性能智能体的关键区分特征。作为首个面向软件工程的长上下文LLM智能体基准，LoCoBench-Agent为衡量智能体能力、识别性能差距及推进规模化自主软件开发建立了严谨的评估基础。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.13998">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.13998">arXiv</a></p>
<hr />
<h3>23. SafeGRPO：基于规则约束策略优化的自奖励多模态安全对齐方法</h3>
<p><strong>原文标题：</strong> SafeGRPO: Self-Rewarded Multimodal Safety Alignment via Rule-Governed Policy Optimization</p>
<p><strong>摘要：</strong>
多模态大语言模型虽展现出卓越的推理与指令跟随能力，但其扩展的模态空间引发了由复杂图文交互产生的新型组合式安全风险。这种跨模态耦合即使在输入内容各自无害时仍可能生成不安全语义，暴露出当前模型脆弱的安全认知。现有研究通过引导模型进行风险推理来增强安全性，但未受约束的推理轨迹可能破坏对齐效果；尽管组相对策略优化（GRPO）提供了无需人工监督的自奖励优化机制，但其缺乏可验证的推理安全信号。为此，我们提出SafeGRPO框架，将规则约束的奖励构建机制融入GRPO，实现可解释、可验证的推理安全优化。基于构建的包含显式视觉、文本及组合安全标签的SafeTag-VL-3K数据集，SafeGRPO通过步骤引导的安全思维机制强化结构化推理与行为对齐，在保持通用能力的同时，显著提升了多模态安全认知、组合鲁棒性及跨基准推理稳定性。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.12982">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.12982">arXiv</a></p>
<hr />
<h3>24. AI-Salesman：构建可信赖的大语言模型驱动电话营销系统</h3>
<p><strong>原文标题：</strong> AI-Salesman: Towards Reliable Large Language Model Driven Telemarketing</p>
<p><strong>摘要：</strong>
以电话营销为代表的目标驱动型说服性对话，需要复杂的多轮规划与严格的事实准确性，这对当前最先进的大语言模型仍构成重大挑战。既往研究常受限于领域特定数据的缺乏，而直接应用大语言模型则存在策略脆弱性和事实幻觉问题。本文首先构建并发布了TeleSalesCorpus——该领域首个基于真实场景的对话数据集。继而提出具有双阶段架构的创新框架AI-Salesman：在训练阶段，我们设计了贝叶斯监督强化学习算法，从含噪声的对话数据中学习稳健的销售策略；在推理阶段，我们引入动态大纲引导智能体（DOGA），通过预设脚本库提供动态的逐轮策略指导。此外，我们构建了结合细粒度销售技能指标与“LLM即评判官”范式的综合评估框架。实验结果表明，我们提出的AI-Salesman在自动评估指标和综合人工评估中均显著优于基线模型，展现了其在复杂说服场景中的卓越效能。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.12133">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.12133">arXiv</a></p>
<hr />
<h3>25. Instella：具备卓越性能的完全开放语言模型</h3>
<p><strong>原文标题：</strong> Instella: Fully Open Language Models with Stellar Performance</p>
<p><strong>摘要：</strong>
大语言模型已在广泛任务中展现出卓越性能，然而多数高性能模型仍保持闭源或部分开放，限制了研究的透明度与可复现性。本研究推出Instella系列——完全开放的三十亿参数语言模型，其训练全程基于公开可获取的数据与代码库。依托AMD Instinct MI300X GPU的算力支持，该模型通过大规模预训练、通用指令微调及人类偏好对齐三个阶段构建。尽管预训练词元数量显著少于同期多数模型，Instella在完全开放模型中取得了领先性能，并与同类规模的顶尖开放权重模型表现相当。我们同时发布两个专项优化版本：支持128K词元上下文长度的Instella-Long，以及通过监督微调与数学任务强化学习增强的推理专用模型Instella-Math。这些成果共同确立了Instella作为透明、高效、多功能的开源替代方案，有力推进了开放可复现语言模型研究的发展目标。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.10628">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.10628">arXiv</a></p>
<hr />
<h3>26. 基于区块链可信源可靠性的去中心化检索增强生成系统</h3>
<p><strong>原文标题：</strong> A Decentralized Retrieval Augmented Generation System with Source Reliabilities Secured on Blockchain</p>
<p><strong>摘要：</strong>
现有的检索增强生成（RAG）系统通常采用集中式架构，导致数据收集、整合与管理成本高昂，并引发隐私担忧。亟需一种去中心化RAG系统，使基础模型能够直接利用数据所有者完全掌控的源信息。然而，去中心化架构带来关键挑战：大量独立数据源的可靠性差异显著，可能降低检索精度与响应质量。为此，我们提出一种新型去中心化RAG系统，其创新性可靠性评分机制能根据各数据源对生成响应的质量贡献进行动态评估，并在检索过程中优先调用高质量源。为确保透明度与可信度，评分过程通过基于区块链的智能合约进行安全管理，建立可验证且防篡改的可靠性记录，无需依赖中心化机构。我们使用两个Llama模型（3B与8B参数）在两种模拟环境中进行系统评估，其中六个数据源具有不同可靠性等级。在类真实世界的不可靠数据环境中，本系统性能较集中式基准提升10.7%。值得注意的是，在理想可靠数据环境下，其性能可逼近集中式系统的理论上限。该去中心化基础设施通过批处理更新操作实现安全可信的评分管理，边际成本降低约56%。代码与系统已在github.com/yining610/Reliable-dRAG开源。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.07577">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.07577">arXiv</a></p>
<hr />
<h3>27. NORA-1.5：基于世界模型与动作偏好奖励训练的视觉-语言-动作模型</h3>
<p><strong>原文标题：</strong> NORA-1.5: A Vision-Language-Action Model Trained using World Model- and Action-based Preference Rewards</p>
<p><strong>摘要：</strong>
视觉-语言-动作模型近期在各类具身任务中展现出良好性能，但其可靠性与泛化能力仍存在不足，尤其在跨智能体部署或真实环境应用时更为明显。本研究提出NORA-1.5模型，该模型基于预训练的NORA主干网络，通过引入基于流匹配的动作专家模块实现架构升级。仅此结构改进即带来显著性能提升，使NORA-1.5在仿真与真实环境测试中均超越原始NORA模型及多项前沿VLA模型。为进一步增强鲁棒性与任务成功率，我们开发了一套用于后训练VLA策略的奖励模型。该奖励体系融合了：（i）动作条件世界模型——评估生成动作是否导向预期目标；（ii）真实动作偏差启发式规则——区分优质与劣质动作。利用这些奖励信号，我们构建偏好数据集并通过直接偏好优化方法使NORA-1.5适配目标智能体。大量实验表明，基于奖励的后训练能持续提升模型在仿真与真实机器人环境中的表现，通过简洁高效的奖励模型实现了VLA模型可靠性的显著进步。我们的研究成果印证了NORA-1.5模型与奖励引导后训练机制可作为开发适用于真实场景的可靠具身智能体的有效路径。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.14659">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.14659">arXiv</a></p>
<hr />
<h3>28. OpenUS：基于自适应掩码对比学习的全开源超声图像分析基础模型</h3>
<p><strong>原文标题：</strong> OpenUS: A Fully Open-Source Foundation Model for Ultrasound Image Analysis via Self-Adaptive Masked Contrastive Learning</p>
<p><strong>摘要：</strong>
超声成像因其低成本、便携性、实时反馈和无电离辐射等优势，已成为应用最广泛的医学影像模态之一。然而，超声图像解读仍高度依赖操作者经验，且受解剖区域、采集协议和设备类型的影响存在显著差异。这些变异因素，连同斑点噪声、低对比度和标准化标注稀缺等特有挑战，严重制约了泛化性强、标注高效的超声AI模型的开发。本文提出OpenUS——首个基于大规模公共数据构建的可复现开源超声基础模型。该模型采用视觉Mamba主干网络，能够同时捕捉图像中的局部特征与全局长程依赖关系。为在预训练阶段提取丰富特征，我们创新性地提出了结合对比学习与掩码图像建模的自适应掩码框架。该策略将教师网络的注意力图与学生网络的重构损失相融合，通过自适应优化临床相关区域的掩码机制来提升预训练效能。OpenUS还采用动态学习调度策略，逐步调整预训练任务的难度级别。为构建基础模型，我们整合了迄今最大的公共超声数据集，涵盖42个公开数据源的逾30.8万张图像，涉及多解剖部位、医疗机构、成像设备及疾病类型。预训练完成的OpenUS模型可作为骨干网络，通过标注高效的微调快速适配特定下游任务。代码已开源：https://github.com/XZheng0427/OpenUS。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.11510">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.11510">arXiv</a></p>
<hr />
<h2>🔍 关键词云图</h2>
<p><img alt="关键词云图" src="../images/keywords_wordcloud.png" /></p>
<h2>📈 近期论文趋势</h2>
<p><img alt="论文趋势" src="../images/daily_papers.png" /></p>
<h2>🎙️ 语音播报</h2>
<ul>
<li><a href="../audio/2025-11-18_daily_papers.mp3">收听今日论文解读</a></li>
</ul>
<h2>📱 订阅渠道</h2>
<ul>
<li>GitHub: <a href="https://github.com/2404589803/hf-daily-paper-newsletter-chinese">hf-daily-paper-newsletter-chinese</a></li>
</ul>
    </div>
</body>
</html>