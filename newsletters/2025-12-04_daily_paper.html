<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Hugging Face 论文日报 - 2025-12-04</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', sans-serif;
            line-height: 1.6;
            color: #333;
            background-color: #f5f5f5;
            padding: 20px;
        }
        
        .container {
            max-width: 900px;
            margin: 0 auto;
            background-color: white;
            padding: 40px;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }
        
        h1 {
            color: #2c3e50;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
            margin-bottom: 30px;
            font-size: 28px;
        }
        
        h1 img {
            vertical-align: middle;
            margin-right: 10px;
        }
        
        h2 {
            color: #34495e;
            margin-top: 40px;
            margin-bottom: 20px;
            font-size: 24px;
            border-left: 4px solid #3498db;
            padding-left: 15px;
        }
        
        h3 {
            color: #2c3e50;
            margin-top: 30px;
            margin-bottom: 15px;
            font-size: 20px;
        }
        
        p {
            margin-bottom: 15px;
            text-align: justify;
        }
        
        ul {
            margin-left: 20px;
            margin-bottom: 20px;
        }
        
        li {
            margin-bottom: 8px;
        }
        
        a {
            color: #3498db;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        strong {
            color: #2c3e50;
            font-weight: 600;
        }
        
        hr {
            border: none;
            border-top: 1px solid #e0e0e0;
            margin: 30px 0;
        }
        
        /* 关键修复:限制图片宽度 */
        img {
            max-width: 100%;
            height: auto;
            display: block;
            margin: 20px auto;
            border-radius: 4px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        
        /* 确保图片容器也有宽度限制 */
        p img {
            max-width: 100%;
        }
        
        /* 论文详情区域样式 */
        .paper-section {
            background-color: #f9f9f9;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 20px;
        }
        
        /* 统计信息样式 */
        .stats {
            background-color: #e8f4f8;
            padding: 15px;
            border-radius: 6px;
            margin-bottom: 20px;
        }
        
        /* 响应式设计 */
        @media (max-width: 768px) {
            .container {
                padding: 20px;
            }
            
            h1 {
                font-size: 24px;
            }
            
            h2 {
                font-size: 20px;
            }
            
            h3 {
                font-size: 18px;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <h1><img src="https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo.png" width="30"/> Hugging Face 2025-12-04 论文日报</h1>
<h2>📊 今日论文统计</h2>
<ul>
<li>总论文数：26</li>
<li>热门领域：Vision, Transformer, RL, LLM, GPT</li>
</ul>
<h2>📝 论文详情</h2>
<h3>1. Qwen3-VL技术报告</h3>
<p><strong>原文标题：</strong> Qwen3-VL Technical Report</p>
<p><strong>摘要：</strong>
本文介绍Qwen系列迄今性能最强的视觉语言模型Qwen3-VL，该模型在广泛的多模态基准测试中均取得卓越表现。其原生支持高达256K令牌的交错上下文，能够无缝整合文本、图像与视频输入。该模型系列包含稠密架构（2B/4B/8B/32B）与混合专家架构（30B-A3B/235B-A22B）变体，以适应不同的延迟-质量权衡需求。Qwen3-VL具备三大核心优势：（一）显著增强的纯文本理解能力，在多项测试中超越同规模纯文本骨干模型；（二）强大的长上下文理解能力，原生支持文本与交错多模态输入的256K令牌窗口，能够对长文档和视频实现精准的信息保持、检索与交叉引用；（三）先进的跨模态推理能力，在单图、多图及视频任务中表现优异，在MMMU综合性评测及视觉数学基准（如MathVista、MathVision）中达到领先水平。在架构层面，我们引入三项关键升级：（一）增强型交错多分辨率旋转位置编码，强化图像与视频的时空建模能力；（二）深度堆叠集成机制，通过有效利用多层级视觉Transformer特征以增强视觉-语言对齐；（三）基于文本的视频时间对齐机制，从时序旋转位置编码演进为显式文本时间戳对齐，实现更精确的时间定位。在可比令牌预算与延迟约束下，Qwen3-VL在稠密架构与混合专家架构中均展现出优越性能。我们展望Qwen3-VL能够作为现实工作流程中图像推理、智能体决策与多模态代码智能的基础引擎。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.21631">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.21631">arXiv</a></p>
<hr />
<h3>2. 引导视觉-语言-动作模型作为反探索：一种测试时缩放方法</h3>
<p><strong>原文标题：</strong> Steering Vision-Language-Action Models as Anti-Exploration: A Test-Time Scaling Approach</p>
<p><strong>摘要：</strong>
通过流匹配或扩散目标训练的视觉-语言-动作模型擅长从大规模多模态数据集中学习复杂行为。然而，由于此类模型在预训练阶段整合了多样化的数据模式，而微调数据集通常包含以运动学上次优或不可取方式收集的演示数据，因此存在与下游任务成功动作模式无关的冗余动作模式。具体而言，我们观察到预训练模型经过监督微调后，在不同采样噪声中存在关键的推理时脆弱性。本文将此不稳定性归因于模型策略与下游任务数据集中稳定成功模式所诱导策略之间的分布偏移。为此，我们提出TACO——一种基于测试时缩放的轻量化框架，采用伪计数估计器作为动作片段的高保真验证器。集成TACO的模型能够从所有采样动作片段中执行具有最大伪计数的动作，从而在保持模型泛化能力的同时防止分布偏移。该方法类似于离线强化学习中的经典反探索原理，且无需梯度计算，相比强化学习更新具有显著计算优势，尤其适用于因去噪过程而难以进行强化学习更新的流模型或扩散模型。在四个仿真基准与双臂机器人平台上的大量实验表明，该方法能显著提升下游任务适应过程中的推理稳定性与成功率。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.02834">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.02834">arXiv</a></p>
<hr />
<h3>3. PretrainZero：强化主动预训练</h3>
<p><strong>原文标题：</strong> PretrainZero: Reinforcement Active Pretraining</p>
<p><strong>摘要：</strong>
模仿人类行为以主动从通用经验中学习并实现通用人工智能，一直是人类的梦想。近期基于强化学习的大规模思维模型在特定领域（如软件和数学）展现出令人印象深刻的专家级能力，但仍严重依赖特定领域内可验证的奖励信号，这极大限制了通用推理能力边界的拓展。本研究提出PretrainZero，一个基于预训练语料库的强化主动学习框架，旨在将强化学习从领域特定的后训练阶段扩展至通用预训练阶段。PretrainZero具备以下特征：1）主动预训练：受人类主动学习能力启发，PretrainZero学习统一的推理策略，以主动从预训练语料库中识别合理且信息丰富的内容，并通过强化学习进行推理预测；2）自监督学习：无需任何可验证标签、预训练奖励模型或监督微调，我们直接在通用维基百科语料库上使用强化学习对3B至30B的基础模型进行预训练，显著突破了通用推理的可验证数据壁垒；3）验证扩展：通过处理难度递增的掩码片段，PretrainZero大幅提升了预训练基础模型的通用推理能力。在强化预训练中，PretrainZero将Qwen3-4B-Base模型在MMLU-Pro、SuperGPQA和数学综合基准上的性能分别提升了8.43、5.96和10.60分。在后训练阶段，预训练模型还可作为下游RLVR任务的推理基础模型。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.03442">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.03442">arXiv</a></p>
<hr />
<h3>4. ViDiC：视频差异描述</h3>
<p><strong>原文标题：</strong> ViDiC: Video Difference Captioning</p>
<p><strong>摘要：</strong>
理解动态场景之间的视觉差异需要对组合、空间和时间变化进行比较性感知——这一能力在现有的视觉-语言系统中仍未得到充分探索。尽管先前关于图像差异描述（IDC）的研究使模型能够描述静态图像之间的语义变化，但这些方法未能捕捉随时间变化的运动连续性、事件演化或编辑一致性。我们提出了ViDiC（视频差异描述）任务及其对应的ViDiC-1K数据集，旨在评估多模态大语言模型（MLLMs）对视频对之间相似性和差异提供细粒度描述的能力。ViDiC-1K包含1,000个精选视频对，标注了超过4,000项比较清单条目，涵盖七个类别：主体、风格、背景、摄影技术、运动、场景和播放技术。为确保可靠评估，我们基于“LLM即评判者”协议提出了一个双重清单框架，分别测量相似性和差异的准确性。对十九个代表性多模态模型的实验揭示了它们在比较描述和差异感知能力方面存在显著性能差距。我们希望ViDiC-1K能成为一个具有挑战性的基准，为推进多模态智能中的视频理解、编辑感知和比较推理奠定坚实基础。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.03405">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.03405">arXiv</a></p>
<hr />
<h3>5. SpaceTools：基于双重交互式强化学习的工具增强空间推理</h3>
<p><strong>原文标题：</strong> SpaceTools: Tool-Augmented Spatial Reasoning via Double Interactive RL</p>
<p><strong>摘要：</strong>
视觉语言模型在定性视觉理解方面展现出强大能力，但在具身应用所需的精确度量空间推理方面仍存在困难。智能体范式表明，视觉语言模型可通过调用多种工具（如深度估计器、分割模型和姿态估计器）来增强这些能力。然而，如何在不依赖人工提示策略或固定预定义工具流程（这些限制会阻碍模型发现最优工具使用模式）的前提下实现该愿景，仍是亟待解决的挑战。强化学习虽有望弥合这一差距，但由于多工具推理的搜索空间庞大，目前仍局限于单一视觉工具的应用。本文提出双重交互式强化学习框架，该训练框架通过交互探索与反馈机制，使视觉语言模型学会协调多种工具。在教学阶段，我们将通过交互式强化学习训练的单工具专家演示与使用全工具的前沿模型轨迹相结合；在探索阶段，模型通过持续强化学习进一步优化多工具协调能力。我们开发的SpaceTools模型具备工具增强的空间推理能力，在空间理解基准测试（RoboSpatial-Home、BLINK、BOP-ASK）中达到最先进性能，并成功将七自由度机器人作为工具实现可靠的现实世界操控。双重交互式强化学习方法相比标准监督微调基线（在RoboSpatial上提升12%）和强化学习基线（在RoboSpatial上提升16%）均有显著改进。项目页面：https://spacetools.github.io/。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.04069">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.04069">arXiv</a></p>
<hr />
<h3>6. OneThinker：面向图像与视频的一体化推理模型</h3>
<p><strong>原文标题：</strong> OneThinker: All-in-one Reasoning Model for Image and Video</p>
<p><strong>摘要：</strong>
强化学习近期在多模态大语言模型中激发视觉推理能力方面取得了显著成功。然而，现有方法通常针对不同任务训练独立模型，并将图像与视频推理视为分离的领域。这导致模型向多模态通用推理系统扩展的能力受限，既限制了实际应用的灵活性，也阻碍了跨任务与跨模态的知识共享。为此，我们提出OneThinker——一个一体化推理模型，能够统一处理涵盖问答、描述、时空定位、跟踪与分割等多种基础视觉任务的图像与视频理解。为实现这一目标，我们构建了覆盖全部上述任务的OneThinker-600k训练数据集，并利用商业模型进行思维链标注，最终得到用于监督微调冷启动的OneThinker-SFT-340k数据集。此外，我们提出EMA-GRPO方法，通过追踪各任务奖励标准差的移动平均值来处理多任务强化学习中的奖励异质性问题，从而实现均衡优化。在多样化视觉基准上的大量实验表明，OneThinker在10类基础视觉理解任务、31个基准测试中均展现出强劲性能。此外，该模型在特定任务间表现出有效的知识迁移能力，并具备初步的零样本泛化性能，标志着向统一多模态通用推理系统迈出了重要一步。所有代码、模型与数据均已公开。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.03043">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.03043">arXiv</a></p>
<hr />
<h3>7. 文本到视觉生成中推理时扩展的提示设计再思考</h3>
<p><strong>原文标题：</strong> Rethinking Prompt Design for Inference-time Scaling in Text-to-Visual Generation</p>
<p><strong>摘要：</strong>
在文本到视觉生成中，实现用户意图与生成视觉内容之间的精确对齐仍是一个核心挑战，因为单次生成往往无法产生理想输出。为应对这一问题，现有方法主要侧重于扩展视觉生成过程（例如增加采样步数或种子数量），但这很快会导致质量提升陷入瓶颈。该局限性的根源在于指导生成的关键要素——提示词——在过程中保持固定。为此，我们提出推理时扩展的提示词重设计框架PRIS，该框架能够在推理过程中根据扩展生成的视觉内容自适应地修订提示词。PRIS的核心思想是：审阅已生成的视觉内容，识别其中反复出现的错误模式，据此重新设计提示词，再使用修订后的提示词重新生成视觉内容。为提供精准的提示词修订对齐反馈，我们引入了一种新型验证机制——元素级事实校正，该机制在细粒度层面评估提示词属性与生成视觉内容之间的对齐关系，相比整体性评估方法能实现更精准且可解释的判断。在文本到图像和文本到视频基准测试上的大量实验证明了我们方法的有效性，其中在VBench 2.0基准上实现了15%的性能提升。这些结果表明，在推理时联合扩展提示词与视觉生成是充分释放扩展定律潜力的关键。可视化结果可通过网站查看：https://subin-kim-cv.github.io/PRIS。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.03534">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.03534">arXiv</a></p>
<hr />
<h3>8. RELIC：具备长时记忆的交互式视频世界模型</h3>
<p><strong>原文标题：</strong> RELIC: Interactive Video World Model with Long-Horizon Memory</p>
<p><strong>摘要：</strong>
一个真正交互式的世界模型需要三个关键要素：实时长时流式生成、一致的空间记忆以及精确的用户控制。然而，现有方法大多仅孤立地解决其中某一个方面，因为同时实现三者极具挑战性——例如，长时记忆机制往往会损害实时性能。本研究提出RELIC，一个统一应对这三项挑战的框架。给定单张图像和文本描述，RELIC能够实时对任意场景进行具备记忆感知的长时探索。基于近期自回归视频扩散蒸馏技术，本模型采用经过高度压缩的历史潜在标记来表示长时记忆，这些标记通过KV缓存中兼具相对动作与绝对相机位姿的编码实现。这种紧凑的相机感知记忆结构支持隐式的三维一致内容检索，并以最小计算开销保障长时连贯性。同时，我们微调了一个双向教师视频模型，使其能够生成超出原始5秒训练时长的序列，并通过一种新型内存高效的自强制范式将其转化为因果性学生生成器，该范式支持对长时教师序列及学生自生成序列进行全上下文蒸馏。RELIC实现为140亿参数模型，并在精心构建的Unreal Engine渲染数据集上训练，能够以16 FPS的速度实时生成视频。与先前工作相比，本模型展现出更精准的动作跟随、更稳定的长时流式生成以及更鲁棒的空间记忆检索能力。这些特性使RELIC成为新一代交互式世界建模的坚实基础。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.04040">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.04040">arXiv</a></p>
<hr />
<h3>9. 基于编程视觉的思考：迈向图像思维的统一视角</h3>
<p><strong>原文标题：</strong> Thinking with Programming Vision: Towards a Unified View for Thinking with Images</p>
<p><strong>摘要：</strong>
基于图像进行思考的多模态大语言模型（MLLMs）能够交互式地使用工具对视觉输入进行推理，但现有方法通常依赖于工具集较为有限，其现实必要性与可扩展性不足。本研究首先揭示了一个关键且此前被忽视的缺陷：即使是最先进的MLLMs也表现出惊人的脆弱性，在图像发生简单方向变化或自然退化时性能显著下降，这凸显了基于工具的推理需要更强的鲁棒性。为解决这一问题，我们提出了CodeVision——一个灵活、可扩展的“代码即工具”框架。该框架以生成代码作为通用接口来调用任意图像操作，从而超越固定的工具注册机制。我们采用两阶段方法训练模型：首先在精心构建的高质量数据集上进行监督微调（SFT），该数据集专注于复杂多轮工具组合与错误恢复；随后进行强化学习（RL），并设计了一种新颖且密集的过程奖励函数，以鼓励策略性、高效的工具使用。为支持相关研究，我们构建了全新的SFT与RL数据集，并引入一套具有挑战性的新基准测试集，旨在严格评估模型对方向变化的鲁棒性及多工具推理能力。在Qwen2.5-VL和Qwen3-VL系列模型上的实验表明，我们的方法显著提升了模型性能，并催生了灵活工具组合、高效链式执行、基于运行时反馈的鲁棒错误恢复等新兴能力。代码已开源：https://github.com/ByteDance-BandAI/CodeVision。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.03746">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.03746">arXiv</a></p>
<hr />
<h3>10. 逆向流动：通过反向表示对齐改进标准化流模型</h3>
<p><strong>原文标题：</strong> Flowing Backwards: Improving Normalizing Flows via Reverse Representation Alignment</p>
<p><strong>摘要：</strong>
标准化流是一类以数学可逆架构为特征的生成模型，其前向过程将数据转换至隐空间以进行密度估计，而反向过程则从该空间生成新样本。这一特性在表示学习与数据生成之间形成了内在的协同机制。然而，传统标准化流的生成质量受限于基于对数似然优化所获得的语义表示能力不足。为解决此问题，本文提出一种创新的对齐策略，该策略创造性地利用标准化流的可逆特性：不同于常规的前向过程正则化方法，我们将生成（反向）过程的中间特征与强大视觉基础模型的表示进行对齐，实验证明该方法相较于简单对齐策略具有显著优势。同时，我们提出一种无需训练、基于测试时优化的新型分类算法，为评估标准化流中嵌入的语义知识提供了更本质的衡量方式。综合实验表明，所提方法将标准化流的训练速度提升3.3倍以上，同时在生成质量与分类精度方面均取得显著提升。我们在ImageNet 64×64和256×256数据集上取得了当前最优的标准化流模型性能。代码已开源：https://github.com/MCG-NJU/FlowBack。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.22345">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.22345">arXiv</a></p>
<hr />
<h3>11. Jina-VLM：小型多语言视觉语言模型</h3>
<p><strong>原文标题：</strong> Jina-VLM: Small Multilingual Vision Language Model</p>
<p><strong>摘要：</strong>
本文提出Jina-VLM，这是一个拥有24亿参数的视觉语言模型，在公开的20亿参数规模视觉语言模型中实现了最先进的多语言视觉问答性能。该模型通过注意力池化连接器将SigLIP2视觉编码器与Qwen3语言主干网络耦合，能够以高效令牌处理方式解析任意分辨率的图像。在标准视觉问答基准测试和多语言评估中，Jina-VLM在保持竞争力的纯文本性能的同时，表现优于同类可比模型。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.04032">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.04032">arXiv</a></p>
<hr />
<h3>12. CookAnything：一种灵活且一致的多步骤食谱图像生成框架</h3>
<p><strong>原文标题：</strong> CookAnything: A Framework for Flexible and Consistent Multi-Step Recipe Image Generation</p>
<p><strong>摘要：</strong>
烹饪是一种具有时序性和视觉依赖性的活动，其中如切菜、搅拌或煎炸等每个步骤都同时包含程序逻辑与视觉语义。尽管近期扩散模型在文本到图像生成方面展现出强大能力，但其在处理如食谱图解这类结构化多步骤场景时仍面临挑战。此外，现有食谱图解方法无法适应食谱步骤数量的自然变化，无论实际指令结构如何均生成固定数量的图像。为应对这些局限性，本文提出CookAnything——一种基于扩散模型的灵活且一致的框架，能够根据任意长度的文本烹饪指令生成连贯且语义分明的图像序列。该框架包含三个核心组件：（1）步骤区域控制，可在单一去噪过程中将文本步骤与对应图像区域对齐；（2）灵活旋转位置编码，这是一种感知步骤顺序的位置编码机制，能同时增强时序连贯性与空间多样性；（3）跨步骤一致性控制，用于保持不同步骤间食材细节的一致性。在食谱图解基准测试上的实验结果表明，CookAnything在基于训练和无训练设置下均优于现有方法。所提框架支持对复杂多步骤指令进行可扩展的高质量视觉合成，在指导性媒体和流程化内容创作领域具有广泛的应用潜力。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.03540">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.03540">arXiv</a></p>
<hr />
<h3>13. AutoNeural：面向NPU推理的视觉-语言模型协同设计</h3>
<p><strong>原文标题：</strong> AutoNeural: Co-Designing Vision-Language Models for NPU Inference</p>
<p><strong>摘要：</strong>
尽管神经处理单元（NPU）在边缘人工智能领域具备较高的理论效率，但专为GPU优化的先进视觉-语言模型（VLM）在这些硬件平台上往往表现不佳。我们将这种硬件与模型之间的不匹配归因于两个主要因素：视觉变换器（ViT）的量化脆弱性，以及自回归注意力机制受输入/输出限制的特性，这些特性无法充分利用NPU的高算术吞吐量。为弥合这一差距，我们提出了AutoNeural——一种专为纯整数推理协同设计的原生NPU视觉-语言模型架构。我们采用基于深度可分离卷积的MobileNetV5风格主干网络替代标准ViT编码器，该设计通过约束激活值分布实现了稳定的INT4/8/16量化。与此相配合，我们的语言主干网络将状态空间模型（SSM）原理与Transformer层相结合，采用高效门控卷积实现线性时间复杂度。这种混合设计消除了生成过程中键值缓存带来的沉重内存I/O开销。相较于传统基线方法，我们的方案显著提升了效率：视觉编码器的量化误差降低高达7倍，端到端延迟减少14倍。AutoNeural的解码速度达到基线的3倍，上下文窗口长度扩展至基线的4倍。我们通过基于高通SA8295P系统级芯片的实际汽车案例研究验证了这些改进，展示了其在座舱应用场景中实现实时性能的能力。研究结果强调，针对NPU硬件约束重新设计模型拓扑结构是实现鲁棒多模态边缘智能的必要前提。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.02924">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.02924">arXiv</a></p>
<hr />
<h3>14. SR-GRPO：将稳定秩作为大语言模型对齐的内在几何奖励信号</h3>
<p><strong>原文标题：</strong> SR-GRPO: Stable Rank as an Intrinsic Geometric Reward for Large Language Model Alignment</p>
<p><strong>摘要：</strong>
将大语言模型（LLM）与人类偏好对齐通常依赖于外部监督，但这种方法存在关键局限：人工标注稀缺且主观，奖励模型易受奖励攻击影响，而自评估方法则受提示敏感性和偏差的困扰。本研究提出稳定秩——一种从模型内部表示中提取的、无需标注的内在质量信号。稳定秩通过计算总方差与主方向方差的比值，衡量隐藏状态的有效维度，从而通过信息在表示维度间的分布方式来捕捉生成质量。实验表明，稳定秩在RewardBench上达到84.04%的准确率，并通过Best-of-N采样策略将任务准确率较贪婪解码平均提升11.3个百分点。基于这一发现，我们提出稳定秩分组相对策略优化（SR-GRPO），将稳定秩作为强化学习的奖励信号。在无外部监督的情况下，SR-GRPO将Qwen2.5-1.5B-Instruct模型在STEM任务上的性能提升10%，在数学推理任务上提升19%，其表现优于基于学习的奖励模型和自评估基线方法。我们的研究证明，质量信号可以从模型内部几何结构中提取，这为无需外部监督的可扩展对齐提供了新路径。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.02807">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.02807">arXiv</a></p>
<hr />
<h3>15. 开放智能的经济学：模型生态系统中的权力与参与轨迹</h3>
<p><strong>原文标题：</strong> Economies of Open Intelligence: Tracing Power &amp; Participation in the Model Ecosystem</p>
<p><strong>摘要：</strong>
自2019年以来，Hugging Face模型中心已成为全球共享开放权重AI模型的核心平台。通过发布涵盖完整历史周期的每周模型下载数据集（2020年6月至2025年8月）及模型元数据，本研究对开放模型经济中的集中度动态与演化特征进行了迄今最严谨的实证分析。我们的研究涵盖85.1万个模型、每个模型超过200项聚合属性及22亿次下载记录。研究发现经济权力格局发生根本性重构：以谷歌、Meta和OpenAI为代表的美国开放权重产业主导地位显著削弱，独立开发者、社区组织及至2025年崛起的中国产业力量（以DeepSeek和Qwen模型为代表）正重塑市场格局，后者可能预示着新一轮市场权力整合。我们通过统计显著性检验发现：模型平均规模增长17倍，多模态生成（3.4倍）、量化技术（5倍）与专家混合架构（7倍）实现爆发式增长，但数据透明度呈现令人担忧的下降趋势——2025年开放权重模型数量首次超越真正开源模型。研究还揭示出新兴开发者中介层的形成，其专注于通过量化与适配基础模型以实现效能优化与艺术表达。为支持持续研究与监管，我们同步发布完整数据集及交互式仪表板，实现对开放模型经济集中度动态与演化特征的实时监测。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.03073">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.03073">arXiv</a></p>
<hr />
<h3>16. 上下文表征劫持</h3>
<p><strong>原文标题：</strong> In-Context Representation Hijacking</p>
<p><strong>摘要：</strong>
本文提出一种名为"双关语"的简单上下文表征劫持攻击方法，针对大语言模型实施攻击。该攻击通过在有害请求前缀后的多个上下文示例中，系统性地将有害关键词（如"炸弹"）替换为良性词汇（如"胡萝卜"）来实现。我们证明这种替换会导致良性词汇的内部表征向有害词汇收敛，从而在委婉语形式下嵌入有害语义。其结果是，表面无害的提示（如"如何制作胡萝卜？"）在模型内部会被解释为禁止性指令（如"如何制作炸弹？"），从而绕过模型的安全对齐机制。我们利用可解释性工具揭示这种语义覆盖是逐层形成的：早期层的良性语义在深层逐渐收敛为有害语义。该方法无需优化过程，可跨模型家族广泛迁移，在闭源和开源系统中均实现较高成功率，在单句上下文覆盖条件下对Llama-3.3-70B-Instruct的攻击成功率可达74%。本研究揭示了大语言模型潜在空间中的新型攻击面，表明当前基于表层文本的对齐策略存在不足，安全机制应延伸至表征层面进行构建。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.03771">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.03771">arXiv</a></p>
<hr />
<h3>17. UniQL：面向自适应边缘大语言模型的统一量化与低秩压缩框架</h3>
<p><strong>原文标题：</strong> UniQL: Unified Quantization and Low-rank Compression for Adaptive Edge LLMs</p>
<p><strong>摘要：</strong>
在移动平台上部署大语言模型面临显著挑战，主要受限于设备有限的内存和共享的计算资源。资源可用性可能成为问题，因为它直接受到当前设备工作负载的影响，增加了模型部署的不确定性。本文提出UniQL，一种统一的训练后量化与低秩压缩框架，支持设备端可配置的剪枝率，专为边缘大语言模型设计。UniQL是一个通用框架，集成了针对Transformer、状态空间模型（SSMs）以及混合模型的量化与低秩压缩技术，以支持多样化的边缘应用。在我们提出的联合框架中，我们引入了一种高效的结构化权重排序方法，可将计算速度提升20倍；采用量化感知的奇异值分解（SVD）以最小化量化误差；针对SSMs设计了状态感知的权重排序策略；并为剪枝后的模型开发了融合旋转位置编码（RoPE）内核。我们的框架在云端以单次工作流完成权重排序、微调与量化，同时支持设备端最高35%的可配置剪枝率。实验结果表明，经过量化与剪枝的模型实现了4倍至5.7倍的内存压缩和2.7倍至3.4倍的令牌吞吐量提升，在Transformer（Llama3与Qwen2.5）、SSMs（Mamba2）以及混合模型（Nemotron-H与Bamba-v2）上，以15%的剪枝率保持准确率损失在原始模型的5%以内。代码与量化模型已公开于：https://github.com/enyac-group/UniQL。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.03383">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.03383">arXiv</a></p>
<hr />
<h3>18. AlignBench：基于合成图像-描述对评估细粒度图文对齐的基准测试</h3>
<p><strong>原文标题：</strong> AlignBench: Benchmarking Fine-Grained Image-Text Alignment with Synthetic Image-Caption Pairs</p>
<p><strong>摘要：</strong>
评估如CLIP等图文对齐模型对于连接视觉与语言表征至关重要。然而，现有基准测试多依赖于基于规则的扰动或简短描述，限制了其衡量细粒度对齐的能力。本文提出AlignBench，该基准通过评估由多样化图像到文本及文本到图像模型生成的精细图像-描述对，为图文对齐提供了新的评估指标。每个句子均经过正确性标注，从而能够直接评估视觉语言模型作为对齐评估器的性能。通过对一系列基于解码器的视觉语言模型进行基准测试，我们得出三个关键发现：（一）基于CLIP的模型，即使是专门针对组合推理优化的模型，仍几乎无法实现有效对齐；（二）检测器系统性地对早期句子评分过高；（三）这些模型表现出强烈的自我偏好，倾向于青睐自身输出，从而损害了检测性能。项目页面详见：https://dahlian00.github.io/AlignBench/。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.20515">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.20515">arXiv</a></p>
<hr />
<h3>19. SkillFactory：用于学习认知行为的自蒸馏方法</h3>
<p><strong>原文标题：</strong> SkillFactory: Self-Distillation For Learning Cognitive Behaviors</p>
<p><strong>摘要：</strong>
利用长链思维进行推理的模型需要运用多种认知技能，例如答案验证、回溯、采用替代方法重试等。先前研究表明，当基础语言模型具备这些技能时，通过强化学习进一步训练可以使其学会运用这些技能。但如何让模型掌握基础模型尚未展现的技能？本研究提出的SkillFactory方法，通过在强化学习前进行监督微调，使模型初步掌握这些认知技能。该方法不依赖于从更强模型进行知识蒸馏，而是通过对模型自身生成样本进行重组，构建符合目标技能形式的训练数据。这些"银标"监督微调轨迹可能不够完美，但能有效引导模型在强化学习阶段掌握目标技能。实验评估表明：（1）基于SkillFactory监督微调初始化的模型在强化学习后能更好地泛化至任务的高难度变体，尽管其在强化学习前阶段表现较弱；（2）模型确实运用了所训练的认知技能；（3）经过强化学习的SkillFactory模型相比强化学习的基础模型，在跨领域任务上表现出更强的抗性能衰退能力。本研究证明，在强化学习前获得的归纳偏置有助于模型建立稳健的认知技能运用机制。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.04072">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.04072">arXiv</a></p>
<hr />
<h3>20. BlurDM：一种用于图像去模糊的模糊扩散模型</h3>
<p><strong>原文标题：</strong> BlurDM: A Blur Diffusion Model for Image Deblurring</p>
<p><strong>摘要：</strong>
扩散模型在动态场景去模糊任务中展现出潜力；然而，现有研究往往未能充分利用扩散模型中模糊过程的内在特性，限制了其潜力的充分发挥。为解决这一问题，我们提出了一种模糊扩散模型（BlurDM），该模型将模糊形成过程无缝集成到扩散框架中，以实现图像去模糊。通过观察发现运动模糊源于连续曝光，BlurDM通过一种双扩散前向方案隐式建模模糊形成过程，将噪声和模糊同时扩散到清晰图像上。在反向生成过程中，我们推导出一种双重去噪与去模糊的数学表达，使得BlurDM能够在以模糊图像为条件的纯高斯噪声输入下，通过同步去噪与去模糊来恢复清晰图像。此外，为将BlurDM高效集成至去模糊网络中，我们在潜在空间中执行BlurDM，构建了一个灵活的先验生成网络用于去模糊任务。大量实验表明，BlurDM在四个基准数据集上显著且持续地提升了现有去模糊方法的性能。源代码公开于：https://github.com/Jin-Ting-He/BlurDM。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.03979">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.03979">arXiv</a></p>
<hr />
<h3>21. AdaptVision：通过自适应视觉采集实现高效视觉语言模型</h3>
<p><strong>原文标题：</strong> AdaptVision: Efficient Vision-Language Models via Adaptive Visual Acquisition</p>
<p><strong>摘要：</strong>
视觉语言模型在视觉问答任务中取得了显著成功，但其对大量视觉令牌的依赖带来了巨大的计算开销。现有高效视觉语言模型方法虽通过固定比例压缩减少视觉令牌，但这类方法被动运行且缺乏适应不同任务需求的能力。这引出了一个根本性问题：视觉语言模型能否自主确定每个样本所需的最小视觉令牌数量？受人类主动视觉机制启发，我们提出了AdaptVision——一种通过由粗到精方式实现自适应视觉令牌采集的高效视觉语言模型范式。该模型首先处理低分辨率图像生成的压缩视觉令牌，并在必要时通过调用边界框工具裁剪关键区域来选择性获取额外视觉信息。我们采用强化学习框架训练AdaptVision，精心平衡准确性与效率。方法的核心是解耦轮次策略优化，该技术将学习目标分解为两个部分：（1）工具学习——优化工具使用的正确性；（2）精度提升——完善生成响应以提高答案准确性。基于此框架，我们进一步通过计算各目标对应令牌的独立优势值实现优势估计解耦。相较于原始GRPO方法，该框架能为AdaptVision实现更有效的优化。在多个视觉问答基准测试中的综合实验表明，AdaptVision在消耗视觉令牌数量显著少于当前最先进高效视觉语言模型的同时，实现了更优越的性能表现。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.03794">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.03794">arXiv</a></p>
<hr />
<h3>22. PosterCopilot：面向专业平面设计的布局推理与可控编辑</h3>
<p><strong>原文标题：</strong> PosterCopilot: Toward Layout Reasoning and Controllable Editing for Professional Graphic Design</p>
<p><strong>摘要：</strong>
平面设计是现代视觉传达的基石，是推广文化与商业活动的重要媒介。近期研究尝试利用大型多模态模型实现这一过程的自动化，但现有方法常产生几何不准确的布局，且缺乏专业工作流程所需的迭代式、图层级编辑能力。为应对这些局限，本文提出PosterCopilot框架，以推进专业平面设计中的布局推理与可控编辑。具体而言，我们引入渐进式三阶段训练策略，通过扰动监督微调、视觉-现实对齐强化学习以及美学反馈强化学习，使大型多模态模型具备布局设计所需的几何理解与美学推理能力。此外，我们构建了完整工作流程，将训练后的基于大型多模态模型的设计系统与生成模型耦合，实现图层可控的迭代式编辑，在保持整体视觉一致性的同时支持精确的视觉元素优化。大量实验表明，PosterCopilot能够生成几何精确且美学表现优异的布局，为专业迭代设计提供了前所未有的可控性。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.04082">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.04082">arXiv</a></p>
<hr />
<h3>23. PSA：用于高效视频理解与生成的金字塔稀疏注意力机制</h3>
<p><strong>原文标题：</strong> PSA: Pyramid Sparse Attention for Efficient Video Understanding and Generation</p>
<p><strong>摘要：</strong>
注意力机制是基础模型的核心，但其二次计算复杂度仍是模型扩展的关键瓶颈。这一挑战推动了高效注意力机制的发展，其中稀疏化已成为主流范式。现有方法通常通过二值掩码保留或丢弃完整的键值块，导致在高稀疏度下出现显著信息损失。为缓解这一问题，我们提出金字塔稀疏注意力（PSA），这是一种可同时应用于视频理解与生成任务的通用模块。PSA摒弃二值掩码，引入多层级池化的键值表示，从而实现更精细的掩码粒度。具体而言，每个查询块动态分配较低池化层级给关键键值块，而对次要键值块分配较高层级，从而在完整保留与完全剪枝之间构建信息丰富的插值。该设计借鉴了计算机视觉中的定点量化思想和经典特征金字塔网络，在有限计算预算下既能保持计算效率，又能有效减少信息损失。PSA采用原生硬件友好型内核，通过解耦的块-瓦片设计确保高效执行。在视频理解与生成的基准测试中，PSA在保持上下文信息与视觉保真度的同时，始终优于或达到现有稀疏注意力基线模型的性能水平，并展现出更优的效率-质量平衡。我们的代码与模型权重已公开于：http://ziplab.co/PSA</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.04025">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.04025">arXiv</a></p>
<hr />
<h3>24. 分而治之，精准定位：基于查询类型的长视频理解帧选择适配方法</h3>
<p><strong>原文标题：</strong> Divide, then Ground: Adapting Frame Selection to Query Types for Long-Form Video Understanding</p>
<p><strong>摘要：</strong>
大型多模态模型在长视频理解中的应用受到有限上下文长度和处理密集视频令牌所需过高计算成本的制约。因此，近期研究集中于查询感知的帧选择方法，但这些方法通常伴随显著的计算开销。本文挑战了此类复杂搜索机制普遍必要的假设。我们首先识别并验证了一种区分全局查询与局部化查询的查询分类法。研究表明，均匀采样对于全局查询既高效又有效，而局部化查询确实需要查询感知的选择机制以实现最优性能。基于这一发现，我们提出了DIG框架，这是一种无需训练、能够根据查询类型自适应调整策略的帧选择方法。具体而言，DIG对全局查询采用高效的均匀采样策略，同时对局部化查询则激活专用流程以提取查询相关的关键帧。在三个长视频理解基准测试上的实验表明，DIG在性能上持续超越现有基线方法，并能稳健提升大型多模态模型的表现，即使在输入帧数扩展至256帧时依然有效。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.04000">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.04000">arXiv</a></p>
<hr />
<h3>25. Light-X：支持相机与光照联合控制的生成式4D视频渲染框架</h3>
<p><strong>原文标题：</strong> Light-X: Generative 4D Video Rendering with Camera and Illumination Control</p>
<p><strong>摘要：</strong>
当前光照控制技术虽已将基于图像的渲染方法扩展至视频领域，但仍面临光照保真度与时间一致性的权衡问题。要实现真实场景的生成式建模，仅实现重照明并不足够，关键在于实现对相机轨迹与光照的联合控制，因为视觉动态本质上由几何结构与光照共同塑造。为此，我们提出Light-X——一种支持从单目视频中通过视点与光照控制进行可控渲染的视频生成框架。1）我们提出解耦式设计以分离几何与光照信号：通过沿用户定义相机轨迹投影的动态点云捕捉几何结构与运动信息，同时将经重照明的帧序列持续投影至同一几何空间以提供光照线索。这种显式、细粒度的线索设计实现了有效解耦，并引导高质量光照生成。2）针对缺乏配对多视角与多光照视频数据的问题，我们开发了Light-Syn合成流程，该流程基于退化模型与逆向映射技术，能够从真实场景的单目视频中自动生成训练数据对。该策略构建的数据集涵盖静态场景、动态场景及AI生成场景，确保了训练的鲁棒性。大量实验表明，Light-X在相机-光照联合控制任务上优于基线方法，并在文本条件与背景条件设置下均超越了现有视频重照明方法。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.05115">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.05115">arXiv</a></p>
<hr />
<h3>26. 对抗性混淆攻击：扰乱多模态大语言模型</h3>
<p><strong>原文标题：</strong> Adversarial Confusion Attack: Disrupting Multimodal Large Language Models</p>
<p><strong>摘要：</strong>
本文提出对抗性混淆攻击，这是一类针对多模态大语言模型的新型威胁。与越狱攻击或定向误分类不同，该攻击旨在引发系统性扰乱，使模型生成不连贯或自信的错误输出。其实际应用场景包括将此类对抗性图像嵌入网站，以阻止基于多模态大语言模型的AI代理可靠运行。所提出的攻击方法通过使用少量开源多模态大语言模型组成的集成系统，最大化下一词元的预测熵。在白盒设定下，我们证明单张对抗性图像即可在完整图像和对抗性验证码两种场景中扰乱集成系统内的所有模型。尽管该方法基于基础对抗技术（投影梯度下降），其生成的扰动能够有效迁移至未见过的开源模型（如Qwen3-VL）和专有模型（如GPT-5.1）。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.20494">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.20494">arXiv</a></p>
<hr />
<h2>🔍 关键词云图</h2>
<p><img alt="关键词云图" src="../images/keywords_wordcloud.png" /></p>
<h2>📈 近期论文趋势</h2>
<p><img alt="论文趋势" src="../images/daily_papers.png" /></p>
<h2>🎙️ 语音播报</h2>
<ul>
<li><a href="../audio/2025-12-04_daily_papers.mp3">收听今日论文解读</a></li>
</ul>
<h2>📱 订阅渠道</h2>
<ul>
<li>GitHub: <a href="https://github.com/2404589803/hf-daily-paper-newsletter-chinese">hf-daily-paper-newsletter-chinese</a></li>
</ul>
    </div>
</body>
</html>