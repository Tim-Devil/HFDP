<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Hugging Face 论文日报 - 2025-12-04</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', sans-serif;
            line-height: 1.6;
            color: #333;
            background-color: #f5f5f5;
            padding: 20px;
        }
        
        .container {
            max-width: 900px;
            margin: 0 auto;
            background-color: white;
            padding: 40px;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }
        
        h1 {
            color: #2c3e50;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
            margin-bottom: 30px;
            font-size: 28px;
        }
        
        h1 img {
            vertical-align: middle;
            margin-right: 10px;
        }
        
        h2 {
            color: #34495e;
            margin-top: 40px;
            margin-bottom: 20px;
            font-size: 24px;
            border-left: 4px solid #3498db;
            padding-left: 15px;
        }
        
        h3 {
            color: #2c3e50;
            margin-top: 30px;
            margin-bottom: 15px;
            font-size: 20px;
        }
        
        p {
            margin-bottom: 15px;
            text-align: justify;
        }
        
        ul {
            margin-left: 20px;
            margin-bottom: 20px;
        }
        
        li {
            margin-bottom: 8px;
        }
        
        a {
            color: #3498db;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        strong {
            color: #2c3e50;
            font-weight: 600;
        }
        
        hr {
            border: none;
            border-top: 1px solid #e0e0e0;
            margin: 30px 0;
        }
        
        /* 关键修复:限制图片宽度 */
        img {
            max-width: 100%;
            height: auto;
            display: block;
            margin: 20px auto;
            border-radius: 4px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        
        /* 确保图片容器也有宽度限制 */
        p img {
            max-width: 100%;
        }
        
        /* 论文详情区域样式 */
        .paper-section {
            background-color: #f9f9f9;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 20px;
        }
        
        /* 统计信息样式 */
        .stats {
            background-color: #e8f4f8;
            padding: 15px;
            border-radius: 6px;
            margin-bottom: 20px;
        }
        
        /* 响应式设计 */
        @media (max-width: 768px) {
            .container {
                padding: 20px;
            }
            
            h1 {
                font-size: 24px;
            }
            
            h2 {
                font-size: 20px;
            }
            
            h3 {
                font-size: 18px;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <h1><img src="https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo.png" width="30"/> Hugging Face 2025-12-04 论文日报</h1>
<h2>📊 今日论文统计</h2>
<ul>
<li>总论文数：26</li>
<li>热门领域：Vision, RL, LLM, Transformer, GPT</li>
</ul>
<h2>📝 论文详情</h2>
<h3>1. Qwen3-VL技术报告</h3>
<p><strong>原文标题：</strong> Qwen3-VL Technical Report</p>
<p><strong>摘要：</strong>
本文介绍Qwen系列迄今性能最强的视觉语言模型Qwen3-VL，该模型在广泛的多模态基准测试中展现出卓越性能。其原生支持高达256K令牌的交错上下文，能够无缝整合文本、图像与视频数据。该模型系列包含稠密架构（2B/4B/8B/32B）与混合专家架构（30B-A3B/235B-A22B）两类变体，以适应不同的延迟-质量权衡需求。Qwen3-VL具备三大核心优势：（一）显著增强的纯文本理解能力，在多项测试中超越可比规模的纯文本骨干模型；（二）强大的长上下文理解能力，针对文本与交错多模态输入均具备原生256K令牌窗口，能够实现对长文档和视频内容的可靠记忆、检索与交叉引用；（三）先进的跨模态推理能力，在单图、多图及视频任务中表现优异，在MMMU综合性测评及视觉数学基准（如MathVista与MathVision）中达到领先水平。在架构层面，我们引入三项关键升级：（一）增强型交错MRoPE机制，强化图像与视频的时空建模能力；（二）DeepStack集成技术，通过有效利用多层级ViT特征提升视觉-语言对齐度；（三）基于文本的视频时间对齐机制，从T-RoPE演进为显式文本时间戳对齐，实现更精确的时间定位。在可比令牌预算与延迟约束下，Qwen3-VL在稠密架构与混合专家架构中均取得优越性能。我们期待Qwen3-VL能够成为实际工作流程中图像推理、智能体决策与多模态代码智能的基础引擎。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.21631">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.21631">arXiv</a></p>
<hr />
<h3>2. 引导视觉-语言-动作模型作为反探索：一种测试时缩放方法</h3>
<p><strong>原文标题：</strong> Steering Vision-Language-Action Models as Anti-Exploration: A Test-Time Scaling Approach</p>
<p><strong>摘要：</strong>
通过流匹配或扩散目标训练的视觉-语言-动作模型擅长从大规模多模态数据集中学习复杂行为。然而，由于此类模型在预训练阶段融合了多样化的数据模式，且微调数据集常包含以运动学次优或不理想方式收集的示范数据，其中存在与下游任务成功动作模式无关的冗余动作模式。具体而言，我们观察到预训练模型在监督微调后，不同采样噪声在推理阶段存在显著的脆弱性。本文将此不稳定性归因于模型策略与下游任务数据集中稳定成功模式所诱导策略之间的分布偏移。为此，我们提出TACO——一种测试时缩放框架，该框架采用轻量级伪计数估计器作为动作片段的高保真验证器。集成TACO的模型可从所有采样动作片段中执行具有最大伪计数的动作，从而在防止分布偏移的同时保持模型的泛化能力。我们的方法类似于离线强化学习中的经典反探索原理，且无需梯度计算，相比强化学习更新具有显著计算优势。在四个仿真基准平台和实体双臂机器人上的实验表明，该方法能显著提升下游任务适应中的推理稳定性与成功率。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.02834">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.02834">arXiv</a></p>
<hr />
<h3>3. PretrainZero：强化主动预训练</h3>
<p><strong>原文标题：</strong> PretrainZero: Reinforcement Active Pretraining</p>
<p><strong>摘要：</strong>
模仿人类行为以主动从通用经验中学习并实现通用人工智能，一直是人类的梦想。近期基于强化学习的大规模思维模型在特定领域（如软件和数学）展现出令人瞩目的专家级能力，但仍严重依赖特定领域内可验证的奖励信号，这极大限制了通用推理能力边界的拓展。本研究提出PretrainZero，一种基于预训练语料库的强化主动学习框架，旨在将强化学习从领域特定的后训练阶段扩展至通用预训练阶段。PretrainZero具有以下特征：1）主动预训练：受人类主动学习能力启发，PretrainZero学习统一的推理策略，主动从预训练语料中识别合理且信息丰富的内容，并通过强化学习进行推理预测；2）自监督学习：无需任何可验证标签、预训练奖励模型或监督微调，我们直接在通用维基百科语料上使用强化学习对3B至30B的基础模型进行预训练，显著突破了通用推理的可验证数据壁垒；3）验证扩展：通过处理难度递增的掩码片段，PretrainZero大幅提升了预训练基础模型的通用推理能力。在强化预训练中，PretrainZero将Qwen3-4B-Base模型在MMLU-Pro、SuperGPQA和数学综合基准上的性能分别提升8.43、5.96和10.60分。在后训练阶段，预训练模型还可作为下游RLVR任务的推理基础模型。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.03442">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.03442">arXiv</a></p>
<hr />
<h3>4. ViDiC：视频差异描述</h3>
<p><strong>原文标题：</strong> ViDiC: Video Difference Captioning</p>
<p><strong>摘要：</strong>
理解动态场景之间的视觉差异需要对组合、空间和时间变化进行比较性感知——这一能力在现有的视觉-语言系统中仍未得到充分探索。尽管先前关于图像差异描述（IDC）的研究使模型能够描述静态图像之间的语义变化，但这些方法无法捕捉随时间变化的运动连续性、事件演化或编辑一致性。我们提出了ViDiC（视频差异描述）任务及其对应的ViDiC-1K数据集，旨在评估多模态大语言模型（MLLMs）对视频对之间相似性和差异进行细粒度描述的能力。ViDiC-1K包含1,000个精选视频对，标注了超过4,000项比较清单条目，涵盖七个类别：主体、风格、背景、摄影技术、运动、位置和播放技术。为确保可靠评估，我们提出了一种基于LLM-as-a-Judge协议的双清单框架，分别衡量相似性和差异的准确性。对十九个代表性多模态模型的实验揭示了它们在比较性描述和差异感知能力方面存在显著性能差距。我们希望ViDiC-1K能够成为一个具有挑战性的基准，为推进多模态智能中的视频理解、编辑感知和比较推理奠定坚实基础。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.03405">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.03405">arXiv</a></p>
<hr />
<h3>5. SpaceTools：基于双重交互式强化学习的工具增强空间推理</h3>
<p><strong>原文标题：</strong> SpaceTools: Tool-Augmented Spatial Reasoning via Double Interactive RL</p>
<p><strong>摘要：</strong>
视觉语言模型在定性视觉理解方面展现出强大能力，但在具身应用所需的精确度量空间推理方面仍存在局限。智能体范式表明，视觉语言模型可通过调用多种工具（如深度估计器、分割模型和姿态估计器）来增强这些能力。然而，如何在不依赖人工提示策略或固定预定义工具流水线（这些限制会阻碍模型发现最优工具使用模式）的前提下实现该愿景，仍是待解决的挑战。强化学习虽有望弥补这一差距，但由于多工具推理搜索空间庞大，现有研究多局限于单一视觉工具的使用。本文提出双重交互式强化学习框架，该两阶段训练框架使视觉语言模型能通过交互探索与反馈学习多工具协同机制。在教学阶段，我们将通过交互式强化学习训练的单工具专家示范与前沿模型使用全工具链的轨迹相结合；在探索阶段，模型通过持续强化学习进一步优化多工具协同策略。所提出的SpaceTools模型具备工具增强的空间推理能力，在空间理解基准测试（RoboSpatial-Home、BLINK、BOP-ASK）中达到最先进性能，并成功将七自由度机器人作为工具实现可靠的现实世界操控任务。双重交互式强化学习方法较原始监督微调基线（RoboSpatial指标提升12%）和强化学习基线（RoboSpatial指标提升16%）均有显著改进。项目主页：https://spacetools.github.io/。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.04069">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.04069">arXiv</a></p>
<hr />
<h3>6. OneThinker：面向图像与视频的一体化推理模型</h3>
<p><strong>原文标题：</strong> OneThinker: All-in-one Reasoning Model for Image and Video</p>
<p><strong>摘要：</strong>
强化学习近期在驱动多模态大语言模型进行视觉推理方面取得了显著成功。然而，现有方法通常针对不同任务训练独立模型，并将图像与视频推理视为分离的领域。这导致模型在向多模态通用推理系统扩展时受限，既制约了实际应用的灵活性，也阻碍了跨任务与跨模态的潜在知识共享。为此，我们提出OneThinker，一个一体化推理模型，能够统一处理涵盖问答、描述、时空定位、跟踪与分割等多种基础视觉任务的图像与视频理解。为实现这一目标，我们构建了覆盖所有上述任务的OneThinker-600k训练数据集，并利用商业模型进行思维链标注，从而得到用于监督微调冷启动的OneThinker-SFT-340k数据。此外，我们提出EMA-GRPO方法，通过追踪各任务奖励标准差的移动平均值来处理多任务强化学习中的奖励异质性问题，以实现均衡优化。在多种视觉基准上的大量实验表明，OneThinker在10类基础视觉理解任务、共31个基准测试中均展现出强劲性能。此外，该模型在特定任务间表现出有效的知识迁移能力，并具备初步的零样本泛化性能，标志着向统一的多模态通用推理系统迈出了重要一步。所有代码、模型与数据均已公开。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.03043">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.03043">arXiv</a></p>
<hr />
<h3>7. 重新思考文本到视觉生成中推理时扩展的提示设计</h3>
<p><strong>原文标题：</strong> Rethinking Prompt Design for Inference-time Scaling in Text-to-Visual Generation</p>
<p><strong>摘要：</strong>
在文本到视觉生成中，实现用户意图与生成视觉内容之间的精确对齐仍是一个核心挑战，因为单次生成尝试往往无法产生期望的输出。为解决这一问题，现有方法主要侧重于扩展视觉生成过程（例如增加采样步数或种子数量），但这很快会导致质量提升进入平台期。这一局限性的根源在于，指导生成的关键要素——提示词——在过程中保持固定。为此，我们提出推理时扩展的提示词重设计框架（简称PRIS），该框架能够在推理过程中根据扩展生成的视觉内容自适应地修订提示词。PRIS的核心思想是：审查已生成的视觉内容，识别其中反复出现的错误模式，并据此重新设计提示词，再使用修订后的提示词重新生成视觉内容。为了给提示词修订提供精确的对齐反馈，我们引入了一种新型验证器——元素级事实校正器，它能在细粒度层面评估提示词属性与生成视觉内容之间的对齐程度，相比整体性评估方法实现了更准确且可解释的判断。在文本到图像和文本到视频基准测试上的大量实验证明了我们方法的有效性，其中在VBench 2.0基准上取得了15%的性能提升。这些结果表明，在推理时联合扩展提示词与视觉生成是充分发挥扩展定律潜力的关键。可视化结果请访问网站：https://subin-kim-cv.github.io/PRIS。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.03534">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.03534">arXiv</a></p>
<hr />
<h3>8. RELIC：具备长时记忆的交互式视频世界模型</h3>
<p><strong>原文标题：</strong> RELIC: Interactive Video World Model with Long-Horizon Memory</p>
<p><strong>摘要：</strong>
一个真正交互式的世界模型需要三个关键要素：实时长时流式生成、一致的空间记忆以及精确的用户控制。然而，现有方法大多仅孤立地解决其中某一个方面，因为同时实现三者极具挑战性——例如，长时记忆机制往往会降低实时性能。本研究提出RELIC，一个统一框架以共同应对这三项挑战。给定单张图像和文本描述，RELIC能够实时对任意场景进行具备记忆感知的长时探索。基于近期自回归视频扩散蒸馏技术，本模型采用经高度压缩的历史潜在令牌表示长时记忆，这些令牌通过KV缓存中既包含相对动作又包含绝对相机位姿的编码方式实现。这种紧凑的相机感知记忆结构支持隐式的三维一致内容检索，并以最小计算开销保障长期连贯性。同时，我们微调了一个双向教师视频模型，使其能够生成超出原始5秒训练时长的序列，并通过一种新型内存高效的自强制范式将其转化为因果性学生生成器，该范式支持对长时教师序列及学生自生成序列进行全上下文蒸馏。RELIC实现为140亿参数模型，并在精心构建的虚幻引擎渲染数据集上训练，能够以16帧/秒的速度实时生成视频。与先前工作相比，本模型展现出更精确的动作跟随、更稳定的长时流式生成以及更鲁棒的空间记忆检索能力。这些特性使RELIC成为下一代交互式世界建模的坚实基础。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.04040">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.04040">arXiv</a></p>
<hr />
<h3>9. 以编程视角思考：迈向基于图像思维的统一框架</h3>
<p><strong>原文标题：</strong> Thinking with Programming Vision: Towards a Unified View for Thinking with Images</p>
<p><strong>摘要：</strong>
基于图像思维的多模态大语言模型（MLLMs）能够通过交互式工具调用对视觉输入进行推理，但现有方法通常依赖工具集较为局限，其实际必要性与可扩展性不足。本研究首先揭示了一个关键且长期被忽视的缺陷：即使是最先进的MLLMs也表现出惊人的脆弱性，在图像发生简单方向变换或自然退化时性能显著下降，这凸显了基于工具的推理需要更强的鲁棒性。为解决这一问题，我们提出CodeVision——一个灵活且可扩展的“代码即工具”框架。该框架以生成代码作为通用接口来调用任意图像操作，突破了固定工具注册表的限制。我们采用两阶段方法训练模型：首先在针对复杂多轮工具组合与错误恢复构建的高质量数据集上进行监督微调（SFT），随后通过强化学习（RL）结合新颖的密集过程奖励函数，以激励策略性、高效的工具使用。为推进相关研究，我们构建了全新的SFT与RL数据集，并设计了一套具有挑战性的基准测试集，用于系统评估模型对方向变化的鲁棒性及多工具推理能力。在Qwen2.5-VL与Qwen3-VL系列模型上的实验表明，该方法显著提升了模型性能，并催生了灵活工具组合、高效链式执行、基于运行时反馈的鲁棒错误恢复等新兴能力。代码已开源：https://github.com/ByteDance-BandAI/CodeVision。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.03746">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.03746">arXiv</a></p>
<hr />
<h3>10. 逆向流动：通过反向表示对齐改进标准化流</h3>
<p><strong>原文标题：</strong> Flowing Backwards: Improving Normalizing Flows via Reverse Representation Alignment</p>
<p><strong>摘要：</strong>
标准化流是一类以数学可逆架构为特色的生成模型，其前向过程将数据转换至隐空间以进行密度估计，而反向过程则从该空间生成新样本。这一特性在表示学习与数据生成之间形成了内在的协同机制。然而，传统标准化流的生成质量受限于基于对数似然优化所获得的语义表示能力不足。为改进此问题，我们提出一种创新的对齐策略，巧妙利用标准化流的可逆特性：不同于常规的前向过程正则化方法，我们将生成（反向）过程的中间特征与强大视觉基础模型的表示进行对齐，实验证明该方法相较于简单对齐策略具有显著优势。同时，我们提出一种无需训练、基于测试时优化的新型分类算法，为评估标准化流内嵌语义知识提供了更本质的衡量方式。综合实验表明，所提方法将标准化流的训练速度提升超过3.3倍，同时在生成质量与分类精度方面均取得显著提升。我们在ImageNet 64×64和256×256数据集上创造了标准化流模型的最新性能纪录。代码已开源：https://github.com/MCG-NJU/FlowBack。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.22345">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.22345">arXiv</a></p>
<hr />
<h3>11. Jina-VLM：小型多语言视觉语言模型</h3>
<p><strong>原文标题：</strong> Jina-VLM: Small Multilingual Vision Language Model</p>
<p><strong>摘要：</strong>
本文提出Jina-VLM，这是一个拥有24亿参数规模的视觉语言模型，在公开的20亿参数级别视觉语言模型中实现了领先的多语言视觉问答性能。该模型通过注意力池化连接器将SigLIP2视觉编码器与Qwen3语言主干网络相结合，实现了对任意分辨率图像的高效令牌处理。在标准视觉问答基准测试和多语言评估中，Jina-VLM在保持竞争力纯文本性能的同时，均优于同类可比模型。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.04032">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.04032">arXiv</a></p>
<hr />
<h3>12. CookAnything：一种灵活且一致的多步骤食谱图像生成框架</h3>
<p><strong>原文标题：</strong> CookAnything: A Framework for Flexible and Consistent Multi-Step Recipe Image Generation</p>
<p><strong>摘要：</strong>
烹饪是一种具有时序性和视觉基础的活动，其中切菜、搅拌或煎炸等每个步骤都包含程序逻辑和视觉语义。尽管近期扩散模型在文本到图像生成方面展现出强大能力，但其在处理如食谱图解这类结构化多步骤场景时仍面临困难。此外，当前食谱图解方法无法适应食谱长度的自然变化，无论实际指令结构如何，均生成固定数量的图像。为应对这些局限，本文提出CookAnything——一种基于扩散模型的灵活且一致的框架，能够根据任意长度的文本烹饪指令生成连贯且语义分明的图像序列。该框架引入三个关键组件：(1) 步骤区域控制，可在单次去噪过程中将文本步骤与对应图像区域对齐；(2) 灵活旋转位置编码，这是一种感知步骤的位置编码机制，能同时增强时序连贯性与空间多样性；(3) 跨步骤一致性控制，用于保持不同步骤间食材的细粒度一致性。在食谱图解基准测试上的实验结果表明，CookAnything在基于训练和无训练设置下均优于现有方法。所提框架支持对复杂多步骤指令进行可扩展的高质量视觉合成，在教学媒体与流程化内容创作等领域具有广泛的应用潜力。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.03540">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.03540">arXiv</a></p>
<hr />
<h3>13. AutoNeural：面向NPU推理的视觉-语言模型协同设计</h3>
<p><strong>原文标题：</strong> AutoNeural: Co-Designing Vision-Language Models for NPU Inference</p>
<p><strong>摘要：</strong>
尽管神经处理单元（NPU）为边缘人工智能提供了较高的理论效率，但专为GPU设计的前沿视觉-语言模型（VLM）在这些硬件平台上往往表现不佳。我们将这种硬件与模型之间的不匹配归因于两个主要因素：视觉变换器（ViT）的量化脆弱性，以及自回归注意力机制受输入/输出限制的特性，后者无法充分利用NPU的高算术吞吐能力。为弥合这一差距，本文提出AutoNeural——一种专为纯整数推理协同设计的原生NPU视觉-语言模型架构。我们采用基于深度可分离卷积的MobileNetV5风格主干网络替代标准ViT编码器，该设计通过限定激活值分布确保了INT4/8/16量化的稳定性。与此互补，我们的语言主干网络将状态空间模型（SSM）原理与变换器层相结合，采用高效门控卷积实现线性时间复杂度。这种混合设计消除了生成过程中键值缓存带来的沉重内存输入/输出开销。相较于传统基线方法，我们的方案实现了显著的效率提升：视觉编码器量化误差降低最高达7倍，端到端延迟减少14倍。AutoNeural的解码速度达到基线的3倍，上下文窗口长度扩展至基线的4倍。我们通过基于高通SA8295P系统级芯片的实际汽车案例研究验证了这些改进，证明了其在座舱应用场景中可实现实时性能。研究结果凸显了针对NPU硬件约束重新设计模型拓扑结构是实现鲁棒多模态边缘智能的必要前提。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.02924">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.02924">arXiv</a></p>
<hr />
<h3>14. SR-GRPO：以稳定秩作为大语言模型对齐的固有几何奖励</h3>
<p><strong>原文标题：</strong> SR-GRPO: Stable Rank as an Intrinsic Geometric Reward for Large Language Model Alignment</p>
<p><strong>摘要：</strong>
将大语言模型与人类偏好对齐通常依赖于外部监督，但这种方法存在关键局限：人工标注稀缺且主观，奖励模型易受奖励攻击影响，而自评估方法则受提示敏感性和偏差的困扰。本研究提出稳定秩这一源自模型表征的、无需标注的固有质量信号。稳定秩通过计算总方差与主导方向方差的比值来度量隐藏状态的有效维度，从信息在表征维度间的分布方式中捕捉质量信息。实验表明，稳定秩在RewardBench上达到84.04%的准确率，并通过N选优采样策略将任务准确率较贪婪解码平均提升11.3个百分点。基于此发现，我们提出稳定秩分组相对策略优化方法，将稳定秩作为强化学习的奖励信号。在无外部监督的情况下，SR-GRPO将Qwen2.5-1.5B-Instruct模型在STEM任务上的性能提升10%，在数学推理任务上提升19%，其表现优于基于学习的奖励模型和自评估基线方法。我们的研究证明，质量信号可从模型内部几何结构中提取，这为无需外部监督的可扩展对齐提供了新路径。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.02807">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.02807">arXiv</a></p>
<hr />
<h3>15. 开放智能的经济学：模型生态系统中的权力与参与轨迹</h3>
<p><strong>原文标题：</strong> Economies of Open Intelligence: Tracing Power &amp; Participation in the Model Ecosystem</p>
<p><strong>摘要：</strong>
自2019年以来，Hugging Face模型中心已成为全球共享开放权重AI模型的核心平台。通过发布涵盖完整历史周期的每周模型下载数据集（2020年6月至2025年8月）及模型元数据，本研究对开放模型经济中的集中度动态与演化特征进行了迄今最严格的实证分析。我们的研究涵盖85.1万个模型、每个模型超过200项聚合属性及22亿次下载数据。研究记录了经济权力的根本性重构：以谷歌、Meta和OpenAI为代表的美国开放权重行业主导地位显著下降，取而代之的是独立开发者、社区组织以及截至2025年崛起的中国产业力量——深度求索（DeepSeek）与通义千问（Qwen）模型可能预示着市场权力的新一轮整合。我们通过统计显著性检验发现：模型属性发生系统性变迁，平均模型规模增长17倍，多模态生成（增长3.4倍）、量化技术（增长5倍）与专家混合架构（增长7倍）呈现爆发式扩张，但数据透明度出现令人担忧的下降趋势——2025年开放权重模型数量首次超越真正开源模型。研究揭示出新兴的开发者中介层，其专注于基础模型的量化与适配，兼顾效率优化与艺术表达。为支持持续研究与监管，我们同步发布完整数据集及交互式仪表板，实现对开放模型经济集中度动态与演化特性的实时监测。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.03073">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.03073">arXiv</a></p>
<hr />
<h3>16. 上下文表征劫持攻击</h3>
<p><strong>原文标题：</strong> In-Context Representation Hijacking</p>
<p><strong>摘要：</strong>
本文提出“双关语攻击”——一种针对大语言模型的简易上下文表征劫持攻击方法。该攻击通过在包含恶意请求前缀的多个上下文示例中，系统性地将危险关键词（如“炸弹”）替换为良性词汇（如“胡萝卜”）来实现。我们证明这种替换会导致良性词汇的内部表征逐渐收敛至危险词汇的表征，从而在委婉语的外衣下有效嵌入危险语义。其结果是，表面无害的提示（如“如何制作胡萝卜？”）在模型内部被解读为禁止性指令（如“如何制作炸弹？”），以此绕过模型的安全对齐机制。我们利用可解释性工具揭示了这种语义覆盖现象逐层涌现的过程：早期层的良性语义在深层逐渐收敛为危险语义。双关语攻击无需优化过程，可跨模型家族广泛迁移，在闭源和开源系统中均实现高攻击成功率——仅通过单句上下文覆盖即在Llama-3.3-70B-Instruct模型上达到74%的攻击成功率。本研究揭示了大型语言模型潜在空间中的新型攻击面，表明当前基于表层文本的对齐策略存在不足，安全防护应延伸至表征层面。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.03771">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.03771">arXiv</a></p>
<hr />
<h3>17. UniQL：面向自适应边缘大语言模型的统一量化与低秩压缩框架</h3>
<p><strong>原文标题：</strong> UniQL: Unified Quantization and Low-rank Compression for Adaptive Edge LLMs</p>
<p><strong>摘要：</strong>
在移动平台上部署大语言模型面临显著挑战，主要受限于设备有限的内存和共享的计算资源。资源可用性可能成为问题，因其直接受当前设备工作负载影响，增加了模型部署的不确定性。本文提出UniQL，一种统一的训练后量化与低秩压缩框架，支持设备端可配置的剪枝率，专为边缘大语言模型设计。UniQL是一个通用框架，集成了针对Transformer、状态空间模型及混合模型的量化与低秩压缩技术，以支持多样化的边缘应用。在我们提出的联合框架中，我们引入了一种高效的结构化权重排序方法，可将计算速度提升20倍；采用量化感知奇异值分解以最小化量化误差；针对状态空间模型设计了状态感知权重排序策略；并为剪枝模型开发了融合旋转位置编码内核。该框架在云端以单流程工作流完成权重排序、微调与量化，同时支持设备端最高35%的可配置剪枝率。实验表明，经过量化与剪枝的模型实现了4倍至5.7倍的内存压缩和2.7倍至3.4倍的令牌吞吐量提升，在Transformer、状态空间模型及混合模型上，当剪枝率为15%时，模型精度损失控制在原始模型5%以内。相关代码与量化模型已开源：https://github.com/enyac-group/UniQL。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.03383">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.03383">arXiv</a></p>
<hr />
<h3>18. AlignBench：基于合成图像-描述对评估细粒度图文对齐的基准</h3>
<p><strong>原文标题：</strong> AlignBench: Benchmarking Fine-Grained Image-Text Alignment with Synthetic Image-Caption Pairs</p>
<p><strong>摘要：</strong>
评估如CLIP等图文对齐模型对于连接视觉与语言表征至关重要。然而，现有基准依赖基于规则的扰动或简短描述，限制了其衡量细粒度对齐的能力。我们提出了AlignBench，该基准通过评估由多样化图像到文本及文本到图像模型生成的精细图像-描述对，为图文对齐提供了新的评估指标。每个句子均标注正确性，从而能够直接评估视觉语言模型作为对齐评估器的性能。对一系列基于解码器的视觉语言模型进行基准测试后，我们得出三个关键发现：（一）基于CLIP的模型，即便是专为组合推理设计的模型，仍近乎“失明”；（二）检测器系统性地对早期句子评分过高；（三）这些模型表现出强烈的自我偏好，倾向于自身输出，从而损害检测性能。项目页面详见：https://dahlian00.github.io/AlignBench/。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.20515">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.20515">arXiv</a></p>
<hr />
<h3>19. SkillFactory：用于学习认知行为的自蒸馏方法</h3>
<p><strong>原文标题：</strong> SkillFactory: Self-Distillation For Learning Cognitive Behaviors</p>
<p><strong>摘要：</strong>
利用长链思维进行推理的模型需要运用多种认知技能，例如答案验证、回溯、采用替代方法重试等。先前研究表明，当基础语言模型具备这些技能时，通过强化学习进一步训练该模型可以使其学会运用这些技能。但如何让模型掌握基础模型原本不具备的技能？本研究提出的SkillFactory方法，通过在强化学习前的监督微调阶段对模型进行精细调优，使其初步掌握这些技能。该方法不依赖于从更强模型进行知识蒸馏，而是利用模型自身生成的样本，通过重组构建符合目标技能格式的训练数据。这些"银标"监督微调轨迹可能不够完美，但能有效引导模型在强化学习阶段掌握相关技能。评估结果表明：（1）基于SkillFactory监督微调初始化的模型在强化学习后能更好地泛化至任务的困难变体，尽管其在强化学习前的表现较弱；（2）模型确实运用了认知技能；（3）经过强化学习的SkillFactory模型相比强化学习的基础模型，在跨领域任务上表现出更强的抗退化能力。本研究说明，在强化学习前获得的归纳偏置有助于模型建立稳健的认知技能运用能力。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.04072">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.04072">arXiv</a></p>
<hr />
<h3>20. BlurDM：一种用于图像去模糊的模糊扩散模型</h3>
<p><strong>原文标题：</strong> BlurDM: A Blur Diffusion Model for Image Deblurring</p>
<p><strong>摘要：</strong>
扩散模型在动态场景去模糊任务中展现出潜力，但现有研究往往未能充分利用扩散模型中模糊过程的内在特性，限制了其性能的充分发挥。为此，我们提出了一种模糊扩散模型（BlurDM），该模型将模糊形成过程无缝集成到扩散框架中，以实现图像去模糊。基于运动模糊源于连续曝光这一观察，BlurDM通过一种双重扩散前向方案隐式建模模糊形成过程，将噪声和模糊同时扩散至清晰图像上。在反向生成过程中，我们推导出双重去噪与去模糊的数学表达，使得BlurDM能够以模糊图像为条件输入的高斯噪声作为起点，通过同步去噪与去模糊来恢复清晰图像。此外，为将BlurDM高效集成至去模糊网络中，我们在隐空间执行BlurDM计算，构建了一个灵活的先验生成网络用于去模糊任务。大量实验表明，BlurDM在四个基准数据集上显著且持续地提升了现有去模糊方法的性能。源代码已公开于https://github.com/Jin-Ting-He/BlurDM。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.03979">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.03979">arXiv</a></p>
<hr />
<h3>21. AdaptVision：通过自适应视觉获取实现高效视觉语言模型</h3>
<p><strong>原文标题：</strong> AdaptVision: Efficient Vision-Language Models via Adaptive Visual Acquisition</p>
<p><strong>摘要：</strong>
视觉语言模型在视觉问答任务中取得了显著成功，但其对大量视觉标记的依赖引入了巨大的计算开销。现有高效视觉语言模型方法通过固定比例压缩减少视觉标记，但这类方法被动运行且缺乏适应不同任务需求的能力。这引出了一个根本性问题：视觉语言模型能否自主决定每个样本所需的最小视觉标记数量？受人类主动视觉机制启发，我们提出了AdaptVision——一种通过粗到细方式实现自适应视觉标记获取的高效视觉语言模型范式。该模型首先处理低分辨率图像生成的压缩视觉标记，并在必要时通过调用边界框工具裁剪关键区域来选择性获取额外视觉信息。我们采用强化学习框架训练AdaptVision，精心平衡准确性与效率。方法的核心是解耦轮次策略优化，该技术将学习目标分解为两个部分：（1）工具学习：优化工具的正确使用；（2）精度提升：优化生成回答以提高答案正确性。基于此框架，我们进一步通过计算各目标对应标记的独立优势值来实现优势估计解耦。相较于原始GRPO方法，该框架能为AdaptVision实现更有效的优化。在多个视觉问答基准测试中的综合实验表明，AdaptVision在消耗视觉标记数量显著少于现有高效视觉语言模型方法的同时，实现了更优越的性能。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.03794">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.03794">arXiv</a></p>
<hr />
<h3>22. PosterCopilot：面向专业平面设计的布局推理与可控编辑</h3>
<p><strong>原文标题：</strong> PosterCopilot: Toward Layout Reasoning and Controllable Editing for Professional Graphic Design</p>
<p><strong>摘要：</strong>
平面设计是现代视觉传达的基石，是推广文化与商业活动的重要媒介。近年来，研究开始探索利用大型多模态模型自动完成这一过程，但现有方法常产生几何不准确的布局，且缺乏专业工作流程所需的迭代式、图层级编辑能力。为应对这些局限，本文提出PosterCopilot框架，以推进专业平面设计中的布局推理与可控编辑。具体而言，我们引入一种渐进式三阶段训练策略，使大型多模态模型具备布局设计所需的几何理解与美学推理能力，该策略包括扰动监督微调、视觉-现实对齐的强化学习以及基于美学反馈的强化学习。此外，我们开发了一套完整的工作流程，将训练后的基于大型多模态模型的设计模型与生成模型相结合，支持图层可控的迭代式编辑，以实现元素的精细化调整，同时保持整体视觉一致性。大量实验表明，PosterCopilot能够生成几何精确且美学优越的布局，为专业迭代式设计提供了前所未有的可控性。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.04082">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.04082">arXiv</a></p>
<hr />
<h3>23. PSA：金字塔稀疏注意力机制用于高效视频理解与生成</h3>
<p><strong>原文标题：</strong> PSA: Pyramid Sparse Attention for Efficient Video Understanding and Generation</p>
<p><strong>摘要：</strong>
注意力机制是基础模型的核心，但其二次计算复杂度仍是模型扩展的关键瓶颈。这一挑战推动了高效注意力机制的发展，其中稀疏化已成为主流范式。现有方法通常通过二值掩码保留或丢弃完整的键值块，在高稀疏度下会导致显著的信息损失。为缓解这一问题，我们提出金字塔稀疏注意力（PSA）——一个可同时应用于视频理解与生成任务的通用模块。PSA摒弃二值掩码，引入多层级池化的键值表示，从而实现更精细的掩码粒度。具体而言，每个查询块动态分配较低池化层级给关键键值块，对次要键值块则分配较高池化层级，从而在完整保留与完全剪枝之间构建信息丰富的插值。该设计借鉴了计算机视觉中的定点量化思想和经典特征金字塔网络，在有限计算预算下既能保持计算效率，又能有效减少信息损失。PSA采用原生硬件友好型内核，通过解耦的块-瓦片设计确保高效执行。在视频理解与生成的各项基准测试中，PSA在保持上下文信息与视觉保真度的同时，始终优于或达到现有稀疏注意力基线模型的性能水平，并实现了更优的效率-质量平衡。我们的代码与模型权重已公开于：http://ziplab.co/PSA</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.04025">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.04025">arXiv</a></p>
<hr />
<h3>24. 分而治之，按需选帧：针对查询类型的长视频理解帧选择适配方法</h3>
<p><strong>原文标题：</strong> Divide, then Ground: Adapting Frame Selection to Query Types for Long-Form Video Understanding</p>
<p><strong>摘要：</strong>
大规模多模态模型在长视频理解中的应用受限于有限的上下文长度以及处理密集视频标记时高昂的计算成本。因此，近期研究集中于查询感知的帧选择方法，但这些方法通常伴随着显著的计算开销。本文挑战了此类复杂搜索机制普遍必要的假设。我们首先识别并验证了一种区分全局查询与局部化查询的查询分类法。研究表明，均匀采样对于全局查询既高效又有效，而局部化查询确实需要查询感知的选择才能达到最佳性能。基于此发现，我们提出了DIG，一个无需训练、可根据查询类型自适应调整策略的帧选择框架。具体而言，DIG对全局查询采用高效的均匀采样，同时针对局部化查询激活一个专用流程以提取查询相关的关键帧。在三个长视频理解基准测试上的实验表明，DIG始终优于现有基线方法，并能稳健地提升大规模多模态模型的性能，即使在输入帧数扩展至256帧时依然有效。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.04000">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.04000">arXiv</a></p>
<hr />
<h3>25. Light-X：支持相机与光照控制的生成式4D视频渲染</h3>
<p><strong>原文标题：</strong> Light-X: Generative 4D Video Rendering with Camera and Illumination Control</p>
<p><strong>摘要：</strong>
当前光照控制技术已将基于图像的方法延伸至视频领域，但在光照保真度与时间一致性之间仍面临权衡。要实现真实场景的生成式建模，仅进行重照明处理远远不够，关键在于实现对相机轨迹与光照的联合控制，因为视觉动态本质上由几何结构与光照共同塑造。为此，我们提出Light-X——一个支持通过单目视频进行视点与光照可控渲染的视频生成框架。1）我们提出解耦式设计以分离几何与光照信号：通过沿用户定义相机轨迹投影的动态点云捕捉几何结构与运动信息，同时将经重照明的帧持续投影至同一几何空间以提供光照线索。这种显式、细粒度的线索设计实现了有效的信号解耦，并引导高质量光照生成。2）针对缺乏配对多视角与多光照视频数据的问题，我们开发了Light-Syn流程：该基于退化增强与逆向映射的管线能够从野外单目影像中合成训练数据对。此策略构建的数据集涵盖静态场景、动态场景及AI生成场景，确保了训练的鲁棒性。大量实验表明，Light-X在相机-光照联合控制任务中优于基线方法，并在文本条件与背景条件设置下均超越了现有视频重照明方法。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.05115">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.05115">arXiv</a></p>
<hr />
<h3>26. 对抗性混淆攻击：对多模态大语言模型的系统性干扰</h3>
<p><strong>原文标题：</strong> Adversarial Confusion Attack: Disrupting Multimodal Large Language Models</p>
<p><strong>摘要：</strong>
本文提出对抗性混淆攻击，这是一类针对多模态大语言模型的新型威胁。与越狱攻击或定向误分类不同，该攻击旨在引发系统性干扰，迫使模型生成语义混乱或自信但错误的输出。其实际应用场景包括将此类对抗性图像嵌入网页，以阻止基于多模态大语言模型的智能体可靠运行。本攻击方法通过使用小型开源多模态大语言模型集合，最大化模型对下一标记预测的熵值。在白盒设定下，实验表明单张对抗性图像即可在完整图像和对抗性验证码两种场景中干扰集合内所有模型。尽管采用基础对抗技术（投影梯度下降），所生成的扰动能够有效迁移至未见过的开源模型（如Qwen3-VL）与闭源商业模型（如GPT-5.1）。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.20494">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.20494">arXiv</a></p>
<hr />
<h2>🔍 关键词云图</h2>
<p><img alt="关键词云图" src="../images/keywords_wordcloud.png" /></p>
<h2>📈 近期论文趋势</h2>
<p><img alt="论文趋势" src="../images/daily_papers.png" /></p>
<h2>🎙️ 语音播报</h2>
<ul>
<li><a href="../audio/2025-12-04_daily_papers.mp3">收听今日论文解读</a></li>
</ul>
<h2>📱 订阅渠道</h2>
<ul>
<li>GitHub: <a href="https://github.com/2404589803/hf-daily-paper-newsletter-chinese">hf-daily-paper-newsletter-chinese</a></li>
</ul>
    </div>
</body>
</html>