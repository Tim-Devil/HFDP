
# <img src="https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo.png" width="30"/> Hugging Face 2025-11-25 论文日报

## 📊 今日论文统计
- 总论文数：31
- 热门领域：RL, AIGC, LLM

## 📝 论文详情


### 1. 基于深度研究的通用智能体记忆框架

**原文标题：** General Agentic Memory Via Deep Research

**摘要：**
记忆对人工智能体至关重要，然而当前广泛采用的静态记忆系统试图预先创建即用型记忆，不可避免地会导致严重的信息损失。为解决这一局限，我们提出了一种名为通用智能体记忆（GAM）的创新框架。GAM遵循"即时编译"原则，在离线阶段仅保留简洁有效的记忆，而在运行时专注于为客户端生成优化上下文。为实现这一目标，GAM采用双组件设计：1）记忆器通过轻量级记忆突出关键历史信息，同时在通用页面存储库中维护完整历史记录；2）研究器基于预构建记忆的指引，从页面存储库中检索并整合有效信息以响应在线请求。该设计使GAM能够有效利用前沿大语言模型的智能体能力与测试时扩展性，同时通过强化学习实现端到端的性能优化。实验研究表明，相较于现有记忆系统，GAM在各类基于记忆的任务完成场景中均实现了显著提升。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.18423) | [arXiv](https://arxiv.org/abs/2511.18423)



---

### 2. AutoEnv：用于测量跨环境智能体学习的自动化环境

**原文标题：** AutoEnv: Automated Environments for Measuring Cross-Environment Agent Learning

**摘要：**
人类能够通过在不同动态特性、观测空间和奖励结构的世界中学习底层规则，自然适应多样化环境。而现有智能体通常通过在单一领域内自我演化来提升性能，这种范式隐含假设了固定的环境分布。跨环境学习至今缺乏系统化度量：既缺乏可控异构环境的标准数据集，也没有统一的方法来表征智能体学习过程。我们通过两个步骤解决这些问题。首先提出AutoEnv自动化框架，将环境建模为可分解的状态转移、观测和奖励分布，实现了低成本（平均4.12美元）生成异构世界。基于AutoEnv构建的AutoEnv-36数据集包含36个环境共358个验证关卡，七个语言模型在该数据集上仅获得12-49%的标准化奖励，证明了其挑战性。其次，我们将智能体学习形式化为以组件为中心的过程，包含作用于可改进智能体组件的选择、优化和评估三个阶段。基于此形式化框架，设计了八种学习方法并在AutoEnv-36上进行评估。实验表明：当环境数量增加时，任何单一学习方法的收益都会快速衰减，证明固定学习方法难以适应异构环境扩展。环境自适应的学习方法选择能显著提升性能，但随着方法空间扩大会出现收益递减。这些结果既揭示了实现可扩展跨环境泛化的必要性，也暴露了当前智能体学习的局限性，确立了AutoEnv与AutoEnv-36作为研究跨环境智能体学习的基准平台。代码已开源：https://github.com/FoundationAgents/AutoEnv。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.19304) | [arXiv](https://arxiv.org/abs/2511.19304)



---

### 3. 计算机使用代理作为生成式用户界面的评估者

**原文标题：** Computer-Use Agents as Judges for Generative User Interface

**摘要：**
计算机使用代理（CUA）通过图形用户界面（GUI）自主操作数字环境的能力日益增强。然而，大多数GUI仍主要面向人类设计——优先考虑美学与可用性——迫使代理采用对人类友好但执行效率低下的操作方式。与此同时，面向编程的语言模型（Coder）的快速发展正在改变自动GUI设计范式。这引发了一个根本性问题：能否将CUA作为评估者协助Coder进行自动GUI设计？为此，我们推出AUI-Gym基准测试平台，涵盖52个跨领域应用程序的自动GUI开发评估。基于语言模型，我们合成了模拟真实场景的1560项任务。为确保任务可靠性，我们进一步开发了可通过编程验证任务在对应环境中可执行性的检测器。在此基础上，我们提出“编码者-代理协同”框架：Coder担任设计师角色，负责生成和修改网站；CUA担任评估者角色，负责功能验证与设计优化。评估标准不再局限于视觉呈现，而是聚焦于任务可解性及CUA导航成功率。为将CUA反馈转化为有效指导，我们设计了CUA仪表盘，将多步导航历史压缩为简洁的可视化摘要，为迭代重设计提供可解释的改进建议。通过让智能体同时担任设计者与评估者，本框架将界面设计推向面向智能体的原生效率与可靠性。我们的工作推动了智能体从被动使用数字环境向主动参与的重要转变。代码与数据集详见https://github.com/showlab/AUI。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.15567) | [arXiv](https://arxiv.org/abs/2511.15567)



---

### 4. DeCo：面向端到端图像生成的频域解耦像素扩散方法

**原文标题：** DeCo: Frequency-Decoupled Pixel Diffusion for End-to-End Image Generation

**摘要：**
像素扩散方法旨在以端到端方式直接在像素空间生成图像。该方法规避了两阶段潜在扩散中VAE的局限性，具有更高的模型容量。现有像素扩散模型存在训练与推理速度缓慢的问题，因其通常采用单一扩散变换器同时对高频信号与低频语义进行建模。为探索更高效的像素扩散范式，本文提出频域解耦的像素扩散框架。基于高低频分量解耦生成的直觉认知，我们采用轻量级像素解码器在扩散变换器提供的语义引导下生成高频细节，从而使扩散变换器专注于低频语义建模。此外，我们引入了频域感知流匹配损失函数，该函数能强化视觉显著频率同时抑制非显著频率。大量实验表明，DeCo在像素扩散模型中实现了卓越性能，在ImageNet数据集上获得1.62（256×256）和2.22（512×512）的FID指标，显著缩小了与潜在扩散方法的差距。此外，我们预训练的文本生成图像模型在GenEval系统级评估中取得了0.86的综合得分，处于领先水平。代码已开源：https://github.com/Zehong-Ma/DeCo。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.19365) | [arXiv](https://arxiv.org/abs/2511.19365)



---

### 5. DR Tulu：基于演化量规强化学习的深度研究框架

**原文标题：** DR Tulu: Reinforcement Learning with Evolving Rubrics for Deep Research

**摘要：**
深度研究模型通过多步骤研究过程生成具有完整溯源的长篇答案。然而当前多数开源深度研究模型基于可验证奖励的强化学习（RLVR）方法，在易于验证的短问答任务上训练，难以适用于现实长篇研究任务。我们提出演化量规强化学习（RLER）方法，通过构建与策略模型协同演化的评估量规，使量规能够融合模型新探索的信息并提供具有区分度的同策略反馈。基于RLER方法，我们开发了深度研究模型DR Tulu-8B，这是首个专门针对开放式长篇深度研究任务直接训练的开源模型。在科学、医疗和通用领域的四个长篇深度研究基准测试中，DR Tulu显著优于现有开源深度研究模型，达到或超越商业深度研究系统的性能，同时具有更小的模型规模与更低的单次查询成本。为促进后续研究，我们完整公开了所有数据、模型与代码，包括新开发的基于MCP架构的深度研究智能体基础设施。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.19399) | [arXiv](https://arxiv.org/abs/2511.19399)



---

### 6. UltraFlux：面向多比例原生4K文本到图像生成的数据-模型协同设计方法

**原文标题：** UltraFlux: Data-Model Co-Design for High-quality Native 4K Text-to-Image Generation across Diverse Aspect Ratios

**摘要：**
扩散变换器近期在1K分辨率文本到图像生成领域取得显著进展，但本研究发现将其扩展至多比例原生4K生成时，会暴露出位置编码、VAE压缩与优化之间的强耦合失效模式。孤立解决任一因素均会导致质量显著损失。为此，我们提出数据-模型协同设计框架，引入基于Flux架构的扩散变换器UltraFlux，该模型在MultiAspect-4K-1M数据集上实现原生4K训练——这是一个包含100万张4K图像的多比例可控语料库，具备双语标注文本及丰富的视觉语言模型/图像质量评估元数据，支持分辨率与比例感知采样。在模型层面，UltraFlux集成四大创新：(i) 采用Resonance二维旋转位置编码与YaRN技术，实现训练窗口、频率及比例感知的4K位置编码；(ii) 提出简洁的非对抗性VAE后训练方案，提升4K重建保真度；(iii) 设计信噪比感知的Huber小波损失函数，重平衡时间步与频带间的梯度分布；(iv) 构建分阶段美学课程学习策略，将高美学监督集中于模型先验主导的高噪声阶段。这些组件共同构建出稳定且保持细节的4K扩散变换器，可泛化至宽屏、方形及竖屏等多种比例。在4096分辨率美学评估基准及多比例4K设置下，UltraFlux在保真度、美学质量与对齐度指标上持续超越主流开源基线模型，结合大型语言模型提示优化器后，性能达到或超越商用系统Seedream 4.0。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.18050) | [arXiv](https://arxiv.org/abs/2511.18050)



---

### 7. 视频内指令：作为生成控制信号的视觉标识

**原文标题：** In-Video Instructions: Visual Signals as Generative Control

**摘要：**
大规模视频生成模型近期展现出强大的视觉能力，能够基于当前观察中的逻辑与物理线索预测符合逻辑的后续帧。本研究探索如何通过解析嵌入视频帧的视觉信号作为控制指令，将这种能力应用于可控的图像-视频生成，我们将其称为"视频内指令"范式。与基于文本提示（其描述 inherently 具有全局性和粗略性）的控制方式不同，视频内指令通过叠加文字、箭头或轨迹等视觉元素，将用户引导直接编码至视觉域。这种方法通过为不同对象分配独立指令，实现了视觉主体与其预期动作之间明确、空间感知且无歧义的对应关系。在Veo 3.1、Kling 2.5和Wan 2.2三种前沿生成器上的大量实验表明，视频模型能够可靠解析并执行此类视觉嵌入指令，尤其在复杂多对象场景中表现突出。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.19401) | [arXiv](https://arxiv.org/abs/2511.19401)



---

### 8. 预算感知的工具使用实现高效智能体扩展

**原文标题：** Budget-Aware Tool-Use Enables Effective Agent Scaling

**摘要：**
扩展测试时计算能够提升大语言模型在不同任务中的表现，这一机制已被延伸至工具增强型智能体领域。对此类智能体而言，扩展不仅涉及基于标记的"思考"，还包括通过工具调用的"行动"。工具调用次数直接制约着智能体与外部环境的交互深度。然而我们发现，单纯增加工具调用预算并不能提升性能，因为智能体缺乏"预算感知"能力，会迅速触及性能天花板。为解决该问题，我们以网络搜索智能体为研究对象，探索如何在明确工具调用预算约束下实现有效扩展。我们首先提出预算追踪器——一种轻量级插件，可为智能体提供持续预算感知，实现简洁高效的扩展。进而开发BATS框架（预算感知测试时扩展），该高级框架利用预算感知动态调整其规划与验证策略，根据剩余资源决定是对潜在线索"深入挖掘"还是"转向探索"新路径。为系统分析成本-性能的扩展关系，我们构建了统一成本度量标准，统筹考量标记消耗与工具调用开销。本研究首次对预算约束型智能体展开系统性分析，表明预算感知方法能产生更优的扩展曲线并推动成本-性能帕累托边界前移。我们的工作为工具增强型智能体的扩展机制提供了更透明、更原则性的实证研究视角。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.17006) | [arXiv](https://arxiv.org/abs/2511.17006)



---

### 9. 视觉思维链：通过连续视觉标记提升视觉语言模型的感知与推理能力

**原文标题：** Chain-of-Visual-Thought: Teaching VLMs to See and Think Better with Continuous Visual Tokens

**摘要：**
视觉语言模型在语言空间的推理方面表现出色，但在需要密集视觉感知的任务（如空间推理和几何认知）中存在明显局限。这一局限源于当前视觉语言模型在捕捉空间维度上的密集视觉信息方面机制不足。我们提出视觉思维链框架，使视觉语言模型不仅能进行语言推理，还能通过连续视觉标记——编码丰富感知线索的紧凑潜在表示——进行视觉推理。该框架仅需约20个标记的有限预算，即可从轻量级视觉专家中提炼知识，捕捉包括二维外观、三维几何、空间布局和边缘结构在内的互补特性。训练过程中，搭载视觉思维链的视觉语言模型通过自回归预测这些视觉标记来重建密集监督信号（如深度、分割、边缘和DINO特征）。在推理阶段，模型直接在连续视觉标记空间中进行推理，在保持效率的同时可选择性解码密集预测以增强可解释性。在涵盖CV-Bench、MMVP、RealWorldQA、MMStar、WorldMedQA和HRBench等十余个多样化感知基准的评估中，将视觉思维链集成至Qwen2.5-VL和LLaVA等强视觉语言模型后，性能持续提升3%至16%，证明紧凑的连续视觉思维能够实现更精准、可验证且可解释的多模态智能。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.19418) | [arXiv](https://arxiv.org/abs/2511.19418)



---

### 10. HunyuanVideo 1.5技术报告

**原文标题：** HunyuanVideo 1.5 Technical Report

**摘要：**
本文提出HunyuanVideo 1.5——一个轻量级但功能强大的开源视频生成模型，仅需83亿参数即可实现顶尖的视觉质量与运动连贯性，支持在消费级GPU上高效推理。该成果基于多项核心技术：精细化的数据治理、采用选择性滑动分块注意力机制（SSTA）的先进DiT架构、通过字形感知文本编码增强的双语理解能力、渐进式预训练与后训练策略，以及高效视频超分网络。依托这些设计，我们构建了能够跨时长跨分辨率生成高质量文本到视频及图像到视频的统一框架。大量实验表明，这个紧凑而高效的模型在开源视频生成领域确立了全新标杆。通过公开代码与模型权重，我们为社区提供了高性能基础平台，显著降低视频创作与研究门槛，使先进视频生成技术惠及更广泛群体。所有开源资源均已发布于https://github.com/Tencent-Hunyuan/HunyuanVideo-1.5。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.18870) | [arXiv](https://arxiv.org/abs/2511.18870)



---

### 11. Pillar-0：放射学基础模型的新前沿

**原文标题：** Pillar-0: A New Frontier for Radiology Foundation Models

**摘要：**
放射学在现代医学中扮演着不可或缺的角色，然而影像数据量的增长速度已远超放射科医师队伍的增长规模。基础模型为辅助完成全链条放射学任务提供了可行路径，但现有医学模型仍存在明显局限：将容积式CT和MRI作为低保真度二维切片处理、丢弃关键灰度对比信息、缺乏反映真实临床场景的评估框架。我们提出Pillar-0放射学基础模型，该模型基于某大型学术中心的42,990例盆腹部CT、86,411例胸部CT、14,348例头部CT及11,543例乳腺MRI进行预训练，并同步开发了RATE框架——该框架利用大语言模型以接近完美的准确率提取366种放射学征象的结构化标签。在包含14,230例盆腹部CT、10,646例胸部CT、4,906例头部CT和1,585例乳腺MRI的内部测试集上，Pillar-0创造了新的性能标杆，平均AUROC分别达到86.4、88.0、90.1和82.9，较MedGemma（谷歌）、MedImageInsight（微软）、灵枢（阿里巴巴）及Merlin（斯坦福）提升7.8-15.8个AUROC点，在87.2%（319/366）的任务中位列最优。在斯坦福腹部CT数据集的外部验证中，Pillar-0同样超越所有基线模型（包括Merlin的80.6 AUROC，达到82.2 AUROC）。该模型还可拓展至预训练范围之外的任务，例如在肺癌长期风险预测中，其于NLST数据集上将当前最优模型Sybil的C指数提升3.0个点，并在MGH和CGMH数据集上分别实现5.9和1.9的泛化增益。在脑出血检测任务中，Pillar-0仅需使用次优基线模型1/20的数据量即可获得>95的AUROC。Pillar-0与RATE共同构建了开放且符合临床严苛要求的放射学系统基础，突破了以往因计算资源、数据获取及评估体系限制而难以实现的应用场景。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.17803) | [arXiv](https://arxiv.org/abs/2511.17803)



---

### 12. Plan-X：基于语义规划的教学视频生成框架

**原文标题：** Plan-X: Instruct Video Generation via Semantic Planning

**摘要：**
扩散变换器在视觉合成领域展现出卓越能力，但其在高层语义推理与长程规划方面仍存在明显不足。这种局限性常导致视觉伪影和用户指令失准，尤其在涉及复杂场景理解、人物-物体交互、多阶段动作及情境运动推理的场景中更为突出。为解决这些挑战，我们提出Plan-X框架，通过显式强化高层语义规划来指导视频生成过程。该框架核心是语义规划器——一个可学习的多模态语言模型，能够基于文本提示和视觉上下文对用户意图进行推理，并自回归生成基于文本的时空语义标记序列。这些语义标记与高层文本提示指导形成互补，作为随时间演进的结构化"语义草图"输入视频扩散模型，后者擅长合成高保真度的视觉细节。Plan-X有效融合了语言模型在多模态情境推理与规划方面的优势，以及扩散模型在逼真视频合成方面的特长。大量实验表明，我们的框架显著减少了视觉伪影，能够生成与多模态语境一致、符合细粒度指令的视频内容。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.17986) | [arXiv](https://arxiv.org/abs/2511.17986)



---

### 13. M3-Bench：多模态、多跳、多线程工具使用型多模态大语言模型智能体基准测试框架

**原文标题：** M3-Bench: Multi-Modal, Multi-Hop, Multi-Threaded Tool-Using MLLM Agent Benchmark

**摘要：**
我们提出M³-Bench——首个基于模型上下文协议评估多模态工具使用能力的基准测试体系。该基准针对需要视觉定位与文本推理、跨工具依赖关系以及跨步骤中间资源持久化的现实多跳多线程工作流。我们引入基于相似度的对齐方法，通过序列化工具调用、使用语句编码器嵌入函数签名，并执行相似度分桶的匈牙利匹配算法，最终获得可审计的一对一对应关系。在此对齐机制基础上，我们提出可解释的评估指标，将语义保真度与工作流一致性进行解耦分析。本基准涵盖28个服务器共231种工具，通过经过人工验证的执行器-评判器流水线提供标准化轨迹；另设包含四个大型语言模型的辅助评判组，专门评估终端任务完成度和信息锚定能力。对代表性前沿多模态大语言模型的评估结果表明，当前在多模态MCP工具使用方面仍存在明显差距，特别是在参数保真度和结构一致性方面，这凸显了需要开发能联合推理图像、文本与工具图结构的新方法。本基准的匿名代码库位于：https://github.com/EtaYang10th/Open-M3-Bench

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.17729) | [arXiv](https://arxiv.org/abs/2511.17729)



---

### 14. 多智能体深度研究：基于M-GRPO的多智能体系统训练方法

**原文标题：** Multi-Agent Deep Research: Training Multi-Agent Systems with M-GRPO

**摘要：**
多智能体系统在通用推理任务中表现优异，但专业领域训练的缺失制约了其准确性。现有训练方法为系统中所有智能体训练统一的大型语言模型，由于不同智能体底层数据分布的差异性，这种模式可能限制系统性能。因此，采用差异化大模型训练多智能体系统成为亟待解决的课题。然而该方法会引发诸多优化挑战：智能体运行频率各异，决策过程涉及可变子智能体调用，且智能体通常部署于独立服务器，导致端到端梯度流中断。为解决这些问题，我们提出M-GRPO——面向具有主智能体（规划器）与多子智能体（多轮工具执行器）的垂直多智能体系统的分层式组相对策略优化算法。M-GRPO通过计算主次智能体的组相对优势值保持分层信用分配，并引入轨迹对齐机制以应对可变子智能体调用生成固定尺寸批次。我们部署了去耦合训练管道，各智能体在独立服务器运行，通过共享存储交换最小化统计量，实现无需跨服务器反向传播的可扩展训练。在真实场景基准测试（如GAIA、XBench-DeepSearch和WebWalkerQA）中，M-GRPO持续优于单智能体GRPO及子智能体冻结的多智能体GRPO，展现出更优的稳定性和样本效率。这些结果表明：对齐异构轨迹与解耦专业智能体优化能有效增强工具增强型推理任务性能。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.13288) | [arXiv](https://arxiv.org/abs/2511.13288)



---

### 15. 超越选择题：可验证开放问答在鲁棒视觉语言强化微调中的应用

**原文标题：** Beyond Multiple Choice: Verifiable OpenQA for Robust Vision-Language RFT

**摘要：**
选择题问答（MCQA）长期以来作为评估现代多模态语言模型及进行强化微调（RFT）的主流范式。其受限的输出格式支持简化且确定性的自动验证机制。然而，我们发现选项设置可能泄露可被模型利用的线索，导致准确率指标无法真实反映模型能力，并会在RFT过程中助长显性或隐性的答案猜测行为。为此，我们提出ReVeL（基于大语言模型的重写验证）框架，将选择题转化为开放形式的问题，同时最大限度保持答案的可验证性。该框架根据问题类型对题目进行分类，并分别采用差异化的重写与验证方案。在RFT应用实践中，我们转换了2万个MCQA样本，采用GRPO算法对Qwen2.5-VL模型进行微调。实验表明：基于ReVeL-OpenQA训练的模型在选择题基准测试中保持原有准确率，同时将开放问答准确率提升约6个百分点，这证明其相较于基于MCQA的训练具有更优的数据效率和更鲁棒的奖励信号。在评估场景中，ReVeL还揭示了MCQA基准测试中最高达20个百分点的分数虚高现象（相对于开放问答），同时提升了评判准确率，并有效降低了成本与延迟。我们将公开相关代码与数据。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.17405) | [arXiv](https://arxiv.org/abs/2511.17405)



---

### 16. MIST：基于监督训练的互信息估计方法

**原文标题：** MIST: Mutual Information Via Supervised Training

**摘要：**
本文提出了一种完全数据驱动的互信息估计器设计方法。由于任何互信息估计器都是两个随机变量观测样本的函数，我们采用神经网络（MIST）对该函数进行参数化，并通过端到端训练来预测互信息值。训练过程基于包含62.5万个已知真实互信息值的合成联合分布元数据集。为处理可变样本量与维度，我们采用二维注意力机制确保输入样本的排列不变性。在不确定性量化方面，通过优化分位数回归损失使估计器能够近似互信息的抽样分布，而非仅提供单点估计。本研究方案与先前工作的根本区别在于采用完全经验化的路径，以理论普适性换取灵活性与效率。实证研究表明，学习得到的估计器在不同样本量与维度条件下均显著优于经典基线方法，包括在训练阶段未出现的联合分布上仍保持优越性能。基于分位数的置信区间校准良好，较基于自助法的置信区间更为可靠，且推理速度比现有神经基线方法快数个数量级。除直接获得的实证优势外，该框架可生成可训练、完全可微的估计器，能够嵌入更大型的学习流程。此外，利用互信息对可逆变换的不变性特性，可通过标准化流将元数据集适配至任意数据模态，从而为不同目标元分布实现灵活训练。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.18945) | [arXiv](https://arxiv.org/abs/2511.18945)



---

### 17. 图像自成奖赏：基于对抗奖励的图像生成强化学习

**原文标题：** The Image as Its Own Reward: Reinforcement Learning with Adversarial Reward for Image Generation

**摘要：**
可靠的奖励函数对于图像生成领域的强化学习至关重要。当前大多数强化学习方法依赖于预训练的偏好模型，这些模型通过输出标量奖励来近似人类偏好。然而，这类奖励往往难以准确捕捉人类感知，且容易遭受奖励破解——即更高的分数并不对应更好的图像质量。为此，我们提出Adv-GRPO框架，该框架采用对抗奖励机制，通过迭代更新奖励模型和生成器来解决这一问题。我们的奖励模型以参考图像作为正样本进行监督训练，能有效规避奖励破解问题。与通过KL正则化约束参数更新的方法不同，我们学习的奖励直接通过视觉输出指导生成器，从而产生更高质量的图像。现有奖励函数的优化虽能缓解奖励破解，但其固有偏差仍然存在：例如PickScore可能降低图像质量，而基于OCR的奖励常损害美学保真度。为此，我们创新性地将图像本身作为奖励，利用参考图像和视觉基础模型（如DINO）提供丰富的视觉奖励。这些密集的视觉信号（而非单一标量）在图像质量、美学价值和任务特定指标上实现了持续提升。最后，我们证明将参考样本与基础模型奖励相结合，可实现分布迁移和灵活的风格定制。在人类评估中，本方法在图像质量和美学维度分别以70.0%和72.4%的胜率超越Flow-GRPO与SD3。代码与模型均已开源。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.20256) | [arXiv](https://arxiv.org/abs/2511.20256)



---

### 18. 可控图层分解在可逆多层图像生成中的应用

**原文标题：** Controllable Layer Decomposition for Reversible Multi-Layer Image Generation

**摘要：**
本研究提出可控图层分解方法（CLD），实现栅格图像精细化可控的多层分离。在实际工作流程中，设计者通常先独立生成并编辑各个RGBA图层，再将其合成为最终栅格图像。然而这一过程具有不可逆性：图层一旦合成即无法进行层级编辑。现有方法多依赖于图像抠图与修复技术，但在可控性与分割精度方面仍存在局限。为解决这些问题，我们提出两个核心模块：图层分解扩散变换器（LD-DiT）通过解耦图像元素至独立图层实现精细化控制；多层条件适配器（MLCA）通过将目标图像信息注入多层标记来实现精准条件生成。为建立全面评估体系，我们构建了新基准数据集并设计了针对性评估指标。实验结果表明，CLD在分解质量与可控性方面均优于现有方法。此外，经CLD分离的图层可直接在PowerPoint等常用设计工具中进行编辑，彰显了该方法在实际创作流程中的实用价值与应用潜力。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.16249) | [arXiv](https://arxiv.org/abs/2511.16249)



---

### 19. MASS：面向视觉语言模型物理推理与理解的运动感知时空定位方法

**原文标题：** MASS: Motion-Aware Spatial-Temporal Grounding for Physics Reasoning and Comprehension in Vision-Language Models

**摘要：**
视觉语言模型在标准视频任务中表现良好，但在涉及运动动力学与空间交互的物理驱动推理方面存在明显局限，这削弱了其解析真实视频/AI生成内容及生成物理一致性内容的能力。本文提出一种创新方法，通过将物理世界上下文线索转化为符合VLM感知、理解与推理机制的可解释表征来解决这一缺陷。我们构建了MASS-Bench综合基准数据集，包含4,350个真实世界与AIGC视频及8,361个自由形式视频问答对，聚焦物理相关理解任务，并提供包含视觉检测、子片段定位及全序列实体三维运动追踪的精细标注。进一步提出MASS——一种模型无关的方法，通过基于深度的三维编码与视觉定位将时空信号注入VLM语言空间，并结合用于物体动态分析的运动追踪器。为增强跨模态对齐与推理能力，我们采用强化微调策略。实验与消融研究表明，优化后的VLM在物理推理与理解任务上分别以8.7%和6.0%的优势超越同类及更大规模基线模型，以及现有先进方法，达到与Gemini-2.5-Flash等闭源顶尖VLS相媲美的性能。这些结果充分验证了本方法的有效性。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.18373) | [arXiv](https://arxiv.org/abs/2511.18373)



---

### 20. 上采样万物：一种简单却难以超越的特征上采样基线方法

**原文标题：** Upsample Anything: A Simple and Hard to Beat Baseline for Feature Upsampling

**摘要：**
本文提出"上采样万物"——一种轻量级测试时优化框架，无需任何训练即可将低分辨率特征恢复为高分辨率像素级输出。尽管视觉基础模型在多样化下游任务中展现出强大的泛化能力，但其表征通常会被下采样14倍/16倍（如ViT），这限制了它们在像素级应用中的直接使用。现有特征上采样方法依赖于特定数据集的重新训练或繁重的隐式优化，制约了可扩展性与泛化能力。本方法通过简单的单图优化学习结合空间与范围信息的各向异性高斯核，有效衔接了高斯泼溅与联合双边上采样技术。该学习得到的核函数作为通用边缘感知算子，可跨架构与模态无缝迁移，实现特征、深度或概率图的精确高分辨率重建。该方法处理224x224图像仅需约0.419秒，在语义分割、深度估计以及深度图与概率图上采样任务中均达到最先进性能。项目页面：https://seominseok0429.github.io/Upsample-Anything/{https://seominseok0429.github.io/Upsample-Anything/}

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.16301) | [arXiv](https://arxiv.org/abs/2511.16301)



---

### 21. PRInTS：面向长视野信息搜索的奖励建模方法

**原文标题：** PRInTS: Reward Modeling for Long-Horizon Information Seeking

**摘要：**
信息搜索是智能代理的核心能力，要求其在长轨迹任务中持续收集工具生成的信息并进行推理。然而，这种多步骤信息搜索任务对于基于语言模型的智能代理仍具挑战性。虽然过程奖励模型（PRM）可通过在测试阶段对候选步骤排序来指导代理，但现有PRM专为二元判断的短程推理设计，既无法捕捉信息搜索步骤中工具交互、工具输出解析等丰富维度，也难以处理长视野任务中快速增长的上下文。为突破这些限制，我们提出PRInTS——具备双重能力的生成式过程奖励模型：（1）基于多维度步骤质量评估（如工具输出解析、工具调用信息量）的密集评分机制；（2）轨迹摘要技术，在压缩增长上下文的同时保留步骤评估的关键信息。通过在FRAMES、GAIA（1-3级）和WebWalkerQA（易-难分级）基准上对多种模型开展的广泛评估及消融实验表明：采用PRInTS的n选优采样策略能显著增强开源模型与专业代理的信息搜索能力，仅需较小规模主干代理即可达到或超越前沿模型性能，并优于其他强奖励建模基线方法。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.19314) | [arXiv](https://arxiv.org/abs/2511.19314)



---

### 22. AICC：更精细解析HTML，打造更优模型——基于模型解析器构建的7.3T人工智能就绪语料库

**原文标题：** AICC: Parse HTML Finer, Make Models Better -- A 7.3T AI-Ready Corpus Built by a Model-Based HTML Parser

**摘要：**
尽管网络数据质量对大型语言模型至关重要，但现有数据策展工作多集中于过滤与去重处理，将HTML文本提取视为固定的预处理环节。现有网络语料库普遍采用基于启发式规则的提取器（如Trafilatura），这类方法难以保持文档结构完整性，且经常破坏公式、代码、表格等结构化元素。我们提出假设：提升提取质量对下游任务性能的影响不亚于激进过滤策略。本文介绍MinerU-HTML创新提取流程，该方案将内容提取重构为由6亿参数语言模型解决的序列标注问题。与文本密度启发式方法不同，MinerU-HTML通过语义理解机制，采用两阶段格式化流程，在转换为Markdown前显式分类语义元素。关键优势在于其基于模型的方法具备内在扩展性，而启发式方法的改进路径有限。在包含7,887个标注网页的基准测试集MainWebBench上，MinerU-HTML的ROUGE-N F1值达到81.8%，显著优于Trafilatura的63.6%，并在结构化元素保留方面表现卓越（代码块90.9%，公式94.0%）。基于该技术，我们构建了AICC（人工智能就绪通用爬虫语料库），该多语言语料库源自两个Common Crawl快照，规模达7.3万亿词元。在严格控制预训练实验中，对经过相同过滤处理的AICC与Trafilatura提取的TfCC进行对比，使用AICC（620亿词元）训练的模型在13个基准测试中平均准确率达50.8%，较TfCC提升1.08个百分点，这为“提取质量显著影响模型能力”提供了直接证据。AICC在关键基准测试中也优于RefinedWeb和FineWeb。我们公开释放MainWebBench、MinerU-HTML和AICC资源，证明HTML提取是网络语料库构建中至关重要却常被低估的环节。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.16397) | [arXiv](https://arxiv.org/abs/2511.16397)



---

### 23. EvoVLA：自演进视觉-语言-动作模型

**原文标题：** EvoVLA: Self-Evolving Vision-Language-Action Model

**摘要：**
尽管视觉-语言-动作模型在零样本泛化和仿真到现实迁移方面取得进展，长周期机器人操作仍是其面临的重要挑战。现有VLA模型存在阶段幻觉问题——智能体利用粗糙的评估信号取巧完成多步任务，虽报告高进度却未真正完成任务。本文提出自监督框架EvoVLA，通过三项互补机制解决该问题：采用三元组对比学习与Gemini生成难负样本的阶段对齐奖励机制，可防止视觉捷径；基于相对位姿的物体探索机制，将好奇心驱动锚定在物体-夹具相对位姿而非原始像素；长周期记忆机制通过选择性上下文保持与门控融合，稳定长周期推演中的内在塑形。在包含三项多阶段任务的Discoverse-L长周期操作基准测试中，EvoVLA以69.2%的平均任务成功率超越最强基线方法（OpenVLA-OFT）10.2个百分点，样本效率提升1.5倍，并将阶段幻觉发生率从38.5%降至14.8%。在实体机器人上的实际部署显示，四项操作任务平均成功率达54.6%，较OpenVLA-OFT提升11个百分点，证明了有效的仿真到现实迁移能力与强泛化性能。代码与项目网站详见：https://github.com/AIGeeksGroup/EvoVLA 与 https://aigeeksgroup.github.io/EvoVLA

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.16166) | [arXiv](https://arxiv.org/abs/2511.16166)



---

### 24. 无数据流映射蒸馏

**原文标题：** Flow Map Distillation Without Data

**摘要：**
当前最先进的流模型虽能实现卓越生成质量，但需要缓慢的迭代采样过程。为加速采样，现有方法通常从预训练教师模型中蒸馏流映射，这一过程传统上依赖外部数据集的采样。我们认为这种数据依赖性会引发教师-数据失配的根本风险——静态数据集可能无法完整反映教师模型全部生成能力，甚至产生表征偏差。这促使我们重新审视数据依赖是否真是流映射蒸馏成功的必要条件。本研究探索了一种无数据替代方案，仅从先验分布中进行采样（该分布通过模型构建可确保与教师模型兼容），从而彻底规避失配风险。为验证该理念的可行性，我们提出了一个原则性框架：该框架既能预测教师模型的采样路径，又能主动修正自身误差累积以确保高保真度。我们的方法超越了所有基于数据的对比方案，以显著优势确立了新的技术标杆。具体而言，在基于SiT-XL/2+REPA的蒸馏实验中，本方法在ImageNet 256×256分辨率上达到1.45的FID指标，在512×512分辨率上达到1.49的FID指标，且均仅需1次采样步长。本研究希望为生成模型加速建立更稳健的范式，并推动无数据流映射蒸馏技术的更广泛采纳。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.19428) | [arXiv](https://arxiv.org/abs/2511.19428)



---

### 25. One4D：基于解耦LoRA控制的统一四维生成与重建框架

**原文标题：** One4D: Unified 4D Generation and Reconstruction via Decoupled LoRA Control

**摘要：**
本文提出One4D——一个统一的四维生成与重建框架，能够生成包含同步RGB帧与点云图的动态四维内容。通过统一掩码条件（UMC）机制对输入帧的不同稀疏度进行一致性处理，本框架可实现从单张图像的四维生成、完整视频的四维重建到稀疏帧混合生成与重建的无缝切换。我们基于强大的视频生成模型设计了专用网络架构，使其适配RGB与点云图的联合生成任务。传统基于扩散模型的深度图或点云图重建微调策略在联合生成任务中往往失效，会导致基础视频模型性能快速退化。为解决该问题，我们提出解耦LoRA控制（DLC）方法，通过两个模态特定的LoRA适配器构建RGB帧与点云图的解耦计算分支，并采用轻量级零初始化控制链接逐步学习像素级一致性。在适度计算资源下，通过合成与真实四维数据集的混合训练，One4D在生成与重建任务中均能产出高质量RGB帧与精确点云图。本工作标志着基于视频扩散模型实现通用高质量几何四维世界建模的重要进展。项目页面：https://mizhenxing.github.io/One4D

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.18922) | [arXiv](https://arxiv.org/abs/2511.18922)



---

### 26. Target-Bench：世界模型能否实现基于语义目标的无地图路径规划？

**原文标题：** Target-Bench: Can World Models Achieve Mapless Path Planning with Semantic Targets?

**摘要：**
尽管当前的世界模型能够生成高度逼真的视频，但其执行机器人路径规划的能力仍不明确且缺乏量化评估。我们推出Target-Bench——首个专门用于评估世界模型在真实环境中基于语义目标进行无地图路径规划的基准测试平台。该平台提供450段机器人采集的视频序列，涵盖45个语义类别，并附带基于SLAM技术的真实轨迹数据。我们的评估流程通过生成视频恢复相机运动，并采用五项互补指标来衡量规划性能，包括目标抵达能力、轨迹精度和方向一致性。我们对包括Sora 2、Veo 3.1及Wan系列在内的前沿模型进行评估。表现最佳的现成模型（Wan2.2-Flash）总体得分仅为0.299，揭示了当前世界模型在机器人规划任务中的显著局限性。实验表明，仅需使用本数据集中的325个场景对开源50亿参数模型进行微调，即可获得0.345的综合评分——较其基础版本（0.066）提升超400%，并优于最佳现成模型15%。我们将开源相关代码与数据集。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.17792) | [arXiv](https://arxiv.org/abs/2511.17792)



---

### 27. 大语言模型中真理的表征稳定性

**原文标题：** Representational Stability of Truth in Large Language Models

**摘要：**
大语言模型（LLMs）被广泛应用于事实性任务，例如"哮喘的治疗方法是什么？"或"拉脱维亚的首都是哪里？"。然而，目前尚不清楚LLMs在其内部概率表征中如何稳定地编码真实、虚假以及非真非假内容之间的区分。我们提出表征稳定性这一概念，用以衡量LLMs对真理操作定义扰动的真实性表征鲁棒性。我们通过以下方法评估表征稳定性：（i）基于LLMs的激活状态训练线性探针以区分真实与非真实陈述；（ii）在受控标签变化下测量其学习决策边界的偏移程度。通过使用16个开源模型和3个事实领域的激活数据，我们比较了两种非真非假陈述：第一种是关于我们确信不存在于任何训练数据中的实体的类事实断言，称之为陌生型非真陈述；第二种是来自知名虚构背景的非事实主张，称之为熟悉型非真陈述。实验表明，陌生型陈述引发了最大的边界偏移，在脆弱领域（如词汇定义）导致高达40%的真值判断翻转，而熟悉的虚构陈述则保持更连贯的聚类特征，仅产生较小变化（≤8.2%）。这些结果表明，表征稳定性更多源于认知熟悉度而非语言形式。从更广义的角度看，我们的研究方法为审计和训练LLMs提供了诊断工具，使其在语义不确定性下保持连贯的真值分配，而非仅仅优化输出准确性。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.19166) | [arXiv](https://arxiv.org/abs/2511.19166)



---

### 28. SyncMV4D：面向手物交互合成的外观与运动同步多视角联合扩散模型

**原文标题：** SyncMV4D: Synchronized Multi-view Joint Diffusion of Appearance and Motion for Hand-Object Interaction Synthesis

**摘要：**
手物交互生成在推动动画与机器人应用发展中具有关键作用。当前基于视频的方法主要局限于单视角，这阻碍了全面的三维几何感知，并常导致几何失真或非真实运动模式。虽然三维手物交互方法能够生成动力学合理的运动，但其对受控实验室环境下采集的高质量三维数据的依赖，严重限制了在真实场景中的泛化能力。为克服这些局限，我们提出SyncMV4D——首个通过统一视觉先验、运动动力学与多视角几何，联合生成同步多视角手物交互视频与四维运动的模型。我们的框架具有两大核心创新：(1) 多视角联合扩散模型，可协同生成手物交互视频与中间运动；(2) 扩散点云对齐器，能将粗粒度中间运动优化为全局对齐的四维度量点轨迹。为实现二维外观与四维动力学的紧密耦合，我们建立了闭环式相互增强循环：在扩散去噪过程中，生成视频为四维运动优化提供条件，而对齐后的四维点轨迹通过重投影指导下一步联合生成。实验表明，本方法在视觉真实感、运动合理性与多视角一致性方面均优于当前最先进方案。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.19319) | [arXiv](https://arxiv.org/abs/2511.19319)



---

### 29. 基于随机路径积分的保真推荐解释方法

**原文标题：** Fidelity-Aware Recommendation Explanations via Stochastic Path Integration

**摘要：**
解释保真度作为衡量解释准确反映模型真实推理过程的指标，在推荐系统领域仍存在严重的研究不足。我们提出SPINRec（神经推荐解释的随机路径积分方法），这是一种模型无关的解决方案，将路径积分技术适配于推荐数据稀疏性和隐式反馈的特性。为克服现有方法的局限性，SPINRec采用随机基线采样策略：通过从经验数据分布中抽取多个可能的用户画像并选择最具忠实度的归因路径，取代传统固定基线或不切实际的参照点。该设计能同时捕捉已观测和未观测交互的影响，生成更稳定且个性化的解释。我们在三种模型（矩阵分解、变分自编码器、神经协同过滤）、三个数据集（MovieLens 1M、雅虎音乐、Pinterest）及包含基于AUC的扰动曲线和定长诊断的反事实评估体系上开展了迄今最全面的保真度评估。实验表明SPINRec持续超越所有基线方法，为推荐系统的可信解释建立了新基准。代码与评估工具已开源：https://github.com/DeltaLabTLV/SPINRec。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.18047) | [arXiv](https://arxiv.org/abs/2511.18047)



---

### 30. 推荐系统中交互感知的单语义概念提取方法

**原文标题：** Extracting Interaction-Aware Monosemantic Concepts in Recommender Systems

**摘要：**
本文提出一种从推荐系统的用户与物品嵌入向量中提取单语义神经元的方法，该方法将潜在维度定义为与连贯可解释概念对齐的语义单元。我们采用稀疏自编码器（SAE）来揭示预训练表征中的语义结构。与语言模型研究不同，推荐系统中的单语义性需保持用户与物品嵌入向量间的交互关系。为此，我们引入了预测感知训练目标，通过冻结推荐模型进行反向传播，使学习到的潜在结构与用户-物品亲和度预测保持一致。最终获得的神经元能够捕捉类型、流行度、时序趋势等特征，并支持包括定向过滤和内容推广在内的后置控制操作，且无需修改基础模型。本方法适用于不同类型的推荐模型和数据集，为可解释与可控的个性化推荐提供了实用工具。代码与评估资源详见：https://github.com/DeltaLabTLV/Monosemanticity4Rec。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.18024) | [arXiv](https://arxiv.org/abs/2511.18024)



---

### 31. MSRNet：用于伪装目标检测的多尺度递归网络

**原文标题：** MSRNet: A Multi-Scale Recursive Network for Camouflaged Object Detection

**摘要：**
伪装目标检测是一项新兴且具有挑战性的计算机视觉任务，其目标在于识别并分割那些因颜色、纹理及尺寸高度相似而与环境融为一体的目标。该任务在弱光条件、部分遮挡、目标尺寸微小、复杂背景模式及多目标共存等场景下尤为困难。尽管已有诸多精密方法被提出，现有技术仍难以在复杂场景中精准检测伪装目标，特别是在处理微小及多目标情况时表现不佳，表明该领域仍存在提升空间。我们提出一种多尺度递归网络，通过金字塔视觉变换器主干网络提取多尺度特征，并借助专用基于注意力的尺度融合单元进行特征选择性融合。为实现更精准的目标检测，解码器通过多粒度融合单元递归优化特征。我们还开发了新颖的递归反馈解码策略以增强全局上下文理解，帮助模型克服本任务中的各项挑战。通过联合利用多尺度学习与递归特征优化，所提方法实现了性能提升，成功检测微小及多目标伪装物体。在伪装目标检测的两个基准数据集上，我们的模型取得了最先进的性能，并在另外两个数据集中位列第二。代码、模型权重及实验结果已公开于：https://github.com/linaagh98/MSRNet。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.12810) | [arXiv](https://arxiv.org/abs/2511.12810)



---


## 🔍 关键词云图
![关键词云图](../images/keywords_wordcloud.png)

## 📈 近期论文趋势
![论文趋势](../images/daily_papers.png)

## 🎙️ 语音播报
- [收听今日论文解读](../audio/2025-11-25_daily_papers.mp3)

## 📱 订阅渠道
- GitHub: [hf-daily-paper-newsletter-chinese](https://github.com/2404589803/hf-daily-paper-newsletter-chinese)