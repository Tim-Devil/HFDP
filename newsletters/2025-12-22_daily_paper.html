<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Hugging Face 论文日报 - 2025-12-22</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', sans-serif;
            line-height: 1.6;
            color: #333;
            background-color: #f5f5f5;
            padding: 20px;
        }
        
        .container {
            max-width: 900px;
            margin: 0 auto;
            background-color: white;
            padding: 40px;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }
        
        h1 {
            color: #2c3e50;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
            margin-bottom: 30px;
            font-size: 28px;
        }
        
        h1 img {
            vertical-align: middle;
            margin-right: 10px;
        }
        
        h2 {
            color: #34495e;
            margin-top: 40px;
            margin-bottom: 20px;
            font-size: 24px;
            border-left: 4px solid #3498db;
            padding-left: 15px;
        }
        
        h3 {
            color: #2c3e50;
            margin-top: 30px;
            margin-bottom: 15px;
            font-size: 20px;
        }
        
        p {
            margin-bottom: 15px;
            text-align: justify;
        }
        
        ul {
            margin-left: 20px;
            margin-bottom: 20px;
        }
        
        li {
            margin-bottom: 8px;
        }
        
        a {
            color: #3498db;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        strong {
            color: #2c3e50;
            font-weight: 600;
        }
        
        hr {
            border: none;
            border-top: 1px solid #e0e0e0;
            margin: 30px 0;
        }
        
        /* 关键修复:限制图片宽度 */
        img {
            max-width: 100%;
            height: auto;
            display: block;
            margin: 20px auto;
            border-radius: 4px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        
        /* 确保图片容器也有宽度限制 */
        p img {
            max-width: 100%;
        }
        
        /* 论文详情区域样式 */
        .paper-section {
            background-color: #f9f9f9;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 20px;
        }
        
        /* 统计信息样式 */
        .stats {
            background-color: #e8f4f8;
            padding: 15px;
            border-radius: 6px;
            margin-bottom: 20px;
        }
        
        /* 响应式设计 */
        @media (max-width: 768px) {
            .container {
                padding: 20px;
            }
            
            h1 {
                font-size: 24px;
            }
            
            h2 {
                font-size: 20px;
            }
            
            h3 {
                font-size: 18px;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <h1><img src="https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo.png" width="30"/> Hugging Face 2025-12-22 论文日报</h1>
<h2>📊 今日论文统计</h2>
<ul>
<li>总论文数：22</li>
<li>热门领域：LLM, GPT, RL, Transformer</li>
</ul>
<h2>📝 论文详情</h2>
<h3>1. 基于科学家对齐工作流程的大语言模型科学通用智能评估</h3>
<p><strong>原文标题：</strong> Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows</p>
<p><strong>摘要：</strong>
尽管科学人工智能领域取得进展，但科学通用智能（SGI）——即跨科学领域自主构思、探究与推理的能力——仍缺乏连贯的评估框架。本研究基于实践探究模型（PIM：审思、构思、行动、感知）提出可操作的SGI定义，并通过四项科学家对齐任务进行具象化：深度研究、创意生成、干/湿实验设计与实验推理。我们构建的SGI-Bench包含1000余个受《科学》杂志"125个重大科学问题"启发的跨学科专家评审样本，可系统评估前沿大语言模型。评估结果显示多重不足：深度研究任务中步骤对齐度虽高但精确匹配率仅10-20%；生成创意缺乏可行性与细节；干实验代码可执行率高但执行结果准确率低；湿实验协议序列保真度不足；多模态比较推理任务仍存在持续挑战。我们进一步提出推理时强化学习方法，通过在推理阶段优化检索增强的新颖性奖励，在无参考答案条件下提升假设生成的新颖度。本研究通过PIM理论框架、以工作流程为核心的评估体系及实证分析，为真正参与科学发现的人工智能系统奠定了理论基础。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.16969">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.16969">arXiv</a></p>
<hr />
<h3>2. PhysBrain：以人类自我中心数据为桥梁实现从视觉语言模型到物理智能的过渡</h3>
<p><strong>原文标题：</strong> PhysBrain: Human Egocentric Data as a Bridge from Vision Language Models to Physical Intelligence</p>
<p><strong>摘要：</strong>
机器人泛化能力依赖于物理智能，即在自我中心感知与行动下进行状态变化推理、密集接触交互以及长时程规划的能力。然而，当前大多数视觉语言模型主要基于第三人称数据进行训练，这导致其人形机器人应用存在根本性的视角错配问题。由于成本高昂与多样性有限，大规模采集机器人自我中心数据仍不现实；而大规模人类自我中心视频则提供了可扩展的替代方案，其天然蕴含丰富的交互语境与因果结构。核心挑战在于如何将原始自我中心视频转化为结构化、可靠的具身训练监督信号。为此，我们提出一种自我中心到具身转换流程，通过强化证据锚定与时间一致性机制，将第一人称视频转化为多层次、模式驱动的视觉问答监督数据，从而规模化构建自我中心到具身数据集（E2E-3M）。基于该数据集训练得到的自我中心感知具身大脑模型（命名为PhysBrain）展现出显著增强的自我中心理解能力，尤其在EgoThink任务中的规划表现突出。该模型提供的自我中心感知初始化权重，能够实现更高效的视觉语言动作模型微调，并在SimplerEnv环境中达到53.9%的成功率，有效验证了从人类自我中心监督到下游机器人控制的知识迁移机制。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.16793">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.16793">arXiv</a></p>
<hr />
<h3>3. 当推理遇见其定律</h3>
<p><strong>原文标题：</strong> When Reasoning Meets Its Laws</p>
<p><strong>摘要：</strong>
尽管大型推理模型（LRMs）表现出卓越的性能，但其推理行为往往违背直觉，导致推理能力未能达到最优。为从理论上形式化理想的推理行为，本文提出推理定律（LoRe）这一统一框架，用以刻画大型推理模型的内在推理模式。我们首先提出计算定律，其核心假设是推理计算量应与问题复杂度呈线性比例关系。除计算维度外，我们进一步通过补充的准确率定律扩展推理定律框架。由于问题复杂度在实践中难以量化，我们通过定律的两个可验证属性——单调性与组合性——来检验这些假设。为此，我们构建了LoRe-Bench基准测试集，系统化地衡量大型推理模型在这两个可度量属性上的表现。评估结果表明，大多数推理模型展现出合理的单调性，但缺乏组合性。针对此问题，我们开发了一种有效的微调方法，以强化计算定律的组合性。大量实证研究表明，更好地遵循计算定律能在多个基准测试中持续提升推理性能，并揭示不同属性与定律之间的协同效应。项目页面：https://lore-project.github.io/</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.17901">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.17901">arXiv</a></p>
<hr />
<h3>4. Seed-Prover 1.5：通过经验学习掌握本科水平定理证明</h3>
<p><strong>原文标题：</strong> Seed-Prover 1.5: Mastering Undergraduate-Level Theorem Proving via Learning from Experience</p>
<p><strong>摘要：</strong>
近期，大语言模型在生成严谨数学证明方面取得了显著进展。然而，利用大语言模型进行形式化语言（如Lean）中的定理证明仍面临挑战且计算成本高昂，尤其是在处理本科及以上难度的问题时。本研究提出了Seed-Prover 1.5，这是一个通过大规模智能体强化学习训练的形式化定理证明模型，并配套设计了高效的测试时扩展工作流程。该模型通过与Lean等工具的广泛交互，在强化学习过程中持续积累经验，显著提升了形式化定理证明的能力与效率。此外，结合自然语言证明领域的最新进展，我们的测试时扩展工作流程有效弥合了自然语言与形式化语言之间的鸿沟。与现有最优方法相比，Seed-Prover 1.5以更少的计算资源实现了更优的性能：其解决了PutnamBench（本科水平）88%的问题、Fate-H（研究生水平）80%的问题以及Fate-X（博士水平）33%的问题。尤为突出的是，使用本系统我们在9小时内解决了2025年普特南数学竞赛12道题目中的11道。我们的研究表明，在高质量形式化反馈驱动下扩展经验学习，对于形式化数学推理的未来发展具有巨大潜力。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.17260">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.17260">arXiv</a></p>
<hr />
<h3>5. 4D-RGPT：通过感知蒸馏实现区域级四维场景理解</h3>
<p><strong>原文标题：</strong> 4D-RGPT: Toward Region-level 4D Understanding via Perceptual Distillation</p>
<p><strong>摘要：</strong>
尽管多模态大语言模型（MLLMs）取得了进展，但其在三维结构与时间动态推理方面的能力仍受限于薄弱的四维感知与时间理解能力。现有的三维与四维视频问答（VQA）基准亦侧重于静态场景，且缺乏区域级提示机制。为应对这些问题，本文提出：（1）4D-RGPT——一种专为从视频输入中捕捉四维表征而设计的MLLM，具备增强的时间感知能力；（2）感知四维蒸馏（P4D）训练框架，通过将冻结专家模型的四维表征迁移至4D-RGPT，实现全面的四维感知；（3）R4D-Bench——一个基于混合自动化与人机协同验证流程构建的、支持区域级提示的深度感知动态场景基准测试集。实验表明，我们的4D-RGPT在现有四维VQA基准及新提出的R4D-Bench基准上均取得了显著性能提升。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.17012">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.17012">arXiv</a></p>
<hr />
<h3>6. 语义与重建并重：让表征编码器胜任文本到图像生成与编辑任务</h3>
<p><strong>原文标题：</strong> Both Semantics and Reconstruction Matter: Making Representation Encoders Ready for Text-to-Image Generation and Editing</p>
<p><strong>摘要：</strong>
现代潜在扩散模型（LDMs）通常在低层变分自编码器（VAE）潜在空间中运行，该空间主要针对像素级重建进行优化。为统一视觉生成与理解任务，新兴趋势是采用表征编码器提取的高维特征作为生成潜变量。然而，我们通过实验发现该范式存在两个根本性障碍：（1）判别性特征空间缺乏紧凑正则化，导致扩散模型易受流形外潜变量影响，从而生成错误的对象结构；（2）编码器固有的弱像素级重建能力阻碍生成器学习精确的细粒度几何与纹理特征。本文提出一个系统性框架，将面向理解任务的编码器特征适配于生成任务。我们引入语义-像素联合重建目标来正则化潜在空间，使语义信息与细粒度细节能同时压缩到高度紧凑的表征中（96通道且空间下采样16倍）。该设计确保潜在空间既保持语义丰富性，又实现最先进的图像重建性能，同时具备足够紧凑性以支持精确生成。基于此表征，我们设计了统一的文本到图像（T2I）与图像编辑模型。通过对多种特征空间的基准测试，我们证明该方法在重建质量上达到最优，具有更快的收敛速度，并在T2I与编辑任务中取得显著性能提升，验证了表征编码器可有效转化为鲁棒的生成组件。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.17909">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.17909">arXiv</a></p>
<hr />
<h3>7. 评估“大语言模型即评委”的方法是否走在正确道路上？</h3>
<p><strong>原文标题：</strong> Are We on the Right Way to Assessing LLM-as-a-Judge?</p>
<p><strong>摘要：</strong>
“大语言模型即评委”作为一种评估方法已被广泛采用，并在模型训练中充当监督奖励信号。然而，现有针对该模式的评测基准主要依赖人工标注的真实数据，这引入了人为偏见，不仅削弱了可靠性评估，还带来了可扩展性限制。为克服这些局限，我们提出了Sage——一个无需任何人工标注即可评估大语言模型评委质量的新型评测框架。受理性选择理论公理的启发，Sage引入两个全新评估维度：局部自一致性（成对偏好稳定性）与全局逻辑一致性（全偏好集的传递性）。我们通过整合结构化基准问题与现实用户查询，构建了包含650个问题的数据集。实验证明，我们的评估指标既具有稳定性，又与LLMBar、RewardBench2等监督式基准保持高度相关性，从而验证了Sage作为评估大语言模型评委鲁棒性与准确性的可靠性。基于Sage的评估，我们发现当前最先进的大语言模型在评分和成对比较场景中担任评委时均存在显著可靠性问题；即使是表现最优异的Gemini-2.5-Pro与GPT-5模型，在近四分之一复杂案例中仍无法保持偏好一致性。我们将此归因于一种称为“情境性偏好”的新现象，该现象解释了为何明确的评分标准或准则能帮助模型在不同答案对间保持判断一致性。进一步分析表明：微调大语言模型评委是提升性能的有效途径，而评审团机制与深度推理能增强判断一致性。我们还发现人类判断存在显著不一致性，这提示人工标注可能并非可靠的黄金标准。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.16041">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.16041">arXiv</a></p>
<hr />
<h3>8. 视觉-语言-动作模型剖析：从模块构成到发展里程碑与挑战</h3>
<p><strong>原文标题：</strong> An Anatomy of Vision-Language-Action Models: From Modules to Milestones and Challenges</p>
<p><strong>摘要：</strong>
视觉-语言-动作（VLA）模型正在推动机器人技术的革命，使机器能够理解指令并与物理世界进行交互。该领域正涌现大量新模型与数据集，在带来发展机遇的同时也使得跟踪研究进展面临挑战。本综述为VLA研究领域提供清晰的结构化指引。我们按照研究者的自然学习路径设计框架：从VLA模型的基础模块出发，梳理关键发展里程碑，进而深入探讨定义当前研究前沿的核心挑战。我们的主要贡献在于对五大挑战领域进行系统性剖析：（1）表征学习，（2）动作执行，（3）泛化能力，（4）安全可靠性，以及（5）数据集与评估体系。这一结构映射出通用智能体的发展路线图：建立感知-动作的基础闭环，在不同具身形式与环境中扩展能力，最终实现可信部署——所有这些都离不开数据基础设施的支撑。针对每个挑战领域，我们系统评述现有研究方法并指明未来机遇。本文既可作为新研究者的基础指南，亦可作为资深学者的战略路线图，旨在加速具身智能领域的学习进程并激发创新思路。本综述的动态更新版本持续维护于https://suyuz1.github.io/Survery/{项目页面}。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.11362">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.11362">arXiv</a></p>
<hr />
<h3>9. RadarGen：基于多视角摄像头的汽车雷达点云生成方法</h3>
<p><strong>原文标题：</strong> RadarGen: Automotive Radar Point Cloud Generation from Cameras</p>
<p><strong>摘要：</strong>
本文提出RadarGen——一种基于扩散模型的多视角摄像头图像到汽车雷达点云的生成方法。该方法通过将雷达测量数据编码为包含空间结构、雷达散射截面（RCS）和多普勒属性的鸟瞰图形式，将高效的图像潜在扩散模型适配至雷达领域。通过轻量级重建步骤，可从生成的特征图中恢复点云数据。为实现生成结果与视觉场景的精准对齐，RadarGen融合了从预训练基础模型中提取的鸟瞰图对齐深度、语义及运动线索，引导随机生成过程形成物理可信的雷达模式。基于图像的条件生成机制使本方法原则上兼容现有视觉数据集与仿真框架，为多模态生成式仿真提供了可扩展的技术路径。在大规模驾驶数据上的评估表明，RadarGen能够准确捕捉雷达测量的特征分布，并显著缩小基于生成数据与真实数据训练的感知模型性能差距，为实现跨传感模态的统一生成式仿真迈出了重要一步。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.17897">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.17897">arXiv</a></p>
<hr />
<h3>10. Robust-R1：面向鲁棒视觉理解的退化感知推理框架</h3>
<p><strong>原文标题：</strong> Robust-R1: Degradation-Aware Reasoning for Robust Visual Understanding</p>
<p><strong>摘要：</strong>
多模态大语言模型在极端现实世界视觉退化条件下难以保持可靠性能，这限制了其实际应用的鲁棒性。现有鲁棒性多模态大语言模型主要依赖仅关注视觉编码器泛化能力的隐式训练/适配方法，存在可解释性有限与优化孤立的问题。为突破这些局限，我们提出Robust-R1——一种通过结构化推理链显式建模视觉退化的新型框架。该框架整合了：（1）面向退化感知推理基础的监督微调；（2）用于精准感知退化参数的奖励驱动对齐机制；（3）适应退化强度的动态推理深度缩放策略。为支撑该方法，我们构建了一个包含11K样本的专用数据集，其通过四个关键现实世界视觉处理阶段合成具有真实感的退化数据，每个样本均标注了连接退化参数、感知影响、原始语义推理链与结论的结构化链条。综合实验表明本方法达到最先进的鲁棒性水平：Robust-R1在现实世界退化基准R-Bench上超越所有通用及鲁棒性基线模型，同时在MMMB、MMStar和RealWorldQA数据集的多强度对抗性退化测试中保持卓越的抗退化性能。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.17532">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.17532">arXiv</a></p>
<hr />
<h3>11. GroundingME：通过多维评估揭示多模态大语言模型中的视觉定位能力差距</h3>
<p><strong>原文标题：</strong> GroundingME: Exposing the Visual Grounding Gap in MLLMs through Multi-Dimensional Evaluation</p>
<p><strong>摘要：</strong>
视觉定位（即根据自然语言描述定位目标物体）是连接语言理解与视觉理解的关键桥梁。尽管多模态大语言模型（MLLMs）在现有基准测试中取得了令人瞩目的分数，但一个根本问题依然存在：MLLMs 是否真能像人类一样精细地将语言锚定于视觉信息，还是仅仅在简化数据集上进行模式匹配？现有基准测试未能捕捉现实世界的复杂性，而人类却能轻松处理模糊指代并识别何时无法进行视觉定位。为严格评估 MLLMs 的真实能力，我们提出了 GroundingME 基准测试，从四个关键维度系统性地挑战模型：（1）判别性——区分高度相似的物体；（2）空间性——理解复杂关系描述；（3）受限性——处理遮挡或微小物体；（4）拒识性——识别无法定位的查询。通过自动化生成与人工验证相结合的精心构建，我们创建了 1,005 个反映现实复杂性的挑战性样本。对 25 个前沿 MLLMs 的评估揭示了显著的能力差距：最佳模型准确率仅为 45.1%，而多数模型在拒识任务中得分为 0%，它们会反射性地幻觉出不存在物体而非承认其缺失，这为实际部署带来了重大安全隐患。我们探索了两种改进策略：（1）测试时扩展技术通过思维轨迹选择最优响应，将复杂场景定位准确率最高提升 2.9%；（2）混合数据训练使模型学会识别不可定位查询，将拒识准确率从 0% 提升至 27.9%。因此，GroundingME 既可作为揭示当前 MLLMs 局限性的诊断工具，也为实现人类水平的视觉定位能力提供了发展路线图。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.17495">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.17495">arXiv</a></p>
<hr />
<h3>12. 语言模型的物理学：第四部分第一节，架构设计与规范层的神奇之处</h3>
<p><strong>原文标题：</strong> Physics of Language Models: Part 4.1, Architecture Design and the Magic of Canon Layers</p>
<p><strong>摘要：</strong>
理解语言模型间的架构差异具有挑战性，尤其是在学术规模的预训练场景中（例如13亿参数、1000亿标记），其结果往往受到噪声和随机性的主导。为克服这一难题，我们引入了受控的合成预训练任务，以隔离并评估模型的核心能力。在此框架下，我们发现了<strong>规范层</strong>：一种轻量级的架构组件——其命名源自音乐术语“卡农”——能够促进相邻标记间的横向信息流动。规范层通过计算邻近标记表示的加权和，可无缝集成至Transformer、线性注意力、状态空间模型或任何序列架构中。</p>
<p>我们展示了12项关键结果。其中包括规范层如何提升推理深度（例如提升2倍）、拓展推理广度、增强知识处理能力等。它们能使弱架构（如NoPE）提升至与RoPE相当的水平，并使线性注意力模型达到与Mamba2/GDN等先进线性模型相媲美的性能——这些结论均通过合成任务和真实学术规模预训练得到验证。这一合成实验环境提供了一条经济且系统化的路径，以揭示在学术规模下常被掩盖的核心模型能力。借助无限的高质量数据，它甚至能够<strong>预测</strong>未来架构在训练流程改进（例如通过更优的数据筛选或基于强化学习的后训练）后的表现，从而解锁更深层次的推理与分层推断能力。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.17351">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.17351">arXiv</a></p>
<hr />
<h3>13. Turn-PPO：基于PPO的回合级优势估计以改进智能大语言模型中的多轮强化学习</h3>
<p><strong>原文标题：</strong> Turn-PPO: Turn-Level Advantage Estimation with PPO for Improved Multi-Turn RL in Agentic LLMs</p>
<p><strong>摘要：</strong>
强化学习已重新成为在现实环境中训练交互式大语言模型智能体的自然方法。然而，直接将广泛使用的组相对策略优化算法应用于多轮任务时，暴露出明显的局限性，尤其是在需要长程推理的场景中。为应对这些挑战，我们研究了更稳定、更有效的优势估计策略，特别是针对多轮交互设置。我们首先探索了近端策略优化作为替代方案，并发现其比GRPO更具鲁棒性。为了在多轮场景中进一步增强PPO，我们提出了turn-PPO，这是一种基于回合级马尔可夫决策过程建模的变体，而非常用的令牌级MDP。我们在WebShop和Sokoban数据集上的实验结果证明了turn-PPO的有效性，无论是否包含长推理组件。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.17008">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.17008">arXiv</a></p>
<hr />
<h3>14. HERBench：视频问答中多证据整合的基准测试框架</h3>
<p><strong>原文标题：</strong> HERBench: A Benchmark for Multi-Evidence Integration in Video Question Answering</p>
<p><strong>摘要：</strong>
视频大语言模型（Video-LLMs）正快速发展，然而当前的视频问答（VideoQA）基准测试常允许仅凭单一显著线索回答问题，未能充分检验模型对多个时间分散的视觉证据进行整合推理的能力。本文提出HERBench，这是一个专门用于评估跨时间多证据整合能力的VideoQA基准测试集。其中每个问题均要求整合至少三个来自不同视频片段的非重叠证据线索，因此仅依赖语言先验或单一视频快照均无法正确作答。HERBench包含2.6万个五选一多项选择题，划分为十二类组合式任务，涵盖身份绑定、跨实体关系、时序排序、共现验证及计数推理等维度。为量化证据需求，我们引入“最小必需帧集”（MRFS）指标，即模型必须融合的最小帧数才能正确回答问题。实验表明HERBench对证据整合的需求显著高于现有数据集（平均MRFS为5.5帧，对比基准数据集的2.6-4.2帧）。通过对13个前沿Video-LLMs的评估，发现普遍存在能力缺陷：模型准确率仅为31%-42%，略高于20%的随机猜测基线。我们将这种缺陷归因于两个关键瓶颈：（1）检索缺陷——帧选择器易遗漏关键证据；（2）融合缺陷——即使提供全部必要证据，模型仍无法有效整合信息。通过构建跨时间证据的强制性需求与可量化评估体系，HERBench为推进鲁棒性、组合式的视频理解研究确立了原则性目标。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.14870">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.14870">arXiv</a></p>
<hr />
<h3>15. 任意世界中的角色动画生成</h3>
<p><strong>原文标题：</strong> Animate Any Character in Any World</p>
<p><strong>摘要：</strong>
世界模型的最新进展显著提升了交互式环境模拟能力。现有方法主要分为两类：(1)静态世界生成模型——构建无主动智能体的三维环境；(2)可控实体模型——允许单个实体在不可控环境中执行有限动作。本研究提出AniX系统，在保持静态世界生成模型真实性与结构基础的同时，扩展可控实体模型以支持用户指定角色执行开放式动作。用户可提供3D高斯泼溅场景与角色模型，通过自然语言指令驱动角色完成从基础移动到以对象为中心的交互等多样化行为，并自由探索环境。AniX将任务构建为条件自回归视频生成问题，能够合成时间连贯且与输入场景、角色视觉保真度一致的视频片段。基于预训练视频生成器构建的训练策略在保持跨动作与角色泛化能力的同时，显著提升了运动动态表现。评估体系涵盖视觉质量、角色一致性、动作可控性及长时序连贯性等多维度指标。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.17796">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.17796">arXiv</a></p>
<hr />
<h3>16. SWE-Bench++：一种基于开源仓库的可扩展软件工程基准生成框架</h3>
<p><strong>原文标题：</strong> SWE-Bench++: A Framework for the Scalable Generation of Software Engineering Benchmarks from Open-Source Repositories</p>
<p><strong>摘要：</strong>
诸如SWE-bench等基准测试已在仓库级软件工程任务上实现了对大语言模型（LLM）评估的标准化。然而，现有工作仍受限于人工整理、静态数据集以及专注于基于Python的错误修复。本文提出SWE-Bench++，这是一个从开源GitHub项目中自动生成仓库级编码任务的框架。与合成方法不同，我们的流水线通过采集实时拉取请求，覆盖了11种编程语言的错误修复与功能需求两类任务。SWE-Bench++通过四个阶段将GitHub拉取请求转化为可复现的、基于执行的任务：程序化采集、环境合成、测试预言提取与质量保证。最后通过提示引导的轨迹合成步骤，将强模型未能解决的任务实例转化为训练轨迹。我们的初始基准包含来自3,971个仓库、涵盖11种语言的11,133个任务实例。在该基准的1,782个实例子集上，当前最强模型的表现如下：claude-sonnet-4.5达到36.20% pass@10，gpt-5-2025-08-07为34.57%，gemini/gemini-2.5-pro为24.92%，gpt-4o为16.89%。我们进一步通过实验证明，在SWE-Bench++实例上进行微调可在SWE-bench多语言基准上带来可测量的性能提升。SWE-Bench++为评估和改进仓库级代码生成提供了一个可扩展、多语言的基准测试框架。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.17419">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.17419">arXiv</a></p>
<hr />
<h3>17. Bolmo：字节化新一代语言模型</h3>
<p><strong>原文标题：</strong> Bolmo: Byteifying the Next Generation of Language Models</p>
<p><strong>摘要：</strong>
我们推出Bolmo，这是首个在10亿和70亿参数规模上具有竞争力的全开放字节级语言模型系列。与先前主要关注从头训练的字节级语言模型研究不同，我们通过对现有子词级语言模型进行字节化来训练Bolmo。字节化能够克服子词分词的局限性——例如对字符理解不足以及固定子词词汇表导致的效率限制——同时达到领先子词级语言模型的性能水平。Bolmo专为字节化设计：我们的架构解决了先前字节级架构与子词级语言模型在表达能力上的不匹配问题，从而能够在Bolmo与源子词模型之间采用有效的精确蒸馏目标。这使得仅需投入不到典型预训练标记预算的1%，即可将子词级语言模型转换为字节级语言模型。Bolmo在性能上显著超越所有同类规模的先前字节级语言模型，并在字符理解任务上优于源子词级语言模型，在某些编码任务中也表现更佳，同时在其他任务上接近原始语言模型的性能。此外，我们证明Bolmo通过采用更高的标记压缩比进行训练，能够实现与子词级语言模型相竞争的推理速度，并且可以借助围绕源子词级语言模型的现有生态系统进行低成本、高效的后续训练。我们的研究成果最终使字节级语言模型成为广泛用例中与子词级语言模型相竞争的实用选择。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.15586">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.15586">arXiv</a></p>
<hr />
<h3>18. StageVAR：面向视觉自回归模型的分阶段感知加速方法</h3>
<p><strong>原文标题：</strong> StageVAR: Stage-Aware Acceleration for Visual Autoregressive Models</p>
<p><strong>摘要：</strong>
视觉自回归模型通过“下一尺度预测”突破了传统自回归模型“下一标记预测”的范式，实现了高质量图像生成。然而，该范式在大规模生成步骤中面临计算复杂度与运行时间急剧增加的问题。现有加速方法虽能减少大规模步骤的运行时间，但依赖人工步骤选择，且忽视了生成过程中不同阶段的重要性差异。为应对这一挑战，本文提出StageVAR——一个针对视觉自回归模型的系统性研究与分阶段感知加速框架。我们的分析表明：早期步骤对保持语义与结构一致性至关重要，应完整保留；而后期步骤主要进行细节优化，可通过剪枝或近似计算实现加速。基于此发现，StageVAR提出一种即插即用的加速策略，利用后期计算中的语义无关性与低秩特性实现加速，且无需额外训练。实验表明，StageVAR在GenEval基准上仅损失0.01分、在DPG基准上仅降低0.26分的情况下，最高可实现3.4倍加速效果，性能持续优于现有加速基线。这些结果证明，分阶段感知设计是实现高效视觉自回归图像生成的有效原则。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.16483">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.16483">arXiv</a></p>
<hr />
<h3>19. 元强化学习在语言智能体中引导探索行为</h3>
<p><strong>原文标题：</strong> Meta-RL Induces Exploration in Language Agents</p>
<p><strong>摘要：</strong>
强化学习（RL）已能够训练大型语言模型（LLM）智能体与环境交互，以解决多轮次长周期任务。然而，经强化学习训练的智能体在需要主动探索的任务中往往表现不佳，且难以从试错经验中高效适应。本文提出LaMer——一个通用的元强化学习框架，使LLM智能体能够在测试阶段主动探索并从环境反馈中学习。LaMer包含两个关键组成部分：（一）跨回合训练框架，以鼓励探索并优化长期奖励；（二）通过反思实现上下文策略自适应，使智能体无需梯度更新即可根据任务反馈信号调整策略。在多样化环境中的实验表明，LaMer性能显著优于强化学习基线方法，在推箱子、扫雷和网络购物任务中分别实现了11%、14%和19%的性能提升。此外，与强化学习训练的智能体相比，LaMer在更具挑战性或先前未见任务上也表现出更好的泛化能力。总体而言，我们的研究结果表明，元强化学习为语言智能体提供了一种引导探索行为的理论方法，使其能够通过习得的探索策略更稳健地适应新环境。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.16848">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.16848">arXiv</a></p>
<hr />
<h3>20. 3D-RE-GEN：基于生成框架的室内场景三维重建</h3>
<p><strong>原文标题：</strong> 3D-RE-GEN: 3D Reconstruction of Indoor Scenes with a Generative Framework</p>
<p><strong>摘要：</strong>
当前三维场景生成技术虽能产生视觉效果出色的成果，但其现有表征形式难以满足视觉特效与游戏开发领域艺术家对可编辑纹理化三维网格场景的工作流程需求。尽管相关技术已取得显著进展，现有纹理网格场景重建方法仍存在物体分解错误、空间关系不准确及背景缺失等问题，远未达到艺术创作实用标准。本文提出3D-RE-GEN——一种能够将单张图像重建为纹理化三维物体与背景的组合式框架。研究表明，通过整合特定领域的前沿模型，本框架在满足艺术创作需求的同时实现了当前最优的场景重建性能。</p>
<p>我们的重建管线集成了资产检测、重建与布局模型，并将部分模型的应用范围拓展至原设计领域之外。针对被遮挡物体的重建问题，我们将其转化为生成式模型驱动的图像编辑任务，通过在一致光照与几何约束下进行场景级推理来实现物体推断与重建。与现有方法不同，3D-RE-GEN能够生成完整的背景环境，在优化过程中为物体提供空间约束，并为视觉特效与游戏中的真实光照模拟任务奠定基础。为获得物理合理的场景布局，我们提出了一种新颖的四自由度可微分优化方法，将重建物体与估计的地平面进行精确对齐。通过精确相机恢复与空间优化指导的组合式生成，3D-RE-GEN在单图像三维场景重建中实现了当前最优性能，能够生成具有空间一致性与可编辑性的完整场景。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.17459">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.17459">arXiv</a></p>
<hr />
<h3>21. 面向长视频全模态推理与工具使用的基准与智能体框架</h3>
<p><strong>原文标题：</strong> A Benchmark and Agentic Framework for Omni-Modal Reasoning and Tool Use in Long Videos</p>
<p><strong>摘要：</strong>
长模态视频理解需要融合视觉、语音与环境音频信息，并进行连贯的长程推理。现有基准往往侧重时序长度或多模态丰富性中的单一维度，鲜有兼顾两者；尽管部分基准引入了开放式问题与高级评估指标，但仍主要依赖单一分数准确率，难以揭示模型失效的具体模式。本文提出LongShOTBench——一个包含开放式意图驱动问题、单轮/多轮对话，以及需要跨视频、音频和语音进行多模态推理与智能体工具使用的诊断性基准。每个评估项均配有参考答案和分级评分标准，支持可解释、可追溯的评估。该基准通过可扩展的人工验证流程构建，确保覆盖度与可复现性，所有样本均经过人工核验与修正。此外，我们提出LongShOTAgent智能体系统，通过预处理、检索和迭代优化实现长视频分析。在LongShOTBench上的实验表明，当前先进多模态大语言模型存在显著差距：Gemini-2.5-Flash仅达到52.95%，开源模型普遍低于30%，而LongShOTAgent获得44.66%的得分。这些结果凸显了现实场景中长模态视频理解的挑战性。LongShOTBench为评估和改进多模态大语言模型提供了实用、可复现的基础框架。所有资源已在GitHub开源：https://github.com/mbzuai-oryx/longshot。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.16978">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.16978">arXiv</a></p>
<hr />
<h3>22. MineTheGap：文本到图像模型中偏见的自动挖掘</h3>
<p><strong>原文标题：</strong> MineTheGap: Automatic Mining of Biases in Text-to-Image Models</p>
<p><strong>摘要：</strong>
文本到图像模型根据文本提示生成图像，而文本提示往往对期望图像的某些方面存在模糊性。面对这些模糊性时，文本到图像模型已显示出在解释过程中存在偏见。这些偏见可能产生社会影响，例如在描述特定职业时仅呈现某一特定种族。当一组生成图像内部出现冗余而非覆盖多样可能性时，这些偏见也会影响用户体验。本文提出MineTheGap——一种自动挖掘导致文本到图像模型产生偏见输出的提示词方法。我们的方法不仅限于检测给定提示词的偏见，更通过遗传算法迭代优化提示词池，寻找能够暴露偏见的提示词。这一优化过程由新颖的偏见评分驱动，该评分根据偏见严重程度进行排序（我们在已知偏见数据集上验证了其有效性）。对于给定提示词，该评分通过比较生成图像的分布与基于提示词变体生成的大语言模型文本分布而获得。代码和示例详见项目网页。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.13427">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.13427">arXiv</a></p>
<hr />
<h2>🔍 关键词云图</h2>
<p><img alt="关键词云图" src="../images/keywords_wordcloud.png" /></p>
<h2>📈 近期论文趋势</h2>
<p><img alt="论文趋势" src="../images/daily_papers.png" /></p>
<h2>🎙️ 语音播报</h2>
<ul>
<li><a href="../audio/2025-12-22_daily_papers.mp3">收听今日论文解读</a></li>
</ul>
<h2>📱 订阅渠道</h2>
<ul>
<li>GitHub: <a href="https://github.com/2404589803/hf-daily-paper-newsletter-chinese">hf-daily-paper-newsletter-chinese</a></li>
</ul>
    </div>
</body>
</html>