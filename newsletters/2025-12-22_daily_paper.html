<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Hugging Face 论文日报 - 2025-12-22</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', sans-serif;
            line-height: 1.6;
            color: #333;
            background-color: #f5f5f5;
            padding: 20px;
        }
        
        .container {
            max-width: 900px;
            margin: 0 auto;
            background-color: white;
            padding: 40px;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }
        
        h1 {
            color: #2c3e50;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
            margin-bottom: 30px;
            font-size: 28px;
        }
        
        h1 img {
            vertical-align: middle;
            margin-right: 10px;
        }
        
        h2 {
            color: #34495e;
            margin-top: 40px;
            margin-bottom: 20px;
            font-size: 24px;
            border-left: 4px solid #3498db;
            padding-left: 15px;
        }
        
        h3 {
            color: #2c3e50;
            margin-top: 30px;
            margin-bottom: 15px;
            font-size: 20px;
        }
        
        p {
            margin-bottom: 15px;
            text-align: justify;
        }
        
        ul {
            margin-left: 20px;
            margin-bottom: 20px;
        }
        
        li {
            margin-bottom: 8px;
        }
        
        a {
            color: #3498db;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        strong {
            color: #2c3e50;
            font-weight: 600;
        }
        
        hr {
            border: none;
            border-top: 1px solid #e0e0e0;
            margin: 30px 0;
        }
        
        /* 关键修复:限制图片宽度 */
        img {
            max-width: 100%;
            height: auto;
            display: block;
            margin: 20px auto;
            border-radius: 4px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        
        /* 确保图片容器也有宽度限制 */
        p img {
            max-width: 100%;
        }
        
        /* 论文详情区域样式 */
        .paper-section {
            background-color: #f9f9f9;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 20px;
        }
        
        /* 统计信息样式 */
        .stats {
            background-color: #e8f4f8;
            padding: 15px;
            border-radius: 6px;
            margin-bottom: 20px;
        }
        
        /* 响应式设计 */
        @media (max-width: 768px) {
            .container {
                padding: 20px;
            }
            
            h1 {
                font-size: 24px;
            }
            
            h2 {
                font-size: 20px;
            }
            
            h3 {
                font-size: 18px;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <h1><img src="https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo.png" width="30"/> Hugging Face 2025-12-22 论文日报</h1>
<h2>📊 今日论文统计</h2>
<ul>
<li>总论文数：22</li>
<li>热门领域：LLM, GPT, RL, Transformer</li>
</ul>
<h2>📝 论文详情</h2>
<h3>1. 基于科学家对齐工作流程的大语言模型科学通用智能测评</h3>
<p><strong>原文标题：</strong> Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows</p>
<p><strong>摘要：</strong>
尽管科学人工智能领域取得进展，但科学通用智能（SGI）——即跨科学领域自主构思、探究与推理的能力——仍缺乏系统化框架。本研究基于实践探究模型（PIM：审思、构思、行动、感知）提出可操作的SGI定义，并通过四项科学家对齐任务进行具象化：深度研究、创意生成、干/湿实验设计与实验推理。我们构建的SGI-Bench基准涵盖千余个受《科学》杂志"125个重大科学问题"启发的跨学科专家标注样本，用于系统评估前沿大语言模型。研究发现显著差距：深度研究任务中步骤对齐度虽高但精确匹配率仅10-20%；生成创意缺乏可行性与细节；干实验代码可执行性高但执行结果准确率低；湿实验协议序列保真度不足；多模态比较推理任务仍存在持续挑战。我们进一步提出测试时强化学习方法，通过在推理阶段优化检索增强的新颖性奖励，在无需参考答案的情况下提升假设生成的新颖度。本研究通过PIM理论框架、以工作流程为核心的测评体系及实证分析，为真正参与科学发现的人工智能系统奠定了理论基础。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.16969">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.16969">arXiv</a></p>
<hr />
<h3>2. PhysBrain：以人类第一人称数据为桥梁，从视觉语言模型迈向物理智能</h3>
<p><strong>原文标题：</strong> PhysBrain: Human Egocentric Data as a Bridge from Vision Language Models to Physical Intelligence</p>
<p><strong>摘要：</strong>
机器人泛化能力依赖于物理智能，即在第一人称感知与行动下，对状态变化、密集接触交互以及长时程规划进行推理的能力。然而，大多数视觉语言模型主要基于第三人称数据进行训练，这导致了与人形机器人之间存在根本性的视角错配。由于成本高昂且多样性有限，扩大机器人第一人称数据采集仍不切实际，而大规模人类第一人称视频则提供了一种可扩展的替代方案，能够自然地捕捉丰富的交互情境与因果结构。核心挑战在于如何将原始的第一人称视频转化为结构化、可靠的具身训练监督信号。为此，我们提出了一种第一人称到具身转换流程，该流程将第一人称视频转化为多层次、模式驱动的视觉问答监督，并强化证据 grounding 与时间一致性，从而实现了大规模第一人称到具身数据集（E2E-3M）的构建。通过在该数据集上进行训练，我们得到了一个具有第一人称感知能力的具身大脑，称为 PhysBrain。PhysBrain 展现出显著提升的第一人称理解能力，尤其在 EgoThink 任务上的规划表现突出。它提供了一个具备第一人称感知的初始化模型，能够实现更高效的视觉语言-动作模型微调，并在 SimplerEnv 上取得了更高的成功率（53.9%），这有效证明了从人类第一人称监督到下游机器人控制的知识迁移。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.16793">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.16793">arXiv</a></p>
<hr />
<h3>3. 当推理遇见其定律</h3>
<p><strong>原文标题：</strong> When Reasoning Meets Its Laws</p>
<p><strong>摘要：</strong>
尽管大型推理模型（LRMs）展现出卓越的性能，但其推理行为往往与直觉相悖，导致推理能力未能达到最优。为从理论上形式化理想的推理行为，本文提出了推理定律（LoRe），这是一个用于刻画大型推理模型中内在推理模式的统一框架。我们首先提出计算定律，其核心假设是推理计算量应与问题复杂度呈线性比例关系。除计算量外，我们进一步通过补充的准确率定律扩展了LoRe框架。由于问题复杂度在实践中难以量化，我们通过定律的两个可检验属性——单调性与组合性——来验证这些假设。为此，我们构建了LoRe-Bench基准测试集，用于系统性地评估大型推理模型在这两个可处理属性上的表现。评估结果表明，大多数推理模型展现出合理的单调性，但缺乏组合性。针对此问题，我们开发了一种有效的微调方法，以强化计算定律的组合性。大量实证研究表明，更好地遵循计算定律能够在多个基准测试中持续提升推理性能，并揭示了不同属性与定律之间的协同效应。项目页面：https://lore-project.github.io/</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.17901">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.17901">arXiv</a></p>
<hr />
<h3>4. Seed-Prover 1.5：通过经验学习掌握本科水平定理证明</h3>
<p><strong>原文标题：</strong> Seed-Prover 1.5: Mastering Undergraduate-Level Theorem Proving via Learning from Experience</p>
<p><strong>摘要：</strong>
近期，大语言模型在生成严谨数学证明方面取得了显著进展。然而，利用大语言模型进行形式化语言（如Lean）的定理证明仍具挑战性且计算成本高昂，尤其是在处理本科及以上难度的问题时。本研究提出Seed-Prover 1.5模型，这是一个通过大规模智能体强化学习训练的形式化定理证明模型，并配套高效测试时扩展工作流。该模型通过与Lean等工具的持续交互，在强化学习过程中不断积累经验，显著提升了形式化定理证明的能力与效率。此外，结合自然语言证明领域的最新进展，我们的测试时扩展工作流有效弥合了自然语言与形式化语言之间的鸿沟。与现有最优方法相比，Seed-Prover 1.5以更少的计算资源实现了更优性能：其解决了PutnamBench（本科水平）88%的问题、Fate-H（研究生水平）80%的问题以及Fate-X（博士水平）33%的问题。尤为突出的是，基于本系统，我们在9小时内解决了2025年普特南数学竞赛12道题目中的11道。我们的研究表明，在高质量形式化反馈驱动下扩展经验学习，对于形式化数学推理的未来发展具有巨大潜力。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.17260">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.17260">arXiv</a></p>
<hr />
<h3>5. 4D-RGPT：基于感知蒸馏的区域级四维理解方法</h3>
<p><strong>原文标题：</strong> 4D-RGPT: Toward Region-level 4D Understanding via Perceptual Distillation</p>
<p><strong>摘要：</strong>
尽管多模态大语言模型（MLLMs）已取得显著进展，但其在三维结构与时间动态推理方面的能力仍受限于薄弱的四维感知与时间理解。现有三维与四维视频问答（VQA）基准亦侧重于静态场景，且缺乏区域级提示机制。针对这些问题，本研究提出：（a）4D-RGPT——一种专为从视频输入中捕捉四维表征而设计的MLLM，其具备增强的时间感知能力；（b）感知四维蒸馏（P4D）训练框架，通过将冻结专家模型的四维表征迁移至4D-RGPT，实现全面的四维感知；（c）R4D-Bench——一个融合深度感知动态场景与区域级提示的基准数据集，该数据集通过自动化与人工校验相结合的流程构建。实验表明，我们的4D-RGPT模型在现有四维VQA基准及新提出的R4D-Bench基准上均取得显著性能提升。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.17012">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.17012">arXiv</a></p>
<hr />
<h3>6. 语义与重建并重：为文本到图像生成与编辑任务优化表征编码器</h3>
<p><strong>原文标题：</strong> Both Semantics and Reconstruction Matter: Making Representation Encoders Ready for Text-to-Image Generation and Editing</p>
<p><strong>摘要：</strong>
现代潜在扩散模型通常运行在低层变分自编码器潜在空间中，该空间主要针对像素级重建进行优化。为统一视觉生成与理解任务，新兴趋势是采用表征编码器的高维特征作为生成潜变量。然而，我们通过实证研究发现该范式存在两个根本性障碍：（1）判别性特征空间缺乏紧凑正则化，导致扩散模型易受流形外潜变量影响，从而产生不准确的物体结构；（2）编码器固有的弱像素级重建能力阻碍生成器学习精确的细粒度几何与纹理特征。本文提出系统性框架，将面向理解任务的编码器特征适配至生成任务。我们引入语义-像素联合重建目标以正则化潜在空间，使语义信息与细粒度细节能同时压缩至高度紧凑的表征（96通道且空间下采样16倍）。该设计确保潜在空间既保持语义丰富性，又实现最先进的图像重建效果，同时维持足够紧凑性以支持精确生成。基于此表征，我们设计了统一的文本到图像生成与图像编辑模型。通过对多种特征空间的基准测试，我们证明该方法在重建质量上达到最优水平，具有更快的收敛速度，并在文本到图像生成与编辑任务中取得显著性能提升，验证了表征编码器可有效适配为鲁棒的生成组件。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.17909">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.17909">arXiv</a></p>
<hr />
<h3>7. LLM-as-a-Judge评估方法是否走在正确的道路上？</h3>
<p><strong>原文标题：</strong> Are We on the Right Way to Assessing LLM-as-a-Judge?</p>
<p><strong>摘要：</strong>
LLM-as-a-Judge作为一种评估方法已被广泛采用，并在模型训练中作为监督奖励信号使用。然而，现有的LLM-as-a-Judge基准主要依赖于人工标注的真实标签，这引入了人为偏见，不仅削弱了可靠性评估，还带来了可扩展性限制。为克服这些局限，我们提出了Sage——一个无需任何人工标注即可评估LLM评判者质量的新型评估套件。受理性选择理论公理的启发，Sage引入了两个衡量LLM-as-a-Judge的新维度：局部自一致性（成对偏好稳定性）与全局逻辑一致性（全偏好集合的传递性）。我们通过结合结构化基准问题与现实用户查询，构建了一个包含650个问题的数据集。实验结果表明，我们的评估指标不仅具有稳定性，还与LLMBar、RewardBench2等监督基准呈现高度相关性，证实了Sage作为评估LLM-as-a-Judge稳健性与准确性的可靠工具。基于Sage评估，我们发现当前最先进的LLM在作为评分型或成对比较型评判者时均存在显著的可靠性问题；即使是表现最优的模型Gemini-2.5-Pro与GPT-5，在近四分之一的困难案例中仍无法保持一致的偏好判断。我们将此归因于一种称为“情境偏好”的新现象，该现象解释了为何明确的评分规则或标准能帮助模型在不同答案对之间保持评判一致性。进一步分析表明，基于微调的LLM-as-a-Judge是提升性能的有效方法，而评审团机制与深度推理能增强评判一致性。我们还发现人类评判存在显著的不一致现象，这表明人工标注可能并非可靠的黄金标准。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.16041">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.16041">arXiv</a></p>
<hr />
<h3>8. 视觉-语言-动作模型剖析：从模块构成到发展里程碑与挑战</h3>
<p><strong>原文标题：</strong> An Anatomy of Vision-Language-Action Models: From Modules to Milestones and Challenges</p>
<p><strong>摘要：</strong>
视觉-语言-动作模型正在推动机器人技术领域的革命，使机器能够理解指令并与物理世界进行交互。该领域正涌现大量新模型与数据集，在令人振奋的同时也带来了紧跟研究进展的挑战。本综述为视觉-语言-动作研究领域提供了一份清晰而结构化的指南。我们按照研究者的自然学习路径设计内容框架：从基础模型模块解析入手，追溯关键发展里程碑，继而深入探讨界定当前研究前沿的核心挑战。我们的主要贡献在于对五大挑战领域进行系统性剖析：（1）表征学习，（2）动作执行，（3）泛化能力，（4）安全可靠性，以及（5）数据集与评估体系。这一结构映射出通用智能体的发展路线图：建立感知-动作的基础闭环，在不同具身形态与环境中扩展能力，最终实现可信赖的部署——所有这些都离不开核心数据基础设施的支撑。针对每个挑战领域，我们系统评述现有研究方法并指明未来发展方向。本文既可作为新研究者的基础指南，亦可作为经验丰富学者的战略路线图，兼具加速具身智能领域知识传播与激发创新研究的双重目标。本综述的动态更新版本持续维护于项目页面：https://suyuz1.github.io/Survery/{project_page}。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.11362">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.11362">arXiv</a></p>
<hr />
<h3>9. RadarGen：基于多视角摄像头的汽车雷达点云生成方法</h3>
<p><strong>原文标题：</strong> RadarGen: Automotive Radar Point Cloud Generation from Cameras</p>
<p><strong>摘要：</strong>
本文提出RadarGen——一种基于扩散模型、可从多视角摄像头图像合成逼真汽车雷达点云的方法。该方法通过将雷达测量值编码为包含空间结构、雷达散射截面（RCS）与多普勒属性的鸟瞰图形式，将高效的图像潜空间扩散技术适配至雷达领域，并通过轻量级重建步骤从生成的特征图中恢复点云。为实现生成结果与视觉场景的精准对齐，RadarGen融合了从预训练基础模型中提取的鸟瞰图对齐深度、语义及运动线索，引导随机生成过程形成物理可信的雷达模式。该方法的图像条件生成机制使其原则上能够兼容现有视觉数据集与仿真框架，为多模态生成式仿真提供了可扩展的技术路径。在大规模驾驶数据上的评估表明，RadarGen能够准确捕捉雷达测量的特征分布，并缩小基于生成数据与真实数据训练的感知模型之间的性能差距，标志着跨传感模态统一生成式仿真研究迈出了重要一步。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.17897">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.17897">arXiv</a></p>
<hr />
<h3>10. Robust-R1：面向鲁棒视觉理解的退化感知推理框架</h3>
<p><strong>原文标题：</strong> Robust-R1: Degradation-Aware Reasoning for Robust Visual Understanding</p>
<p><strong>摘要：</strong>
多模态大语言模型在极端真实世界视觉退化条件下难以保持可靠性能，这限制了其实际应用的鲁棒性。现有鲁棒性多模态大语言模型主要依赖仅关注视觉编码器泛化能力的隐式训练/适应方法，存在可解释性有限与优化孤立的问题。为突破这些局限，我们提出Robust-R1——一种通过结构化推理链显式建模视觉退化的新型框架。该框架整合了：（1）面向退化感知推理基础的监督微调；（2）用于精准感知退化参数的奖励驱动对齐机制；（3）适应退化强度的动态推理深度缩放技术。为支撑该方法，我们构建了一个包含11K样本的专用数据集，其通过四个关键真实世界视觉处理阶段合成具有现实意义的退化类型，每个样本均标注了连接退化参数、感知影响、原始语义推理链与结论的结构化链条。综合实验表明本方法达到最先进的鲁棒性水平：Robust-R1在真实世界退化基准R-Bench上超越所有通用及鲁棒性基线模型，同时在MMMB、MMStar和RealWorldQA数据集的多强度对抗性退化测试中保持卓越的抗退化性能。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.17532">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.17532">arXiv</a></p>
<hr />
<h3>11. GroundingME：通过多维评估揭示多模态大语言模型中的视觉定位差距</h3>
<p><strong>原文标题：</strong> GroundingME: Exposing the Visual Grounding Gap in MLLMs through Multi-Dimensional Evaluation</p>
<p><strong>摘要：</strong>
视觉定位——即根据自然语言描述定位物体——是连接语言理解与视觉理解的关键桥梁。尽管多模态大语言模型（MLLMs）在现有基准测试中取得了令人瞩目的分数，但一个根本问题依然存在：MLLMs 是否真的能够像人类一样精细地将语言锚定于视觉，还是仅仅在简化数据集上进行模式匹配？当前的基准测试未能捕捉到人类能够轻松处理模糊指代、并识别何时无法实现定位的真实世界复杂性。为了严格评估 MLLMs 的真实能力，我们提出了 GroundingME 基准测试，该系统性地在四个关键维度上挑战模型：（1）判别性，区分高度相似的物体；（2）空间性，理解复杂的关系描述；（3）局限性，处理遮挡或微小物体；（4）拒斥性，识别无法定位的查询。通过结合自动生成与人工验证的精心构建，我们创建了 1,005 个反映真实世界复杂性的挑战性示例。对 25 个最先进的 MLLMs 进行评估后，揭示了一个显著的能力差距：最佳模型的准确率仅为 45.1%，而大多数模型在拒斥任务上得分为 0%，它们会反射性地产生物体幻觉，而非承认其不存在，这为其部署带来了严重的安全隐患。我们探索了两种改进策略：（1）测试时扩展，通过思维轨迹选择最优响应，将复杂定位任务的性能提升高达 2.9%；（2）数据混合训练，教导模型识别无法定位的查询，将拒斥准确率从 0% 提升至 27.9%。因此，GroundingME 既是一个揭示当前 MLLMs 局限性的诊断工具，也是一条通向人类水平视觉定位的发展路线图。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.17495">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.17495">arXiv</a></p>
<hr />
<h3>12. 语言模型的物理学：第4.1部分，架构设计与规范层的神奇之处</h3>
<p><strong>原文标题：</strong> Physics of Language Models: Part 4.1, Architecture Design and the Magic of Canon Layers</p>
<p><strong>摘要：</strong>
理解语言模型的架构差异具有挑战性，尤其是在学术规模的预训练中（例如13亿参数、1000亿词元），其结果往往受到噪声和随机性的主导。为克服这一难题，我们引入了受控的合成预训练任务，以隔离并评估模型的核心能力。在此框架下，我们发现了<strong>规范层</strong>：这是一种轻量级的架构组件——其命名源于音乐术语“卡农”——旨在促进相邻词元间的横向信息流动。规范层通过计算邻近词元表示的加权和，能够无缝集成到Transformer、线性注意力、状态空间模型或任何序列架构中。</p>
<p>我们展示了12项关键结果。其中包括规范层如何提升推理深度（例如提升2倍）、推理广度以及知识操控能力等。它们能够提升如NoPE等较弱架构的性能，使其匹配RoPE的水平，并使线性注意力模型达到与Mamba2/GDN等先进线性模型相当的水平——这些结论均通过合成任务和真实学术规模预训练得到验证。这一合成实验环境为隔离学术规模下常被掩盖的核心模型能力提供了一条经济且基于原理的路径。借助无限高质量数据，它甚至能够<strong>预测</strong>未来架构在训练流程改进（例如通过更优的数据整理或基于强化学习的训练后优化）下的表现，从而解锁更深层次的推理与分层推断能力。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.17351">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.17351">arXiv</a></p>
<hr />
<h3>13. Turn-PPO：基于PPO的回合级优势估计以改进智能大语言模型中的多轮强化学习</h3>
<p><strong>原文标题：</strong> Turn-PPO: Turn-Level Advantage Estimation with PPO for Improved Multi-Turn RL in Agentic LLMs</p>
<p><strong>摘要：</strong>
强化学习已成为在现实环境中训练交互式大语言模型智能体的一种自然方法。然而，直接将广泛使用的组相对策略优化算法应用于多轮任务时，暴露出明显的局限性，尤其是在需要长程推理的场景中。为应对这些挑战，我们研究了更稳定、更有效的优势估计策略，特别是针对多轮交互的设置。我们首先探索了近端策略优化算法作为替代方案，并发现其比GRPO更具鲁棒性。为了进一步提升PPO在多轮场景中的性能，我们提出了turn-PPO，这是一种基于回合级马尔可夫决策过程建模的变体，而非常用的令牌级MDP。我们在WebShop和Sokoban数据集上的实验结果表明，无论是否包含长推理组件，turn-PPO均表现出显著的有效性。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.17008">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.17008">arXiv</a></p>
<hr />
<h3>14. HERBench：视频问答中多证据整合的基准测试框架</h3>
<p><strong>原文标题：</strong> HERBench: A Benchmark for Multi-Evidence Integration in Video Question Answering</p>
<p><strong>摘要：</strong>
视频大语言模型（Video-LLMs）发展迅速，然而当前的视频问答（VideoQA）基准测试常允许仅通过单一显著线索回答问题，未能充分检验模型整合多个时间分散视觉证据的推理能力。本文提出HERBench——一个专门用于评估跨时间多证据整合能力的VideoQA基准。每个问题要求整合至少三个分布于不同视频片段且互不重叠的证据线索，因此仅依赖语言先验或单一画面均无法正确作答。HERBench包含2.6万个五选一选择题，划分为十二项组合式任务，涵盖身份绑定、跨实体关系、时序排序、共现验证及计数等能力维度。为量化证据需求，我们提出最小必需帧集（MRFS）指标，即模型必须融合的最小帧数才能正确回答问题。实验表明HERBench对证据整合的要求显著高于现有数据集（平均MRFS为5.5帧，对比基准数据集为2.6-4.2帧）。对13个前沿Video-LLMs的评估揭示了普遍性缺陷：模型准确率仅为31%-42%，略高于20%的随机猜测基线。我们将这种缺陷归因于两个关键瓶颈：（1）检索缺陷：帧选择器遗漏关键证据；（2）融合缺陷：即使提供全部必要证据，模型仍无法有效整合信息。通过构建不可避免且可量化的跨时间证据需求，HERBench为推进鲁棒性、组合式的视频理解研究确立了原则性目标。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.14870">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.14870">arXiv</a></p>
<hr />
<h3>15. 任意世界中的角色动画生成</h3>
<p><strong>原文标题：</strong> Animate Any Character in Any World</p>
<p><strong>摘要：</strong>
世界模型的最新进展显著增强了交互式环境模拟能力。现有方法主要分为两类：（1）静态世界生成模型，可构建无主动智能体的三维环境；（2）可控实体模型，允许单一实体在不可控环境中执行有限动作。本研究提出AniX系统，在保留静态世界生成模型真实感与结构基础的同时，将可控实体模型扩展至支持用户自定义角色执行开放式动作。用户可提供3D高斯泼溅场景与角色模型，通过自然语言指令引导角色完成从基础移动到以物体为中心的多样化交互行为，并自由探索环境。AniX将任务构建为条件自回归视频生成问题，能够合成时间连贯且保持输入场景与角色视觉保真度的视频片段。基于预训练视频生成器，我们的训练策略在保持跨动作与角色泛化能力的同时，显著提升了运动动态表现。评估体系涵盖视觉质量、角色一致性、动作可控性及长时序连贯性等多维度指标。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.17796">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.17796">arXiv</a></p>
<hr />
<h3>16. SWE-Bench++：基于开源仓库的可扩展软件工程基准生成框架</h3>
<p><strong>原文标题：</strong> SWE-Bench++: A Framework for the Scalable Generation of Software Engineering Benchmarks from Open-Source Repositories</p>
<p><strong>摘要：</strong>
SWE-bench等基准测试已为大型语言模型在仓库级软件工程任务上的评估提供了标准化方法。然而，这些工作仍受限于人工标注、静态数据集以及对基于Python的错误修复的侧重。本文提出SWE-Bench++，这是一个从开源GitHub项目中自动生成仓库级编码任务的框架。与合成方法不同，我们的流水线通过采集实时拉取请求，覆盖11种编程语言的错误修复与功能需求任务。SWE-Bench++通过四个阶段将GitHub拉取请求转化为可复现的、基于执行的任务：程序化采集、环境合成、测试预言提取与质量保证。最后通过提示引导的轨迹合成步骤，将强模型未能解决的任务实例转化为训练轨迹。我们的初始基准包含来自3,971个仓库的11,133个任务实例，涵盖11种编程语言。在该基准的1,782个实例子集上，当前最强模型的表现如下：claude-sonnet-4.5达到36.20% pass@10，gpt-5-2025-08-07为34.57%，gemini/gemini-2.5-pro为24.92%，gpt-4o为16.89%。我们进一步通过实验证明，使用SWE-Bench++实例进行微调可在SWE-bench多语言基准上带来显著性能提升。SWE-Bench++为评估和改进仓库级代码生成能力提供了一个可扩展、多语言的基准测试框架。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.17419">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.17419">arXiv</a></p>
<hr />
<h3>17. Bolmo：字节化新一代语言模型</h3>
<p><strong>原文标题：</strong> Bolmo: Byteifying the Next Generation of Language Models</p>
<p><strong>摘要：</strong>
我们推出Bolmo，这是首个在10亿和70亿参数规模上具有竞争力的全开放字节级语言模型系列。与以往主要关注从头训练的字节级语言模型研究不同，我们通过对现有子词级语言模型进行字节化来训练Bolmo。字节化能够克服子词分词的限制——例如字符理解不足和固定子词词汇表导致的效率约束——同时达到领先子词级语言模型的性能水平。Bolmo专为字节化设计：我们的架构解决了先前字节级架构与子词级语言模型在表达能力上的不匹配问题，使得在Bolmo与源子词模型之间采用有效的精确蒸馏目标成为可能。这使得仅需投入不到典型预训练token预算的1%，即可将子词级语言模型转换为字节级语言模型。Bolmo在可比规模上显著优于所有先前的字节级语言模型，并在字符理解及部分代码任务上超越源子词级语言模型，同时在其他任务上接近原始模型的性能。此外，我们通过采用更高的token压缩比进行训练，证明Bolmo能够实现与子词级语言模型相竞争的推理速度，并可依托源子词级语言模型的现有生态系统进行低成本高效的后训练。我们的研究成果最终使字节级语言模型成为广泛用例中与子词级语言模型相竞争的实用选择。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.15586">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.15586">arXiv</a></p>
<hr />
<h3>18. StageVAR：面向视觉自回归模型的阶段感知加速方法</h3>
<p><strong>原文标题：</strong> StageVAR: Stage-Aware Acceleration for Visual Autoregressive Models</p>
<p><strong>摘要：</strong>
视觉自回归模型通过"下一尺度预测"机制突破了传统自回归模型的"下一标记预测"范式，实现了高质量图像生成。然而，该范式在大尺度生成步骤中面临计算复杂度与运行时间急剧增加的问题。现有加速方法虽能缩减大尺度步骤的运行时间，但依赖人工步骤选择且忽视了生成过程中不同阶段的重要性差异。针对这一挑战，本文提出StageVAR——一个面向视觉自回归模型的系统性研究与阶段感知加速框架。分析表明：早期步骤对保持语义与结构一致性至关重要，应当完整保留；而后期步骤主要进行细节优化，可通过剪枝或近似计算实现加速。基于此发现，StageVAR提出即插即用的加速策略，利用后期计算中的语义无关性与低秩特性实现加速，且无需额外训练。实验表明，StageVAR在GenEval基准上仅损失0.01分、DPG基准上仅下降0.26分的情况下，最高可实现3.4倍加速，持续优于现有加速基线方法。这些结果证明阶段感知设计是提升视觉自回归图像生成效率的有效原则。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.16483">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.16483">arXiv</a></p>
<hr />
<h3>19. 元强化学习在语言智能体中引导探索行为</h3>
<p><strong>原文标题：</strong> Meta-RL Induces Exploration in Language Agents</p>
<p><strong>摘要：</strong>
强化学习（RL）已能够训练大型语言模型（LLM）智能体与环境交互，以解决多轮次长周期任务。然而，经过强化学习训练的智能体在需要主动探索的任务中往往表现不佳，且难以从试错经验中高效适应。本文提出LaMer——一个通用的元强化学习框架，使LLM智能体能够在测试阶段主动探索并从环境反馈中学习。LaMer包含两个核心组件：（1）跨回合训练框架，以鼓励探索并优化长期奖励；（2）通过反思实现情境策略适应，使智能体无需梯度更新即可根据任务反馈信号调整策略。在多样化环境中的实验表明，LaMer在推箱子、扫雷和网络购物任务上分别实现了11%、14%和19%的性能提升，显著优于传统强化学习基线方法。此外，与强化学习训练的智能体相比，LaMer在更具挑战性或先前未见任务上也展现出更好的泛化能力。总体而言，我们的研究结果表明，元强化学习为语言智能体提供了引导探索行为的理论框架，使其能够通过习得的探索策略更稳健地适应新环境。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.16848">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.16848">arXiv</a></p>
<hr />
<h3>20. 3D-RE-GEN：基于生成式框架的室内场景三维重建</h3>
<p><strong>原文标题：</strong> 3D-RE-GEN: 3D Reconstruction of Indoor Scenes with a Generative Framework</p>
<p><strong>摘要：</strong>
当前三维场景生成技术虽能产生视觉上吸引人的结果，但其现有表示形式难以满足视觉特效与游戏开发领域艺术家对可编辑纹理化三维网格场景工作流程的需求。尽管相关研究已取得显著进展，现有纹理网格场景重建方法仍存在物体分解错误、空间关系不准确及背景缺失等问题，远未达到艺术创作实用标准。本文提出3D-RE-GEN——一个能够将单张图像重建为纹理化三维物体与背景的组合式框架。研究表明，通过整合特定领域的前沿模型，本方法在满足艺术创作需求的同时实现了当前最优的场景重建性能。</p>
<p>我们的重建流程融合了资源检测、重建与布局模型，推动部分模型突破其原始设计领域。针对被遮挡物体的获取问题，我们将其构建为生成式模型的图像编辑任务，通过在一致光照与几何约束下进行场景级推理来实现推断与重建。与现有方法不同，3D-RE-GEN能够生成完整的背景环境，在优化过程中对物体进行空间约束，并为视觉特效与游戏中的真实光照模拟任务提供基础支撑。为获得物理合理的场景布局，我们提出一种新颖的四自由度可微分优化方法，将重建物体与估计的地平面对齐。通过精确相机恢复与空间优化指导的组合式生成，3D-RE-GEN在单图像三维场景重建中实现了当前最优性能，能够生成具有空间一致性且可编辑的完整场景。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.17459">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.17459">arXiv</a></p>
<hr />
<h3>21. 面向长视频全模态推理与工具使用的基准与智能体框架</h3>
<p><strong>原文标题：</strong> A Benchmark and Agentic Framework for Omni-Modal Reasoning and Tool Use in Long Videos</p>
<p><strong>摘要：</strong>
长模态视频理解需要融合视觉、语音与环境音频信息，并进行连贯的长程推理。现有基准大多侧重时间长度或多模态丰富性中的单一维度，鲜有同时兼顾两者；尽管部分基准引入了开放式问题与高级评估指标，但仍主要依赖单一分数制准确率，难以揭示模型的具体失效模式。本文提出LongShOTBench——一个包含开放式意图驱动问题、单轮/多轮对话，以及需要跨视频、音频和语音进行多模态推理与智能体工具使用任务的诊断性基准。每个评估项均配有参考答案和分级评估准则，支持可解释、可追溯的性能评估。该基准通过可扩展的人工验证流程构建，确保覆盖度与可复现性，所有样本均经过人工校验与修正。此外，我们开发了LongShOTAgent智能体系统，通过预处理、检索与迭代优化实现对长视频的分析。在LongShOTBench上的实验表明，当前先进多模态大语言模型存在显著性能差距：Gemini-2.5-Flash仅达到52.95%，开源模型普遍低于30%，而LongShOTAgent取得44.66%的结果。这些发现凸显了现实场景中长模态视频理解任务的挑战性。LongShOTBench为评估与改进多模态大语言模型提供了实用且可复现的基础框架。所有资源已在GitHub开源：https://github.com/mbzuai-oryx/longshot。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.16978">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.16978">arXiv</a></p>
<hr />
<h3>22. MineTheGap：文本到图像模型中偏见的自动挖掘方法</h3>
<p><strong>原文标题：</strong> MineTheGap: Automatic Mining of Biases in Text-to-Image Models</p>
<p><strong>摘要：</strong>
文本到图像（TTI）模型根据文本提示生成图像，而文本提示往往对期望图像的某些方面表述模糊。面对这些模糊性时，TTI模型在解读过程中已表现出存在偏见。这些偏见可能产生社会影响，例如在描述某种职业时仅呈现特定种族形象。当一组生成图像内部出现冗余而非展现多样可能性时，也会影响用户体验。本文提出MineTheGap——一种自动挖掘导致TTI模型产生偏见输出的提示词方法。我们的方法不仅限于检测给定提示词的偏见，更通过遗传算法迭代优化提示词集合，主动探寻那些能暴露偏见的提示词。该优化过程由新颖的偏见评分驱动，该评分根据偏见严重程度进行排序（我们在已知偏见数据集上验证了其有效性）。对于给定提示词，该评分通过比较生成图像的分布与大型语言模型生成的提示词变体文本分布获得。代码与示例详见项目网页。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.13427">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.13427">arXiv</a></p>
<hr />
<h2>🔍 关键词云图</h2>
<p><img alt="关键词云图" src="../images/keywords_wordcloud.png" /></p>
<h2>📈 近期论文趋势</h2>
<p><img alt="论文趋势" src="../images/daily_papers.png" /></p>
<h2>🎙️ 语音播报</h2>
<ul>
<li><a href="../audio/2025-12-22_daily_papers.mp3">收听今日论文解读</a></li>
</ul>
<h2>📱 订阅渠道</h2>
<ul>
<li>GitHub: <a href="https://github.com/2404589803/hf-daily-paper-newsletter-chinese">hf-daily-paper-newsletter-chinese</a></li>
</ul>
    </div>
</body>
</html>