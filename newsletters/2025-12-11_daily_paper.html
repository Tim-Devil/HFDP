<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Hugging Face 论文日报 - 2025-12-11</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', sans-serif;
            line-height: 1.6;
            color: #333;
            background-color: #f5f5f5;
            padding: 20px;
        }
        
        .container {
            max-width: 900px;
            margin: 0 auto;
            background-color: white;
            padding: 40px;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }
        
        h1 {
            color: #2c3e50;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
            margin-bottom: 30px;
            font-size: 28px;
        }
        
        h1 img {
            vertical-align: middle;
            margin-right: 10px;
        }
        
        h2 {
            color: #34495e;
            margin-top: 40px;
            margin-bottom: 20px;
            font-size: 24px;
            border-left: 4px solid #3498db;
            padding-left: 15px;
        }
        
        h3 {
            color: #2c3e50;
            margin-top: 30px;
            margin-bottom: 15px;
            font-size: 20px;
        }
        
        p {
            margin-bottom: 15px;
            text-align: justify;
        }
        
        ul {
            margin-left: 20px;
            margin-bottom: 20px;
        }
        
        li {
            margin-bottom: 8px;
        }
        
        a {
            color: #3498db;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        strong {
            color: #2c3e50;
            font-weight: 600;
        }
        
        hr {
            border: none;
            border-top: 1px solid #e0e0e0;
            margin: 30px 0;
        }
        
        /* 关键修复:限制图片宽度 */
        img {
            max-width: 100%;
            height: auto;
            display: block;
            margin: 20px auto;
            border-radius: 4px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        
        /* 确保图片容器也有宽度限制 */
        p img {
            max-width: 100%;
        }
        
        /* 论文详情区域样式 */
        .paper-section {
            background-color: #f9f9f9;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 20px;
        }
        
        /* 统计信息样式 */
        .stats {
            background-color: #e8f4f8;
            padding: 15px;
            border-radius: 6px;
            margin-bottom: 20px;
        }
        
        /* 响应式设计 */
        @media (max-width: 768px) {
            .container {
                padding: 20px;
            }
            
            h1 {
                font-size: 24px;
            }
            
            h2 {
                font-size: 20px;
            }
            
            h3 {
                font-size: 18px;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <h1><img src="https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo.png" width="30"/> Hugging Face 2025-12-11 论文日报</h1>
<h2>📊 今日论文统计</h2>
<ul>
<li>总论文数：22</li>
<li>热门领域：RL, Transformer, LLM, Diffusion</li>
</ul>
<h2>📝 论文详情</h2>
<h3>1. StereoWorld：几何感知的单目到立体视频生成框架</h3>
<p><strong>原文标题：</strong> StereoWorld: Geometry-Aware Monocular-to-Stereo Video Generation</p>
<p><strong>摘要：</strong>
随着XR设备的日益普及，市场对高质量立体视频的需求持续增长，但其制作仍面临成本高昂且易产生视觉伪影的挑战。为解决这一问题，我们提出了StereoWorld——一个端到端的框架，通过改造预训练视频生成模型，实现高保真的单目到立体视频生成。该框架在模型中以单目视频输入为联合条件，同时通过几何感知正则化对生成过程进行显式监督，以确保三维结构的保真度。我们还进一步整合了时空分块机制，以实现高效的高分辨率合成。为支持大规模训练与评估，我们构建了一个高清立体视频数据集，包含超过1100万帧与自然人眼瞳距（IPD）对齐的视频帧。大量实验表明，StereoWorld在视觉保真度与几何一致性方面显著优于现有方法，能够生成质量更优的立体视频。项目页面详见：https://ke-xing.github.io/StereoWorld/。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.09363">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.09363">arXiv</a></p>
<hr />
<h3>2. BrainExplore：大规模发现人脑中可解释的视觉表征</h3>
<p><strong>原文标题：</strong> BrainExplore: Large-Scale Discovery of Interpretable Visual Representations in the Human Brain</p>
<p><strong>摘要：</strong>
理解人脑如何表征视觉概念以及这些表征在哪些脑区编码，仍然是一个长期存在的挑战。数十年的研究增进了我们对视觉表征的理解，但脑信号依然庞大而复杂，且可能的视觉概念空间极为广阔。因此，大多数研究仍局限于小规模、依赖人工检查、聚焦于特定区域和属性，并很少包含系统性验证。本文提出一个大规模、自动化的框架，用于发现和解释跨越人类大脑皮层的视觉表征。我们的方法包含两个主要阶段：首先，通过无监督的数据驱动分解方法，从功能磁共振成像活动中发现候选的可解释模式；其次，通过识别最能激发该模式的自然图像集，并生成对其共享视觉意义的自然语言描述，来解释每个模式。为实现规模化处理，我们引入了一个自动化流程，用于测试多个候选解释、分配定量可靠性评分，并为每个体素模式选择最一致的描述。我们的框架揭示了数千个可解释模式，涵盖许多不同的视觉概念，包括此前未报道过的细粒度表征。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.08560">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.08560">arXiv</a></p>
<hr />
<h3>3. OmniPSD：基于扩散Transformer的分层PSD生成方法</h3>
<p><strong>原文标题：</strong> OmniPSD: Layered PSD Generation with Diffusion Transformer</p>
<p><strong>摘要：</strong>
扩散模型的最新进展显著提升了图像生成与编辑能力，然而生成或重建具有透明Alpha通道的分层PSD文件仍极具挑战性。本文提出OmniPSD——一个基于Flux生态系统的统一扩散框架，通过上下文学习同时实现文本到PSD生成与图像到PSD分解。在文本到PSD生成任务中，OmniPSD将多个目标图层空间排列至单一画布，并通过空间注意力机制学习其组合关系，从而生成语义连贯且层次结构清晰的图层。在图像到PSD分解任务中，该框架执行迭代式上下文编辑，逐步提取并擦除文本与前景元素，从单张扁平化图像重建可编辑的PSD图层。我们引入RGBA-VAE作为辅助表征模块，在不影响结构学习的前提下保持透明度信息。基于新建的RGBA分层数据集的大量实验表明，OmniPSD能够实现高保真生成、结构一致性与透明度感知，为基于扩散Transformer的分层设计生成与分解提供了新范式。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.09247">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.09247">arXiv</a></p>
<hr />
<h3>4. 通过概念提示绑定实现图像与视频的概念组合</h3>
<p><strong>原文标题：</strong> Composing Concepts from Images and Videos via Concept-prompt Binding</p>
<p><strong>摘要：</strong>
视觉概念组合旨在将图像和视频中的不同元素整合为单一、连贯的视觉输出，但现有方法在从视觉输入中准确提取复杂概念、灵活组合图像与视频概念方面仍存在不足。本文提出Bind &amp; Compose方法，这是一种单次学习技术，通过将视觉概念与对应的提示词绑定，并利用来自多源的绑定词组合目标提示，实现灵活的视觉概念组合。该方法采用分层绑定器结构，在扩散变换器中通过跨注意力机制将视觉概念编码为对应的提示词，从而实现对复杂视觉概念的精确解构。为提高概念-词绑定的准确性，我们设计了"多样化-吸收机制"，通过引入额外的吸收词来消除训练过程中多样化提示带来的无关细节干扰。为增强图像与视频概念的兼容性，我们提出"时序解耦策略"，通过双分支绑定器结构将视频概念训练解耦为两个阶段，以优化时序建模。实验评估表明，本方法在概念一致性、提示保真度和运动质量方面均优于现有技术，为视觉创意应用开辟了新可能。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.09824">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.09824">arXiv</a></p>
<hr />
<h3>5. InfiniteVL：融合线性与稀疏注意力实现高效、无限输入的视觉语言模型</h3>
<p><strong>原文标题：</strong> InfiniteVL: Synergizing Linear and Sparse Attention for Highly-Efficient, Unlimited-Input Vision-Language Models</p>
<p><strong>摘要：</strong>
窗口注意力和线性注意力是缓解视觉语言模型（VLMs）中二次复杂度与持续增长的KV缓存的两类主要策略。然而，我们观察到基于窗口的VLMs在序列长度超过窗口大小时会出现性能下降，而线性注意力在OCR和文档理解等信息密集型任务上表现欠佳。为克服这些限制，本文提出InfiniteVL，一种线性复杂度的VLM架构，它将滑动窗口注意力（SWA）与门控DeltaNet相融合。为了在有限资源下实现具有竞争力的多模态性能，我们设计了一个包含蒸馏预训练、指令微调和长序列监督微调的三阶段训练策略。值得注意的是，在使用不到主流VLM所需训练数据2%的情况下，InfiniteVL不仅显著超越了以往的线性复杂度VLMs，而且达到了基于Transformer的主流VLMs的性能水平，同时展现出有效的长期记忆保持能力。与通过FlashAttention-2加速的同类规模Transformer VLM相比，InfiniteVL在保持恒定延迟和内存占用的同时，实现了超过3.6倍的推理加速。在流式视频理解场景中，它能够维持稳定的24 FPS实时预填充速度，同时保持长期记忆缓存。代码与模型已发布于https://github.com/hustvl/InfiniteVL。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.08829">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.08829">arXiv</a></p>
<hr />
<h3>6. 重新思考视频中的思维链推理</h3>
<p><strong>原文标题：</strong> Rethinking Chain-of-Thought Reasoning for Videos</p>
<p><strong>摘要：</strong>
思维链推理在解决自然语言处理中的复杂任务方面取得了显著成功，而近期出现的多模态大语言模型将这一范式扩展至视频推理领域。然而，现有模型通常依赖于冗长的推理链和大量的输入视觉标记。基于基准研究的实证观察，我们提出假设：结合精简视觉标记的简洁推理可能足以实现有效的视频推理。为验证这一假设，我们设计并验证了一种高效的训练后处理与推理框架，该框架能够增强视频多模态大语言模型的推理能力。我们的框架使模型能够在压缩视觉标记上运行，并在回答问题前生成简明的推理轨迹。实验结果表明，优化后的模型在显著提升推理效率的同时，在多样化基准测试中展现出具有竞争力的性能，且无需依赖人工标注的思维链数据或有监督微调。综合而言，我们的研究结果表明，对于通用视频推理任务，冗长类人思维链推理可能并非必要，而简洁推理既能保持有效性又能提升效率。代码将在 https://github.com/LaVi-Lab/Rethink_CoT_Video 发布。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.09616">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.09616">arXiv</a></p>
<hr />
<h3>7. HiF-VLA：基于运动表征的视觉-语言-动作模型后视、洞察与前瞻框架</h3>
<p><strong>原文标题：</strong> HiF-VLA: Hindsight, Insight and Foresight through Motion Representation for Vision-Language-Action Models</p>
<p><strong>摘要：</strong>
视觉-语言-动作模型近期通过将视觉与语言线索映射为动作，实现了机器人操控任务。然而，现有模型大多遵循马尔可夫假设，仅依赖当前观测状态，因而存在时序短视问题，导致长时序任务中的连贯性下降。本研究提出将运动视为一种更紧凑且信息丰富的时序上下文与世界动态表征，其能捕捉状态间变化并过滤静态像素级噪声。基于此，我们构建了HiF-VLA（视觉-语言-动作模型的后视、洞察与前瞻框架），该统一框架利用运动信息实现双向时序推理。HiF-VLA通过后验先验编码历史动态，借助前瞻推理预测未来运动，并通过后验调制联合专家模块将二者融合，从而构建适用于长时序操控的“行动中思考”范式。实验表明，HiF-VLA在LIBERO-Long与CALVIN ABC-D基准测试中均超越现有强基线模型，且仅带来可忽略的额外推理延迟。此外，该模型在现实长时序操控任务中取得显著性能提升，证明了其在实际机器人应用场景中的广泛有效性。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.09928">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.09928">arXiv</a></p>
<hr />
<h3>8. 基于进度感知置信度调度的扩散语言模型快速解码方法</h3>
<p><strong>原文标题：</strong> Fast-Decoding Diffusion Language Models via Progress-Aware Confidence Schedules</p>
<p><strong>摘要：</strong>
扩散大语言模型（dLLMs）为自回归模型提供了一种有前景的替代方案，但其缓慢的迭代采样过程严重限制了实际应用。本文提出SchED算法，这是一种无需训练、与模型无关的提前退出算法，通过聚合全跨度对数边际值，并在达到平滑的进度相关置信度阈值时停止解码。我们在两个dLLM系列（Dream和LLaDA）上评估了SchED，涵盖基础版本和指令微调版本，并在十个基准测试中进行验证，包括多项选择题回答（MCQ）、数学、长文本问答/摘要和翻译等下游任务。SchED实现了显著且稳定的加速效果：在指令微调模型上，平均加速比达到3.8-4.0倍，同时保持基线得分的99.8%-100%；在基础模型上，SchED在保持99.1%-100%性能的前提下获得稳定的加速增益，在更激进的设置下加速比最高可达2.34倍。采用严格惩罚质量损失的速度评估指标（QPS, γ=4）表明，SchED具有强鲁棒性，明显优于先前基于置信度的提前退出方法（后者在长文本生成任务中失效）。对模型令牌预测的熵分析显示，指令微调会加速预测熵的衰减。通过将真实的置信度稳定转化为计算效率提升，SchED显著提高了dLLM的解码效率。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.02892">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.02892">arXiv</a></p>
<hr />
<h3>9. UniUGP：面向端到端自动驾驶的理解、生成与规划统一框架</h3>
<p><strong>原文标题：</strong> UniUGP: Unifying Understanding, Generation, and Planing For End-to-end Autonomous Driving</p>
<p><strong>摘要：</strong>
自动驾驶系统因世界知识有限与视觉动态建模能力不足，在长尾场景中面临挑战。现有基于视觉-语言-动作的方法难以利用无标注视频进行视觉因果学习，而基于世界模型的方法缺乏大语言模型的推理能力。本文构建了多个专用数据集，为复杂场景提供推理与规划标注。在此基础上，提出名为UniUGP的统一理解-生成-规划框架，通过混合专家架构协同实现场景推理、未来视频生成与轨迹规划。该框架整合预训练的视觉语言模型与视频生成模型，利用视觉动态与语义推理提升规划性能。系统以多帧观测数据与语言指令作为输入，可生成可解释的思维链推理、物理一致的轨迹以及连贯的未来场景视频。我们设计了四阶段训练策略，在多个现有自动驾驶数据集及新建专用数据集上逐步构建上述能力。实验结果表明，该系统在感知、推理与决策任务上达到最先进性能，并在具有挑战性的长尾场景中展现出卓越的泛化能力。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.09864">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.09864">arXiv</a></p>
<hr />
<h3>10. WonderZoom：多尺度三维世界生成</h3>
<p><strong>原文标题：</strong> WonderZoom: Multi-Scale 3D World Generation</p>
<p><strong>摘要：</strong>
本文提出WonderZoom，这是一种从单张图像生成跨多空间尺度三维场景内容的新方法。现有三维世界生成模型仍局限于单尺度合成，无法在不同粒度上生成连贯的场景内容。其根本挑战在于缺乏能够生成和渲染空间尺寸差异巨大内容的尺度感知三维表征。WonderZoom通过两项关键创新解决此问题：（1）用于多尺度三维场景生成与实时渲染的尺度自适应高斯面元；（2）可迭代生成更精细尺度三维内容的渐进式细节合成器。该方法使用户能够"放大"三维区域，并自回归地合成从景观到微观特征等原本不存在的精细细节。实验表明，WonderZoom在生成质量和对齐度上显著优于当前最先进的视频与三维模型，实现了从单张图像创建多尺度三维世界。生成的多尺度三维世界视频结果与交互式查看器详见 https://wonderzoom.github.io/</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.09164">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.09164">arXiv</a></p>
<hr />
<h3>11. EtCon：先编辑后巩固——一种可靠的知识编辑方法</h3>
<p><strong>原文标题：</strong> EtCon: Edit-then-Consolidate for Reliable Knowledge Editing</p>
<p><strong>摘要：</strong>
知识编辑旨在无需完整重训练的情况下更新大语言模型（LLM）中的特定事实。先前研究主要通过调整LLM的知识层来实现选择性编辑，并在受控的教师强制评估中取得了良好效果。然而，这类方法在受控评估中的表现与其在终身学习场景中的实际效能之间存在显著差距，这严重限制了其实际应用价值。本文的实证分析揭示了导致该差距的两个常见问题：（1）多数传统方法会使编辑后的模型过度拟合新事实，从而损害其预训练能力；（2）关键缺乏知识巩固阶段，导致新知识未能充分整合到LLM在自回归生成时的推理行为中，造成参数化知识与实际生成行为之间的错配。为此，我们提出“先编辑后巩固”这一新型知识编辑范式，旨在弥合理论性知识编辑方法与实际应用之间的鸿沟。具体而言：（1）本框架通过“目标近端监督微调”缓解过拟合问题，该技术基于信任区域目标定位编辑范围以限制策略漂移；（2）随后采用“群体相对策略优化”进行知识巩固，通过在多维度奖励信号下优化轨迹级行为，将编辑后的知识与基于思维链的推理策略对齐。大量实验表明，本框架在现实场景评估中能持续提升编辑的可靠性与泛化能力，同时更好地保持了编辑的局部性与模型的预训练能力。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.04753">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.04753">arXiv</a></p>
<hr />
<h3>12. 扩散语言模型中的解掩码策略学习</h3>
<p><strong>原文标题：</strong> Learning Unmasking Policies for Diffusion Language Models</p>
<p><strong>摘要：</strong>
扩散（大）语言模型（dLLMs）目前在多项任务的下游性能上已与自回归模型相当，同时在推理效率方面展现出更大潜力。其中一种尤为成功的变体是掩码离散扩散模型，该模型通过逐步将填充特殊掩码标记的缓冲区替换为从模型词汇表中采样的标记来实现生成。通过并行解掩码多个标记可提升效率，但一次性解掩过多标记可能导致生成质量下降。因此，dLLMs的一个关键设计环节是在扩散过程的每一步选择待替换标记的采样策略。近期研究已发现，与随机解掩码相比，基于置信度阈值等启发式策略能同时提升生成质量和标记吞吐量。然而，此类启发式方法存在局限：需要人工调参，且我们发现其性能会随缓冲区规模扩大而下降。本研究提出采用强化学习训练采样策略的新方法。具体而言，我们将掩码扩散采样形式化为马尔可夫决策过程，其中dLLM作为环境，并设计了一种基于单层Transformer的轻量级策略架构，该架构可将dLLM的标记置信度映射为解掩码决策。实验表明，经过训练的采样策略在半自回归生成场景中能达到最先进启发式方法的性能水平，并在完整扩散设定中表现更优。我们还验证了策略的可迁移性，发现其能泛化至新的底层dLLM和更长序列。但研究同时指出，这些策略在处理域外数据时性能会下降，且通过本方法精细调节精度-效率权衡仍存在挑战。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.09106">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.09106">arXiv</a></p>
<hr />
<h3>13. 迈向智能体系统规模化科学</h3>
<p><strong>原文标题：</strong> Towards a Science of Scaling Agent Systems</p>
<p><strong>摘要：</strong>
基于语言模型的智能体系统——具备推理、规划与行动能力的系统——正成为现实世界人工智能应用的主导范式。尽管其应用日益广泛，决定其性能的内在原理仍未得到充分探索，导致实践者往往依赖经验法则而非基于原理的设计选择。本研究通过推导智能体系统的定量规模化原理来填补这一空白。我们在四个多样化基准测试（Finance-Agent、BrowseComp-Plus、PlanCraft和Workbench）中进行评估，采用五种典型架构（单智能体、独立智能体、集中式、分布式及混合式），并在三大语言模型系列中实例化，通过标准化工具与计算资源预算对180种配置进行受控实验。我们利用经验性协调指标（包括效率、开销、误差放大和冗余度）构建预测模型，其交叉验证R²达到0.513。研究发现三大主导效应：（1）工具-协调权衡：在固定计算预算下，工具密集型任务会因多智能体协调开销而承受不成比例的损失；（2）能力饱和效应：当单智能体基线性能超过约45%时，协调带来的收益呈现递减甚至负回报（β=-0.408，p&lt;0.001）；（3）拓扑依赖的误差放大：独立智能体因未受控的误差传播使误差放大17.2倍，而集中式协调可将其控制在4.4倍。在金融推理等可并行任务中，集中式协调使性能提升80.9%；在动态网络导航任务中，分布式协调表现更优（+9.2%对比+0.2%）。然而对于顺序推理任务，所有多智能体架构均导致性能下降39-70%。该框架对87%的预留配置能预测最优协调策略，为基于可测量任务特性的智能体规模化提供了可预测的设计原则。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.08296">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.08296">arXiv</a></p>
<hr />
<h3>14. IF-Bench：基于生成式视觉提示的红外图像多模态大语言模型评测与增强</h3>
<p><strong>原文标题：</strong> IF-Bench: Benchmarking and Enhancing MLLMs for Infrared Images with Generative Visual Prompting</p>
<p><strong>摘要：</strong>
多模态大语言模型（MLLMs）的最新进展已在各类评测基准上取得显著成果，然而其在红外图像理解方面的能力尚未得到充分探索。为填补这一空白，我们提出了IF-Bench——首个面向红外图像多模态理解评估的高质量基准。该基准包含从23个红外数据集中精选的499幅图像，以及680组精心构建的视觉问答对，涵盖图像理解的十个核心维度。基于此基准，我们系统评估了超过40个开源与闭源MLLMs，采用循环评估、双语测评和混合判读策略以提升结果可靠性。分析揭示了模型规模、架构及推理范式对红外图像理解的影响，为该领域提供了重要洞见。此外，我们提出了一种免训练的生成式视觉提示方法（GenViP），该方法利用先进图像编辑模型将红外图像转换为语义与空间对齐的RGB对应图像，从而缓解领域分布偏移问题。大量实验表明，该方法在多种MLLMs上均能带来显著的性能提升。基准数据与代码已公开于：https://github.com/casiatao/IF-Bench。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.09663">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.09663">arXiv</a></p>
<hr />
<h3>15. TED-4DGS：面向4DGS压缩的时间激活与嵌入式形变方法</h3>
<p><strong>原文标题：</strong> TED-4DGS: Temporally Activated and Embedding-based Deformation for 4DGS Compression</p>
<p><strong>摘要：</strong>
基于三维高斯泼溅（3DGS）在静态三维场景表示中的成功，其向动态场景的扩展（通常称为4DGS或动态3DGS）正受到日益广泛的关注。然而，如何为动态3DGS表示设计更紧凑高效的形变方案，并结合率失真优化的压缩策略，仍是一个尚未充分探索的领域。现有方法要么依赖具有过度参数化、短生命周期高斯基元的时空4DGS，要么采用缺乏显式时间控制的规范3DGS形变框架。为此，我们提出TED-4DGS——一种面向率失真优化4DGS压缩的时间激活与嵌入式形变方案，该方案融合了两类方法的优势。TED-4DGS建立在基于稀疏锚点的3DGS表示基础上：每个规范锚点被赋予可学习的时间激活参数，以控制其随时间推移的出现与消失状态；同时通过轻量化的锚点时序嵌入查询共享形变库，生成锚点特异性形变。为实现率失真压缩，我们引入基于隐式神经表示的超先验模型来建模锚点属性分布，并结合通道自回归模型以捕捉锚点内部关联。凭借这些创新设计，本方案在多个真实世界数据集上实现了领先的率失真性能。据我们所知，本研究是首个针对动态3DGS表示构建率失真优化压缩框架的探索性工作之一。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.05446">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.05446">arXiv</a></p>
<hr />
<h3>16. MotionEdit：面向运动中心图像编辑的基准构建与学习方法</h3>
<p><strong>原文标题：</strong> MotionEdit: Benchmarking and Learning Motion-Centric Image Editing</p>
<p><strong>摘要：</strong>
本文提出MotionEdit，一个面向运动中心图像编辑任务的新型数据集——该任务旨在修改主体动作与交互关系，同时保持身份特征、场景结构和物理合理性。与现有专注于静态外观修改或仅包含稀疏低质量运动编辑的数据集不同，MotionEdit通过从连续视频中提取并验证的真实运动变换，提供了描绘高保真运动转换的图像对。这一新任务不仅具有科学挑战性，更具备实际应用价值，可为帧控视频合成与动画生成等下游应用提供支持。</p>
<p>为评估模型在该新任务上的性能，我们构建了MotionEdit-Bench基准测试平台，通过生成式、判别式和基于偏好的多维度指标，系统评估模型在运动中心编辑任务上的表现。基准测试结果表明，当前基于扩散模型的先进编辑方法在运动编辑任务上仍面临巨大挑战。为应对这一局限，我们提出MotionNFT（运动引导的负向感知微调）框架，该训练后优化框架通过计算输入图像与模型编辑后图像间的运动流与真实运动场的匹配度，构建运动对齐奖励机制，引导模型实现精准的运动转换。在FLUX.1 Kontext和Qwen-Image-Edit模型上的大量实验表明，MotionNFT在保持通用编辑能力的同时，能持续提升基础模型在运动编辑任务中的编辑质量与运动保真度，验证了该框架的有效性。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.10284">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.10284">arXiv</a></p>
<hr />
<h3>17. 超越统一模型：面向服务的低延迟上下文感知音素化方法在实时TTS中的应用</h3>
<p><strong>原文标题：</strong> Beyond Unified Models: A Service-Oriented Approach to Low Latency, Context Aware Phonemization for Real Time TTS</p>
<p><strong>摘要：</strong>
轻量级实时文本转语音系统对于信息无障碍至关重要。然而最高效的TTS模型通常依赖轻量级音素转换器，这类系统在处理上下文相关挑战时表现欠佳。相比之下，具有更深层语言理解能力的先进音素转换器往往会产生高昂计算成本，从而无法实现实时性能。本文研究了G2P辅助TTS系统中音素化质量与推理速度之间的权衡关系，提出了一种弥合该差距的实用框架。我们设计了轻量级上下文感知音素化策略，并构建了面向服务的TTS架构，将这些模块作为独立服务执行。该设计将高计算负载的上下文感知组件与核心TTS引擎解耦，有效突破了延迟瓶颈，实现了高质量音素化模型的实时调用。实验结果表明，所提系统在保持实时响应能力的同时，显著提升了发音合理性与语言准确性，特别适用于离线及终端设备的TTS应用场景。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.08006">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.08006">arXiv</a></p>
<hr />
<h3>18. VideoSSM：基于混合状态空间记忆的自回归长视频生成</h3>
<p><strong>原文标题：</strong> VideoSSM: Autoregressive Long Video Generation with Hybrid State-Space Memory</p>
<p><strong>摘要：</strong>
自回归扩散模型通过因果式逐帧生成实现了流式、交互式的长视频生成，但由于误差累积、运动漂移和内容重复等问题，在分钟级时间跨度上保持视频连贯性仍具挑战。本文从记忆视角切入，将视频合成视为一种需要协调短期与长期上下文的循环动态过程。我们提出VideoSSM——一种将自回归扩散与混合状态空间记忆相统一的长视频生成模型。其中状态空间模型作为演化中的全局记忆，持续追踪整个序列的场景动态；而局部上下文窗口则为运动线索与细节特征提供短期记忆。这种混合设计能够在避免画面冻结与模式重复的前提下保持全局一致性，支持基于提示的自适应交互，并以序列长度的线性时间复杂度实现扩展。在短期与长期基准测试上的实验表明，该模型在自回归视频生成器中实现了最先进的时间连贯性与运动稳定性，尤其在分钟级跨度上表现突出，能够同时保障内容多样性与基于提示的交互控制，从而为长视频生成建立了一个可扩展的、具备记忆感知能力的框架。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.04519">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.04519">arXiv</a></p>
<hr />
<h3>19. GimbalDiffusion：面向视频生成的重力感知相机控制框架</h3>
<p><strong>原文标题：</strong> GimbalDiffusion: Gravity-Aware Camera Control for Video Generation</p>
<p><strong>摘要：</strong>
当前文本到视频生成技术已取得显著的真实感进展，但对相机运动与朝向的细粒度控制仍面临挑战。现有方法通常通过相对或模糊的表征编码相机轨迹，限制了显式几何控制能力。本文提出GimbalDiffusion框架，该框架基于物理世界坐标系实现相机控制，并以重力作为全局参考基准。与依赖相邻帧相对运动的传统方法不同，本方法在绝对坐标系中定义相机轨迹，无需初始参考帧即可实现对相机参数精确且可解释的控制。我们利用全景360度视频构建多样化的相机运动轨迹，其范围远超传统视频数据中主要存在的直线前向运动模式。为增强相机引导效果，我们提出零俯仰条件标注策略，该策略能在相机参数与文本描述冲突时（例如相机朝向天空时需生成草地场景）降低模型对文本内容的依赖。此外，我们通过重构SpatialVID-HQ数据集建立相机感知视频生成基准测试平台，支持大范围相机俯仰变化下的综合性能评估。这些创新共同提升了文本到视频模型的可控性与鲁棒性，实现在生成框架内进行精确且重力对齐的相机运动控制。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.09112">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.09112">arXiv</a></p>
<hr />
<h3>20. 减少对功能词的关注以提升视觉语言模型的鲁棒性</h3>
<p><strong>原文标题：</strong> Pay Less Attention to Function Words for Free Robustness of Vision-Language Models</p>
<p><strong>摘要：</strong>
为解决鲁棒视觉语言模型（VLM）中鲁棒性与性能之间的权衡问题，本文发现功能词可能导致VLM在跨模态对抗攻击下产生脆弱性，并据此提出功能词去关注（FDA）机制以减轻功能词的影响。类似于差分放大器的工作原理，FDA在注意力头内分别计算原始跨注意力与功能词跨注意力，并通过差分减法将后者从前者中去除，从而构建更对齐且更鲁棒的VLM。综合实验涵盖2个下游任务、3个数据集和3种模型，在6种不同攻击下与2种前沿基线方法进行对比。总体而言，在检索任务中，FDA在3个测试模型上平均实现18%/13%/53%的攻击成功率下降，性能仅下降0.2%/0.3%/0.6%；在视觉定位任务中实现90%的攻击成功率下降，同时性能提升0.3%。实验进一步验证了FDA的可扩展性、泛化能力和零样本性能，并提供了深入的消融研究与分析。代码将公开于https://github.com/michaeltian108/FDA。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.07222">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.07222">arXiv</a></p>
<hr />
<h3>21. 智能挖矿时机选择：一种基于深度学习的比特币硬件投资回报率预测框架</h3>
<p><strong>原文标题：</strong> Smart Timing for Mining: A Deep Learning Framework for Bitcoin Hardware ROI Prediction</p>
<p><strong>摘要：</strong>
由于市场波动剧烈、技术迭代迅速以及协议驱动的收益周期特性，比特币挖矿硬件的购置需要战略性择时。尽管挖矿已演变为资本密集型产业，但目前关于何时购置新型专用集成电路（ASIC）硬件的指导极为有限，且尚无先前的计算框架能够系统解决这一决策问题。为填补这一空白，本研究将硬件购置问题构建为时间序列分类任务，旨在预测购置ASIC矿机是否能在一年内产生盈利（投资回报率（ROI）≥1）、边际收益（0 &lt; ROI &lt; 1）或亏损（ROI ≤ 0）。我们提出了MineROI-Net，一种基于Transformer架构的开源模型，专门设计用于捕捉挖矿收益的多尺度时序模式。通过在2015年至2024年间发布的20款ASIC矿机数据上进行测试，并覆盖多种市场行情，MineROI-Net的表现优于基于LSTM和TSLANet的基线模型，实现了83.7%的准确率和83.1%的宏观F1分数。该模型展现出显著的经济实用性：在识别亏损时段时精确率达到93.6%，识别盈利时段时精确率达98.5%，且能有效避免将盈利场景误判为亏损（反之亦然）。结果表明，MineROI-Net为挖矿硬件购置时机选择提供了一个实用、数据驱动的工具，有望降低资本密集型挖矿运营的财务风险。模型可通过以下链接获取：https://github.com/AMAAI-Lab/MineROI-Net。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.05402">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.05402">arXiv</a></p>
<hr />
<h3>22. 重塑临床对话：基于大语言模型的医疗沟通智能体范式</h3>
<p><strong>原文标题：</strong> Reinventing Clinical Dialogue: Agentic Paradigms for LLM Enabled Healthcare Communication</p>
<p><strong>摘要：</strong>
临床对话呈现复杂的双重性，既需要自然交流的共情流畅性，又要求循证医学的严谨精确性。尽管大语言模型具备前所未有的语言能力，但其依赖反应式无状态处理的架构特点往往更倾向于概率合理性而非事实准确性。这一结构性局限推动了医疗人工智能领域的范式转变——从生成式文本预测转向智能体自主架构，使模型能够作为具备审慎规划与持久记忆能力的核心推理引擎。本文超越现有主要罗列下游应用的综述，从第一性原理出发分析了支撑这一转变的认知架构。我们提出了一种基于知识来源与智能体目标正交轴构建的新型分类体系，用以界定临床知识的溯源与系统操作范围的对应关系。该框架通过将现有方法归纳为四大原型——潜在空间临床医师、涌现式规划器、知识锚定合成器及可验证工作流自动化系统，系统性地揭示了创造力与可靠性之间的内在权衡。针对每种范式，我们解构了其在完整认知流程（涵盖战略规划、记忆管理、行动执行、协作与演进）中的技术实现路径，从而阐明不同架构选择如何平衡自主性与安全性之间的张力。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.01453">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.01453">arXiv</a></p>
<hr />
<h2>🔍 关键词云图</h2>
<p><img alt="关键词云图" src="../images/keywords_wordcloud.png" /></p>
<h2>📈 近期论文趋势</h2>
<p><img alt="论文趋势" src="../images/daily_papers.png" /></p>
<h2>🎙️ 语音播报</h2>
<ul>
<li><a href="../audio/2025-12-11_daily_papers.mp3">收听今日论文解读</a></li>
</ul>
<h2>📱 订阅渠道</h2>
<ul>
<li>GitHub: <a href="https://github.com/2404589803/hf-daily-paper-newsletter-chinese">hf-daily-paper-newsletter-chinese</a></li>
</ul>
    </div>
</body>
</html>