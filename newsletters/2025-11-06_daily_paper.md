
# <img src="https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo.png" width="30"/> Hugging Face 2025-11-06 论文日报

## 📊 今日论文统计
- 总论文数：13
- 热门领域：LLM, GPT

## 📝 论文详情


### 1. 扩散语言模型：卓越的数据学习器

**原文标题：** Diffusion Language Models are Super Data Learners

**摘要：**
在严格控制的预训练设定下，我们观察到交叉现象：当唯一数据有限时，扩散语言模型通过增加训练周期数持续超越自回归模型。这种交叉点会随着数据量增加或质量提升而后移，随模型规模扩大而前移，并在稠密与稀疏架构中均稳定存在。我们将性能增益归因于三个复合因素：（1）任意顺序建模能力；（2）迭代双向去噪带来的超密集计算；（3）内置蒙特卡洛增强机制。虽然输入噪声或参数噪声能在数据受限时提升自回归模型表现，但仍无法弥合性能差距。大规模实验表明，在约1.5T令牌计算预算下，使用100亿个独特Python令牌训练的17亿参数扩散语言模型，在严格匹配的设定下超越了自回归编程模型。此外，10亿参数扩散语言模型仅使用10亿令牌进行标准预训练数据重复训练（未采用特殊技巧），即在HellaSwag上取得超过56%的准确率，在MMLU上超过33%。我们还验证了在此机制下，验证集交叉熵的上升并不代表下游任务性能的退化。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.03276) | [arXiv](https://arxiv.org/abs/2511.03276)



---

### 2. UniAVGen：基于非对称跨模态交互的音频视频联合生成框架

**原文标题：** UniAVGen: Unified Audio and Video Generation with Asymmetric Cross-Modal
  Interactions

**摘要：**
由于缺乏有效的跨模态建模机制，现有开源音视频生成方法常存在唇部同步偏差与语义一致性不足的问题。为解决这些缺陷，我们提出UniAVGen——一个统一的音视频联合生成框架。该框架采用双分支联合合成架构，通过两个并行的扩散变换器构建统一的跨模态潜在空间。其核心创新在于非对称跨模态交互机制，该机制通过双向时序对齐的交叉注意力实现，确保精确的时空同步与语义一致性。此外，我们引入面部感知调制模块，在交互过程中动态增强显著区域的表征权重。为提升推理阶段的生成质量，我们进一步提出模态感知的无分类器引导策略，显式强化跨模态关联信号。值得注意的是，UniAVGen凭借其鲁棒的联合合成设计，可在单一模型中无缝集成关键音视频任务，包括音视频联合生成与续写、视频到音频的配音转换以及音频驱动视频生成。综合实验表明，在训练样本量显著减少的情况下（130万 vs 3010万），UniAVGen在音视频同步性、音色一致性与情感一致性方面均展现出综合优势。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.03334) | [arXiv](https://arxiv.org/abs/2511.03334)



---

### 3. LEGO-Eval：基于工具增强的细粒度三维具身环境合成评估框架

**原文标题：** LEGO-Eval: Towards Fine-Grained Evaluation on Synthesizing 3D Embodied
  Environments with Tool Augmentation

**摘要：**
尽管利用大语言模型自动生成三维场景已取得进展，但生成的场景往往缺乏真实环境中的合理空间布局与物体属性。该问题的根源在于指令描述不够详尽且粒度粗糙，因此推进基于反映真实环境的细粒度详细指令的三维场景合成至关重要。若缺乏真实场景支撑，在不真实环境中训练具身智能体将导致其学习到与现实世界物理规则和语义特征显著偏离的先验知识，从而降低实际部署时的性能表现。因此，验证细粒度指令与生成场景之间的对齐关系对有效学习至关重要。然而现有评估方法（如CLIPScore和视觉语言模型）往往难以可靠评估这种对齐关系，这主要源于其对三维场景的理解较为浅层，常导致场景要素的 grounding 结果失准。为此，我们提出LEGO-Eval评估框架，该框架配备多样化工具以显式实现场景要素的 grounding，从而支持更精准的对齐评估。同时我们构建LEGO-Bench基准数据集，包含指定真实环境复杂布局与属性的详细指令集。实验表明，LEGO-Eval在场景-指令对齐评估中的F1分数较VLM-as-a-judge方法提升0.41。基于LEGO-Bench的基准测试揭示了当前生成方法的显著局限：在所有评估方法中，能完全符合细粒度指令的场景生成成功率最高仅达10%。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.03001) | [arXiv](https://arxiv.org/abs/2511.03001)



---

### 4. Orion-MSP：面向表格上下文学习的多尺度稀疏注意力机制

**原文标题：** Orion-MSP: Multi-Scale Sparse Attention for Tabular In-Context Learning

**摘要：**
表格数据仍是现实应用中最主要的数据形式。然而由于异构特征类型和多尺度下的复杂交互作用，开发有效的表格数据神经网络模型仍面临挑战。表格上下文学习的最新进展（如TabPFN和TabICL）已实现与梯度提升树相当的最优性能，且无需任务特定微调。但现有架构存在关键局限：（1）单尺度特征处理忽视层次化依赖关系；（2）稠密注意力机制存在表宽度的二次方复杂度；（3）严格顺序的组件处理阻碍迭代表示优化与跨组件通信。为解决这些问题，我们提出Orion-MSP表格上下文学习架构，其具备三大创新：（1）多尺度处理机制捕获层次化特征交互；（2）融合窗口化、全局化与随机模式的块稀疏注意力，实现可扩展效率与长程连接；（3）感知器式记忆模块确保组件间安全的双向信息流。在多样化基准测试中，Orion-MSP在有效扩展至高维表格的同时，达到或超越现有最优性能，为高效表格上下文学习确立了新标准。该模型已开源：https://github.com/Lexsi-Labs/Orion-MSP。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.02818) | [arXiv](https://arxiv.org/abs/2511.02818)



---

### 5. TabTune：面向表格基础模型推理与微调的统一算法库

**原文标题：** TabTune: A Unified Library for Inference and Fine-Tuning Tabular
  Foundation Models

**摘要：**
表格基础模型正成为结构化数据学习的重要范式，将大规模预训练的优势扩展至表格数据领域。然而，由于异构的预处理流程、分散的应用程序接口、不一致的微调流程，以及缺乏面向部署的校准度与公平性等标准化评估指标，其实际应用仍受限。本文提出TabTune——通过统一接口标准化表格基础模型完整工作流程的算法库。该库为七种前沿模型提供了一致的调用支持，涵盖零样本推理、元学习、监督微调和参数高效微调等多种适应策略。该框架实现了模型感知的自动化预处理，内部管理架构异构性，并集成了性能指标、校准度与公平性评估模块。TabTune以可扩展性和可复现性为设计原则，能够对表格基础模型的适应策略进行标准化基准测试。本库已开源，访问地址为：https://github.com/Lexsi-Labs/TabTune。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.02802) | [arXiv](https://arxiv.org/abs/2511.02802)



---

### 6. Kinematify：高自由度铰接物体的开放词汇合成

**原文标题：** Kinematify: Open-Vocabulary Synthesis of High-DoF Articulated Objects

**摘要：**
深入理解运动学结构与可动部件对于机器人操控物体及建模自身关节形态至关重要。这类认知通过铰接物体得以呈现，其在物理仿真、运动规划与策略学习等任务中具有核心价值。然而针对高自由度物体的建模仍存在显著挑战。现有方法通常依赖手工标注数据集中的运动序列或强假设条件，制约了方法的可扩展性。本文提出Kinematify自动化框架，可直接基于任意RGB图像或文本描述合成铰接物体。我们的方法攻克了两大核心难题：(i)高自由度物体的运动拓扑结构推断；(ii)基于静态几何的关节参数估计。通过融合蒙特卡洛树搜索的结构推理与几何驱动优化的关节解析，本方法可生成物理一致且功能有效的描述。我们在合成环境与真实场景的多样化输入上对Kinematify进行评估，结果表明其在配准精度与运动拓扑准确性方面均优于现有方法。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.01294) | [arXiv](https://arxiv.org/abs/2511.01294)



---

### 7. MME-CC：面向认知能力的挑战性多模态评估基准

**原文标题：** MME-CC: A Challenging Multi-Modal Evaluation Benchmark of Cognitive
  Capacity

**摘要：**
随着推理模型的快速发展，多模态在人类认知中的核心作用日益凸显，亟需系统探究以视觉为核心的认知行为。然而现有多模态基准或过度侧重文本推理，或未能系统捕捉视觉核心的认知行为，导致对多模态大语言模型认知能力的评估存在不足。为此，我们提出MME-CC（多模态认知能力评估基准），这一基于视觉的基准将11类代表性推理任务划分为空间推理、几何推理与知识推理三大基础类别，从多维度对MLLMs的认知能力进行细粒度解析。基于该基准，我们对16个代表性MLLMs展开大规模实验。研究表明：闭源模型目前整体领先（如Gemini-2.5-Pro得分42.66 vs GLM-4.5V得分30.45），而空间与几何推理能力普遍薄弱（≤30%）。我们进一步识别出常见错误模式，包括方向判断失误、跨视角身份一致性维持脆弱、反事实指令遵循能力差等，并观察到思维链通常遵循“提取-推理-验证”三阶段模式且高度依赖视觉提取。本研究期望推动学界将认知能力作为MLLMs评估与模型设计的核心考量。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.03146) | [arXiv](https://arxiv.org/abs/2511.03146)



---

### 8. LiveTradeBench：运用大语言模型探寻现实世界阿尔法收益

**原文标题：** LiveTradeBench: Seeking Real-World Alpha with Large Language Models

**摘要：**
大语言模型在各类基准测试中表现优异——从知识问答、数学推理到网络智能体任务——但这些测试均处于静态环境，缺乏真实的动态性与不确定性。因此它们评估的是孤立推理或问题解决能力，而非不确定情境下的决策能力。为解决这一问题，我们推出LiveTradeBench，这是一个实时交易环境，用于在真实演进的市场中评估大语言模型智能体。该平台遵循三大设计原则：（一）市场价格与新闻的实时数据流，摆脱对离线回测的依赖，在捕捉实时不确定性的同时防止信息泄露；（二）组合管理抽象框架，将控制范围从单一资产操作扩展至多资产配置，整合风险管理与跨资产推理能力；（三）跨市场评估机制，覆盖波动性、流动性和信息流特征迥异的结构性差异环境——美国股票市场与Polymarket预测市场。在每个决策步，智能体观察价格波动、新闻动态及其投资组合，随后输出平衡风险与收益的资产配置比例。通过LiveTradeBench，我们对21个不同系列的大语言模型进行了为期50天的实盘评估。结果表明：（1）较高的LMArena评分并不等同于卓越的交易表现；（2）不同模型展现出反映风险偏好与推理动态的差异化组合风格；（3）部分大语言模型能有效利用实时信号调整决策。这些发现揭示了静态评估与现实应用能力之间的鸿沟，呼吁建立能够检验持续决策能力与动态不确定环境下稳定性的新基准。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.03628) | [arXiv](https://arxiv.org/abs/2511.03628)



---

### 9. CostBench：动态环境中LLM工具使用代理的多轮成本最优规划与适应性评估

**原文标题：** CostBench: Evaluating Multi-Turn Cost-Optimal Planning and Adaptation in
  Dynamic Environments for LLM Tool-Use Agents

**摘要：**
当前对大语言模型（LLM）智能体的评估主要关注任务完成度，往往忽略资源效率与适应能力。这种忽视导致了一个关键能力的缺失：智能体在动态环境中制定并调整成本最优方案的能力。为填补这一空白，我们推出CostBench——一个可扩展的以成本为中心的基准测试框架，旨在评估智能体的经济推理与重规划能力。该框架基于旅行规划领域构建，包含可通过具有多样化可定制成本的原子工具与复合工具组合求解的任务，同时支持工具故障、成本变动等四类动态阻断事件，以模拟现实世界的不确定性并促使智能体实时调整策略。通过对主流开源与专有模型在CostBench上的测试发现，当前智能体在成本感知规划方面存在显著不足：在静态环境中经常无法识别成本最优方案（即便GPT-5在最困难任务上的精确匹配率也不足75%），而在动态环境中性能进一步下降约40%。通过系统诊断这些缺陷，CostBench为开发兼具经济合理性与鲁棒性的下一代智能体奠定了理论基础。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.02734) | [arXiv](https://arxiv.org/abs/2511.02734)



---

### 10. 序列化优势：在匹配计算量下逆熵投票机制超越并行自一致性方法

**原文标题：** The Sequential Edge: Inverse-Entropy Voting Beats Parallel
  Self-Consistency at Matched Compute

**摘要：**
本研究重新审视语言模型推理的测试时扩展策略，并探讨一个核心问题：在相同令牌预算和计算资源下，是运行多个独立推理链的并行策略更优，还是采用较少推理链通过序列化步骤迭代优化的方案更佳？通过对5个前沿开源模型和3个具有挑战性的推理基准进行全面评估，我们发现采用显式继承先前推理结果的序列化扩展策略在95.6%的实验配置中持续优于主流的并行自一致性范式，准确率提升最高达46.7%。此外，我们提出逆熵加权投票这一无需训练的新方法，进一步强化序列化扩展的准确性。通过按推理链逆熵值比例加权投票，该方法相较于并行多数表决机制实现了成功率提升，确立了其作为最优测试时扩展策略的地位。我们的研究结果从根本上挑战了自Wang等人提出自一致性解码（Wang et al., 2022）以来主导测试时扩展的并行推理范式，将序列化精炼定位为现代大语言模型推理的稳健默认方案，亟需推动推理时优化方法的范式转变。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.02309) | [arXiv](https://arxiv.org/abs/2511.02309)



---

### 11. Jr. AI科学家及其风险报告：基于基准论文的自主科学探索

**原文标题：** Jr. AI Scientist and Its Risk Report: Autonomous Scientific Exploration
  from a Baseline Paper

**摘要：**
理解AI科学家系统的当前能力与风险，对于确保可信赖且可持续的AI驱动科学进步，同时维护学术生态系统的完整性至关重要。为此，我们开发了Jr. AI科学家——一个模拟初级学生研究者核心研究流程的先进自主AI科学家系统：在获得人类导师提供的基准论文后，该系统能分析其局限性，提出改进的创新假设，通过严格实验进行验证，并撰写成果论文。与以往假定全自动化或仅处理小规模代码的方法不同，Jr. AI科学家遵循明确的研究流程，利用现代代码代理处理复杂的多文件实现，最终产生具有科学价值的成果。在评估方面，我们采用AI评审员进行自动评估、作者主导评估，并向专注于AI驱动科学贡献的平台Agents4Science投稿。结果表明，Jr. AI科学家生成的论文评审分数优于现有全自动系统。然而，通过作者评估和Agents4Science评审，我们发现了当前系统的重要局限，这些局限揭示了直接应用现有AI科学家系统的潜在风险及未来研究的关键挑战。最后，我们全面报告了开发过程中识别的各类风险，期望这些发现能深化对AI科学家发展现状与风险的理解。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.04583) | [arXiv](https://arxiv.org/abs/2511.04583)



---

### 12. 非对称对话中的具身误解：MapTask的视角主义标注框架

**原文标题：** Grounded Misunderstandings in Asymmetric Dialogue: A Perspectivist
  Annotation Scheme for MapTask

**摘要：**
协作对话依赖于参与者逐步建立共同基础，但在非对称情境下，参与者可能自认为达成共识却实际指涉不同实体。本文针对HCRC MapTask语料库（Anderson等，1991）提出视角主义标注框架，分别捕捉说话者与受话者对每个指称表达的具身化解读，从而追踪理解形成、分歧产生与修复的动态过程。通过采用框架约束的大语言模型标注流程，我们获得1.3万个带可信度评估的标注指称表达，并解析其理解状态。研究表明：在统一词汇变体后，完全误解现象较为罕见，但多重性差异会系统引发理解分歧，揭示表面共识可能掩盖指称错位的本质。本框架既为研究具身误解提供资源与方法，也为评估（视觉）大语言模型在协作对话中建模视角依存性基础的能力提供分析工具。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.03718) | [arXiv](https://arxiv.org/abs/2511.03718)



---

### 13. 通过自适应查询增强让多模态嵌入器学习何时增强查询

**原文标题：** Let Multimodal Embedders Learn When to Augment Query via Adaptive Query
  Augmentation

**摘要：**
查询增强通过向查询附加额外信息使其更具意义，从而帮助寻找相关文档。当前研究提出了基于大语言模型（LLM）的嵌入器，通过利用LLM的生成能力，以多任务方式同时学习嵌入表示和查询增强生成。在推理过程中，这些联合训练的嵌入器先执行查询增强再进行嵌入，已展现出显著效果。然而，对所有查询进行增强会导致显著的嵌入延迟，且某些查询的增强反而会损害性能。此外，现有方法尚未在多模态环境中进行探索。为解决这些问题，我们提出M-Solomon——一种能够自适应决定何时增强查询的通用多模态嵌入器。我们的方法首先在数据集层面将训练数据中的查询划分为两组：一组需要增强的查询，另一组无需增强的查询。随后，我们引入合成过程，通过强大的多模态大语言模型（MLLM）为需要增强的查询生成合适的增强内容。接着，我们提出自适应查询增强机制。通过该步骤，M-Solomon能够仅对需要增强的查询学习生成带有/augment前缀的合成增强内容，对其他查询则生成简单字符串/embed，从而实现按需增强。实验结果表明，M-Solomon不仅大幅超越无增强的基线模型，其性能也优于持续使用增强的基线方法，同时显著降低了嵌入延迟。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.02358) | [arXiv](https://arxiv.org/abs/2511.02358)



---


## 🔍 关键词云图
![关键词云图](../images/keywords_wordcloud.png)

## 📈 近期论文趋势
![论文趋势](../images/daily_papers.png)

## 🎙️ 语音播报
- [收听今日论文解读](../audio/2025-11-06_daily_papers.mp3)

## 📱 订阅渠道
- GitHub: [hf-daily-paper-newsletter-chinese](https://github.com/2404589803/hf-daily-paper-newsletter-chinese)