<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Hugging Face 论文日报 - 2025-11-12</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', sans-serif;
            line-height: 1.6;
            color: #333;
            background-color: #f5f5f5;
            padding: 20px;
        }
        
        .container {
            max-width: 900px;
            margin: 0 auto;
            background-color: white;
            padding: 40px;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }
        
        h1 {
            color: #2c3e50;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
            margin-bottom: 30px;
            font-size: 28px;
        }
        
        h1 img {
            vertical-align: middle;
            margin-right: 10px;
        }
        
        h2 {
            color: #34495e;
            margin-top: 40px;
            margin-bottom: 20px;
            font-size: 24px;
            border-left: 4px solid #3498db;
            padding-left: 15px;
        }
        
        h3 {
            color: #2c3e50;
            margin-top: 30px;
            margin-bottom: 15px;
            font-size: 20px;
        }
        
        p {
            margin-bottom: 15px;
            text-align: justify;
        }
        
        ul {
            margin-left: 20px;
            margin-bottom: 20px;
        }
        
        li {
            margin-bottom: 8px;
        }
        
        a {
            color: #3498db;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        strong {
            color: #2c3e50;
            font-weight: 600;
        }
        
        hr {
            border: none;
            border-top: 1px solid #e0e0e0;
            margin: 30px 0;
        }
        
        /* 关键修复:限制图片宽度 */
        img {
            max-width: 100%;
            height: auto;
            display: block;
            margin: 20px auto;
            border-radius: 4px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        
        /* 确保图片容器也有宽度限制 */
        p img {
            max-width: 100%;
        }
        
        /* 论文详情区域样式 */
        .paper-section {
            background-color: #f9f9f9;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 20px;
        }
        
        /* 统计信息样式 */
        .stats {
            background-color: #e8f4f8;
            padding: 15px;
            border-radius: 6px;
            margin-bottom: 20px;
        }
        
        /* 响应式设计 */
        @media (max-width: 768px) {
            .container {
                padding: 20px;
            }
            
            h1 {
                font-size: 24px;
            }
            
            h2 {
                font-size: 20px;
            }
            
            h3 {
                font-size: 18px;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <h1><img src="https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo.png" width="30"/> Hugging Face 2025-11-12 论文日报</h1>
<h2>📊 今日论文统计</h2>
<ul>
<li>总论文数：15</li>
<li>热门领域：RL, GPT, LLM</li>
</ul>
<h2>📝 论文详情</h2>
<h3>1. 基于人类示范的计算机使用代理系统基础构建</h3>
<p><strong>原文标题：</strong> Grounding Computer Use Agents on Human Demonstrations</p>
<p><strong>摘要：</strong>
构建可靠的计算机使用代理需要实现精准的基础关联：将自然语言指令准确对应至正确的屏幕元素。尽管网络和移动交互领域存在大规模数据集，但针对桌面环境的高质量资源仍然有限。为填补这一空白，我们推出GroundCUA——一个基于专家人类示范构建的大规模桌面基础关联数据集。该数据集涵盖12个类别下的87种应用程序，包含5.6万张屏幕截图，每个屏幕元素均经过精细标注，累计获得超过356万条人工验证的标注数据。基于这些示范，我们生成涵盖各类现实任务场景的多样化指令，为模型训练提供高质量数据支撑。利用GroundCUA数据集，我们开发了GroundNext系列模型，能够将指令映射至目标用户界面元素。在30亿和70亿参数规模下，通过监督微调，GroundNext在五项基准测试中均取得最先进成果，且所需训练数据量不足前人工作的十分之一。强化学习后训练进一步提升了模型性能，在OSWorld基准测试中配合o3规划器进行智能体评估时，GroundNext取得了与使用更大量数据训练的模型相当或更优的结果。这些发现证实了由专家驱动的高质量数据集在推进通用计算机使用代理发展中的关键作用。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.07332">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.07332">arXiv</a></p>
<hr />
<h3>2. 小模型，大逻辑：多样性驱动优化激发VibeThinker-1.5B的大模型推理能力</h3>
<p><strong>原文标题：</strong> Tiny Model, Big Logic: Diversity-Driven Optimization Elicits Large-Model
  Reasoning Ability in VibeThinker-1.5B</p>
<p><strong>摘要：</strong>
本文通过提出的频谱-信号原则（SSP）开发出具有15亿参数的稠密模型VibeThinker-1.5B，对“小模型天然缺乏强推理能力”的主流共识提出挑战。该方法突破了当前通过扩大模型参数提升能力的主流范式（如DeepSeek R1的6710亿参数、Kimi k2的万亿级参数）。SSP框架首先采用两阶段多样性探索蒸馏（SFT）生成广谱解决方案，再通过最大熵引导策略优化（RL）强化正确信号。在总训练成本仅7800美元的情况下，VibeThinker-1.5B展现出优于闭源模型Magistral Medium和Claude Opus 4的推理能力，并与开源模型GPT OSS-20B Medium性能相当。值得注意的是，在三个数学基准测试中超越了参数量400倍以上的DeepSeek R1：AIME24（80.3对79.8）、AIME25（74.4对70.0）和HMMT25（50.4对41.7）。相较于其基础模型（原成绩分别为6.7、4.3和0.6）实现显著提升。在LiveCodeBench V6测试中获得51.1分，超越Magistral Medium的50.3分及其基础模型的0分。这些发现证明小模型同样能实现与大模型相媲美的推理能力，大幅降低训练与推理成本，为先进AI研究的普及化开辟了新路径。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.06221">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.06221">arXiv</a></p>
<hr />
<h3>3. 对话系统中自适应多智能体响应优化机制研究</h3>
<p><strong>原文标题：</strong> Adaptive Multi-Agent Response Refinement in Conversational Systems</p>
<p><strong>摘要：</strong>
大语言模型在对话系统中通过生成类人响应已取得显著成就，但在需要体现个性化或特定知识的情境下仍存在不足。现实场景中依赖用户主动识别错误并请求重新生成响应显然不具可行性。针对此问题，可在返回响应前对其进行优化改进。现有研究方法主要聚焦于单一模型内部的响应优化，难以兼顾有效对话所需的多元维度。本研究提出基于多智能体框架的响应优化方法，通过为每个维度分配特定角色的智能体实现协同优化。我们聚焦对话质量的三个核心维度：事实准确性、个性化适配与逻辑连贯性。每个智能体负责其中一个维度的审查与优化，并通过反馈融合机制提升整体响应质量。为增强智能体间协作，我们引入动态通信策略——该方法摒弃固定执行序列，根据具体查询需求自适应选择并协调最相关的智能体。我们在具有挑战性的对话数据集上验证本框架，结果表明该方案显著优于现有基线模型，在涉及知识检索、用户画像或二者兼具的任务中表现尤为突出。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.08319">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.08319">arXiv</a></p>
<hr />
<h3>4. Wasm：构建结构化阿拉伯语交错多模态语料库的流程体系</h3>
<p><strong>原文标题：</strong> Wasm: A Pipeline for Constructing Structured Arabic Interleaved
  Multimodal Corpora</p>
<p><strong>摘要：</strong>
大语言模型（LLMs）与大模态模型（LMMs）的性能高度依赖于其预训练数据集的质量与规模。最新研究表明，在自然文档（图文交错编排）上训练的大模态模型，通过利用先进的预训练模型强化语义对齐、图像序列一致性和文本连贯性，在广泛基准测试中的表现优于仅基于图文对训练的模型。然而对于阿拉伯语而言，由于缺乏保持文档结构的高质量多模态数据集，其发展进程受到限制。本文提出Wasm处理流程，通过对Common Crawl数据集进行加工，构建了首个提供Markdown输出的阿拉伯语多模态数据集。与现有仅关注文本提取的阿拉伯语语料库不同，我们的方法在保持网页内容结构完整性的同时，兼具纯文本与多模态预训练场景的灵活性。我们通过全面的对比分析，将本数据处理流程与现有主流数据集的处理方法进行对照，既揭示了过滤策略的共性特征，也论证了我们特定设计决策的合理性。为支持后续研究，我们公开发布了具有代表性的数据集样本及完整的阿拉伯语多模态处理流程。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.07080">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.07080">arXiv</a></p>
<hr />
<h3>5. KLASS：基于KL散度的掩码扩散模型快速推理方法</h3>
<p><strong>原文标题：</strong> KLASS: KL-Guided Fast Inference in Masked Diffusion Models</p>
<p><strong>摘要：</strong>
掩码扩散模型在语言生成等多项任务中已展现出卓越性能。然而受迭代优化机制制约，其推理过程常受限于缓慢且固定的采样速度。为解决此问题，我们提出"KL自适应稳定采样法"（KLASS），该高效采样方法通过利用词元级KL散度识别稳定高置信度预测结果。该方法无需额外模型训练即可在每轮迭代中同步解掩多个词元，在保证生成质量的同时显著提升生成速度。在推理基准测试中，KLASS在超越标准贪心解码性能的同时实现了最高2.78倍的实际加速，在基于扩散的采样器中达到最优水平。我们进一步在文本、图像及分子生成等多领域验证KLASS的有效性，证明其可作为适用于不同模型的通用采样器。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.05664">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.05664">arXiv</a></p>
<hr />
<h3>6. VideoSSR：视频自监督强化学习框架</h3>
<p><strong>原文标题：</strong> VideoSSR: Video Self-Supervised Reinforcement Learning</p>
<p><strong>摘要：</strong>
可验证奖励强化学习（RLVR）显著提升了多模态大语言模型（MLLMs）的视频理解能力。然而，现有视频数据集的复杂度已难以匹配MLLMs的快速发展，而人工标注高质量新数据的成本依然居高不下。本研究探索了一个关键问题：能否利用视频内蕴的丰富信息自生成高质量可验证训练数据？为此，我们提出三种自监督预训练任务：异常定位、物体计数与时序拼图，并构建视频内在理解基准（VIUBench）验证任务难度。实验表明当前最先进的MLLMs在这些任务上表现显著不足。基于这些预训练任务，我们构建了VideoSSR-30K数据集，并提出VideoSSR——一种面向RLVR的新型视频自监督强化学习框架。在涵盖四大视频领域（通用视频问答、长视频问答、时序定位和复杂推理）的17个基准测试中，大量实验表明VideoSSR能持续提升模型性能，平均改进幅度超过5%。这些成果确立了VideoSSR作为推动MLLMs视频理解能力进阶的有效基础框架。代码已开源：https://github.com/lcqysl/VideoSSR。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.06281">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.06281">arXiv</a></p>
<hr />
<h3>7. 超越英语：基于大语言模型构建包容可扩展的多语言机器翻译系统</h3>
<p><strong>原文标题：</strong> Beyond English: Toward Inclusive and Scalable Multilingual Machine Translation with LLMs</p>
<p><strong>摘要：</strong>
大语言模型显著推动了多语言机器翻译的发展，但广泛的语言覆盖、稳定的翻译质量以及以英语为中心的处理倾向仍是待解难题。为应对这些挑战，我们推出LMT系列模型——一个以中英双语为核心的大规模多语言翻译系统，涵盖60种语言及234个翻译方向。在研发过程中，我们发现了一种被忽视的方向性退化现象：对称多语微调数据过度强调反向翻译（其他语言至英语/中文），导致过多多对一映射并降低翻译质量。为此我们提出策略性降采样方法，通过简单而有效的技术手段缓解此退化现象。同时，我们设计了并行多语言提示机制，利用类型学相关的辅助语言增强跨语言迁移能力。通过严格的数据筛选与优化的适配策略，LMT在同等语言覆盖规模的模型中实现最优性能，其中40亿参数模型（LMT-60-4B）显著超越参数量更大的Aya-101-13B和NLLB-54B模型。我们发布四种规模版本（6亿/17亿/40亿/80亿）以推动包容可扩展的高质量多语言机器翻译研究，并为该领域提供强力基准\href{https://github.com/NiuTrans/LMT}{https://github.com/NiuTrans/LMT}。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.07003">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.07003">arXiv</a></p>
<hr />
<h3>8. 未择之路：RLVR可证明性偏离主成分学习机制研究</h3>
<p><strong>原文标题：</strong> The Path Not Taken: RLVR Provably Learns Off the Principals</p>
<p><strong>摘要：</strong>
可验证奖励强化学习（RLVR）能持续提升大语言模型的推理性能，但其似乎仅修改极少部分参数。我们重新审视这一悖论，发现稀疏性实为模型条件优化偏差的表象：对于固定预训练模型，参数更新始终集中于偏好区域，该现象在不同实验间高度一致，且对数据集与RL配方保持基本不变。我们通过三重门控理论机制性解释该动态：门控I（KL锚点）施加KL约束的梯度更新；门控II（模型几何）将更新方向从主成分轴引导至低曲率、谱保持子空间；门控III（精度掩码）将非偏好区域的微观更新隐藏，使得偏离主成分的偏差呈现为稀疏性。我们验证该理论并首次实现RLVR学习动态的参数级刻画：RLVR在权重空间中沿非主成分方向学习，通过最小化谱漂移、减少主成分子空间旋转及实现非主成分更新对齐获得增益。相比之下，监督微调（SFT）以主成分权重为目标，扭曲频谱特征，其效果甚至落后于RLVR。</p>
<p>综合而言，这些研究首次从参数空间视角阐释RLVR的训练动态，揭示了参数演化过程中的清晰规律。关键的是，我们证明RL运行在不同于SFT的优化机制中，因此直接沿用SFT时代的参数高效微调（PEFT）方法存在缺陷——我们针对先进稀疏微调及LoRA变体的案例研究证实了这一点。本研究旨在为理解RLVR的白盒化机制指明方向，推动几何感知的RLVR原生算法设计，而非简单沿用SFT时代的启发式方法。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.08567">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.08567">arXiv</a></p>
<hr />
<h3>9. 超越事实检索：基于生成式语义工作区的RAG情景记忆框架</h3>
<p><strong>原文标题：</strong> Beyond Fact Retrieval: Episodic Memory for RAG with Generative Semantic Workspaces</p>
<p><strong>摘要：</strong>
大语言模型在长上下文推理中面临根本性挑战：许多文档超出其有限上下文窗口，而适配文本的性能随序列长度增加而衰减，亟需通过外部记忆框架进行增强。现有解决方案从语义嵌入检索演进至更复杂的结构化知识图谱表示，虽提升了意义构建与关联能力，但仅适用于基于事实的检索，无法构建时空锚定的叙事表征以追踪情景事件中的实体。为弥补这一缺陷，我们提出生成式语义工作区——一种受神经科学启发的生成式记忆框架，能构建演化情境的结构化可解释表征，使大语言模型能够对动态角色、行为及时空语境进行推理。该框架包含将观测数据映射为中间语义结构的操作器，以及将这些结构整合至保障时间、空间与逻辑一致性的持久工作区的协调器。在包含10万至100万标记语料库的情景记忆基准测试中，本框架相较现有基于RAG的基线模型性能提升达20%。此外，该框架具有高效性，相比次优基线模型可减少51%的查询时上下文标记，显著降低推理时间成本。更广泛而言，本框架为赋予大语言模型类人情景记忆提供了具体蓝图，为实现长周期推理的智能体开辟了新路径。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.07587">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.07587">arXiv</a></p>
<hr />
<h3>10. DynaAct：动态动作空间下的大语言模型推理研究</h3>
<p><strong>原文标题：</strong> DynaAct: Large Language Model Reasoning with Dynamic Action Spaces</p>
<p><strong>摘要：</strong>
在现代序列决策系统中，构建最优候选动作空间对提升推理效率至关重要。然而现有方法要么依赖缺乏可扩展性的人工定义动作空间，要么采用非结构化空间导致穷举搜索的计算成本过高。本文提出名为DynaAct的创新框架，通过自动构建紧凑动作空间来增强复杂问题解决场景中的序列推理能力。我们的方法首先利用大语言模型从涵盖多样化复杂推理问题的语料库中提取通用模式框架，以此估算完整动作空间的代理表征。随后构建一个子模函数，综合评估候选动作在当前状态下的效用价值及其多样性特征，并采用贪心算法选取最优候选集合。在六个多样化标准基准上的大量实验表明，本方法在保持高效推理且未引入显著延迟的同时，显著提升了整体性能。实现代码已发布于https://github.com/zhaoxlpku/DynaAct。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.08043">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.08043">arXiv</a></p>
<hr />
<h3>11. 瓦特智能：衡量本地人工智能的智能效率</h3>
<p><strong>原文标题：</strong> Intelligence per Watt: Measuring Intelligence Efficiency of Local AI</p>
<p><strong>摘要：</strong>
当前大型语言模型的查询处理主要依赖于集中式云基础设施中的前沿模型。快速增长的算力需求使该模式面临巨大压力，云服务商难以同步扩展基础设施规模。两项技术进展促使我们重新审视这一模式：小型语言模型（活跃参数≤200亿）已在多项任务中达到与前沿模型相当的性能水平，而本地加速器（如苹果M4 Max）能以交互级延迟运行这些模型。这引出一个关键问题：本地推理能否有效分流集中式基础设施的算力需求？解答该问题需要从两个维度进行衡量：本地模型能否准确响应真实场景查询，以及能否在功耗受限设备（如笔记本电脑）上保持足够能效以实现实用化。我们提出“瓦特智能”这一指标——即任务准确率与单位功耗的比值，用于评估不同模型-加速器组合的本地推理能力与能效。我们开展了大规模实证研究，覆盖20余个前沿本地语言模型、8类加速器，以及具有代表性的百万级真实场景单轮对话与推理查询。针对每个查询，我们精确测量了准确率、能耗、延迟与功率参数。研究得出三项关键发现：首先，本地模型能准确应答88.7%的单轮对话与推理查询，准确率随领域不同存在差异；其次，2023至2025年间，瓦特智能指标提升5.3倍，本地查询覆盖率从23.2%增至71.3%；最后，运行相同模型时，本地加速器的瓦特智能指标较云加速器至少降低1.4倍，显示存在显著优化空间。这些发现证明本地推理能有效分流集中式基础设施的算力需求，而瓦特智能可作为追踪这一转型进程的关键指标。我们同步开源了瓦特智能分析工具包，为系统化能效基准测试提供支持。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.07885">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.07885">arXiv</a></p>
<hr />
<h3>12. BiCA：基于引文感知困难负样本的生物医学稠密检索有效方法</h3>
<p><strong>原文标题：</strong> BiCA: Effective Biomedical Dense Retrieval with Citation-Aware Hard Negatives</p>
<p><strong>摘要：</strong>
困难负样本对于训练高效检索模型至关重要。困难负样本挖掘通常依赖于使用基于余弦距离等相似性度量的交叉编码器或静态嵌入模型对文档进行排序。由于难以区分源文档与困难负样本文档，生物医学和科学领域的困难负样本挖掘面临挑战。然而，被引文献天然与源文档具有上下文关联性却非重复内容，这使其成为理想的困难负样本。本研究提出BiCA：基于引文感知困难负样本的生物医学稠密检索方法，通过利用20,000篇PubMed文献中的引文链接进行困难负样本挖掘，以优化特定领域的小型稠密检索器。我们使用这些引文指导的负样本对GTE_small和GTE_Base模型进行微调，在BEIR基准的领域内和领域外任务中，基于nDCG@10指标的零样本稠密检索性能均获得持续提升，并在LoTTE数据集的长尾主题任务中使用Success@5指标超越基线模型。研究结果揭示了利用文档链接结构生成高信息量负样本的潜力，仅需少量微调即可实现前沿性能，为高数据效率的领域自适应提供了可行路径。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.08029">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.08029">arXiv</a></p>
<hr />
<h3>13. 软件开发中大型语言模型的平衡之道：从业者视角</h3>
<p><strong>原文标题：</strong> Walking the Tightrope of LLMs for Software Development: A Practitioners' Perspective</p>
<p><strong>摘要：</strong>
背景：大型语言模型的出现可能引发软件开发领域的革命（例如流程自动化、劳动力转型）。尽管已有研究开始探讨LLM对软件开发的感知影响，但仍需通过实证研究来理解如何平衡使用LLM带来的正向与负向效应。目标：我们从软件开发者的视角出发，研究LLM如何影响软件开发及其管理策略。方法：在2024年10月至2025年9月期间，通过三轮数据收集与分析，对22位软件从业者进行访谈。采用社会技术扎根理论（STGT）对访谈数据进行严谨分析。结果：我们识别出在个体、团队、组织和社会层面使用LLM的益处（如维持开发流程、改善开发者心智模型、促进创新实践）与弊端（如对开发者个性的负面影响及声誉损害），并总结了LLM落地的最佳实践。结论：我们重点揭示了软件从业者、团队及组织在运用LLM时面临的权衡关系。本研究结论特别适用于软件开发团队负责人和IT管理者评估LLM在其特定环境中的适用性。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.06428">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.06428">arXiv</a></p>
<hr />
<h3>14. 基于对齐模型协作的多样性与质量优化方法</h3>
<p><strong>原文标题：</strong> Optimizing Diversity and Quality through Base-Aligned Model Collaboration</p>
<p><strong>摘要：</strong>
对齐技术虽然显著提升了大语言模型的输出质量，却以牺牲多样性为代价，导致生成内容高度趋同。我们提出基模型对齐协作框架（BACo），该推理阶段词元级模型协作框架通过动态融合基模型与其对齐版本，实现多样性与质量的协同优化。受前沿研究启发（Fei等人，2025），BACo采用路由策略，根据下一词元预测不确定性和预测内容的语义角色，逐词元确定解码来源模型。现有提升多样性的方法（如重训练、提示工程、多采样等）虽能增强多样性，但往往导致质量下降或需要高昂的解码/后训练成本。相较而言，BACo在单次推理中即可实现高质量的多样性与质量平衡，并具备显著的可控性。我们在三类开放生成任务中系统评估了多种路由策略，涵盖13项多样性与质量指标。实验表明BACo持续超越最先进的推理阶段基线方法，采用最优路由策略时可实现多样性与质量综合指标21.3%的提升。人工评估结果与自动指标高度一致。本研究证实基模型与对齐模型间的协作能有效优化并控制生成内容的多样性与质量。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.05650">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.05650">arXiv</a></p>
<hr />
<h3>15. TimeSearch-R：基于自验证强化学习的自适应时序搜索实现长视频理解</h3>
<p><strong>原文标题：</strong> TimeSearch-R: Adaptive Temporal Search for Long-Form Video Understanding via Self-Verification Reinforcement Learning</p>
<p><strong>摘要：</strong>
时序搜索旨在根据给定查询从数万帧视频中识别最小相关帧集合，为精准的长视频理解奠定基础。现有研究尝试逐步缩小搜索范围，但这类方法通常依赖人工设计的搜索流程，缺乏对最优搜索策略的端到端优化。本文提出TimeSearch-R框架，将时序搜索重构为交错式文本-视频思维过程，通过强化学习将视频片段搜索无缝集成到推理流程中。然而，将群体相对策略优化等强化学习训练方法应用于视频推理时，会导致无监督的中间搜索决策，进而引发视频内容探索不足与逻辑推理不一致的问题。为此，我们提出带完整性自验证的群体相对策略优化方法，通过收集交错推理过程中的已搜索视频帧，并利用同一策略模型验证已搜索帧的充分性，从而提升视频推理的完整性。此外，我们构建了专门用于完整性自验证群体相对策略优化的指令微调冷启动与强化学习训练数据集，通过筛选时序依赖性弱的样本增强任务难度，提升时序搜索能力。大量实验表明，TimeSearch-R在Haystack-LVBench和Haystack-Ego4D等时序搜索基准，以及VideoMME和MLVU等长视频理解基准上均取得显著提升。值得注意的是，TimeSearch-R在LongVideoBench上创造了最新纪录，较基础模型Qwen2.5-VL提升4.1%，较先进视频推理模型Video-R1提升2.0%。代码已开源：https://github.com/Time-Search/TimeSearch-R。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.05489">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.05489">arXiv</a></p>
<hr />
<h2>🔍 关键词云图</h2>
<p><img alt="关键词云图" src="../images/keywords_wordcloud.png" /></p>
<h2>📈 近期论文趋势</h2>
<p><img alt="论文趋势" src="../images/daily_papers.png" /></p>
<h2>🎙️ 语音播报</h2>
<ul>
<li><a href="../audio/2025-11-12_daily_papers.mp3">收听今日论文解读</a></li>
</ul>
<h2>📱 订阅渠道</h2>
<ul>
<li>GitHub: <a href="https://github.com/2404589803/hf-daily-paper-newsletter-chinese">hf-daily-paper-newsletter-chinese</a></li>
</ul>
    </div>
</body>
</html>