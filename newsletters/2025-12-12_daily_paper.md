
# <img src="https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo.png" width="30"/> Hugging Face 2025-12-12 论文日报

## 📊 今日论文统计
- 总论文数：25
- 热门领域：LLM, RL, Transformer, GPT

## 📝 论文详情


### 1. T-pro 2.0：一种高效的俄语混合推理模型与实验平台

**原文标题：** T-pro 2.0: An Efficient Russian Hybrid-Reasoning Model and Playground

**摘要：**
本文介绍T-pro 2.0，一个用于混合推理与高效推理的开源权重俄语大语言模型。该模型支持直接回答与推理轨迹生成，通过采用西里尔字母密集型分词器及适配的EAGLE推测解码流水线以降低推理延迟。为促进可复现与可扩展的研究，我们在Hugging Face平台公开了模型权重、T-Wix 50万条指令数据集、T-Math推理基准测试集及EAGLE权重。这些资源使用户能够深入研究俄语推理任务，并可对模型及推理流水线进行扩展与适配。公开的网页演示展示了推理与非推理两种模式，并呈现了我们的推理架构在多领域实现的加速效果。T-pro 2.0由此成为一个易于使用的开放系统，可用于构建和评估高效、实用的俄语大语言模型应用。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.10430) | [arXiv](https://arxiv.org/abs/2512.10430)



---

### 2. 面向奥赛级数学问题求解的长程推理智能体

**原文标题：** Long-horizon Reasoning Agent for Olympiad-Level Mathematical Problem Solving

**摘要：**
大型语言模型（LLM）通过基于可验证奖励的强化学习（RLVR）在解决复杂推理任务方面取得了显著进展。这一进步同样离不开可靠验证器提供的自动化监督。然而，当前基于结果的验证器（OV）无法有效检测长链思维推理（CoT）中不可靠的中间步骤。同时，现有基于过程的验证器（PV）受限于人工标注成本高昂导致的高质量标注数据稀缺，难以可靠地识别复杂长链CoT中的错误。为此，我们提出基于结果的过程验证器（OPV），该验证器通过检验长链CoT中总结性结果的推理过程，实现精准高效的验证并支持大规模标注。为增强该验证器的能力，我们采用结合专家标注的迭代式主动学习框架，以较低标注成本逐步提升OPV的验证性能。具体而言，在每轮迭代中，当前最优OPV预测最不确定的案例将由专家标注，随后通过拒绝微调（RFT）和RLVR训练新一代OPV用于后续轮次。大量实验证明OPV具有卓越性能与广泛适用性：在内部基准测试\thisbench 中取得最新最优结果，以83.1的F1分数超越Qwen3-Max-Preview等更大规模开源模型（其F1分数为76.3）；同时，OPV能有效检测合成数据集中的误判案例，其判断与专家评估高度一致。在与策略模型协同工作时，OPV持续带来性能提升，例如在计算资源扩展条件下，将DeepSeek-R1-Distill-Qwen-32B在AIME2025上的准确率从55.2%提升至73.3%。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.10739) | [arXiv](https://arxiv.org/abs/2512.10739)



---

### 3. 文本到三维生成是否已为强化学习做好准备？一项渐进式探究

**原文标题：** Are We Ready for RL in Text-to-3D Generation? A Progressive Investigation

**摘要：**
强化学习（RL）先前已被证明在大语言模型和多模态模型中具有显著效果，近期已成功扩展至增强二维图像生成领域。然而，由于三维对象具有更高的空间复杂性——需要全局一致的几何结构与细粒度局部纹理，将强化学习应用于三维生成的研究仍处于探索阶段。这使得三维生成对奖励设计与强化学习算法尤为敏感。为应对这些挑战，我们首次从多个维度对文本到三维自回归生成中的强化学习进行了系统性研究：（1）奖励设计：我们评估了奖励维度与模型选择，证明与人类偏好对齐至关重要，且通用多模态模型能为三维属性提供稳健的信号反馈；（2）强化学习算法：我们研究了GRPO算法的变体，突显了词元级优化的有效性，并进一步探究了训练数据与迭代次数的规模化影响；（3）文本到三维基准测试：针对现有基准无法有效衡量三维生成模型隐含推理能力的问题，我们提出了MME-3DR基准；（4）先进强化学习范式：基于三维生成天然的层次性结构，我们提出Hi-GRPO方法，通过定制化奖励组合优化从全局到局部的层次化三维生成。基于以上发现，我们开发了AR3D-R1模型——首个通过强化学习增强的文本到三维生成系统，实现了从粗糙形状到纹理细节的专家级优化。本研究旨在为强化学习驱动的三维生成推理提供理论洞见与实践参考。代码已发布于 https://github.com/Ivan-Tang-3D/3DGen-R1。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.10949) | [arXiv](https://arxiv.org/abs/2512.10949)



---

### 4. OPV：基于结果的过程验证器——实现高效长链思维验证的新方法

**原文标题：** OPV: Outcome-based Process Verifier for Efficient Long Chain-of-Thought Verification

**摘要：**
大型语言模型（LLM）通过可验证奖励的强化学习（RLVR）在解决复杂推理任务方面取得了显著进展，这一进步同样离不开可靠验证器的自动化监督。然而，当前基于结果的验证器（OV）难以审查长链思维（CoT）推理中不可靠的中间步骤；而基于过程的验证器（PV）由于人工标注成本高昂导致高质量标注稀缺，难以可靠地检测复杂长链思维中的错误。为此，我们提出基于结果的过程验证器（OPV），该方法通过验证长链思维中总结性结果的推理过程，实现精准高效的验证并支持大规模标注。为增强该验证器的能力，我们采用结合专家标注的迭代式主动学习框架，以较低标注成本逐步提升OPV的验证性能。具体而言，在每轮迭代中，当前最优OPV预测最不确定的案例由专家标注，随后通过拒绝微调（RFT）和RLVR训练新一代OPV用于下一轮验证。大量实验证明了OPV的卓越性能与广泛适用性：在自建的OPV-Bench测试集上取得最新最优结果，F1分数达83.1，显著优于Qwen3-Max-Preview等更大规模开源模型（76.3分）；同时，OPV能有效检测合成数据集中的误报案例，其判断与专家评估高度一致。在与策略模型协同工作时，OPV持续带来性能提升，例如在AIME2025任务中，随着计算预算增加，成功将DeepSeek-R1-Distill-Qwen-32B的准确率从55.2%提升至73.3%。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.10756) | [arXiv](https://arxiv.org/abs/2512.10756)



---

### 5. 通过复杂度提升强化学习实现奥林匹克水平的几何大语言模型智能体

**原文标题：** Achieving Olympia-Level Geometry Large Language Model Agent via Complexity Boosting Reinforcement Learning

**摘要：**
大语言模型智能体展现出强大的数学问题解决能力，甚至能够在形式化证明系统的辅助下解决国际数学奥林匹克竞赛级别的几何问题。然而，由于在辅助构造方面的启发式能力较弱，几何问题求解领域仍由专家模型主导，例如AlphaGeometry 2，其训练和评估严重依赖大规模数据合成与搜索。本研究首次尝试构建具有奖牌获得者水平的几何大语言模型智能体，提出了InternGeometry。该模型通过迭代提出命题与辅助构造、使用符号引擎进行验证，并基于引擎反馈指导后续提议，从而克服了几何问题中的启发式局限。动态记忆机制使InternGeometry能够针对每个问题与符号引擎进行超过两百次交互。为进一步加速学习，我们提出了复杂度提升强化学习方法，通过在训练阶段逐步增加合成问题的复杂度来优化模型性能。基于InternThinker-32B构建的InternGeometry，在仅使用13K训练样本（仅为AlphaGeometry 2数据量的0.004%）的情况下，成功解决了2000年至2024年50道国际数学奥林匹克几何问题中的44道，超过了金牌获得者的平均得分（40.9分），这证明了大语言模型智能体在专家级几何任务上的潜力。此外，InternGeometry能够针对人类解答中未出现的国际数学奥林匹克问题提出新颖的辅助构造方案。我们将公开模型、数据及符号引擎以支持后续研究。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.10534) | [arXiv](https://arxiv.org/abs/2512.10534)



---

### 6. MoCapAnything：基于单目视频的任意骨架统一三维运动捕捉

**原文标题：** MoCapAnything: Unified 3D Motion Capture for Arbitrary Skeletons from Monocular Videos

**摘要：**
运动捕捉技术如今已支撑起远超数字人范畴的内容创作，但现有流程大多仍局限于特定物种或固定模板。我们将这一局限形式化为类别无关运动捕捉：给定单目视频与任意绑定三维资产作为提示，目标在于重建可直接驱动该特定资产的基于旋转的动画（如BVH格式）。本文提出MoCapAnything——一个参考引导的因子化框架，首先生成三维关节轨迹，再通过约束感知逆向运动学恢复资产专属旋转。该系统包含三个可学习模块与轻量级逆向运动学阶段：（1）参考提示编码器：从资产骨架、网格及渲染图像中提取逐关节查询向量；（2）视频特征提取器：计算稠密视觉描述符并重建粗粒度四维变形网格，以弥合视频空间与关节空间的鸿沟；（3）统一运动解码器：融合多模态线索生成时序连贯的轨迹。我们还构建了包含1038个动作片段的Truebones Zoo数据集，每个片段均提供标准化的骨架-网格-渲染三元组。在领域内基准测试与真实场景视频上的实验表明，MoCapAnything能生成高质量骨骼动画，在异构绑定系统间实现有效的跨物种动作重定向，为任意资产提供了可扩展的提示驱动三维运动捕捉方案。项目页面：https://animotionlab.github.io/MoCapAnything/

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.10881) | [arXiv](https://arxiv.org/abs/2512.10881)



---

### 7. BEAVER：一种高效的大语言模型确定性验证器

**原文标题：** BEAVER: An Efficient Deterministic LLM Verifier

**摘要：**
随着大语言模型从研究原型转向生产系统，从业者通常需要可靠的方法来验证模型输出是否满足特定约束。虽然基于采样的评估方法能够提供模型行为的直观认知，但无法提供严格的理论保证。本文提出BEAVER框架——首个能够计算大语言模型约束满足确定性概率边界的实用系统。针对任意前缀封闭的语义约束，BEAVER通过创新的词汇树和边界数据结构系统性地探索生成空间，在每次迭代中始终保持可证明的严格边界。我们形式化定义了验证问题，证明了方法的可靠性，并在多个前沿大语言模型上对BEAVER进行了正确性验证、隐私验证和安全代码生成任务的评估。在相同计算资源下，BEAVER获得的概率边界比基线方法精确6至8倍，识别出的高风险实例数量增加3至4倍，实现了宽松边界或经验评估无法达成的精确特性刻画与风险评估。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.05439) | [arXiv](https://arxiv.org/abs/2512.05439)



---

### 8. 从宏观到微观：基于视觉语言模型的分子微观空间智能基准测试

**原文标题：** From Macro to Micro: Benchmarking Microscopic Spatial Intelligence on Molecules via Vision-Language Models

**摘要：**
本文提出微观空间智能（MiSI）的概念，即感知与推断不可见微观实体空间关系的能力，这是科学发现的基础。为评估视觉语言模型（VLMs）在该领域的潜力，我们构建了系统化基准测试框架MiSI-Bench。该框架包含超过16.3万个问答对和58.7万张图像，数据源自约4000个分子结构，涵盖九项互补任务，评估能力范围涵盖基础空间变换到复杂关系识别。实验结果表明，当前最先进的VLMs在该基准测试中的表现显著低于人类水平。然而，经过微调的70亿参数模型展现出巨大潜力，甚至在空间变换任务中超越人类表现，而其在氢键识别等科学基础任务中的薄弱表现，凸显了整合显式领域知识对于实现科学通用人工智能的必要性。数据集发布于https://huggingface.co/datasets/zongzhao/MiSI-bench。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.10867) | [arXiv](https://arxiv.org/abs/2512.10867)



---

### 9. VQRAE：面向多模态理解、生成与重建的表征量化自编码器

**原文标题：** VQRAE: Representation Quantization Autoencoders for Multimodal Understanding, Generation and Reconstruction

**摘要：**
在单一标记器中统一多模态理解、生成与重建表征，仍是构建统一模型的核心挑战。现有研究主要尝试在双编码器范式下解决此问题，例如分别使用独立编码器进行理解与生成，或通过对比损失平衡语义表征与低级特征。本文提出VQRAE（表征自编码器的向量量化版本），首次在统一标记器框架内探索统一表征，以生成用于图像理解的连续语义特征和用于视觉生成的离散标记。具体而言，我们基于预训练视觉基础模型构建对称ViT解码器，并采用两阶段训练策略：首先冻结编码器，以像素重建为目标学习高维语义向量量化码本；随后通过自蒸馏约束联合优化编码器。该设计能够在保持多模态理解能力的同时，使语义信息损失可忽略不计，并生成兼容生成任务与细粒度重建的离散标记。此外，我们发现语义编码器量化依赖于高维码本，这与图像重建中常见的低维码本实践形成对比。语义向量量化码本在1536维度下可实现100%的利用率。VQRAE在视觉理解、生成与重建的多个基准测试中展现出竞争力，其离散特性在自回归范式下具有优异的扩展潜力。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.23386) | [arXiv](https://arxiv.org/abs/2511.23386)



---

### 10. 在Veo世界模拟器中评估Gemini机器人策略

**原文标题：** Evaluating Gemini Robotics Policies in a Veo World Simulator

**摘要：**
生成式世界模型在模拟不同环境中视觉运动策略的交互方面具有显著潜力。前沿视频模型能够以可扩展且通用的方式生成逼真的观测结果与环境交互。然而，视频模型在机器人领域的应用主要局限于分布内评估，即评估与训练策略或微调基础视频模型所用场景相似的场景。本研究报告证明，视频模型可适用于机器人策略评估的全场景应用：从评估标称性能到分布外泛化能力，再到探究物理安全与语义安全性。我们提出了一种基于前沿视频基础模型（Veo）构建的生成式评估系统。该系统通过优化设计支持机器人动作条件约束与多视角一致性，同时集成生成式图像编辑与多视角补全技术，能够沿多个泛化维度合成真实场景的逼真变体。实验表明，该系统保留了视频模型的基础能力，可精确模拟经过编辑的场景——包括添加新型交互物体、新颖视觉背景及干扰物体。这种高保真特性使得系统能够准确预测不同策略在标称条件与分布外条件下的相对性能，确定不同泛化维度对策略性能的相对影响，并对策略进行红队测试以暴露违反物理或语义安全约束的行为。我们通过对八种Gemini机器人策略检查点及双手机械臂的五项任务进行1600余次真实世界评估，验证了该系统的各项能力。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.10675) | [arXiv](https://arxiv.org/abs/2512.10675)



---

### 11. 基于自调用智能体的图像思维推理

**原文标题：** Thinking with Images via Self-Calling Agent

**摘要：**
图像思维推理范式通过将视觉信息作为动态元素整合至思维链中，已展现出卓越的视觉推理能力。然而，由于依赖稀缺的高质量推理数据，通过强化学习优化交错式多模态思维链仍面临挑战。本研究提出自调用思维链——一种创新的视觉推理范式，将交错式多模态思维链重构为具有自调用机制的纯语言思维链。具体而言，主智能体将复杂视觉推理任务分解为原子子任务，并调用其虚拟副本（即参数共享的子智能体）在隔离上下文中解决问题。该范式无需显式的模态交错处理，从而显著提升了训练效能与效率。通过采用组间相对策略优化方法，系统能够强化有效推理行为以增强优化效果。在HR-Bench 4K数据集上的实验表明，相较于强基线方法，自调用思维链在减少约75%GPU计算时的同时，将整体推理性能提升最高达1.9%。代码已开源：https://github.com/YWenxi/think-with-images-through-self-calling。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.08511) | [arXiv](https://arxiv.org/abs/2512.08511)



---

### 12. StereoSpace：基于规范空间端到端扩散的无深度立体几何合成方法

**原文标题：** StereoSpace: Depth-Free Synthesis of Stereo Geometry via End-to-End Diffusion in a Canonical Space

**摘要：**
本文提出StereoSpace，一种基于扩散模型的单目到立体合成框架，该框架仅通过视点条件建模几何信息，无需显式深度估计或图像扭曲操作。通过构建规范矫正空间并结合条件引导，生成器能够端到端地推断对应关系并填补遮挡区域。为确保评估的公平性与无信息泄露，我们设计了一套端到端评估协议，在测试阶段完全排除真实几何数据或代理几何估计的介入。该协议重点关注反映下游应用价值的指标：以iSQoE衡量感知舒适度，以MEt3R评估几何一致性。实验表明，StereoSpace在扭曲修复、潜在空间扭曲及扭曲条件生成等类别方法中均取得优势，能够在层叠场景与非朗伯场景中生成清晰视差并保持强鲁棒性。这证实了视点条件扩散模型可作为无需深度信息的可扩展立体生成解决方案。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.10959) | [arXiv](https://arxiv.org/abs/2512.10959)



---

### 13. 无需归一化的更强Transformer模型

**原文标题：** Stronger Normalization-Free Transformers

**摘要：**
尽管归一化层长期以来被视为深度学习架构中不可或缺的组成部分，但动态双曲正切函数（DyT）的提出表明替代方案是存在的。该点态函数通过约束极端值实现稳定收敛，并达到归一化级别的性能；本研究旨在探索能够超越其性能的函数设计。我们首先研究点态函数的内在特性如何影响训练与性能表现。基于这些发现，我们进行了大规模搜索以寻找更有效的函数设计。通过系统探索，我们提出Derf(x) = erf(αx + s)函数，其中erf(x)为缩放后的高斯累积分布函数，并验证其为当前最优设计。在视觉（图像识别与生成）、语音表征及DNA序列建模等多个领域，Derf在性能上均超越层归一化、RMSNorm及DyT方法。研究结果表明，Derf的性能提升主要源于其增强的泛化能力而非拟合能力。其简洁性与卓越性能使Derf成为无需归一化的Transformer架构的理想选择。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.10938) | [arXiv](https://arxiv.org/abs/2512.10938)



---

### 14. FACTS排行榜：大型语言模型事实性综合评测基准

**原文标题：** The FACTS Leaderboard: A Comprehensive Benchmark for Large Language Model Factuality

**摘要：**
本文推出FACTS排行榜——一套在线评测体系及相关基准测试，旨在全面评估语言模型在不同场景下生成事实准确文本的能力。该体系通过整合模型在四个独立子榜单的表现提供整体事实性度量：（1）FACTS多模态榜单，通过图像问答任务评估响应的事实性；（2）FACTS参数化榜单，通过闭卷事实型问题测试模型基于内部参数的世界知识；（3）FACTS搜索榜单，在信息检索场景中评估模型调用搜索API时的事实准确性；（4）FACTS文本锚定榜单（v2版），评估长文本响应是否基于给定文档，其判定模型性能显著提升。各子榜单均采用自动化判定模型对输出进行评分，最终体系得分为四项得分的平均值，从而实现对模型整体事实性的稳健均衡评估。FACTS排行榜体系将持续维护，包含公开与私有数据分割以兼顾开放参与和评测完整性。评测平台地址：https://www.kaggle.com/benchmarks/google/facts。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.10791) | [arXiv](https://arxiv.org/abs/2512.10791)



---

### 15. 工具增强时空推理：面向视频问答任务的高效化处理

**原文标题：** Tool-Augmented Spatiotemporal Reasoning for Streamlining Video Question Answering Task

**摘要：**
视频问答任务作为评估基础模型能否有效感知、理解并推理动态现实场景的关键测试平台。然而，现有的多模态大语言模型在复杂且需要深度推理的视频问答任务中，难以同时建模视频帧内的空间关系并理解时序演化的因果动态。本研究为多模态大语言模型配备了一个全面且可扩展的视频工具包，以增强其时空推理能力，并确保工具数量与多样性之间的协调。为了更好地控制工具调用顺序并避免工具链捷径问题，我们提出了一种时空推理框架，该框架策略性地调度时序与空间工具，从而逐步定位视频中的关键区域。我们的时空推理框架通过轻量级工具增强了GPT-4o的性能，在VideoMME基准上实现了8.2%的性能提升，在LongVideoBench基准上提升了4.6%。我们相信，所提出的视频工具包与时空推理框架为构建自主智能的视频分析助手迈出了重要一步。代码已公开于https://github.com/fansunqi/VideoTool。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.10359) | [arXiv](https://arxiv.org/abs/2512.10359)



---

### 16. H2R-Grounder：一种无需配对数据的范式，用于将人类交互视频转化为物理接地的机器人视频

**原文标题：** H2R-Grounder: A Paired-Data-Free Paradigm for Translating Human Interaction Videos into Physically Grounded Robot Videos

**摘要：**
能够从日常人类视频中学习操作技能的机器人，有望在不依赖繁琐的机器人数据收集的情况下获得广泛的能力。我们提出了一种视频到视频的翻译框架，该框架能将普通的人-物交互视频转换为具有真实、物理接地交互效果且运动一致的机器人操作视频。我们的方法在训练时无需任何配对的人类-机器人视频，仅需一组非配对的机器人视频，这使得系统易于扩展。我们引入了一种可迁移的表征来弥合具身鸿沟：通过在训练视频中对机器人手臂进行修复以获得干净的背景，并叠加一个简单的视觉提示（指示夹爪位置和方向的标记与箭头），我们可以引导生成模型将机器人手臂重新插入场景中。在测试时，我们对人类视频应用相同的过程（修复人物并叠加人体姿态提示），从而生成模仿人类动作的高质量机器人视频。我们以情境学习的方式对最先进的视频扩散模型（Wan 2.2）进行微调，以确保时间连贯性并充分利用其丰富的先验知识。实验结果表明，与基线方法相比，我们的方法能生成显著更真实、更接地的机器人运动，这为从无标注的人类视频中规模化学习机器人技能指出了一个有前景的方向。项目页面：https://showlab.github.io/H2R-Grounder/

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.09406) | [arXiv](https://arxiv.org/abs/2512.09406)



---

### 17. MoRel：基于锚点中继双向混合与层次化致密化的长程无闪烁四维运动建模

**原文标题：** MoRel: Long-Range Flicker-Free 4D Motion Modeling via Anchor Relay-based Bidirectional Blending with Hierarchical Densification

**摘要：**
四维高斯泼溅（4DGS）技术的最新进展将三维高斯泼溅（3DGS）的高速渲染能力扩展至时间域，实现了动态场景的实时渲染。然而，当前仍存在的主要挑战之一在于对包含长程运动的动态视频进行建模——现有方法的简单扩展会导致严重的内存爆炸、时间闪烁，以及无法处理随时间出现或消失的遮挡。为应对这些挑战，我们提出了一种新颖的4DGS框架，其核心为基于锚点中继的双向混合（ARBB）机制，命名为MoRel。该框架能够以内存高效的方式对长程动态场景进行时间一致性建模。我们的方法在关键帧时间索引处逐步构建局部规范锚点空间，并在锚点层级建模帧间形变，从而增强时间连贯性。通过学习关键帧锚点之间的双向形变，并通过可学习的透明度控制进行自适应混合，我们的方法有效缓解了时间不连续性与闪烁伪影。我们进一步提出了一种特征方差引导的层次化致密化（FHD）方案，该方案基于指定的特征方差水平，在保持渲染质量的同时有效致密化关键帧锚点。为有效评估模型处理真实世界长程四维运动的能力，我们新构建了一个包含长程四维运动的数据集，命名为SelfCap_{LR}。与先前的动态视频数据集相比，该数据集具有更大的平均动态运动幅度，并在空间更广的范围内采集。总体而言，我们的MoRel方法在保持内存使用可控的同时，实现了时间连贯且无闪烁的长程四维重建，展现了基于高斯表示的动态场景建模的可扩展性与高效性。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.09270) | [arXiv](https://arxiv.org/abs/2512.09270)



---

### 18. Omni-Attribute：面向视觉概念个性化的开放词汇属性编码器

**原文标题：** Omni-Attribute: Open-vocabulary Attribute Encoder for Visual Concept Personalization

**摘要：**
视觉概念个性化旨在仅将特定图像属性（如身份、表情、光照和风格）迁移至未见过的场景中。然而，现有方法依赖于通用图像编码器提取的整体嵌入表示，这些表示往往纠缠了多种视觉因素，难以分离单一属性，常导致信息泄露与合成结果不一致。为克服这一局限，本文提出Omni-Attribute——首个开放词汇图像属性编码器，旨在学习高保真、属性特定的表征。我们的方法协同设计了数据与模型：（1）通过构建带有正负属性标注的语义关联图像对，显式指导编码器学习应保留或抑制的特征；（2）采用双目标训练范式，在生成保真度与对比解耦之间取得平衡。实验表明，所得嵌入表示能有效支持开放词汇属性检索、个性化及组合生成任务，在多个基准测试中达到了最先进的性能水平。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.10955) | [arXiv](https://arxiv.org/abs/2512.10955)



---

### 19. 孔子代码智能体：工业级开源人工智能软件工程师

**原文标题：** Confucius Code Agent: An Open-sourced AI Software Engineer at Industrial Scale

**摘要：**
现实世界的人工智能软件工程需要能够对海量代码库进行推理、在长会话中保持持久记忆、并在测试阶段稳健协调复杂工具链的编码智能体。现有开源编码智能体虽具透明度，但在应对工业级工作负载时常显不足；而专有编码智能体虽实践性能强劲，却在可扩展性、可解释性与可控性方面存在局限。本文提出孔子代码智能体——一个可在工业级规模运行的开源人工智能软件工程师。该智能体构建于孔子软件开发工具包之上，该开源智能体开发平台围绕三个互补维度设计：智能体体验、用户体验与开发者体验。该工具包引入具备分层工作记忆的统一编排器以支持长上下文推理，配备跨会话持续学习的持久笔记系统，并采用模块化扩展机制保障工具使用的稳健性。此外，元智能体通过“构建-测试-优化”循环自动完成智能体配置的合成、评估与精调，使其能快速适应新任务、新环境与新工具栈。基于孔子软件开发工具包实例化的孔子代码智能体在真实软件工程任务中展现出卓越性能：在SWE-Bench-Pro基准测试中，其Resolve@1指标达到54.3%的领先水平，较现有编码智能体实现显著提升。孔子软件开发工具包与孔子代码智能体共同为人工智能智能体提供了透明、可扩展、可复现的基础框架，弥合了研究原型与生产级系统之间的鸿沟，为工业级智能体的开发与部署提供支撑。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.10398) | [arXiv](https://arxiv.org/abs/2512.10398)



---

### 20. ReViSE：基于自反思学习的统一模型推理感知视频编辑研究

**原文标题：** ReViSE: Towards Reason-Informed Video Editing in Unified Models with Self-Reflective Learning

**摘要：**
视频统一模型在内容理解与生成方面展现出强大能力，但即使配备先进的内部视觉语言模型，其在推理感知的视频编辑任务中仍存在明显局限。我们认为这一差距源于两方面因素：1）现有数据集难以支撑推理感知视频编辑的训练与评估；2）模型推理能力与编辑能力之间存在固有割裂，导致丰富的语义理解无法有效指导编辑过程。弥合这一差距需要构建连接推理与视觉转换的集成框架。为此，我们提出推理感知视频编辑任务，该任务要求编辑过程中综合考虑物理合理性与因果动态关系。为建立系统化评估体系，我们构建了RVE-Bench综合基准数据集，包含两个互补子集：推理感知视频编辑与上下文视频生成，覆盖多维推理场景与现实编辑需求。基于此，我们提出ReViSE自反思推理框架，该框架将生成与评估功能整合于统一架构中。模型通过内部视觉语言模型对编辑后视频是否符合指令逻辑进行内在评估，其产生的差分反馈在训练过程中持续优化生成器的推理行为。在RVE-Bench上的大量实验表明，ReViSE显著提升了编辑准确度与视觉保真度，在推理感知视频编辑子集上的综合得分较现有最优方法提升32%。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.09924) | [arXiv](https://arxiv.org/abs/2512.09924)



---

### 21. Fed-SE：面向隐私受限多环境大语言模型智能体的联邦自进化框架

**原文标题：** Fed-SE: Federated Self-Evolution for Privacy-Constrained Multi-Environment LLM Agents

**摘要：**
大语言模型智能体已广泛应用于复杂的交互式任务，然而隐私约束通常阻碍了其在动态环境中的集中式优化与协同进化。尽管联邦学习在静态数据集上已被证明有效，但其在智能体开放式自进化方面的扩展仍研究不足。直接应用标准联邦学习面临挑战：异构任务以及稀疏的轨迹级奖励会引发严重的梯度冲突，从而破坏全局优化过程的稳定性。为弥合这一差距，本文提出Fed-SE，一个面向大语言模型智能体的联邦自进化框架。Fed-SE构建了“局部进化-全局聚合”范式。在局部，智能体通过对筛选出的高回报轨迹进行参数高效微调，实现稳定的梯度更新。在全局层面，Fed-SE在一个低秩子空间内聚合更新，该子空间能解耦环境特定的动态特性，从而有效减少客户端间的负迁移。在五个异构环境中的实验表明，相较于联邦学习基线方法，Fed-SE将平均任务成功率提升了约18%，验证了其在隐私受限部署场景下实现鲁棒跨环境知识迁移的有效性。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.08870) | [arXiv](https://arxiv.org/abs/2512.08870)



---

### 22. MOA：面向角色扮演智能体的多目标对齐框架

**原文标题：** MOA: Multi-Objective Alignment for Role-Playing Agents

**摘要：**
角色扮演智能体（RPAs）需同时掌握多项相互冲突的技能——遵循多轮次指令、展现领域知识并保持连贯的语言风格。现有研究要么依赖监督微调（SFT）方法（易过度拟合表面特征且生成多样性不足），要么采用强化学习（RL）方法（难以学习多维度指标以实现全面的RPA优化）。本文提出MOA（多目标对齐），一种基于强化学习的框架，能够为通用角色扮演智能体实现多维度、细粒度评估标准的优化。MOA引入创新的多目标优化策略，通过同步训练多个细粒度评估标准以提升优化性能。此外，针对模型输出多样性与质量的挑战，我们采用思维增强推演与离轨策略引导相结合的方法。在PersonaGym和RoleMRC等复杂基准测试上的大量实验表明，MOA能使80亿参数模型在多个维度上达到甚至超越GPT-4o和Claude等强基线模型的性能。这证明了MOA在构建角色扮演智能体方面的巨大潜力，使其能够同时满足角色知识、人物风格、多样化场景及复杂多轮对话的综合需求。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.09756) | [arXiv](https://arxiv.org/abs/2512.09756)



---

### 23. X-Humanoid：大规模人形视频生成——基于人类视频的机器人化转换

**原文标题：** X-Humanoid: Robotize Human Videos to Generate Humanoid Videos at Scale

**摘要：**
具身人工智能的发展为智能人形机器人开辟了巨大潜力。然而，视觉-语言-动作模型与世界模型的进展均受限于大规模多样化训练数据的稀缺。一种可行的解决方案是对网络规模的人类视频进行“机器人化”处理，该方法已在策略训练中被证明有效。但现有方案主要将机械臂“叠加”于第一人称视角视频，无法处理第三人称视频中复杂的全身运动与场景遮挡，因而难以实现人类动作的机器人化转换。为弥补这一空白，我们提出X-Humanoid——一种生成式视频编辑方法，将强大的Wan 2.2模型适配为视频到视频架构，并针对人类到人形机器人的转换任务进行微调。该微调需要成对的人类-人形机器人视频数据，为此我们设计了可扩展的数据生成流程，利用虚幻引擎将社区资源转化为超过17小时的配对合成视频。随后，我们将训练模型应用于60小时的Ego-Exo4D视频数据集，生成并发布了包含超过360万帧“机器人化”人形视频的大规模新数据集。定量分析与用户研究证实了本方法相较于现有基线的优越性：69%的用户认为其在运动连贯性方面表现最佳，62.1%的用户认可其具身形态准确性。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.04537) | [arXiv](https://arxiv.org/abs/2512.04537)



---

### 24. DuetSVG：基于内部视觉引导的统一多模态SVG生成方法

**原文标题：** DuetSVG: Unified Multimodal SVG Generation with Internal Visual Guidance

**摘要：**
近期基于视觉语言模型（VLM）的方法在SVG生成领域取得了显著成果。然而，由于这些方法在解码过程中仅生成文本而缺乏视觉信号，它们往往难以处理复杂语义，且生成的SVG在视觉吸引力与几何一致性方面存在不足。本文提出DuetSVG，一种统一的多模态模型，能够以端到端方式联合生成图像标记与对应的SVG标记。该模型在图像与SVG数据集上进行训练。在推理阶段，我们采用一种新颖的测试时缩放策略，利用模型自身的视觉预测结果作为引导，以提升SVG解码质量。大量实验表明，本方法在多种应用场景中均优于现有方法，能够生成视觉逼真、语义对齐且语法简洁的SVG图形。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.10894) | [arXiv](https://arxiv.org/abs/2512.10894)



---

### 25. DragMesh：轻松实现交互式三维生成

**原文标题：** DragMesh: Interactive 3D Generation Made Easy

**摘要：**
尽管生成模型在创建静态三维内容方面表现出色，但如何构建能够理解物体运动方式及交互响应的系统仍是根本性挑战。当前关节运动生成方法面临两难困境：要么符合物理规律但速度过慢无法实时应用，要么具备生成能力却违背基本运动学约束。本文提出DragMesh——一个围绕轻量级运动生成核心构建的实时交互式三维关节运动鲁棒框架。我们的核心贡献在于新颖的解耦式运动学推理与运动生成框架：首先通过分离语义意图推理（确定关节类型）与几何回归（使用运动学预测网络KPP-Net确定轴线和原点）来推断潜在关节参数；其次，为利用对偶四元数表示刚体运动的紧凑性、连续性和无奇异性优势，我们开发了新型对偶四元数变分自编码器（DQ-VAE）。该DQ-VAE接收预测先验参数与原始用户拖拽指令，生成完整合理的运动轨迹。为确保严格遵循运动学规律，我们通过特征线性调制（FiLM）条件化方法，在DQ-VAE非自回归Transformer解码器的每一层注入关节先验信息。这种持续多尺度引导机制辅以数值稳定的叉积损失函数，共同保障轴线对齐精度。解耦式设计使DragMesh既能实现实时性能，又能在未经重新训练的情况下对新物体生成合理的关节运动，为生成式三维智能迈出实用化一步。代码：https://github.com/AIGeeksGroup/DragMesh。项目网站：https://aigeeksgroup.github.io/DragMesh。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.06424) | [arXiv](https://arxiv.org/abs/2512.06424)



---


## 🔍 关键词云图
![关键词云图](../images/keywords_wordcloud.png)

## 📈 近期论文趋势
![论文趋势](../images/daily_papers.png)

## 🎙️ 语音播报
- [收听今日论文解读](../audio/2025-12-12_daily_papers.mp3)

## 📱 订阅渠道
- GitHub: [hf-daily-paper-newsletter-chinese](https://github.com/2404589803/hf-daily-paper-newsletter-chinese)