<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Hugging Face 论文日报 - 2026-01-07</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', sans-serif;
            line-height: 1.6;
            color: #333;
            background-color: #f5f5f5;
            padding: 20px;
        }
        
        .container {
            max-width: 900px;
            margin: 0 auto;
            background-color: white;
            padding: 40px;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }
        
        h1 {
            color: #2c3e50;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
            margin-bottom: 30px;
            font-size: 28px;
        }
        
        h1 img {
            vertical-align: middle;
            margin-right: 10px;
        }
        
        h2 {
            color: #34495e;
            margin-top: 40px;
            margin-bottom: 20px;
            font-size: 24px;
            border-left: 4px solid #3498db;
            padding-left: 15px;
        }
        
        h3 {
            color: #2c3e50;
            margin-top: 30px;
            margin-bottom: 15px;
            font-size: 20px;
        }
        
        p {
            margin-bottom: 15px;
            text-align: justify;
        }
        
        ul {
            margin-left: 20px;
            margin-bottom: 20px;
        }
        
        li {
            margin-bottom: 8px;
        }
        
        a {
            color: #3498db;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        strong {
            color: #2c3e50;
            font-weight: 600;
        }
        
        hr {
            border: none;
            border-top: 1px solid #e0e0e0;
            margin: 30px 0;
        }
        
        /* 关键修复:限制图片宽度 */
        img {
            max-width: 100%;
            height: auto;
            display: block;
            margin: 20px auto;
            border-radius: 4px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        
        /* 确保图片容器也有宽度限制 */
        p img {
            max-width: 100%;
        }
        
        /* 论文详情区域样式 */
        .paper-section {
            background-color: #f9f9f9;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 20px;
        }
        
        /* 统计信息样式 */
        .stats {
            background-color: #e8f4f8;
            padding: 15px;
            border-radius: 6px;
            margin-bottom: 20px;
        }
        
        /* 响应式设计 */
        @media (max-width: 768px) {
            .container {
                padding: 20px;
            }
            
            h1 {
                font-size: 24px;
            }
            
            h2 {
                font-size: 20px;
            }
            
            h3 {
                font-size: 18px;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <h1><img src="https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo.png" width="30"/> Hugging Face 2026-01-07 论文日报</h1>
<h2>📊 今日论文统计</h2>
<ul>
<li>总论文数：27</li>
<li>热门领域：Transformer, GPT, LLM</li>
</ul>
<h2>📝 论文详情</h2>
<h3>1. InfiniDepth：基于神经隐式场的任意分辨率与细粒度深度估计</h3>
<p><strong>原文标题：</strong> InfiniDepth: Arbitrary-Resolution and Fine-Grained Depth Estimation with Neural Implicit Fields</p>
<p><strong>摘要：</strong>
现有的深度估计方法本质上局限于在离散的图像网格上预测深度。此类表示形式限制了其向任意输出分辨率扩展的能力，并阻碍了几何细节的恢复。本文提出InfiniDepth，该方法将深度表示为神经隐式场。通过一个简单而有效的局部隐式解码器，我们能够在连续的二维坐标处查询深度，从而实现任意分辨率与细粒度的深度估计。为了更好地评估本方法的性能，我们从五款不同的游戏中构建了一个高质量的4K合成基准数据集，涵盖了具有丰富几何与外观细节的多样化场景。大量实验表明，InfiniDepth在相对深度估计和度量深度估计任务中，于合成及真实世界基准数据集上均达到了最先进的性能，尤其在精细细节区域表现优异。该方法还有益于大视角变化下的新视角合成任务，能够生成空洞与伪影更少的高质量结果。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.03252">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.03252">arXiv</a></p>
<hr />
<h3>2. LTX-2：一种高效的联合视听基础模型</h3>
<p><strong>原文标题：</strong> LTX-2: Efficient Joint Audio-Visual Foundation Model</p>
<p><strong>摘要：</strong>
近期的文本到视频扩散模型已能生成引人入胜的视频序列，但这些视频仍处于“静默”状态——缺失了音频所能提供的语义、情感与氛围线索。本文介绍LTX-2，一种能够以统一方式生成高质量、时间同步的视听内容的开源基础模型。LTX-2采用非对称双流Transformer架构，包含一个140亿参数的视频流与一个50亿参数的音频流，二者通过具有时序位置编码的双向视听交叉注意力层以及用于共享时间步条件控制的跨模态AdaLN模块相耦合。该架构在实现统一视听模型高效训练与推理的同时，为视频生成分配了比音频生成更多的参数量。我们采用多语言文本编码器以提升对多样化提示的理解能力，并引入一种模态感知的无分类器引导机制，以增强视听对齐效果与可控性。除生成语音外，LTX-2能够生成丰富、连贯的音频轨道，其内容贴合场景中的人物、环境、风格与情感，并包含自然的背景音与拟声音效。评估结果表明，该模型在开源系统中实现了最先进的视听质量与提示跟随性能，同时以远低于专有模型的计算成本与推理时间，取得了与之相当的生成效果。所有模型权重与代码均已公开发布。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.03233">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.03233">arXiv</a></p>
<hr />
<h3>3. MOSS Transcribe Diarize：具备说话人日志功能的精准转录系统</h3>
<p><strong>原文标题：</strong> MOSS Transcribe Diarize: Accurate Transcription with Speaker Diarization</p>
<p><strong>摘要：</strong>
说话人归属时间戳转录旨在实现语音内容转写并精确判定每位说话人的发言时序，这对会议转录场景尤为重要。现有系统鲜少采用端到端架构，且普遍受限于上下文窗口狭窄、长程说话人记忆能力薄弱以及无法输出时间戳等问题。为突破这些局限，我们提出MOSS Transcribe Diarize——一个统一的多模态大语言模型，以端到端范式联合实现说话人归属时间戳转录。通过海量真实场景数据训练，并配备支持90分钟输入时长的128k上下文窗口，该系统具备优异的扩展能力和鲁棒泛化性能。在全面评估中，其在多个公开及内部基准测试上均超越当前最先进的商业系统。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.01554">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.01554">arXiv</a></p>
<hr />
<h3>4. SciEvalKit：面向科学通用智能的开源评估工具包</h3>
<p><strong>原文标题：</strong> SciEvalKit: An Open-source Evaluation Toolkit for Scientific General Intelligence</p>
<p><strong>摘要：</strong>
本文介绍SciEvalKit——一个统一的基准测试工具包，旨在跨多学科领域与任务能力评估面向科学的人工智能模型。与通用评估平台不同，SciEvalKit聚焦科学智能的核心能力体系，涵盖科学多模态感知、科学多模态推理、科学多模态理解、科学符号推理、科学代码生成、科学假设生成及科学知识理解七大维度。该工具包支持物理学、化学、天文学、材料科学等六大基础科学领域，通过从真实领域特定数据集中遴选构建专家级科学基准，确保评估任务反映实际科学挑战。工具包采用灵活可扩展的评估架构，支持跨模型与数据集的批量评估，允许自定义模型与数据集集成，并提供透明、可复现、可比较的评估结果。通过融合能力导向评估与学科多样性，SciEvalKit为新一代科学基础模型与智能代理的基准测试提供了标准化且可定制的基础设施。本工具包已开源并持续维护，以促进AI4Science领域的社区驱动发展与进步。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.22334">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.22334">arXiv</a></p>
<hr />
<h3>5. UniCorn：通过自生成监督实现自改进的统一多模态模型</h3>
<p><strong>原文标题：</strong> UniCorn: Towards Self-Improving Unified Multimodal Models through Self-Generated Supervision</p>
<p><strong>摘要：</strong>
尽管统一多模态模型在跨模态理解方面取得了显著成功，但其利用内部知识进行高质量生成的能力仍存在明显不足。我们将这种差异形式化为传导性失语现象，即模型能够准确解读多模态输入，却难以将这种理解转化为忠实且可控的合成内容。为解决这一问题，我们提出UniCorn——一个简洁而精巧的自改进框架，无需依赖外部数据或教师监督。通过将单个统一多模态模型划分为提议者、求解者和评判者三个协作角色，UniCorn通过自我博弈生成高质量交互，并运用认知模式重构将潜在理解提炼为显式生成信号。为验证多模态连贯性的恢复效果，我们引入UniCycle基准测试，该测试基于“文本到图像再到文本”的重构循环构建。大量实验表明，在六个通用图像生成基准测试中，UniCorn相比基础模型实现了全面且显著的提升。特别值得注意的是，该方法在TIIF（73.8）、DPG（86.8）、CompBench（88.5）和UniCycle基准上达到最先进性能，同时在WISE和OneIG基准上分别取得+5.0和+6.5的显著增益。这些结果充分证明，我们的方法在保持强大理解能力的同时显著提升了文本到图像生成质量，展现了全自监督优化范式对于统一多模态智能系统的可扩展性。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.03193">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.03193">arXiv</a></p>
<hr />
<h3>6. NitroGen：面向通用游戏智能体的开放基础模型</h3>
<p><strong>原文标题：</strong> NitroGen: An Open Foundation Model for Generalist Gaming Agents</p>
<p><strong>摘要：</strong>
本文介绍NitroGen——一个面向通用游戏智能体的视觉-动作基础模型，该模型基于超过1,000款游戏、总计40,000小时的游戏录像进行训练。我们整合了三个关键要素：1）通过自动提取公开游戏录像中的玩家操作构建的网络级视频-动作数据集；2）能够衡量跨游戏泛化能力的多游戏基准测试环境；3）采用大规模行为克隆训练的统一视觉-动作模型。NitroGen在多个领域展现出卓越能力，包括3D动作游戏的战斗场景、2D平台游戏的高精度操控以及程序生成世界的探索任务。该模型能有效迁移至未见过的游戏，相比从头训练的模型在任务成功率上最高可获得52%的相对提升。我们公开了数据集、评估工具集和模型权重，以推动通用具身智能体的相关研究。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.02427">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.02427">arXiv</a></p>
<hr />
<h3>7. SOP：一种面向视觉-语言-动作模型的可扩展在线后训练系统</h3>
<p><strong>原文标题：</strong> SOP: A Scalable Online Post-Training System for Vision-Language-Action Models</p>
<p><strong>摘要：</strong>
视觉-语言-动作模型通过大规模预训练实现了强大的泛化能力，但在实际部署中，除了广泛通用性外还需要专家级的任务熟练度。现有的VLA模型后训练方法通常是离线、单机器人或任务特定的，限制了有效的同策略适应和从真实世界交互中实现可扩展学习。我们提出了一种可扩展在线后训练系统，该系统能够在物理世界中直接对通用型VLA模型进行在线、分布式、多任务后训练。SOP通过闭环架构紧密耦合执行与学习：机器人集群持续将同策略经验与人工干预信号流式传输至中央云端学习器，并异步接收更新后的策略。该设计支持即时同策略修正，通过并行部署扩展经验收集规模，并在适应过程中保持模型通用性。SOP对后训练算法选择具有兼容性；我们通过交互式模仿学习（HG-DAgger）和强化学习（RECAP）两种方式实现了该系统。在包括布料折叠、箱体组装、货品补货等一系列真实世界操作任务中，SOP显著提升了大型预训练VLA模型的性能，同时保持跨任务共享的单一策略。仅需数小时的真实世界交互即可实现有效后训练，且性能提升与机器人集群规模呈近线性关系。这些结果表明，在线学习与集群部署的紧密耦合是实现物理世界中通用机器人策略高效、可靠、可扩展后训练的关键。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.03044">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.03044">arXiv</a></p>
<hr />
<h3>8. DreamStyle：一种视频风格化的统一框架</h3>
<p><strong>原文标题：</strong> DreamStyle: A Unified Framework for Video Stylization</p>
<p><strong>摘要：</strong>
视频风格化作为视频生成模型的重要下游任务，目前尚未得到充分探索。其输入风格条件通常包括文本、风格图像和风格化首帧。每种条件均具有独特优势：文本更具灵活性，风格图像提供更精确的视觉锚点，而风格化首帧则使长视频风格化成为可能。然而，现有方法大多局限于单一类型的风格条件，限制了其应用范围。此外，高质量数据集的缺失导致风格不一致性与时序闪烁问题。为应对这些局限，本文提出DreamStyle——一种支持（1）文本引导、（2）风格图像引导及（3）首帧引导视频风格化的统一框架，并配套设计了高质量配对视频数据构建流程。该框架基于基础图像到视频（I2V）模型构建，通过采用具有特定令牌上矩阵的低秩自适应（LoRA）进行训练，有效降低了不同条件令牌间的混淆。定性与定量评估均表明，DreamStyle能够胜任全部三类视频风格化任务，并在风格一致性与视频质量方面优于现有方法。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.02785">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.02785">arXiv</a></p>
<hr />
<h3>9. MiMo-V2-Flash技术报告</h3>
<p><strong>原文标题：</strong> MiMo-V2-Flash Technical Report</p>
<p><strong>摘要：</strong>
本文提出MiMo-V2-Flash，这是一个采用专家混合架构的模型，总参数量达3090亿，激活参数量为150亿，专为快速、强大的推理与智能体能力而设计。该模型采用混合注意力架构，以5:1的混合比例将滑动窗口注意力与全局注意力交错排列，滑动窗口长度为128个词元。模型通过多词元预测方法在27万亿词元上进行预训练，原生支持32k上下文长度，并后续扩展至256k。为高效扩展训练后计算，MiMo-V2-Flash引入了创新的多教师同策略蒸馏范式。在此框架中，领域专用教师模型（例如通过大规模强化学习训练）提供密集的词元级奖励，使学生模型能够精准掌握教师专业知识。尽管总参数量分别仅为DeepSeek-V3.2和Kimi-K2的1/2与1/3，MiMo-V2-Flash的性能仍可与这些顶尖开源模型相媲美。在推理阶段，通过将多词元预测机制改造为推测解码的草稿模型，仅使用三层多词元预测结构即可实现最高3.6的接受长度和2.6倍的解码加速。我们将同步开源模型权重与三层多词元预测权重，以促进开放研究与社区协作。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.02780">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.02780">arXiv</a></p>
<hr />
<h3>10. CogFlow：通过知识内化连接感知与推理的视觉数学问题求解框架</h3>
<p><strong>原文标题：</strong> CogFlow: Bridging Perception and Reasoning through Knowledge Internalization for Visual Mathematical Problem Solving</p>
<p><strong>摘要：</strong>
尽管取得了显著进展，多模态大语言模型在视觉数学问题求解方面仍面临挑战。近期研究认识到视觉感知是视觉数学推理的关键瓶颈，但其解决方案主要局限于改进视觉信息的提取与解读。值得注意的是，这些研究均未解决一个核心问题：提取的视觉线索是否被忠实整合并有效运用于后续推理过程。基于此，我们提出CogFlow——一种受认知启发的三阶段创新框架，通过引入知识内化阶段，显式模拟人类推理的层次化流程：感知⇒内化⇒推理。遵循这一层次化流程，我们对各阶段进行全面增强。我们设计协同视觉奖励机制，在参数空间与语义空间中提升感知能力，协同优化符号与图形的视觉信息提取。为确保提取的视觉线索能忠实融入后续推理，我们在内化阶段引入知识内化奖励模型，构建感知与推理间的桥梁。此外，我们提出视觉门控策略优化算法，进一步确保推理过程基于视觉知识，防止模型产生表面连贯但脱离视觉依据的推理捷径。同时，我们构建了包含超过12万条高质量感知-推理对齐标注样本的新数据集MathCog以支持模型训练。在常用视觉数学推理基准上的综合实验与分析验证了CogFlow框架的优越性。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.01874">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.01874">arXiv</a></p>
<hr />
<h3>11. 数字孪生人工智能：从大语言模型到世界模型的机遇与挑战</h3>
<p><strong>原文标题：</strong> Digital Twin AI: Opportunities and Challenges from Large Language Models to World Models</p>
<p><strong>摘要：</strong>
数字孪生作为物理系统的精确数字化表征，通过人工智能技术的融合，已从被动仿真工具演变为具备智能与自主性的实体。本文提出一个统一的四阶段框架，系统性地刻画了人工智能在数字孪生全生命周期——涵盖建模、映射、干预与自主管理——中的整合路径。通过综合现有技术与实践，我们提炼出一个统一框架，系统阐述人工智能方法如何嵌入数字孪生生命周期：（1）通过基于物理机制与物理信息的人工智能方法构建物理孪生模型；（2）通过实时同步将物理系统映射为数字孪生；（3）借助预测建模、异常检测与优化策略对物理孪生实施干预；（4）通过大语言模型、基础模型与智能体实现自主管理。我们分析了基于物理的建模与数据驱动学习之间的协同关系，强调物理系统建模正从传统数值求解器转向物理信息模型与基础模型。进一步地，我们探讨了包括大语言模型与生成式世界模型在内的生成式人工智能技术如何将数字孪生转化为具备推理、交互与创造性场景生成能力的主动式、可自我优化的认知系统。通过对医疗健康、航空航天、智能制造、机器人、智慧城市等十一个应用领域的跨领域综述，我们指出了在可扩展性、可解释性与可信赖性方面面临的共性挑战，并展望了负责任的人工智能驱动型数字孪生系统的发展方向。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.01321">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.01321">arXiv</a></p>
<hr />
<h3>12. WebGym：面向真实任务的视觉网页智能体规模化训练环境</h3>
<p><strong>原文标题：</strong> WebGym: Scaling Training Environments for Visual Web Agents with Realistic Tasks</p>
<p><strong>摘要：</strong>
本文提出WebGym，这是迄今为止规模最大的开源真实视觉网页智能体训练环境。真实网站具有非稳态性和多样性特征，使得人工构建或小规模任务集难以支撑稳健的策略学习。WebGym包含近30万个任务，基于标准化评估体系覆盖多样化真实网站及不同难度层级。我们采用简洁的强化学习方案训练智能体：通过智能体自身交互轨迹（滚动执行）进行训练，并以任务奖励作为学习反馈信号。为实现强化学习的规模化扩展，我们专门针对网页智能体开发了高吞吐量异步滚动执行系统，将WebGym中的轨迹采样速度提升至基础实现的4-5倍。其次，我们通过拓展任务集的广度、深度与规模，实现了持续的性能提升。基于Qwen-3-VL-8B-Instruct这一强视觉语言基础模型在WebGym上进行微调后，其在分布外测试集上的成功率从26.2%提升至42.9%，显著优于基于GPT-4o（27.1%）和GPT-5-Thinking（29.8%）等专有模型的智能体。这一提升具有实质性意义，因为我们的测试集完全由训练阶段未出现过的网站任务构成，这与多数现有视觉网页智能体训练研究形成鲜明对比。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.02439">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.02439">arXiv</a></p>
<hr />
<h3>13. Muses：无需训练即可设计、组合并生成虚构奇幻三维生物的方法</h3>
<p><strong>原文标题：</strong> Muses: Designing, Composing, Generating Nonexistent Fantasy 3D Creatures without Training</p>
<p><strong>摘要：</strong>
本文提出Muses，一种在前馈范式下无需训练即可生成奇幻三维生物的首创方法。现有方法依赖部件感知优化、人工组装或二维图像生成，常因复杂部件级操控的挑战与跨域生成能力的局限，产生不真实或不协调的三维资产。与之相反，Muses以三维骨架——生物形态的基础表征——为核心，通过显式且合理的方式组合多样化元素。这一骨架基础将三维内容创作形式化为结构感知的设计、组合与生成流程。Muses首先通过图约束推理构建具有协调布局与比例、富有创意的组合三维骨架；随后在结构化潜空间内，以骨架引导体素化组装过程，融合不同对象的局部特征；最终在骨架约束下进行图像引导的外观建模，为组装形态生成风格统一且和谐的表面纹理。大量实验表明，Muses在视觉保真度、文本描述对齐度方面达到领先水平，并展现出灵活的三维物体编辑潜力。项目页面：https://luhexiao.github.io/Muses.github.io/。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.03256">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.03256">arXiv</a></p>
<hr />
<h3>14. 基于系统二策略的大语言模型大规模计数机制可解释性研究</h3>
<p><strong>原文标题：</strong> Mechanistic Interpretability of Large-Scale Counting in LLMs through a System-2 Strategy</p>
<p><strong>摘要：</strong>
大语言模型虽然在复杂数学问题上表现出色，但在计数任务中存在系统性局限。这一问题源于Transformer架构的限制：计数操作需跨层执行，而模型深度约束导致较大规模计数任务的精度下降。为突破此限制，我们受系统二认知过程启发，提出一种简单的测试时策略，将大规模计数任务分解为模型可可靠求解的独立子问题。我们通过观测性分析和因果中介分析评估该方法，以理解此类系统二策略的内在机制。机制分析揭示了三个关键环节：潜在计数结果在各部分的最终项目表征中计算存储，通过专用注意力头传递至中间步骤，并在最终阶段聚合生成总数。实验结果表明，该策略能使大语言模型突破架构限制，在大规模计数任务中实现高精度。本研究不仅揭示了大语言模型中系统二计数行为的机制原理，更为改进和理解其推理行为提供了可推广的方法论路径。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.02989">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.02989">arXiv</a></p>
<hr />
<h3>15. OpenRT：面向多模态大语言模型的开源红队测试框架</h3>
<p><strong>原文标题：</strong> OpenRT: An Open-Source Red Teaming Framework for Multimodal LLMs</p>
<p><strong>摘要：</strong>
多模态大语言模型（MLLMs）在关键应用中的快速部署正因其持续存在的安全漏洞而日益受阻。然而，现有的红队测试基准往往较为零散，通常仅限于单轮文本交互，且缺乏系统化评估所需的可扩展性。为此，我们提出了OpenRT——一个统一、模块化、高吞吐的红队测试框架，旨在对MLLMs的安全性进行全面评估。该框架的核心在于通过引入一个对抗性内核，在自动化红队测试中实现范式转变，该内核支持在五个关键维度上进行模块化分离：模型集成、数据集管理、攻击策略、判定方法与评估指标。通过标准化攻击接口，OpenRT将对抗逻辑与高吞吐异步运行时解耦，从而实现了跨不同模型的系统化扩展。本框架集成了37种不同的攻击方法，涵盖白盒梯度攻击、多模态扰动以及复杂的多智能体进化策略等。通过对20个先进模型（包括GPT-5.2、Claude 4.5和Gemini 3 Pro）进行广泛实证研究，我们揭示了关键的安全缺陷：即使是前沿模型也难以泛化至不同的攻击范式，领先模型的平均攻击成功率高达49.14%。值得注意的是，我们的研究发现推理模型在面对复杂、多轮越狱攻击时并不具备内在的更强鲁棒性。通过开源OpenRT，我们提供了一个可持续、可扩展且持续维护的基础设施，以加速AI安全领域的开发与标准化进程。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.01592">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.01592">arXiv</a></p>
<hr />
<h3>16. MindWatcher：迈向更智能的多模态工具集成推理</h3>
<p><strong>原文标题：</strong> MindWatcher: Toward Smarter Multimodal Tool-Integrated Reasoning</p>
<p><strong>摘要：</strong>
传统基于工作流的智能体在解决需要调用工具的实际问题时表现出有限的智能。能够自主推理和调用工具的工具集成推理智能体正迅速崛起，成为处理涉及多步骤外部环境交互的复杂决策任务的有效方法。本研究提出MindWatcher——一种融合交错式思维与多模态思维链推理的工具集成推理智能体。该智能体能够自主决定是否及如何调用多样化工具并协调其使用，无需依赖人工提示或预设工作流。交错式思维范式使模型能在任意中间阶段灵活切换思考与工具调用，而其多模态思维链能力支持在推理过程中操作图像以获得更精确的搜索结果。我们构建了自动化数据审计与评估流程，辅以人工标注的高质量训练数据集，并创建名为MindWatcher评估基准的测试平台以系统评估其性能。MindWatcher配备了一套完整的辅助推理工具集，使其能够处理广域多模态问题。通过构建涵盖汽车、动植物等八大类别的大规模高质量本地图像检索数据库，模型即便在轻量化架构下仍具备强大的物体识别能力。最后，我们为MindWatcher设计了更高效的训练基础设施，显著提升了训练速度与硬件利用率。实验结果表明：MindWatcher不仅通过卓越的工具调用能力达到或超越了更大规模或更新型模型的性能，还揭示了智能体训练中的关键发现（例如智能体强化学习中的遗传继承现象），为相关研究提供了重要洞见。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.23412">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.23412">arXiv</a></p>
<hr />
<h3>17. 大型推理模型（尚未）成为多语言潜在推理者</h3>
<p><strong>原文标题：</strong> Large Reasoning Models Are (Not Yet) Multilingual Latent Reasoners</p>
<p><strong>摘要：</strong>
大型推理模型在数学推理任务上展现出卓越性能，这通常归因于其生成显式思维链解释的能力。然而，近期研究表明，模型往往在完成文本推理步骤之前就已得出正确答案，这表明存在潜在推理——即隐藏状态中编码的内部非语言计算过程。尽管这一现象在英语环境中已得到初步探索，但其在多语言场景下的表现仍属未知。本文针对11种语言，系统性地研究了大型推理模型中的多语言潜在推理现象。通过采用基于截断的策略，我们考察了当模型仅获得部分推理轨迹时正确答案如何逐步显现，从而实现对潜在预测形成的阶段性度量。实验结果表明，多语言潜在推理确实存在，但呈现不均衡性：在高资源语言中表现显著，在低资源语言中较弱，且在难度较高的基准测试中普遍难以观测。为探究这些差异是否反映不同的内部机制，我们进一步进行了表征分析。研究发现，尽管存在表层差异，但预测的内部演化过程在跨语言间高度一致，且与英语模式基本吻合——这一规律暗示了以英语为中心的潜在推理路径。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.02996">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.02996">arXiv</a></p>
<hr />
<h3>18. FFP-300K：面向可泛化视频编辑的首帧传播规模化研究</h3>
<p><strong>原文标题：</strong> FFP-300K: Scaling First-Frame Propagation for Generalizable Video Editing</p>
<p><strong>摘要：</strong>
首帧传播（FFP）为可控视频编辑提供了一种前景广阔的范式，但现有方法受限于对繁琐运行时引导的依赖。我们认为这一局限的根本原因在于当前训练数据集的不足：其视频时长往往过短、分辨率较低，且缺乏能够教授鲁棒时序先验的任务多样性。为弥补这一基础性数据缺口，我们首先提出了FFP-300K数据集——该大规模数据集包含30万对720p分辨率、81帧长度的高保真视频对，通过一个结构化的双轨流程构建，以实现多样化的局部与全局编辑。基于此数据集，我们提出了一种真正无需引导的FFP新框架，该框架解决了保持首帧外观与保留源视频运动之间的关键矛盾。在架构层面，我们引入了自适应时空旋转位置编码（AST-RoPE），通过动态重映射位置编码以解耦外观与运动参考。在目标层面，我们采用自蒸馏策略，其中身份传播任务作为强正则化器，确保长期时序稳定性并防止语义漂移。在EditVerseBench基准上的综合实验表明，我们的方法显著优于现有学术及商业模型，在PickScore和VLM评分上分别获得约0.2分和0.3分的提升。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.01720">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.01720">arXiv</a></p>
<hr />
<h3>19. 声纳时刻：音频-语言模型在音频地理定位中的基准测试</h3>
<p><strong>原文标题：</strong> The Sonar Moment: Benchmarking Audio-Language Models in Audio Geo-Localization</p>
<p><strong>摘要：</strong>
地理定位旨在推断给定信号的地理来源。在计算机视觉领域，地理定位已成为组合推理能力的重要基准，并与公共安全密切相关。相比之下，音频地理定位的研究进展因缺乏高质量的音频-地理位置配对数据而受到限制。为填补这一空白，我们提出了AGL1K——首个面向音频语言模型的音频地理定位基准数据集，涵盖72个国家和地区。为从众包平台中提取具有可靠定位价值的样本，我们提出了音频可定位性度量指标，用于量化每条录音的信息丰富度，最终筛选出1,444条精校音频片段。对16个音频语言模型的评估表明，此类模型已初步具备音频地理定位能力。研究发现，闭源模型显著优于开源模型，且语言线索常作为预测的主要推理依据。我们进一步分析了音频语言模型的推理路径、区域偏差、错误成因以及可定位性指标的可解释性。总体而言，AGL1K为音频地理定位建立了基准测试体系，有望推动音频语言模型发展出更强大的地理空间推理能力。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.03227">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.03227">arXiv</a></p>
<hr />
<h3>20. X-MuTeST：一个面向可解释仇恨言论检测的多语言基准及新型大语言模型咨询解释框架</h3>
<p><strong>原文标题：</strong> X-MuTeST: A Multilingual Benchmark for Explainable Hate Speech Detection and A Novel LLM-consulted Explanation Framework</p>
<p><strong>摘要：</strong>
社交媒体上的仇恨言论检测在准确性和可解释性方面均面临挑战，尤其对于研究不足的印度语言而言。本文提出一种新颖的可解释性引导训练框架——X-MuTeST（可解释多语言仇恨言论检测框架），该框架将大语言模型的高层语义推理与传统注意力增强技术相结合。我们通过为每个单词提供基准人工标注的归因依据以证明类别标签的合理性，将研究范围扩展至印地语、泰卢固语及英语。X-MuTeST可解释性方法通过计算原始文本与单字组、双字组、三字组的预测概率差异生成解释。最终解释通过融合大语言模型解释与X-MuTeST解释生成。研究表明，在训练过程中利用人工归因依据能同时提升分类性能与可解释性。此外，将人工归因依据与我们的可解释性方法结合以优化模型注意力机制，可带来进一步性能提升。我们使用合理性指标（如Token-F1和IOU-F1）与忠实性指标（如完备性和充分性）评估可解释性。通过聚焦资源匮乏语言，本研究推动了跨多元语言环境的仇恨言论检测进展。我们的数据集包含6,004个印地语样本、4,492个泰卢固语样本和6,334个英语样本的词元级归因标注。数据与代码已公开于https://github.com/ziarehman30/X-MuTeST。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.03194">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.03194">arXiv</a></p>
<hr />
<h3>21. 并行潜在推理在序列推荐中的应用</h3>
<p><strong>原文标题：</strong> Parallel Latent Reasoning for Sequential Recommendation</p>
<p><strong>摘要：</strong>
从稀疏行为序列中捕捉复杂的用户偏好仍然是序列推荐领域的基础性挑战。最近的潜在推理方法通过多步推理扩展测试时计算展现出潜力，但这些方法仅依赖单一轨迹的深度级扩展，随着推理深度增加会出现收益递减问题。为解决这一局限性，我们提出并行潜在推理（PLR）这一创新框架，该框架通过同时探索多个多样化推理轨迹，开创性地实现了宽度级计算扩展。PLR通过在连续潜在空间中的可学习触发令牌构建并行推理流，通过全局推理正则化保持流间多样性，并通过混合推理流聚合机制自适应地综合多流输出。在三个真实数据集上的大量实验表明，PLR在保持实时推理效率的同时，显著优于现有最先进的基线方法。理论分析进一步验证了并行推理对提升泛化能力的有效性。本研究为超越现有深度扩展模式、增强序列推荐中的推理能力开辟了新途径。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.03153">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.03153">arXiv</a></p>
<hr />
<h3>22. 统一思考者：面向图像生成的通用推理模块化核心</h3>
<p><strong>原文标题：</strong> Unified Thinker: A General Reasoning Modular Core for Image Generation</p>
<p><strong>摘要：</strong>
尽管高保真图像合成已取得显著进展，但生成模型在处理逻辑密集型指令时仍面临困难，暴露出长期存在的推理与执行能力之间的差距。与此同时，闭源系统（如Nano Banana）已展现出强大的推理驱动图像生成能力，凸显了当前开源模型与之存在的显著差距。我们认为，弥合这一差距不仅需要更优的视觉生成器，更需要可执行的推理能力：即将高层意图分解为可直接指导生成过程的、可验证的具象化规划方案。为此，我们提出“统一思考者”——一种面向通用图像生成的任务无关推理架构，其设计为一个可接入多种生成器与工作流的统一规划核心。该架构将专用思考模块与图像生成器解耦，使得推理能力能够在不重新训练整个生成模型的情况下实现模块化升级。我们进一步提出两阶段训练范式：首先为思考模块构建结构化规划接口，随后通过强化学习将其策略与像素级反馈相锚定，从而激励规划方案优先优化视觉正确性而非文本合理性。在文本到图像生成与图像编辑任务上的大量实验表明，统一思考者能显著提升图像推理与生成质量。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.03127">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.03127">arXiv</a></p>
<hr />
<h3>23. ExposeAnyone：基于个性化音频-表情扩散模型的鲁棒零样本人脸伪造检测器</h3>
<p><strong>原文标题：</strong> ExposeAnyone: Personalized Audio-to-Expression Diffusion Models Are Robust Zero-Shot Face Forgery Detectors</p>
<p><strong>摘要：</strong>
检测未知深度伪造操作仍然是人脸伪造检测领域最具挑战性的问题之一。当前最先进的方法难以泛化至未见过的伪造类型，主要因其依赖于现有深度伪造或伪伪造数据的监督训练，导致对特定伪造模式过拟合。相比之下，自监督方法具有更强的泛化潜力，但现有研究难以仅通过自监督学习获得具有判别性的表征。本文提出ExposeAnyone，一种基于扩散模型的完全自监督方法，该模型能够从音频生成表情序列。其核心思想是：当模型通过参考集对特定对象完成个性化适配后，可通过扩散重建误差计算可疑视频与个性化对象之间的身份距离，从而实现针对特定目标的人脸伪造检测。大量实验表明：1）在DF-TIMIT、DFDCP、KoDF和IDForge数据集上，本方法的平均AUC比先前最优方法提升4.22个百分点；2）本模型能够有效检测Sora2生成的视频（现有方法对此类视频检测效果较差）；3）本方法对模糊、压缩等干扰具有高度鲁棒性，凸显了其在现实场景人脸伪造检测中的适用性。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.02359">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.02359">arXiv</a></p>
<hr />
<h3>24. AceFF：面向小分子的前沿机器学习势函数</h3>
<p><strong>原文标题：</strong> AceFF: A State-of-the-Art Machine Learning Potential for Small Molecules</p>
<p><strong>摘要：</strong>
本文介绍AceFF——一种专为小分子药物发现优化的预训练机器学习原子间势函数（MLIP）。尽管MLIP已成为密度泛函理论（DFT）的高效替代方案，但其在不同化学空间中的泛化能力仍面临挑战。AceFF通过基于类药化合物综合数据集精调的TensorNet2架构解决了这一问题，实现了高通量推理速度与DFT级别精度的平衡。该势函数完整支持药物化学核心元素（H、B、C、N、O、F、Si、P、S、Cl、Br、I），并经过专项训练以处理带电态。通过复杂扭转能扫描、分子动力学轨迹、批量能量最小化以及力与能量的精确度验证等严格基准测试表明，AceFF为有机分子体系建立了新的性能标杆。AceFF-2模型权重与推理代码已发布于https://huggingface.co/Acellera/AceFF-2.0。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.00581">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.00581">arXiv</a></p>
<hr />
<h3>25. 基于U-Net架构的脉冲神经网络单幅图像去雾方法</h3>
<p><strong>原文标题：</strong> U-Net-Like Spiking Neural Networks for Single Image Dehazing</p>
<p><strong>摘要：</strong>
图像去雾是计算机视觉领域的关键挑战，对于提升雾霾条件下图像清晰度至关重要。传统方法多依赖于大气散射模型，而近期深度学习技术——特别是卷积神经网络（CNNs）与Transformer架构——通过有效分析图像特征显著提升了去雾性能。然而，卷积神经网络难以捕捉长程依赖关系，而Transformer模型则需要大量计算资源。为克服这些局限，本文提出DehazeSNN创新架构，将类U-Net设计与脉冲神经网络（SNNs）相结合。该模型能够捕获多尺度图像特征，同时高效处理局部与长程依赖关系。通过引入正交泄漏积分发放模块（OLIFBlock），增强了跨通道信息交互能力，在降低计算负担的同时实现了更优的去雾性能。大量实验表明，DehazeSNN在基准数据集上与最先进方法相比具有显著竞争力，能以更小的模型规模和更少的乘累加运算生成高质量无雾图像。本去雾方法已公开于https://github.com/HaoranLiu507/DehazeSNN。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.23950">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.23950">arXiv</a></p>
<hr />
<h3>26. Doc-PP：面向大型视觉语言模型的文档策略保持基准</h3>
<p><strong>原文标题：</strong> Doc-PP: Document Policy Preservation Benchmark for Large Vision-Language Models</p>
<p><strong>摘要：</strong>
大型视觉语言模型在实际文档问答任务中的部署，常受到动态、用户自定义策略的制约，这些策略根据具体情境规定了信息可披露的范围。尽管确保模型遵循这些显式约束至关重要，但现有的安全性研究主要集中于隐式社会规范或纯文本场景，忽视了多模态文档的复杂性。本文提出Doc-PP（文档策略保持基准），这是一个基于真实世界报告构建的新型基准，要求模型在严格的非披露政策下，对异质的视觉与文本元素进行跨模态推理。我们的评估揭示了一个系统性的“推理诱发安全漏洞”：当答案需要通过复杂综合或跨模态信息聚合推断得出时，模型频繁泄露敏感信息，从而有效绕过了现有的安全约束。此外，我们发现提供提取的文本虽能提升模型感知能力，却无意中助长了信息泄露。为应对这些漏洞，我们提出了DVA（分解-验证-聚合）框架，这是一种将推理过程与策略验证解耦的结构化推断方法。实验结果表明，DVA显著优于标准的提示防御方法，为符合策略要求的文档理解提供了一个鲁棒的基准方案。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.03926">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.03926">arXiv</a></p>
<hr />
<h3>27. 大语言模型中工具性趋同倾向的可操控性研究</h3>
<p><strong>原文标题：</strong> Steerability of Instrumental-Convergence Tendencies in LLMs</p>
<p><strong>摘要：</strong>
本研究探讨人工智能系统的两个核心属性：能力（系统能够执行的任务）与可操控性（行为向预期目标可靠转移的程度）。核心问题在于能力提升是否会削弱可操控性并引发控制失效风险。我们进一步区分授权可操控性（开发者可靠实现预期行为）与非授权可操控性（攻击者诱导出禁止行为），这一区分揭示了AI模型面临的基础性安全-防护困境：安全性要求高可操控性以实施控制（如停止/拒绝指令），而防护性则要求对恶意行为者保持低可操控性以防止有害行为生成。这种矛盾对开源权重模型构成重大挑战，当前这类模型通过微调或对抗攻击等常见技术展现出高可操控性。基于Qwen3模型与InstrumentalEval评估工具，我们发现简短的反工具性提示后缀能显著降低测得的趋同率（如关机规避、自我复制等）。在Qwen3-30B Instruct模型中，趋同率从支持工具性后缀下的81.69%骤降至反工具性后缀下的2.82%。在反工具性提示条件下，较大规模的对齐模型比较小模型展现出更低的趋同率（Instruct版：2.82%对比4.23%；思考版：4.23%对比9.86%）。相关代码已发布于github.com/j-hoscilowicz/instrumental_steering。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.01584">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.01584">arXiv</a></p>
<hr />
<h2>🔍 关键词云图</h2>
<p><img alt="关键词云图" src="../images/keywords_wordcloud.png" /></p>
<h2>📈 近期论文趋势</h2>
<p><img alt="论文趋势" src="../images/daily_papers.png" /></p>
<h2>🎙️ 语音播报</h2>
<ul>
<li><a href="../audio/2026-01-07_daily_papers.mp3">收听今日论文解读</a></li>
</ul>
<h2>📱 订阅渠道</h2>
<ul>
<li>GitHub: <a href="https://github.com/2404589803/hf-daily-paper-newsletter-chinese">hf-daily-paper-newsletter-chinese</a></li>
</ul>
    </div>
</body>
</html>