
# <img src="https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo.png" width="30"/> Hugging Face 2026-01-06 论文日报

## 📊 今日论文统计
- 总论文数：25
- 热门领域：LLM, Transformer, RL, GPT

## 📝 论文详情


### 1. 大语言模型能否预测自身失败？基于内部电路的自感知机制

**原文标题：** Can LLMs Predict Their Own Failures? Self-Awareness via Internal Circuits

**摘要：**
大语言模型（LLMs）能够生成流畅而复杂的输出，但往往无法识别自身的错误与幻觉。现有方法通常依赖外部评判器、多样本一致性检验或基于文本的自我批判，这些方法要么增加额外计算成本，要么与真实正确性的关联较弱。本文提出核心问题：大语言模型能否通过推理过程中对内部状态的监测来预测自身失败？我们提出Gnosis——一种轻量级自感知机制，使冻结参数的大语言模型能够通过解码隐藏状态与注意力模式的信号进行内在自我验证。Gnosis被动观察内部轨迹，将其压缩为固定预算的描述符，并以可忽略的推理成本预测正确性，仅增加约500万参数且运算独立于序列长度。在数学推理、开放域问答和学术知识基准测试中，针对1.7B至20B参数的冻结主干模型，Gnosis在准确率与校准度上均持续优于强内部基线及大型外部评判器。此外，该机制能零样本泛化至部分生成结果，实现对失败轨迹的早期检测及计算感知控制。这些结果表明，可靠的正确性线索内生于生成过程，无需外部监督即可高效提取。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.20578) | [arXiv](https://arxiv.org/abs/2512.20578)



---

### 2. K-EXAONE 技术报告

**原文标题：** K-EXAONE Technical Report

**摘要：**
本技术报告介绍了由LG AI Research开发的大规模多语言语言模型K-EXAONE。K-EXAONE基于混合专家架构构建，总参数量达2360亿，推理时激活参数量为230亿。它支持256K令牌的上下文窗口，并涵盖六种语言：韩语、英语、西班牙语、德语、日语和越南语。我们在涵盖推理、智能体、通用能力、韩语能力及多语言能力的综合基准测试套件上对K-EXAONE进行了评估。在所有评估中，K-EXAONE展现出与同规模开源权重模型相媲美的性能。K-EXAONE旨在推动人工智能以创造更美好的生活，其定位是面向广泛工业和研究应用的强大专有AI基础模型。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2601.01739) | [arXiv](https://arxiv.org/abs/2601.01739)



---

### 3. NextFlow：统一序列建模激活多模态理解与生成能力

**原文标题：** NextFlow: Unified Sequential Modeling Activates Multimodal Understanding and Generation

**摘要：**
本文提出NextFlow模型，这是一种基于6万亿交错排列的文本-图像离散标记训练的统一解码器自回归变换器。通过在统一的自回归架构中采用统一的视觉表征方法，NextFlow原生激活了多模态理解与生成能力，实现了图像编辑、交错内容生成及视频生成等功能。针对不同模态的特性——文本具有严格序列性而图像具有内在层次性——我们保留文本的下一标记预测机制，但对视觉生成采用下一尺度预测方法。这一设计突破了传统光栅扫描方法的局限，仅需5秒即可生成1024×1024分辨率图像，比同类自回归模型快数个数量级。我们通过稳健的训练方案解决了多尺度生成的不稳定性问题，并提出了强化学习的前缀调优策略。实验表明，NextFlow在统一模型中实现了最先进的性能，其视觉质量可与专业扩散基线模型相媲美。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2601.02204) | [arXiv](https://arxiv.org/abs/2601.02204)



---

### 4. DreamID-V：基于扩散Transformer的高保真人脸视频替换——弥合图像到视频的鸿沟

**原文标题：** DreamID-V:Bridging the Image-to-Video Gap for High-Fidelity Face Swapping via Diffusion Transformer

**摘要：**
视频人脸替换（VFS）要求将源身份无缝注入目标视频，同时精确保持原始姿态、表情、光照、背景及动态信息。现有方法在保持时序一致性的同时，难以兼顾身份相似性与属性保留。为应对这一挑战，我们提出了一个综合性框架，将图像人脸替换（IFS）的优势无缝迁移至视频领域。我们首先引入了新型数据流水线SyncID-Pipe，通过预训练身份锚定视频合成器并结合IFS模型，构建双向身份四元组以实现显式监督。基于配对数据，我们提出了首个基于扩散Transformer的框架DreamID-V，其核心模态感知调节模块能够差异化注入多模态条件。同时，我们提出了合成到真实的渐进式训练机制与身份一致性强化学习策略，以增强复杂场景下的视觉真实感与身份一致性。针对现有基准数据有限的问题，我们构建了涵盖多样化场景的综合评测基准IDBench-V。大量实验表明，DreamID-V在性能上超越现有最优方法，并展现出卓越的泛化能力，可无缝适配多种人脸替换相关任务。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2601.01425) | [arXiv](https://arxiv.org/abs/2601.01425)



---

### 5. VAR强化学习的正确路径：解决视觉自回归生成中的异步策略冲突

**原文标题：** VAR RL Done Right: Tackling Asynchronous Policy Conflicts in Visual Autoregressive Generation

**摘要：**
视觉生成领域主要由三种范式主导：自回归模型、扩散模型以及视觉自回归模型。与自回归和扩散模型不同，视觉自回归模型在其生成步骤中处理异构输入结构，这导致了严重的异步策略冲突。该问题在强化学习场景中尤为突出，常引发训练过程不稳定与目标对齐欠佳。为解决此问题，我们提出一种创新框架，通过显式管理这些冲突来增强分组相对策略优化方法。该框架整合了三个协同组件：1）用于引导早期生成阶段的稳定性中间奖励机制；2）实现精确信用分配的动态时间步重加权方案；3）基于奖励反馈学习原理设计的新型掩码传播算法，该算法能在空间与时间维度同时隔离优化效应。实验表明，相较于原始分组相对策略优化基线，我们的方法在生成样本质量与目标对齐度上均取得显著提升，为视觉自回归模型实现了鲁棒且高效的优化。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2601.02256) | [arXiv](https://arxiv.org/abs/2601.02256)



---

### 6. GARDO：无需奖励破解的扩散模型强化方法

**原文标题：** GARDO: Reinforcing Diffusion Models without Reward Hacking

**摘要：**
通过在线强化学习对扩散模型进行微调已展现出提升文本-图像对齐能力的巨大潜力。然而，由于视觉任务中精确指定真实目标仍具挑战性，模型通常使用仅部分反映真实目标的代理奖励进行优化。这种不匹配常导致奖励破解现象，即代理分数上升而实际图像质量下降、生成多样性崩溃。常见解决方案通过添加针对参考策略的正则化来防止奖励破解，但由于参考策略通常非最优，这类方法会牺牲样本效率并阻碍对新颖高奖励区域的探索。为平衡样本效率、有效探索和缓解奖励破解之间的竞争性需求，我们提出具备多样性感知优化的门控自适应正则化框架（GARDO），该通用框架可与多种强化学习算法兼容。我们的核心见解在于：正则化无需普遍应用，而选择性地对高不确定性样本子集进行惩罚效果显著。针对探索挑战，GARDO引入自适应正则化机制，定期更新参考模型以匹配在线策略的能力，确保正则化目标的相关性。针对强化学习中的模式崩溃问题，GARDO通过放大兼具高质量与高多样性的样本奖励，在保持优化过程稳定的同时促进模式覆盖。在多种代理奖励和未见保留指标上的大量实验一致表明，GARDO能在不牺牲样本效率或探索能力的前提下有效缓解奖励破解并提升生成多样性，彰显了其效能与鲁棒性。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.24138) | [arXiv](https://arxiv.org/abs/2512.24138)



---

### 7. VINO：一种基于交错式全模态上下文的统一视觉生成器

**原文标题：** VINO: A Unified Visual Generator with Interleaved OmniModal Context

**摘要：**
本文提出VINO，一种在统一框架内实现图像与视频生成及编辑的视觉生成器。该方法摒弃了为不同模态设计独立任务模型或模块的传统思路，采用共享的扩散模型主干结构，通过融合文本、图像和视频作为条件输入，使单一模型能够支撑广泛的视觉创作与编辑任务。具体而言，VINO将视觉语言模型（VLM）与多模态扩散变换器（MMDiT）相结合，将多模态输入编码为交错排列的条件标记，进而引导扩散生成过程。该设计在避免引入模态专用组件的同时，实现了多参考信息关联、长指令序列跟随以及静态与动态内容间的连贯身份保持。为训练这一统一系统，我们提出多阶段训练流程，逐步将基础视频生成模型扩展为支持图像与视频输入输出的多任务生成器。在多样化的生成与编辑基准测试中，VINO展现出卓越的视觉质量、精准的指令跟随能力、优化的参考与属性保持效果，以及更具可控性的多身份编辑性能。本研究为可扩展的统一视觉生成提供了可行路径，并揭示了交错式上下文计算作为通用视觉创作基础框架的潜力。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2601.02358) | [arXiv](https://arxiv.org/abs/2601.02358)



---

### 8. InfiniteVGGT：面向无限流数据的视觉几何基础Transformer

**原文标题：** InfiniteVGGT: Visual Geometry Grounded Transformer for Endless Streams

**摘要：**
实现持久、大规模三维视觉几何理解的宏伟愿景，一直受制于可扩展性与长期稳定性之间难以调和的需求。尽管如VGGT等离线模型展现出卓越的几何理解能力，但其基于批处理的特性使其无法适用于实时系统。流式架构虽为实时操作而生，但现有方法或无法支持真正无限时长的输入，或在长序列上遭受灾难性漂移问题。我们通过InfiniteVGGT彻底打破了这一长期困境——这是一种因果视觉几何Transformer，它通过一个有界但自适应且持续具备表达力的KV缓存机制，实现了滚动内存的概念。基于此，我们设计了一种无需训练、与注意力机制无关的剪枝策略，智能地丢弃过时信息，随着每一帧新数据的到来有效地“滚动”更新内存。InfiniteVGGT完全兼容FlashAttention，最终消除了传统权衡，在实现无限时长流式处理的同时，其长期稳定性超越了现有流式方法。对此类系统的终极考验在于其在真正无限时长上的性能，而由于缺乏极长期连续基准测试，这一能力一直无法被严格验证。为填补这一关键空白，我们引入了Long3D基准测试，首次实现了对约10,000帧序列的连续三维几何估计进行严格评估，为未来长期三维几何理解研究提供了权威评估平台。代码已开源：https://github.com/AutoLab-SAI-SJTU/InfiniteVGGT

**论文链接：** [HuggingFace](https://huggingface.co/papers/2601.02281) | [arXiv](https://arxiv.org/abs/2601.02281)



---

### 9. 递归语言模型

**原文标题：** Recursive Language Models

**摘要：**
本文从推理时扩展的视角出发，研究如何使大型语言模型能够处理任意长度的提示文本。我们提出递归语言模型，这是一种通用推理策略，将长提示文本视为外部环境的一部分，允许大型语言模型以编程方式检查、分解提示片段并递归调用自身进行处理。实验表明，递归语言模型能够成功处理超出模型上下文窗口两个数量级的输入；即使在较短提示任务中，其在四项不同的长上下文任务中也显著优于基础大型语言模型及常见的长上下文框架方法，同时保持可比（或更低）的单次查询成本。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.24601) | [arXiv](https://arxiv.org/abs/2512.24601)



---

### 10. Falcon-H1R：通过混合模型推动推理前沿，实现高效测试时扩展

**原文标题：** Falcon-H1R: Pushing the Reasoning Frontiers with a Hybrid Model for Efficient Test-Time Scaling

**摘要：**
本文介绍了Falcon-H1R，这是一个拥有70亿参数、专为推理优化的模型，它证明了小型语言模型（SLMs）在实现具有竞争力的推理性能方面的可行性。Falcon-H1R以其参数效率脱颖而出，在多种推理密集型基准测试中，持续匹配或超越参数量是其2至7倍的最先进推理模型。这些结果凸显了精细的数据整理和针对性训练策略（通过高效的监督微调和强化学习扩展）的重要性，它们能在不增加模型规模的情况下带来显著的性能提升。此外，Falcon-H1R通过结合更快的推理速度（得益于其混合并行架构设计）、更高的令牌效率和更高的准确性，推进了推理效率的三维极限。这种独特的融合使Falcon-H1R-7B成为扩展高级推理系统的实用骨干，特别是在需要大量思维链生成和并行测试时扩展的场景中。利用最近提出的DeepConf方法，Falcon-H1R实现了最先进的测试时扩展效率，在准确性和计算成本方面均提供了显著改进。因此，Falcon-H1R证明，通过针对性的模型训练和架构选择，紧凑模型能够提供强大且可扩展的推理性能。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2601.02346) | [arXiv](https://arxiv.org/abs/2601.02346)



---

### 11. SimpleMem：面向大语言模型代理的高效终身记忆系统

**原文标题：** SimpleMem: Efficient Lifelong Memory for LLM Agents

**摘要：**
为支持复杂环境中可靠的长期交互，大语言模型（LLM）代理需要能够高效管理历史经验的记忆系统。现有方法要么通过被动扩展上下文保留完整的交互历史，导致大量冗余；要么依赖迭代推理过滤噪声，产生高昂的令牌消耗。为解决这一挑战，我们提出SimpleMem——一种基于语义无损压缩的高效记忆框架。我们设计了一个三阶段流程以最大化信息密度与令牌利用率：（1）语义结构化压缩：通过熵感知过滤将非结构化交互提炼为紧凑的多视角索引记忆单元；（2）递归记忆整合：通过异步过程将相关单元整合为更高层次的抽象表示以降低冗余；（3）自适应查询感知检索：根据查询复杂度动态调整检索范围，高效构建精准上下文。在基准数据集上的实验表明，本方法在准确性、检索效率与推理成本上均优于基线方法，平均F1值提升26.4%，同时将推理阶段令牌消耗降低最高达30倍，实现了性能与效率的优越平衡。代码已开源：https://github.com/aiming-lab/SimpleMem。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2601.02553) | [arXiv](https://arxiv.org/abs/2601.02553)



---

### 12. Talk2Move：基于强化学习的场景中文本指令对象级几何变换方法

**原文标题：** Talk2Move: Reinforcement Learning for Text-Instructed Object-Level Geometric Transformation in Scenes

**摘要：**
本文提出Talk2Move，一种基于强化学习（RL）的扩散框架，用于实现场景中对象的文本指令空间变换。通过自然语言对场景中的对象进行空间操控是多模态生成系统面临的一项挑战。现有的基于文本的操控方法虽能调整外观或风格，但由于缺乏成对的监督数据以及像素级优化的局限性，难以执行对象级的几何变换（如平移、旋转或缩放）。Talk2Move采用组相对策略优化（GRPO），通过输入图像和轻量级文本变体生成多样化的轨迹来探索几何动作，从而无需昂贵的配对数据。空间奖励引导模型将几何变换与语言描述对齐，同时离策略步骤评估和主动步骤采样通过聚焦于信息丰富的变换阶段来提高学习效率。此外，我们设计了以对象为中心的空间奖励，直接评估位移、旋转和缩放行为，从而实现可解释且连贯的变换。在精选基准测试上的实验表明，Talk2Move能够实现精确、一致且语义保真的对象变换，在空间准确性和场景连贯性方面均优于现有的文本引导编辑方法。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2601.02356) | [arXiv](https://arxiv.org/abs/2601.02356)



---

### 13. 大语言模型多轮交互中的置信度估计

**原文标题：** Confidence Estimation for LLMs in Multi-turn Interactions

**摘要：**
尽管置信度估计是缓解大语言模型幻觉问题的有效方向，但现有研究主要集中于单轮交互场景。在多轮对话中，上下文信息持续累积且歧义逐步消解，模型置信度在此动态过程中的变化机制尚未得到充分探索。可靠的置信度估计对自主智能体、人机协同系统等下游应用至关重要。本研究首次系统性地探讨多轮交互中的置信度估计问题，构建了基于双重核心需求的评估框架：单轮校准性与信息增量下的置信度单调性。为此，我们提出创新性评估指标（包括长度归一化的期望校准误差InfoECE），并设计“线索提供者-猜测者”范式以生成可控评估数据集。实验表明，当前主流置信度估计方法在多轮对话中普遍存在校准失效与单调性缺失问题。我们提出的基于对数概率的探测方法P(Sufficient)虽取得相对更优性能，但该任务远未完全解决。本研究为开发更可靠、可信的对话系统提供了基础方法论。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2601.02179) | [arXiv](https://arxiv.org/abs/2601.02179)



---

### 14. KV-Embedding：基于解码器专用大语言模型内部KV重路由的无训练文本嵌入方法

**原文标题：** KV-Embedding: Training-free Text Embedding via Internal KV Re-routing in Decoder-only LLMs

**摘要：**
尽管大语言模型（LLMs）是强大的嵌入骨干网络，但其在无训练场景中的应用面临两大结构挑战：因果注意力机制限制了早期令牌访问后续上下文的能力，而下一令牌预测目标则使表征偏向生成任务而非语义压缩。为克服这些局限，本文提出KV-Embedding框架，旨在激活冻结大语言模型的潜在表征能力。该方法基于关键发现：每个网络层中最终令牌的键值（KV）状态编码了序列的压缩视图。通过将这些状态重路由为前置前缀，我们使所有令牌能在单次前向传播中访问序列级上下文。为确保模型无关的适用性，我们提出了基于内在维度的自动化层级选择策略。基于Qwen、Mistral和Llama骨干网络在MTEB基准上的评估表明，KV-Embedding相较现有无训练基线方法性能提升最高达10%，同时在长达4,096个令牌的序列上保持稳健性能。这些结果证明，内部状态操作为输入修改提供了高效替代方案，本研究有望推动基于大语言模型内部机制的表示学习进一步探索。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2601.01046) | [arXiv](https://arxiv.org/abs/2601.01046)



---

### 15. CPPO：面向视觉语言策略优化的对比感知方法

**原文标题：** CPPO: Contrastive Perception for Vision Language Policy Optimization

**摘要：**
本文提出CPPO（对比感知策略优化方法），用于微调视觉语言模型（VLMs）。尽管强化学习（RL）在提升语言模型的推理能力方面取得了进展，但将其扩展至多模态推理仍需同时改进感知与推理两个维度。先前研究主要通过显式的感知奖励应对这一挑战，但将感知标记与推理标记分离存在困难，往往需要额外的大语言模型（LLMs）、真实标注数据、强制策略模型分离感知与推理，或对所有输出标记 indiscriminately 施加奖励。CPPO通过分析输入图像扰动下模型输出的熵变来检测感知标记，从而解决上述问题。该方法进一步在RL目标函数中引入对比感知损失（CPL），以强化模型在信息保留扰动下的输出一致性，并提升其在信息移除扰动下的敏感性。实验表明，CPPO在超越以往感知奖励方法的同时，无需引入额外模型，使训练过程更高效且具备更好的扩展性。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2601.00501) | [arXiv](https://arxiv.org/abs/2601.00501)



---

### 16. DiffProxy：基于扩散生成密集代理的多视角人体网格恢复方法

**原文标题：** DiffProxy: Multi-View Human Mesh Recovery via Diffusion-Generated Dense Proxies

**摘要：**
多视角图像的人体网格恢复面临一个根本性挑战：现实世界数据集包含不完美的真实标注，会导致模型训练产生偏差；而具有精确标注的合成数据则存在领域差异问题。本文提出DiffProxy，一种通过生成多视角一致人体代理进行网格恢复的新框架。该框架的核心在于利用基于扩散模型的生成先验知识，以弥合合成数据训练与真实场景泛化之间的差距。其主要创新包括：（1）用于生成多视角一致、像素对齐人体代理的多条件生成机制；（2）结合灵活视觉提示以增强局部细节的手部精细化模块；（3）在优化过程中提升对挑战性场景鲁棒性的不确定性感知测试时缩放方法。这些设计确保网格恢复过程能有效利用精确的合成真实标注及扩散流程的生成优势。DiffProxy仅使用合成数据训练，便在五个真实世界基准测试中取得了最先进的性能，尤其在存在遮挡和局部视角的挑战性场景中展现出强大的零样本泛化能力。项目页面：https://wrk226.github.io/DiffProxy.html

**论文链接：** [HuggingFace](https://huggingface.co/papers/2601.02267) | [arXiv](https://arxiv.org/abs/2601.02267)



---

### 17. COMPASS：评估大型语言模型中组织特定政策对齐性的框架

**原文标题：** COMPASS: A Framework for Evaluating Organization-Specific Policy Alignment in LLMs

**摘要：**
随着大型语言模型在从医疗保健到金融等高风险企业应用中的部署，确保其遵守组织特定政策已变得至关重要。然而，现有的安全性评估仅聚焦于普遍性危害。本文提出COMPASS（公司/组织政策对齐评估框架），这是首个系统化评估大型语言模型是否符合组织允许清单与禁止清单政策的框架。我们将COMPASS应用于八个不同行业场景，通过策略性设计的边缘案例，生成并验证了5,920条测试查询，以检验常规合规性与对抗性鲁棒性。通过对七个前沿模型的评估，我们发现了一个根本性不对称现象：模型能可靠处理合法请求（准确率>95%），但在执行禁令时却出现灾难性失效——仅能拒绝13-40%的对抗性禁止清单违规请求。这些结果表明，当前大型语言模型缺乏政策关键型部署所需的鲁棒性，从而确立了COMPASS作为组织人工智能安全性评估的核心框架地位。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2601.01836) | [arXiv](https://arxiv.org/abs/2601.01836)



---

### 18. SWE-Lego：探索监督微调在软件问题解决中的性能极限

**原文标题：** SWE-Lego: Pushing the Limits of Supervised Fine-tuning for Software Issue Resolving

**摘要：**
本文提出SWE-Lego，一种旨在实现软件工程问题解决任务最先进性能的监督微调方案。与当前依赖复杂训练范式（如中期训练、监督微调、强化学习及其组合）的主流方法不同，本研究探索如何突破轻量级纯监督微调方法在软件工程任务中的性能极限。SWE-Lego包含三个核心构建模块，关键发现总结如下：1）SWE-Lego数据集——包含3.2万个高质量任务实例与1.8万条验证轨迹，通过真实数据与合成数据的结合实现质量与数量的互补；2）采用错误掩码与难度分级课程的改进型监督微调流程，可显著提升动作质量与整体性能。实验结果表明，仅凭这两个构建模块，监督微调即可使SWE-Lego模型在同等规模的开源模型中达到SWE-bench Verified基准的最先进水平：SWE-Lego-Qwen3-8B达到42.2%，SWE-Lego-Qwen3-32B达到52.6%。3）我们在监督微调基础上进一步评估并改进了测试时扩展方法。基于训练完备的验证器，SWE-Lego模型性能可获得显著提升——在TTS@16设置下，8B模型从42.2%提升至49.6%，32B模型从52.6%提升至58.8%。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2601.01426) | [arXiv](https://arxiv.org/abs/2601.01426)



---

### 19. 面向稳定半监督遥感分割的协同引导与协同融合方法

**原文标题：** Toward Stable Semi-Supervised Remote Sensing Segmentation via Co-Guidance and Co-Fusion

**摘要：**
半监督遥感图像语义分割为减轻详尽标注负担提供了可行方案，但其本质上受伪标签漂移现象的制约——这种由确认偏差导致训练过程中误差累积的问题严重影响了模型性能。本研究提出Co2S框架，通过协同融合视觉语言模型与自监督模型的先验知识，构建稳定的半监督遥感分割体系。具体而言，我们设计了基于预训练CLIP和DINOv3初始化的异构双学生架构，该架构包含两种不同的视觉基础Transformer模型，旨在抑制误差累积与伪标签漂移。为有效整合异构先验，我们提出显式-隐式语义协同引导机制：分别利用文本嵌入向量与可学习查询向量提供显式与隐式的类别级语义引导，共同增强语义一致性。此外，开发了全局-局部特征协同融合策略，将CLIP捕获的全局上下文信息与DINOv3提取的局部细节特征有效融合，使模型能够生成高精度分割结果。在六个主流数据集上的大量实验表明，该方法在不同数据划分协议和多样场景下均能保持领先性能，验证了其优越性。项目页面详见：https://xavierjiezou.github.io/Co2S/。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.23035) | [arXiv](https://arxiv.org/abs/2512.23035)



---

### 20. OpenNovelty：一种基于大语言模型的代理系统，用于可验证的学术创新性评估

**原文标题：** OpenNovelty: An LLM-powered Agentic System for Verifiable Scholarly Novelty Assessment

**摘要：**
在同行评审中，评估创新性至关重要但也极具挑战，因为评审人必须在庞大且快速发展的文献背景下对投稿进行评判。本报告介绍了OpenNovelty，这是一个基于大语言模型的代理系统，旨在实现透明、基于证据的创新性分析。该系统通过四个阶段运行：（1）提取核心任务与贡献主张以生成检索查询；（2）通过语义搜索引擎，基于提取的查询检索相关先前工作；（3）构建与核心任务相关工作的层次化分类体系，并在全文层面对每项贡献进行对比分析；（4）将所有分析综合成一份结构化的创新性报告，其中包含明确的引用和证据片段。与简单基于大语言模型的方法不同，OpenNovelty将所有评估建立在检索到的真实论文基础上，确保判断可验证。我们在500多篇ICLR 2026投稿上部署了本系统，所有报告均在我们的网站上公开，初步分析表明该系统能够识别相关先前工作，包括作者可能忽略的密切关联论文。OpenNovelty旨在为研究界提供一个可扩展的工具，以促进公平、一致且基于证据的同行评审。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2601.01576) | [arXiv](https://arxiv.org/abs/2601.01576)



---

### 21. 选择性不完美：作为分析、创造与发现的生成框架

**原文标题：** Selective Imperfection as a Generative Framework for Analysis, Creativity and Discovery

**摘要：**
本文提出"物质音乐"这一生成框架，将物质的层级结构与音乐的创作逻辑相联结。从蛋白质、蜘蛛网到火焰动力学，振动与建筑原理以音调层级、和声进行与宏观曲式等形式重现。通过可逆映射——从分子光谱到乐音，从三维网络到可演奏乐器——我们揭示声音如何作为科学探针，实现认知反转：聆听成为观察方式，音乐创作转化为物质蓝图。这些映射发掘深层时间：源自飞秒分子振动或亿万年演化史的模式变得可闻。我们主张，当约束无法在现有自由度内满足时，科学与艺术的新颖性便随之涌现，这迫使可行构型空间发生扩展。选择性不完美提供了恢复连贯性与适应性平衡的机制。对全部2^12种音阶的穷举枚举提供了量化支持：具有文化意义的音乐体系聚集于中等熵值与中等缺陷的通道，这直接对应于霍尔-佩奇最优区间——即中等缺陷密度使材料强度最大化。迭代这些映射在人类创造力与物理规律之间创造生产性碰撞，当音乐结构遭遇演化约束时便生成新信息。我们展示基于群体智能的AI模型如何创作出具有类人结构特征的音乐，如小世界连接性、模块化整合、长程连贯性，这指明了超越插值迈向创造的路径。研究表明，科学与艺术都是在约束下进行世界构建的生成性实践，振动作为共享语法组织着跨尺度的结构。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2601.00863) | [arXiv](https://arxiv.org/abs/2601.00863)



---

### 22. IMA++：ISIC档案多标注者皮肤镜皮肤病变分割数据集

**原文标题：** IMA++: ISIC Archive Multi-Annotator Dermoscopic Skin Lesion Segmentation Dataset

**摘要：**
多标注者医学图像分割是一个重要的研究课题，但需要耗费高昂成本收集标注数据集。皮肤镜皮肤病变成像技术使得人类专家和人工智能系统能够观察到常规临床照片无法辨别的形态学结构。然而，目前尚无大规模公开的、包含标注者标签的皮肤镜皮肤病变分割多标注者数据集。本研究提出了ISIC MultiAnnot++，这是一个基于ISIC档案图像的大型公开多标注者皮肤病变分割数据集。最终数据集包含覆盖14,967张皮肤镜图像的17,684个分割掩码，其中2,394张皮肤镜图像每张包含2-5个分割标注，使其成为当前最大的公开皮肤病变分割数据集。此外，数据集还提供了包括标注者技能水平和分割工具在内的分割元数据，支持针对标注者特异性偏好建模、标注者元数据分析等课题的研究。本文对该数据集的特征、整理的数据分区及共识分割掩码进行了系统性分析。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.21472) | [arXiv](https://arxiv.org/abs/2512.21472)



---

### 23. Prithvi-互补自适应融合编码器（CAFE）：释放洪水淹没制图的全潜力

**原文标题：** Prithvi-Complimentary Adaptive Fusion Encoder (CAFE): unlocking full-potential for flood inundation mapping

**摘要：**
地理基础模型（GFMs）已被证明在多种下游应用中具有良好效果，包括语义分割、分类和回归任务。然而，在使用Sen1Flood11数据集进行洪水制图的下游任务中，GFMs难以超越基准U-Net模型，突显了其在捕捉关键局部细节方面的局限性。为解决这一问题，我们提出了Prithvi-互补自适应融合编码器（CAFE），该模型将预训练的Prithvi GFM编码器与一个通过卷积注意力模块（CAM）增强的并行CNN残差分支相结合。Prithvi-CAFE通过适配器实现Prithvi模型的快速高效微调，并与CNN特征进行多尺度、多层次融合，从而在保持长程依赖关系的同时捕捉关键的局部细节。我们在两个综合性洪水制图数据集（Sen1Flood11和FloodPlanet）上取得了最先进的结果。在Sen1Flood11测试数据上，Prithvi-CAFE（交并比83.41）优于原始Prithvi模型（交并比82.50）及其他主要GFMs（TerraMind 82.90、DOFA 81.54、spectralGPT 81.02）。在独立测试站点上改进更为显著：Prithvi-CAFE交并比达到81.37，而基准U-Net为70.57，原始Prithvi为72.42。在FloodPlanet数据集上，Prithvi-CAFE同样超越了基准U-Net及其他GFMs，交并比达到64.70，优于U-Net（60.14）、Terramind（62.33）、DOFA（59.15）和Prithvi 2.0（61.91）。我们提出的Prithvi-CAFE方法简洁而高效，在多通道多模态数据提供互补信息且局部细节至关重要的分割任务中展现出强大潜力。代码已发布于https://github.com/Sk-2103/Prithvi-CAFE。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2601.02315) | [arXiv](https://arxiv.org/abs/2601.02315)



---

### 24. 阿里阿德涅项目：一种用于审计大语言模型智能体忠实性的结构因果框架

**原文标题：** Project Ariadne: A Structural Causal Framework for Auditing Faithfulness in LLM Agents

**摘要：**
随着大语言模型智能体日益承担高风险自主决策任务，其推理过程的透明度已成为关键的安全关切。尽管思维链提示技术使智能体能够生成人类可读的推理轨迹，但这些轨迹究竟是模型输出的真实生成驱动因素，抑或仅为事后合理化解释，目前尚不明确。本文提出“阿里阿德涅项目”——一种创新的可解释人工智能框架，该框架利用结构因果模型与反事实逻辑来审计智能体推理的因果完整性。与依赖表层文本相似度的现有可解释性方法不同，本项目通过对中间推理节点实施硬干预（do-演算），系统性地进行逻辑反转、前提否定与事实主张逆转，以测量最终答案的因果敏感度。我们对前沿模型的实证评估揭示了持续存在的“忠实性鸿沟”。我们定义并检测到一种普遍存在的失效模式——“因果解耦”，该模式在事实与科学领域中表现出高达0.77的违反密度。在此类案例中，智能体在内部逻辑矛盾的情况下仍得出相同结论，证明其推理轨迹实为“推理剧场”，而决策过程实则受潜在参数先验支配。我们的研究结果表明，当前智能体架构本质上易产生不忠实的解释，为此我们提出“阿里阿德涅分数”作为衡量陈述逻辑与模型行为一致性的新基准。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2601.02314) | [arXiv](https://arxiv.org/abs/2601.02314)



---

### 25. M-ErasureBench：扩散模型概念擦除的多模态综合评估基准

**原文标题：** M-ErasureBench: A Comprehensive Multimodal Evaluation Benchmark for Concept Erasure in Diffusion Models

**摘要：**
文本到图像扩散模型可能生成有害或受版权保护的内容，这推动了概念擦除技术的研究。然而，现有方法主要集中于从文本提示中擦除概念，忽视了在实际应用（如图像编辑和个性化生成）中日益重要的其他输入模态。这些模态可能成为攻击面，导致已擦除的概念绕过防御机制重新出现。为弥补这一空白，我们提出了M-ErasureBench——一个新颖的多模态评估框架，系统性地在三种输入模态（文本提示、学习嵌入和反转潜在表示）上对概念擦除方法进行基准测试。针对后两种模态，我们同时评估白盒与黑盒访问场景，共构建五种评估情境。分析表明，现有方法在应对文本提示时表现出较强的擦除能力，但在学习嵌入和反转潜在表示场景下普遍失效，其中白盒设置下的概念再现率（CRR）超过90%。为应对这些漏洞，我们提出IRECE（概念擦除的推理时鲁棒性增强模块），该即插即用模块通过交叉注意力定位目标概念，并在去噪过程中扰动相关潜在表示。实验证明，IRECE能持续恢复模型鲁棒性，在最具挑战性的白盒潜在反转场景中将CRR降低达40%，同时保持视觉质量。据我们所知，M-ErasureBench首次建立了超越文本提示的全面概念擦除评估基准。结合IRECE方法，本基准为构建更可靠的保护性生成模型提供了实用保障。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.22877) | [arXiv](https://arxiv.org/abs/2512.22877)



---


## 🔍 关键词云图
![关键词云图](../images/keywords_wordcloud.png)

## 📈 近期论文趋势
![论文趋势](../images/daily_papers.png)

## 🎙️ 语音播报
- [收听今日论文解读](../audio/2026-01-06_daily_papers.mp3)

## 📱 订阅渠道
- GitHub: [hf-daily-paper-newsletter-chinese](https://github.com/2404589803/hf-daily-paper-newsletter-chinese)