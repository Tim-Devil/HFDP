
# <img src="https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo.png" width="30"/> Hugging Face 2026-01-06 论文日报

## 📊 今日论文统计
- 总论文数：25
- 热门领域：LLM, Transformer, RL, GPT

## 📝 论文详情


### 1. 大语言模型能否预测自身失误？基于内部回路的自我感知机制

**原文标题：** Can LLMs Predict Their Own Failures? Self-Awareness via Internal Circuits

**摘要：**
大语言模型（LLMs）能够生成流畅复杂的输出，却往往无法识别自身的错误与幻觉。现有方法通常依赖外部评判器、多样本一致性检验或基于文本的自我批判，这些方式要么需要额外计算资源，要么与真实准确性的关联度较弱。本文提出核心问题：大语言模型能否通过推理过程中对内部状态的监测来预测自身失误？我们引入Gnosis——一种轻量级自我感知机制，使冻结参数的大语言模型能够通过解码隐藏状态与注意力模式的信号进行内在自我验证。Gnosis被动观测内部计算轨迹，将其压缩为固定预算的描述符，并以可忽略的推理成本预测正确性，仅增加约500万参数且运算独立于序列长度。在数学推理、开放域问答和学术知识基准测试中，针对1.7B至20B参数规模的冻结骨干模型，Gnosis在准确率与校准度方面持续优于强内部基线及大型外部评判器。此外，该机制能零样本泛化至部分生成结果，实现对错误轨迹的早期检测及计算感知控制。这些结果表明，可靠的正确性线索内生于生成过程，无需外部监督即可高效提取。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.20578) | [arXiv](https://arxiv.org/abs/2512.20578)



---

### 2. K-EXAONE 技术报告

**原文标题：** K-EXAONE Technical Report

**摘要：**
本技术报告介绍了由 LG AI Research 开发的大规模多语言语言模型 K-EXAONE。K-EXAONE 基于混合专家架构构建，总参数量达 2360 亿，推理时激活 230 亿参数。它支持 256K 令牌的上下文窗口，并涵盖六种语言：韩语、英语、西班牙语、德语、日语和越南语。我们在涵盖推理、智能体、通用能力、韩语能力及多语言能力的综合基准测试套件上对 K-EXAONE 进行了评估。在所有评估中，K-EXAONE 展现出与同类规模开源模型相当的性能。K-EXAONE 旨在推动人工智能发展以创造更美好的生活，其定位是为广泛的工业和研究应用提供强大的专有 AI 基础模型。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2601.01739) | [arXiv](https://arxiv.org/abs/2601.01739)



---

### 3. NextFlow：统一序列建模激活多模态理解与生成能力

**原文标题：** NextFlow: Unified Sequential Modeling Activates Multimodal Understanding and Generation

**摘要：**
本文提出NextFlow模型，这是一种仅含解码器的统一自回归Transformer架构，在6万亿交错排列的文本-图像离散标记上进行训练。通过在统一的自回归架构中采用统一的视觉表征方法，NextFlow原生激活了多模态理解与生成能力，实现了图像编辑、交错内容生成和视频生成等功能。针对不同模态的本质特性——文本具有严格序列性而图像具有内在层次性——我们保留文本的下一标记预测机制，但对视觉生成采用下一尺度预测方法。这一设计突破了传统光栅扫描方法的局限，仅需5秒即可生成1024×1024分辨率图像，比同类自回归模型快数个数量级。我们通过稳健的训练方案解决了多尺度生成的不稳定性问题，并提出了用于强化学习的前缀调优策略。实验表明，NextFlow在统一模型中实现了最先进的性能，其视觉质量可与专业扩散基线模型相媲美。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2601.02204) | [arXiv](https://arxiv.org/abs/2601.02204)



---

### 4. DreamID-V：基于扩散Transformer的高保真人脸视频替换——弥合图像到视频的鸿沟

**原文标题：** DreamID-V:Bridging the Image-to-Video Gap for High-Fidelity Face Swapping via Diffusion Transformer

**摘要：**
视频人脸替换任务要求将源身份无缝注入目标视频，同时精确保持原始姿态、表情、光照、背景及动态信息。现有方法在保持时序一致性的同时，难以兼顾身份相似性与属性保留。为解决这一挑战，我们提出一个完整框架，将图像人脸替换技术的优势无缝迁移至视频领域。我们首先设计新型数据流水线SyncID-Pipe，通过预训练身份锚定视频合成器并结合图像人脸替换模型，构建双向身份四元组以实现显式监督。基于配对数据，我们提出首个基于扩散Transformer的框架DreamID-V，其核心模态感知调节模块能够 discriminatively 注入多模态条件。同时，我们提出合成到真实的课程学习机制与身份一致性强化学习策略，以增强复杂场景下的视觉真实感与身份一致性。针对现有基准数据不足的问题，我们构建了涵盖多样化场景的综合评测基准IDBench-V。大量实验表明，DreamID-V在性能上超越现有最优方法，并展现出卓越的泛化能力，可无缝适配多种替换相关任务。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2601.01425) | [arXiv](https://arxiv.org/abs/2601.01425)



---

### 5. VAR强化学习的正确实现：解决视觉自回归生成中的异步策略冲突

**原文标题：** VAR RL Done Right: Tackling Asynchronous Policy Conflicts in Visual Autoregressive Generation

**摘要：**
视觉生成领域主要由三种范式主导：自回归模型、扩散模型以及视觉自回归模型。与自回归和扩散模型不同，视觉自回归模型在其生成步骤中处理异构输入结构，这导致了严重的异步策略冲突。该问题在强化学习场景中尤为突出，常引发训练过程不稳定与目标对齐欠佳。为解决此问题，我们提出一种新颖框架，通过显式管理这些冲突来增强分组相对策略优化方法。该框架整合了三个协同组件：1）用于引导早期生成阶段的稳定化中间奖励机制；2）实现精确贡献度分配的动态时间步重加权方案；3）一种基于奖励反馈学习原理设计的新型掩码传播算法，可在空间与时间维度同时隔离优化效应。实验表明，相较于原始分组相对策略优化基线，我们的方法在样本质量与目标对齐方面均取得显著提升，为视觉自回归模型实现了鲁棒且高效的优化。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2601.02256) | [arXiv](https://arxiv.org/abs/2601.02256)



---

### 6. GARDO：无需奖励破解的扩散模型强化方法

**原文标题：** GARDO: Reinforcing Diffusion Models without Reward Hacking

**摘要：**
通过在线强化学习对扩散模型进行微调已展现出提升文本-图像对齐能力的巨大潜力。然而，由于视觉任务中精确指定真实目标仍具挑战性，模型通常使用仅部分反映真实目标的代理奖励进行优化。这种不匹配常导致奖励破解现象，即代理分数上升的同时真实图像质量下降且生成多样性崩溃。常见解决方案通过添加针对参考策略的正则化来防止奖励破解，但由于参考策略通常并非最优，这些方法会牺牲样本效率并阻碍对新颖高奖励区域的探索。为平衡样本效率、有效探索和缓解奖励破解之间的竞争性需求，我们提出具有多样性感知优化的门控自适应正则化框架（GARDO），该通用框架可与多种强化学习算法兼容。我们的核心见解是：正则化无需普遍应用，而选择性地对高不确定性样本子集进行惩罚效果显著。针对探索挑战，GARDO引入自适应正则化机制，定期更新参考模型以匹配在线策略的能力，从而确保正则化目标的时效性。针对强化学习中的模式崩溃问题，GARDO通过放大兼具高质量与高多样性的样本奖励，在不破坏优化稳定性的前提下促进模式覆盖。在多种代理奖励和未见保留指标上的大量实验一致表明，GARDO能在不牺牲样本效率或探索能力的情况下有效缓解奖励破解并提升生成多样性，彰显了其效能与鲁棒性。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.24138) | [arXiv](https://arxiv.org/abs/2512.24138)



---

### 7. VINO：基于交错式全模态上下文的统一视觉生成器

**原文标题：** VINO: A Unified Visual Generator with Interleaved OmniModal Context

**摘要：**
本文提出VINO，一个在单一框架内实现图像与视频生成及编辑的统一视觉生成器。不同于依赖针对特定任务的模型或为各模态设计独立模块的传统方案，VINO采用共享的扩散模型主干，能够同时接受文本、图像和视频作为条件输入，从而在单一模型下支持广泛的视觉创作与编辑任务。具体而言，VINO将视觉语言模型（VLM）与多模态扩散变换器（MMDiT）相结合，其中多模态输入被编码为交错排列的条件标记，进而引导扩散生成过程。该设计支持多参考对象关联、长指令序列跟随以及静态与动态内容间的连贯身份保持，同时避免了针对特定模态的专用结构组件。为训练这一统一系统，我们提出多阶段训练流程，逐步将基础视频生成模型扩展为能够同时处理图像与视频输入输出的统一多任务生成器。在多样化的生成与编辑基准测试中，VINO展现出卓越的视觉质量、精准的指令跟随能力、改进的参考对象与属性保持效果，以及更可控的多身份编辑性能。我们的研究成果为可扩展的统一视觉生成提供了可行路径，并揭示了交错式上下文计算作为通用视觉创作基础技术的潜力。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2601.02358) | [arXiv](https://arxiv.org/abs/2601.02358)



---

### 8. InfiniteVGGT：面向无限流数据的视觉几何基础Transformer

**原文标题：** InfiniteVGGT: Visual Geometry Grounded Transformer for Endless Streams

**摘要：**
实现持久、大规模三维视觉几何理解的宏伟愿景，一直受制于可扩展性与长期稳定性之间难以调和的矛盾。尽管VGGT等离线模型展现出卓越的几何理解能力，但其批处理模式使其无法适用于实时系统。而专为实时操作设计的流式架构，现有方法亦被证明存在不足：它们要么无法支持真正无限时长的输入，要么在长序列处理中遭受灾难性的性能漂移。本文提出的InfiniteVGGT彻底打破了这一长期困境。该模型是一种因果视觉几何Transformer，通过构建一个有界、自适应且持续保持表达力的KV缓存，实现了“滚动记忆”的操作化。基于此，我们设计了一种无需训练、与注意力机制无关的剪枝策略，能够智能地丢弃过时信息，随着每一帧新数据的输入有效“滚动”更新记忆。InfiniteVGGT完全兼容FlashAttention，最终消除了传统方案中的性能妥协，在支持无限时长流式处理的同时，其长期稳定性超越了现有所有流式方法。此类系统的终极考验在于其在真正无限时长上的性能，而由于极度缺乏超长时、连续的基准测试数据，该能力一直无法得到严格验证。为填补这一关键空白，我们首次提出了Long3D基准测试集，该数据集支持对约10,000帧长度的序列进行连续三维几何估计的严格评估，从而为未来长期三维几何理解研究提供了权威性的评估平台。代码已开源：https://github.com/AutoLab-SAI-SJTU/InfiniteVGGT

**论文链接：** [HuggingFace](https://huggingface.co/papers/2601.02281) | [arXiv](https://arxiv.org/abs/2601.02281)



---

### 9. 递归语言模型

**原文标题：** Recursive Language Models

**摘要：**
本文从推理时扩展的视角出发，研究如何使大型语言模型能够处理任意长度的提示文本。我们提出递归语言模型——一种通用的推理策略，将长提示文本视为外部环境的一部分，允许大型语言模型以编程方式对提示片段进行检测、分解和递归调用。实验表明，递归语言模型能够成功处理超出模型上下文窗口两个数量级的输入；即使在较短提示任务中，其在四项不同的长上下文任务上也显著优于基础大型语言模型及常见的长上下文框架方法，同时保持可比（或更低）的单次查询成本。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.24601) | [arXiv](https://arxiv.org/abs/2512.24601)



---

### 10. Falcon-H1R：通过混合模型推动推理前沿，实现高效测试时扩展

**原文标题：** Falcon-H1R: Pushing the Reasoning Frontiers with a Hybrid Model for Efficient Test-Time Scaling

**摘要：**
本研究介绍了Falcon-H1R，一个拥有70亿参数、专为推理优化的模型，它证明了小型语言模型（SLMs）也能实现具有竞争力的推理性能。Falcon-H1R以其参数效率脱颖而出，在多种推理密集型基准测试中，其性能持续匹配甚至超越参数量为其2至7倍的当前最优推理模型。这些结果凸显了精细的数据筛选和针对性训练策略（通过高效的监督微调和强化学习扩展）的重要性，它们能在不增加模型规模的情况下带来显著的性能提升。此外，Falcon-H1R通过结合更快的推理速度（得益于其混合并行架构设计）、更高的令牌效率和更高的准确性，推进了推理效率的“三维”极限。这种独特的组合使Falcon-H1R-7B成为扩展高级推理系统的实用骨干模型，尤其适用于需要大量思维链生成和并行测试时扩展的场景。借助近期提出的DeepConf方法，Falcon-H1R实现了最先进的测试时扩展效率，在准确性和计算成本方面均有显著改善。因此，Falcon-H1R证明，通过针对性的模型训练和架构选择，紧凑模型能够提供强大且可扩展的推理性能。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2601.02346) | [arXiv](https://arxiv.org/abs/2601.02346)



---

### 11. SimpleMem：面向大语言模型智能体的高效终身记忆框架

**原文标题：** SimpleMem: Efficient Lifelong Memory for LLM Agents

**摘要：**
为支持复杂环境中可靠的长期交互，大语言模型智能体需要能高效管理历史经验的记忆系统。现有方法要么通过被动扩展上下文保留完整交互历史，导致显著冗余；要么依赖迭代推理过滤噪声，产生高昂的令牌成本。为应对这一挑战，我们提出SimpleMem——一种基于语义无损压缩的高效记忆框架。我们设计了一个三阶段流程以最大化信息密度与令牌利用率：（1）语义结构化压缩：通过熵感知过滤将非结构化交互提炼为紧凑的多视图索引记忆单元；（2）递归记忆整合：通过异步过程将相关单元融合为更高层次的抽象表征以降低冗余；（3）自适应查询感知检索：根据查询复杂度动态调整检索范围，高效构建精准上下文。在基准数据集上的实验表明，本方法在准确性、检索效率与推理成本方面均优于基线方法，平均F1值提升26.4%，推理阶段令牌消耗最高降低至三十分之一，实现了性能与效率的卓越平衡。代码已开源：https://github.com/aiming-lab/SimpleMem。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2601.02553) | [arXiv](https://arxiv.org/abs/2601.02553)



---

### 12. Talk2Move：基于强化学习的场景中文本指令对象级几何变换方法

**原文标题：** Talk2Move: Reinforcement Learning for Text-Instructed Object-Level Geometric Transformation in Scenes

**摘要：**
本文提出Talk2Move，一种基于强化学习（RL）的扩散框架，用于实现场景中对象的文本指令空间变换。通过自然语言对场景中的对象进行空间操控是多模态生成系统面临的挑战。现有基于文本的编辑方法虽能调整外观或风格，但由于缺乏成对的监督数据及像素级优化的局限性，难以实现对象级的几何变换（如平移、旋转或缩放）。Talk2Move采用组相对策略优化（GRPO），通过输入图像与轻量级文本变体生成多样化轨迹来探索几何动作，从而避免了对高成本配对数据的依赖。空间奖励引导模型将几何变换与语言描述对齐，同时离轨步长评估与主动步长采样通过聚焦于信息丰富的变换阶段提升了学习效率。此外，我们设计了以对象为中心的空间奖励机制，直接评估位移、旋转和缩放行为，实现了可解释且连贯的变换效果。在精选基准测试上的实验表明，Talk2Move能够实现精确、一致且语义保真的对象变换，在空间准确性与场景连贯性方面均优于现有文本引导编辑方法。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2601.02356) | [arXiv](https://arxiv.org/abs/2601.02356)



---

### 13. 多轮交互中大语言模型的置信度估计

**原文标题：** Confidence Estimation for LLMs in Multi-turn Interactions

**摘要：**
尽管置信度估计是缓解大语言模型幻觉现象的重要方向，但现有研究主要集中于单轮交互场景。在多轮对话中，随着上下文信息累积与歧义逐步消解，模型置信度的动态变化机制尚未得到充分探索。可靠的置信度估计对自主智能体、人机协同系统等下游应用至关重要。本研究首次系统性地探讨多轮交互中的置信度估计问题，建立了基于双重核心需求的规范化评估框架：单轮校准性及信息增量下的置信度单调性。为此，我们提出了创新性评估指标（包括长度归一化的预期校准误差InfoECE），并设计了"提示者-猜测者"范式以构建受控评估数据集。实验表明，当前主流置信度估计技术在多轮对话中普遍存在校准失效与单调性缺失问题。我们提出的基于逻辑值的探测方法P(Sufficient)取得了相对更优的性能，但该任务仍远未完全解决。本研究为开发更可靠、可信的对话智能体奠定了方法论基础。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2601.02179) | [arXiv](https://arxiv.org/abs/2601.02179)



---

### 14. KV-Embedding：基于仅解码器大语言模型内部KV重路由的无训练文本嵌入方法

**原文标题：** KV-Embedding: Training-free Text Embedding via Internal KV Re-routing in Decoder-only LLMs

**摘要：**
尽管大语言模型（LLMs）是强大的嵌入骨干网络，但其在无训练场景下的应用面临两大结构挑战：因果注意力机制限制了早期令牌访问后续上下文的能力，而下一令牌预测目标使表示偏向生成任务而非语义压缩。为克服这些局限，本文提出KV-Embedding框架，旨在激活冻结大语言模型的潜在表示能力。该方法基于以下观察：每个网络层中最终令牌的键值（KV）状态编码了序列的压缩视图。通过将这些状态重路由为前置前缀，我们使所有令牌能在单次前向传播中访问序列级上下文。为确保模型无关的适用性，我们提出基于本征维度的自动化层级选择策略。在Qwen、Mistral和Llama骨干网络上进行的MTEB评估表明，KV-Embedding相比现有无训练基线方法性能提升最高达10%，同时在长达4,096个令牌的序列上保持稳健性能。这些结果证明，内部状态操作为输入修改提供了高效替代方案，我们期望此项工作能推动基于大语言模型内部机制的表示学习研究。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2601.01046) | [arXiv](https://arxiv.org/abs/2601.01046)



---

### 15. CPPO：面向视觉语言策略优化的对比感知方法

**原文标题：** CPPO: Contrastive Perception for Vision Language Policy Optimization

**摘要：**
本文提出CPPO（对比感知策略优化方法），一种用于微调视觉语言模型（VLM）的新方法。尽管强化学习（RL）已显著提升了语言模型的推理能力，但将其扩展至多模态推理需要同时改进感知与推理两个维度。现有研究主要通过显式感知奖励应对这一挑战，但将感知标记与推理标记有效分离存在困难——往往需要额外的大语言模型、真实标注数据、强制策略模型分离感知与推理，或对全部输出标记 indiscriminately 施加奖励。CPPO通过分析输入图像受扰动时模型输出的熵值变化来检测感知标记，进而提出对比感知损失（CPL）以扩展RL目标函数：该方法在信息保留型扰动下强化输出一致性，在信息消除型扰动下增强敏感性。实验表明，CPPO在超越现有感知奖励方法的同时，无需引入额外模型，显著提升了训练效率与可扩展性。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2601.00501) | [arXiv](https://arxiv.org/abs/2601.00501)



---

### 16. DiffProxy：基于扩散生成密集代理的多视角人体网格重建方法

**原文标题：** DiffProxy: Multi-View Human Mesh Recovery via Diffusion-Generated Dense Proxies

**摘要：**
多视角图像的人体网格重建面临一个根本性挑战：现实世界数据集包含不完美的真实标注，会导致模型训练产生偏差；而具有精确标注的合成数据则存在领域差异问题。本文提出DiffProxy，一种通过生成多视角一致的人体代理进行网格重建的创新框架。该框架的核心在于利用基于扩散模型的生成先验知识，弥合合成数据训练与真实场景泛化之间的差距。其主要创新包括：（1）采用多条件生成机制，实现多视角一致且像素对齐的人体代理生成；（2）设计手部精细化模块，通过灵活的可视化提示增强局部细节；（3）提出不确定性感知的测试时缩放方法，在优化过程中提升对挑战性场景的鲁棒性。这些设计确保网格重建过程能有效利用精确的合成真实标注，并充分发挥基于扩散流程的生成优势。DiffProxy完全在合成数据上训练，在五个真实世界基准测试中均达到最先进性能，尤其在存在遮挡和局部视角的挑战性场景中展现出强大的零样本泛化能力。项目页面：https://wrk226.github.io/DiffProxy.html

**论文链接：** [HuggingFace](https://huggingface.co/papers/2601.02267) | [arXiv](https://arxiv.org/abs/2601.02267)



---

### 17. COMPASS：评估大型语言模型中组织特定政策对齐性的框架

**原文标题：** COMPASS: A Framework for Evaluating Organization-Specific Policy Alignment in LLMs

**摘要：**
随着大型语言模型在从医疗保健到金融等高风险企业应用中的部署，确保其遵守组织特定政策已变得至关重要。然而现有的安全性评估仅聚焦于通用风险。本文提出COMPASS（公司/组织政策对齐评估）框架，这是首个系统化评估大型语言模型是否符合组织允许清单与禁止清单政策的框架。我们将COMPASS应用于八个不同行业场景，通过战略设计的边缘案例生成并验证了5,920条查询，同时测试常规合规性与对抗鲁棒性。在对七个前沿模型进行评估后，我们发现了一个根本性不对称现象：模型能可靠处理合法请求（准确率>95%），但在执行禁令时出现严重失效，仅能拒绝13-40%的对抗性禁止清单违规请求。这些结果表明当前大型语言模型缺乏政策关键型部署所需的鲁棒性，从而确立了COMPASS作为组织人工智能安全评估的核心框架地位。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2601.01836) | [arXiv](https://arxiv.org/abs/2601.01836)



---

### 18. SWE-Lego：探索监督微调在软件问题解决中的性能极限

**原文标题：** SWE-Lego: Pushing the Limits of Supervised Fine-tuning for Software Issue Resolving

**摘要：**
本文提出SWE-Lego——一种专为软件工程问题解决任务设计的监督微调方案，旨在实现最先进的性能表现。与当前依赖复杂训练范式（如中期训练、监督微调、强化学习及其组合）的主流方法不同，本研究探索如何突破轻量级纯监督微调方法在软件工程任务中的性能极限。SWE-Lego包含三个核心构建模块，关键发现总结如下：1）SWE-Lego数据集包含3.2万个高质量任务实例与1.8万条已验证执行轨迹，融合真实数据与合成数据以在质量与数量上形成互补；2）采用错误掩码与难度分级课程的精细化监督微调流程，可显著提升动作质量与整体性能。实验结果表明，仅通过这两个构建模块，监督微调即可使SWE-Lego模型在同等规模的开源模型中达到SWE-bench Verified基准的最优性能：SWE-Lego-Qwen3-8B达到42.2%，SWE-Lego-Qwen3-32B达到52.6%。3）我们在监督微调基础上进一步评估并改进了测试时扩展方法。基于训练完备的验证器，SWE-Lego模型性能可获得显著提升——在TTS@16设置下，8B模型从42.2%提升至49.6%，32B模型从52.6%提升至58.8%。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2601.01426) | [arXiv](https://arxiv.org/abs/2601.01426)



---

### 19. 迈向稳定半监督遥感分割：基于协同引导与协同融合的方法

**原文标题：** Toward Stable Semi-Supervised Remote Sensing Segmentation via Co-Guidance and Co-Fusion

**摘要：**
半监督遥感图像语义分割为缓解详尽标注负担提供了有前景的解决方案，但其本质上受伪标签漂移问题困扰——这种因确认偏差导致训练过程中误差累积的现象。本研究提出Co2S框架，一种通过协同融合视觉语言模型与自监督模型先验知识的稳定半监督遥感分割方法。具体而言，我们构建了异构双学生架构，包含两个基于ViT的视觉基础模型，分别采用预训练的CLIP和DINOv3初始化，以抑制误差累积和伪标签漂移。为有效整合这些异构先验知识，我们引入显式-隐式语义协同引导机制：利用文本嵌入提供显式类别级引导，同时通过可学习查询向量实现隐式类别引导，共同增强语义一致性。此外，设计了全局-局部特征协同融合策略，将CLIP捕获的全局上下文信息与DINOv3提取的局部细节有效融合，使模型能够生成高精度分割结果。在六个主流数据集上的大量实验表明，该方法在不同数据划分协议和多样场景中均取得领先性能，验证了其优越性。项目页面详见：https://xavierjiezou.github.io/Co2S/。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.23035) | [arXiv](https://arxiv.org/abs/2512.23035)



---

### 20. OpenNovelty：一种基于大语言模型的可验证学术新颖性评估智能体系统

**原文标题：** OpenNovelty: An LLM-powered Agentic System for Verifiable Scholarly Novelty Assessment

**摘要：**
在同行评审中，评估新颖性至关重要但也极具挑战，因为评审人必须在庞大且快速发展的文献背景下对投稿进行评判。本报告介绍了OpenNovelty，这是一个基于大语言模型的智能体系统，旨在实现透明、基于证据的新颖性分析。该系统通过四个阶段运行：（1）提取核心任务与贡献主张以生成检索查询；（2）通过语义搜索引擎基于提取的查询检索相关先前工作；（3）构建与核心任务相关工作的层次化分类体系，并在全文层面逐项对比各贡献点；（4）将所有分析整合为一份结构化的新颖性报告，其中包含明确的引用和证据片段。与简单基于大语言模型的方法不同，OpenNovelty将所有评估建立在检索到的真实论文基础上，确保判断可验证。我们在500多篇ICLR 2026投稿上部署了本系统，所有报告均公开于项目网站，初步分析表明该系统能够识别相关先前工作，包括作者可能忽略的密切关联论文。OpenNovelty旨在为研究社区提供一个可扩展的工具，以促进公平、一致且基于证据的同行评审。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2601.01576) | [arXiv](https://arxiv.org/abs/2601.01576)



---

### 21. 选择性不完美：作为分析、创造与发现的生成框架

**原文标题：** Selective Imperfection as a Generative Framework for Analysis, Creativity and Discovery

**摘要：**
本文提出“物质音乐学”这一生成框架，将物质的层级结构与音乐的创作逻辑相联结。从蛋白质、蜘蛛网到火焰动力学，振动与建筑原理以音调层级、和声进行与宏观音乐形式反复呈现。通过可逆映射（从分子光谱到乐音，从三维网络到可演奏乐器），我们揭示声音如何作为科学探针，实现认知反转——聆听成为观察模式，音乐创作转化为物质蓝图。这些映射发掘深层时间：源自飞秒分子振动或亿万年演化史的模式变得可闻。我们主张，当约束无法在现有自由度内满足时，科学与艺术的新颖性便随之涌现，这迫使可行构型空间发生扩展。选择性不完美提供了恢复连贯性与适应性平衡的机制。量化支持来自对全部2^12种音阶的穷举枚举，结果表明具有文化意义的音乐体系聚集于中等熵值与中等缺陷的通道中，这与霍尔-佩奇最优区间直接对应——中等缺陷密度使材料强度最大化。迭代这些映射在人类创造力与物理规律之间创造生产性碰撞，当音乐结构遭遇演化约束时即生成新信息。我们展示基于群体智能的AI模型如何创作出具有类人结构特征的音乐，如小世界连接性、模块化整合、长程连贯性，这为超越插值迈向发明指明路径。研究表明，科学与艺术皆是约束下的世界建构生成行为，而振动正是跨尺度组织结构的共享语法。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2601.00863) | [arXiv](https://arxiv.org/abs/2601.00863)



---

### 22. IMA++：ISIC档案多标注者皮肤镜皮肤病变分割数据集

**原文标题：** IMA++: ISIC Archive Multi-Annotator Dermoscopic Skin Lesion Segmentation Dataset

**摘要：**
多标注者医学图像分割是一个重要的研究课题，但需要耗费高昂成本收集标注数据集。皮肤镜皮肤病变成像技术使得人类专家和人工智能系统能够观察到常规临床照片无法辨别的形态学结构。然而，目前尚无大规模公开可用的、包含标注者标签的皮肤镜皮肤病变分割多标注者数据集。本研究推出ISIC MultiAnnot++，这是一个基于ISIC档案图像的大型公开多标注者皮肤病变分割数据集。该数据集最终包含覆盖14,967张皮肤镜图像的17,684个分割掩码，其中2,394张皮肤镜图像每张包含2-5个独立分割标注，使其成为当前最大的公开皮肤病变分割数据集。此外，数据集还包含关于分割的元数据（包括标注者技能水平和分割工具），支持开展标注者特异性分割偏好建模、标注者元数据分析等方向的研究。我们对该数据集的特征进行了分析，提供了经过筛选的数据分区和共识分割掩码。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.21472) | [arXiv](https://arxiv.org/abs/2512.21472)



---

### 23. Prithvi-互补自适应融合编码器（CAFE）：释放洪水淹没制图的全潜力

**原文标题：** Prithvi-Complimentary Adaptive Fusion Encoder (CAFE): unlocking full-potential for flood inundation mapping

**摘要：**
地理基础模型（GFMs）已被证明在多种下游应用中具有显著效果，包括语义分割、分类和回归任务。然而，在将Sen1Flood11数据集用于洪水制图这一下游任务时，GFMs难以超越基准U-Net模型，突显了其在捕捉关键局部细节方面的局限性。为解决这一问题，本文提出了Prithvi-互补自适应融合编码器（CAFE），该模型将预训练的Prithvi GFM编码器与一个由卷积注意力模块（CAM）增强的并行CNN残差分支相结合。Prithvi-CAFE通过适配器实现Prithvi模型的快速高效微调，并与CNN特征进行多尺度、多层次融合，在保持长程依赖关系的同时捕捉关键的局部细节。我们在两个综合性洪水制图数据集（Sen1Flood11和FloodPlanet）上取得了最先进的成果。在Sen1Flood11测试数据上，Prithvi-CAFE（交并比83.41）优于原始Prithvi模型（交并比82.50）及其他主要GFMs（TerraMind 82.90、DOFA 81.54、spectralGPT 81.02）。在保留测试站点上，改进更为显著：Prithvi-CAFE的交并比达到81.37，而基准U-Net为70.57，原始Prithvi为72.42。在FloodPlanet数据集上，Prithvi-CAFE同样超越了基准U-Net及其他GFMs，交并比达到64.70，优于U-Net（60.14）、Terramind（62.33）、DOFA（59.15）和Prithvi 2.0（61.91）。我们提出的Prithvi-CAFE结构简洁而高效，在多通道与多模态数据提供互补信息且局部细节至关重要的分割任务中展现出强大潜力。代码已发布于https://github.com/Sk-2103/Prithvi-CAFE。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2601.02315) | [arXiv](https://arxiv.org/abs/2601.02315)



---

### 24. 阿里阿德涅项目：一种用于审计大语言模型智能体忠实度的结构因果框架

**原文标题：** Project Ariadne: A Structural Causal Framework for Auditing Faithfulness in LLM Agents

**摘要：**
随着大语言模型智能体越来越多地承担高风险自主决策任务，其推理过程的透明度已成为关键的安全关切。尽管思维链提示法使智能体能够生成人类可读的推理轨迹，但这些轨迹究竟是模型输出的真实生成驱动因素，抑或仅是事后合理化解释，目前尚不明确。我们提出“阿里阿德涅项目”——一个创新的可解释人工智能框架，该框架利用结构因果模型与反事实逻辑来审计智能体推理的因果完整性。与依赖表层文本相似性的现有可解释性方法不同，阿里阿德涅项目通过对中间推理节点实施硬干预（do-演算）——系统性地反转逻辑、否定前提并颠覆事实主张——以测量最终答案的因果敏感度。我们对前沿模型的实证评估揭示了一个持续存在的“忠实度鸿沟”。我们定义并检测到一种普遍存在的故障模式，称为“因果解耦”，该模式下智能体在事实与科学领域中的违规密度最高可达0.77。在这些案例中，尽管内部逻辑相互矛盾，智能体却得出相同结论，证明其推理轨迹实为“推理剧场”，而决策过程实则受隐式参数先验支配。我们的研究结果表明，当前智能体架构本质上易于产生不忠实的解释，为此我们提出“阿里阿德涅分数”作为评估陈述逻辑与模型行为一致性的新基准。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2601.02314) | [arXiv](https://arxiv.org/abs/2601.02314)



---

### 25. M-ErasureBench：扩散模型概念擦除的多模态综合评估基准

**原文标题：** M-ErasureBench: A Comprehensive Multimodal Evaluation Benchmark for Concept Erasure in Diffusion Models

**摘要：**
文本到图像扩散模型可能生成有害或受版权保护的内容，这推动了概念擦除技术的研究。然而，现有方法主要集中于从文本提示中擦除概念，忽视了图像编辑和个性化生成等实际应用中日益重要的其他输入模态。这些模态可能成为攻击面，导致已擦除的概念绕过防御机制重新出现。为弥补这一空白，我们提出了M-ErasureBench——一个新颖的多模态评估框架，系统性地在三种输入模态（文本提示、学习嵌入和反转潜在表示）上对概念擦除方法进行基准测试。针对后两种模态，我们同时评估白盒与黑盒访问场景，共构建五种评估情境。分析表明，现有方法在应对文本提示时表现出较强的擦除性能，但在学习嵌入和反转潜在表示场景下普遍失效，其中白盒设置下的概念再现率（CRR）超过90%。为应对这些漏洞，我们提出IRECE（概念擦除的推理时鲁棒性增强模块），该即插即用模块通过交叉注意力定位目标概念，并在去噪过程中扰动相关潜在表示。实验证明，IRECE能持续恢复模型鲁棒性，在最具挑战性的白盒潜在反转场景中将CRR降低达40%，同时保持视觉质量。据我们所知，M-ErasureBench首次建立了超越文本提示的全面概念擦除评估基准。结合IRECE，本基准为构建更可靠的保护性生成模型提供了实用保障方案。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.22877) | [arXiv](https://arxiv.org/abs/2512.22877)



---


## 🔍 关键词云图
![关键词云图](../images/keywords_wordcloud.png)

## 📈 近期论文趋势
![论文趋势](../images/daily_papers.png)

## 🎙️ 语音播报
- [收听今日论文解读](../audio/2026-01-06_daily_papers.mp3)

## 📱 订阅渠道
- GitHub: [hf-daily-paper-newsletter-chinese](https://github.com/2404589803/hf-daily-paper-newsletter-chinese)