<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Hugging Face 论文日报 - 2025-11-14</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', sans-serif;
            line-height: 1.6;
            color: #333;
            background-color: #f5f5f5;
            padding: 20px;
        }
        
        .container {
            max-width: 900px;
            margin: 0 auto;
            background-color: white;
            padding: 40px;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }
        
        h1 {
            color: #2c3e50;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
            margin-bottom: 30px;
            font-size: 28px;
        }
        
        h1 img {
            vertical-align: middle;
            margin-right: 10px;
        }
        
        h2 {
            color: #34495e;
            margin-top: 40px;
            margin-bottom: 20px;
            font-size: 24px;
            border-left: 4px solid #3498db;
            padding-left: 15px;
        }
        
        h3 {
            color: #2c3e50;
            margin-top: 30px;
            margin-bottom: 15px;
            font-size: 20px;
        }
        
        p {
            margin-bottom: 15px;
            text-align: justify;
        }
        
        ul {
            margin-left: 20px;
            margin-bottom: 20px;
        }
        
        li {
            margin-bottom: 8px;
        }
        
        a {
            color: #3498db;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        strong {
            color: #2c3e50;
            font-weight: 600;
        }
        
        hr {
            border: none;
            border-top: 1px solid #e0e0e0;
            margin: 30px 0;
        }
        
        /* 关键修复:限制图片宽度 */
        img {
            max-width: 100%;
            height: auto;
            display: block;
            margin: 20px auto;
            border-radius: 4px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        
        /* 确保图片容器也有宽度限制 */
        p img {
            max-width: 100%;
        }
        
        /* 论文详情区域样式 */
        .paper-section {
            background-color: #f9f9f9;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 20px;
        }
        
        /* 统计信息样式 */
        .stats {
            background-color: #e8f4f8;
            padding: 15px;
            border-radius: 6px;
            margin-bottom: 20px;
        }
        
        /* 响应式设计 */
        @media (max-width: 768px) {
            .container {
                padding: 20px;
            }
            
            h1 {
                font-size: 24px;
            }
            
            h2 {
                font-size: 20px;
            }
            
            h3 {
                font-size: 18px;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <h1><img src="https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo.png" width="30"/> Hugging Face 2025-11-14 论文日报</h1>
<h2>📊 今日论文统计</h2>
<ul>
<li>总论文数：18</li>
<li>热门领域：GPT, LLM, Transformer, Audio</li>
</ul>
<h2>📝 论文详情</h2>
<h3>1. 潜在空间一小步，像素生成大飞跃：面向扩散模型的快速潜在上采样适配器</h3>
<p><strong>原文标题：</strong> One Small Step in Latent, One Giant Leap for Pixels: Fast Latent Upscale Adapter for Your Diffusion Models</p>
<p><strong>摘要：</strong>
扩散模型难以突破训练分辨率限制，因为直接进行高分辨率采样速度缓慢且成本高昂，而事后图像超分辨率（ISR）方法在解码后执行操作，不仅会引入伪影还会增加额外延迟。我们提出潜在上采样适配器（LUA），这是一种轻量级模块，可在最终VAE解码步骤之前直接在生成器的潜在代码上执行超分辨率。LUA可作为即插即用组件集成，无需修改基础模型或增加额外扩散阶段，通过潜在空间中的单次前向传播即可实现高分辨率合成。采用共享Swin风格主干网络配合尺度特异性像素重组头，支持2倍和4倍缩放因子，同时保持与图像空间SR基线的兼容性，在实现相当感知质量的前提下，将解码与上采样时间降低近3倍（从512px生成1024px仅增加0.42秒，而使用相同SwinIR架构的像素空间SR需1.87秒）。此外，LUA在不同VAE的潜在空间中展现出强大泛化能力，无需为每个新解码器从头训练即可快速部署。大量实验表明，LUA在保真度上可媲美原生高分辨率生成，同时为现代扩散流程中的可扩展高保真图像合成提供了实用高效的技术路径。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.10629">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.10629">arXiv</a></p>
<hr />
<h3>2. PAN：面向通用、可交互与长时程世界模拟的世界模型</h3>
<p><strong>原文标题：</strong> PAN: A World Model for General, Interactable, and Long-Horizon World Simulation</p>
<p><strong>摘要：</strong>
世界模型使智能体能够通过想象来预测和推理世界如何响应其行为而演变，并据此进行规划与决策。尽管当前视频生成模型能生成逼真的视觉序列，但它们通常采用提示到完整视频的生成模式，缺乏因果控制、交互能力以及 purposeful 推理所需的长时程一致性。而现有的世界建模研究往往局限于特定领域（如物理系统、游戏或三维场景动态），其深度与可控性有限，且难以泛化至多样化环境与交互形式。本文提出PAN模型——一个通用、可交互且具备长时程预测能力的世界模型，能够基于历史状态与自然语言指令，通过高质量视频模拟预测未来世界状态。PAN采用生成式潜在预测架构，结合基于大语言模型的自回归潜在动态主干（利用文本知识实现语义 grounding 并支持语言指令条件生成）与视频扩散解码器（重建感知细节丰富且时序一致的视觉观测），实现了潜在空间推理（想象）与可实现世界动态（现实）的统一。通过在大规模跨领域视频-动作配对数据上训练，PAN支持开放领域、动作条件化的长时程动态模拟。大量实验表明，PAN在动作条件化世界模拟、长时程预测和模拟推理任务中均优于现有视频生成器与世界模型，为构建能够通过预测性模拟未来世界状态以支持推理与决策的通用世界模型迈出关键一步。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.09057">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.09057">arXiv</a></p>
<hr />
<h3>3. UniVA：面向开源的新一代视频通用智能体的通用视频智能体框架</h3>
<p><strong>原文标题：</strong> UniVA: Universal Video Agent towards Open-Source Next-Generation Video Generalist</p>
<p><strong>摘要：</strong>
尽管专业人工智能模型在视频生成或理解等独立任务中表现出色，但实际应用需要结合这些能力的复杂迭代工作流。为弥补这一差距，我们推出UniVA——一个面向新一代视频通用场景的开源全能力多智能体框架，将视频理解、分割、编辑与生成统一为连贯的工作流。UniVA采用规划-执行双智能体架构驱动高度自动化的工作流程：规划智能体解析用户意图并将其分解为结构化视频处理步骤，执行智能体则通过基于模型控制协议（MCP）的模块化工具服务器（支持分析、生成、编辑、跟踪等功能）实施这些步骤。通过分层多级记忆系统（全局知识、任务上下文与用户特定偏好），UniVA支持长周期推理、上下文连续性及智能体间通信，实现具有全链路可追溯性的交互式自反思视频创作。该设计使迭代式任意条件视频工作流（例如文本/图像/视频条件生成→多轮编辑→对象分割→组合合成）成为可能，这些流程以往使用单功能模型或单体视频语言模型难以实现。我们同时推出UniVA-Bench基准测试套件，涵盖理解、编辑、分割与生成的多步骤视频任务，用于严格评估此类智能视频系统。UniVA与UniVA-Bench均已全面开源，旨在推动面向新一代多模态AI系统的交互式、智能化通用视频智能研究。(https://univa.online/)</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.08521">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.08521">arXiv</a></p>
<hr />
<h3>4. 大语言模型的黑盒同策略蒸馏</h3>
<p><strong>原文标题：</strong> Black-Box On-Policy Distillation of Large Language Models</p>
<p><strong>摘要：</strong>
黑盒蒸馏技术仅通过专有教师模型的文本输出训练学生大语言模型，无需访问其内部逻辑值或参数。本研究提出生成对抗蒸馏方法，实现同策略的黑盒蒸馏。该方法将学生大语言模型构建为生成器，并训练判别器区分其响应与教师模型的输出，形成极小极大博弈框架。判别器作为随学生模型协同演进的同策略奖励模型，可提供稳定自适应的反馈机制。实验结果表明，生成对抗蒸馏方法持续优于常用的序列级知识蒸馏。特别值得注意的是，采用该方法训练的Qwen2.5-14B-Instruct模型在LMSYS-Chat自动评估中达到了与教师模型GPT-5-Chat相当的性能。这些成果确立了生成对抗蒸馏作为黑盒大语言模型蒸馏领域具有前景的有效范式。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.10643">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.10643">arXiv</a></p>
<hr />
<h3>5. 窃国者诛：探索去中心化GRPO中的攻击与防御策略</h3>
<p><strong>原文标题：</strong> Hail to the Thief: Exploring Attacks and Defenses in Decentralised GRPO</p>
<p><strong>摘要：</strong>
群体相对策略优化（GRPO）在大语言模型（LLM）的后训练中展现出显著效用。该机制通过模型对提示词生成回答，并借助强化学习掌握更优的完成模式。由于通信量较小，GRPO天然适用于去中心化训练——多个节点可并行处理提示词，随后以字符串形式交换结果。本研究首次揭示了去中心化GRPO环境中的对抗攻击现象。我们论证了恶意参与方可通过上下文无关与上下文关联两种攻击模式，在良性模型中注入任意恶意标记以毒化系统。通过数学运算与编程任务的实证案例，我们证明对抗攻击能轻易污染良性节点，破坏其本地LLM后训练过程，在不足50次迭代中即可实现高达100%的攻击成功率。针对用户训练相同模型或不同模型两种场景，我们提出两种防御方案。实验表明这些防御措施可实现最高100%的攻击阻断率，从而有效遏制此类攻击。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.09780">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.09780">arXiv</a></p>
<hr />
<h3>6. Depth Anything 3：从任意视角重建视觉空间</h3>
<p><strong>原文标题：</strong> Depth Anything 3: Recovering the Visual Space from Any Views</p>
<p><strong>摘要：</strong>
本文提出Depth Anything 3（DA3）模型，该模型能够从任意数量的视觉输入中预测空间一致的几何结构，且无需已知相机位姿。为实现极简建模，DA3获得两项关键发现：采用单一标准Transformer（如原始DINO编码器）作为主干网络即可满足需求，无需专门架构设计；采用单一深度射线预测目标即可规避复杂的多任务学习。通过师生训练范式，该模型在细节还原与泛化能力方面达到与Depth Anything 2（DA2）相当的水平。我们建立了涵盖相机位姿估计、任意视角几何重建与视觉渲染的新视觉几何基准测试集。在该基准测试中，DA3在所有任务中均创下最新性能纪录，相机位姿估计精度较先前最优方法VGGT平均提升44.3%，几何精度平均提升25.1%。此外，该模型在单目深度估计任务中的表现也优于DA2。所有模型均仅使用公开学术数据集进行训练。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.10647">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.10647">arXiv</a></p>
<hr />
<h3>7. 叠加梯度下降法：运用量子原理优化模型训练</h3>
<p><strong>原文标题：</strong> Superpositional Gradient Descent: Harnessing Quantum Principles for
  Model Training</p>
<p><strong>摘要：</strong>
当前大规模语言模型普遍采用AdamW等经典优化技术进行训练以提升收敛性与泛化能力，然而量子启发性方法增强经典训练的内在机制仍待深入探索。本文提出叠加梯度下降法——一种通过注入量子电路扰动将梯度更新与量子叠加态相关联的新型优化器。我们构建了完整的数学框架，并在PyTorch和Qiskit平台上实现了混合量子-经典电路。在合成序列分类任务和大规模语言模型微调实验中，该方法相较AdamW实现了更快的收敛速度与更低的最终损失值。尽管实验结果令人鼓舞，可扩展性与硬件限制仍是实际应用的瓶颈。本研究为量子计算与深度学习的交叉领域提供了新视角，揭示了运用量子原理调控模型行为的可行路径。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.01918">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.01918">arXiv</a></p>
<hr />
<h3>8. 零误差完成百万步大语言模型任务的实现方案</h3>
<p><strong>原文标题：</strong> Solving a Million-Step LLM Task with Zero Errors</p>
<p><strong>摘要：</strong>
大语言模型在推理能力、洞察深度和工具使用方面取得了显著突破，但将这些能力串联成人类、组织和社会日常执行的扩展流程仍面临挑战。模型存在的持续错误率阻碍了规模扩展：例如近期在汉诺塔基准领域的实验表明，该过程在最多数百步后必然失控。因此，尽管大语言模型研究仍常以逻辑依赖步骤较少的任务作为基准，但学界日益关注其执行长程任务的能力局限。本文提出的MAKER系统首次实现了零误差完成超百万步大语言模型任务的突破，且理论上可扩展至更大规模。该方案通过将任务极致分解为可由专注微智能体处理的子任务，借助分解产生的高度模块化特性，通过高效的多智能体投票机制在每一步实施纠错。这种极致分解与纠错机制的结合使规模扩展成为可能。研究结果表明，相较于持续改进现有大语言模型，采用大规模分解智能体流程可能为有效解决组织与社会层级问题提供新路径。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.09030">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.09030">arXiv</a></p>
<hr />
<h3>9. AlphaResearch：利用语言模型加速新算法发现的探索</h3>
<p><strong>原文标题：</strong> AlphaResearch: Accelerating New Algorithm Discovery with Language Models</p>
<p><strong>摘要：</strong>
大型语言模型在复杂但易于验证的问题上已取得显著进展，但在探索未知领域方面仍面临挑战。本文提出AlphaResearch——一种面向开放性问题自主探索新算法的研究智能体。为协同实现发现过程的可行性与创新性，我们通过融合基于执行的验证机制与模拟现实同行评审环境，构建了新型双重研究环境。该系统的算法发现流程通过迭代执行以下步骤实现：（1）提出新构想；（2）在双重研究环境中验证构想；（3）优化研究方案以提升性能。为建立透明化评估体系，我们构建了AlphaResearchComp评估基准，包含八项开放式算法问题的竞赛，每个问题均通过可执行流程、客观指标与可复现性检验进行精心设计与验证。在与人类研究者的直接对比中，AlphaResearch取得了2/8的胜率，证明了利用大语言模型加速算法发现的可行性。值得注意的是，该系统在"圆形填充"问题上发现的算法实现了当前最佳性能，超越了人类研究者及现有强基线方法（如AlphaEvolve）的结果。此外，我们对剩余6/8失败案例进行了全面归因分析，为后续研究提供了重要参考。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.08522">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.08522">arXiv</a></p>
<hr />
<h3>10. Music Flamingo：音频语言模型中音乐理解能力的规模化拓展</h3>
<p><strong>原文标题：</strong> Music Flamingo: Scaling Music Understanding in Audio Language Models</p>
<p><strong>摘要：</strong>
本文提出Music Flamingo——一种新型大规模音频语言模型，旨在提升基础音频模型对音乐（含歌曲）的理解能力。尽管音频语言研究发展迅速，但由于音乐具有动态性、层次性和信息密集性等特点，其理解仍面临挑战。开放音频理解模型的规模化进展进一步受限于高质量音乐数据与标注的稀缺性，导致现有模型仅能生成简短的高层描述、回答浅层问题，且在不同音乐文化间的泛化能力有限。为应对这些挑战，我们构建了MF-Skills大规模数据集，通过多阶段标注流程生成涵盖和声、曲式、音色、歌词及文化背景的丰富描述与问答对。基于该数据集，我们对增强版Audio Flamingo 3主干网络进行微调，并强化了多项音乐理解相关技能。为提升模型推理能力，我们提出一种后训练方案：首先基于音乐理论构建的思维链数据集MF-Think进行冷启动训练，随后采用定制奖励函数通过GRPO强化学习进行优化。Music Flamingo在10余项音乐理解与推理基准测试中达到最先进水平，确立了其作为通用型音乐智能音频语言模型的地位。除实证结果外，该模型通过展现从表层识别转向人类式多层次歌曲感知的能力，为高级音乐理解设立了新标准。我们相信这项工作既为学界提供了基准参照，也为构建具有人类级音乐交互能力的新一代模型奠定了基石。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.10289">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.10289">arXiv</a></p>
<hr />
<h3>11. 基于量规的基准测试与强化学习在提升大语言模型指令遵循能力中的应用</h3>
<p><strong>原文标题：</strong> Rubric-Based Benchmarking and Reinforcement Learning for Advancing LLM Instruction Following</p>
<p><strong>摘要：</strong>
大语言模型近期取得的进展使其在一系列任务中展现出卓越性能，然而在复杂、多轮次和系统提示的指令遵循方面仍面临重大挑战。由于缺乏高质量人工标注的基准测试和可靠可解释的奖励信号，针对此类能力的严格评估和有效训练受到制约。本研究提出AdvancedIF（即将发布该基准），这是一个包含1,600余条提示词和专家制定量规的综合基准，用于评估大语言模型遵循复杂、多轮次及系统级指令的能力。我们进一步提出RIFL（基于量规的指令遵循学习），这是一种创新的后训练流程，通过量规生成、微调的量规验证器和奖励塑造来实现有效的指令遵循强化学习。大量实验表明，RIFL显著提升了大语言模型的指令遵循能力，在AdvancedIF基准上取得6.7%的绝对性能提升，并在公共基准测试中表现优异。消融研究证实了RIFL各组成部分的有效性。本工作确立了量规作为大语言模型高级指令遵循训练与评估的强大工具，为开发更具能力与可靠性的AI系统开辟了新路径。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.10507">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.10507">arXiv</a></p>
<hr />
<h3>12. 基于属性条件人工评估的图像生成多样性基准测试</h3>
<p><strong>原文标题：</strong> Benchmarking Diversity in Image Generation via Attribute-Conditional Human Evaluation</p>
<p><strong>摘要：</strong>
尽管生成质量不断提升，当前文生图模型仍普遍存在输出同质化、缺乏多样性的问题。本研究提出了一套系统性评估框架，以解决文生图模型多样性稳健评估的需求。该框架通过评估独立概念及其相关变异因子，实现对多样性的系统化度量。主要贡献包括：（1）提出用于精细化多样性评估的新型人工评估模板；（2）构建涵盖多维度概念的提示词集，并标注对应的变异因子（如提示词："苹果图像"，变异因子：颜色）；（3）建立基于二项检验的人工标注模型对比方法。此外，我们系统比较了多种图像嵌入方法在多样性度量中的表现。值得强调的是，本方法能够根据多样性表现对文生图模型进行排序，并精准识别其表现薄弱的类别。本研究不仅提供了严谨的评估方法论与关键见解，更为提升文生图模型多样性及改进评估指标奠定了坚实基础。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.10547">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.10547">arXiv</a></p>
<hr />
<h3>13. ResearchRubrics：深度研究智能体评估的提示与评分标准基准</h3>
<p><strong>原文标题：</strong> ResearchRubrics: A Benchmark of Prompts and Rubrics For Evaluating Deep Research Agents</p>
<p><strong>摘要：</strong>
深度研究是一种新兴的智能体应用，其利用大语言模型处理开放式查询。该技术需要整合多重能力，包括多步推理、跨文档综合以及生成具有证据支撑的长篇回答。由于回答内容冗长多样、存在多种有效解决方案且常依赖动态信息源，深度研究的评估仍具挑战性。本文提出ResearchRubrics——一个基于2,800+人工工时构建的标准化深度研究基准，包含真实且领域多样的提示语，以及2,500+专家撰写的细粒度评分标准，用于评估事实依据、推理严谨性和表述清晰度。我们还提出了新的复杂度框架，从概念广度、逻辑嵌套度和探索维度三个轴向对深度研究任务进行分类。此外，我们开发了人工与模型结合的评估方案，用以衡量深度研究智能体对评分标准的符合程度。通过对若干前沿深度研究系统的评估，我们发现即使是Gemini DR和OpenAI DR等领先智能体，其平均标准符合度也不足68%，主要归因于对隐含语境的遗漏以及对检索信息推理不足。研究结果凸显了对深度研究能力进行稳健、可扩展评估的必要性。为此我们开源ResearchRubrics（包含全部提示语、评分标准和评估代码），以推动具有充分论证能力的研究助手的发展。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.07685">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.07685">arXiv</a></p>
<hr />
<h3>14. MuSc-V2：基于无标签样本互评的零样本多模态工业异常分类与分割方法</h3>
<p><strong>原文标题：</strong> MuSc-V2: Zero-Shot Multimodal Industrial Anomaly Classification and Segmentation with Mutual Scoring of Unlabeled Samples</p>
<p><strong>摘要：</strong>
零样本异常分类与分割方法旨在无需任何标注样本的情况下实现缺陷识别与定位。本文揭示了现有方法忽视的关键特性：工业产品中的正常图像块通常在二维外观和三维形状上存在大量相似样本，而异常则保持多样性和孤立性。为显式利用这一判别特性，我们提出用于零样本异常分类/分割的互评框架MuSc-V2，该框架灵活支持单模态（2D/3D）或多模态应用。具体而言，我们首先通过迭代点分组技术改进三维表征，降低因表面不连续性产生的误判；接着采用多阶相似邻域聚合方法，将2D/3D邻域信息融合为更具判别力的多尺度图像块特征用于互评。核心机制包括：模态内样本相互赋分的互评机制，以及融合2D与3D评分以恢复模态特异性缺失异常的跨模态异常增强模块。最后，通过约束邻域重评分机制，基于与更具代表性样本的相似性来抑制误分类。本框架在完整数据集和较小子集上均能保持稳定的强劲性能，确保跨产品线的无缝适配。该创新框架使MuSc-V2实现显著性能提升：在MVTec 3D-AD数据集上平均精度提升23.7%，在Eyecandies数据集上提升19.3%，不仅超越现有零样本基准，更优于多数小样本方法。代码将发布于https://github.com/HUST-SLOW/MuSc-V2。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.10047">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.10047">arXiv</a></p>
<hr />
<h3>15. AffordBot：基于多模态大语言模型的3D细粒度具身推理</h3>
<p><strong>原文标题：</strong> AffordBot: 3D Fine-grained Embodied Reasoning via Multimodal Large Language Models</p>
<p><strong>摘要：</strong>
在物理环境中实现有效的人机协作，不仅需要理解操作对象，还需精确定位可操作元素的位置并规划交互方式。现有方法多在物体层面进行操作，或将细粒度功能推理割裂处理，缺乏基于指令的连贯性 grounding 与推理机制。本研究提出"细粒度3D具身推理"新任务，要求智能体根据任务指令，为三维场景中每个被引用的功能元素预测包含空间位置、运动类型与运动轴的结构化三元组。为此，我们设计AffordBot创新框架，通过整合多模态大语言模型与定制化的思维链推理范式来解决该任务。为弥合3D输入与2D兼容MLLMs之间的鸿沟，我们渲染场景环视图像并将3D候选元素投影至这些视图，构建与场景几何对齐的丰富视觉表征。我们的思维链流程始于主动感知阶段：首先引导MLLM根据指令选择最具信息量的视角，随后通过逐步推理实现功能元素定位及合理交互运动推断。在SceneFun3D数据集上的评估表明，AffordBot仅凭3D点云输入与MLLMs即达到最先进性能，展现出强大的泛化能力与物理 grounded 推理能力。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.10017">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.10017">arXiv</a></p>
<hr />
<h3>16. SliderEdit：基于细粒度指令控制的连续图像编辑</h3>
<p><strong>原文标题：</strong> SliderEdit: Continuous Image Editing with Fine-Grained Instruction Control</p>
<p><strong>摘要：</strong>
基于指令的图像编辑模型近期取得了显著进展，能够通过多指令提示对输入图像进行复杂编辑。然而，现有模型对提示中的每条指令均采用固定强度执行，限制了用户对单次编辑强度进行精确连续控制的能力。本文提出SliderEdit框架，通过细粒度、可解释的指令控制实现连续图像编辑。该框架在接收复合编辑指令后，可解耦各子指令并将其转化为全局训练的滑动控制器，支持通过平滑调节实现编辑强度控制。与文本到图像生成领域需要为每个属性或概念单独训练滑动控制器的方法不同，本方法仅需学习一组低秩适配矩阵，即可泛化应用于多样化编辑任务、属性调整及组合指令场景。该方法在实现单维度编辑连续插值的同时，能有效保持空间局部特征与全局语义一致性。我们将SliderEdit应用于FLUX-Kontext和Qwen-Image-Edit等前沿图像编辑模型，在编辑可控性、视觉一致性和用户导向性方面均取得显著提升。据我们所知，这是首个在基于指令的图像编辑模型中实现细粒度连续控制的研究框架。本研究成果为构建具有连续组合控制能力的交互式指令驱动图像处理系统开辟了新途径。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.09715">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.09715">arXiv</a></p>
<hr />
<h3>17. MM-CRITIC：大型多模态模型多模态评判能力的系统性评估</h3>
<p><strong>原文标题：</strong> MM-CRITIC: A Holistic Evaluation of Large Multimodal Models as Multimodal Critique</p>
<p><strong>摘要：</strong>
评判能力对模型实现自我提升并成为可靠AI助手至关重要。尽管在纯语言场景中已有深入研究，但大型多模态模型（LMMs）的多模态评判能力——尽管其在图像描述和视觉推理等任务中的性能日益增强——仍未得到充分探索。本研究提出MM-CRITIC，一个从基础评判、修正评判和比较评判三个维度系统评估LMMs评判能力的基准框架。该框架涵盖8种主要任务类型超过500项任务，收集了不同参数规模LMMs的响应数据，共构成4471个评估样本。为提升评估信度，我们通过专家知识构建标准答案并融入评分标准，指导GPT-4o对模型响应进行标注并生成参考评判，形成可靠判断基准。大规模实验验证了MM-CRITIC的有效性，并对主流LMMs在多维度的评判能力进行了全面评估。深入分析揭示了若干关键发现，包括响应质量与评判能力的相关性，以及不同评估维度中评判难度的差异性。代码已开源：https://github.com/MichealZeng0420/MM-Critic。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.09067">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.09067">arXiv</a></p>
<hr />
<h3>18. CC30k：面向可复现性情感分析的引用上下文数据集</h3>
<p><strong>原文标题：</strong> CC30k: A Citation Contexts Dataset for Reproducibility-Oriented Sentiment Analysis</p>
<p><strong>摘要：</strong>
下游文献中对被引论文可复现性的情感评价反映了学术界的共识，已被证明是预测已发表研究成果实际可复现性的有效指标。为训练能精准预测可复现性情感并系统研究其与可复现性关联的模型，我们构建了CC30k数据集，该数据集包含机器学习领域论文中的30,734条引用上下文。每条引用上下文均标注有三种面向可复现性的情感标签之一：积极、消极或中立，用以反映被引论文的感知可复现性。其中25,829条通过众包完成标注，并采用受控流程生成负面样本以缓解负面标签稀缺问题。与传统情感分析数据集不同，CC30k专注于可复现性情感分析，填补了计算可复现性研究领域的资源空白。本数据集通过包含数据清洗、严格众包筛选和多重验证的流程构建，最终标注准确率达94%。实验表明，使用本数据集微调后，三种大语言模型在可复现性情感分类任务上的性能显著提升。该数据集为大规模评估机器学习论文的可复现性奠定了基础。CC30k数据集及相关的数据生成分析代码已公开于https://github.com/lamps-lab/CC30k。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.07790">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.07790">arXiv</a></p>
<hr />
<h2>🔍 关键词云图</h2>
<p><img alt="关键词云图" src="../images/keywords_wordcloud.png" /></p>
<h2>📈 近期论文趋势</h2>
<p><img alt="论文趋势" src="../images/daily_papers.png" /></p>
<h2>🎙️ 语音播报</h2>
<ul>
<li><a href="../audio/2025-11-14_daily_papers.mp3">收听今日论文解读</a></li>
</ul>
<h2>📱 订阅渠道</h2>
<ul>
<li>GitHub: <a href="https://github.com/2404589803/hf-daily-paper-newsletter-chinese">hf-daily-paper-newsletter-chinese</a></li>
</ul>
    </div>
</body>
</html>