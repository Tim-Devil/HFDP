<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Hugging Face 论文日报 - 2025-12-15</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', sans-serif;
            line-height: 1.6;
            color: #333;
            background-color: #f5f5f5;
            padding: 20px;
        }
        
        .container {
            max-width: 900px;
            margin: 0 auto;
            background-color: white;
            padding: 40px;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }
        
        h1 {
            color: #2c3e50;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
            margin-bottom: 30px;
            font-size: 28px;
        }
        
        h1 img {
            vertical-align: middle;
            margin-right: 10px;
        }
        
        h2 {
            color: #34495e;
            margin-top: 40px;
            margin-bottom: 20px;
            font-size: 24px;
            border-left: 4px solid #3498db;
            padding-left: 15px;
        }
        
        h3 {
            color: #2c3e50;
            margin-top: 30px;
            margin-bottom: 15px;
            font-size: 20px;
        }
        
        p {
            margin-bottom: 15px;
            text-align: justify;
        }
        
        ul {
            margin-left: 20px;
            margin-bottom: 20px;
        }
        
        li {
            margin-bottom: 8px;
        }
        
        a {
            color: #3498db;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        strong {
            color: #2c3e50;
            font-weight: 600;
        }
        
        hr {
            border: none;
            border-top: 1px solid #e0e0e0;
            margin: 30px 0;
        }
        
        /* 关键修复:限制图片宽度 */
        img {
            max-width: 100%;
            height: auto;
            display: block;
            margin: 20px auto;
            border-radius: 4px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        
        /* 确保图片容器也有宽度限制 */
        p img {
            max-width: 100%;
        }
        
        /* 论文详情区域样式 */
        .paper-section {
            background-color: #f9f9f9;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 20px;
        }
        
        /* 统计信息样式 */
        .stats {
            background-color: #e8f4f8;
            padding: 15px;
            border-radius: 6px;
            margin-bottom: 20px;
        }
        
        /* 响应式设计 */
        @media (max-width: 768px) {
            .container {
                padding: 20px;
            }
            
            h1 {
                font-size: 24px;
            }
            
            h2 {
                font-size: 20px;
            }
            
            h3 {
                font-size: 18px;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <h1><img src="https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo.png" width="30"/> Hugging Face 2025-12-15 论文日报</h1>
<h2>📊 今日论文统计</h2>
<ul>
<li>总论文数：21</li>
<li>热门领域：LLM, GPT, Transformer</li>
</ul>
<h2>📝 论文详情</h2>
<h3>1. EgoX：基于单视角第三人称视频的第一人称视频生成</h3>
<p><strong>原文标题：</strong> EgoX: Egocentric Video Generation from a Single Exocentric Video</p>
<p><strong>摘要：</strong>
第一人称感知使人类能够直接从自身视角体验和理解世界。将第三人称视频转换为第一人称视频为沉浸式理解开辟了新途径，但由于相机位姿的极端变化和视角重叠度极低，该任务仍极具挑战性。这需要在保持可见内容真实性的同时，以几何一致的方式合成未观测区域。为此，我们提出EgoX——一种从单段第三人称视频生成第一人称视频的创新框架。EgoX通过轻量级LoRA适配器利用大规模视频扩散模型的预训练时空知识，并提出一种统一的条件控制策略：通过宽度维与通道维拼接融合第三人称与第一人称先验信息。此外，我们设计了几何引导的自注意力机制，该机制能选择性关注空间相关区域，确保几何连贯性与高视觉保真度。本方法在实现连贯逼真的第一人称视频生成的同时，在未见过的真实场景视频中展现出强大的可扩展性与鲁棒性。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.08269">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.08269">arXiv</a></p>
<hr />
<h3>2. DentalGPT：激励牙科多模态复杂推理能力的发展</h3>
<p><strong>原文标题：</strong> DentalGPT: Incentivizing Multimodal Complex Reasoning in Dentistry</p>
<p><strong>摘要：</strong>
牙科多模态数据的可靠解读对于自动化口腔医疗保健至关重要，然而当前的多模态大语言模型（MLLMs）难以捕捉细粒度的牙科视觉细节，且缺乏进行精确诊断所需的充分推理能力。为应对这些局限，我们提出了DentalGPT，这是一个通过高质量领域知识注入和强化学习开发的专用牙科MLLM。具体而言，我们通过整合超过12万张牙科图像及其详细描述，构建了迄今为止最大的标注多模态牙科数据集，该描述突出了与诊断相关的视觉特征，使其成为迄今为止牙科图像收集最广泛的多模态数据集。在此数据集上的训练显著增强了MLLM对牙科病况的视觉理解能力，而随后的强化学习阶段则进一步强化了其多模态复杂推理能力。在口内和全景影像基准测试以及医学视觉问答（VQA）基准的牙科子集上的综合评估表明，DentalGPT在疾病分类和牙科VQA任务中取得了卓越性能，尽管仅有70亿参数，但仍优于许多最先进的多模态大语言模型。这些结果表明，高质量的牙科数据结合分阶段适应，为构建能力强且领域专用的牙科多模态大语言模型提供了一条有效途径。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.11558">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.11558">arXiv</a></p>
<hr />
<h3>3. SVG-T2I：无需变分自编码器即可扩展文本到图像的潜在扩散模型</h3>
<p><strong>原文标题：</strong> SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder</p>
<p><strong>摘要：</strong>
基于视觉基础模型（VFM）表征的视觉生成为整合视觉理解、感知与生成提供了一个极具前景的统一路径。尽管潜力巨大，但完全在VFM表征空间内训练大规模文本到图像扩散模型的研究仍较为有限。为填补这一空白，我们扩展了SVG（用于视觉生成的自监督表征）框架，提出SVG-T2I以直接在VFM特征域中支持高质量的文本到图像合成。通过采用标准文本到图像扩散流程，SVG-T2I实现了具有竞争力的性能，在GenEval上达到0.75分，在DPG-Bench上达到85.78分。这一性能验证了VFM在生成任务中固有的表征能力。我们全面开源了该项目，包括自编码器与生成模型，以及其训练、推理、评估流程和预训练权重，以促进表征驱动视觉生成的进一步研究。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.11749">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.11749">arXiv</a></p>
<hr />
<h3>4. V-RGBX：基于本征属性精确控制的视频编辑</h3>
<p><strong>原文标题：</strong> V-RGBX: Video Editing with Accurate Controls over Intrinsic Properties</p>
<p><strong>摘要：</strong>
大规模视频生成模型在模拟真实场景中逼真的外观与光照交互方面展现出显著潜力。然而，现有研究尚未实现一个能够联合理解场景本征属性（如反照率、法线、材质和辐照度）、利用这些属性进行视频合成，并支持可编辑本征表征的闭环框架。本文提出V-RGBX——首个面向本征感知视频编辑的端到端框架。V-RGBX整合了三大核心功能：(1) 将视频逆向渲染分解为本征通道，(2) 基于本征表征生成逼真视频，(3) 通过本征通道条件实现基于关键帧的视频编辑。该框架的核心是交错条件机制，允许用户通过选定关键帧进行直观且符合物理规律的视频编辑，支持对任意本征模态的灵活操控。大量定性与定量实验表明，V-RGBX能生成时序一致、视觉逼真的视频，并以符合物理规律的方式将关键帧编辑效果传播至整个序列。我们在物体外观编辑与场景级重照明等多种应用中验证了其有效性，其性能超越现有方法。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.11799">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.11799">arXiv</a></p>
<hr />
<h3>5. 滑动窗口注意力自适应方法</h3>
<p><strong>原文标题：</strong> Sliding Window Attention Adaptation</p>
<p><strong>摘要：</strong>
基于Transformer架构的大语言模型（LLM）中的自注意力机制计算复杂度随输入长度呈二次方增长，导致长上下文推理成本高昂。滑动窗口注意力（SWA）可将计算复杂度降低至线性级别，但对于采用完整注意力（FA）机制预训练的模型，若在推理阶段直接完全切换为SWA，会因训练与推理模式失配而导致长上下文性能严重下降。这引发我们思考：能否在不重新预训练的情况下，使FA预训练的LLM有效适配SWA？为此，我们提出滑动窗口注意力自适应（SWAA）方案，整合五种方法以实现更优适配：（1）仅在预填充阶段应用SWA；（2）保留“汇聚”标记；（3）交错配置FA/SWA注意力层；（4）思维链（CoT）推理；（5）微调技术。实验表明，SWA适配具有可行性但非易事：单一方法均不足够，而特定的协同组合能有效恢复原始长上下文性能。我们进一步分析了不同SWAA配置在性能与效率间的权衡关系，并为多样化场景提供了推荐方案。代码已开源：https://github.com/yuyijiong/sliding-window-attention-adaptation</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.10411">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.10411">arXiv</a></p>
<hr />
<h3>6. PersonaLive! 面向直播的富有表现力肖像图像动画生成</h3>
<p><strong>原文标题：</strong> PersonaLive! Expressive Portrait Image Animation for Live Streaming</p>
<p><strong>摘要：</strong>
当前基于扩散模型的肖像动画方法主要关注提升视觉质量与表情真实感，而忽视了生成延迟与实时性能，这限制了其在直播场景中的应用范围。本文提出PersonaLive——一种基于扩散模型的新型框架，通过多阶段训练方案实现流式实时肖像动画生成。具体而言，我们首先采用混合隐式信号（即隐式面部表征与三维隐式关键点）来实现富有表现力的图像级运动控制。随后，提出一种少步数外观蒸馏策略，以消除去噪过程中的外观冗余，显著提升推理效率。最后，我们引入一种自回归微片段流式生成范式，配合滑动训练策略与历史关键帧机制，实现低延迟且稳定的长时序视频生成。大量实验表明，PersonaLive在达到最先进性能的同时，相比现有基于扩散模型的肖像动画方法实现了7至22倍的加速。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.11253">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.11253">arXiv</a></p>
<hr />
<h3>7. 基于MetaCanvas探索多模态大语言模型与扩散模型的信息传递</h3>
<p><strong>原文标题：</strong> Exploring MLLM-Diffusion Information Transfer with MetaCanvas</p>
<p><strong>摘要：</strong>
多模态学习通过以强大语言模型作为认知核心的多模态大语言模型，显著推动了视觉理解领域的发展。然而在视觉生成任务中，这些核心模型通常仅被简化为扩散模型的全局文本编码器，其绝大部分推理与规划能力未能得到有效利用。这导致了一个显著差距：当前多模态大语言模型能够解析复杂布局、属性及知识密集型场景，却难以生成具有同等精确度与结构化控制能力的图像或视频。我们提出MetaCanvas——一个轻量级框架，使多模态大语言模型能够直接在空间与时空隐空间中进行推理规划，并与扩散生成器实现紧密协同。我们在三种不同扩散模型骨干上实证实现了MetaCanvas，并在六类任务中进行了系统评估，包括文本到图像生成、文本/图像到视频生成、图像/视频编辑以及上下文视频生成。这些任务均要求精确的布局控制、鲁棒的属性绑定及高强度的推理能力。实验表明，MetaCanvas在各项任务中均稳定优于全局条件控制的基线方法，这证明将多模态大语言模型作为隐空间规划器，是弥合多模态理解与生成能力差距的有效路径。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.11464">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.11464">arXiv</a></p>
<hr />
<h3>8. MeshSplatting：基于不透明网格的可微分渲染</h3>
<p><strong>原文标题：</strong> MeshSplatting: Differentiable Rendering with Opaque Meshes</p>
<p><strong>摘要：</strong>
以基元为基础的溅射方法，如3D高斯溅射，已通过实时渲染革新了新视角合成技术。然而，其基于点的表示方式仍无法与驱动AR/VR及游戏引擎的基于网格的流程兼容。本文提出MeshSplatting，一种基于网格的重建方法，通过可微分渲染联合优化几何结构与外观属性。该方法通过受限Delaunay三角剖分强制保持网格连通性，并优化表面一致性，从而构建出端到端平滑、视觉质量高的网格模型，能够在实时3D引擎中高效渲染。在Mip-NeRF360数据集上，相较于当前基于网格的新视角合成最优方法MiLo，本方法将PSNR提升了+0.69 dB，同时训练速度加快2倍，内存占用减少一半，有效弥合了神经渲染与交互式3D图形之间的技术鸿沟，为实现无缝实时场景交互提供了新途径。项目页面详见：https://meshsplatting.github.io/。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.06818">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.06818">arXiv</a></p>
<hr />
<h3>9. 基于跟踪的结构生成：为视频生成提炼结构保持运动</h3>
<p><strong>原文标题：</strong> Structure From Tracking: Distilling Structure-Preserving Motion for Video Generation</p>
<p><strong>摘要：</strong>
现实世界是刚性约束与可变形结构之间的动态平衡。对于视频生成模型而言，这意味着需要生成既保持真实感又维持结构一致性的运动。尽管扩散模型已取得进展，但生成逼真且结构保持的运动仍具挑战性，尤其对于人体、动物等具有关节和可变形结构的对象。仅依靠扩大训练数据规模至今未能解决物理上不合理的运动过渡问题。现有方法通常依赖于带有噪声的运动表征（如光流或通过外部不完美模型提取的骨架）作为条件输入。为应对这些挑战，我们提出一种算法，将自回归视频跟踪模型（SAM2）中的结构保持运动先验知识提炼至双向视频扩散模型（CogVideoX）中。基于该方法，我们训练出SAM2VideoX模型，其包含两项创新：（1）双向特征融合模块，可从SAM2等循环模型中提取全局结构保持运动先验；（2）局部格拉姆流损失函数，用于对齐局部特征的协同运动方式。在VBench基准测试和人工评估中，SAM2VideoX相较于现有基线模型均取得稳定提升（VBench得分提升2.60%，FVD降低21-22%，人工偏好率达71.4%）。具体而言，在VBench上我们获得95.51%的得分，较REPA（92.91%）提升2.60%；同时将FVD降至360.57，较REPA微调和LoRA微调分别提升21.20%和22.46%。项目网站详见：https://sam2videox.github.io/。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.11792">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.11792">arXiv</a></p>
<hr />
<h3>10. LEO-RobotAgent：面向语言驱动具身操作器的通用机器人智能体</h3>
<p><strong>原文标题：</strong> LEO-RobotAgent: A General-purpose Robotic Agent for Language-driven Embodied Operator</p>
<p><strong>摘要：</strong>
本文提出LEO-RobotAgent，一种面向机器人的通用语言驱动智能体框架。在该框架下，大语言模型能够操作不同类型的机器人，完成跨场景的不可预测复杂任务。该框架具有强泛化性、鲁棒性和高效性特点，围绕其构建的应用级系统可全面增强人机双向意图理解，降低人机交互门槛。在机器人任务规划方面，现有研究大多聚焦于大模型在单一任务场景及单一机器人类型中的应用，此类算法往往结构复杂且缺乏普适性。因此，所提出的LEO-RobotAgent框架在设计上尽可能采用精简结构，使大模型能够在这一清晰框架内自主进行思考、规划与行动。我们提供模块化且易于注册的工具集，允许大模型灵活调用各类工具以满足多样化需求。同时，框架融合了人机协作交互机制，使算法能够像合作伙伴般与人类协同工作。实验验证表明，该框架可轻松适配包括无人机、机械臂与轮式机器人在内的主流机器人平台，并能高效执行多种精心设计的不同复杂度任务。我们的代码已开源：https://github.com/LegendLeoChen/LEO-RobotAgent。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.10605">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.10605">arXiv</a></p>
<hr />
<h3>11. 因果评判评估：面向大语言模型系统的校准替代指标</h3>
<p><strong>原文标题：</strong> Causal Judge Evaluation: Calibrated Surrogate Metrics for LLM Systems</p>
<p><strong>摘要：</strong>
大语言模型即评判（LLM-as-judge）评估已成为扩展模型评估的事实标准，但该实践在统计上存在缺陷：未经校准的分数可能导致偏好反转，基于未校准分数的朴素置信区间覆盖率趋近于零，且重要性加权估计量在有效样本量（ESS）较高的情况下，仍会因有限重叠而失效。本文提出因果评判评估（CJE）框架，可同时解决上述三类问题。在经筛选的4,961条Chatbot Arena提示（从5,000条中过滤）上，CJE通过仅使用5%的黄金标准标签（约250条）对成本降低16倍的评判模型进行校准，以降低14倍的成本（用于评估5个策略），实现了全样本量下99%的配对排序准确率（各配置平均为94%），达到与黄金标准相当的质量。CJE包含三个核心组件：（1）AutoCal-R：通过均值保持保序回归进行奖励校准；（2）SIMCal-W：通过S-单调候选模型的堆叠实现权重稳定；（3）黄金标准不确定性感知（OUA）推断，将校准不确定性传递至置信区间。我们形式化提出了覆盖率受限效率（CLE）诊断指标，该指标解释了为何即使ESS超过90%，IPS类估计量仍会失效：日志数据极少覆盖目标策略集中的区域。关键发现：由于权重不稳定，即使经过奖励校准，SNIPS方法仍会出现排序反转（配对准确率38%，肯德尔τ系数为负）；尽管进行了权重稳定处理，校准后的IPS方法准确率仍接近随机水平（47%），这与CLE诊断一致；OUA推断将覆盖率从接近0%提升至约86%（直接估计）和约96%（堆叠双重稳健估计），而朴素置信区间则存在严重覆盖不足。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.11150">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.11150">arXiv</a></p>
<hr />
<h3>12. Fairy2i：在参数取值{±1, ±i}条件下从实数大语言模型训练复数大语言模型</h3>
<p><strong>原文标题：</strong> Fairy2i: Training Complex LLMs from Real LLMs with All Parameters in {pm 1, pm i}</p>
<p><strong>摘要：</strong>
大语言模型（LLMs）已彻底改变人工智能领域，但其巨大的内存与计算需求迫使人们采用激进的量化技术，日益将表示能力推向单比特的理论极限。虽然复数大语言模型（如iFairy）相较于实数模型在低比特表示方面具有显著优势，但它们需要从头开始训练，无法利用海量的预训练实数基础模型生态。本文提出Fairy2i——一种通用框架，可将预训练的实数层转换为等效的广义线性复数形式，在复用现有模型参数的同时实现极低比特量化。通过证明实数映射与广义线性复数映射间的无损数学等价性，我们将标准Transformer架构转换至复数域，并采用基于四次单位根高效码本的相位感知量化方案。此外，我们引入递归残差量化机制，通过迭代最小化量化误差，实现无需乘法运算的高效累积推理过程。实验表明，Fairy2i能将LLaMA-2 7B模型在等效2比特精度下的性能恢复至接近全精度基准水平，显著优于当前最先进的实数二值化与三值化量化方法。这项工作弥合了复数运算的表征效率与预训练模型实用价值之间的鸿沟，为在通用硬件上实现高效推理开辟了新路径。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.02901">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.02901">arXiv</a></p>
<hr />
<h3>13. CLINIC：面向医疗健康领域的语言模型多语言可信度评估框架</h3>
<p><strong>原文标题：</strong> CLINIC: Evaluating Multilingual Trustworthiness in Language Models for Healthcare</p>
<p><strong>摘要：</strong>
将语言模型整合到医疗健康系统中，对优化医疗工作流程与临床决策具有重要前景。然而，其在实际应用中的关键障碍在于缺乏可靠的可信度评估机制，尤其是在多语言医疗场景下。现有语言模型主要基于高资源语言训练，难以有效处理中低资源语言中医疗查询的复杂性与多样性，这在以语言多样性为特征的全球医疗健康部署中构成了显著挑战。本研究提出CLINIC——一个用于评估医疗健康领域语言模型可信度的综合性多语言基准体系。该框架通过18项差异化任务，系统性地从五个关键可信度维度对语言模型进行测评：真实性、公平性、安全性、鲁棒性与隐私保护，覆盖15种语言（涵盖全球主要大洲），并涉及疾病状况、预防措施、诊断检测、治疗方案、外科手术及药物使用等广泛的关键医疗主题。大规模评估表明，现有语言模型在事实准确性方面存在不足，在不同人口统计学特征与语言群体间表现出偏见，且易受隐私泄露与对抗性攻击的影响。通过揭示这些缺陷，CLINIC为提升语言模型在全球多语言医疗健康场景中的适用性与安全性奠定了重要基础。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.11437">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.11437">arXiv</a></p>
<hr />
<h3>14. 离散扩散语言模型的缩放行为研究</h3>
<p><strong>原文标题：</strong> Scaling Behavior of Discrete Diffusion Language Models</p>
<p><strong>摘要：</strong>
现代大语言模型预训练消耗大量计算资源和训练数据，使得不同模型的缩放行为（即缩放定律）成为关键区分因素。离散扩散语言模型作为自回归语言模型的替代方案被提出，但其缩放行为尚未得到充分探索，先前研究表明其需要更多数据和计算资源才能达到自回归语言模型的性能水平。本研究通过平滑插值掩码扩散与均匀扩散两种噪声类型，并重点关注批次大小与学习率等关键超参数，系统探究了离散扩散语言模型的缩放行为。实验表明：离散扩散模型的缩放行为高度依赖于噪声类型，且与自回归模型存在显著差异。在计算资源受限的缩放场景中，所有噪声类型最终收敛至相近的损失值；但相较于掩码扩散，均匀扩散在计算效率优化训练中需要更多参数和更少数据，使其在数据受限场景中展现出显著优势。我们将均匀扩散模型扩展至100亿参数规模，训练计算量达10^{22} FLOPs，验证了预测的缩放规律，该模型也成为目前公开已知最大规模的均匀扩散语言模型。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.10858">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.10858">arXiv</a></p>
<hr />
<h3>15. 视觉-语言-动作模型的任务适应性：2025年BEHAVIOR挑战赛冠军解决方案</h3>
<p><strong>原文标题：</strong> Task adaptation of Vision-Language-Action model: 1st Place Solution for the 2025 BEHAVIOR Challenge</p>
<p><strong>摘要：</strong>
本文提出一种视觉-动作策略模型，该模型在2025年BEHAVIOR挑战赛中荣获冠军。该挑战赛采用包含50项多样化长周期家庭任务的大规模基准测试，在逼真仿真环境中要求模型完成双手操作、导航及情境感知决策。基于Pi0.5架构，我们引入多项创新：核心贡献是提出面向流匹配的关联噪声机制，通过提升训练效率实现关联感知的图像修复，从而生成平滑的动作序列；同时采用可学习的混合层注意力机制与系统二阶段追踪方法以解决任务歧义。训练阶段运用多样本流匹配降低方差，推理阶段则采用动作压缩技术与挑战赛专用校正规则。该方法在公开与私有排行榜的50项任务中均取得26%的综合质量评分。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.06951">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.06951">arXiv</a></p>
<hr />
<h3>16. Fast-FoundationStereo：实时零样本立体匹配</h3>
<p><strong>原文标题：</strong> Fast-FoundationStereo: Real-Time Zero-Shot Stereo Matching</p>
<p><strong>摘要：</strong>
立体基础模型虽能实现强大的零样本泛化能力，但其计算成本过高，难以满足实时应用需求。另一方面，高效的立体架构往往以牺牲鲁棒性为代价来提升速度，且需要针对不同领域进行成本高昂的微调。为弥合这一差距，本文提出Fast-FoundationStereo系列架构，首次在实时帧率下实现了强大的零样本泛化性能。我们采用分治加速策略，该策略包含三个核心组件：（1）通过知识蒸馏将混合主干网络压缩为单一高效的学生模型；（2）采用分块神经架构搜索，在延迟预算约束下自动发现最优代价滤波设计，将搜索复杂度指数级降低；（3）通过结构化剪枝消除迭代优化模块中的冗余。此外，我们引入自动伪标注流程，用于构建包含140万张真实场景立体图像对的数据集，以补充合成训练数据并促进知识蒸馏。最终模型在保持与FoundationStereo相近的零样本精度的同时，运行速度提升超过10倍，从而在实时方法中确立了新的技术标杆。项目页面：https://nvlabs.github.io/Fast-FoundationStereo/</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.11130">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.11130">arXiv</a></p>
<hr />
<h3>17. CheXmask-U：X光影像中基于解剖标志点分割的不确定性量化</h3>
<p><strong>原文标题：</strong> CheXmask-U: Quantifying uncertainty in landmark-based anatomical segmentation for X-ray images</p>
<p><strong>摘要：</strong>
不确定性估计对于医学影像分割系统的安全临床部署至关重要，它能够识别不可靠的预测结果并支持人工监督。尽管先前的研究主要集中于像素级不确定性，但基于解剖标志点的分割方法虽具有固有的拓扑结构保证，却从未在不确定性视角下得到充分探索。本研究针对胸部X光影像中基于解剖标志点的分割任务，系统探究了不确定性估计方法。受结合标准图像卷积编码器与基于图结构的生成式解码器的混合神经网络架构启发，并利用其变分潜在空间，我们推导出两种互补的度量指标：（1）潜在不确定性——直接从学习得到的分布参数中捕获；（2）预测不确定性——通过对潜在空间采样生成多个随机输出预测获得。通过受控数据破坏实验，我们证明这两种不确定性度量均随扰动强度增加而上升，能够同时反映全局与局部层面的数据退化。通过与人工标注金标准对比，我们验证了这些不确定性信号可有效识别不可靠预测，并在CheXmask数据集上实现了分布外检测。更重要的是，我们发布了CheXmask-U数据集（huggingface.co/datasets/mcosarinsky/CheXmask-U），该大规模数据集包含657,566例胸部X光解剖标志点分割结果及每个节点的不确定性估计，使研究人员在使用这些解剖掩模时能够充分考虑分割质量的空间异质性。我们的研究结果表明，不确定性估计是提升胸部X光解剖标志点分割方法鲁棒性与安全部署前景的重要方向。该方法的完整交互式演示可通过huggingface.co/spaces/matiasky/CheXmask-U访问，源代码已发布于github.com/mcosarinsky/CheXmask-U。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.10715">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.10715">arXiv</a></p>
<hr />
<h3>18. N体问题：基于单人第一人称视频的并行执行</h3>
<p><strong>原文标题：</strong> The N-Body Problem: Parallel Execution from Single-Person Egocentric Video</p>
<p><strong>摘要：</strong>
人类能够直观地对复杂活动进行并行化处理，但模型能否通过观察单个人的行为学习这种能力？给定一段第一人称视频，我们提出N体问题：假设存在N个个体，如何协同完成该视频中观察到的同一组任务。目标在于最大化执行加速比，但将视频片段简单分配给不同个体常会违反现实约束，导致物理上不可行的场景（如两人同时使用同一物体或占据同一空间）。为解决此问题，我们形式化定义了N体问题，并提出一套兼顾性能（加速比、任务覆盖率）与可行性（空间碰撞、物体冲突及因果约束）的评估指标。进而设计了一种结构化提示策略，引导视觉语言模型（VLM）对三维环境、物体使用时序及因果依赖进行推理，以生成可行的并行执行方案。在EPIC-Kitchens和HD-EPIC数据集的100段视频上，当N=2时，我们的方法相较于Gemini 2.5 Pro的基线提示，将动作覆盖率提升45%，同时将碰撞率、物体冲突与因果冲突分别降低55%、45%和55%。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.11393">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.11393">arXiv</a></p>
<hr />
<h3>19. 一秒内实现锐利的单目视图合成</h3>
<p><strong>原文标题：</strong> Sharp Monocular View Synthesis in Less Than a Second</p>
<p><strong>摘要：</strong>
本文提出SHARP方法，通过单张图像实现逼真的视图合成。给定单张照片，SHARP通过神经网络单次前向传播，在标准GPU上以不足一秒的时间回归出场景的三维高斯表示参数。该方法生成的三维高斯表示可实时渲染，为邻近视点生成高分辨率逼真图像。该表示具有绝对尺度的度量特性，支持度量级相机运动。实验结果表明，SHARP在不同数据集上均展现出强大的零样本泛化能力。在多个数据集上创造了新的技术标杆，与现有最佳模型相比，LPIPS指标降低25-34%，DISTS指标降低21-43%，同时将合成时间缩短三个数量级。代码与权重已开源：https://github.com/apple/ml-sharp</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.10685">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.10685">arXiv</a></p>
<hr />
<h3>20. 基于稀疏自编码器的可解释嵌入：一种数据分析工具包</h3>
<p><strong>原文标题：</strong> Interpretable Embeddings with Sparse Autoencoders: A Data Analysis Toolkit</p>
<p><strong>摘要：</strong>
分析大规模文本语料库是机器学习领域的核心挑战，对于识别模型不良行为或训练数据偏见等任务至关重要。当前方法通常依赖成本高昂的基于大语言模型的技术（例如标注数据集差异）或稠密嵌入模型（例如用于聚类），这些方法难以针对特定关注属性进行控制。我们提出使用稀疏自编码器创建SAE嵌入：其维度可映射到可解释概念的表示形式。通过四项数据分析任务，我们证明SAE嵌入相较于大语言模型更具成本效益和可靠性，同时比稠密嵌入更具可控性。利用SAE庞大的假设空间，我们能够揭示如下洞察：（1）数据集间的语义差异；（2）文档中意外的概念关联。例如，通过比较模型响应，我们发现Grok-4模型比其他九种前沿模型更频繁地澄清歧义。相较于大语言模型，SAE嵌入能以降低2-8倍的成本发现更显著的差异，并更可靠地识别偏见。此外，SAE嵌入具有可控性：通过概念过滤，我们能够（3）沿关注维度对文档进行聚类，并（4）在基于属性的检索任务中超越稠密嵌入性能。借助SAE嵌入，我们通过两个案例研究模型行为：探究OpenAI模型随时间的行为演变，以及发现Tulu-3模型（Lambert等人，2024）从训练数据中学到的“触发”短语。这些成果确立了稀疏自编码器作为非结构化数据分析的通用工具地位，并凸显了通过数据解读模型这一被忽视的重要维度。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.10092">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.10092">arXiv</a></p>
<hr />
<h3>21. Particulate：一种用于三维物体关节结构的前馈推断方法</h3>
<p><strong>原文标题：</strong> Particulate: Feed-Forward 3D Object Articulation</p>
<p><strong>摘要：</strong>
本文提出Particulate方法，这是一种前馈式推断框架，能够基于日常物体的单个静态三维网格，直接推断其底层关节结构的所有属性，包括三维部件、运动学结构及运动约束。该方法的核心是一个基于Transformer架构的网络——部件关节变换器，该网络通过灵活可扩展的架构处理输入网格的点云数据，以原生多关节支持的方式预测上述所有属性。我们利用公开数据集中多样化的关节化三维资源对网络进行端到端训练。在推断阶段，Particulate将网络的前馈预测结果映射至输入网格，可在数秒内生成完整的关节化三维模型，其速度远超以往需要针对单个对象进行优化的方法。此外，Particulate能够准确推断由人工智能生成的三维资源的关节结构，当与现成的图像到三维生成器结合时，可实现从单张（真实或合成）图像中完整提取关节化三维物体。我们还引入了一个基于高质量公开三维资源构建的、更具挑战性的三维关节估计基准测试集，并重新设计了更符合人类偏好的评估协议。定量与定性实验结果表明，Particulate在性能上显著优于现有最先进方法。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.11798">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.11798">arXiv</a></p>
<hr />
<h2>🔍 关键词云图</h2>
<p><img alt="关键词云图" src="../images/keywords_wordcloud.png" /></p>
<h2>📈 近期论文趋势</h2>
<p><img alt="论文趋势" src="../images/daily_papers.png" /></p>
<h2>🎙️ 语音播报</h2>
<ul>
<li><a href="../audio/2025-12-15_daily_papers.mp3">收听今日论文解读</a></li>
</ul>
<h2>📱 订阅渠道</h2>
<ul>
<li>GitHub: <a href="https://github.com/2404589803/hf-daily-paper-newsletter-chinese">hf-daily-paper-newsletter-chinese</a></li>
</ul>
    </div>
</body>
</html>