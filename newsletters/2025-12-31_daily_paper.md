
# <img src="https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo.png" width="30"/> Hugging Face 2025-12-31 论文日报

## 📊 今日论文统计
- 总论文数：7
- 热门领域：Transformer, RL, LLM

## 📝 论文详情


### 1. UltraShape 1.0：基于可扩展几何优化的高保真三维形状生成方法

**原文标题：** UltraShape 1.0: High-Fidelity 3D Shape Generation via Scalable Geometric Refinement

**摘要：**
本报告提出UltraShape 1.0——一个可扩展的三维扩散框架，用于实现高保真的三维几何形状生成。该方法采用两阶段生成流程：首先生成粗糙的全局结构，随后通过细化处理生成具有丰富细节的高质量几何形状。为支持可靠的三维生成，我们构建了完整的数据处理流程，包含创新的水密性处理方法与高质量数据过滤机制。该流程通过剔除低质量样本、填补孔洞及增厚薄壁结构，在保留细粒度几何细节的同时，显著提升了公开三维数据集的几何质量。为实现细粒度几何优化，我们在扩散过程中将空间定位与几何细节合成进行解耦：通过在固定空间位置执行基于体素的精细化操作，利用粗粒度几何生成的体素查询提供通过RoPE编码的显式位置锚点，使扩散模型能够聚焦于在结构化简化的解空间内合成局部几何细节。本模型完全基于公开三维数据集训练，在有限训练资源下仍实现了优异的几何质量。大量实验评估表明，UltraShape 1.0在数据处理质量与几何生成能力方面均与现有开源方法具有竞争优势。所有代码与训练模型将公开发布以支持后续研究。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.21185) | [arXiv](https://arxiv.org/abs/2512.21185)



---

### 2. DreamOmni3：基于涂鸦的编辑与生成

**原文标题：** DreamOmni3: Scribble-based Editing and Generation

**摘要：**
近期，统一的生成与编辑模型凭借其卓越性能取得了显著成功。这些模型主要依赖文本提示进行基于指令的编辑与生成，但语言往往难以准确捕捉用户期望的编辑位置及细粒度视觉细节。为此，我们提出两项任务：基于涂鸦的编辑与生成，以结合用户文本、图像及手绘草图在图形用户界面（GUI）上实现更灵活的创作。我们提出DreamOmni3模型，重点解决两大挑战：数据构建与框架设计。我们的数据合成流程包含两部分：基于涂鸦的编辑与生成。在基于涂鸦的编辑方面，我们定义了四项子任务：基于涂鸦与指令的编辑、基于涂鸦与多模态指令的编辑、图像融合以及涂鸦编辑。基于DreamOmni2数据集，我们提取可编辑区域并叠加手绘方框、圆形、涂鸦或裁剪图像以构建训练数据。在基于涂鸦的生成方面，我们定义了基于涂鸦与指令的生成、基于涂鸦与多模态指令的生成以及涂鸦生成三项子任务，并采用类似的数据构建流程。在框架设计上，针对涉及多涂鸦、多图像及多指令的复杂编辑场景中二值掩码方法的局限性，我们提出一种联合输入方案：将原始图像与涂鸦后的源图像同时输入模型，并通过不同颜色区分区域以简化处理。通过对两幅图像应用相同的索引与位置编码，模型能够精确定位涂鸦区域并保持编辑准确性。最后，我们为这些任务建立了综合基准测试以推动后续研究。实验结果表明，DreamOmni3实现了优异性能，相关模型与代码将公开发布。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.22525) | [arXiv](https://arxiv.org/abs/2512.22525)



---

### 3. 面向长上下文建模的端到端测试时训练方法

**原文标题：** End-to-End Test-Time Training for Long Context

**摘要：**
本文将长上下文语言建模重新定义为持续学习问题而非架构设计问题。在此框架下，我们仅采用标准架构——基于滑动窗口注意力的Transformer模型。该模型在测试阶段通过给定上下文的下一个词元预测持续学习，将读取的上下文信息压缩至权重参数中。此外，我们通过在训练阶段引入元学习策略，优化模型在测试时学习的初始化状态。总体而言，本方法作为一种测试时训练形式，在测试阶段（通过词元预测）与训练阶段（通过元学习）均实现端到端优化，与此前方法形成鲜明对比。我们开展了系统性实验并重点考察扩展特性。实验表明：对于使用1640亿词元训练的30亿参数模型，本方法（TTT-E2E）在上下文长度扩展方面展现出与全注意力Transformer相同的特性，而其他方法（如Mamba 2与门控DeltaNet）则不具备该特性。值得注意的是，与循环神经网络类似，TTT-E2E的推理延迟不受上下文长度影响，在处理128K上下文时比全注意力机制快2.7倍。相关代码已公开。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.23675) | [arXiv](https://arxiv.org/abs/2512.23675)



---

### 4. RLVR中参数高效方法的评估研究

**原文标题：** Evaluating Parameter Efficient Methods for RLVR

**摘要：**
本研究在可验证奖励强化学习范式下，系统评估了参数高效微调方法。RLVR通过可验证反馈机制激励语言模型提升推理能力，然而尽管LoRA等方法被广泛采用，适用于RLVR的最佳PEFT架构仍未明确。本文首次在DeepSeek-R1-Distill系列模型上，基于数学推理基准对12种PEFT方法进行了全面评估。实验结果表明标准LoRA的默认采用存在局限性，主要发现如下：首先，DoRA、AdaLoRA和MiSS等结构变体始终优于标准LoRA；其次，我们发现了SVD初始化策略中存在谱崩溃现象，其根本原因在于主成分更新与RL优化目标之间的本质错配；此外，消融实验表明极端参数压缩会严重制约模型推理能力。我们通过系统消融研究与规模扩展实验验证了上述结论。本研究为推进参数高效强化学习方法的探索提供了权威指导。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.23165) | [arXiv](https://arxiv.org/abs/2512.23165)



---

### 5. GraphLocator：基于图引导因果推理的问题定位方法

**原文标题：** GraphLocator: Graph-guided Causal Reasoning for Issue Localization

**摘要：**
问题定位任务旨在根据自然语言问题描述，识别软件仓库中需要修改的代码位置。由于问题描述与源代码实现之间存在语义鸿沟，该任务在自动化软件工程中具有基础性且充满挑战。这一鸿沟具体表现为两种不匹配现象：（1）症状与原因不匹配，即描述未能明确揭示潜在的根本原因；（2）一对多不匹配，即单个问题对应多个相互依赖的代码实体。为应对这两种不匹配，本文提出GraphLocator方法，其通过因果结构发现缓解症状与原因不匹配，并借助动态问题解耦解决一对多不匹配。该方法的核心构件是因果问题图（CIG），其中顶点表示已发现的子问题及其关联代码实体，边则编码它们之间的因果依赖关系。GraphLocator的工作流程包含两个阶段：症状顶点定位与动态CIG发现；该方法首先在仓库图中定位症状位置，随后通过迭代推理相邻顶点动态扩展CIG。在三个真实数据集上的实验验证了GraphLocator的有效性：（1）与基线方法相比，GraphLocator实现了更精准的定位，在函数级召回率与精确率上平均提升分别达+19.49%与+11.89%；（2）在症状-原因不匹配和一对多不匹配场景中，GraphLocator均优于基线方法，召回率分别提升+16.44%与+19.18%，精确率分别提升+7.78%与+13.23%；（3）GraphLocator生成的CIG带来最高相对性能改进，使下游问题解决任务的性能提升达28.74%。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.22469) | [arXiv](https://arxiv.org/abs/2512.22469)



---

### 6. CosineGate：基于余弦不兼容性的残差网络语义动态路由

**原文标题：** CosineGate: Semantic Dynamic Routing via Cosine Incompatibility in Residual Networks

**摘要：**
现代深度残差网络在处理每个输入时均需评估所有残差块，即使恒等映射已足够，这导致了大量冗余计算。本文提出CosineGate——一种端到端可微分的残差网络动态路由架构，该架构利用恒等特征表示与残差特征表示之间的余弦不兼容性作为自监督跳跃信号。CosineGate通过余弦不兼容比（CIR，定义为1 - cos(x, F(x))）度量语义冗余，并采用Gumbel-Softmax松弛方法实现训练过程中逐样本、逐模块的门控机制。渐进式浮点运算正则化项可在不破坏优化稳定性的前提下控制平均计算量。在CIFAR-10数据集上的实验表明，CosineGate能够覆盖精度-效率的帕累托边界：激进配置在节省24.1%浮点运算量的同时达到89.9%准确率；平衡配置在第160轮训练时以28.5%的运算节省实现91.3%准确率；保守配置则以最小计算量削减达到93.2%的峰值准确率。这些结果在减少计算量的同时达到或超越了ResNet-20基准性能（91.3%），且无需辅助监督、知识蒸馏或任务特定启发式方法。我们的研究证明，基于特征不兼容性的简单几何度量可为动态残差路由提供原理清晰且高效的控制信号。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.22206) | [arXiv](https://arxiv.org/abs/2512.22206)



---

### 7. GateBreaker：基于门控引导的混合专家大语言模型攻击方法

**原文标题：** GateBreaker: Gate-Guided Attacks on Mixture-of-Expert LLMs

**摘要：**
混合专家（MoE）架构通过仅激活每个输入对应的稀疏参数子集，推动了大语言模型（LLM）的规模化发展，在降低计算成本的同时实现了先进的性能。随着此类模型在关键领域日益广泛的应用，理解并强化其对齐机制对于防止有害输出至关重要。然而，现有的大语言模型安全研究几乎完全集中于稠密架构，对MoE模型独特的安全特性尚未进行充分探究。MoE模块化、稀疏激活的设计表明其安全机制的运行方式可能与稠密模型存在差异，这引发了关于其鲁棒性的疑问。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.21008) | [arXiv](https://arxiv.org/abs/2512.21008)



---


## 🔍 关键词云图
![关键词云图](../images/keywords_wordcloud.png)

## 📈 近期论文趋势
![论文趋势](../images/daily_papers.png)

## 🎙️ 语音播报
- [收听今日论文解读](../audio/2025-12-31_daily_papers.mp3)

## 📱 订阅渠道
- GitHub: [hf-daily-paper-newsletter-chinese](https://github.com/2404589803/hf-daily-paper-newsletter-chinese)