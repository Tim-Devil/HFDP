
# <img src="https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo.png" width="30"/> Hugging Face 2025-12-31 论文日报

## 📊 今日论文统计
- 总论文数：7
- 热门领域：Transformer, RL, LLM

## 📝 论文详情


### 1. UltraShape 1.0：基于可扩展几何优化的高保真三维形状生成

**原文标题：** UltraShape 1.0: High-Fidelity 3D Shape Generation via Scalable Geometric Refinement

**摘要：**
本报告介绍了UltraShape 1.0，一个可扩展的三维扩散框架，用于实现高保真的三维几何形状生成。该方法采用两阶段生成流程：首先生成粗略的全局结构，随后通过细化处理生成细节丰富的高质量几何形状。为支持可靠的三维生成，我们开发了完整的数据处理流程，包括新颖的封闭化处理方法与高质量数据筛选机制。该流程通过剔除低质量样本、填补孔洞及加厚薄壁结构，在保留细粒度几何细节的同时，显著提升了公开三维数据集的几何质量。为实现细粒度几何优化，我们在扩散过程中将空间定位与几何细节合成进行解耦：通过在固定空间位置执行基于体素的细化操作，利用粗粒度几何生成的体素查询提供通过RoPE编码的显式位置锚点，使扩散模型能够聚焦于在结构化压缩解空间中合成局部几何细节。我们的模型完全基于公开三维数据集进行训练，在有限训练资源下仍实现了优异的几何质量。大量实验评估表明，UltraShape 1.0在数据处理质量与几何生成能力方面均与现有开源方法具有竞争优势。所有代码与训练模型将公开发布以支持后续研究。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.21185) | [arXiv](https://arxiv.org/abs/2512.21185)



---

### 2. DreamOmni3：基于涂鸦的编辑与生成

**原文标题：** DreamOmni3: Scribble-based Editing and Generation

**摘要：**
近期，统一的生成与编辑模型凭借其卓越性能取得了显著成功。这些模型主要依赖文本提示进行基于指令的编辑与生成，但语言往往难以准确捕捉用户期望的编辑位置及细粒度视觉细节。为此，我们提出两项任务：基于涂鸦的编辑与生成，以结合用户文本、图像和手绘草图在图形用户界面（GUI）上实现更灵活的创作。我们推出DreamOmni3模型，重点解决两大挑战：数据构建与框架设计。我们的数据合成流程包含两部分：基于涂鸦的编辑与生成。在基于涂鸦的编辑方面，我们定义了四项子任务：基于涂鸦与指令的编辑、基于涂鸦与多模态指令的编辑、图像融合以及涂鸦编辑。基于DreamOmni2数据集，我们提取可编辑区域并叠加手绘框、圆形、涂鸦或裁剪图像以构建训练数据。在基于涂鸦的生成方面，我们定义了基于涂鸦与指令的生成、基于涂鸦与多模态指令的生成以及涂鸦生成三项子任务，并采用类似的数据构建流程。在框架设计上，针对传统二值掩码难以处理涉及多重涂鸦、图像及指令的复杂编辑任务，我们提出一种联合输入方案：将原始图像与涂鸦后的源图像同时输入模型，通过不同颜色区分区域以简化处理流程。通过对两幅图像施加相同的索引与位置编码，模型能够精确定位涂鸦区域并保持编辑准确性。最后，我们为这些任务建立了综合评估基准以推动后续研究。实验结果表明，DreamOmni3取得了优异性能，相关模型与代码将公开释放。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.22525) | [arXiv](https://arxiv.org/abs/2512.22525)



---

### 3. 面向长上下文的端到端测试时训练

**原文标题：** End-to-End Test-Time Training for Long Context

**摘要：**
本文将长上下文语言建模构建为一个持续学习问题而非架构设计问题。在此框架下，我们仅采用标准架构——基于滑动窗口注意力的Transformer模型。然而，该模型在测试时通过给定上下文中的下一词元预测持续学习，将其读取的上下文信息压缩至权重参数中。此外，我们通过在训练阶段引入元学习策略，优化模型在测试时学习的初始化状态。总体而言，本方法作为一种测试时训练形式，在测试阶段（通过下一词元预测）与训练阶段（通过元学习）均实现端到端处理，这与既有方法形成鲜明对比。我们开展了系统性实验，重点关注方法的扩展特性。具体而言，对于使用1640亿词元训练的30亿参数模型，本方法（TTT-E2E）在上下文长度扩展方面展现出与完全注意力Transformer相同的特性，而其他方法（如Mamba 2和门控DeltaNet）则不具备该特性。值得注意的是，与循环神经网络类似，TTT-E2E的推理延迟不受上下文长度影响，在处理128K上下文时比完全注意力机制快2.7倍。相关代码已公开。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.23675) | [arXiv](https://arxiv.org/abs/2512.23675)



---

### 4. RLVR中参数高效方法的评估研究

**原文标题：** Evaluating Parameter Efficient Methods for RLVR

**摘要：**
本研究在可验证奖励强化学习（RLVR）范式下，系统评估了参数高效微调（PEFT）方法。RLVR通过可验证反馈机制激励语言模型提升推理能力；然而，尽管LoRA等方法被广泛采用，适用于RLVR的最佳PEFT架构仍未明确。本文首次在数学推理基准上对DeepSeek-R1-Distill系列模型的12种PEFT方法进行了全面评估。实证结果对标准LoRA的默认使用提出了挑战，主要发现如下：首先，结构变体方法（如DoRA、AdaLoRA和MiSS）持续优于标准LoRA；其次，我们揭示了基于SVD的初始化策略（如PiSSA、MiLoRA）存在谱崩溃现象，其失效归因于主成分更新与RL优化的根本性错配；此外，消融实验表明极端参数压缩（如VeRA、Rank-1）会严重制约推理能力。我们通过系列消融实验与规模扩展研究验证了上述结论。本研究为倡导参数高效强化学习方法的深入探索提供了权威指导。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.23165) | [arXiv](https://arxiv.org/abs/2512.23165)



---

### 5. GraphLocator：基于图引导因果推理的缺陷定位方法

**原文标题：** GraphLocator: Graph-guided Causal Reasoning for Issue Localization

**摘要：**
缺陷定位任务旨在根据自然语言缺陷描述，识别软件代码库中需要修改的位置。由于缺陷描述与源代码实现之间存在语义鸿沟，该任务在自动化软件工程中既是基础性工作又极具挑战性。这种鸿沟具体表现为两种不匹配现象：（1）症状与原因不匹配，即缺陷描述未明确揭示潜在的根本原因；（2）一对多不匹配，即单个缺陷对应多个相互依赖的代码实体。为应对这两种不匹配，本文提出GraphLocator方法，通过因果结构发现缓解症状与原因不匹配问题，并借助动态缺陷解耦机制解决一对多不匹配问题。该方法的核心构件是因果缺陷图，其中顶点表示已发现的子缺陷及其关联代码实体，边则编码它们之间的因果依赖关系。GraphLocator的工作流程包含两个阶段：症状顶点定位与动态因果缺陷图发现；该方法首先在代码库图中定位症状位置，随后通过迭代推理相邻顶点动态扩展因果缺陷图。在三个真实数据集上的实验验证了GraphLocator的有效性：（1）与基线方法相比，GraphLocator实现了更精确的定位，在函数级召回率上平均提升19.49%，精确率平均提升11.89%；（2）在症状与原因不匹配及一对多不匹配场景中，GraphLocator均优于基线方法，召回率分别提升16.44%和19.18%，精确率分别提升7.78%和13.23%；（3）GraphLocator生成的因果缺陷图带来最高相对性能提升，使下游缺陷修复任务的性能提高28.74%。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.22469) | [arXiv](https://arxiv.org/abs/2512.22469)



---

### 6. CosineGate：基于余弦不兼容性的残差网络语义动态路由机制

**原文标题：** CosineGate: Semantic Dynamic Routing via Cosine Incompatibility in Residual Networks

**摘要：**
现代深度残差网络在处理每个输入时均需评估所有残差块，即使恒等映射已足够，这导致了大量冗余计算。本文提出CosineGate——一种端到端可微分的残差网络动态路由架构，该架构利用恒等特征表示与残差特征表示之间的余弦不兼容性作为自监督跳跃信号。CosineGate通过余弦不兼容比（CIR，定义为 1 - cos(x, F(x))）度量语义冗余，并采用Gumbel-Softmax松弛方法实现训练过程中逐样本、逐块的门控机制。渐进式浮点运算正则化项可在不破坏优化稳定性的前提下控制平均计算量。在CIFAR-10数据集上的实验表明，CosineGate能够覆盖精度-效率的帕累托前沿：激进配置在节省24.1%浮点运算量的同时达到89.9%准确率；平衡配置在第160轮训练时以28.5%的运算量节省实现91.3%准确率；保守配置则以最小计算量削减达到93.2%的峰值准确率。这些结果在减少计算量的同时达到或超越了ResNet-20基准性能（91.3%），且无需辅助监督、知识蒸馏或任务特定启发式方法。我们的研究证明，基于特征不兼容性的简单几何度量可为动态残差路由提供原理清晰且高效的信令机制。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.22206) | [arXiv](https://arxiv.org/abs/2512.22206)



---

### 7. GateBreaker：基于门控引导的混合专家大语言模型攻击方法

**原文标题：** GateBreaker: Gate-Guided Attacks on Mixture-of-Expert LLMs

**摘要：**
混合专家（MoE）架构通过仅激活每个输入对应的稀疏参数子集，推动了大语言模型（LLM）的规模化发展，在降低计算成本的同时实现了先进的性能。随着此类模型在关键领域日益广泛部署，理解并强化其对齐机制对于防止有害输出至关重要。然而，现有的大语言模型安全研究几乎完全集中于密集架构，对MoE模型独特的安全特性尚未充分探究。MoE模块化、稀疏激活的设计特点意味着其安全机制的运行方式可能与密集模型存在差异，这引发了对其鲁棒性的质疑。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.21008) | [arXiv](https://arxiv.org/abs/2512.21008)



---


## 🔍 关键词云图
![关键词云图](../images/keywords_wordcloud.png)

## 📈 近期论文趋势
![论文趋势](../images/daily_papers.png)

## 🎙️ 语音播报
- [收听今日论文解读](../audio/2025-12-31_daily_papers.mp3)

## 📱 订阅渠道
- GitHub: [hf-daily-paper-newsletter-chinese](https://github.com/2404589803/hf-daily-paper-newsletter-chinese)