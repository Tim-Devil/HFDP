
# <img src="https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo.png" width="30"/> Hugging Face 2025-12-03 论文日报

## 📊 今日论文统计
- 总论文数：48
- 热门领域：GPT, RL, Audio, Transformer, LLM

## 📝 论文详情


### 1. DeepSeek-V3.2：推动开源大型语言模型的前沿发展

**原文标题：** DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models

**摘要：**
本文介绍DeepSeek-V3.2模型，该模型实现了高计算效率与卓越推理及智能体性能的协同优化。DeepSeek-V3.2的核心技术突破包括：（1）DeepSeek稀疏注意力机制：我们提出了一种高效注意力机制，在长上下文场景中显著降低计算复杂度的同时保持模型性能。（2）可扩展强化学习框架：通过实施鲁棒的强化学习协议并扩展后训练计算规模，DeepSeek-V3.2达到与GPT-5相当的性能。特别值得注意的是，我们的高计算变体DeepSeek-V3.2-Speciale在推理能力上超越GPT-5，达到与Gemini-3.0-Pro同等水平，并在2025年国际数学奥林匹克竞赛和国际信息学奥林匹克竞赛中均获得金牌级表现。（3）大规模智能体任务合成流程：为将推理能力融入工具使用场景，我们开发了创新的合成流程，系统化生成大规模训练数据。该方法实现了可扩展的智能体后训练，在复杂交互环境中显著提升了泛化能力与指令遵循的鲁棒性。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.02556) | [arXiv](https://arxiv.org/abs/2512.02556)



---

### 2. ToolOrchestra：通过高效模型与工具编排提升智能水平

**原文标题：** ToolOrchestra: Elevating Intelligence via Efficient Model and Tool Orchestration

**摘要：**
大型语言模型是强大的通用系统，但解决如“人类终极考试”（HLE）所涉及的深层复杂问题，在概念上仍具挑战性且计算成本高昂。本文研究表明，通过小型编排器管理其他模型及多样化工具，既能提升智能上限，又能提高解决困难智能体任务的效率。我们提出了ToolOrchestra方法，用于训练能够协调智能工具的小型编排器。该方法明确采用强化学习，并结合结果感知、效率感知和用户偏好感知的奖励机制。基于ToolOrchestra，我们训练出Orchestrator模型（参数量80亿），该模型在实现比以往工具使用智能体更高准确率的同时成本更低，并能根据用户偏好为给定查询选择合适的工具。在HLE测试中，Orchestrator获得37.1%的得分，优于GPT-5（35.1%），且效率提升2.5倍。在tau2-Bench和FRAMES基准测试中，Orchestrator以显著优势超越GPT-5，而成本仅为其约30%。深入分析表明，Orchestrator在多项指标下实现了性能与成本的最佳平衡，并对未见工具展现出稳健的泛化能力。这些结果证明，通过轻量级编排模型整合多样化工具，比现有方法更高效、更有效，为实用且可扩展的工具增强推理系统开辟了道路。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.21689) | [arXiv](https://arxiv.org/abs/2511.21689)



---

### 3. MultiShotMaster：一种可控的多镜头视频生成框架

**原文标题：** MultiShotMaster: A Controllable Multi-Shot Video Generation Framework

**摘要：**
当前的视频生成技术擅长生成单镜头片段，但在生成叙事性多镜头视频方面仍面临挑战。这类视频不仅需要灵活的镜头编排和连贯的叙事逻辑，还要求具备超越文本提示的可控性。为应对这些挑战，我们提出了MultiShotMaster，一个高度可控的多镜头视频生成框架。我们通过集成两种新颖的RoPE变体，对预训练的单镜头模型进行了扩展。首先，我们引入了多镜头叙事RoPE，它在镜头转场处施加显式的相位偏移，从而在保持时序叙事顺序的同时实现灵活的镜头编排。其次，我们设计了时空位置感知RoPE，以融入参考标记和定位信号，实现基于时空定位的参考信息注入。此外，为克服数据稀缺问题，我们建立了一个自动化数据标注流程，用于提取多镜头视频、描述文本、跨镜头定位信号以及参考图像。本框架利用其内在的架构特性支持多镜头视频生成，具备文本驱动的镜头间一致性、支持运动控制的定制主体以及背景驱动的定制场景等特征。镜头数量和时长均可灵活配置。大量实验证明了我们框架的卓越性能和出色的可控性。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.03041) | [arXiv](https://arxiv.org/abs/2512.03041)



---

### 4. MG-Nav：基于稀疏空间记忆的双尺度视觉导航框架

**原文标题：** MG-Nav: Dual-Scale Visual Navigation via Sparse Spatial Memory

**摘要：**
本文提出MG-Nav（记忆引导导航），一种用于零样本视觉导航的双尺度框架，该框架将全局记忆引导规划与局部几何增强控制相统一。其核心是稀疏空间记忆图（SMG），这是一种以区域为中心的紧凑记忆结构，其中每个节点聚合多视角关键帧与物体语义信息，在保持视角多样性的同时捕捉外观与空间结构。在全局层面，智能体基于SMG进行定位，并通过图像-实例混合检索规划目标条件节点路径，生成一系列可达航点以实现长程导航引导。在局部层面，导航基础策略以点目标模式结合障碍物感知控制执行这些航点，并在从最终节点向视觉目标导航时切换至图像目标模式。为进一步增强视角对齐与目标识别能力，我们提出VGGT适配器——一种基于预训练VGGT模型构建的轻量化几何模块，可在共享三维感知空间中对齐观测特征与目标特征。MG-Nav以不同频率执行全局规划与局部控制，并通过周期性重定位修正误差。在HM3D实例-图像-目标与MP3D图像-目标基准测试上的实验表明，MG-Nav实现了最先进的零样本性能，并在动态场景重组与未知场景条件下保持鲁棒性。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.22609) | [arXiv](https://arxiv.org/abs/2511.22609)



---

### 5. Skywork-R1V4：通过图像与深度研究的交错思考迈向具身多模态智能

**原文标题：** Skywork-R1V4: Toward Agentic Multimodal Intelligence through Interleaved Thinking with Images and DeepResearch

**摘要：**
尽管多模态具身系统近期取得了进展，但现有方法通常将图像操作与网络搜索视为分离的能力，严重依赖高成本的强化学习，且缺乏基于真实工具执行轨迹的规划。为应对这些局限，我们提出了Skywork-R1V4——一个拥有300亿（实际30亿）参数的多模态具身模型。该模型统一了多模态规划、主动图像操作（“图像思考”）、深度多模态搜索，以及最关键的在视觉操作与外部知识检索之间动态交替的交错推理能力。通过仅对少于3万条高质量、规划-执行一致的轨迹进行监督微调训练，并经过逐步一致性过滤验证，Skywork-R1V4在感知与多模态搜索基准测试中取得了领先性能：在MMSearch上获得66.1分，在FVQA上获得67.2分，全部11项指标均超越Gemini 2.5 Flash模型。Skywork-R1V4在推理时展现出新兴的长程推理能力，能成功协调超过10次工具调用来解决复杂的多步骤任务。我们的结果表明，无需依赖强化学习，仅通过精心构建的监督学习即可实现高度复杂的具身多模态智能。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.02395) | [arXiv](https://arxiv.org/abs/2512.02395)



---

### 6. DualCamCtrl：用于几何感知相机控制视频生成的双分支扩散模型

**原文标题：** DualCamCtrl: Dual-Branch Diffusion Model for Geometry-Aware Camera-Controlled Video Generation

**摘要：**
本文提出DualCamCtrl，一种用于相机控制视频生成的新型端到端扩散模型。现有研究通过将相机位姿表示为基于光线的条件推动了该领域发展，但这些方法往往缺乏充分的场景理解与几何感知能力。DualCamCtrl针对这一局限性，设计了能协同生成相机一致RGB序列与深度序列的双分支框架。为协调这两种模态，我们进一步提出语义引导互对齐机制，以语义引导、相互增强的方式实现RGB与深度信息的融合。这些设计使DualCamCtrl能更好解耦外观与几何建模，生成更精准遵循指定相机轨迹的视频。此外，我们分析揭示了深度信息与相机位姿在去噪各阶段的差异化影响，并论证了早期与晚期阶段在构建全局结构与细化局部细节方面的互补作用。大量实验表明，DualCamCtrl实现了更一致的相机控制视频生成，其相机运动误差较现有方法降低超过40%。项目页面：https://soyouthinkyoucantell.github.io/dualcamctrl-page/

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.23127) | [arXiv](https://arxiv.org/abs/2511.23127)



---

### 7. 基于最小化人工监督的引导式大语言模型自演化

**原文标题：** Guided Self-Evolving LLMs with Minimal Human Supervision

**摘要：**
人工智能的自演化长期以来被视为通往超级智能的路径，即模型能够从其自身学习经验中自主获取、精化并内化知识。然而在实践中，无引导的自演化系统往往在训练过程中迅速进入平台期甚至性能退化。这类失败源于概念漂移、多样性崩溃与错误演化等问题，即模型不断强化自身偏见并收敛至低熵行为。为实现模型在稳定可控的前提下进行自演化，并最大限度减少对人类监督的依赖，本文提出R-Few——一种融合情境锚定与混合训练的引导式自我博弈挑战者-求解器框架。在每轮迭代中，挑战者通过少量人工标注样本引导合成问题生成，而求解器则依据动态难度课程，对人工与合成样本进行联合训练。在数学与通用推理基准测试中，R-Few实现了持续迭代的性能提升。以Qwen3-8B-Base模型为例，其在数学任务上较R-Zero提升3.0分，且性能与使用20倍人工数据训练的General-Reasoner模型相当。消融实验证实了锚定式挑战者训练与课程化求解器训练的互补作用，进一步分析表明R-Few能有效缓解概念漂移，产生更稳定可控的协同演化动态。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.02472) | [arXiv](https://arxiv.org/abs/2512.02472)



---

### 8. SimScale：基于大规模真实世界模拟的驾驶学习

**原文标题：** SimScale: Learning to Drive via Real-World Simulation at Scale

**摘要：**
实现完全自动驾驶系统需要在广泛场景中学习理性决策，包括安全关键场景和分布外场景。然而，人类专家收集的真实世界数据集中此类案例代表性不足。为弥补数据多样性的缺失，我们提出一种新颖且可扩展的模拟框架，能够在现有驾驶日志基础上合成海量未见状态。该框架采用先进神经渲染技术与反应式环境，通过扰动自车轨迹生成高保真多视角观测数据。此外，我们为这些新模拟状态开发了伪专家轨迹生成机制以提供动作监督。基于合成数据的研究发现，对真实样本与模拟样本采用简单的协同训练策略，能够显著提升多种规划方法在挑战性真实世界基准测试中的鲁棒性与泛化能力——在navhard基准上最高提升+6.8 EPDMS，在navtest基准上提升+2.9。更重要的是，仅通过增加模拟数据（无需额外真实世界数据流），策略改进效果即可实现平滑扩展。我们进一步揭示了此类模拟-真实学习系统（命名为SimScale）的若干关键发现，包括伪专家设计原则及不同策略架构的扩展特性。本研究的模拟数据与代码将予以公开。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.23369) | [arXiv](https://arxiv.org/abs/2511.23369)



---

### 9. InnoGym：评估AI智能体创新潜力的基准测试

**原文标题：** InnoGym: Benchmarking the Innovation Potential of AI Agents

**摘要：**
大语言模型与智能体在代码生成、数学推理和科学发现等领域已取得显著进展。然而，现有基准测试主要关注解决方案的正确性，忽视了方法背后的多样性。真正的创新不仅取决于答案的正确性，更取决于方法的原创性。本文提出InnoGym——首个系统评估AI智能体创新潜力的基准测试框架。InnoGym引入两项互补指标：性能增益（衡量对已知最优方案的改进程度）和新颖性（捕捉与既有方法的方法论差异）。该基准包含18项从真实工程与科学领域精心筛选的任务，每项任务均通过资源过滤、评估验证和方案收集实现标准化。此外，我们提供iGym统一执行环境，支持可复现的长周期评估。大量实验表明，虽然部分智能体能产生新颖方法，但其鲁棒性不足限制了性能提升。这些结果揭示了创造力与有效性之间的关键差距，凸显了需要同时评估两者的基准测试体系。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.01822) | [arXiv](https://arxiv.org/abs/2512.01822)



---

### 10. ViSAudio：端到端视频驱动的双耳空间音频生成

**原文标题：** ViSAudio: End-to-End Video-Driven Binaural Spatial Audio Generation

**摘要：**
尽管视频到音频生成领域已取得进展，但现有研究主要集中于单声道输出，缺乏空间沉浸感。现有的双耳音频生成方法仍受限于两阶段流程，即首先生成单声道音频再进行空间化处理，这往往导致误差累积与时空不一致问题。为突破这一局限，本文提出从无声视频直接端到端生成双耳空间音频的任务。为支持该任务，我们构建了BiAudio数据集，该数据集通过半自动化流程整合了约9.7万个视频-双耳音频对，涵盖多样化的真实场景与摄像机旋转轨迹。进一步，我们提出ViSAudio端到端框架，该框架采用条件流匹配与双分支音频生成架构，通过两个独立分支对音频隐空间流进行建模。结合条件时空模块，该框架在保持声道间一致性的同时保留了独特的空间特征，确保了音频与输入视频间精确的时空对齐。综合实验表明，ViSAudio在客观指标与主观评估上均优于现有先进方法，能够生成具有空间沉浸感的高质量双耳音频，并能有效适应视角变化、声源运动及多样声学环境。项目网站：https://kszpxxzmc.github.io/ViSAudio-project。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.03036) | [arXiv](https://arxiv.org/abs/2512.03036)



---

### 11. Glance：基于单样本的扩散模型加速方法

**原文标题：** Glance: Accelerating Diffusion Models with 1 Sample

**摘要：**
扩散模型在图像生成领域取得了显著成功，但其应用仍受限于高昂的计算成本和大量的推理步骤需求。先前关于少步蒸馏的研究试图通过训练紧凑的学生模型来跳过冗余步骤，但这些方法往往面临繁重的重新训练成本与泛化性能下降的问题。本研究提出一种新视角：采用智能而非均匀的加速策略，对早期语义阶段施加较小的加速，而对后期冗余阶段实施较大的加速。我们通过两个分别专注于慢速去噪阶段和快速去噪阶段的专家模型来实现这种阶段感知策略。令人惊讶的是，与投入大量资源重新训练学生模型不同，我们发现仅需为基础模型配备轻量级LoRA适配器即可同时实现高效加速与强大泛化能力。我们将这两种适配器分别命名为慢速LoRA与快速LoRA。大量实验表明，该方法在保持多种基准测试中视觉质量相当的前提下，实现了相较于基础模型最高达5倍的加速效果。值得注意的是，LoRA专家模型仅需在单张V100显卡上使用1个样本训练一小时，所得模型在未见过的提示词上仍展现出强大的泛化能力。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.02899) | [arXiv](https://arxiv.org/abs/2512.02899)



---

### 12. WorldMM：面向长视频推理的动态多模态记忆智能体

**原文标题：** WorldMM: Dynamic Multimodal Memory Agent for Long Video Reasoning

**摘要：**
视频大语言模型的最新进展在理解短视频片段方面展现出强大能力。然而，由于上下文容量有限以及在抽象过程中关键视觉细节的丢失，将其扩展至数小时甚至数天时长的视频仍极具挑战性。现有的记忆增强方法通过利用视频片段的文本摘要来缓解这一问题，但这些方法严重依赖文本，在复杂场景推理时未能有效利用视觉证据。此外，基于固定时间尺度的检索方式进一步限制了其捕捉不同持续时间事件的灵活性。为此，我们提出WorldMM，一种新颖的多模态记忆智能体，它构建并检索多种互补记忆，涵盖文本与视觉表征。WorldMM包含三类记忆：情景记忆通过多时间尺度索引事实事件，语义记忆持续更新高层概念知识，视觉记忆则保留场景的细节信息。在推理过程中，自适应检索智能体根据查询内容迭代选择最相关的记忆源，并利用多时间粒度进行分析，直至确定已收集足够信息。在五个长视频问答基准测试中，WorldMM显著优于现有基线方法，相较先前最优方法平均性能提升8.4%，充分证明了其在长视频推理任务上的有效性。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.02425) | [arXiv](https://arxiv.org/abs/2512.02425)



---

### 13. 深度研究：一项系统性综述

**原文标题：** Deep Research: A Systematic Survey

**摘要：**
大语言模型已从文本生成器迅速发展为强大的问题解决工具。然而，许多开放性任务需要批判性思维、多源信息与可验证的输出，这已超出单次提示或标准检索增强生成的能力范围。近期，大量研究开始探索深度研究，其目标是将大语言模型的推理能力与搜索引擎等外部工具相结合，从而使大语言模型能够作为研究智能体完成复杂、开放式的任务。本综述对深度研究系统进行了全面而系统的梳理，包括清晰的路线图、基础组件、实践实现技术、重要挑战与未来方向。具体而言，我们的主要贡献如下：（一）提出了三阶段路线图的形式化框架，并明确了深度研究与相关范式的区别；（二）介绍了四个关键组件：查询规划、信息获取、记忆管理与答案生成，每个组件均配有细粒度的子分类体系；（三）总结了包括提示工程、监督微调与智能体强化学习在内的优化技术；（四）整合了评估标准与开放挑战，旨在引导和促进该领域的未来发展。随着深度研究领域的持续快速演进，我们将持续更新本综述以反映该领域的最新进展。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.02038) | [arXiv](https://arxiv.org/abs/2512.02038)



---

### 14. PixelDiT：用于图像生成的像素扩散Transformer

**原文标题：** PixelDiT: Pixel Diffusion Transformers for Image Generation

**摘要：**
潜空间建模一直是扩散Transformer（DiTs）的标准范式。然而，该方法依赖于两阶段流程，其中预训练的自编码器会引入有损重构，导致误差累积并阻碍联合优化。为解决这些问题，我们提出PixelDiT——一种单阶段、端到端的模型，无需依赖自编码器，直接在像素空间中学习扩散过程。PixelDiT采用全Transformer架构，其设计包含双重层级：捕捉全局语义的块级DiT与细化纹理细节的像素级DiT，在保留精细细节的同时实现了像素空间扩散模型的高效训练。我们的分析表明，有效的像素级令牌建模是像素扩散成功的关键。PixelDiT在ImageNet 256×256分辨率上取得了1.61的FID分数，显著超越了现有像素生成模型。我们进一步将PixelDiT扩展至文本到图像生成任务，并在像素空间中以1024×1024分辨率进行预训练。该模型在GenEval评测中达到0.74分，在DPG-bench中取得83.5分，性能已接近最佳潜扩散模型。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.20645) | [arXiv](https://arxiv.org/abs/2511.20645)



---

### 15. 动作分块中的混合视野策略

**原文标题：** Mixture of Horizons in Action Chunking

**摘要：**
视觉-语言-动作模型在机器人操作任务中展现出卓越能力，但其性能对训练时采用的动作块长度（称为视野）十分敏感。实证研究表明其中存在固有权衡：较长视野能提供更强的全局预见性，但会降低细粒度动作精度；较短视野虽能提升局部控制精度，却难以应对长期任务，这意味着固定单一视野的选择具有次优性。为缓解这一矛盾，本文提出混合视野策略。该策略将动作块重组为具有不同视野的多个片段，通过共享动作变换器进行并行处理，并利用轻量级线性门融合输出。该方法具有三大优势：1）在单一模型内协同利用长期预见性与短期精确性，提升复杂任务中的性能与泛化能力；2）可作为即插即用模块适配全注意力动作架构，仅引入极少的训练与推理开销；3）支持自适应视野的动态推理机制，通过跨视野一致性筛选稳定动作，在保持优异性能的同时实现比基线方法高2.5倍的吞吐量。基于流式策略π_0、π_{0.5}及单步回归策略π_{reg}的广泛实验表明，混合视野策略在仿真与真实任务中均能带来持续显著的性能提升。值得注意的是，在混合任务场景下，搭载混合视野的π_{0.5}策略仅经过3万次训练迭代即在LIBERO基准上达到99%的平均成功率，创造了新的性能纪录。项目页面：https://github.com/Timsty1/MixtureOfHorizons

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.19433) | [arXiv](https://arxiv.org/abs/2511.19433)



---

### 16. WUSH：面向大语言模型量化的近最优自适应变换

**原文标题：** WUSH: Near-Optimal Adaptive Transforms for LLM Quantization

**摘要：**
低比特量化是部署大语言模型的常用方法，但少数极端权重和激活值会扩大动态范围，降低量化器的有效分辨率。常见的缓解方法是在量化前应用固定的正交变换（如哈达玛矩阵），这通常能压缩动态范围。然而，这类变换忽略了数据统计特性，其最优性尚未得到充分论证。本研究首次针对常见数值格式，推导出基于标准无数据量化器的联合权重-激活值最优线性分块变换闭式解。具体而言，我们推导了适用于整数与浮点格式的最近舍入（RTN）和绝对值最大分块缩放量化器的最优自适应（数据感知）变换。所得构建方法命名为WUSH，其将哈达玛矩阵主干与基于二阶矩的数据依赖组件相结合，形成一种在温和假设下可证明最优的非正交变换，同时保持结构化特性以实现高效计算。初步实验结果表明，该方法在常见数值格式上均能稳定超越哈达玛变换的效果。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.00956) | [arXiv](https://arxiv.org/abs/2512.00956)



---

### 17. GoRL：一种与算法无关的生成策略在线强化学习框架

**原文标题：** GoRL: An Algorithm-Agnostic Framework for Online Reinforcement Learning with Generative Policies

**摘要：**
强化学习（RL）面临一个长期存在的矛盾：易于优化的策略往往过于简单，难以表达复杂控制所需的多模态动作分布。高斯策略提供了易于处理的似然函数和平滑梯度，但其单模态形式限制了表达能力。相反，基于扩散或流匹配的生成策略能够建模丰富的多模态行为；然而，在在线强化学习中，由于似然函数难以处理以及梯度在深度采样链中传播时的噪声问题，这些策略常常不稳定。我们通过一个关键的结构性原则来解决这一矛盾：将优化过程与生成过程解耦。基于这一思路，我们提出了GoRL（生成式在线强化学习），该框架通过优化一个易于处理的潜在策略，并利用条件生成解码器来合成动作。采用双时间尺度更新机制，使得潜在策略能够稳定学习，同时解码器逐步增强表达能力，且无需计算可处理的动作似然函数。在一系列连续控制任务中，GoRL的表现始终优于高斯策略和近期提出的生成策略基线方法。值得注意的是，在HopperStand任务中，其归一化回报达到870以上，超过最强基线方法的三倍。这些结果表明，将优化与生成分离为实现既稳定又具有高度表达能力的策略提供了一条实用路径。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.02581) | [arXiv](https://arxiv.org/abs/2512.02581)



---

### 18. CUDA-L2：通过强化学习超越cuBLAS的矩阵乘法性能

**原文标题：** CUDA-L2: Surpassing cuBLAS Performance for Matrix Multiplication through Reinforcement Learning

**摘要：**
本文提出CUDA-L2系统，该系统结合大语言模型（LLMs）与强化学习（RL），以自动化方式优化半精度通用矩阵乘法（HGEMM）的CUDA内核。以CUDA执行速度作为强化学习奖励，CUDA-L2在1,000种配置上自动优化HGEMM内核。实验表明，CUDA-L2系统性地超越了当前主要的矩阵乘法基准方法，从广泛使用的{\it torch.matmul}到最先进的英伟达闭源库（即{\it cuBLAS}和{\it cuBLASLt}）。在离线模式下（内核连续执行无时间间隔），CUDA-L2相比{\it torch.matmul}平均提升22.0%；相比采用最优布局配置（normal-normal NN与transposed-normal TN）的{\it cuBLAS}提升19.2%；相比基于启发式建议从{\it cuBLASLt}库选择算法的{\it cuBLASLt-heuristic}提升16.8%；相比最具竞争力的{\it cuBLASLt-AutoTuning}模型（从{\it cuBLASLt}提供的多达100个候选算法中选择最优方案）提升11.4%。在模拟实时推理的随机间隔执行内核的服务器模式下，加速效果进一步提升：相比{\it torch.matmul}、{\it cuBLAS}、{\it cuBLASLt-heuristic}和{\it cuBLASLt-AutoTuning}分别达到+28.7%、+26.0%、+22.4%和+15.9%。CUDA-L2证明，即使是如HGEMM这类对性能要求极高且经过深度优化的内核，仍可通过大语言模型引导的强化学习自动化实现性能突破——该系统以人类难以实现的规模系统探索配置空间。项目与代码详见github.com/deepreinforce-ai/CUDA-L2。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.02551) | [arXiv](https://arxiv.org/abs/2512.02551)



---

### 19. 听觉能否辅助视觉？视频生成中音视频联合去噪机制研究

**原文标题：** Does Hearing Help Seeing? Investigating Audio-Video Joint Denoising for Video Generation

**摘要：**
近期音视频生成系统研究表明，多模态耦合不仅有助于提升音视频同步性，更能优化视频模态本身的生成质量。本文提出一个基础性问题：即使仅关注视频质量，音视频联合去噪训练是否仍能提升视频生成效果？为探究此问题，我们设计了一种参数高效的音视频全扩散变换器（AVFullDiT）架构，该架构利用预训练的文本到视频（T2V）与文本到音频（T2A）模块进行联合去噪训练。我们在相同实验设置下分别训练了：（1）基于AVFullDiT的T2AV多模态模型；（2）仅使用视频模态的T2V对照模型。实验结果首次系统性地证明，音视频联合去噪训练带来的效益超越同步性提升。在包含大幅物体运动及物体接触动作的挑战性数据子集上，我们观察到模型性能的持续改善。我们推测，音频预测作为一种特权信号，能够促使模型内化视觉事件与其声学后果之间的因果关系（例如碰撞时机对声音的影响），从而实现对视频动态特性的正则化约束。本研究结果表明，跨模态协同训练是构建更强大、更符合物理规律的世界模型的有效路径。代码与数据集将公开发布。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.02457) | [arXiv](https://arxiv.org/abs/2512.02457)



---

### 20. 类比推理的独特案例：探究大语言模型中的类比推理机制

**原文标题：** The Curious Case of Analogies: Investigating Analogical Reasoning in Large Language Models

**摘要：**
类比推理是人类认知的核心，为多种智力活动提供了重要基础。尽管已有研究表明大语言模型能够表征任务模式与表层概念，但这些模型能否编码高层次关系概念并通过结构化比较将其应用于新情境仍不明确。本研究通过比例类比与故事类比探究这一基础问题，并得出三个关键发现：首先，大语言模型能有效编码类比实体间的深层关系——在正确推理案例中，属性信息与关系信息均通过中高层网络传播；而推理失败案例则反映出这些层级中关系信息的缺失。其次，与人类不同，大语言模型不仅在关系信息缺失时表现困难，在尝试将关系信息迁移至新实体时也常遇阻碍。在此类情境中，对关键标记位置的隐层表征进行策略性修补可在一定程度上促进信息传递。最后，成功的类比推理以类比情境间强烈的结构对齐为特征，而推理失败往往表现为结构对齐的弱化或错位。总体而言，本研究表明大语言模型在编码与应用高层次关系概念方面呈现出初现但有限的能力，既揭示了其与人类认知的相通之处，也凸显了二者间的显著差距。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.20344) | [arXiv](https://arxiv.org/abs/2511.20344)



---

### 21. DiG-Flow：基于差异引导的流匹配方法用于增强视觉-语言-动作模型的鲁棒性

**原文标题：** DiG-Flow: Discrepancy-Guided Flow Matching for Robust VLA Models

**摘要：**
通过流匹配训练的视觉-语言-动作模型在机器人操作任务中展现出卓越的性能。然而，在分布偏移和复杂多步骤任务中，其性能常出现下降，这表明所学表征可能未能鲁棒地捕捉任务相关的语义信息。本文提出DiG-Flow，一个通过几何正则化增强VLA模型鲁棒性的理论框架。我们的核心见解是：观测嵌入与动作嵌入之间的分布差异能够提供有意义的几何信号——较低的传输成本表征兼容的表征，而较高的成本则暗示潜在的对齐偏差。DiG-Flow通过计算观测与动作嵌入经验分布间的差异度量，借助单调函数将其映射为调制权重，并在流匹配前对观测嵌入施加残差更新。关键的是，这种干预操作在表征层面进行，无需修改流匹配路径或目标向量场。我们提供了理论证明：差异引导的训练可严格降低训练目标函数，且引导的推断优化过程具有收敛性与收缩性。实验表明，DiG-Flow能以可忽略的开销集成到现有VLA架构中，持续提升模型性能，在复杂多步骤任务及有限训练数据场景下提升尤为显著。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.01715) | [arXiv](https://arxiv.org/abs/2512.01715)



---

### 22. RULER-Bench：面向视觉基础智能的新一代视频生成模型规则推理能力测评基准

**原文标题：** RULER-Bench: Probing Rule-based Reasoning Abilities of Next-level Video Generation Models for Vision Foundation Intelligence

**摘要：**
视频生成技术的最新进展使得合成视频在时间连贯性和视觉质量方面表现突出，这标志着向视觉基础模型迈出了关键一步。为评估这些视频生成模型，现有基准主要关注与视觉感知和理解相关的因素，如视觉美感、指令遵循度和时间一致性。然而，视频生成模型的规则推理能力在很大程度上仍未得到充分探索。尽管近期研究已对视频模型能否作为零样本学习者进行了初步探索，但仍缺乏对推理能力的细粒度分解和全面的评估方案。为填补这一空白，我们提出了RULER-Bench，这是一个旨在从认知规则角度评估视频生成模型推理能力的基准。基于文本到视频和图像到视频两种基本范式，RULER-Bench涵盖了六大规则类别下的40项代表性任务，包含622个高质量标注实例。针对每个生成视频的评估，我们构建了涵盖四项指标的检查表，并利用GPT-4o为每个问题分配分数，实现了与人工判断85%的一致性。大量实验表明，当前最先进的模型在规则连贯性指标上仅达到48.87%，凸显了新一代视频模型在推理能力方面仍有显著提升空间。我们期望通过RULER-Bench获得的见解将推动具有推理意识的视频生成的进一步发展，促进视频生成模型向视觉基础智能迈进。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.02622) | [arXiv](https://arxiv.org/abs/2512.02622)



---

### 23. TRivia：面向表格识别的视觉-语言模型自监督微调方法

**原文标题：** TRivia: Self-supervised Fine-tuning of Vision-Language Models for Table Recognition

**摘要：**
表格识别（TR）旨在将表格图像转换为半结构化表示（如HTML或Markdown）。作为文档解析的核心组成部分，TR长期依赖监督学习，近期研究主要通过标注数据对视觉-语言模型（VLMs）进行微调。尽管VLMs已将TR性能提升至新高度，但进一步突破需要大规模标注数据，其获取成本高昂。因此，尽管专有模型持续突破性能边界，开源模型——常因资源受限而训练不足，且因隐私法规成为许多场景下的唯一可行选择——仍存在显著差距。为弥合这一鸿沟，我们提出TRivia：一种自监督微调方法，使预训练VLMs能够直接从无标注的真实场景表格图像中学习TR。该方法基于组相对策略优化构建，可自动识别最能促进学习的无标注样本，并通过基于问答的奖励机制消除对人工标注的依赖。注意力引导模块为每张表格图像生成多样化问题，而模型对识别结果的解析与正确回答能力将反馈至TR模型优化过程。这一闭环流程使TR模型能够无需标注数据即可自主学习表格的识别、结构化与推理。基于此框架，我们推出TRivia-3B——一个开源、紧凑且性能领先的TR模型，在三个主流基准测试中超越现有系统（如Gemini 2.5 Pro、MinerU2.5）。模型与代码已发布于：https://github.com/opendatalab/TRivia

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.01248) | [arXiv](https://arxiv.org/abs/2512.01248)



---

### 24. MagicQuillV2：基于分层视觉线索的精确交互式图像编辑

**原文标题：** MagicQuillV2: Precise and Interactive Image Editing with Layered Visual Cues

**摘要：**
本文提出MagicQuill V2系统，该创新系统通过引入分层组合范式到生成式图像编辑领域，弥合了扩散模型的语义生成能力与传统图形软件精细化控制之间的鸿沟。尽管扩散变换器在整体图像生成方面表现出色，但其使用的单一聚合式提示词难以区分用户在内容、位置和外观等方面的不同创作意图。为解决此问题，我们的方法将创作意图解构为可控制视觉线索的层级结构：内容层定义生成对象，空间层确定对象位置，结构层控制形态轮廓，色彩层管理色调搭配。我们的技术贡献包括：用于上下文感知内容融合的专用数据生成流程、可统一处理所有视觉线索的控制模块，以及支持精确局部编辑（包括对象移除）的微调空间分支。大量实验验证表明，这种分层方法能有效解决用户意图传达的鸿沟，使创作者能够直接、直观地控制图像生成过程。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.03046) | [arXiv](https://arxiv.org/abs/2512.03046)



---

### 25. 重新审视视觉中心推理泛化中长链思维的必要性

**原文标题：** Revisiting the Necessity of Lengthy Chain-of-Thought in Vision-centric Reasoning Generalization

**摘要：**
本研究探讨不同链式思维设计如何影响视觉语言模型泛化性视觉推理能力的习得。尽管链式思维数据（尤其是长链或视觉链式思维，如“基于图像的思考”）已被广泛用于监督中间推理过程，但特定链式思维设计为何有效、何种设计真正支持泛化性推理仍不明确。为系统评估该问题，我们采用可控的迷宫求解基准任务，其中推理规则完全基于视觉呈现，难度可通过网格尺寸调节，且所有中间步骤均可自动生成。在标准监督微调后强化学习的训练框架下，我们使用Qwen2.5-VL-7B模型比较了三种代表性链式思维格式：语言链式思维、具象化链式思维（含空间坐标轨迹）和视觉链式思维（含图像操作）。实验表明：视觉化与长链式思维主要加速收敛速度但未提升最终性能上限；仅包含必要具象化步骤的简洁链式思维优于长链轨迹；值得注意的是，仅保留最小必要具象化信息的链式思维在不同迷宫尺寸中展现出最佳泛化能力。我们进一步在其他视觉中心任务上验证了这些发现。这些结果揭示了“少即是多”效应，为构建更具泛化性的视觉推理监督微调数据集提供了实践指导。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.22586) | [arXiv](https://arxiv.org/abs/2511.22586)



---

### 26. PAI-Bench：面向物理人工智能的综合基准测试框架

**原文标题：** PAI-Bench: A Comprehensive Benchmark For Physical AI

**摘要：**
物理人工智能旨在开发能够感知并预测现实世界动态的模型；然而，当前多模态大语言模型与视频生成模型对这些能力的支持程度尚未得到充分理解。本研究提出物理人工智能基准测试框架（PAI-Bench），这是一个统一且全面的评估体系，涵盖视频生成、条件视频生成及视频理解三大任务的感知与预测能力评估。该基准包含2,808个真实场景案例，并采用任务对齐的度量标准，旨在捕捉物理合理性与领域特定推理能力。通过对近期模型的系统性评估，研究发现：视频生成模型虽具备较强的视觉保真度，但在保持物理动态连贯性方面仍存在明显不足；而多模态大语言模型在动态预测与因果解释任务中表现有限。这些发现表明，现有系统在应对物理人工智能的感知与预测需求方面仍处于早期发展阶段。总体而言，PAI-Bench为评估物理人工智能建立了现实基础，并揭示了未来系统必须解决的关键能力缺口。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.01989) | [arXiv](https://arxiv.org/abs/2512.01989)



---

### 27. Video4Spatial：基于上下文引导视频生成的视觉空间智能探索

**原文标题：** Video4Spatial: Towards Visuospatial Intelligence with Context-Guided Video Generation

**摘要：**
本研究探讨视频生成模型能否仅凭视觉数据展现出人类认知核心能力之一的视觉空间智能。为此，我们提出Video4Spatial框架，证明仅以视频场景上下文为条件的视频扩散模型能够执行复杂空间任务。我们在两项任务上验证其有效性：场景导航——在遵循相机位姿指令的同时保持与场景三维几何结构的一致性；以及对象定位——该任务需要语义定位、指令跟随与路径规划能力。两项任务均仅使用视频输入，无需深度信息或位姿等辅助模态。通过框架设计与数据构建中简洁高效的技术方案，Video4Spatial展现出基于视频上下文的强大空间理解能力：能够端到端规划导航路径并定位目标对象，在遵循相机位姿指令的同时保持空间一致性，并能泛化至长序列上下文及域外环境。综合而言，这些成果推动视频生成模型向通用视觉空间推理迈出了重要一步。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.03040) | [arXiv](https://arxiv.org/abs/2512.03040)



---

### 28. YingVideo-MV：音乐驱动的多阶段视频生成

**原文标题：** YingVideo-MV: Music-Driven Multi-Stage Video Generation

**摘要：**
尽管基于扩散模型的音频驱动虚拟形象视频生成在合成具有自然音画同步与身份一致性的长序列方面已取得显著进展，但包含摄像机运动的音乐表演视频生成领域仍鲜有探索。本文提出YingVideo-MV，首个面向音乐驱动的长视频生成的级联框架。该方法整合了音频语义分析、可解释的镜头规划模块（MV-Director）、时序感知的扩散Transformer架构以及长序列一致性建模技术，实现了从音频信号自动合成高质量音乐表演视频。我们通过收集网络数据构建了大规模“野外音乐数据集”，以支持生成多样化、高质量的结果。针对现有长视频生成方法缺乏显式摄像机运动控制的问题，我们引入了摄像机适配器模块，将摄像机位姿嵌入潜在噪声空间。为提升长序列推理中片段间的连续性，进一步提出基于时序感知的动态窗口范围策略，可根据音频嵌入自适应调整去噪范围。综合基准测试表明，YingVideo-MV在生成连贯且富有表现力的音乐视频方面表现优异，并能实现精确的音乐-动作-摄像机同步。更多视频请访问项目页面：https://giantailab.github.io/YingVideo-MV/。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.02492) | [arXiv](https://arxiv.org/abs/2512.02492)



---

### 29. SimWorld：面向物理与社会世界中自主智能体的开放式真实模拟器

**原文标题：** SimWorld: An Open-ended Realistic Simulator for Autonomous Agents in Physical and Social Worlds

**摘要：**
尽管基于大语言模型/视觉语言模型的人工智能体在数学、编程和计算机应用领域发展迅速，但它们在复杂物理与社会环境中的应用仍面临挑战。构建能够在现实世界中生存与发展（例如通过自主创收或运营企业）的智能体，需要在多样化的具身场景中进行大规模交互、推理、训练与评估。然而，现有用于此类开发的世界模拟器存在明显不足：它们通常依赖有限的手工构建环境，模拟简化的游戏式物理与社会规则，且缺乏对大语言模型/视觉语言模型智能体的原生支持。本文提出SimWorld——一个基于虚幻引擎5构建的新型模拟器，旨在为开发与评估大语言模型/视觉语言模型智能体提供丰富、类真实世界的环境。SimWorld具备三项核心能力：（1）真实、开放的世界模拟，包括精确的物理与社会动态机制，以及基于语言的程序化环境生成；（2）面向大语言模型/视觉语言模型智能体的丰富交互接口，支持多模态世界输入和不同抽象层级的开放词汇动作；（3）多样化、可扩展的物理与社会推理场景，用户可便捷进行定制。我们通过在前沿大语言模型智能体（如GPT-4o、Gemini-2.5-Flash、Claude-3.5和DeepSeek-Prover-V2）上部署涉及战略合作与竞争的长期多智能体配送任务，展示了SimWorld的实用性。实验结果揭示了不同模型间显著的推理模式差异与局限性。我们已开源SimWorld平台，期待其成为推进跨学科现实世界智能体智能发展的基础平台：https://simworld.org。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.01078) | [arXiv](https://arxiv.org/abs/2512.01078)



---

### 30. SwiftVLA：以最小开销解锁轻量级视觉-语言-动作模型的时空动态理解

**原文标题：** SwiftVLA: Unlocking Spatiotemporal Dynamics for Lightweight VLA Models at Minimal Overhead

**摘要：**
基于预训练视觉-语言模型构建的视觉-语言-动作模型展现出强大潜力，但因其参数量庞大而实用性受限。为缓解此问题，已有研究探索使用轻量级视觉-语言模型，但这会牺牲时空推理能力。尽管部分方法提出引入额外三维输入可改善性能，但它们通常依赖大型视觉-语言模型融合三维与二维输入，且仍缺乏时序理解能力。为此，我们提出SwiftVLA架构，在保持设计效率的同时，为紧凑模型赋予四维理解能力。具体而言，本方法采用预训练的时序缓存四维视觉几何变换器，从二维图像中提取四维特征。随后，为增强视觉-语言模型协同利用二维图像与四维特征的能力，我们引入融合标记——一组通过未来预测目标训练的可学习标记，用于生成动作预测的统一表征。最后，我们提出掩码重构策略：通过掩码输入视觉-语言模型的四维数据并训练视觉-语言-动作模型进行重构，使视觉-语言模型能学习有效的四维表征，进而在推理阶段可舍弃四维分支且性能损失极小。真实环境与仿真实验表明，SwiftVLA性能优于轻量级基线模型，并与参数量达其7倍的大型视觉-语言-动作模型相当，在边缘设备上实现可比性能的同时，推理速度提升18倍，内存占用减少12倍。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.00903) | [arXiv](https://arxiv.org/abs/2512.00903)



---

### 31. Ovis-Image技术报告

**原文标题：** Ovis-Image Technical Report

**摘要：**
本文介绍Ovis-Image，这是一个专门针对高质量文本渲染优化的70亿参数文生图模型，其设计目标是在严格的计算限制下高效运行。该模型基于我们先前提出的Ovis-U1框架构建，将基于扩散的视觉解码器与性能更强的Ovis 2.5多模态骨干网络相结合，并采用以文本为中心的训练流程——该流程融合了大规模预训练与精心设计的训练后优化阶段。尽管采用紧凑架构，Ovis-Image在文本渲染性能上已达到Qwen-Image等显著更大规模开源模型的水平，并接近Seedream、GPT4o等闭源系统。关键优势在于，该模型仅需单张高端GPU与适中显存即可部署，从而显著缩小了前沿文本渲染能力与实际应用部署之间的鸿沟。实验结果表明：通过将强大的多模态骨干网络与精心设计的文本导向训练方案相结合，无需依赖超大参数规模或专有模型，即可实现可靠的双语文本渲染能力。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.22982) | [arXiv](https://arxiv.org/abs/2511.22982)



---

### 32. GUI探索实验室：通过多轮强化学习增强智能体在屏幕导航中的能力

**原文标题：** GUI Exploration Lab: Enhancing Screen Navigation in Agents via Multi-Turn Reinforcement Learning

**摘要：**
随着大规模视觉语言模型的快速发展，图形用户界面智能体任务的研究重点已从单一屏幕任务转向复杂的屏幕导航挑战。然而，现实中的图形用户界面环境（如PC软件和移动应用）通常结构复杂且具有专有性，难以获取智能体训练与评估所需的完整环境信息。这一限制阻碍了对智能体导航能力的系统性研究与基准测试。为应对此问题，我们引入了GUI探索实验室——一个专为图形用户界面智能体导航研究设计的仿真环境引擎。该引擎支持灵活定义与组合屏幕、图标及导航图结构，同时提供完整的环境信息访问权限，以实现全面的智能体训练与评估。通过大量实验，我们发现监督微调能够有效促进基础知识的记忆，为后续训练奠定关键基础。在此基础上，单轮强化学习进一步增强了模型对未见场景的泛化能力。最终，多轮强化学习通过交互式试错机制鼓励探索策略的发展，从而显著提升屏幕导航性能。我们在静态与交互式基准测试中验证了所提方法的有效性，证明研究成果能够很好地推广到实际应用场景。这些发现不仅彰显了强化学习方法在图形用户界面导航中的优势，也为构建更强大、更具泛化能力的图形用户界面智能体提供了实践指导。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.02423) | [arXiv](https://arxiv.org/abs/2512.02423)



---

### 33. FlashVGGT：基于压缩描述符注意力的高效可扩展视觉几何变换器

**原文标题：** FlashVGGT: Efficient and Scalable Visual Geometry Transformers with Compressed Descriptor Attention

**摘要：**
从多视角图像进行三维重建是计算机视觉领域的核心挑战。近年来，前馈方法已成为传统逐场景优化技术的高效且鲁棒的替代方案。其中，视觉几何基础变换器（VGGT）等先进模型通过对所有图像令牌进行全局自注意力来捕捉全局关系。然而，由于自注意力的二次复杂度以及长图像序列中生成的大量令牌，该方法存在可扩展性不足的问题。本研究提出了FlashVGGT，一种基于描述符的注意力机制的高效替代方案，以解决这一瓶颈。FlashVGGT不再对所有令牌进行密集的全局注意力计算，而是将每帧图像的空间信息压缩为一组紧凑的描述符令牌。随后，通过完整图像令牌集与较小描述符集之间的交叉注意力来计算全局注意力，从而显著降低计算开销。此外，描述符的紧凑性使其能够通过分块递归机制实现长序列的在线推理，该机制可复用先前分块的缓存描述符。实验结果表明，FlashVGGT在重建精度上与VGGT相当，同时在处理1000张图像时将推理时间降至VGGT的9.3%，并能高效扩展至超过3000张图像的序列。项目页面详见：https://wzpscott.github.io/flashvggt_page/。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.01540) | [arXiv](https://arxiv.org/abs/2512.01540)



---

### 34. BlockVid：基于块扩散的高质量、一致性分钟级视频生成方法

**原文标题：** BlockVid: Block Diffusion for High-Quality and Consistent Minute-Long Video Generation

**摘要：**
生成长达分钟级别的视频是构建世界模型的关键一步，可为生成逼真的扩展场景和高级人工智能模拟器奠定基础。新兴的半自回归（块扩散）范式融合了扩散模型与自回归模型的优势，支持生成长度任意的视频，并通过KV缓存与并行采样提升了推理效率。然而，该方法仍面临两个长期存在的挑战：（i）由KV缓存引发的长时程误差累积问题；（ii）缺乏细粒度的长视频基准数据集以及针对连贯性评估的专用指标。为克服这些局限，本文提出BlockVid——一种新颖的块扩散框架。该框架配备了语义感知的稀疏KV缓存机制，采用名为“块强制训练”的高效训练策略，并结合专门设计的块级噪声调度与重排方法，以降低误差传播并增强时序一致性。我们进一步提出了LV-Bench，一个针对分钟级视频的细粒度基准数据集，其中包含评估长程连贯性的新指标。在VBench和LV-Bench上的大量实验表明，BlockVid在生成高质量、连贯的分钟级视频方面持续优于现有方法。具体而言，在LV-Bench评测中，相较于当前最优方法，BlockVid在VDE Subject指标上提升了22.2%，在VDE Clarity指标上提升了19.4%。项目网站：https://ziplab.co/BlockVid。代码仓库：https://github.com/alibaba-damo-academy/Inferix。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.22973) | [arXiv](https://arxiv.org/abs/2511.22973)



---

### 35. C^2DLM：基于因果概念引导的扩散大语言模型

**原文标题：** C^2DLM: Causal Concept-Guided Diffusion Large Language Models

**摘要：**
自回归语言模型与扩散语言模型构成了大语言模型的两大主流范式。然而，这两种范式均存在推理能力不足的问题。人类的推理本质上依赖于因果知识与思维，这些在自然语言中得以体现。但在自回归范式下，语言被建模为下一词元预测（严格的从左到右、逐词元顺序），而自然语言本身展现出更为灵活的因果结构。在扩散范式下，注意力机制采用全连接方式，完全忽略了因果顺序。为填补这一空白，本文提出一种**基于因果概念引导的扩散语言模型**。C^2DLM 从扩散模型的全连接注意力机制出发，首先从教师模型中获取概念级因果图，进而显式引导注意力学习概念间的因果关系。通过聚焦于因果关系并避免涉及因果反转的困难子目标干扰，C^2DLM 在 COT-OrderPerturb 任务中实现了 12% 的性能提升与约 3.2 倍的训练加速，并在六项下游推理任务中平均获得 1.31% 的性能增益。更多细节请参见代码仓库 ~https://github.com/Kairong-Han/C-2-DLM{此处}。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.22146) | [arXiv](https://arxiv.org/abs/2511.22146)



---

### 36. 超越描述：具身智能体细粒度动作的认知基准测试

**原文标题：** Beyond Description: Cognitively Benchmarking Fine-Grained Action for Embodied Agents

**摘要：**
多模态大语言模型作为在复杂物理环境中运行的具身智能体决策引擎展现出良好前景。然而，现有基准测试往往侧重于高层规划或空间推理，对具身物理交互所需的细粒度动作智能探索不足。为填补这一空白，我们提出CFG-Bench——一个旨在系统评估该关键能力的新型基准测试。该测试包含1,368个精选视频及与之匹配的19,562组三模态问答对，聚焦四大认知能力：1）物理交互，2）时序因果关联，3）意图理解，4）评估判断。这些维度共同构成了系统评估模型将视觉观察转化为可执行知识能力的框架，超越了表层识别范畴。我们在CFG-Bench上的综合评估表明：主流多模态大语言模型在生成物理交互的详细指令方面存在困难，在意图与评估的高阶推理上表现出明显局限。此外，基于本数据的监督微调实验证明，通过训练多模态大语言模型描述细粒度动作，能直接转化为现有具身基准测试上的显著性能提升。本研究的分析揭示了这些局限性，并为开发更具能力且接地气的具身智能体提供了重要洞见。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.18685) | [arXiv](https://arxiv.org/abs/2511.18685)



---

### 37. 面向人像视频编辑的上下文同步低秩适应方法

**原文标题：** In-Context Sync-LoRA for Portrait Video Editing

**摘要：**
人像视频编辑是一项具有挑战性的任务，需要对多种修改（如外观变化、表情调整或对象添加）实现灵活而精确的控制。其核心难点在于保持主体原有的时序行为，要求每一帧编辑后的画面都能与对应源帧保持精确同步。本文提出同步低秩适应方法，该人像视频编辑方法在实现高质量视觉修改的同时，能够保持帧级精确同步与身份一致性。我们的方法基于图像到视频扩散模型，通过修改首帧定义编辑内容，并将其传播至整个序列。为实现精确同步，我们利用描绘相同运动轨迹但外观各异的配对视频训练上下文低秩适应模块。这些配对数据通过基于同步性的筛选流程自动生成与筛选，仅选择时序对齐程度最高的样本用于训练。该训练机制使模型能够将源视频的运动线索与编辑首帧引入的视觉变化相结合。通过在紧凑且精心筛选的同步人像数据集上训练，该方法能够泛化至未见过的身份特征和多样化编辑任务（如修改外观、添加对象或更换背景），并能稳健处理姿态与表情变化。实验结果表明，该方法在视觉保真度和时序连贯性方面表现优异，实现了编辑保真度与精确运动保持之间的稳健平衡。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.03013) | [arXiv](https://arxiv.org/abs/2512.03013)



---

### 38. 基于VideoScience-Bench的视频生成科学理解与推理能力评估

**原文标题：** Benchmarking Scientific Understanding and Reasoning for Video Generation using VideoScience-Bench

**摘要：**
视频生成领域的下一个前沿在于开发具备零样本推理能力的模型，其中对现实世界科学定律的理解对于在不同条件下准确建模物理结果至关重要。然而，现有视频基准测试主要基于物理常识，对视频模型的科学推理能力评估有限。我们提出了VideoScience-Bench，这是一个专门用于评估视频模型对本科阶段科学概念理解能力的基准测试。每个提示都编码了一个复合科学场景，需要模型理解和推理多个科学概念才能生成正确的物理现象。该基准包含200个精心设计的提示，涵盖物理和化学领域的14个主题和103个概念。我们在文本到视频和图像到视频两种设置下，对七个前沿视频模型从五个维度进行了专家标注评估：提示一致性、现象符合性、动态正确性、不变性和时空连续性。通过使用视觉语言模型作为评判器评估视频生成结果，我们观察到其与人工评估结果具有强相关性。据我们所知，VideoScience-Bench是首个不仅将视频模型视为生成器，更将其作为推理器进行评估的基准测试，要求其生成内容展现出与预期物理化学现象一致的科学理解能力。我们的数据与评估代码已公开于：https://github.com/hao-ai-lab/VideoScience。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.02942) | [arXiv](https://arxiv.org/abs/2512.02942)



---

### 39. 理解与利用统一多模态模型中的稀疏性

**原文标题：** Understanding and Harnessing Sparsity in Unified Multimodal Models

**摘要：**
大型多模态模型在理解与生成任务上均取得了显著进展。近期研究致力于构建统一多模态模型，通过整合异构组件以在单一框架内同时支持这两种能力。然而，这种统一性会引入推理效率问题，例如特定任务或样本可能无需调用统一模型的全部知识或容量。目前，对于这些效率问题在不同组件中如何体现的系统性认识仍然有限。本研究首先采用免训练剪枝作为探测方法，从深度剪枝与宽度缩减两个维度，对统一多模态模型组件进行了系统性分析。研究发现：理解组件在理解与生成任务中均表现出显著的压缩潜力，且在生成任务中更为明显；相比之下，生成组件对压缩高度敏感，即使在中等压缩比下性能也会急剧下降。为应对这一局限，我们受不同样本间动态激活模式的启发，提出了专家混合适应方法。该方法将生成模块划分为多个专家，并通过稀疏激活机制以恢复生成质量。我们通过专家冻结微调验证了稀疏激活的有效性，并进一步证明完全可训练的适应策略能带来额外增益。实验表明，经过适应的BAGEL模型在仅激活约半数参数的情况下，达到了与完整模型相当的性能。代码已发布于https://github.com/Shwai-He/SparseUnifiedModel。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.02351) | [arXiv](https://arxiv.org/abs/2512.02351)



---

### 40. 视觉同步：基于跨视角物体运动的多相机同步方法

**原文标题：** Visual Sync: Multi-Camera Synchronization via Cross-View Object Motion

**摘要：**
如今，人们能够轻松使用多台消费级相机记录从音乐会、体育赛事、讲座、家庭聚会到生日派对等各类值得纪念的时刻。然而，对这些跨相机视频流进行同步仍然具有挑战性。现有方法通常依赖于受控环境、特定目标、人工校正或昂贵硬件。本文提出VisualSync——一种基于多视角动态特性的优化框架，能够以毫秒级精度对齐未标定、未同步的视频。我们的核心思路是：任意在三维空间中运动的点，若同时被两台相机观测到，在正确同步后将满足极几何约束。为实现这一目标，VisualSync利用现成的三维重建、特征匹配与密集跟踪技术，提取短轨迹、相对位姿及跨视角对应关系，进而通过联合最小化极线误差来估计各相机的时间偏移量。在四个多样化、高挑战性的数据集上的实验表明，VisualSync优于现有基准方法，其中位同步误差低于50毫秒。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.02017) | [arXiv](https://arxiv.org/abs/2512.02017)



---

### 41. Artemis：面向感知策略学习的结构化视觉推理框架

**原文标题：** Artemis: Structured Visual Reasoning for Perception Policy Learning

**摘要：**
当前基于强化学习的视觉感知策略框架开始引入以自然语言表达的中间推理链。实证研究表明，这种纯语言形式的中间推理往往会降低感知任务的性能。我们认为核心问题不在于推理本身，而在于推理形式：现有推理链在非结构化的语言空间中进行语义推理，而视觉感知需要在以物体为中心的空间结构中进行推理。为此，我们提出Artemis——一种基于结构化提案推理的感知策略学习框架，其中每个中间步骤均表示为可验证视觉状态的（标签，边界框）对。该设计实现了对中间状态的显式追踪、对提案质量的直接监督，并避免了语言推理引入的歧义性。Artemis基于Qwen2.5-VL-3B构建，在指代定位与检测任务中表现优异，并在计数与几何感知任务上展现出显著的泛化能力。这些多样化场景中一致的性能提升证实了空间表征对齐的推理能有效增强感知策略学习。得益于强化的视觉推理能力，Artemis在通用多模态大模型基准测试中也展现出竞争力，表明基于空间锚定的推理为构建可扩展、泛化性强的感知策略提供了理论可行的路径。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.01988) | [arXiv](https://arxiv.org/abs/2512.01988)



---

### 42. UnicEdit-10M：通过统一验证打破规模-质量壁垒的推理增强编辑数据集与基准

**原文标题：** UnicEdit-10M: A Dataset and Benchmark Breaking the Scale-Quality Barrier via Unified Verification for Reasoning-Enriched Edits

**摘要：**
随着GPT-4o、Nano Banana和Seedream 4.0等强大多模态模型在图像编辑领域的快速发展，闭源模型与开源模型之间的性能差距日益扩大，这主要源于缺乏大规模、高质量的训练数据以及能够全面诊断模型在不同编辑行为中弱点的综合性基准。现有数据构建方法面临规模与质量的权衡：人工标注质量高但难以扩展，而自动化流程则受限于错误传播和噪声问题。为解决这一挑战，我们提出一种轻量级数据构建流程，通过端到端模型和统一的后验证阶段替代多工具链。为实现可扩展的质量控制，我们训练了一个70亿参数的双任务专家模型Qwen-Verify，用于高效失败检测和指令重描述。该流程最终构建出UnicEdit-10M——一个涵盖多样化基础与复杂编辑任务的千万级规模数据集。同时，我们提出通用基准UnicBench，其评估范围超越基础编辑，可显式评估空间与知识驱动的推理能力。为支持细粒度诊断，我们引入包括非编辑一致性与推理准确度在内的新型评估指标。通过对主流模型在UnicBench上的系统性分析，我们揭示了现有模型的局限性，并为未来研究指明了明确方向。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.02790) | [arXiv](https://arxiv.org/abs/2512.02790)



---

### 43. 鞋型不变与地面感知的密集足部接触估计学习

**原文标题：** Shoe Style-Invariant and Ground-Aware Learning for Dense Foot Contact Estimation

**摘要：**
足部接触在人与世界的交互中起着关键作用，因此探索足部接触能够增进我们对人体运动与物理交互的理解。尽管其重要性显著，现有方法通常采用零速度约束来近似足部接触，并侧重于关节层面的接触估计，未能捕捉足部与世界之间的细节交互。足部接触的密集估计对于精确建模这种交互至关重要，然而从单张RGB图像预测密集足部接触的研究仍处于探索不足的状态。学习密集足部接触估计主要面临两大挑战：首先，鞋类外观高度多样化，导致模型难以在不同鞋型间泛化；其次，地面通常呈现单调外观，使得信息特征的提取变得困难。为解决这些问题，我们提出了一个足部接触估计框架，该框架通过鞋型不变与地面感知学习来实现密集足部接触估计。为应对鞋类外观多样性的挑战，我们的方法引入了鞋型对抗训练，以强制模型学习鞋型不变的特征用于接触估计。为有效利用地面信息，我们设计了一个地面特征提取器，基于空间上下文捕捉地面属性。实验表明，所提方法能够实现不受鞋类外观影响的鲁棒足部接触估计，并有效利用了地面信息。代码将公开发布。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.22184) | [arXiv](https://arxiv.org/abs/2511.22184)



---

### 44. 基于高效启发式辅助构造的奥数几何金牌级解题方法

**原文标题：** Gold-Medal-Level Olympiad Geometry Solving with Efficient Heuristic Auxiliary Constructions

**摘要：**
欧几里得几何的自动定理证明，特别是针对国际数学奥林匹克竞赛（IMO）级别的问题，仍然是人工智能领域的重大挑战和重要研究方向。本文提出一种高效的几何定理证明方法，该方法完全在CPU上运行，无需依赖基于神经网络的推理。我们的初步研究表明，采用简单的随机添加辅助点策略即可在IMO问题上达到银牌级别的人类解题水平。在此基础上，我们提出了HAGeo方法——一种基于启发式的几何推理辅助构造技术，该方法在IMO-30基准测试的30道题目中成功解答28道，达到金牌级解题水平，并以显著优势超越了基于神经网络的竞争性方法AlphaGeometry。为了更全面地评估本方法及现有技术，我们进一步构建了HAGeo-409基准测试集，该数据集包含409道经过人工难度评级的几何问题。与广泛使用的IMO-30相比，我们的基准测试集提出了更大挑战，提供了更精确的评估标准，为几何定理证明领域设立了更高的技术门槛。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.00097) | [arXiv](https://arxiv.org/abs/2512.00097)



---

### 45. 掩码可能成为干扰：论扩散语言模型中的上下文理解能力

**原文标题：** Masks Can Be Distracting: On Context Comprehension in Diffusion Language Models

**摘要：**
掩码扩散语言模型（MDLMs）近期作为自回归语言模型（ARLMs）的一种有前景的替代方案出现，其利用去噪目标函数，在理论上应能实现更均衡的上下文利用。本研究系统考察了MDLMs的上下文理解能力，并揭示了两项关键局限。首先，尽管MDLMs采用更具全局性的训练目标和双向注意力机制，但其与ARLMs相似地表现出强烈的局部性偏好：模型性能对输入中相关信息的位置高度敏感，更倾向于利用局部上下文而非远距离上下文。其次，我们发现生成所需的大量掩码标记会显著削弱模型的上下文理解能力。通过系统性消融实验，这些掩码被证实具有干扰作用，会降低模型处理相关信息的能力。为应对此问题，我们提出了一种掩码无关的损失函数，该函数促使模型预测结果对附加掩码数量保持稳定。基于此目标函数的微调显著缓解了掩码的干扰效应，提升了MDLMs的鲁棒性。总体而言，我们的研究揭示了当前MDLM训练范式的关键局限，并为构建具有更强上下文理解能力的扩散式语言模型提供了可操作的改进方向。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.21338) | [arXiv](https://arxiv.org/abs/2511.21338)



---

### 46. CodeV：基于工具感知策略优化的可信视觉推理图像编码方法

**原文标题：** CodeV: Code with Images for Faithful Visual Reasoning via Tool-Aware Policy Optimization

**摘要：**
具身视觉语言模型正日益通过调用图像操作来实现“基于图像的思考”。然而，我们发现最终答案的高准确率往往掩盖了不可信的视觉推理过程：模型可能在无关区域调用工具，或完全忽略工具输出，却仍能猜测出正确答案。本研究首先提出一种可信度评估协议，用于衡量中间视觉工具输出（如图像裁剪区域）是否实际包含查询所需的证据。分析表明，当前主流视觉代理模型虽然在最终答案准确率上表现优异，但在视觉搜索基准测试中展现出较低的可信工具使用率。为此，我们提出CodeV——一种基于代码的视觉代理模型，采用工具感知策略优化方法进行训练。该方法是一种过程级强化学习框架，在GRPO基础上引入直接针对视觉工具输入输出的密集奖励机制（而非思维链标记），使监督验证更易实施且能有效规避奖励操纵问题。CodeV将视觉工具定义为可执行的Python代码，其优化框架仅依据问题与工具输出分配逐步奖励，从而促进必要且符合证据一致性的工具使用。通过两阶段监督微调与强化学习训练流程，CodeV在相关视觉搜索基准测试中不仅取得具有竞争力的准确率，更显著提升了可信工具使用率。除视觉搜索任务外，CodeV在多项多模态推理与数学基准测试中均表现出色，这表明对中间工具行为的显式监督对于构建可信赖的具身视觉推理系统具有关键意义。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.19661) | [arXiv](https://arxiv.org/abs/2511.19661)



---

### 47. BOOM：超越单一模态——KIT多模态多语言讲座伴侣系统

**原文标题：** BOOM: Beyond Only One Modality KIT's Multimodal Multilingual Lecture Companion

**摘要：**
教育全球化与在线学习的快速发展使得教育内容本地化成为关键挑战。讲座材料本质上是多模态的，结合了语音音频与视觉幻灯片，这要求系统具备处理多种输入模态的能力。为提供完整且无障碍的学习体验，翻译必须保留所有模态特性：用于阅读的文本、用于视觉理解的幻灯片以及用于听觉学习的语音。本文提出BOOM多模态多语言讲座伴侣系统，通过联合翻译讲座音频与幻灯片，生成跨三种模态的同步输出：翻译文本、保留视觉元素的本地化幻灯片以及合成语音。这种端到端的解决方案使学生能够以母语访问讲座内容，同时力求完整保留原始信息。实验表明，融合幻灯片信息的转录文本还能为摘要生成和问答等下游任务带来级联效益。我们在https://github.com/saikoneru/image-translator发布了幻灯片翻译代码，并将其集成至讲座翻译系统（https://gitlab.kit.edu/kit/isl-ai4lt/lt-middleware/ltpipeline）。所有公开代码与模型均遵循MIT开源许可协议。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.02817) | [arXiv](https://arxiv.org/abs/2512.02817)



---

### 48. Click2Graph：基于单次点击的交互式全景视频场景图生成

**原文标题：** Click2Graph: Interactive Panoptic Video Scene Graphs from a Single Click

**摘要：**
当前最先进的视频场景图生成系统虽能提供结构化的视觉理解，但其作为封闭的前馈式流程运行，无法融入人工引导。相比之下，SAM2等可提示分割模型支持精确的用户交互，却缺乏语义或关系推理能力。本文提出Click2Graph，这是首个面向全景视频场景图生成的交互式框架，它将视觉提示与空间、时序及语义理解相融合。该系统仅需用户提供单次点击或边界框等简单线索，即可跨时序分割并跟踪目标主体，自主发现交互对象，进而预测〈主体，对象，谓词〉三元组以构建时序一致的场景图。本框架包含两个核心组件：动态交互发现模块（用于生成基于主体条件的对象提示）和语义分类头（用于执行实体与谓词的联合推理）。在OpenPVSG基准测试上的实验表明，Click2Graph为用户引导的全景视频场景图生成奠定了坚实基础，展示了如何将人工提示与全景定位及关系推理相结合，从而实现可控且可解释的视频场景理解。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.15948) | [arXiv](https://arxiv.org/abs/2511.15948)



---


## 🔍 关键词云图
![关键词云图](../images/keywords_wordcloud.png)

## 📈 近期论文趋势
![论文趋势](../images/daily_papers.png)

## 🎙️ 语音播报
- [收听今日论文解读](../audio/2025-12-03_daily_papers.mp3)

## 📱 订阅渠道
- GitHub: [hf-daily-paper-newsletter-chinese](https://github.com/2404589803/hf-daily-paper-newsletter-chinese)