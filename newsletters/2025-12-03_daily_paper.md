
# <img src="https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo.png" width="30"/> Hugging Face 2025-12-03 论文日报

## 📊 今日论文统计
- 总论文数：48
- 热门领域：GPT, RL, Audio, LLM, Transformer

## 📝 论文详情


### 1. DeepSeek-V3.2：开拓开源大语言模型新前沿

**原文标题：** DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models

**摘要：**
本文介绍DeepSeek-V3.2模型，该模型实现了高计算效率与卓越推理及智能体性能的协同优化。DeepSeek-V3.2的核心技术突破包括：（1）深度稀疏注意力机制：我们提出一种高效注意力机制，在长上下文场景中显著降低计算复杂度的同时保持模型性能。（2）可扩展强化学习框架：通过实施鲁棒的强化学习协议并扩展训练后计算规模，DeepSeek-V3.2达到与GPT-5相当的性能。特别值得注意的是，我们的高计算变体DeepSeek-V3.2-Speciale在多项指标上超越GPT-5，其推理能力与Gemini-3.0-Pro持平，并在2025年国际数学奥林匹克竞赛和国际信息学奥林匹克竞赛中均获得金牌级表现。（3）大规模智能体任务合成流程：为将推理能力融入工具使用场景，我们开发了创新的合成流程，可系统化生成大规模训练数据。该方法实现了可扩展的智能体训练后优化，在复杂交互环境中显著提升了泛化能力与指令遵循的鲁棒性。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.02556) | [arXiv](https://arxiv.org/abs/2512.02556)



---

### 2. ToolOrchestra：通过高效模型与工具编排提升智能水平

**原文标题：** ToolOrchestra: Elevating Intelligence via Efficient Model and Tool Orchestration

**摘要：**
大型语言模型是强大的通用型系统，但在解决如“人类终极考试”（HLE）这类深刻而复杂的问题时，仍面临概念上的挑战和高昂的计算成本。本文研究表明，通过小型编排器管理其他模型及多样化工具，既能提升智能水平的上限，也能提高解决复杂智能体任务的效率。我们提出了ToolOrchestra方法，用于训练能够协调智能工具的小型编排器。该方法明确采用强化学习，并结合了结果感知、效率感知和用户偏好感知的奖励机制。基于ToolOrchestra，我们训练出Orchestrator模型（参数量80亿），该模型在较低成本下取得了比以往工具使用智能体更高的准确率，并能根据用户偏好为给定查询选择适用工具。在HLE测试中，Orchestrator取得了37.1%的得分，优于GPT-5（35.1%），同时效率提升了2.5倍。在tau2-Bench和FRAMES基准测试中，Orchestrator以显著优势超越GPT-5，而成本仅为其约30%。深入分析表明，Orchestrator在多项指标下实现了性能与成本的最佳平衡，并对未见工具展现出强大的泛化能力。这些结果证明，通过轻量级编排模型整合多样化工具，比现有方法更高效、更有效，为实用且可扩展的工具增强推理系统开辟了道路。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.21689) | [arXiv](https://arxiv.org/abs/2511.21689)



---

### 3. MultiShotMaster：一种可控的多镜头视频生成框架

**原文标题：** MultiShotMaster: A Controllable Multi-Shot Video Generation Framework

**摘要：**
当前的视频生成技术擅长生成单镜头片段，但在生成叙事性多镜头视频方面存在困难，因为后者需要灵活的镜头编排、连贯的叙事以及超越文本提示的可控性。为应对这些挑战，我们提出了MultiShotMaster，这是一个用于高度可控的多镜头视频生成的框架。我们通过集成两种新颖的RoPE变体，扩展了一个预训练的单镜头模型。首先，我们引入了多镜头叙事RoPE，它在镜头转换处应用显式的相位偏移，从而在保持时间叙事顺序的同时实现灵活的镜头编排。其次，我们设计了时空位置感知RoPE，以融入参考标记和接地信号，实现基于时空的参考信息注入。此外，为克服数据稀缺问题，我们建立了一个自动化数据标注流程，用于提取多镜头视频、描述文本、跨镜头接地信号以及参考图像。我们的框架利用其固有的架构特性来支持多镜头视频生成，具备文本驱动的镜头间一致性、支持运动控制的定制主体以及背景驱动的定制场景等特征。镜头数量和时长均可灵活配置。大量实验证明了我们框架的卓越性能和出色的可控性。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.03041) | [arXiv](https://arxiv.org/abs/2512.03041)



---

### 4. MG-Nav：基于稀疏空间记忆的双尺度视觉导航框架

**原文标题：** MG-Nav: Dual-Scale Visual Navigation via Sparse Spatial Memory

**摘要：**
本文提出MG-Nav（记忆引导导航），一种用于零样本视觉导航的双尺度框架，该框架将全局记忆引导规划与局部几何增强控制相统一。其核心是稀疏空间记忆图（SMG），这是一种以区域为中心的紧凑记忆结构，其中每个节点聚合多视角关键帧与物体语义信息，在保持视角多样性的同时捕获外观与空间结构。在全局层面，智能体通过SMG进行定位，并基于图像-实例混合检索机制规划目标导向的节点路径，生成一系列可达航点以实现长时程导航引导。在局部层面，导航基础策略以点目标模式执行这些航点，并采用障碍物感知控制；当从最终节点向视觉目标导航时，系统切换至图像目标模式。为进一步增强视角对齐与目标识别能力，我们提出VGGT适配器——一个基于预训练VGGT模型构建的轻量化几何模块，可在共享三维感知空间中对齐观测特征与目标特征。MG-Nav以不同频率执行全局规划与局部控制，并通过周期性重定位修正误差。在HM3D实例-图像-目标与MP3D图像-目标基准测试上的实验表明，MG-Nav实现了最先进的零样本性能，并在动态场景重组与未知场景条件下保持鲁棒性。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.22609) | [arXiv](https://arxiv.org/abs/2511.22609)



---

### 5. Skywork-R1V4：通过图像与深度研究的交错思考迈向具身多模态智能

**原文标题：** Skywork-R1V4: Toward Agentic Multimodal Intelligence through Interleaved Thinking with Images and DeepResearch

**摘要：**
尽管多模态具身系统近期取得进展，但现有方法常将图像处理与网络搜索视为分离能力，严重依赖高成本的强化学习，且缺乏基于真实工具执行轨迹的规划。为应对这些局限，我们提出Skywork-R1V4——一个拥有300亿（实际激活30亿）参数的多模态具身模型。该模型统一了多模态规划、主动图像处理（“图像思考”）、深度多模态搜索，以及最关键的在视觉操作与外部知识检索间动态交替的交错推理能力。通过仅对不足3万条高质量、规划-执行一致的轨迹进行监督微调训练，并经过逐步一致性过滤验证，Skywork-R1V4在感知与多模态搜索基准测试中取得领先性能：在MMSearch上获得66.1分，在FVQA上获得67.2分，全部11项指标均超越Gemini 2.5 Flash模型。该模型在推理时展现出新兴的长程推理能力，可成功协调超过10次工具调用来解决复杂多步骤任务。我们的结果表明，无需依赖强化学习，仅通过精心构建的监督学习即可实现高度复杂的具身多模态智能。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.02395) | [arXiv](https://arxiv.org/abs/2512.02395)



---

### 6. DualCamCtrl：用于几何感知相机控制视频生成的双分支扩散模型

**原文标题：** DualCamCtrl: Dual-Branch Diffusion Model for Geometry-Aware Camera-Controlled Video Generation

**摘要：**
本文提出DualCamCtrl，一种用于相机控制视频生成的新型端到端扩散模型。近期研究通过将相机姿态表示为基于光线的条件推动了该领域进展，但这些方法往往缺乏充分的场景理解与几何感知能力。DualCamCtrl针对这一局限性，引入能够协同生成相机一致性的RGB序列与深度序列的双分支框架。为协调这两种模态，我们进一步提出语义引导互对齐机制，以语义引导、相互增强的方式实现RGB-深度融合。这些设计使DualCamCtrl能更好解耦外观与几何建模，生成更精准遵循指定相机轨迹的视频。此外，我们分析揭示了深度信息与相机姿态在去噪各阶段的差异化影响，并论证早期与后期阶段在构建全局结构与细化局部细节方面具有互补作用。大量实验表明，DualCamCtrl实现了更一致的相机控制视频生成，相机运动误差较现有方法降低超过40%。项目页面：https://soyouthinkyoucantell.github.io/dualcamctrl-page/

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.23127) | [arXiv](https://arxiv.org/abs/2511.23127)



---

### 7. 基于最小化人工监督的引导式自演化大语言模型

**原文标题：** Guided Self-Evolving LLMs with Minimal Human Supervision

**摘要：**
人工智能的自演化长期以来被视为通往超智能的路径，即模型能够从自身学习经验中自主获取、优化并内化知识。然而在实践中，无引导的自演化系统往往在训练过程中快速陷入停滞甚至性能衰退。这类失败源于概念漂移、多样性坍缩和错误演化等问题——模型会不断强化自身偏见并收敛至低熵行为。为实现模型在稳定可控的前提下进行自演化，同时最大限度减少对人类监督的依赖，本文提出R-Few框架：一种融合轻量化人工监督的引导式自我博弈挑战者-求解器架构。该框架通过情境锚定与混合训练机制实现人类监督的嵌入。在每轮迭代中，挑战者模块采样少量人工标注示例以引导合成问题生成，而求解器模块则依据在线难度分级课程，对人工示例与合成示例进行联合训练。在数学与通用推理基准测试中，R-Few实现了持续迭代的性能提升。以Qwen3-8B-Base模型为例，在数学任务上较R-Zero提升3.0个点，其表现与通用推理模型持平，而后者训练所用的人工标注数据量是前者的20倍。消融实验证实了锚定式挑战者训练与课程化求解器训练的互补效应，进一步分析表明R-Few能有效缓解概念漂移，产生更稳定可控的协同演化动态。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.02472) | [arXiv](https://arxiv.org/abs/2512.02472)



---

### 8. SimScale：基于大规模真实世界模拟的驾驶学习

**原文标题：** SimScale: Learning to Drive via Real-World Simulation at Scale

**摘要：**
实现完全自动驾驶系统需要在广泛场景（包括安全关键场景与分布外场景）中学习理性决策。然而，人类专家采集的真实世界数据集中此类场景的代表性不足。为弥补数据多样性的缺失，我们提出一种新颖且可扩展的模拟框架，能够在现有驾驶日志基础上合成海量未见状态。该框架利用先进神经渲染技术与反应式环境，生成由扰动自车轨迹控制的高保真多视角观测数据。此外，我们为这些新模拟状态开发了伪专家轨迹生成机制以提供动作监督。基于合成数据，我们发现对真实世界样本与模拟样本采用简单的协同训练策略，可使多种规划方法在具有挑战性的真实世界基准测试中的鲁棒性与泛化能力显著提升——在navhard基准上最高提升+6.8 EPDMS，在navtest基准上提升+2.9。更重要的是，即使没有额外真实世界数据流输入，仅通过增加模拟数据即可实现策略性能的平稳提升。我们进一步揭示了此类模拟-真实学习系统（命名为SimScale）的若干关键发现，包括伪专家机制的设计方案以及不同策略架构的扩展特性。我们的模拟数据与代码将予以公开。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.23369) | [arXiv](https://arxiv.org/abs/2511.23369)



---

### 9. InnoGym：人工智能代理创新潜能的基准测试框架

**原文标题：** InnoGym: Benchmarking the Innovation Potential of AI Agents

**摘要：**
大语言模型与智能代理在代码生成、数学推理及科学发现领域已取得显著进展。然而，现有基准测试主要关注结果正确性，忽视了解决方案背后方法的多样性。真正的创新不仅要求答案正确，更取决于方法的原创性。本文提出InnoGym——首个系统性评估人工智能代理创新潜能的基准测试框架。该框架引入两项互补指标：性能增益（衡量对已知最优方案的改进程度）与新颖性（量化与既有方法论的差异度）。基准测试涵盖从现实工程与科学领域中精选的18项任务，每项任务均通过资源筛选、评估验证和解决方案收集实现标准化。此外，我们同步推出iGym统一执行环境，支持可复现的长周期评估。大量实验表明，尽管部分代理能提出新颖方法，但其鲁棒性不足限制了性能提升。这些结果揭示了创造力与有效性之间的关键差距，凸显了需要同时评估两者的基准测试体系的重要性。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.01822) | [arXiv](https://arxiv.org/abs/2512.01822)



---

### 10. ViSAudio：端到端视频驱动的双耳空间音频生成

**原文标题：** ViSAudio: End-to-End Video-Driven Binaural Spatial Audio Generation

**摘要：**
尽管视频到音频生成领域已取得进展，但现有研究主要集中于单声道输出，缺乏空间沉浸感。现有的双耳音频生成方法仍受限于两阶段流程，即首先生成单声道音频再进行空间化处理，这往往导致误差累积和时空不一致性问题。为突破这一局限，我们提出了从无声视频直接端到端生成双耳空间音频的任务。为支持该任务，我们构建了BiAudio数据集，包含约9.7万个视频-双耳音频对，涵盖多样化的真实场景与摄像机旋转轨迹，并通过半自动化流程构建。进一步，我们提出ViSAudio端到端框架，采用条件流匹配与双分支音频生成架构，其中两个独立分支分别建模音频潜在流。该框架结合条件时空模块，在保持声道间一致性的同时保留独特的空间特征，确保音频与输入视频的精确时空对齐。综合实验表明，ViSAudio在客观指标和主观评估上均优于现有先进方法，能够生成具有空间沉浸感的高质量双耳音频，并能有效适应视角变化、声源运动及多样声学环境。项目网站：https://kszpxxzmc.github.io/ViSAudio-project。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.03036) | [arXiv](https://arxiv.org/abs/2512.03036)



---

### 11. Glance：单样本加速扩散模型

**原文标题：** Glance: Accelerating Diffusion Models with 1 Sample

**摘要：**
扩散模型在图像生成领域取得了显著成功，但其部署仍受限于高昂的计算成本和大量推理步骤的需求。先前关于减少步数的蒸馏方法试图通过训练紧凑的学生模型来跳过冗余步骤，但这些方法通常面临繁重的重新训练成本与泛化性能下降的问题。在本研究中，我们采取了一种不同的视角：我们进行智能而非均匀的加速，对早期语义阶段施加较小的加速，而对后期冗余阶段施加较大的加速。我们通过两个分别专注于慢速和快速去噪阶段的专家模型来实现这一阶段感知策略。令人惊讶的是，我们并未投入大量精力重新训练学生模型，而是发现仅需为基础模型配备轻量级的LoRA适配器，即可同时实现高效加速与强大的泛化能力。我们将这两种适配器称为Slow-LoRA与Fast-LoRA。通过大量实验验证，我们的方法在保持多种基准测试中视觉质量相当的前提下，相比基础模型实现了最高5倍的加速。值得注意的是，LoRA专家模型仅需在单个V100显卡上使用1个样本训练一小时，所得模型在未见过的提示词上仍展现出强大的泛化性能。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.02899) | [arXiv](https://arxiv.org/abs/2512.02899)



---

### 12. WorldMM：面向长视频推理的动态多模态记忆智能体

**原文标题：** WorldMM: Dynamic Multimodal Memory Agent for Long Video Reasoning

**摘要：**
近期视频大语言模型的发展在短视频理解方面展现出强大能力。然而，由于上下文容量有限以及在抽象过程中关键视觉细节的丢失，将其扩展至数小时甚至数天时长的视频仍面临巨大挑战。现有基于记忆增强的方法通过利用视频片段的文本摘要缓解了这一问题，但这些方法严重依赖文本，在复杂场景推理时未能有效利用视觉证据。此外，固定时间尺度的检索机制进一步限制了其捕捉可变时长事件的灵活性。为此，我们提出WorldMM——一种新颖的多模态记忆智能体，它能够构建并检索包含文本与视觉表征的多种互补记忆。WorldMM包含三类记忆：跨多时间尺度索引事实事件的**情景记忆**、持续更新高层概念知识的**语义记忆**，以及保留场景细节信息的**视觉记忆**。在推理过程中，自适应检索智能体会根据查询内容迭代选择最相关的记忆源，并利用多时间粒度进行检索，直至确定已收集足够信息。在五个长视频问答基准测试中，WorldMM显著优于现有基线方法，较先前最优方法的平均性能提升达8.4%，充分证明了其在长视频推理任务中的有效性。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.02425) | [arXiv](https://arxiv.org/abs/2512.02425)



---

### 13. 深度研究：一项系统性综述

**原文标题：** Deep Research: A Systematic Survey

**摘要：**
大型语言模型已从文本生成工具迅速发展为强大的问题解决者。然而，许多开放性任务需要批判性思维、多源信息与可验证的输出，这超出了单次提示或标准检索增强生成的能力范围。近年来，众多研究开始探索深度研究范式，其目标是将大型语言模型的推理能力与搜索引擎等外部工具相结合，从而使大型语言模型能够作为研究智能体完成复杂、开放式的任务。本综述对深度研究系统进行了全面而系统的梳理，包括清晰的发展路线图、基础构成要素、实际实现技术、重要挑战与未来方向。具体而言，我们的主要贡献如下：（一）提出三阶段发展路线图，明确区分深度研究与相关范式；（二）系统阐述四大核心组件：查询规划、信息获取、记忆管理与答案生成，并为每个组件建立细粒度分类体系；（三）总结提示工程、监督微调与智能体强化学习等优化技术；（四）整合评估标准与开放挑战，旨在引导和促进该领域的未来发展。随着深度研究领域的持续快速演进，我们将持续更新本综述以反映该领域的最新进展。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.02038) | [arXiv](https://arxiv.org/abs/2512.02038)



---

### 14. PixelDiT：用于图像生成的像素扩散变换器

**原文标题：** PixelDiT: Pixel Diffusion Transformers for Image Generation

**摘要：**
潜空间建模一直是扩散变换器（DiTs）的标准范式。然而，该方法依赖于两阶段流程，其中预训练的自编码器会引入有损重构，导致误差累积并阻碍联合优化。为解决这些问题，我们提出PixelDiT——一种单阶段端到端模型，无需依赖自编码器，直接在像素空间中学习扩散过程。PixelDiT采用全变换器架构，其设计包含双重层级：捕捉全局语义的块级DiT与细化纹理细节的像素级DiT，在保持精细细节的同时实现了像素空间扩散模型的高效训练。我们的分析表明，有效的像素级令牌建模是像素扩散成功的关键。PixelDiT在ImageNet 256×256数据集上取得了1.61的FID分数，大幅超越现有像素生成模型。我们进一步将PixelDiT扩展至文本到图像生成任务，并在像素空间中以1024×1024分辨率进行预训练。该模型在GenEval评测中达到0.74分，在DPG-bench中取得83.5分，性能已接近最优潜扩散模型。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.20645) | [arXiv](https://arxiv.org/abs/2511.20645)



---

### 15. 动作分块中的混合视界策略

**原文标题：** Mixture of Horizons in Action Chunking

**摘要：**
视觉-语言-动作模型在机器人操控任务中展现出卓越能力，但其性能对训练时采用的动作块长度（称为视界）极为敏感。我们的实证研究揭示了一个内在权衡：较长视界能提供更强的全局预见性，但会降低细粒度动作精度；较短视界虽能提升局部控制精度，却在长期任务中表现欠佳，这意味着固定单一视界的选择具有次优性。为缓解这一矛盾，我们提出混合视界策略。该策略将动作块重组为具有不同视界的多个片段，通过共享动作变换器进行并行处理，并利用轻量级线性门融合输出结果。该方法具有三大优势：1）在单一模型内协同利用长期预见性与短期精确性，提升复杂任务中的性能表现与泛化能力；2）可作为即插即用模块适配全注意力动作架构，仅引入极小的训练与推理开销；3）支持自适应视界的动态推理机制，通过跨视界一致性筛选稳定动作，在保持卓越性能的同时实现比基线方法高2.5倍的吞吐量。基于流式策略π_0、π_0.5及单步回归策略π_reg的广泛实验表明，混合视界策略在仿真与真实场景任务中均能带来持续显著的性能提升。值得注意的是，在混合任务设定下，采用混合视界的π_0.5策略仅通过3万次训练迭代即在LIBERO基准上达到99%的平均成功率，创造了新的性能纪录。项目页面：https://github.com/Timsty1/MixtureOfHorizons

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.19433) | [arXiv](https://arxiv.org/abs/2511.19433)



---

### 16. WUSH：面向大语言模型量化的近最优自适应变换方法

**原文标题：** WUSH: Near-Optimal Adaptive Transforms for LLM Quantization

**摘要：**
低比特量化是部署大语言模型的常用方法，但少数极端权重和激活值会扩大动态范围，降低量化器的有效分辨率。常见的缓解方法是在量化前应用固定正交变换（如哈达玛矩阵），这通常能压缩动态范围。然而，此类变换忽略了数据统计特性，其最优性尚未得到充分论证。本研究首次推导出针对通用数值格式、采用无数据标准量化器的权重-激活联合量化的闭式最优线性分块变换。具体而言，我们推导了适用于整数与浮点格式的最近舍入（RTN）和绝对值最大分块量化器的最优自适应（数据感知）变换。所得构建方法命名为WUSH，其将哈达玛矩阵主干与基于二阶矩的数据依赖组件相结合，形成一种在温和假设下可证明最优的非正交变换，同时保持结构化特性以实现高效计算。初步实验结果表明，该方法在多种常用数值格式下均能稳定超越哈达玛变换的性能。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.00956) | [arXiv](https://arxiv.org/abs/2512.00956)



---

### 17. GoRL：一种与算法无关的生成策略在线强化学习框架

**原文标题：** GoRL: An Algorithm-Agnostic Framework for Online Reinforcement Learning with Generative Policies

**摘要：**
强化学习面临一个长期存在的矛盾：易于优化的策略往往过于简单，难以表达复杂控制所需的多模态动作分布。高斯策略提供了易于处理的似然函数和平滑梯度，但其单模态形式限制了表达能力。相反，基于扩散或流匹配的生成策略能够建模丰富的多模态行为；然而，在在线强化学习中，由于似然函数难以计算且梯度在深度采样链中传播时存在噪声，这类策略常常表现出不稳定性。我们通过一个关键的结构性原则来解决这一矛盾：将优化过程与生成过程解耦。基于这一思路，我们提出了GoRL（生成式在线强化学习）框架，该框架通过优化一个易于处理的潜变量策略，同时利用条件生成解码器来合成动作。采用双时间尺度更新机制，使得潜变量策略能够稳定学习，而解码器则逐步提升表达能力，且无需计算动作的易处理似然函数。在一系列连续控制任务中，GoRL的表现始终优于高斯策略及近期提出的生成策略基线方法。值得注意的是，在HopperStand任务中，其归一化回报超过870，达到最强基线方法的三倍以上。这些结果表明，将优化与生成分离为实现既稳定又具有高表达能力的策略提供了一条可行路径。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.02581) | [arXiv](https://arxiv.org/abs/2512.02581)



---

### 18. CUDA-L2：通过强化学习超越cuBLAS性能的矩阵乘法优化系统

**原文标题：** CUDA-L2: Surpassing cuBLAS Performance for Matrix Multiplication through Reinforcement Learning

**摘要：**
本文提出CUDA-L2系统，该系统结合大型语言模型（LLM）与强化学习（RL），实现半精度通用矩阵乘法（HGEMM）CUDA核函数的自动优化。以CUDA执行速度为强化学习奖励，CUDA-L2在1,000种配置中自动优化HGEMM核函数。实验表明，CUDA-L2系统性地超越了当前主流矩阵乘法基准方案：从广泛使用的{\it torch.matmul}到英伟达最新的闭源库（包括{\it cuBLAS}和{\it cuBLASLt}）。在离线模式下（核函数连续执行无时间间隔），CUDA-L2相比{\it torch.matmul}平均提升22.0%；相比采用最优布局配置（正常-正常NN与转置-正常TN）的{\it cuBLAS}提升19.2%；相比基于启发式建议从{\it cuBLASLt}库选择算法的{\it cuBLASLt-heuristic}提升16.8%；相比最具竞争力的{\it cuBLASLt-AutoTuning}模型（从{\it cuBLASLt}提供的多达100个候选算法中选择最优方案）提升11.4%。在模拟实时推理的服务器模式下（核函数随机间隔执行），加速效果进一步提升：相比{\it torch.matmul}、{\it cuBLAS}、{\it cuBLASLt-heuristic}和{\it cuBLASLt-AutoTuning}分别达到28.7%、26.0%、22.4%和15.9%的性能增益。CUDA-L2证明，即使是HGEMM这类高度优化、对性能极其敏感的核函数，也能通过LLM引导的强化学习自动化实现性能突破——该系统以人力难以企及的规模系统探索配置空间。项目与代码详见github.com/deepreinforce-ai/CUDA-L2。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.02551) | [arXiv](https://arxiv.org/abs/2512.02551)



---

### 19. 听觉是否有助于视觉？视频生成中音视频联合去噪机制研究

**原文标题：** Does Hearing Help Seeing? Investigating Audio-Video Joint Denoising for Video Generation

**摘要：**
近期音视频生成系统的研究表明，多模态耦合不仅有助于提升音视频同步性，还能改善视频模态本身的生成质量。我们提出一个基础性问题：即使仅关注视频质量，音视频联合去噪训练是否能提升视频生成效果？为探究此问题，我们设计了一种参数高效的音视频全扩散变换器（AVFullDiT）架构，该架构利用预训练的文本到视频（T2V）与文本到音频（T2A）模块进行联合去噪训练。我们在相同实验设置下分别训练了：（1）采用AVFullDiT的T2AV模型；（2）仅使用视频模态的对照模型。实验结果首次系统性地证明，音视频联合去噪训练能带来超越同步性提升的额外增益。在包含大幅运动与物体接触动作的挑战性数据子集上，我们观察到模型性能的持续改善。我们假设音频预测作为一种特权信号，能够促使模型内化视觉事件与其声学后果之间的因果关系（例如碰撞时机对声音的影响），从而对视频动态特性产生正则化作用。本研究结果表明，跨模态协同训练是构建更强大、更符合物理规律的世界模型的有效途径。代码与数据集将公开提供。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.02457) | [arXiv](https://arxiv.org/abs/2512.02457)



---

### 20. 类比推理的奇特案例：探究大语言模型中的类比推理机制

**原文标题：** The Curious Case of Analogies: Investigating Analogical Reasoning in Large Language Models

**摘要：**
类比推理是人类认知的核心，为多种智力活动提供了重要基础。尽管已有研究表明大语言模型能够表征任务模式与表层概念，但这些模型能否编码高层次关系概念并通过结构化比较将其应用于新情境仍不明确。本研究通过比例类比与叙事类比探究这一基础问题，并发现三个关键结论：首先，大语言模型能有效编码类比实体间的深层关系——在正确推理案例中，属性信息与关系信息均通过中高层网络传播；而推理失败案例则反映出这些层级中关系信息的缺失。其次，与人类不同，大语言模型不仅在关系信息缺失时表现困难，在尝试将已有关系应用于新实体时也常遇障碍。在此类情境中，对关键标记位置的隐层表征进行策略性修补可在一定程度上促进信息迁移。最后，成功的类比推理以类比情境间强烈的结构对齐为特征，而推理失败往往伴随对齐结构的退化或错位。总体而言，本研究表明大语言模型在编码与应用高层次关系概念方面呈现出初现但有限的能力，既揭示了其与人类认知的共通之处，也凸显了二者间的显著差距。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.20344) | [arXiv](https://arxiv.org/abs/2511.20344)



---

### 21. DiG-Flow：基于差异引导流匹配的鲁棒视觉-语言-动作模型

**原文标题：** DiG-Flow: Discrepancy-Guided Flow Matching for Robust VLA Models

**摘要：**
通过流匹配训练的视觉-语言-动作模型在机器人操作任务中展现出卓越的性能。然而，其表现常在分布偏移和复杂多步任务中出现退化，这表明所学表征可能未能稳健地捕捉任务相关语义。本文提出DiG-Flow，一个通过几何正则化增强VLA模型鲁棒性的原理性框架。我们的核心洞见在于：观测与动作嵌入之间的分布差异可提供有意义的几何信号——较低的传输成本表征兼容性，而较高成本则暗示潜在错位。DiG-Flow通过计算观测与动作嵌入经验分布间的差异度量，经单调函数映射为调制权重，进而在流匹配前对观测嵌入施加残差更新。关键的是，这种干预作用于表征层面，无需修改流匹配路径或目标向量场。我们提供了理论证明：差异引导训练可严格降低训练目标函数，且引导推理优化过程具有收缩收敛性。实证研究表明，DiG-Flow能以可忽略的开销集成至现有VLA架构，持续提升模型性能，在复杂多步任务及有限训练数据场景下增益尤为显著。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.01715) | [arXiv](https://arxiv.org/abs/2512.01715)



---

### 22. RULER-Bench：面向视觉基础智能的下一代视频生成模型规则推理能力评测基准

**原文标题：** RULER-Bench: Probing Rule-based Reasoning Abilities of Next-level Video Generation Models for Vision Foundation Intelligence

**摘要：**
近期视频生成技术的进展使得合成视频在时间连贯性和视觉质量方面表现突出，标志着向视觉基础模型迈出了关键一步。为评估这些视频生成模型，现有基准主要关注与视觉感知和理解相关的因素，如视觉美感、指令遵循性和时间一致性。然而，视频生成模型的规则推理能力在很大程度上尚未得到充分探索。尽管近期研究对视频模型能否作为零样本学习者进行了初步探索，但仍缺乏对推理能力的细粒度分解和全面的评估方案。为填补这一空白，我们提出了RULER-Bench，这是一个旨在从认知规则角度评估视频生成模型推理能力的基准。基于文本到视频和图像到视频两种基本范式，RULER-Bench涵盖了六大规则类别下的40个代表性任务，包含622个高质量标注实例。针对每个生成视频的评估，我们构建了涵盖四项指标的检查表，并利用GPT-4o为每个问题分配分数，实现了与人工判断85%的一致性。大量实验表明，当前最先进的模型在规则连贯性指标上仅达到48.87%，凸显了下一代视频模型在推理能力方面存在显著的提升空间。我们期望通过RULER-Bench获得的见解能够推动具备推理意识的视频生成的进一步发展，促进视频生成模型向视觉基础智能迈进。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.02622) | [arXiv](https://arxiv.org/abs/2512.02622)



---

### 23. TRivia：基于自监督微调的视觉语言模型表格识别方法

**原文标题：** TRivia: Self-supervised Fine-tuning of Vision-Language Models for Table Recognition

**摘要：**
表格识别（TR）旨在将表格图像转换为HTML或Markdown等半结构化表示形式。作为文档解析的核心组成部分，表格识别长期依赖监督学习，近期研究主要通过标注数据微调视觉语言模型（VLMs）来实现突破。尽管视觉语言模型将表格识别性能提升至新高度，但进一步突破需要大规模标注数据，其获取成本高昂。因此，尽管专有模型不断突破性能边界，但受限于训练资源且因隐私法规成为实际应用中唯一可行选择的开源模型，其性能仍远落后于前沿水平。为弥合这一差距，我们提出TRivia——一种自监督微调方法，使预训练的视觉语言模型能够直接从无标注的真实场景表格图像中学习表格识别。该方法基于群体相对策略优化构建，可自动识别最能促进学习的无标注样本，并通过基于问答的奖励机制消除对人工标注的依赖。注意力引导模块为每个表格图像生成多样化问题，而模型对识别结果的解析能力与正确答案的匹配度将为优化表格识别模型提供反馈。这种闭环流程使得表格识别模型能够在无标注数据条件下，自主学习表格的识别、结构化与推理。基于此流程，我们推出TRivia-3B——一个开源、紧凑且性能领先的表格识别模型，在三个主流基准测试中超越现有系统（如Gemini 2.5 Pro、MinerU2.5）。模型与代码已发布于：https://github.com/opendatalab/TRivia

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.01248) | [arXiv](https://arxiv.org/abs/2512.01248)



---

### 24. MagicQuillV2：基于分层视觉线索的精确交互式图像编辑

**原文标题：** MagicQuillV2: Precise and Interactive Image Editing with Layered Visual Cues

**摘要：**
我们提出MagicQuill V2，这是一个新颖的系统，它将分层组合范式引入生成式图像编辑，弥合了扩散模型的语义生成能力与传统图形软件的精细控制之间的鸿沟。虽然扩散变换器擅长整体生成，但其使用的单一、整体的提示词无法区分用户在内容、位置和外观方面的不同意图。为克服此限制，我们的方法将创作意图解构为一组可控的视觉线索：内容层决定创建什么，空间层决定放置位置，结构层决定形状构成，色彩层决定调色方案。我们的技术贡献包括：用于上下文感知内容集成的专用数据生成流程、处理所有视觉线索的统一控制模块，以及用于精确局部编辑（包括对象移除）的微调空间分支。大量实验验证表明，这种分层方法有效解决了用户意图鸿沟问题，使创作者能够对生成过程进行直接、直观的控制。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.03046) | [arXiv](https://arxiv.org/abs/2512.03046)



---

### 25. 重新审视视觉中心推理泛化中长链思维的必要性

**原文标题：** Revisiting the Necessity of Lengthy Chain-of-Thought in Vision-centric Reasoning Generalization

**摘要：**
本研究探讨不同链式思维设计如何影响视觉语言模型获取可泛化的视觉推理能力。尽管链式思维数据（尤其是长链或视觉链式思维，如“基于图像的思考”）已被广泛用于监督中间推理过程，但特定设计为何有效、何种设计真正支持可泛化推理仍不明确。为系统评估该问题，我们采用受控迷宫求解基准任务，其中推理规则完全基于视觉，难度可通过网格尺寸调节，且所有中间步骤均可自动生成。在标准监督微调-强化学习流程下，使用Qwen2.5-VL-7B模型比较了三种代表性链式思维格式：语言链式思维、具象化链式思维（含空间坐标轨迹）和视觉链式思维（含图像操作）。实验表明：视觉化与长链式思维主要加速收敛过程，但未提升最终性能上限；仅包含必要具象化步骤的简洁链式思维优于长链追踪；值得注意的是，仅保留最简具象化信息的链式思维在不同迷宫尺寸中展现出最佳泛化能力。我们进一步在其他视觉中心任务中验证了这些发现。这些结果揭示了“短即长”效应，并为构建更具泛化能力的视觉推理监督微调数据集提供了实践指导。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.22586) | [arXiv](https://arxiv.org/abs/2511.22586)



---

### 26. PAI-Bench：面向物理人工智能的综合基准测试

**原文标题：** PAI-Bench: A Comprehensive Benchmark For Physical AI

**摘要：**
物理人工智能旨在开发能够感知和预测现实世界动态的模型；然而，当前多模态大语言模型与视频生成模型对这些能力的支持程度尚未得到充分理解。我们提出了物理人工智能基准测试（PAI-Bench），这是一个统一且全面的基准测试体系，用于评估模型在视频生成、条件视频生成及视频理解任务中的感知与预测能力。该基准包含2,808个真实世界案例，并采用任务对齐的评估指标，旨在捕捉物理合理性与领域特定推理能力。本研究对近期主流模型进行了系统性评估，结果表明：视频生成模型尽管在视觉保真度上表现突出，却常难以保持物理连贯的动态过程；而多模态大语言模型在动态预测与因果解释方面仍存在明显局限。这些发现表明，当前系统在应对物理人工智能的感知与预测需求方面尚处于早期发展阶段。综上所述，PAI-Bench为评估物理人工智能建立了贴近现实的基准框架，并揭示了未来系统需着力突破的关键能力缺口。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.01989) | [arXiv](https://arxiv.org/abs/2512.01989)



---

### 27. Video4Spatial：基于上下文引导视频生成的视觉空间智能探索

**原文标题：** Video4Spatial: Towards Visuospatial Intelligence with Context-Guided Video Generation

**摘要：**
本研究探讨视频生成模型能否仅凭视觉数据展现出人类认知核心能力之一的视觉空间智能。为此，我们提出Video4Spatial框架，证明仅以视频场景上下文为条件的视频扩散模型能够执行复杂空间任务。我们在两项任务上进行了验证：场景导航——在遵循相机位姿指令的同时保持与场景三维几何结构的一致性；以及物体定位——需要语义定位、指令跟随与路径规划。两项任务均仅使用视频输入，无需深度或位姿等辅助模态。通过框架设计与数据构建中简洁有效的策略，Video4Spatial展现出基于视频上下文的强大空间理解能力：能够端到端规划导航路径并定位目标物体，在遵循相机位姿指令的同时保持空间一致性，并能泛化至长序列上下文及域外环境。这些成果共同推动了视频生成模型向通用视觉空间推理能力的发展。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.03040) | [arXiv](https://arxiv.org/abs/2512.03040)



---

### 28. YingVideo-MV：音乐驱动的多阶段视频生成

**原文标题：** YingVideo-MV: Music-Driven Multi-Stage Video Generation

**摘要：**
尽管基于扩散模型的音频驱动虚拟形象视频生成在合成具有自然音画同步与身份一致性的长序列方面已取得显著进展，但包含摄像机运动的音乐表演视频生成领域仍鲜有探索。本文提出YingVideo-MV——首个面向音乐驱动的长视频生成的级联框架。该方法通过整合音频语义分析、可解释的镜头规划模块（MV-Director）、时序感知的扩散Transformer架构以及长序列一致性建模，实现了从音频信号自动合成高质量音乐表演视频。我们通过收集网络数据构建了大规模真实场景音乐数据集，以支撑多样化高质量结果的生成。针对现有长视频生成方法缺乏显式摄像机运动控制的问题，我们设计了摄像机适配模块，将摄像机位姿嵌入隐空间噪声。为增强长序列推理中片段间的连续性，进一步提出基于音频嵌入自适应调整去噪范围的时序感知动态窗口策略。综合基准测试表明，YingVideo-MV在生成连贯且富有表现力的音乐视频方面表现优异，并能实现精确的音乐-动作-摄像机同步。更多视频请访问项目页面：https://giantailab.github.io/YingVideo-MV/。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.02492) | [arXiv](https://arxiv.org/abs/2512.02492)



---

### 29. SimWorld：面向物理与社会世界中自主智能体的开放式真实模拟器

**原文标题：** SimWorld: An Open-ended Realistic Simulator for Autonomous Agents in Physical and Social Worlds

**摘要：**
尽管基于大语言模型/视觉语言模型的人工智能体在数学、编程及计算机应用领域发展迅速，其在复杂物理与社会环境中的应用仍面临挑战。构建能够在现实世界中生存与发展（例如通过自主创收或运营企业）的智能体，需要在大规模具身化场景中进行交互、推理、训练与评估。然而，现有支持此类开发的世界模拟器存在明显不足：它们通常依赖有限的手工构建环境，模拟简化的游戏式物理与社会规则，且缺乏对大语言模型/视觉语言模型智能体的原生支持。本文提出SimWorld——一个基于虚幻引擎5构建的新型模拟器，专为在丰富、类真实世界的场景中开发与评估大语言模型/视觉语言模型智能体而设计。SimWorld具备三项核心能力：（1）真实、开放的世界模拟，包括精确的物理与社会动态机制，以及基于语言的程序化环境生成；（2）面向大语言模型/视觉语言模型智能体的丰富交互接口，支持多模态世界信息输入与多抽象层级的开放词汇动作指令；（3）多样化、可扩展的物理与社会推理场景，用户可便捷进行定制化配置。我们通过部署前沿大语言模型智能体（如GPT-4o、Gemini-2.5-Flash、Claude-3.5及DeepSeek-Prover-V2）在涉及战略合作与竞争的长期多智能体配送任务中验证了SimWorld的功能。实验结果揭示了不同模型间显著的推理模式差异与局限性。我们将SimWorld开源发布，期待其成为推动跨学科现实世界智能体智能发展的基础平台：https://simworld.org。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.01078) | [arXiv](https://arxiv.org/abs/2512.01078)



---

### 30. SwiftVLA：以最小开销解锁轻量级视觉-语言-动作模型的时空动态理解

**原文标题：** SwiftVLA: Unlocking Spatiotemporal Dynamics for Lightweight VLA Models at Minimal Overhead

**摘要：**
基于预训练视觉-语言模型构建的视觉-语言-动作模型展现出强大潜力，但其庞大的参数量限制了实际应用。为缓解此问题，现有研究尝试采用轻量级视觉-语言模型，但这会牺牲时空推理能力。尽管有方法提出引入额外三维输入可改善性能，但这些方案通常依赖大型视觉-语言模型融合三维与二维输入，且仍缺乏时序理解能力。为此，我们提出SwiftVLA架构，在保持设计效率的同时为紧凑模型赋予四维理解能力。具体而言，本方法采用预训练的时序缓存四维视觉几何变换器，从二维图像中提取四维特征；随后，为增强视觉-语言模型协同利用二维图像与四维特征的能力，我们引入融合令牌——一组以未来预测为目标训练的可学习令牌，用于生成动作合成的统一表征；最后，我们设计掩码重构策略：通过掩码四维输入并训练视觉-语言-动作模型进行重构，使视觉-语言模型能学习有效的四维表征，进而在推理阶段可舍弃四维分支且性能损失最小。真实环境与仿真实验表明，SwiftVLA在边缘设备上不仅超越轻量级基线模型，其性能更可媲美参数量达7倍的视觉-语言-动作模型，在实现相当性能的同时推理速度提升18倍，内存占用减少12倍。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.00903) | [arXiv](https://arxiv.org/abs/2512.00903)



---

### 31. Ovis-Image技术报告

**原文标题：** Ovis-Image Technical Report

**摘要：**
本文介绍Ovis-Image——一个专门针对高质量文本渲染优化的70亿参数文生图模型，其设计目标是在严格的计算限制下实现高效运行。该模型基于我们先前提出的Ovis-U1框架构建，将基于扩散的视觉解码器与更强大的Ovis 2.5多模态骨干网络相结合，采用以文本为中心的训练流程，融合了大规模预训练与精细调整的后训练优化策略。尽管采用紧凑架构，Ovis-Image在文本渲染性能上已达到Qwen-Image等显著更大规模开源模型的水平，并接近Seedream、GPT4o等闭源系统。关键优势在于，该模型仅需单个高端GPU与适中内存即可部署，显著缩小了前沿文本渲染能力与实际应用部署之间的鸿沟。实验结果表明：通过将强大的多模态骨干网络与精心设计的文本导向训练方案相结合，无需依赖超大参数规模或专有模型，即可实现可靠的双语文本渲染能力。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.22982) | [arXiv](https://arxiv.org/abs/2511.22982)



---

### 32. GUI探索实验室：通过多轮强化学习增强智能体界面导航能力

**原文标题：** GUI Exploration Lab: Enhancing Screen Navigation in Agents via Multi-Turn Reinforcement Learning

**摘要：**
随着大视觉语言模型的快速发展，图形用户界面智能体任务的重点已从单屏幕任务转向复杂的屏幕导航挑战。然而，现实中的图形用户界面环境（如电脑软件和移动应用）通常结构复杂且具有专有性，难以获取智能体训练与评估所需的完整环境信息。这一局限阻碍了对智能体导航能力的系统性研究与基准测试。为突破此限制，我们提出了GUI探索实验室——一个面向图形用户界面智能体导航研究的仿真环境引擎。该引擎支持灵活定义与组合屏幕、图标及导航图谱，同时提供完整的环境信息访问，以实现全面的智能体训练与评估。通过大量实验发现，监督微调能够有效促进基础知识的记忆，为后续训练奠定关键基础。在此基础上，单轮强化学习可进一步提升对未见过场景的泛化能力。最终，多轮强化学习通过交互式试错过程鼓励探索策略的形成，从而显著提高屏幕导航性能。我们在静态与交互式基准测试中验证了所提方法的有效性，证明研究结论能够很好地推广至实际应用场景。这些发现彰显了强化学习方法在图形用户界面导航中的优势，并为构建更具能力与泛化性的图形用户界面智能体提供了实践指导。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.02423) | [arXiv](https://arxiv.org/abs/2512.02423)



---

### 33. FlashVGGT：基于压缩描述符注意力机制的高效可扩展视觉几何变换器

**原文标题：** FlashVGGT: Efficient and Scalable Visual Geometry Transformers with Compressed Descriptor Attention

**摘要：**
基于多视图图像的3D重建是计算机视觉领域的核心挑战。近年来，前馈方法已成为传统逐场景优化技术的高效且鲁棒的替代方案。其中，视觉几何基础变换器（VGGT）等先进模型通过对所有图像标记进行完全自注意力计算来捕捉全局关系。然而，由于自注意力的二次复杂度以及长图像序列生成的大量标记，该方法存在可扩展性不足的问题。本研究提出FlashVGGT，这是一种基于描述符注意力机制的高效替代方案，旨在解决这一瓶颈。FlashVGGT不再对所有标记进行密集的全局注意力计算，而是将每帧图像的空间信息压缩为一组紧凑的描述符标记。随后通过完整图像标记集与压缩描述符集之间的交叉注意力实现全局关系建模，从而显著降低计算开销。此外，描述符的紧凑性使其能够通过分块递归机制实现长序列在线推理，该机制可复用历史分块的缓存描述符。实验结果表明，FlashVGGT在重建精度上与VGGT相当，同时对1000张图像的推理时间降至VGGT的9.3%，并能高效扩展至超过3000张图像的长序列处理。项目页面详见：https://wzpscott.github.io/flashvggt_page/。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.01540) | [arXiv](https://arxiv.org/abs/2512.01540)



---

### 34. BlockVid：基于块扩散的高质量一致性分钟级视频生成方法

**原文标题：** BlockVid: Block Diffusion for High-Quality and Consistent Minute-Long Video Generation

**摘要：**
生成长达分钟级别的视频是构建世界模型的关键一步，可为生成逼真扩展场景和先进人工智能模拟器奠定基础。新兴的半自回归（块扩散）范式融合了扩散模型与自回归模型的优势，通过KV缓存与并行采样实现了任意长度视频生成并提升了推理效率。然而，该方法仍面临两大持续存在的挑战：（一）KV缓存引发的长时序误差累积问题；（二）缺乏细粒度长视频基准测试体系及连贯性感知评估指标。为突破这些局限，本文提出BlockVid——一种创新的块扩散框架。该框架配备语义感知稀疏KV缓存机制，采用名为“块强制训练”的有效训练策略，并设计专用的分块噪声调度与重排方案，以降低误差传播并增强时序一致性。我们进一步构建了LV-Bench细粒度分钟级视频基准测试集，配套提出评估长程连贯性的新型指标体系。在VBench与LV-Bench上的大量实验表明，BlockVid在生成高质量、高连贯性分钟级视频方面持续优于现有方法。具体而言，在LV-Bench评估中，其VDE主体指标较现有最优方法提升22.2%，VDE清晰度指标提升19.4%。项目网站：https://ziplab.co/BlockVid。代码仓库：https://github.com/alibaba-damo-academy/Inferix。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.22973) | [arXiv](https://arxiv.org/abs/2511.22973)



---

### 35. C^2DLM：因果概念引导的扩散大语言模型

**原文标题：** C^2DLM: Causal Concept-Guided Diffusion Large Language Models

**摘要：**
自回归语言模型与扩散语言模型构成了大语言模型的两大主要范式。然而，这两种范式均存在推理能力不足的问题。人类的推理本质上依赖于因果知识与思维，这些在自然语言中有所体现。但在自回归范式中，语言被建模为下一个词预测（严格遵循从左到右、逐词生成的顺序），而自然语言本身展现出更为灵活的因果结构。在扩散范式下，注意力机制采用全连接方式，完全忽略了因果顺序。为填补这一空白，本文提出一种**因**果**概**念引导的**扩**散**语**言**模**型（C^2DLM）。该模型从扩散语言模型的全连接注意力机制出发，首先从教师模型中获取概念级因果图，进而显式引导注意力学习概念间的因果关系。通过聚焦于因果关系，并避免涉及因果反转的困难子目标带来的干扰，C^2DLM在COT-OrderPerturb任务中实现了12%的性能提升与约3.2倍的训练加速，并在六项下游推理任务中平均获得1.31%的性能增益。更多细节请参见代码仓库：https://github.com/Kairong-Han/C-2-DLM。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.22146) | [arXiv](https://arxiv.org/abs/2511.22146)



---

### 36. 超越描述：具身智能体细粒度动作的认知基准测试

**原文标题：** Beyond Description: Cognitively Benchmarking Fine-Grained Action for Embodied Agents

**摘要：**
多模态大语言模型作为在复杂物理环境中运行的具身智能体的决策引擎，已展现出广阔前景。然而，现有基准测试往往侧重于高层规划或空间推理，对具身物理交互所需的细粒度动作智能探索不足。为填补这一空白，我们提出了CFG-Bench——一个旨在系统评估这一关键能力的新型基准测试。该基准包含1,368个精选视频及与之匹配的19,562组三模态问答对，聚焦四大认知能力维度：1）物理交互，2）时序因果关联，3）意图理解，4）价值判断。这些维度共同构成了评估模型将视觉观察转化为可执行知识能力的系统框架，超越了表层识别范畴。我们在CFG-Bench上的综合评估表明，当前领先的多模态大语言模型在生成物理交互的详细指令方面存在困难，在意图理解与价值判断等高阶推理能力上表现出明显局限。此外，基于本数据集的监督微调实验证明，通过训练多模态大语言模型描述细粒度动作，能直接转化为其在现有具身基准测试上的显著性能提升。本研究不仅揭示了当前模型的局限性，更为开发更具能力且贴合现实的具身智能体提供了重要洞见。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.18685) | [arXiv](https://arxiv.org/abs/2511.18685)



---

### 37. 面向人像视频编辑的上下文同步LoRA方法

**原文标题：** In-Context Sync-LoRA for Portrait Video Editing

**摘要：**
人像视频编辑是一项具有挑战性的任务，需要对多种修改（如外观变化、表情调整或对象添加）实现灵活而精确的控制。其核心难点在于保持主体原有的时序行为，要求每一帧编辑后的画面与对应原始帧保持精确同步。本文提出Sync-LoRA方法，该人像视频编辑技术能够在实现高质量视觉修改的同时，保持帧级精确同步与身份一致性。我们的方法基于图像到视频的扩散模型，通过修改首帧定义编辑内容，并将其传播至整个序列。为实现精确同步，我们使用成对视频训练上下文LoRA模型，这些视频具有相同的运动轨迹但外观不同。这些训练对通过基于同步性的筛选流程自动生成和筛选，仅选择时序对齐程度最高的样本进行训练。该训练机制使模型能够将源视频的运动线索与编辑首帧引入的视觉变化相结合。通过在精筛选的同步人像数据集上进行训练，Sync-LoRA能够泛化至未见过的身份和多样化编辑任务（如修改外观、添加对象或更换背景），并稳健处理姿态与表情的变化。实验结果表明，该方法在视觉保真度和时序连贯性方面表现优异，实现了编辑保真度与精确运动保持之间的稳健平衡。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.03013) | [arXiv](https://arxiv.org/abs/2512.03013)



---

### 38. 基于VideoScience-Bench的视频生成科学理解与推理能力评估

**原文标题：** Benchmarking Scientific Understanding and Reasoning for Video Generation using VideoScience-Bench

**摘要：**
视频生成领域的下一个前沿在于开发具备零样本推理能力的模型，其中理解现实世界的科学定律对于在不同条件下准确建模物理结果至关重要。然而，现有的视频评估基准多基于物理常识，对视频模型的科学推理能力评估有限。我们提出了VideoScience-Bench，这是一个专门设计用于评估视频模型对本科层次科学理解能力的基准测试。每个提示都编码了一个复合科学场景，要求模型理解并综合多个科学概念以生成正确的现象。该基准包含200个精心设计的提示，涵盖物理和化学领域的14个主题及103个核心概念。我们在文本到视频和图像到视频两种设置下，对七个前沿视频模型进行了专家标注评估，评估维度包括：提示一致性、现象符合性、动态正确性、对象恒常性以及时空连续性。通过使用视觉语言模型作为评判器对视频生成结果进行评估，我们发现其与人工评估结果具有高度相关性。据我们所知，VideoScience-Bench是首个不仅将视频模型视为生成器，更将其作为推理系统进行评估的基准，要求其生成内容展现出与预期物理化学现象相一致的科学理解能力。我们的数据与评估代码已公开于：https://github.com/hao-ai-lab/VideoScience。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.02942) | [arXiv](https://arxiv.org/abs/2512.02942)



---

### 39. 理解与利用统一多模态模型中的稀疏性

**原文标题：** Understanding and Harnessing Sparsity in Unified Multimodal Models

**摘要：**
大型多模态模型在理解与生成任务上均取得了显著进展。近期研究致力于构建统一多模态模型，通过整合异构组件在单一框架内同时支持这两种能力。然而，这种统一性会引入推理效率问题，例如特定任务或样本可能无需调用统一模型的全部知识或容量。目前，对于这些效率问题在不同组件中如何具体体现，仍缺乏系统性的理解。本研究首先采用免训练剪枝作为探测方法，从深度剪枝与宽度缩减两个维度，对统一多模态模型组件进行了系统性分析。研究发现：理解组件在理解与生成任务中均表现出显著的压缩潜力，且在生成任务中更为明显；相比之下，生成组件对压缩高度敏感，即使在中等压缩比下性能也会急剧下降。为应对这一局限，我们受不同样本间动态激活模式的启发，提出了专家混合适配方法。该方法将生成模块划分为多个专家，并通过稀疏激活机制以恢复生成质量。我们通过专家冻结微调验证了稀疏激活的有效性，并进一步证明完全可训练的适配能带来额外性能提升。实验结果表明，适配后的BAGEL模型在仅激活约半数参数的情况下，达到了与完整模型相当的性能。代码已发布于https://github.com/Shwai-He/SparseUnifiedModel。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.02351) | [arXiv](https://arxiv.org/abs/2512.02351)



---

### 40. 视觉同步：基于跨视角物体运动的多相机同步方法

**原文标题：** Visual Sync: Multi-Camera Synchronization via Cross-View Object Motion

**摘要：**
如今，人们能够轻松使用多台消费级相机记录从音乐会、体育赛事、讲座、家庭聚会到生日派对等各种值得纪念的时刻。然而，同步这些跨相机视频流仍然具有挑战性。现有方法通常依赖于受控环境、特定目标、人工校正或昂贵硬件。本文提出VisualSync——一种基于多视角动态特性的优化框架，能够以毫秒级精度对齐无位姿信息且未同步的视频。我们的核心发现是：任意运动的三维点若在两个相机中同时可见，在正确同步后将满足极几何约束。基于此，VisualSync利用现成的三维重建、特征匹配与密集跟踪技术提取轨迹片段、相对位姿及跨视角对应关系，进而通过联合最小化极线误差来估计各相机的时间偏移量。在四个多样化且具有挑战性的数据集上的实验表明，VisualSync优于现有基线方法，其中位同步误差可控制在50毫秒以内。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.02017) | [arXiv](https://arxiv.org/abs/2512.02017)



---

### 41. Artemis：面向感知策略学习的结构化视觉推理框架

**原文标题：** Artemis: Structured Visual Reasoning for Perception Policy Learning

**摘要：**
近期面向视觉感知策略的强化学习框架开始引入以自然语言表达的中间推理链。实证研究表明，这种纯语言形式的中间推理往往会降低感知任务的性能。我们认为核心问题不在于推理本身，而在于推理形式：现有推理链在非结构化的语言空间中进行语义推理，而视觉感知需要在以空间和物体为中心的空间中进行推理。为此，我们提出Artemis——一个基于结构化候选框推理的感知策略学习框架，其中每个中间步骤均表示为可验证视觉状态的（标签，边界框）对。该设计实现了对中间状态的显式追踪、对候选框质量的直接监督，并避免了语言推理引入的歧义性。Artemis基于Qwen2.5-VL-3B构建，在指代定位与检测任务中表现优异，并在计数与几何感知任务上展现出显著的泛化能力。这些多样化场景中一致的性能提升证实了将推理与空间表征对齐能够增强感知策略学习。得益于其强化的视觉推理能力，Artemis在通用多模态大模型基准测试中也取得了具有竞争力的性能，这表明基于空间锚定的推理为构建可扩展、泛化性强的感知策略提供了理论可行的路径。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.01988) | [arXiv](https://arxiv.org/abs/2512.01988)



---

### 42. UnicEdit-10M：通过统一验证打破规模-质量壁垒的推理增强编辑数据集与基准

**原文标题：** UnicEdit-10M: A Dataset and Benchmark Breaking the Scale-Quality Barrier via Unified Verification for Reasoning-Enriched Edits

**摘要：**
随着GPT-4o、Nano Banana、Seedream 4.0等强大多模态模型在图像编辑领域的快速发展，闭源模型与开源模型之间的性能差距日益扩大，这主要源于大规模高质量训练数据的稀缺，以及缺乏能够全面诊断模型在不同编辑行为中弱点的综合性基准。现有数据构建方法面临规模与质量的权衡：人工标注质量高但难以扩展，而自动化流程则受错误传播和噪声干扰。为解决这一问题，我们提出一种轻量级数据构建流程，以端到端模型和统一的后验阶段替代多工具链。为实现可扩展的质量控制，我们训练了一个70亿参数的双任务专家模型Qwen-Verify，用于高效失败检测和指令重描述。该流程最终构建出UnicEdit-10M——一个涵盖多样化基础与复杂编辑任务的千万级规模数据集。同时，我们提出UnicBench通用基准，其不仅涵盖基础编辑任务，更扩展至对空间推理与知识驱动推理的显式评估。为支持细粒度诊断，我们引入了包括非编辑一致性与推理准确度在内的新型评估指标。通过对主流模型在UnicBench上的分析，我们揭示了其局限性，并为未来研究指明了明确方向。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.02790) | [arXiv](https://arxiv.org/abs/2512.02790)



---

### 43. 鞋型不变与地面感知的密集足部接触估计学习

**原文标题：** Shoe Style-Invariant and Ground-Aware Learning for Dense Foot Contact Estimation

**摘要：**
足部接触在人与世界的交互中起着关键作用，因此探究足部接触能够深化我们对人体运动与物理交互的理解。尽管其重要性显著，现有方法通常采用零速度约束来近似足部接触，并侧重于关节层面的接触估计，未能捕捉足部与世界之间的细节交互。密集足部接触估计对于精确建模此类交互至关重要，然而从单张RGB图像预测密集足部接触的研究仍处于探索不足的状态。学习密集足部接触估计主要面临两大挑战：其一，鞋履外观高度多样化，导致模型难以在不同鞋型间泛化；其二，地面通常呈现单调外观，使得信息特征的提取较为困难。为解决这些问题，我们提出一种足部接触估计（FECO）框架，通过鞋型不变学习与地面感知学习实现密集足部接触估计。为应对鞋履外观多样性的挑战，我们的方法引入鞋型对抗训练，强制模型提取鞋型不变特征以进行接触估计。为有效利用地面信息，我们设计了地面特征提取器，基于空间上下文捕捉地面属性。实验表明，所提方法能够实现不受鞋履外观影响的鲁棒性足部接触估计，并有效利用地面信息。代码将公开发布。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.22184) | [arXiv](https://arxiv.org/abs/2511.22184)



---

### 44. 基于高效启发式辅助构造的奥林匹克几何金牌级解题方法

**原文标题：** Gold-Medal-Level Olympiad Geometry Solving with Efficient Heuristic Auxiliary Constructions

**摘要：**
欧几里得几何中的自动定理证明，特别是针对国际数学奥林匹克（IMO）级别的问题，仍然是人工智能领域的重大挑战和重要研究方向。本文提出一种高效的几何定理证明方法，该方法完全在CPU上运行，无需依赖基于神经网络的推理。我们的初步研究表明，采用简单的随机添加辅助点策略即可在IMO问题上达到银牌级别的人类解题水平。在此基础上，我们提出了HAGeo方法——一种基于启发式的几何演绎辅助构造技术。该方法在IMO-30基准测试中成功解决了30道题目中的28道，达到了金牌级别的解题水平，并以显著优势超越了基于神经网络的竞争性方法AlphaGeometry。为了更全面地评估本方法及现有技术，我们进一步构建了HAGeo-409基准测试集，该数据集包含409道经过人工难度评估的几何问题。与广泛使用的IMO-30相比，我们的基准测试集提出了更大挑战，提供了更精确的评估标准，为几何定理证明领域设立了更高的技术门槛。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.00097) | [arXiv](https://arxiv.org/abs/2512.00097)



---

### 45. 掩码可能造成干扰：论扩散语言模型中的上下文理解能力

**原文标题：** Masks Can Be Distracting: On Context Comprehension in Diffusion Language Models

**摘要：**
掩码扩散语言模型（MDLMs）近期作为自回归语言模型（ARLMs）的一种有前景的替代方案出现，其利用去噪目标在理论上应能实现更均衡的上下文利用。本研究系统考察了MDLMs的上下文理解能力，并揭示了两项关键局限。首先，尽管MDLMs采用更全局的训练目标和双向注意力机制，但其与ARLMs相似地表现出强烈的局部性偏好：模型性能对输入中相关信息的位置高度敏感，更倾向于利用局部上下文而非远距离上下文。其次，我们发现生成所需的大量掩码标记会显著削弱上下文理解能力。通过系统性消融实验，这些掩码被证实具有干扰作用，会降低模型处理相关信息的能力。为应对此问题，我们提出一种掩码无关的损失函数，该函数促使模型预测结果对附加掩码数量保持稳定。基于此目标进行微调可有效缓解掩码的干扰效应，显著提升MDLMs的鲁棒性。总体而言，本研究揭示了当前MDLM训练范式的关键局限，并为构建具有更强上下文理解能力的扩散式语言模型提供了可操作的改进方向。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.21338) | [arXiv](https://arxiv.org/abs/2511.21338)



---

### 46. CodeV：基于工具感知策略优化的图像代码化可信视觉推理

**原文标题：** CodeV: Code with Images for Faithful Visual Reasoning via Tool-Aware Policy Optimization

**摘要：**
当前代理式视觉语言模型正越来越多地通过调用图像操作来训练“基于图像的思考”能力。然而，我们发现最终答案的高准确率往往掩盖了不可信的视觉推理过程：模型可能在无关区域调用工具，或完全忽略工具输出，却仍能猜测出正确答案。本研究首先提出一种可信度评估协议，用于衡量中间视觉工具输出（如图像裁剪区域）是否真实包含查询证据。该评估显示，当前先进的视觉代理虽然在最终答案准确率上表现优异，但在视觉搜索基准测试中展现出较低的可信工具使用率。为此，我们提出CodeV——一种基于代码的视觉代理，采用工具感知策略优化（TAPO）进行训练。TAPO是一种过程级强化学习框架，它在GRPO基础上增加了直接针对视觉工具输入输出定义的密集奖励机制（而非基于思维链标记），使得监督更易于验证且不易受到奖励篡改的影响。CodeV将视觉工具表示为可执行的Python代码，TAPO仅依据问题和工具输出分配逐步奖励，从而促进必要且与证据一致的工具使用。通过两阶段监督微调与强化学习相结合的流程，CodeV在相关视觉搜索基准测试中实现了具有竞争力的准确率，同时显著提升了可信工具使用率。除视觉搜索任务外，CodeV在多种多模态推理和数学基准测试中均表现出色，这表明对中间工具行为的显式监督对于构建可信赖的代理式视觉推理系统至关重要。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.19661) | [arXiv](https://arxiv.org/abs/2511.19661)



---

### 47. BOOM：超越单一模态——KIT的多模态多语言讲座伴侣

**原文标题：** BOOM: Beyond Only One Modality KIT's Multimodal Multilingual Lecture Companion

**摘要：**
教育全球化与在线学习的迅猛发展使得教育内容本地化成为一项关键挑战。讲座材料本质上是多模态的，融合了语音音频与视觉幻灯片，这要求系统具备处理多种输入模态的能力。为提供无障碍且完整的学习体验，翻译必须保留所有模态：用于阅读的文本、用于视觉理解的幻灯片以及用于听觉学习的语音。本文提出BOOM——一种多模态多语言讲座伴侣系统，它能联合翻译讲座音频与幻灯片，生成跨三种模态的同步输出：翻译文本、保留视觉元素的本地化幻灯片以及合成语音。这种端到端的方法使学生能够以母语获取讲座内容，同时力求完整保留原始信息。实验表明，融合幻灯片信息的转录文本还能为摘要生成和问答等下游任务带来连锁增益。我们在https://github.com/saikoneru/image-translator发布了幻灯片翻译代码，并将其集成至讲座翻译系统（https://gitlab.kit.edu/kit/isl-ai4lt/lt-middleware/ltpipeline）\footnote{所有公开代码与模型均遵循MIT许可证发布。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.02817) | [arXiv](https://arxiv.org/abs/2512.02817)



---

### 48. Click2Graph：基于单次点击的交互式全景视频场景图生成

**原文标题：** Click2Graph: Interactive Panoptic Video Scene Graphs from a Single Click

**摘要：**
当前最先进的视频场景图生成系统虽能提供结构化的视觉理解，但其作为封闭的前馈式流水线运行，无法融入人工引导。相比之下，以SAM2为代表的可提示分割模型虽支持精确的用户交互，却缺乏语义或关系推理能力。本文提出Click2Graph——首个面向全景视频场景图生成的交互式框架，实现了视觉提示与空间、时间及语义理解的统一。该系统仅需用户提供单次交互提示（如点击或边界框），即可跨时间分割并跟踪目标主体，自主发现交互对象，进而预测<主体，对象，谓词>三元组以构建时序一致的场景图。本框架包含两个核心组件：动态交互发现模块（用于生成基于主体条件的对象提示）和语义分类头（用于执行联合实体与谓词推理）。在OpenPVSG基准测试上的实验表明，Click2Graph为用户引导的全景视频场景图生成奠定了坚实基础，展示了如何将人工提示与全景定位及关系推理相结合，从而实现可控且可解释的视频场景理解。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.15948) | [arXiv](https://arxiv.org/abs/2511.15948)



---


## 🔍 关键词云图
![关键词云图](../images/keywords_wordcloud.png)

## 📈 近期论文趋势
![论文趋势](../images/daily_papers.png)

## 🎙️ 语音播报
- [收听今日论文解读](../audio/2025-12-03_daily_papers.mp3)

## 📱 订阅渠道
- GitHub: [hf-daily-paper-newsletter-chinese](https://github.com/2404589803/hf-daily-paper-newsletter-chinese)