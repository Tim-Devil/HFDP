
# <img src="https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo.png" width="30"/> Hugging Face 2025-11-27 论文日报

## 📊 今日论文统计
- 总论文数：20
- 热门领域：LLM, RL, GPT

## 📝 论文详情


### 1. 俄语架构的多模态评估框架

**原文标题：** Multimodal Evaluation of Russian-language Architectures

**摘要：**
多模态大语言模型目前已成为研究焦点，其在规模与能力层面呈现快速发展，但其智能水平、局限性及潜在风险仍未得到充分认知。针对这一现状，特别是在俄语语境下尚无多模态基准的背景下，我们推出Mera Multi——一个面向俄语架构的开放式多模态评估框架。该基准采用指令驱动模式，涵盖默认的文本、图像、音频与视频模态，包含18项全新构建的评估任务，既适用于通用模型，也针对特定模态架构（图像到文本、视频到文本及音频到文本）。我们的贡献包括：（i）建立多模态能力的统一分类体系；（ii）完全从零构建的18个数据集，重点关注俄罗斯文化及语言特性，并统一提示词与评估指标；（iii）闭源与开源模型的基线结果；（iv）包含水印技术与私有数据集许可的基准泄露防范方法论。虽然当前研究聚焦俄语，但本基准为在类型学多样的语言（特别是斯拉夫语系）中构建多模态评估体系提供了可复现的方法论路径。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.15552) | [arXiv](https://arxiv.org/abs/2511.15552)



---

### 2. 多智能体系统中的潜在协作

**原文标题：** Latent Collaboration in Multi-Agent Systems

**摘要：**
多智能体系统（MAS）将大语言模型（LLMs）从独立的单模型推理扩展至协同的系统级智能。现有LLM智能体依赖基于文本的中介进行推理与通信，本研究通过实现模型在连续潜在空间内的直接协作推进了一步。我们提出LatentMAS——一种支持LLM智能体间纯潜在协作的端到端免训练框架。在该框架中，每个智能体首先通过末层隐藏嵌入进行自回归潜在思维生成，随后通过共享潜在工作记忆保存并传递各智能体的内部表征，确保无损信息交换。理论分析表明，相较于传统基于文本的MAS，LatentMAS在显著降低复杂度的同时实现了更强的表达能力与无损信息保存。在涵盖数学科学推理、常识理解和代码生成的9个综合基准测试中，实证评估表明LatentMAS持续优于强单模型及基于文本的MAS基线，准确率最高提升14.6%，输出令牌使用量减少70.8%-83.7%，端到端推理速度提升4-4.3倍。这些结果证明，我们的新型潜在协作框架在无需额外训练的情况下，既能提升系统级推理质量，又可实现显著的效率增益。代码与数据已通过https://github.com/Gen-Verse/LatentMAS 完全开源。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.20639) | [arXiv](https://arxiv.org/abs/2511.20639)



---

### 3. Inferix：基于块扩散的新一代世界模拟推理引擎

**原文标题：** Inferix: A Block-Diffusion based Next-Generation Inference Engine for World Simulation

**摘要：**
世界模型作为智能体AI、具身AI和游戏等领域的核心模拟器，能够生成长时序、物理真实且交互式的高质量视频。此外，扩展这些模型有望释放视觉感知、理解与推理的涌现能力，为超越当前以LLM为中心的视觉基础模型开辟新范式。实现这一突破的关键在于半自回归（块扩散）解码范式，该范式融合了扩散方法与自回归方法的优势，通过分块生成视频令牌——在每个块内应用扩散过程的同时以前序块为条件，从而产生更连贯稳定的视频序列。尤为关键的是，该范式通过重新引入LLM风格的KV缓存管理机制，突破了标准视频扩散模型的局限，实现了高效、可变长度的高质量生成。
因此，Inferix被专门设计为新一代推理引擎，通过优化的半自回归解码过程实现沉浸式世界合成。这种对世界模拟的专注定位使其明显区别于面向高并发场景的系统（如vLLM或SGLang）及经典视频扩散模型（如xDiTs）。Inferix进一步通过交互式视频流传输与性能分析功能增强系统能力，支持实时交互与逼真模拟以精确建模世界动态。此外，系统通过无缝集成LV-Bench——专为分钟级视频生成场景定制的新型细粒度评估基准，实现了高效性能评测。我们期待学界携手推进Inferix发展，共同推动世界模型的前沿探索。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.20714) | [arXiv](https://arxiv.org/abs/2511.20714)



---

### 4. Harmony：通过跨任务协同实现音视频生成的和谐统一

**原文标题：** Harmony: Harmonizing Audio and Video Generation through Cross-Task Synergy

**摘要：**
同步音视频内容的合成是生成式人工智能面临的关键挑战，开源模型在实现鲁棒的音视频对齐方面存在诸多困难。我们的分析表明，这一问题根源于联合扩散过程的三个基本挑战：(1) 对应漂移问题——并发演化的噪声潜在空间阻碍了对齐关系的稳定学习；(2) 低效的全局注意力机制难以捕捉细粒度时序特征；(3) 传统无分类器引导机制存在的模态内偏差，虽能增强条件可控性却无益于跨模态同步。为攻克这些难题，我们提出Harmony这一创新框架，通过机制化设计强化音视频同步。首先，我们建立跨任务协同训练范式，利用音频驱动视频生成与视频驱动音频生成任务中的强监督信号来抑制对应漂移。继而，设计全局-局部解耦交互模块，实现高效精准的时序风格对齐。最后，提出同步增强型无分类器引导机制，在推理阶段显式分离并强化对齐信号。大量实验表明，Harmony开创了全新性能标杆，在生成保真度及关键的细粒度音视频同步指标上均显著超越现有方法。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.21579) | [arXiv](https://arxiv.org/abs/2511.21579)



---

### 5. 重新审视跨难度水平的泛化能力：并非易事

**原文标题：** Revisiting Generalization Across Difficulty Levels: It's Not So Easy

**摘要：**
本文系统研究大型语言模型在不同任务难度间的泛化能力，这是影响数据筛选与评估效果的核心问题。现有研究对"使用简易或困难训练数据能否获得更优结果"及"性能提升体现在何种难度测试数据上"存在分歧。我们通过构建跨模型、跨数据集、跨细粒度难度分组的系统评估框架，采用数千种大型语言模型的输出结果结合教育测试领域的成熟难度度量指标——项目反应理论（IRT），对六个数据集中的样本进行难度分级。与既往研究不同，本研究的难度评级完全基于多样本LLM的自身能力，排除了人类主观难度判断。通过更客观、大规模、细粒度的分析，我们发现：跨难度泛化能力普遍受限；仅使用简易或困难数据训练均无法在全难度范围内实现持续性能提升。这些结果表明，在LLM的训练与评估数据中保持难度多样性至关重要，任何在难度维度上的捷径策略都存在风险。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.21692) | [arXiv](https://arxiv.org/abs/2511.21692)



---

### 6. NVIDIA Nemotron解析模型1.1版

**原文标题：** NVIDIA Nemotron Parse 1.1

**摘要：**
本文介绍Nemotron-Parse-1.1——一个轻量级文档解析与OCR模型，该模型在前代Nemoretriever-Parse-1.0的基础上实现了全面升级。本模型在通用OCR识别、Markdown格式解析、结构化表格处理以及图像/图表/示意图文本提取等任务中展现出显著增强的性能，同时支持更长的输出序列以处理视觉密集文档。与前一版本相同，该模型能够提取文本段的边界框及对应语义类别。Nemotron-Parse-1.1采用编码器-解码器架构，参数量达8.85亿（其中语言解码器为紧凑的2.56亿参数），在公开基准测试中达到具有竞争力的准确率，堪称优秀的轻量级OCR解决方案。我们已在Huggingface平台公开发布模型权重、优化的NIM推理容器，并作为Nemotron-VLM-v2数据集组成部分开放部分训练数据。此外，同步推出Nemotron-Parse-1.1-TC版本，该版本通过缩减视觉令牌长度实现推理速度提升20%，且质量损失控制在最小范围。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.20478) | [arXiv](https://arxiv.org/abs/2511.20478)



---

### 7. Monet：超越图像与语言的潜在视觉空间推理

**原文标题：** Monet: Reasoning in Latent Visual Space Beyond Images and Language

**摘要：**
"图像思维"已成为推进视觉推理的有效范式，通过将视觉证据注入中间推理步骤，超越了纯文本的思维链条。然而，现有方法因受限于外部工具，其灵活性远未达到类人的抽象视觉思维水平。本研究提出Monet训练框架，使多模态大语言模型能够通过生成作为中间视觉思维的连续嵌入表示，直接在潜在视觉空间中进行推理。我们识别出训练潜在视觉推理MLLMs的两个核心挑战：潜在视觉对齐的高计算成本与对潜在嵌入监督不足，并通过三阶段基于蒸馏的监督微调流程予以解决。进一步发现GRPO在潜在推理中的局限：其主要增强文本推理而非潜在推理。为此提出VLPO（视觉潜在策略优化），一种将潜在嵌入显式纳入策略梯度更新的强化学习方法。为支持监督微调，我们构建了包含12.5万条真实世界图表、OCR和几何思维链的高质量图文交错数据集Monet-SFT-125K。我们的Monet-7B模型在真实世界感知与推理基准测试中表现持续提升，并在具有挑战性的抽象视觉推理任务上展现出强大的分布外泛化能力。我们通过实证分析各训练组件的作用，并讨论早期失败尝试，为视觉潜在推理的未来发展提供见解。模型、数据与代码已开源：https://github.com/NOVAglow646/Monet。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.21395) | [arXiv](https://arxiv.org/abs/2511.21395)



---

### 8. 终端速度匹配

**原文标题：** Terminal Velocity Matching

**摘要：**
本文提出终端速度匹配（TVM）方法，作为流匹配的泛化形式，能够实现高保真度的单步及少步生成建模。TVM模拟任意两个扩散时间步之间的过渡过程，并在终止时刻而非初始时刻对其行为进行正则化处理。我们证明当模型满足Lipschitz连续性时，TVM可为数据分布与模型分布之间的2-Wasserstein距离提供上界。鉴于扩散变换器不具备该性质，我们引入最小化架构调整以实现稳定的单阶段训练。为提升TVM实际效率，我们开发了融合注意力核函数，支持在雅可比-向量积上进行反向传播，该机制与变换器架构具有良好的扩展适配性。在ImageNet-256x256数据集上，TVM以单次函数评估（NFE）取得3.29 FID，4次NFE取得1.99 FID；在ImageNet-512x512数据集上分别实现4.32（1-NFE）和2.94（4-NFE）的FID成绩，这代表了从零开始训练的单步/少步模型达到的最先进性能。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.19797) | [arXiv](https://arxiv.org/abs/2511.19797)



---

### 9. UniGame：将统一多模态模型转化为自身对抗者

**原文标题：** UniGame: Turning a Unified Multimodal Model Into Its Own Adversary

**摘要：**
统一多模态模型（UMMs）通过单一架构在理解与生成任务中均展现出卓越性能。然而这类模型仍存在根本性不一致问题：理解任务偏好紧凑嵌入表示，而生成任务需要重建丰富的表征。这种结构性权衡导致决策边界失准、跨模态连贯性下降，以及在分布偏移和对抗攻击下的脆弱性加剧。本文提出UniGame——一种针对不一致性问题的自对抗后训练框架。通过在共享令牌接口施加轻量化扰动器，该框架使生成分支能够主动探寻并挑战脆弱理解环节，将模型自身转化为其对抗者。实验表明，UniGame显著提升模型一致性（+4.6%），同时在理解任务（+3.6%）、生成质量（+0.02）以及分布外鲁棒性（NaturalBench +4.8%）与对抗鲁棒性（AdVQA +6.2%）方面取得显著提升。该框架具备架构无关性，仅增加不足1%的参数量，且可与现有后训练方法形成互补。这些成果确立了对抗自博弈作为增强未来多模态基础模型的连贯性、稳定性与统一能力的普适性原则。项目代码已开源：https://github.com/AIFrontierLab/UniGame

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.19413) | [arXiv](https://arxiv.org/abs/2511.19413)



---

### 10. G^2VLM：基于几何基础的视觉语言模型——统一三维重建与空间推理的新范式

**原文标题：** G^2VLM: Geometry Grounded Vision Language Model with Unified 3D Reconstruction and Spatial Reasoning

**摘要：**
当前视觉语言模型在空间智能方面仍存在明显不足，其在空间理解与推理任务中的表现亟待提升。我们认为这一缺陷源于缺乏能够从二维图像重建三维空间的视觉几何学习过程。本文提出G^2VLM这一基于几何基础的视觉语言模型，成功融合空间智能的两个核心维度：三维空间重建与空间理解。该模型通过原生利用学习得到的三维视觉几何特征，直接预测三维属性，并借助上下文学习与交叉推理机制增强空间推理任务。我们的统一架构在空间理解方面具有显著扩展性：既能利用海量多视角图像和视频数据进行训练，又可充分获取通常仅能通过难以采集的标注数据才能得到的三维视觉先验优势。实验结果表明，G^2VLM在两项任务中均表现优异，其三维重建效果与前沿的前馈式模型相当，在空间理解与推理任务中取得领先或具有竞争力的成绩。通过将语义强大的视觉语言模型与底层三维视觉任务相融合，我们期待G^2VLM能成为该领域的强基准，并为三维场景编辑等未来应用开启新的可能。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.21688) | [arXiv](https://arxiv.org/abs/2511.21688)



---

### 11. 块级联：无需训练的块因果视频模型加速方法

**原文标题：** Block Cascading: Training Free Acceleration of Block-Causal Video Models

**摘要：**
块因果视频生成面临显著的速度-质量权衡困境：13亿参数的小型模型仅能达到16 FPS，而140亿参数的大型模型更是低至4.5 FPS，迫使用户在响应速度与生成质量之间做出取舍。块级联技术通过免训练的并行化方案显著缓解了这一矛盾。我们的核心发现是：后续视频块的生成无需等待当前块完全去噪完成。通过基于前驱块部分去噪的上下文信息启动新块生成，我们将串行处理流程转变为多块同时去噪的并行级联系统。借助5张GPU实现时序并行计算，所有模型规模均实现约2倍加速：13亿模型从16 FPS提升至30 FPS，140亿模型从4.5 FPS提升至12.5 FPS。除推理速度提升外，块级联还消除了交互式生成中上下文切换时KV重缓存带来的约200毫秒开销。针对多种块因果流程的广泛评估表明，从传统块因果推理切换到块级联推理时，生成质量未出现显著下降。项目页面：https://hmrishavbandy.github.io/block_cascading_page/

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.20426) | [arXiv](https://arxiv.org/abs/2511.20426)



---

### 12. MobileVLA-R1：强化移动机器人的视觉-语言-动作协同框架

**原文标题：** MobileVLA-R1: Reinforcing Vision-Language-Action for Mobile Robots

**摘要：**
将自然语言指令落地实现为四足机器人的连续控制，始终是视觉-语言-动作领域的核心挑战。现有方法难以衔接高层语义推理与底层驱动控制，导致现实场景中的指令落地不稳定且泛化能力薄弱。为解决这些问题，我们提出MobileVLA-R1——一个支持四足机器人显式推理与连续控制的统一视觉-语言-动作框架。通过构建MobileVLA-CoT这一包含具身轨迹多粒度思维链的大规模数据集，为语义对齐提供结构化推理监督。在此基础上，我们引入结合监督式思维链对齐与GRPO强化学习的双阶段训练范式，显著提升推理一致性、控制稳定性和长周期任务执行能力。在VLN和VLA任务上的系统评估表明，本方法较现有基线模型实现约5%的性能提升。在四足机器人实体上的部署实验验证了其在复杂环境中的鲁棒表现。代码库：https://github.com/AIGeeksGroup/MobileVLA-R1 项目网站：https://aigeeksgroup.github.io/MobileVLA-R1

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.17889) | [arXiv](https://arxiv.org/abs/2511.17889)



---

### 13. 通过预测强化动作策略

**原文标题：** Reinforcing Action Policies by Prophesying

**摘要：**
视觉-语言-动作策略在语言、感知与机器人控制的协同方面表现卓越。然而大多数VLA策略仅通过模仿学习进行训练，这种方法会对示范数据产生过拟合，且在分布偏移时表现脆弱。强化学习通过直接优化任务奖励来解决此类错位问题，但真实机器人交互成本高昂，传统模拟器又难以开发与迁移。我们通过构建学习型世界模型和针对流式动作头优化的强化学习流程，同步解决VLA后训练阶段的数据效率与优化稳定性问题。具体而言，我们提出Prophet模型——一种基于大规模异构机器人数据预训练的统一动作到视频机器人执行框架，可学习可重用的动作-结果动态关系。该框架能够快速适应新的机器人、物体及环境，形成可直接部署的模拟系统。基于Prophet，我们通过适配VLA动作的流式动作GRPO算法，以及通过FlowScale梯度重缩放技术对流式动作头进行逐步梯度调整，共同强化动作策略。Prophet模型、FA-GRPO算法与FlowScale技术共同构成ProphRL框架，为VLA后训练提供了实用且节约数据计算资源的解决方案。实验表明，该方案在公开基准测试中实现5-17%的成功率提升，在不同VLA变体的真实机器人测试中取得24-30%的性能增益。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.20633) | [arXiv](https://arxiv.org/abs/2511.20633)



---

### 14. 基于轨迹采样对连续时间一致性的免图像时间步蒸馏方法

**原文标题：** Image-Free Timestep Distillation via Continuous-Time Consistency with Trajectory-Sampled Pairs

**摘要：**
时间步蒸馏是提升扩散模型生成效率的有效途径。一致性模型作为基于轨迹的框架，凭借其坚实的理论基础和高质量少步生成能力展现出巨大潜力。然而，当前连续时间一致性蒸馏方法仍严重依赖训练数据和计算资源，制约了其在资源受限场景的部署及跨领域扩展能力。针对此问题，我们提出轨迹反向一致性模型（TBCM），通过直接从教师模型生成轨迹中提取潜在表征，消除对外部训练数据的依赖。与需要VAE编码和大规模数据集的传统方法不同，我们的自包含蒸馏范式显著提升了效率与简洁性。此外，轨迹提取样本天然弥合了训练与推理间的分布差异，从而实现更有效的知识迁移。实验表明，在单步生成条件下，TBCM在MJHQ-30k数据集上达到6.52 FID和28.08 CLIP得分，相较Sana-Sprint缩短约40%训练时间并显著节省GPU显存，在保持质量的同时展现出卓越效率。我们进一步揭示了连续时间一致性蒸馏中的扩散-生成空间差异，分析了采样策略对蒸馏性能的影响，为未来蒸馏研究提供重要参考。项目地址：https://github.com/hustvl/TBCM。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.20410) | [arXiv](https://arxiv.org/abs/2511.20410)



---

### 15. SPHINX：面向视觉感知与推理的合成环境

**原文标题：** SPHINX: A Synthetic Environment for Visual Perception and Reasoning

**摘要：**
本文提出Sphinx——一个针对核心认知基元的视觉感知与推理合成环境。该系统通过程序化生成包含纹样、图块、图表、图标及几何基元的谜题，每个谜题均配有可验证的基准解，既能实现精准评估又可支持大规模数据集构建。该基准测试涵盖对称检测、几何变换、空间推理、图表解读与序列预测等25类任务。对近期大尺度视觉语言模型的评估表明，即便最先进的GPT-5模型准确率也仅为51.1%，远低于人类表现。最后我们验证了基于可验证奖励的强化学习能显著提升模型在这些任务上的准确率，并在外部视觉推理基准测试中产生增益，凸显了该方法对推进多模态推理发展的潜力。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.20814) | [arXiv](https://arxiv.org/abs/2511.20814)



---

### 16. 立场声明：完美AI对齐的复杂性——形式化RLHF三难困境

**原文标题：** Position: The Complexity of Perfect AI Alignment -- Formalizing the RLHF Trilemma

**摘要：**
基于人类反馈的强化学习（RLHF）被广泛用于对齐大型语言模型，但实践者始终面临一个难题：提升安全性往往损害公平性，扩展到多样化群体时计算复杂度难以处理，增强系统鲁棒性又会放大主流偏见。我们将其形式化为对齐三难困境：任何RLHF系统都无法同时实现（i）跨多元人类价值观的ε代表性，（ii）样本与计算复杂度的多项式可处理性，以及（iii）对抗性扰动与分布偏移的δ鲁棒性。通过融合统计学习理论与鲁棒优化的计算复杂性分析，我们证明要实现全球规模群体的代表性（ε≤0.01）与鲁棒性（δ≤0.001）需要Ω(2^{d_context})量级运算，这在上下文维度上呈超多项式增长。研究表明当前RLHF实施方案通过牺牲代表性来解决该困境：仅从同质化标注群体收集10^3-10^4样本，而真实全球代表性需要10^7-10^8样本。我们的框架为已记录的RLHF缺陷（包括偏好坍缩、谄媚效应和系统性偏见放大）提供了统一解释。最后提出通过策略性放宽对齐要求来应对这些根本性权衡的具体方向。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.19504) | [arXiv](https://arxiv.org/abs/2511.19504)



---

### 17. NAF：基于邻域注意力滤波的零样本特征上采样方法

**原文标题：** NAF: Zero-Shot Feature Upsampling via Neighborhood Attention Filtering

**摘要：**
视觉基础模型提取的空间降采样表征为像素级任务带来挑战。现有上采样方法面临根本性权衡：经典滤波器虽快速通用但依赖固定形式，而现代上采样器通过可学习的VFM专用形式实现更高精度，却需为每个VFM重新训练。我们提出邻域注意力滤波方法，通过跨尺度邻域注意力与旋转位置编码，仅以高分辨率输入图像为指导学习自适应空间-内容权重，成功弥合这一差距。NAF具备零样本特性：无需重新训练即可对任意VFM的特征进行上采样，成为首个在多个下游任务中超越VFM专用上采样器并达到最先进性能的VFM无关架构。该方法保持高效性，可扩展至2K特征图并以18FPS速率重建中间分辨率图。除特征上采样外，NAF在图像复原任务中也展现出卓越性能，体现了其多功能的特性。代码与检查点已发布于https://github.com/valeoai/NAF。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.18452) | [arXiv](https://arxiv.org/abs/2511.18452)



---

### 18. RAISECity：面向城市级现实对齐三维世界生成的多模态智能体框架

**原文标题：** RAISECity: A Multimodal Agent Framework for Reality-Aligned 3D World Generation at City-Scale

**摘要：**
城市级三维生成对于具身智能与世界模型的发展具有重要意义。然而现有方法在三维世界生成的质量、保真度与可扩展性方面面临重大挑战。为此，我们提出RAISECity——一个能够创建精细化城市级三维世界的现实对齐智能合成引擎。我们引入了一种智能体框架，通过利用多样化多模态基础工具获取现实世界知识，维护鲁棒的中间表征，并构建复杂三维场景。该智能体设计具备动态数据处理、迭代式自反思优化及先进多模态工具调用等特性，能有效减少累积误差并提升整体性能。大量定量实验与定性分析表明，RAISECity在现实对齐度、几何精度、纹理保真度与美学水准方面均表现优异，在整体感知质量评估中以超过90%的胜率领先现有基线方法。这种集三维质量、现实对齐性、可扩展性及与计算机图形管线无缝兼容于一体的特性，使RAISECity成为沉浸式媒体、具身智能与世界模型应用的理想基础平台。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.18005) | [arXiv](https://arxiv.org/abs/2511.18005)



---

### 19. I-GLIDE：退化估计中潜在健康指标的输入分组方法

**原文标题：** I-GLIDE: Input Groups for Latent Health Indicators in Degradation Estimation

**摘要：**
准确的剩余使用寿命预测取决于健康指标的质量，然而现有方法往往难以解耦多传感器系统中的复杂退化机制，也无法量化健康指标可靠性的不确定性。本文提出了一种创新的健康指标构建框架，具有三个关键贡献：首先，我们首次将投影路径重构方法改造为适用于剩余使用寿命预测的健康指标，证明其性能优于传统重构误差度量；其次，通过蒙特卡洛丢弃法和概率潜在空间实现认知与随机不确定性量化，显著增强了基于投影路径重构的健康指标在剩余使用寿命预测中的鲁棒性；最后，我们提出指标分组这一创新范式，通过分离传感器子集来建模系统特定退化机制，由此形成名为I-GLIDE的新方法，可实现可解释的机制特异性诊断。在航空航天与制造系统数据上的实验表明，相较于最先进的健康指标方法，本方案在预测精度与泛化能力方面均取得显著提升，同时为系统失效路径提供了可操作的洞见。该研究填补了异常检测与预测性维护之间的空白，为复杂系统中的不确定性感知退化建模提供了理论框架。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.21208) | [arXiv](https://arxiv.org/abs/2511.21208)



---

### 20. 基于频率自适应锐度正则化的三维高斯溅射泛化能力提升方法

**原文标题：** Frequency-Adaptive Sharpness Regularization for Improving 3D Gaussian Splatting Generalization

**摘要：**
尽管三维高斯溅射（3DGS）在多数配置中表现卓越，但在稀疏观测场景下由于对有限样本的过拟合，其在新视角下的泛化能力存在不足。本文从机器学习视角重新审视3DGS优化过程，将新视角合成问题定义为对未观测视角的泛化问题——这一研究方向尚未得到充分探索。我们提出频率自适应锐度正则化方法（FASR），通过重构3DGS训练目标函数，引导3DGS收敛至具有更优泛化能力的解。虽然锐度感知最小化（SAM）方法同样通过降低损失景观的锐度来提升分类模型的泛化能力，但由于任务差异，直接将其应用于3DGS会导致次优结果。具体而言，过强的正则化会阻碍高频细节重建，而减弱正则化强度又会导致锐度惩罚不足。为解决该问题，我们根据图像局部频率特性动态设置正则化权重及局部锐度估计的邻域半径。该方法既能有效抑制新视角下的浮游伪影，又能重建SAM方法容易过度平滑的精细细节。在多种配置的数据集测试中，本方法持续提升了各类基线的性能表现。代码将在https://bbangsik13.github.io/FASR 发布。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.17918) | [arXiv](https://arxiv.org/abs/2511.17918)



---


## 🔍 关键词云图
![关键词云图](../images/keywords_wordcloud.png)

## 📈 近期论文趋势
![论文趋势](../images/daily_papers.png)

## 🎙️ 语音播报
- [收听今日论文解读](../audio/2025-11-27_daily_papers.mp3)

## 📱 订阅渠道
- GitHub: [hf-daily-paper-newsletter-chinese](https://github.com/2404589803/hf-daily-paper-newsletter-chinese)