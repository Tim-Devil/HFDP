<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Hugging Face 论文日报 - 2025-12-29</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', sans-serif;
            line-height: 1.6;
            color: #333;
            background-color: #f5f5f5;
            padding: 20px;
        }
        
        .container {
            max-width: 900px;
            margin: 0 auto;
            background-color: white;
            padding: 40px;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }
        
        h1 {
            color: #2c3e50;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
            margin-bottom: 30px;
            font-size: 28px;
        }
        
        h1 img {
            vertical-align: middle;
            margin-right: 10px;
        }
        
        h2 {
            color: #34495e;
            margin-top: 40px;
            margin-bottom: 20px;
            font-size: 24px;
            border-left: 4px solid #3498db;
            padding-left: 15px;
        }
        
        h3 {
            color: #2c3e50;
            margin-top: 30px;
            margin-bottom: 15px;
            font-size: 20px;
        }
        
        p {
            margin-bottom: 15px;
            text-align: justify;
        }
        
        ul {
            margin-left: 20px;
            margin-bottom: 20px;
        }
        
        li {
            margin-bottom: 8px;
        }
        
        a {
            color: #3498db;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        strong {
            color: #2c3e50;
            font-weight: 600;
        }
        
        hr {
            border: none;
            border-top: 1px solid #e0e0e0;
            margin: 30px 0;
        }
        
        /* 关键修复:限制图片宽度 */
        img {
            max-width: 100%;
            height: auto;
            display: block;
            margin: 20px auto;
            border-radius: 4px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        
        /* 确保图片容器也有宽度限制 */
        p img {
            max-width: 100%;
        }
        
        /* 论文详情区域样式 */
        .paper-section {
            background-color: #f9f9f9;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 20px;
        }
        
        /* 统计信息样式 */
        .stats {
            background-color: #e8f4f8;
            padding: 15px;
            border-radius: 6px;
            margin-bottom: 20px;
        }
        
        /* 响应式设计 */
        @media (max-width: 768px) {
            .container {
                padding: 20px;
            }
            
            h1 {
                font-size: 24px;
            }
            
            h2 {
                font-size: 20px;
            }
            
            h3 {
                font-size: 18px;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <h1><img src="https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo.png" width="30"/> Hugging Face 2025-12-29 论文日报</h1>
<h2>📊 今日论文统计</h2>
<ul>
<li>总论文数：15</li>
<li>热门领域：Vision, RL</li>
</ul>
<h2>📝 论文详情</h2>
<h3>1. InsertAnywhere：融合4D场景几何与扩散模型以实现逼真的视频对象插入</h3>
<p><strong>原文标题：</strong> InsertAnywhere: Bridging 4D Scene Geometry and Diffusion Models for Realistic Video Object Insertion</p>
<p><strong>摘要：</strong>
基于扩散模型的视频生成技术的最新进展为可控视频编辑开辟了新的可能性，然而，由于对4D场景理解的局限以及对遮挡和光照效果处理不足，实现逼真的视频对象插入仍然面临挑战。本文提出InsertAnywhere，一种新的视频对象插入框架，能够实现几何一致的对象放置和外观保真的视频合成。我们的方法始于一个4D感知掩码生成模块，该模块重建场景几何结构，并在保持时间连贯性和遮挡一致性的同时，将用户指定的对象放置跨帧传播。在此空间基础上，我们扩展了一种基于扩散的视频生成模型，以联合合成插入的对象及其周围的局部变化（如光照和阴影）。为了支持有监督训练，我们引入了ROSE++，这是一个光照感知的合成数据集，通过将ROSE对象移除数据集转换为对象移除视频、对象存在视频以及由视觉语言模型生成的参考图像的三元组而构建。通过大量实验，我们证明该框架能够在多样化的真实场景中生成几何合理且视觉连贯的对象插入效果，显著优于现有研究和商业模型。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.17504">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.17504">arXiv</a></p>
<hr />
<h3>2. 基于心智图景感知的检索增强生成技术提升长文本理解能力</h3>
<p><strong>原文标题：</strong> Mindscape-Aware Retrieval Augmented Generation for Improved Long Context Understanding</p>
<p><strong>摘要：</strong>
人类通过构建内容的整体语义表征来理解长篇复杂文本。心理学研究揭示的人类心智图景感知能力表明，这种全局视角有助于组织先验知识、解读新信息并整合散布在文档中的证据。当前检索增强生成系统缺乏此类引导机制，因此在长文本处理任务中表现受限。本文提出心智图景感知的检索增强生成方法，首次为基于大语言模型的检索增强生成系统赋予显式的全局上下文感知能力。该方法通过分层摘要构建心智图景，并基于该全局语义表征同步优化检索与生成过程。这种设计使检索器能够形成增强的查询嵌入表示，同时使生成器能够在连贯的全局语境中对检索证据进行推理。我们在多类长文本及双语基准测试中评估该方法在证据理解与全局语义构建方面的性能。实验表明该方法持续超越基线模型，进一步分析显示其能够将局部细节与连贯的全局表征相融合，从而实现更接近人类认知模式的长文本检索与推理。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.17220">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.17220">arXiv</a></p>
<hr />
<h3>3. MAI-UI技术报告：面向真实世界的通用图形用户界面智能体</h3>
<p><strong>原文标题：</strong> MAI-UI Technical Report: Real-World Centric Foundation GUI Agents</p>
<p><strong>摘要：</strong>
图形用户界面（GUI）智能体的发展有望彻底革新下一代人机交互。基于这一愿景，我们提出了MAI-UI——一个涵盖全尺寸谱系（包括2B、8B、32B及235B-A22B变体）的通用GUI智能体家族。我们识别出现实部署面临的四大关键挑战：缺乏原生智能体-用户交互、纯UI操作的限制、实用部署架构的缺失，以及在动态环境中的脆弱性。MAI-UI通过一套统一方法论应对这些问题：采用自演进数据管道将导航数据扩展至包含用户交互与MCP工具调用；设计原生设备-云协作系统，依据任务状态路由执行流程；并构建具备先进优化能力的在线强化学习框架，以扩展并行环境规模与上下文长度。MAI-UI在GUI基础任务与移动导航任务上均取得了最先进的性能。在基础任务基准测试中，其在ScreenSpot-Pro达到73.5%，在MMBench GUI L2达到91.3%，在OSWorld-G达到70.9%，在UI-Vision达到49.2%，其中ScreenSpot-Pro成绩超越Gemini-3-Pro与Seed1.8。在移动GUI导航任务中，其于AndroidWorld创下76.7%的新SOTA记录，超越UI-Tars-2、Gemini-2.5-Pro及Seed1.8；在MobileWorld上获得41.7%的成功率，显著优于端到端GUI模型，并与基于Gemini-3-Pro的智能体框架性能相当。我们的在线强化学习实验表明，将并行环境从32扩展至512可带来5.2个百分点的性能提升，将环境步数预算从15增加至50可带来4.3个百分点的提升。最后，原生设备-云协作系统使设备端性能提升33%，云端模型调用减少超40%，同时有效保障了用户隐私。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.22047">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.22047">arXiv</a></p>
<hr />
<h3>4. UniPercept：面向美学、质量、结构与纹理的统一感知级图像理解</h3>
<p><strong>原文标题：</strong> UniPercept: Towards Unified Perceptual-Level Image Understanding across Aesthetics, Quality, Structure, and Texture</p>
<p><strong>摘要：</strong>
多模态大语言模型在视觉定位、分割与描述等视觉理解任务中取得了显著进展，但其对感知级图像特征的认知能力仍存在局限。本研究提出UniPercept-Bench，一个跨美学、质量、结构与纹理三大关键领域的统一感知级图像理解框架。我们建立了层次化定义体系并构建大规模数据集以评估感知级图像理解能力。在此基础上，通过领域自适应预训练与任务对齐强化学习，开发出具有强泛化能力的基线模型UniPercept，该模型在视觉评分与视觉问答任务中均表现优异。UniPercept在感知级图像理解任务上超越现有多模态大语言模型，并可作为即插即用的奖励模型服务于文本到图像生成任务。本研究在多模态大语言模型时代明确定义了感知级图像理解，并通过构建综合性基准与强基线模型，为推进感知级多模态图像理解研究奠定了坚实基础。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.21675">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.21675">arXiv</a></p>
<hr />
<h3>5. ProEdit：基于提示的反演编辑方法优化研究</h3>
<p><strong>原文标题：</strong> ProEdit: Inversion-based Editing From Prompts Done Right</p>
<p><strong>摘要：</strong>
基于反演的视觉编辑技术为用户指令驱动的图像或视频编辑提供了一种高效且无需训练的方法。现有方法通常在采样过程中注入源图像信息以保持编辑一致性，但该采样策略过度依赖源信息，会对目标图像的编辑效果产生负面影响（例如无法按指令改变主体的姿态、数量或颜色等属性）。本研究提出ProEdit方法，从注意力机制与潜在空间两个维度解决该问题。在注意力机制方面，我们提出KV混合技术，通过在编辑区域混合源图像与目标图像的键值特征，在保持背景一致性的同时减弱源图像对编辑区域的影响。在潜在空间方面，我们提出潜在偏移技术，通过对源潜在空间的编辑区域施加扰动，消除反演潜在向量对采样过程的影响。在多个图像与视频编辑基准测试上的大量实验表明，本方法达到了当前最优性能。此外，本设计具备即插即用特性，可无缝集成至现有反演与编辑方法（如RF-Solver、FireFlow和UniEdit）中。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.22118">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.22118">arXiv</a></p>
<hr />
<h3>6. TimeBill：面向大语言模型的预算时间推理框架</h3>
<p><strong>原文标题：</strong> TimeBill: Time-Budgeted Inference for Large Language Models</p>
<p><strong>摘要：</strong>
大语言模型正日益应用于时间敏感系统，如机器人、自动驾驶、具身智能和工业自动化等领域。在这些场景中，在给定时间预算内生成准确响应对于决策、控制或安全关键任务至关重要。然而，大语言模型的自回归生成特性使其端到端执行时间的建模与估计面临挑战。此外，现有基于固定键值缓存淘汰比例的高效推理方法难以适应具有不同时间预算的多样化任务，不恰当的淘汰比例可能导致推理不完整或响应性能下降。本文提出TimeBill，一种新颖的面向大语言模型的预算时间推理框架，旨在平衡推理效率与响应性能。具体而言，我们设计了细粒度响应长度预测器与执行时间估计器，以精准预测大语言模型的端到端执行时间。在此基础上，开发了一种预算时间高效推理方法，能够根据执行时间预测与给定时间预算自适应调整键值缓存淘汰比例。最后，通过大量实验验证了TimeBill在多种超时策略下提升任务完成率并保持响应性能的优势。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.21859">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.21859">arXiv</a></p>
<hr />
<h3>7. Omni-Weather：面向天气生成与理解的多模态统一基础模型</h3>
<p><strong>原文标题：</strong> Omni-Weather: Unified Multimodal Foundation Model for Weather Generation and Understanding</p>
<p><strong>摘要：</strong>
天气建模既需要精确的预测，也需要机理性的解释，然而现有方法将这两个目标割裂处理，将生成与理解分离开来。为弥补这一不足，我们提出了Omni-Weather，这是首个将天气生成与理解统一于单一架构内的多模态基础模型。Omni-Weather集成了用于天气生成任务的雷达编码器，并采用共享的自注意力机制进行统一处理。此外，我们构建了一个用于天气生成因果推理的思维链数据集，以实现可解释的输出并提升感知质量。大量实验表明，Omni-Weather在天气生成与理解两方面均达到了最先进的性能。我们的研究进一步表明，天气领域的生成任务与理解任务能够相互促进。Omni-Weather也证明了统一天气生成与理解的可行性与价值。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.21643">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.21643">arXiv</a></p>
<hr />
<h3>8. 少看而精看：面向多模态推理的双向感知塑造</h3>
<p><strong>原文标题：</strong> See Less, See Right: Bi-directional Perceptual Shaping For Multimodal Reasoning</p>
<p><strong>摘要：</strong>
大型视觉语言模型（VLMs）通常受益于中间视觉线索的辅助，这些线索或通过外部工具注入，或在推理过程中生成为潜在视觉标记。然而，现有机制仍存在以下局限：忽视细粒度视觉证据（如图表中的折线）、跨领域泛化能力不足，且推理时计算成本高昂。本文提出双向感知塑造方法，该方法将问题引导的掩码视图转化为双向的“关注何处”信号，从而在训练过程中塑造模型的感知能力。BiPS首先在原始图像与仅保留问题相关区域的证据保留视图之间施加KL一致性约束，以鼓励模型对支持性像素进行粗略但完整的覆盖。随后，在原始图像与关键像素被掩码的证据消除视图之间施加KL分离约束，该视图因掩码操作而不再支持原始答案，从而抑制仅依赖文本的捷径策略（即仅从文本中获取答案），并强化模型对细粒度视觉信息的依赖。在八个基准测试中，BiPS将Qwen2.5-VL-7B模型的平均性能提升了8.2%，并在未见过的数据集和图像类型上展现出强大的跨领域泛化能力。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.22120">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.22120">arXiv</a></p>
<hr />
<h3>9. InSight-o3：通过广义视觉搜索增强多模态基础模型能力</h3>
<p><strong>原文标题：</strong> InSight-o3: Empowering Multimodal Foundation Models with Generalized Visual Search</p>
<p><strong>摘要：</strong>
人工智能系统“以图像进行思考”的能力需要推理与感知的深度融合。然而，当前开源的智能体在推理能力方面仍存在明显不足，而这对于分析包含密集图表/图示的文档、地图导航等现实任务至关重要。为弥补这一差距，我们提出了O3-Bench——一个专门用于评估多模态推理能力、并强调对视觉细节交错关注的新基准。该基准包含一系列具有挑战性的问题，要求智能体通过多步推理整合来自图像不同区域的细微视觉信息。即使对于OpenAI o3等前沿系统，这些问题也极具挑战性，其在O3-Bench上的准确率仅为40.8%。为推进此领域发展，我们提出了InSight-o3，这是一个由视觉推理智能体（vReasoner）与视觉搜索智能体（vSearcher）构成的多智能体框架。我们为此框架引入了广义视觉搜索任务——其目标不仅是定位自然图像中的简单物体或图形，更在于根据自由形式语言描述，定位具有关联性、模糊性或概念性的图像区域。随后，我们通过强化学习训练了一个专为此任务设计的模态大语言模型。作为即插即用模块，我们的vSearcher能够有效增强前沿多模态模型（作为vReasoner），显著提升它们在多种基准测试上的性能。这标志着我们在构建强大的类o3开源系统方面迈出了坚实一步。相关代码与数据集可在 https://github.com/m-Just/InSight-o3 获取。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.18745">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.18745">arXiv</a></p>
<hr />
<h3>10. SWE-RM：面向软件工程智能体的免执行反馈</h3>
<p><strong>原文标题：</strong> SWE-RM: Execution-free Feedback For Software Engineering Agents</p>
<p><strong>摘要：</strong>
基于执行的反馈（如单元测试）通过测试时扩展（TTS）和强化学习（RL）被广泛用于编码智能体的开发。该范式需要可扩展且可靠的单元测试用例收集以提供准确反馈，但由此产生的反馈往往较为稀疏，且难以有效区分同为成功或同为失败的执行轨迹。相比之下，来自奖励模型的免执行反馈能够在不依赖单元测试用例的情况下提供更细粒度的信号。尽管具备这一潜力，针对实际软件工程（SWE）智能体的免执行反馈研究仍显不足。为开发在TTS和RL中均有效的通用奖励模型，我们观察到两个在TTS性能上几乎相同的验证器在RL中可能产生截然不同的结果。直观而言，TTS主要反映模型选择最佳轨迹的能力，但该能力未必能泛化至RL场景。为克服这一局限，我们识别出对RL训练至关重要的两个额外维度：分类准确性与校准性。随后，我们通过全面的对照实验探究如何训练一个在这些指标上均表现稳健的奖励模型，特别分析了训练数据规模、策略混合方式及数据源构成等多种因素的影响。基于这些研究，我们提出了SWE-RM——一个采用专家混合架构的精准且鲁棒的奖励模型，其总参数量为300亿，推理时激活参数量为30亿。SWE-RM显著提升了SWE智能体在TTS和RL上的性能表现：例如在SWE-Bench Verified基准测试中，通过TTS将Qwen3-Coder-Flash的准确率从51.6%提升至62.0%，将Qwen3-Coder-Max的准确率从67.0%提升至74.6%，在开源模型中实现了新的最优性能。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.21919">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.21919">arXiv</a></p>
<hr />
<h3>11. SlideTailor：面向科研论文的个性化演示文稿幻灯片生成系统</h3>
<p><strong>原文标题：</strong> SlideTailor: Personalized Presentation Slide Generation for Scientific Papers</p>
<p><strong>摘要：</strong>
自动演示文稿幻灯片生成技术能够显著简化内容创作流程。然而，由于不同用户的偏好存在差异，现有研究中定义模糊的生成框架往往产生不符合个性化需求的结果。本文提出一种基于用户偏好条件的论文到幻灯片生成新任务，并设计了一种受人类行为启发的智能体框架——SlideTailor，该框架能够以适应用户偏好的方式逐步生成可编辑的幻灯片。与要求用户以详细文本形式描述偏好的传统方法不同，本系统仅需用户提供一对论文-幻灯片示例及一个视觉模板——这些自然且易于获取的素材隐式编码了用户在内容组织与视觉风格层面的丰富偏好。尽管输入信息具有隐式且无标注的特性，本框架仍能有效提炼并泛化用户偏好，从而指导定制化幻灯片的生成。此外，我们创新性地引入语音链式机制，使幻灯片内容与预设的口头讲述规划保持协同。这一设计显著提升了生成幻灯片的质量，并为视频演示等下游应用提供了支持。为推进该新任务的研究，我们构建了一个涵盖多样化用户偏好的基准数据集，并设计了具有可解释性的评估指标以进行鲁棒性验证。大量实验结果表明，本框架在个性化幻灯片生成方面具有显著优势。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.20292">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.20292">arXiv</a></p>
<hr />
<h3>12. SVBench：视频生成模型在社会推理能力上的评估</h3>
<p><strong>原文标题：</strong> SVBench: Evaluation of Video Generation Models on Social Reasoning</p>
<p><strong>摘要：</strong>
当前的文本到视频生成模型在视觉真实性、运动流畅度以及文本-视频对齐方面取得了显著进展，但其生成社会性连贯行为的能力仍存在根本性局限。与人类能够轻松从短暂视觉线索中推断意图、信念、情感和社会规范不同，现有模型往往仅呈现表面场景，而未能捕捉背后的因果或心理逻辑。为系统评估这一差距，我们首次提出了针对视频生成中社会推理能力的评测基准。基于发展心理学与社会心理学的研究成果，本基准将三十个经典社会认知范式归纳为七个核心维度，包括心理状态推断、目标导向行为、共同注意、社会协调、亲社会行为、社会规范以及多智能体策略。为实现这些范式的可操作化，我们开发了一套完全无需训练的基于智能体的流程，该流程能够：（一）提炼每个实验的推理机制；（二）合成多样化的视频适用场景；（三）通过基于线索的批判机制确保概念中立性与难度控制；（四）利用高性能视觉语言模型作为评判者，从社会推理的五个可解释维度对生成视频进行评估。基于此框架，我们对七种前沿视频生成系统进行了首次大规模研究。结果表明存在显著的性能差距：尽管现代模型在表层合理性方面表现优异，但在意图识别、信念推理、共同注意和亲社会行为推断等维度上均存在系统性缺陷。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.21507">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.21507">arXiv</a></p>
<hr />
<h3>13. 基于可验证奖励的强化学习中样本极性问题的再思考</h3>
<p><strong>原文标题：</strong> Rethinking Sample Polarity in Reinforcement Learning with Verifiable Rewards</p>
<p><strong>摘要：</strong>
大型推理模型通常通过基于可验证奖励的强化学习进行训练，以提升其推理能力。在该范式下，策略更新同时使用正负两种自生成轨迹，这两种轨迹对应着不同的样本极性。本文系统研究了样本极性如何影响基于可验证奖励的强化学习的训练动态与行为表现。研究发现，正样本能够强化已有的正确推理模式，而负样本则有助于探索新的推理路径。我们进一步探讨了在样本层面和词元层面对正负样本优势值进行调整如何影响训练过程。基于这些发现，我们提出了一种面向策略优化的自适应非对称词元级优势塑造方法（A3PO），该方法能够更精准地针对不同极性的关键词元分配优势信号。在五个推理基准测试上的实验验证了该方法的有效性。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.21625">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.21625">arXiv</a></p>
<hr />
<h3>14. 通用3x3矩阵乘法的58次加法、秩23算法方案</h3>
<p><strong>原文标题：</strong> A 58-Addition, Rank-23 Scheme for General 3x3 Matrix Multiplication</p>
<p><strong>摘要：</strong>
本文提出了一种针对一般非交换环上精确3×3矩阵乘法的最新算法，该算法仅需58次标量加法即可实现秩23的计算方案。在不改变基的情况下，此结果将先前最佳的加法复杂度从60次加法进一步降低。该算法是通过结合三元限制翻转图探索与贪婪交集约简以消除公共子表达式的自动化搜索所发现的。所得方案仅使用系数集{-1, 0, 1}，确保了算法在任意域上的高效性与可移植性。标量运算总次数从83次减少至81次。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.21980">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.21980">arXiv</a></p>
<hr />
<h3>15. 遮蔽教师与强化学生：视觉语言模型的知识蒸馏方法</h3>
<p><strong>原文标题：</strong> Masking Teacher and Reinforcing Student for Distilling Vision-Language Models</p>
<p><strong>摘要：</strong>
大规模视觉语言模型（VLMs）近期在多模态理解方面取得了显著成就，但其庞大的参数量使其难以部署于移动或边缘设备。这催生了对紧凑且高性能VLMs的需求，这类模型需能够高效地从强大的大型教师模型中学习。然而，由于师生模型间巨大的规模差异，将知识从大型教师模型蒸馏至小型学生模型仍面临挑战：学生模型往往难以复现教师模型复杂的高维表示，导致学习过程不稳定且性能下降。为解决这一问题，我们提出Masters（遮蔽教师与强化学生）框架——一种基于掩码渐进强化学习（RL）的蒸馏方法。Masters首先遮蔽教师模型中的非主导权重以降低不必要的复杂度，随后在训练过程中通过逐步恢复教师模型容量来实现渐进式知识传递。该策略使学生模型能够以平稳、稳定的方式从教师模型中学习更丰富的表示。为进一步优化知识迁移，Masters整合了离线强化学习阶段，包含两种互补奖励机制：衡量生成响应正确性的准确度奖励，以及量化从教师到学生响应迁移难度的蒸馏奖励。与计算成本高昂且生成冗长响应的在线“思考-回答”强化学习范式不同，我们的离线强化学习利用来自遮蔽教师模型的预生成响应。这些响应提供了丰富而高效的指导，使学生模型无需经过“思考-回答”过程即可实现强劲性能。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.22238">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.22238">arXiv</a></p>
<hr />
<h2>🔍 关键词云图</h2>
<p><img alt="关键词云图" src="../images/keywords_wordcloud.png" /></p>
<h2>📈 近期论文趋势</h2>
<p><img alt="论文趋势" src="../images/daily_papers.png" /></p>
<h2>🎙️ 语音播报</h2>
<ul>
<li><a href="../audio/2025-12-29_daily_papers.mp3">收听今日论文解读</a></li>
</ul>
<h2>📱 订阅渠道</h2>
<ul>
<li>GitHub: <a href="https://github.com/2404589803/hf-daily-paper-newsletter-chinese">hf-daily-paper-newsletter-chinese</a></li>
</ul>
    </div>
</body>
</html>