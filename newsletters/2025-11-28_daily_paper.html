<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Hugging Face 论文日报 - 2025-11-28</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', sans-serif;
            line-height: 1.6;
            color: #333;
            background-color: #f5f5f5;
            padding: 20px;
        }
        
        .container {
            max-width: 900px;
            margin: 0 auto;
            background-color: white;
            padding: 40px;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }
        
        h1 {
            color: #2c3e50;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
            margin-bottom: 30px;
            font-size: 28px;
        }
        
        h1 img {
            vertical-align: middle;
            margin-right: 10px;
        }
        
        h2 {
            color: #34495e;
            margin-top: 40px;
            margin-bottom: 20px;
            font-size: 24px;
            border-left: 4px solid #3498db;
            padding-left: 15px;
        }
        
        h3 {
            color: #2c3e50;
            margin-top: 30px;
            margin-bottom: 15px;
            font-size: 20px;
        }
        
        p {
            margin-bottom: 15px;
            text-align: justify;
        }
        
        ul {
            margin-left: 20px;
            margin-bottom: 20px;
        }
        
        li {
            margin-bottom: 8px;
        }
        
        a {
            color: #3498db;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        strong {
            color: #2c3e50;
            font-weight: 600;
        }
        
        hr {
            border: none;
            border-top: 1px solid #e0e0e0;
            margin: 30px 0;
        }
        
        /* 关键修复:限制图片宽度 */
        img {
            max-width: 100%;
            height: auto;
            display: block;
            margin: 20px auto;
            border-radius: 4px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        
        /* 确保图片容器也有宽度限制 */
        p img {
            max-width: 100%;
        }
        
        /* 论文详情区域样式 */
        .paper-section {
            background-color: #f9f9f9;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 20px;
        }
        
        /* 统计信息样式 */
        .stats {
            background-color: #e8f4f8;
            padding: 15px;
            border-radius: 6px;
            margin-bottom: 20px;
        }
        
        /* 响应式设计 */
        @media (max-width: 768px) {
            .container {
                padding: 20px;
            }
            
            h1 {
                font-size: 24px;
            }
            
            h2 {
                font-size: 20px;
            }
            
            h3 {
                font-size: 18px;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <h1><img src="https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo.png" width="30"/> Hugging Face 2025-11-28 论文日报</h1>
<h2>📊 今日论文统计</h2>
<ul>
<li>总论文数：7</li>
<li>热门领域：GPT</li>
</ul>
<h2>📝 论文详情</h2>
<h3>1. 视频生成模型具备优异的潜在奖励建模能力</h3>
<p><strong>原文标题：</strong> Video Generation Models Are Good Latent Reward Models</p>
<p><strong>摘要：</strong>
奖励反馈学习在图像生成与人类偏好对齐方面已展现显著成效，但其在视频生成领域的扩展面临重大挑战。现有视频奖励模型依赖为像素空间输入设计的视觉语言模型，将奖励反馈学习优化限制在计算成本高昂的VAE解码后接近完成的去噪阶段。这种像素空间方法存在内存开销大、训练时间长等局限性，且其后期优化缺乏早期监督机制，仅能改善视觉质量而无法优化基础运动动态与结构连贯性。本研究表明，预训练视频生成模型天然适用于噪声潜在空间的奖励建模，因其专为处理任意时间步的噪声潜在表示而设计，并通过序列建模能力固有地保持时序信息。据此，我们提出过程奖励反馈学习框架，该框架完全在潜在空间执行偏好优化，无需VAE解码即可实现贯穿全去噪链的高效梯度反向传播。大量实验表明，相较于RGB空间的奖励反馈学习，该框架在显著提升人类偏好对齐度的同时，可实现内存消耗与训练时间的大幅降低。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.21541">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.21541">arXiv</a></p>
<hr />
<h3>2. 画布到图像：基于多模态控制的组合式图像生成</h3>
<p><strong>原文标题：</strong> Canvas-to-Image: Compositional Image Generation with Multimodal Controls</p>
<p><strong>摘要：</strong>
尽管现代扩散模型在生成高质量多样化图像方面表现卓越，但在处理高保真度的组合式与多模态控制时仍面临挑战，特别是当用户需要同时指定文本提示、主体参照、空间布局、姿态约束和版面标注时。我们提出“画布到图像”这一统一框架，将异构控制信号整合至单一画布界面，使用户能够生成精准反映创作意图的图像。核心创新在于将多样化控制信号编码为模型可直接解析的复合画布图像，从而实现集成的视觉空间推理。我们进一步构建了多任务数据集组合，并提出多任务画布训练策略，通过统一学习范式优化扩散模型对异构控制信号的理解与融合能力。这种联合训练使模型能够进行跨模态推理，而非依赖特定任务的启发式方法，并在推理阶段对多控制场景展现出卓越的泛化能力。大量实验表明，在多人组合、姿态控制合成、布局约束生成及多控制生成等挑战性基准测试中，Canvas-to-Image在身份保持与控制遵循方面显著优于现有前沿方法。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.21691">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.21691">arXiv</a></p>
<hr />
<h3>3. MIRA：面向图像编辑的多模态迭代推理智能体</h3>
<p><strong>原文标题：</strong> MIRA: Multimodal Iterative Reasoning Agent for Image Editing</p>
<p><strong>摘要：</strong>
指令引导的图像编辑为用户提供了一种通过自然语言编辑图像的直观方式。然而，基于扩散的编辑模型往往难以准确解析复杂的用户指令，特别是涉及组合关系、上下文线索或指代表达的指令，导致编辑结果出现语义偏差或无法体现预期修改。针对这一问题，我们提出MIRA（多模态迭代推理智能体）——一种轻量级即插即用的多模态推理智能体，通过感知-推理-行动的迭代循环执行编辑任务，有效模拟了多轮人机交互过程。与直接生成单次提示或静态方案不同，MIRA通过逐步预测原子化编辑指令，并利用视觉反馈进行决策优化。我们构建的15万规模多模态工具使用数据集MIRA-Editing，结合两阶段SFT+GRPO训练流程，使MIRA能够对复杂编辑指令执行推理与编辑。当与Flux.1-Kontext、Step1X-Edit、Qwen-Image-Edit等开源图像编辑模型配合使用时，MIRA在语义一致性和感知质量方面均实现显著提升，其性能达到甚至超越GPT-Image、Nano-Banana等专有系统。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.21087">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.21087">arXiv</a></p>
<hr />
<h3>4. ENACT：基于自我中心交互世界建模的具身认知评估框架</h3>
<p><strong>原文标题：</strong> ENACT: Evaluating Embodied Cognition with World Modeling of Egocentric Interaction</p>
<p><strong>摘要：</strong>
具身认知理论主张智能产生于感觉运动交互而非被动观察。这引发了一个耐人寻味的问题：主要以非具身方式训练的现代视觉语言模型是否展现出具身认知的特征？我们提出ENACT基准测试，通过视觉问答形式将具身认知评估构建为基于自我中心交互的世界建模任务。该框架将交互过程建模为动作即场景图变化的部分可观测马尔可夫决策过程，包含两项互补的序列重组任务：前向世界建模（根据动作重排乱序观察）与逆向世界建模（根据观察重排乱序动作）。这些任务虽概念简洁，但求解过程隐式要求模型具备具身认知的核心能力——可供性识别、动作效果推理、具身意识，以及基于部分可观测自我中心输入的交互式长程记忆，同时规避可能干扰评估的低层次图像合成。我们开发了可扩展的生成流程，从机器人仿真环境（BEHAVIOR）合成问答对，构建包含8,972个问答样本的家庭场景长程活动数据集。实验表明前沿视觉语言模型与人类表现存在差距，且该差距随交互跨度增加而扩大。模型在逆向任务中表现始终优于前向任务，并呈现出人类中心偏见——包括偏好右利手动作，以及当相机参数或视角偏离人类视觉时性能显著下降。项目网站详见https://enact-embodied-cognition.github.io/。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.20937">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.20937">arXiv</a></p>
<hr />
<h3>5. Multi-Crit：基于多元化标准遵循的多模态评估基准测试</h3>
<p><strong>原文标题：</strong> Multi-Crit: Benchmarking Multimodal Judges on Pluralistic Criteria-Following</p>
<p><strong>摘要：</strong>
大型多模态模型因其强大的指令遵循能力以及与人类偏好的一致性，正日益被用作多模态评估系统的评判工具。然而，这些模型在遵循多样化、细粒度评估标准方面的能力仍有待深入探索。本研究开发了Multi-Crit基准测试，用于评估多模态评判模型在遵循多元化标准并生成可靠标准级判断的能力。该基准涵盖开放式生成与可验证推理两类任务，通过严格的数据筛选流程构建，收集了具有多标准人工标注的挑战性响应对，并创新性地提出三项评估指标：系统性评估多元标准遵循度、标准切换灵活性以及识别标准级偏好冲突的能力。对25个大型多模态模型的综合分析表明：1）专有模型在保持对多元标准的一致性遵循方面仍存在困难——尤其在开放式评估场景；2）开源模型在灵活遵循多样化标准方面差距更为显著；3）基于整体判断信号的批评微调虽能增强视觉基础能力，但无法泛化至多元标准级判断。针对推理微调、测试时扩展以及开源与专有模型边界一致性的补充分析，进一步揭示了当前多模态评判模型的局限性。作为开创性研究，Multi-Crit为构建可靠且可调控的多模态人工智能评估体系奠定了重要基础。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.21662">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.21662">arXiv</a></p>
<hr />
<h3>6. 理解语言意味着什么？</h3>
<p><strong>原文标题：</strong> What does it mean to understand language?</p>
<p><strong>摘要：</strong>
语言理解不仅需要提取语言输入的表层意义，更在于构建描述情境的丰富心理模型。本文提出，由于大脑核心语言系统的处理能力存在根本局限，深度理解语言需要将信息从语言系统输出至其他脑区——这些区域负责处理感知与运动表征、构建心理模型，并存储我们的世界知识与自传体记忆。我们系统回顾了支持该假说的现有证据，指出认知神经科学的最新进展既为其提供了理论基础，也创造了直接验证的研究方法，从而开辟了新途径以揭示理解语言在认知与神经层面的本质内涵。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.19757">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.19757">arXiv</a></p>
<hr />
<h3>7. 具身化学习者：基于生长-精炼机制的多模态语义记忆系统</h3>
<p><strong>原文标题：</strong> Agentic Learner with Grow-and-Refine Multimodal Semantic Memory</p>
<p><strong>摘要：</strong>
多模态大语言模型在独立查询推理中表现优异，但其运作始终处于"从零开始"模式——每个问题都独立求解且常重复相同错误。现有记忆增强智能体主要存储过往执行轨迹以供复用，然而基于轨迹的记忆存在简略偏差，会逐渐丢失关键领域知识。更严重的是，即使在真正的多模态问题求解场景中，此类系统仅记录单模态行为轨迹，未能保留视觉注意与逻辑推理如何协同促成解决方案的过程。这与人类认知机制存在根本性错位：语义记忆兼具多模态与整合特性，通过协同且独立的表征流同时保存视觉与抽象知识。为此我们提出ViLoMem——采用双通道记忆架构构建基于图式的紧凑记忆系统，分别编码视觉干扰模式与逻辑推理错误，使多模态大语言模型能够从成功与失败经验中持续学习。遵循生长-精炼原则，该系统通过渐进积累与更新多模态语义知识，在保持稳定可泛化策略的同时避免灾难性遗忘。在六个多模态基准测试中，ViLoMem持续提升pass@1准确率，并显著减少重复出现的视觉与逻辑错误。消融实验验证了具有显式干扰-幻觉分离机制的双通道记忆的必要性，证明误差感知多模态记忆在终身学习与跨领域具身学习中的价值。项目页面详见：https://weihao-bo.github.io/ViLoMeo-page。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.21678">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.21678">arXiv</a></p>
<hr />
<h2>🔍 关键词云图</h2>
<p><img alt="关键词云图" src="../images/keywords_wordcloud.png" /></p>
<h2>📈 近期论文趋势</h2>
<p><img alt="论文趋势" src="../images/daily_papers.png" /></p>
<h2>🎙️ 语音播报</h2>
<ul>
<li><a href="../audio/2025-11-28_daily_papers.mp3">收听今日论文解读</a></li>
</ul>
<h2>📱 订阅渠道</h2>
<ul>
<li>GitHub: <a href="https://github.com/2404589803/hf-daily-paper-newsletter-chinese">hf-daily-paper-newsletter-chinese</a></li>
</ul>
    </div>
</body>
</html>