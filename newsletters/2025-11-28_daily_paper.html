<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Hugging Face 论文日报 - 2025-11-28</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', sans-serif;
            line-height: 1.6;
            color: #333;
            background-color: #f5f5f5;
            padding: 20px;
        }
        
        .container {
            max-width: 900px;
            margin: 0 auto;
            background-color: white;
            padding: 40px;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }
        
        h1 {
            color: #2c3e50;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
            margin-bottom: 30px;
            font-size: 28px;
        }
        
        h1 img {
            vertical-align: middle;
            margin-right: 10px;
        }
        
        h2 {
            color: #34495e;
            margin-top: 40px;
            margin-bottom: 20px;
            font-size: 24px;
            border-left: 4px solid #3498db;
            padding-left: 15px;
        }
        
        h3 {
            color: #2c3e50;
            margin-top: 30px;
            margin-bottom: 15px;
            font-size: 20px;
        }
        
        p {
            margin-bottom: 15px;
            text-align: justify;
        }
        
        ul {
            margin-left: 20px;
            margin-bottom: 20px;
        }
        
        li {
            margin-bottom: 8px;
        }
        
        a {
            color: #3498db;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        strong {
            color: #2c3e50;
            font-weight: 600;
        }
        
        hr {
            border: none;
            border-top: 1px solid #e0e0e0;
            margin: 30px 0;
        }
        
        /* 关键修复:限制图片宽度 */
        img {
            max-width: 100%;
            height: auto;
            display: block;
            margin: 20px auto;
            border-radius: 4px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        
        /* 确保图片容器也有宽度限制 */
        p img {
            max-width: 100%;
        }
        
        /* 论文详情区域样式 */
        .paper-section {
            background-color: #f9f9f9;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 20px;
        }
        
        /* 统计信息样式 */
        .stats {
            background-color: #e8f4f8;
            padding: 15px;
            border-radius: 6px;
            margin-bottom: 20px;
        }
        
        /* 响应式设计 */
        @media (max-width: 768px) {
            .container {
                padding: 20px;
            }
            
            h1 {
                font-size: 24px;
            }
            
            h2 {
                font-size: 20px;
            }
            
            h3 {
                font-size: 18px;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <h1><img src="https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo.png" width="30"/> Hugging Face 2025-11-28 论文日报</h1>
<h2>📊 今日论文统计</h2>
<ul>
<li>总论文数：7</li>
<li>热门领域：GPT</li>
</ul>
<h2>📝 论文详情</h2>
<h3>1. 视频生成模型具备优异的潜在奖励建模能力</h3>
<p><strong>原文标题：</strong> Video Generation Models Are Good Latent Reward Models</p>
<p><strong>摘要：</strong>
奖励反馈学习（ReFL）在图像生成与人类偏好对齐方面已证明具有显著效果。然而，该方法向视频生成领域的扩展面临重大挑战。现有视频奖励模型依赖为像素空间输入设计的视觉语言模型，将ReFL优化限制在计算成本高昂的VAE解码后接近完成的去噪阶段。这种像素空间方法存在内存开销大、训练时间长的缺陷，且其后期优化缺乏早期监督，仅能提升视觉质量而无法改善基础运动动态与结构连贯性。本研究证明，预训练视频生成模型天然适用于噪声潜在空间的奖励建模，因为它们专为处理任意时间步的噪声潜在表示而设计，并通过序列建模能力固有地保留时序信息。据此，我们提出过程奖励反馈学习（PRFL）框架，该框架完全在潜在空间中进行偏好优化，无需VAE解码即可实现贯穿整个去噪链的高效梯度反向传播。大量实验表明，与RGB空间的ReFL相比，PRFL在显著提升人类偏好对齐度的同时，可实现内存消耗与训练时间的大幅降低。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.21541">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.21541">arXiv</a></p>
<hr />
<h3>2. 画布到图像：基于多模态控制的组合式图像生成</h3>
<p><strong>原文标题：</strong> Canvas-to-Image: Compositional Image Generation with Multimodal Controls</p>
<p><strong>摘要：</strong>
尽管现代扩散模型能够生成高质量且多样化的图像，但在实现高保真度的组合式与多模态控制方面仍面临挑战，特别是当用户需要同时指定文本提示、主体参照、空间布局、姿态约束和版面标注时。我们提出"画布到图像"这一统一框架，将异构控制信号整合至单一画布界面，使用户能够生成精准反映创作意图的图像。我们的核心创新在于将多样化控制信号编码为复合画布图像，使模型能够直接进行视觉空间推理。通过构建多任务数据集并提出多任务画布训练策略，我们在统一学习范式下优化扩散模型，使其能够协同理解并整合异构控制信号至文本到图像的生成过程。这种联合训练使Canvas-to-Image具备跨多控制模态的推理能力，而非依赖任务特定启发式方法，并在推理阶段对多控制场景展现出卓越的泛化能力。大量实验表明，在多人组合、姿态控制合成、布局约束生成及多控制生成等具有挑战性的基准测试中，Canvas-to-Image在身份保持与控制遵循方面显著优于现有最优方法。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.21691">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.21691">arXiv</a></p>
<hr />
<h3>3. MIRA：面向图像编辑的多模态迭代推理智能体</h3>
<p><strong>原文标题：</strong> MIRA: Multimodal Iterative Reasoning Agent for Image Editing</p>
<p><strong>摘要：</strong>
指令引导的图像编辑为用户提供了通过自然语言编辑图像的直观方式。然而，基于扩散的编辑模型往往难以准确解析复杂的用户指令，特别是涉及组合关系、上下文线索或指代表达的指令，导致编辑结果出现语义偏差或无法体现预期修改。我们通过提出MIRA（多模态迭代推理智能体）来解决这一问题——这是一个轻量级即插即用的多模态推理智能体，通过感知-推理-行动的迭代循环执行编辑任务，有效模拟多轮人机交互过程。与生成单一提示或静态方案不同，MIRA通过逐步预测原子化编辑指令，并利用视觉反馈进行决策优化。基于我们构建的15万规模多模态工具使用数据集MIRA-Editing，结合两阶段SFT+GRPO训练流程，MIRA能够对复杂编辑指令执行推理与编辑。当与Flux.1-Kontext、Step1X-Edit、Qwen-Image-Edit等开源图像编辑模型配合使用时，MIRA在语义一致性和感知质量方面均实现显著提升，其性能达到甚至超越GPT-Image、Nano-Banana等专有系统。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.21087">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.21087">arXiv</a></p>
<hr />
<h3>4. ENACT：通过具身交互的第一人称世界建模评估具身认知能力</h3>
<p><strong>原文标题：</strong> ENACT: Evaluating Embodied Cognition with World Modeling of Egocentric Interaction</p>
<p><strong>摘要：</strong>
具身认知理论主张智能产生于感知运动交互而非被动观察。这引发了一个耐人寻味的问题：主要以非具身方式训练的现代视觉语言模型（VLM）是否展现出具身认知的特征？我们提出ENACT基准测试，通过视觉问答（VQA）形式下的第一人称交互世界建模来评估具身认知能力。该框架将评估构建为部分可观测马尔可夫决策过程（POMDP），其动作表现为场景图变化，包含两个互补的序列重组任务：前向世界建模（根据动作重组乱序观察）和逆向世界建模（根据观察重组乱序动作）。尽管概念简洁，解决这些任务需隐含具备具身认知的核心能力——可供性识别、动作效果推理、具身意识，以及基于部分可观测第一人称输入的交互式长程记忆，同时规避可能干扰评估的低层级图像合成。我们开发了可扩展流程，从机器人仿真（BEHAVIOR）生成问答对，在涵盖长程家居活动的8,972组问答对上评估模型。实验显示前沿VLM与人类表现存在差距，且该差距随交互跨度增加而扩大。模型在逆向任务中表现持续优于前向任务，并呈现人类中心偏见——包括偏好右手动作、当相机参数或视角偏离人类视觉时性能下降。项目网站详见https://enact-embodied-cognition.github.io/。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.20937">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.20937">arXiv</a></p>
<hr />
<h3>5. Multi-Crit：基于多元化标准遵循的多模态评估基准测试</h3>
<p><strong>原文标题：</strong> Multi-Crit: Benchmarking Multimodal Judges on Pluralistic Criteria-Following</p>
<p><strong>摘要：</strong>
大型多模态模型因其出色的指令遵循能力以及与人类偏好的一致性，正被越来越多地用作多模态评估系统的评判工具。然而，这些模型在遵循多样化、细粒度评估标准方面的能力仍有待深入探索。我们开发了Multi-Crit基准测试，用于评估多模态评判模型在遵循多元化标准并生成可靠标准级判断的能力。该基准涵盖开放式生成和可验证推理两类任务，通过严格的数据筛选流程构建，收集了具有多标准人工标注的挑战性响应对，并创新性地提出三项评估指标：系统性评估模型对多元标准的遵循程度、标准切换灵活性以及识别标准级偏好冲突的能力。对25个大型多模态模型的综合分析表明：1）专有模型在保持对多元标准的一致性遵循方面仍存在困难——尤其在开放式评估场景；2）开源模型在灵活遵循多样化标准方面差距更为显著；3）基于整体判断信号的批判性微调虽能增强视觉基础能力，但无法泛化至标准级多元判断。针对推理微调、测试时扩展以及开源与专有模型边界一致性的补充分析，进一步揭示了当前多模态评判系统的局限性。作为开创性研究，Multi-Crit为构建可靠且可调控的多模态人工智能评估体系奠定了基础。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.21662">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.21662">arXiv</a></p>
<hr />
<h3>6. [理解语言意味着什么？]</h3>
<p><strong>原文标题：</strong> What does it mean to understand language?</p>
<p><strong>摘要：</strong>
语言理解不仅需要提取语言输入的表层意义，更要求构建其所描述情境的丰富心理模型。本文提出，由于大脑核心语言系统的处理能力存在根本局限，深度理解语言需要将信息从语言系统输出至其他脑区——这些区域负责计算感知与运动表征、构建心理模型，并存储我们的世界知识与自传体记忆。我们系统回顾了支持该假说的现有证据，指出认知神经科学的最新进展既为其提供了理论基础，也创造了直接验证的研究方法，由此开辟了新路径以揭示理解语言在认知与神经层面的本质内涵。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.19757">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.19757">arXiv</a></p>
<hr />
<h3>7. 具备生长-精炼多模态语义记忆的能动学习者</h3>
<p><strong>原文标题：</strong> Agentic Learner with Grow-and-Refine Multimodal Semantic Memory</p>
<p><strong>摘要：</strong>
多模态大语言模型在独立查询推理方面表现优异，但其运作始终处于零起点状态——每个问题都独立求解且常重复相同错误。现有记忆增强智能体主要存储过往轨迹以供复用，然而基于轨迹的记忆存在简略偏差，会逐渐丢失关键领域知识。更严峻的是，即使在真正的多模态问题解决场景中，此类系统仅记录单模态行为轨迹，未能保留视觉注意与逻辑推理如何协同促成解决方案。这与人类认知存在根本性错位：语义记忆兼具多模态与整合特性，通过协调且独立的表征流同时保存视觉与抽象知识。为此我们提出ViLoMem——采用双通道记忆架构构建紧凑的基于图式的记忆系统。该框架分别编码视觉干扰模式与逻辑推理错误，使多模态大语言模型能够从成功与失败经验中学习。遵循生长-精炼原则，系统持续积累并更新多模态语义知识，在保持稳定、可泛化策略的同时避免灾难性遗忘。在六大多模态基准测试中，ViLoMem持续提升pass@1准确率，并显著减少重复出现的视觉与逻辑错误。消融实验证实了具有显式干扰-幻觉分离的双通道记忆的必要性，验证了面向终身与跨域能动学习的错误感知多模态记忆价值。项目页面详见：https://weihao-bo.github.io/ViLoMeo-page</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.21678">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.21678">arXiv</a></p>
<hr />
<h2>🔍 关键词云图</h2>
<p><img alt="关键词云图" src="../images/keywords_wordcloud.png" /></p>
<h2>📈 近期论文趋势</h2>
<p><img alt="论文趋势" src="../images/daily_papers.png" /></p>
<h2>🎙️ 语音播报</h2>
<ul>
<li><a href="../audio/2025-11-28_daily_papers.mp3">收听今日论文解读</a></li>
</ul>
<h2>📱 订阅渠道</h2>
<ul>
<li>GitHub: <a href="https://github.com/2404589803/hf-daily-paper-newsletter-chinese">hf-daily-paper-newsletter-chinese</a></li>
</ul>
    </div>
</body>
</html>