
# <img src="https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo.png" width="30"/> Hugging Face 2025-12-10 论文日报

## 📊 今日论文统计
- 总论文数：27
- 热门领域：Vision, LLM, Transformer, RL, GPT

## 📝 论文详情


### 1. Wan-Move：基于潜在轨迹引导的运动可控视频生成

**原文标题：** Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance

**摘要：**
本文提出Wan-Move——一个简洁且可扩展的框架，旨在为视频生成模型实现运动控制。现有运动可控方法通常存在控制粒度粗糙与可扩展性有限的问题，导致其输出难以满足实际应用需求。我们通过实现精确且高质量的运动控制来缩小这一差距。其核心思想是直接使原始条件特征具备运动感知能力以指导视频合成。为此，我们首先通过密集点轨迹表征物体运动，实现对场景的细粒度控制；随后将这些轨迹映射至潜在空间，并沿每条轨迹传播首帧特征，生成对齐的时空特征图以指示各场景元素的运动方式。该特征图作为更新后的潜在条件，可无缝集成至现成的图像到视频模型（如Wan-I2V-14B）中作为运动引导，无需改变模型架构。这种方法无需辅助运动编码器，并使基础模型的微调具备高度可扩展性。通过规模化训练，Wan-Move可生成5秒时长、480p分辨率的视频，用户研究表明其运动控制能力已达到Kling 1.5 Pro商用版“运动笔刷”的水平。为支持全面评估，我们进一步设计了MoveBench基准测试集，该数据集经过严格筛选，涵盖多样化的内容类别并采用混合验证标注，其特点在于数据量更大、视频时长更长且运动标注质量更高。在MoveBench与公开数据集上的大量实验一致证明Wan-Move具备卓越的运动生成质量。相关代码、模型及基准数据均已公开。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.08765) | [arXiv](https://arxiv.org/abs/2512.08765)



---

### 2. Visionary：基于WebGPU高斯溅射平台的世界模型载体

**原文标题：** Visionary: The World Model Carrier Built on WebGPU-Powered Gaussian Splatting Platform

**摘要：**
神经渲染，特别是三维高斯溅射（3DGS）技术，近年来发展迅速，已成为构建世界模型的关键组成部分。然而，现有的可视化解决方案仍然存在碎片化、笨重或受限于传统技术流程的问题，导致部署门槛高，且对动态内容与生成模型的支持有限。本研究提出了Visionary，一个开放的、基于网页的原生平台，用于实时渲染各类高斯溅射模型与网格。该平台构建于高效的WebGPU渲染器之上，并集成了逐帧ONNX推理功能，使其能够在保持轻量级、“点击即运行”的浏览器体验的同时，实现动态神经处理。Visionary引入了一个标准化的高斯生成器合约，不仅支持标准3DGS渲染，还允许即插即用的算法在每一帧生成或更新高斯元素。这种推理机制也使我们能够应用前馈生成式后处理。该平台进一步提供了一个可与three.js库集成的插件，并提供简洁的TypeScript API，以便无缝整合到现有的网络应用中。实验表明，在相同的3DGS资源下，得益于基于GPU的图元排序，Visionary相比现有的网页查看器实现了更优的渲染效率。目前，它已支持多种变体模型，包括基于MLP的3DGS、4DGS、神经化身以及风格转换或增强网络。通过将推理与渲染直接在浏览器中统一，Visionary显著降低了3DGS系列方法的复现、比较与部署门槛，成为一个统一服务于重建与生成范式的世界模型载体。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.08478) | [arXiv](https://arxiv.org/abs/2512.08478)



---

### 3. 保持源视频真实感：面向影视级画质的高保真人脸替换

**原文标题：** Preserving Source Video Realism: High-Fidelity Face Swapping for Cinematic Quality

**摘要：**
视频人脸替换技术在影视与娱乐制作中至关重要，然而在长时段复杂视频序列中实现高保真度与时间一致性仍面临重大挑战。受近期参考引导图像编辑进展的启发，本研究探索能否通过类似方式利用源视频丰富的视觉属性，以同时提升视频人脸替换的保真度与时间连贯性。基于此洞见，本文提出首个视频参考引导的人脸替换模型LivingSwap。该方法以关键帧作为条件信号注入目标身份特征，实现灵活可控的编辑。通过将关键帧条件机制与视频参考引导相结合，模型执行时序拼接处理，确保长视频序列中身份特征的稳定保持与高保真重建。针对参考引导训练数据稀缺的问题，我们构建了配对人脸替换数据集Face2Face，并通过数据对反转技术确保可靠的基准监督。大量实验表明，本方法在无缝融合目标身份与源视频表情、光照及运动特征的同时，显著减少了制作流程中的人工干预，达到了当前最优性能。项目主页：https://aim-uofa.github.io/LivingSwap

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.07951) | [arXiv](https://arxiv.org/abs/2512.07951)



---

### 4. OneStory：基于自适应记忆的连贯多镜头视频生成

**原文标题：** OneStory: Coherent Multi-Shot Video Generation with Adaptive Memory

**摘要：**
现实世界视频中的叙事通常通过多个镜头展开——这些镜头在时间上不连续但语义相互关联，共同构成连贯的叙述。然而，现有的多镜头视频生成方法难以有效建模跨镜头的长程上下文关系，因其依赖有限的时间窗口或单关键帧条件约束，导致在复杂叙事场景下性能下降。本研究提出OneStory框架，通过全局且紧凑的跨镜头上下文建模实现连贯可扩展的叙事生成。该方法将多镜头视频生成重构为"下一镜头生成"任务，在利用预训练图像到视频模型实现强视觉条件约束的同时，支持自回归式镜头合成。我们引入两个核心模块：基于历史镜头信息帧构建语义相关全局记忆的帧选择模块，以及执行重要性引导分块化处理以生成紧凑上下文条件的自适应调节器。此外，我们构建了包含指代性描述的高质量多镜头数据集以反映真实叙事模式，并在下一镜头范式下设计了有效的训练策略。通过在自建6万规模数据集上对预训练图像到视频模型进行微调，OneStory在文本条件与图像条件设置下，对多样复杂场景均实现了业界领先的叙事连贯性，为可控沉浸式长视频叙事提供了有效解决方案。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.07802) | [arXiv](https://arxiv.org/abs/2512.07802)



---

### 5. ThreadWeaver：面向语言模型高效并行推理的自适应线程技术

**原文标题：** ThreadWeaver: Adaptive Threading for Efficient Parallel Reasoning in Language Models

**摘要：**
扩展推理时计算能力已使大语言模型（LLM）实现了强大的推理性能，但其固有的顺序解码机制会导致显著延迟，尤其在复杂任务上。近期自适应并行推理研究旨在通过将问题求解过程分解为并发的推理线程来提升推理效率，但现有方法在现实任务中要么局限于监督行为克隆，要么相比广泛使用的顺序长思维链基线出现显著的准确率下降。此外，许多方法需要定制化推理引擎，增加了部署复杂度。本文提出ThreadWeaver——一种自适应并行推理框架，在保持与同规模主流顺序推理模型相当准确率的同时，显著降低推理延迟。该框架的性能源于三项关键创新：1）两阶段并行轨迹生成器，可产生大规模、高质量的带并行标注的思维链数据用于监督微调；2）基于字典树的训练-推理协同设计，无需修改位置编码或KV缓存即可在任何现成的自回归推理引擎上实现并行推理；3）并行感知的强化学习框架，指导模型在准确率与有效并行化之间取得平衡。在六个具有挑战性的数学推理基准测试中，基于Qwen3-8B训练的ThreadWeaver实现了与前沿顺序推理模型相当的准确率（平均71.9%，AIME24数据集上达79.9%），同时词元延迟平均加速比最高达1.53倍，在准确率与效率之间建立了新的帕累托前沿。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.07843) | [arXiv](https://arxiv.org/abs/2512.07843)



---

### 6. 基于自动质量引导自训练的无监督视频实例分割性能提升

**原文标题：** Boosting Unsupervised Video Instance Segmentation with Automatic Quality-Guided Self-Training

**摘要：**
视频实例分割任务因同时需要像素级掩码标注和时间一致性标签而面临显著的标注挑战。尽管近期如VideoCutLER等无监督方法通过合成数据消除了光流依赖，但仍受限于合成数据与真实场景间的域差异。本文提出AutoQ-VIS——一种通过质量引导自训练来弥合这一差异的新型无监督框架。该方法在伪标签生成与自动质量评估之间构建闭环系统，实现了从合成视频到真实视频的渐进式自适应。实验表明，本方法在YouTubeVIS-2019验证集上达到52.6 AP_{50}的先进性能，较先前最优方法VideoCutLER提升4.4%，且无需任何人工标注。这证明了质量感知自训练在无监督视频实例分割中的可行性。代码将在https://github.com/wcbup/AutoQ-VIS开源。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.06864) | [arXiv](https://arxiv.org/abs/2512.06864)



---

### 7. 套利推理：基于优势感知推测的高效推理方法

**原文标题：** Arbitrage: Efficient Reasoning via Advantage-Aware Speculation

**摘要：**
现代大语言模型通过长链思维展现出卓越的推理能力，但其推理过程会产生高昂的计算成本，这促使研究者探索提升性能成本比的技术。在各类加速技术中，推测解码通过采用快速但不精确的草稿模型自回归地生成候选标记，再由更强目标模型并行验证，以此加速推理过程。然而，传统标记级推测解码在语义等价步骤中常因标记不匹配产生不必要的拒绝，导致其在推理任务中表现受限。尽管近期研究转向步骤级语义验证（通过整体接受或拒绝推理步骤提升效率），现有步骤级方法仍需重新生成大量被拒绝步骤，改进有限且浪费目标模型计算资源。为解决这一挑战，本文提出套利推理框架——一种基于草稿模型与目标模型相对优势动态路由生成过程的新型步骤级推测生成框架。该框架摒弃固定接受阈值，转而采用轻量级路由器预测目标模型何时可能生成显著更优的推理步骤。这种路由机制近似于理想的套利预言机（始终选择更高质量的步骤），实现了接近最优的效率-精度平衡。在多个数学推理基准测试中，套利推理框架持续超越现有步骤级推测解码基线方法，在保持相同精度的条件下将推理延迟降低最高约2倍。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.05033) | [arXiv](https://arxiv.org/abs/2512.05033)



---

### 8. MIND-V：基于强化学习物理对齐的长时序机器人操作分层视频生成框架

**原文标题：** MIND-V: Hierarchical Video Generation for Long-Horizon Robotic Manipulation with RL-based Physical Alignment

**摘要：**
具身模仿学习受限于多样化长时序机器人操作数据的稀缺性。该领域现有视频生成模型仅能合成简单动作的短片段，且常依赖人工定义轨迹。为此，我们提出MIND-V分层框架，旨在生成物理合理且逻辑连贯的长时序机器人操作视频。受认知科学启发，MIND-V通过三个核心组件桥接高层推理与像素级合成：语义推理中枢（SRH）利用预训练视觉-语言模型进行任务规划；行为语义桥梁（BSB）将抽象指令转换为领域无关表征；运动视频生成器（MVG）实现条件视频渲染。MIND-V采用分阶段视觉未来推演策略作为测试时优化方法，以增强长时序鲁棒性。为使生成视频符合物理规律，我们引入基于新型物理前瞻一致性（PFC）奖励指导的GRPO强化学习后训练阶段。PFC利用V-JEPA世界模型，通过特征空间中预测动态演化与实际演化的对齐来保障物理合理性。实验表明，MIND-V在长时序机器人操作视频生成任务中达到最先进性能，为具身数据合成建立了可扩展且可控的范式。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.06628) | [arXiv](https://arxiv.org/abs/2512.06628)



---

### 9. DeepCode：开放式智能体编码

**原文标题：** DeepCode: Open Agentic Coding

**摘要：**
大型语言模型（LLM）的最新进展催生了强大的编码智能体，使得代码助手有望演变为代码工程师。然而，现有方法在实现高保真度的文档到代码库合成（例如从科学论文生成代码）方面仍面临重大挑战，这主要源于信息过载与LLM上下文容量限制之间的根本矛盾。本研究提出DeepCode，一个完全自主的框架，通过规范化的信息流管理从根本上解决这一难题。该框架将代码库合成视为信道优化问题，通过协调四种信息操作在有限上下文预算下最大化任务相关信号：基于蓝图提炼的源文件压缩、采用状态化代码记忆的结构化索引、通过检索增强生成的条件知识注入，以及闭环错误修正。在PaperBench基准测试中的广泛评估表明，DeepCode实现了最先进的性能，显著超越Cursor、Claude Code等主流商业智能体，并在关键复现指标上超越顶尖机构的博士级人类专家。通过系统化地将论文规范转化为可媲美人类专家水准的生产级实现，本研究为自主科学复现奠定了新基础，有望加速研究评估与科学发现进程。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.07921) | [arXiv](https://arxiv.org/abs/2512.07921)



---

### 10. 从下一词元到下一区块：扩散大语言模型的原理性适配路径

**原文标题：** From Next-Token to Next-Block: A Principled Adaptation Path for Diffusion LLMs

**摘要：**
大语言模型（LLM）在生成任务上表现卓越，但主流的自回归解码方式本质上是顺序执行的，这造成了吞吐量瓶颈。扩散语言模型（DLM）——尤其是基于区块的变体——能够实现并行生成和区块内双向推理，然而从头训练大型DLM成本高昂，且浪费了成熟的自回归检查点中蕴含的知识。先前的“适配”尝试要么通过修改逻辑输出或随机扩展注意力掩码来实现全序列扩散，要么简单地将自回归权重移植到区块扩散框架中，未能解决自回归因果性与区块双向性之间的根本性错配。我们通过将自回归视为区块大小等于1的区块扩散模型，将适配重新定义为从自回归到区块扩散的范式内路径。具体而言，我们设计了如下适配路径：采用上下文因果注意力掩码（在上下文维度保持因果性，仅在当前生成区块内允许双向注意力）、高效的并行适配流程、辅助自回归损失以最大化数据利用并保留预训练知识，以及逐步增加生成区块大小。该方法与掩码区块扩散框架无缝集成，并保持了训练与推理的一致性。基于这些组件构建的NBDiff-7B（基础版与指导版）能够继承长上下文建模与推理能力，在7B量级的DLM中达到最先进的性能，在通用知识、数学和代码基准测试上相较于强基线模型取得显著提升。这些结果表明，基于原理的自回归到区块扩散适配是一种有效且计算高效的方法，可作为从头训练DLM的替代方案。代码地址：https://github.com/YuchuanTian/NBDiff。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.06776) | [arXiv](https://arxiv.org/abs/2512.06776)



---

### 11. 视听与理解：多模态大语言模型中人类语音理解的基准测试

**原文标题：** See, Hear, and Understand: Benchmarking Audiovisual Human Speech Understanding in Multimodal Large Language Models

**摘要：**
多模态大语言模型（MLLMs）被期望能够协同解读视觉、听觉与语言信息，然而现有的视频基准测试很少评估其对于人类语音的细粒度推理能力。许多任务仍可通过视觉信息单独解决，或仅对语音进行粗粒度评估，这难以揭示模型是否能够准确关联“谁在说话”、“说了什么”以及“何时发生”。我们提出了AV-SpeakerBench，一个精心构建的基准测试集，包含3,212道基于真实世界视频的、以说话者为中心的多选题，专注于视听推理。其特点包括：（1）以说话者而非场景为核心的推理单元设计；（2）融合导向的问题设计，将视听依赖关系嵌入问题语义中；（3）专家精心标注，确保时间精度与跨模态有效性。综合评估表明，Gemini系列模型持续优于开源系统，其中Gemini 2.5 Pro取得了最佳结果。在开源模型中，Qwen3-Omni-30B的表现接近Gemini 2.0 Flash，但仍远落后于Gemini 2.5 Pro，其主要差距源于视听融合能力较弱，而非视觉感知不足。我们相信，AV-SpeakerBench为推进未来多模态系统中的细粒度视听推理研究奠定了严谨的基础。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.02231) | [arXiv](https://arxiv.org/abs/2512.02231)



---

### 12. COREA：基于双向3D到3D监督的可重光照三维高斯与SDF之间的由粗到细三维表示对齐方法

**原文标题：** COREA: Coarse-to-Fine 3D Representation Alignment Between Relightable 3D Gaussians and SDF via Bidirectional 3D-to-3D Supervision

**摘要：**
本文提出COREA，首个联合学习可重光照三维高斯与符号距离场（SDF）的统一框架，旨在实现精确几何重建与真实感重光照。尽管近期三维高斯泼溅（3DGS）方法已扩展至网格重建与基于物理的渲染（PBR），其几何信息仍从二维渲染中学习，导致表面粗糙且BRDF-光照分解不可靠。为解决这些局限，COREA引入一种由粗到细的双向3D到3D对齐策略，使几何信号能在三维空间中直接学习。该策略中，深度信息为两种表示提供粗粒度对齐，深度梯度与法向量则优化细尺度结构，所得几何支撑稳定的BRDF-光照分解。密度控制机制进一步稳定高斯分布增长，在几何保真度与内存效率间取得平衡。在标准基准测试上的实验表明，COREA在统一框架内实现了新颖视角合成、网格重建与PBR方面的卓越性能。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.07107) | [arXiv](https://arxiv.org/abs/2512.07107)



---

### 13. TreeGRPO：基于树优势的GRPO用于扩散模型在线强化学习后训练

**原文标题：** TreeGRPO: Tree-Advantage GRPO for Online RL Post-Training of Diffusion Models

**摘要：**
强化学习后训练对于使生成模型与人类偏好对齐至关重要，但其高昂的计算成本仍是广泛采用的主要障碍。我们提出了TreeGRPO，一种新颖的强化学习框架，通过将去噪过程重构为搜索树，显著提升了训练效率。该方法从共享的初始噪声样本出发，通过策略性分支生成多个候选轨迹，同时高效复用其公共前缀。这种树形结构方法具有三大关键优势：（1）高样本效率，在相同训练样本下实现更优性能；（2）通过奖励反向传播实现细粒度信用分配，计算步骤级优势值，克服了基于轨迹方法中均匀信用分配的局限性；（3）摊销式计算，多子节点分支机制使得单次前向传播即可完成多次策略更新。在扩散模型和流模型上的大量实验表明，TreeGRPO实现了2.4倍的训练加速，并在效率-奖励权衡空间中建立了更优的帕累托前沿。我们的方法在多个基准测试和奖励模型中持续超越GRPO基线，为基于强化学习的视觉生成模型对齐提供了可扩展且有效的技术路径。项目网站详见treegrpo.github.io。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.08153) | [arXiv](https://arxiv.org/abs/2512.08153)



---

### 14. 高效动态场景重建：一次一个D4RT

**原文标题：** Efficiently Reconstructing Dynamic Scenes One D4RT at a Time

**摘要：**
从视频中理解并重建动态场景的复杂几何结构与运动，仍然是计算机视觉领域的一项艰巨挑战。本文介绍了D4RT，一个简单而强大的前馈模型，旨在高效解决此任务。D4RT采用统一的Transformer架构，从单个视频中联合推断深度、时空对应关系以及完整的相机参数。其核心创新在于一种新颖的查询机制，它绕开了密集逐帧解码的繁重计算，也避免了管理多个任务特定解码器的复杂性。我们的解码接口使模型能够独立且灵活地探测空间和时间中任意点的三维位置。由此产生了一种轻量级且高度可扩展的方法，实现了极为高效的训练与推理。我们证明，该方法在广泛的4D重建任务中超越了先前方法，确立了新的技术标杆。动态效果请参见项目网页：https://d4rt-paper.github.io/。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.08924) | [arXiv](https://arxiv.org/abs/2512.08924)



---

### 15. 模块化神经图像信号处理

**原文标题：** Modular Neural Image Signal Processing

**摘要：**
本文提出了一种模块化神经图像信号处理（ISP）框架，该框架处理原始输入并渲染出高质量的显示参考图像。与先前的神经ISP设计不同，我们的方法引入了高度的模块化，从而能够对渲染过程的多个中间阶段进行完全控制。这种模块化设计不仅实现了高渲染精度，还提升了可扩展性、可调试性、对未知相机的泛化能力以及匹配不同用户偏好风格的灵活性。为展示该设计的优势，我们构建了一个用户交互式照片编辑工具，该工具利用我们的神经ISP来支持多样化的编辑操作和图片风格。该工具经过精心设计，旨在充分利用我们神经ISP的高质量渲染能力，并实现无限次可后期编辑的重新渲染。我们的方法是一个完全基于学习的框架，包含不同计算能力的变体，所有变体规模适中（整个处理流程的参数规模约为0.5M至3.9M），并在多个测试集上持续提供具有竞争力的定性与定量结果。补充视频请访问：https://youtu.be/ByhQjQSjxVM

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.08564) | [arXiv](https://arxiv.org/abs/2512.08564)



---

### 16. 慢思快行：一种面向泛化视觉语言导航的双系统基础模型

**原文标题：** Ground Slow, Move Fast: A Dual-System Foundation Model for Generalizable Vision-and-Language Navigation

**摘要：**
尽管近期大规模视觉语言模型（VLMs）在视觉语言导航（VLN）的泛化能力上有所提升，但现有方法通常依赖于端到端管道，直接将视觉语言输入映射为短视程离散动作。此类设计常导致动作片段化、延迟较高，且难以应对动态避障等现实挑战。本文提出DualVLN，首个协同整合高层推理与低层动作执行的双系统VLN基础模型。系统2作为基于VLM的全局规划器，通过基于图像的推理预测中程航点目标，实现“慢思”；系统1作为轻量级多模态条件扩散Transformer策略，利用系统2提供的显式像素目标与潜在特征生成平滑精准的轨迹，实现“快行”。该双系统设计能够在复杂动态环境中实现鲁棒的实时控制与自适应局部决策。通过解耦训练，VLM保持其泛化能力，而系统1实现了可解释且高效的局部导航。DualVLN在所有VLN基准测试中均优于现有方法，真实环境实验验证了其在动态环境中具备鲁棒的长视程规划能力与实时适应性。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.08186) | [arXiv](https://arxiv.org/abs/2512.08186)



---

### 17. SUCCESS-GS：面向高效静态与动态高斯泼溅的紧凑性与压缩技术综述

**原文标题：** SUCCESS-GS: Survey of Compactness and Compression for Efficient Static and Dynamic Gaussian Splatting

**摘要：**
3D高斯泼溅（3DGS）已成为一种强大的显式表示方法，能够实现实时、高保真的三维重建与新视角合成。然而，其实际应用受到存储和渲染数百万高斯分布所需巨大内存与计算需求的限制，这些挑战在四维动态场景中尤为严峻。为应对这些问题，高效高斯泼溅领域迅速发展，提出了在保持重建质量的同时减少冗余的方法。本综述首次对高效三维与四维高斯泼溅技术进行了统一梳理。针对三维和四维场景，我们系统地将现有方法归纳为参数压缩与结构重组压缩两大方向，并全面总结了各类方法的核心思想与技术趋势。此外，我们涵盖了广泛使用的数据集、评估指标以及代表性基准比较结果。最后，我们讨论了当前技术的局限性，并展望了面向可扩展、紧凑且实时的静态与动态三维场景表示的高斯泼溅技术的未来研究方向。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.07197) | [arXiv](https://arxiv.org/abs/2512.07197)



---

### 18. EcomBench：面向电商领域基础智能体的综合性评估基准

**原文标题：** EcomBench: Towards Holistic Evaluation of Foundation Agents in E-commerce

**摘要：**
基础智能体在推理和与现实环境交互方面的能力迅速发展，对其核心能力的评估变得日益重要。尽管已有许多基准被开发用于评估智能体性能，但大多数集中于学术环境或人为设计的场景，忽视了实际应用中出现的挑战。为解决这一问题，我们聚焦于一个高度实用的现实场景——电子商务领域，该领域涉及大量多样化的用户交互、动态的市场条件以及与真实决策过程直接相关的任务。为此，我们提出了EcomBench，一个全面的电子商务基准，旨在评估智能体在真实电商环境中的性能。EcomBench基于全球领先电商生态系统中真实的用户需求构建，并通过领域专家精心筛选和标注，以确保其清晰性、准确性和领域相关性。它涵盖了电商场景中的多个任务类别，并定义了三个难度级别，用于评估智能体在深度信息检索、多步推理和跨源知识整合等关键能力上的表现。通过将评估根植于真实的电商情境中，EcomBench为衡量智能体在现代电子商务中的实际能力提供了一个严谨且动态的测试平台。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.08868) | [arXiv](https://arxiv.org/abs/2512.08868)



---

### 19. TrackingWorld：以世界坐标系为中心的几乎全像素单目三维跟踪

**原文标题：** TrackingWorld: World-centric Monocular 3D Tracking of Almost All Pixels

**摘要：**
单目三维跟踪旨在从单目视频中捕获像素在三维空间中的长期运动，近年来取得了快速进展。然而，我们认为现有的单目三维跟踪方法在分离相机运动与前景动态运动方面仍存在不足，且无法对视频中新出现的动态目标进行密集跟踪。针对这两点局限性，我们提出了TrackingWorld——一种在以世界坐标系为中心的三维坐标系中对几乎所有像素进行密集三维跟踪的新流程。首先，我们引入一种跟踪上采样器，能够高效地将任意稀疏二维跟踪结果提升为密集二维跟踪。其次，为将现有跟踪方法推广至新出现的目标，我们将该上采样器应用于所有帧，并通过消除重叠区域的跟踪轨迹来减少二维跟踪的冗余。最后，我们提出一种基于优化的高效框架，通过估计相机位姿及这些二维跟踪点的三维坐标，将密集二维跟踪反投影至以世界坐标系为中心的三维轨迹。在合成数据集与真实数据集上的大量实验表明，我们的系统能够在世界坐标系框架下实现准确且密集的三维跟踪。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.08358) | [arXiv](https://arxiv.org/abs/2512.08358)



---

### 20. 基于新型深度学习架构的脑部MRI图像肿瘤分类与分割方法研究

**原文标题：** Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images

**摘要：**
脑肿瘤对人类生命构成严重威胁，因此在早期阶段进行精准检测对于优化诊疗方案至关重要。目前脑肿瘤主要通过放射科医生人工判读患者MRI扫描图像进行诊断。然而近年来儿童与青少年脑肿瘤发病率持续上升，导致医学影像数据量急剧增长，人工检测方式已面临耗时费力、效率低下的挑战。随着人工智能技术的快速发展及其在医疗领域的广泛应用，我们可借助计算机辅助诊断系统实现脑肿瘤的自动化早期检测。现有模型普遍存在泛化能力不足、验证集性能欠佳等问题。为此，本文提出两种新型深度学习架构：（a）基于自注意力增强机制的肿瘤分类网络，能够实现多类别脑肿瘤精准分类。我们在包含胶质瘤、脑膜瘤、垂体瘤及非肿瘤病例的数据集上进行训练，最终在验证集上取得99.38%的分类准确率，成为少数能够实现脑肿瘤高精度检测的新型深度学习架构；（b）基于自注意力机制的分割网络，专门用于脑肿瘤的精确区域分割，最终获得99.23%的整体像素精度。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.06531) | [arXiv](https://arxiv.org/abs/2512.06531)



---

### 21. LYNX：基于置信度控制推理的动态出口学习机制

**原文标题：** LYNX: Learning Dynamic Exits for Confidence-Controlled Reasoning

**摘要：**
大型推理模型通过生成扩展的思维链在复杂任务上表现出色，但它们常存在“过度思考”现象：即在已掌握足够信息给出正确答案后仍持续推理。这不仅浪费推理计算资源，还可能降低准确率。现有早期停止方法或依赖额外采样与启发式策略干预解码过程，或需要辅助验证模型，抑或仅作为事后分析流程而缺乏形式化保证。本文提出LYNX——一种在线早期退出机制，将模型自身的隐状态感知转化为置信度控制的停止决策。LYNX在生成过程中将退出决策锚定于自然出现的推理线索（如“嗯”“等等”），利用强制退出生成的监督信号在这些线索标记对应的隐状态上训练轻量级探针，并通过分裂共形预测框架对得分进行封装，从而实现对过早退出的无分布控制。关键创新在于：该探针仅需在通用数学语料上进行一次性训练与校准，即可跨基准测试、解码温度乃至非数学任务直接复用。在涵盖15亿至320亿参数规模的三种模型系列中，每个基础模型仅需一个数学训练的探针即可实现优异的准确率-效率权衡。在GSM8K数据集上，LYNX在减少40-65%标记使用量的同时保持或提升基线准确率；在MATH-500数据集上以约35-60%的标记缩减实现最高12个百分点的准确率提升；在AIME 2024竞赛题中恢复基线准确率的同时节省超50%的标记；在常识推理基准CommonsenseQA（非数学任务）上实现零样本迁移，在获得适度准确率提升的同时减少高达70%的标记消耗。与现有最优早期退出方法相比，LYNX在保持完全在线推理、无需调用代理模型且提供用户可调置信度保证的前提下，实现了具有竞争力或更优的帕累托前沿。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.05325) | [arXiv](https://arxiv.org/abs/2512.05325)



---

### 22. MemLoRA：面向设备端内存系统的专家适配器蒸馏方法

**原文标题：** MemLoRA: Distilling Expert Adapters for On-Device Memory Systems

**摘要：**
记忆增强型大语言模型（LLMs）通过存储相关记忆并将其作为上下文整合，在长对话中展现出卓越的一致性。这种基于记忆的个性化对于允许用户保持对话和数据私密性的设备端场景尤为关键。然而，记忆增强系统通常依赖于计算成本高昂的大语言模型，难以在本地设备端部署。尽管小语言模型（SLMs）比大语言模型更适合设备端推理，但其性能往往不足。此外，现有基于大语言模型的系统缺乏原生视觉能力，限制了其在多模态场景中的应用。本文提出：（1）MemLoRA，一种新型内存系统，通过为小语言模型配备专用记忆适配器实现本地部署；（2）其视觉扩展版本MemLoRA-V，将小型视觉语言模型（SVLMs）集成到内存系统中，实现原生视觉理解。基于知识蒸馏原理，每个适配器针对特定记忆操作（知识提取、记忆更新和记忆增强生成）进行独立训练。配备记忆适配器的小型模型能够在无需云端依赖的情况下执行精确的设备端记忆操作。在纯文本任务中，MemLoRA的性能超越规模10倍的基线模型（如Gemma2-27B），并在LoCoMo基准测试中达到与规模60倍模型（如GPT-OSS-120B）相当的水平。为评估视觉理解能力，我们扩展了LoCoMo基准，引入需要直接视觉推理的挑战性视觉问答任务。在此测试中，集成视觉语言模型的MemLoRA-V较基于图像描述的方法实现显著提升（准确率81.3 vs. 23.7），同时在文本任务中保持强劲性能，证明了该方法在多模态场景中的有效性。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.04763) | [arXiv](https://arxiv.org/abs/2512.04763)



---

### 23. SegEarth-OV3：探索SAM 3在遥感图像开放词汇语义分割中的应用

**原文标题：** SegEarth-OV3: Exploring SAM 3 for Open-Vocabulary Semantic Segmentation in Remote Sensing Images

**摘要：**
目前大多数无需训练的开放词汇语义分割方法基于CLIP模型。尽管这些方法已取得进展，但在精确定位方面常面临挑战，或需要复杂的流程来整合独立模块，尤其在遥感场景中存在大量密集小目标时更为突出。近期提出的Segment Anything Model 3通过可提示框架统一了分割与识别任务。本文首次尝试将SAM 3应用于遥感开放词汇语义分割任务，且无需任何训练。首先，我们设计了一种掩码融合策略，将SAM 3语义分割头与Transformer解码器的输出相结合，从而充分发挥两个模块的优势以提升地物覆盖效果。其次，利用存在性头输出的存在分数过滤场景中不存在的类别，减少地理空间场景中因词汇量庞大和块级处理导致的误报。我们在多个遥感数据集上评估了该方法，实验表明这种简易适配方案取得了显著效果，证明了SAM 3在遥感开放词汇语义分割中的潜力。代码已发布于https://github.com/earth-insights/SegEarth-OV-3。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.08730) | [arXiv](https://arxiv.org/abs/2512.08730)



---

### 24. SAM-Body4D：无需训练的从视频中恢复四维人体网格

**原文标题：** SAM-Body4D: Training-Free 4D Human Body Mesh Recovery from Videos

**摘要：**
人体网格恢复（HMR）旨在从二维观测中重建三维人体姿态与形状，是现实场景中以人为中心理解的基础任务。尽管近期基于图像的HMR方法（如SAM 3D Body）在自然场景图像中展现出较强的鲁棒性，但在处理视频时依赖逐帧推理，易导致时序不一致性且在遮挡情况下性能下降。本文通过利用视频中人体固有的连续性，在不增加额外训练的前提下解决上述问题。我们提出SAM-Body4D——一种无需训练的框架，能够从视频中实现时序一致且抗遮挡的HMR。我们首先使用可提示视频分割模型生成身份一致的掩码片段，随后通过遮挡感知模块进行精细化处理以恢复缺失区域。优化后的掩码片段引导SAM 3D Body生成连贯的全身体网格轨迹，而基于填充的并行策略实现了高效的多人体推理。实验结果表明，SAM-Body4D在具有挑战性的自然场景视频中显著提升了时序稳定性与鲁棒性，且无需任何重新训练。我们的代码与演示已公开于：https://github.com/gaomingqi/sam-body4d。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.08406) | [arXiv](https://arxiv.org/abs/2512.08406)



---

### 25. 地形扩散：基于扩散模型的无限实时地形生成技术——柏林噪声的继任者

**原文标题：** Terrain Diffusion: A Diffusion-Based Successor to Perlin Noise in Infinite, Real-Time Terrain Generation

**摘要：**
数十年来，程序化生成的世界一直建立在柏林噪声等程序化噪声函数基础上，这些函数虽然生成快速且具备无限性，但在真实感与大规模连贯性方面存在根本性局限。本文提出“地形扩散”——一种人工智能时代下柏林噪声的继任技术，它将扩散模型的高保真特性与程序化噪声不可或缺的核心优势（无缝无限延展、种子一致性、恒定时间随机访问）相结合。其核心是“无限扩散”算法，该创新算法实现了无边景观的无缝实时合成。通过层级化扩散模型堆栈，系统将行星级环境信息与局部细节相耦合，而紧凑的拉普拉斯编码机制则保障了地球尺度动态范围内的输出稳定性。开源无限张量框架支持对无界张量的恒定内存操作，结合少步一致性蒸馏技术实现了高效生成。这些组件共同确立了扩散模型作为程序化世界生成实践基础的地位，使其能够以连贯、可控且无边界限制的方式合成完整行星地貌。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.08309) | [arXiv](https://arxiv.org/abs/2512.08309)



---

### 26. 基于算子网络预测复杂几何结构上的时变流动

**原文标题：** Predicting Time-Dependent Flow Over Complex Geometries Using Operator Networks

**摘要：**
构建快速且能泛化至不同几何结构的非定常流动替代模型仍具挑战性。本文提出一种时变且几何敏感的深度算子网络，用于预测参数化与非参数化形状周围中等雷诺数流动的速度场。该模型通过符号距离场主干网络编码几何信息，并借助卷积神经网络分支编码流动历史特征，基于841组高精度仿真数据进行训练。在未见过的几何形状上，模型实现了约5%的相对L2单步误差，计算速度较传统计算流体力学方法提升最高达1000倍。我们提供了以物理特性为核心的推演诊断方法，包括监测点相位误差与散度范数，以量化长期预测的保真度。结果显示模型能准确预测短期瞬态过程，但在精细尺度尾流中会出现误差累积，这一现象在锐角几何结构中尤为显著。本文系统分析了失效模式并提出了实际改进方案。为支持可重复性与基准测试，代码、数据划分及脚本已开源发布：https://github.com/baskargroup/TimeDependent-DeepONet。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.04434) | [arXiv](https://arxiv.org/abs/2512.04434)



---

### 27. 相同内容，不同答案：多模态大语言模型中的跨模态不一致性研究

**原文标题：** Same Content, Different Answers: Cross-Modal Inconsistency in MLLMs

**摘要：**
本文提出两个新基准测试REST与REST+（渲染等价压力测试），用于系统评估多模态大语言模型中的跨模态不一致性。多模态大语言模型虽经训练将视觉与语言表征于同一嵌入空间，却无法在两种模态中执行相同任务。我们的基准测试包含三种模态（图像、文本、混合模态）下语义信息完全一致的样本，实验表明当前最先进的多模态大语言模型在不同模态间的推理存在系统性不一致。通过对15个主流模型进行评估，我们发现即使排除文字识别误差的影响，各模型的模态不一致程度仍存在显著差异。将文本渲染为图像或将图像转换为文本均无法解决该问题。研究进一步发现，即使文字识别完全正确，视觉特征（文字颜色与分辨率，而非字体）及视觉标记数量仍会影响模型表现。最后，我们验证了模型一致性评分与文本-图像模态差距存在相关性，这为理解多模态大语言模型的跨模态不一致机制提供了理论依据。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.08923) | [arXiv](https://arxiv.org/abs/2512.08923)



---


## 🔍 关键词云图
![关键词云图](../images/keywords_wordcloud.png)

## 📈 近期论文趋势
![论文趋势](../images/daily_papers.png)

## 🎙️ 语音播报
- [收听今日论文解读](../audio/2025-12-10_daily_papers.mp3)

## 📱 订阅渠道
- GitHub: [hf-daily-paper-newsletter-chinese](https://github.com/2404589803/hf-daily-paper-newsletter-chinese)