<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Hugging Face 论文日报 - 2025-12-09</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', sans-serif;
            line-height: 1.6;
            color: #333;
            background-color: #f5f5f5;
            padding: 20px;
        }
        
        .container {
            max-width: 900px;
            margin: 0 auto;
            background-color: white;
            padding: 40px;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }
        
        h1 {
            color: #2c3e50;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
            margin-bottom: 30px;
            font-size: 28px;
        }
        
        h1 img {
            vertical-align: middle;
            margin-right: 10px;
        }
        
        h2 {
            color: #34495e;
            margin-top: 40px;
            margin-bottom: 20px;
            font-size: 24px;
            border-left: 4px solid #3498db;
            padding-left: 15px;
        }
        
        h3 {
            color: #2c3e50;
            margin-top: 30px;
            margin-bottom: 15px;
            font-size: 20px;
        }
        
        p {
            margin-bottom: 15px;
            text-align: justify;
        }
        
        ul {
            margin-left: 20px;
            margin-bottom: 20px;
        }
        
        li {
            margin-bottom: 8px;
        }
        
        a {
            color: #3498db;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        strong {
            color: #2c3e50;
            font-weight: 600;
        }
        
        hr {
            border: none;
            border-top: 1px solid #e0e0e0;
            margin: 30px 0;
        }
        
        /* 关键修复:限制图片宽度 */
        img {
            max-width: 100%;
            height: auto;
            display: block;
            margin: 20px auto;
            border-radius: 4px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        
        /* 确保图片容器也有宽度限制 */
        p img {
            max-width: 100%;
        }
        
        /* 论文详情区域样式 */
        .paper-section {
            background-color: #f9f9f9;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 20px;
        }
        
        /* 统计信息样式 */
        .stats {
            background-color: #e8f4f8;
            padding: 15px;
            border-radius: 6px;
            margin-bottom: 20px;
        }
        
        /* 响应式设计 */
        @media (max-width: 768px) {
            .container {
                padding: 20px;
            }
            
            h1 {
                font-size: 24px;
            }
            
            h2 {
                font-size: 20px;
            }
            
            h3 {
                font-size: 18px;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <h1><img src="https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo.png" width="30"/> Hugging Face 2025-12-09 论文日报</h1>
<h2>📊 今日论文统计</h2>
<ul>
<li>总论文数：32</li>
<li>热门领域：RL, GPT, LLM</li>
</ul>
<h2>📝 论文详情</h2>
<h3>1. 原生并行推理器：基于自蒸馏强化学习的并行推理框架</h3>
<p><strong>原文标题：</strong> Native Parallel Reasoner: Reasoning in Parallelism via Self-Distilled Reinforcement Learning</p>
<p><strong>摘要：</strong>
本文提出原生并行推理器（NPR），一种无需教师模型的框架，使大语言模型能够自主演化出真正的并行推理能力。NPR通过三项关键创新将模型从序列化模仿转变为原生并行认知：1）自蒸馏渐进式训练范式，在无外部监督条件下实现从“冷启动”格式发现到严格拓扑约束的过渡；2）新颖的并行感知策略优化算法，直接在执行图中优化分支策略，使模型通过试错学习自适应分解；3）稳健的NPR引擎，重构SGLang的内存管理与流程控制，实现稳定的大规模并行强化学习训练。在八个推理基准测试中，基于Qwen3-4B训练的NPR实现了最高24.5%的性能提升和4.6倍的推理加速。与常退化为自回归解码的基线方法不同，NPR展现出100%的真实并行执行能力，为自主演化、高效可扩展的智能体推理建立了新标准。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.07461">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.07461">arXiv</a></p>
<hr />
<h3>2. 超越实数：长上下文大语言模型中旋转位置编码的虚数扩展</h3>
<p><strong>原文标题：</strong> Beyond Real: Imaginary Extension of Rotary Position Embeddings for Long-Context LLMs</p>
<p><strong>摘要：</strong>
旋转位置编码（RoPE）已成为大语言模型中编码序列顺序的标准方法，其通过在复平面对查询向量和键向量施加旋转来实现。然而，标准实现仅利用复数值点积的实部进行注意力分数计算。这种简化丢弃了包含宝贵相位信息的虚部，可能导致对建模长上下文依赖至关重要的关系细节损失。本文提出一种扩展方法，重新整合被丢弃的虚部。我们的方法利用完整的复数值表示构建双分量注意力分数。我们从理论和实验上证明，该方法通过保留更多位置信息，增强了对长上下文依赖的建模能力。此外，在一系列长上下文语言建模基准测试上的评估表明，我们的方法相较于标准RoPE能够持续提升性能，且随着上下文长度的增加，改进效果更为显著。代码已发布于 https://github.com/OpenMOSS/rope_pp。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.07525">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.07525">arXiv</a></p>
<hr />
<h3>3. 基于时序推理器的统一视频编辑方法</h3>
<p><strong>原文标题：</strong> Unified Video Editing with Temporal Reasoner</p>
<p><strong>摘要：</strong>
现有视频编辑方法面临关键权衡：专家模型虽能提供精确编辑效果，但依赖掩码等任务特定先验知识，难以实现统一框架；反之，基于时序上下文学习的统一模型虽无需掩码，但缺乏显式空间线索，导致指令-区域映射能力薄弱与定位精度不足。为解决这一矛盾，我们受思维链推理启发，提出创新性的帧链式方法VideoCoF。该方法通过强制视频扩散模型在生成目标视频标记前先预测推理标记（编辑区域潜在表示），构建“观察-推理-编辑”的流程。这种显式推理步骤在无需用户提供掩码的前提下，实现了精准的指令-区域对齐与细粒度视频编辑。此外，我们提出旋转位置编码对齐策略，利用推理标记确保运动一致性，并实现超越训练时长的序列长度外推能力。实验表明，仅使用5万对视频数据的极低训练成本，VideoCoF即在VideoCoF评测基准上达到最先进性能，验证了本方法的高效性与有效性。相关代码、权重及数据已发布于https://github.com/knightyxp/VideoCoF。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.07469">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.07469">arXiv</a></p>
<hr />
<h3>4. Voxify3D：像素艺术与体素渲染的融合</h3>
<p><strong>原文标题：</strong> Voxify3D: Pixel Art Meets Volumetric Rendering</p>
<p><strong>摘要：</strong>
体素艺术是一种广泛应用于游戏和数字媒体的独特风格化形式，然而从三维网格自动生成体素艺术仍面临几何抽象、语义保持与离散色彩一致性等多重相互冲突的要求，极具挑战性。现有方法往往过度简化几何结构，或难以实现像素级精确、调色板受限的体素艺术美学。本文提出Voxify3D，一个可微分的两阶段框架，将三维网格优化与二维像素艺术监督相结合。我们的核心创新在于三个组件的协同整合：（1）正交像素艺术监督，消除透视畸变以实现体素与像素的精确对齐；（2）基于分块的CLIP对齐机制，在离散化过程中保持跨层级的语义一致性；（3）调色板约束的Gumbel-Softmax量化方法，支持在离散色彩空间上进行可微分优化，并提供可控的调色板策略。该整合方案解决了三个关键难题：极端离散化下的语义保持、通过体素渲染实现像素艺术美学，以及端到端的离散优化。实验表明，本方法在多样化角色模型和可控抽象程度（2-8种颜色、20倍至50倍分辨率）下均表现出优异性能（CLIP-IQA得分37.12，用户偏好率77.90%）。项目页面：https://yichuanh.github.io/Voxify-3D/</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.07834">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.07834">arXiv</a></p>
<hr />
<h3>5. 规模化零样本参考图像到视频生成</h3>
<p><strong>原文标题：</strong> Scaling Zero-Shot Reference-to-Video Generation</p>
<p><strong>摘要：</strong>
参考图像到视频（R2V）生成旨在合成既符合文本提示又保持参考图像中主体身份一致性的视频。然而，现有R2V方法受限于对显式参考图像-视频-文本三元组数据的依赖，此类数据的构建成本极高且难以规模化。我们通过引入Saber框架绕过了这一瓶颈，该框架是一种无需显式R2V数据的可扩展零样本解决方案。Saber仅使用视频-文本对进行训练，通过掩码训练策略与定制化的基于注意力的模型设计，学习身份一致且具有参考感知能力的表征。该框架进一步整合了掩码增强技术，以缓解参考图像到视频生成中常见的复制粘贴伪影问题。此外，Saber在不同数量参考图像条件下展现出卓越的泛化能力，并在OpenS2V-Eval基准测试中取得了优于使用R2V数据训练方法的性能表现。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.06905">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.06905">arXiv</a></p>
<hr />
<h3>6. DoVer：面向大语言模型多智能体系统的干预驱动自动调试方法</h3>
<p><strong>原文标题：</strong> DoVer: Intervention-Driven Auto Debugging for LLM Multi-Agent Systems</p>
<p><strong>摘要：</strong>
基于大语言模型的多智能体系统调试难度较高，其故障往往源于冗长且分支复杂的交互轨迹。当前主流方法依赖大语言模型进行基于日志的故障定位，将错误归因于特定智能体或执行步骤。然而，该范式存在两个关键局限：（一）纯日志调试缺乏验证环节，仅能生成未经检验的假设；（二）单步骤或单智能体归因往往定义不当，因为我们发现存在多种不同的干预方式均可独立修复任务故障。针对第一个局限，我们提出DoVer——一种干预驱动的调试框架，通过定向干预（如编辑消息、调整计划）进行主动验证，从而增强假设生成的可信度。对于第二个局限，我们不再评估归因准确性，转而聚焦于衡量系统是否能够解决故障或在任务成功路径上取得可量化的进展，这体现了以结果为导向的调试视角。在Magnetic-One智能体框架中，基于GAIA与AssistantBench衍生的数据集进行实验，DoVer成功将18-28%的失败案例转化为成功案例，实现最高16%的里程碑进度提升，并能验证或推翻30-60%的故障假设。在另一数据集（GSMPlus）与智能体框架（AG2）的测试中，DoVer同样表现优异，成功修复了49%的失败案例。这些结果凸显了干预机制在提升智能体系统可靠性方面的实用价值，并为开发更鲁棒、可扩展的大语言模型多智能体系统调试方法开辟了新路径。项目网站与代码将通过 https://aka.ms/DoVer 发布。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.06749">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.06749">arXiv</a></p>
<hr />
<h3>7. EgoEdit：面向第一人称视频编辑的数据集、实时流式处理模型与基准测试框架</h3>
<p><strong>原文标题：</strong> EgoEdit: Dataset, Real-Time Streaming Model, and Benchmark for Egocentric Video Editing</p>
<p><strong>摘要：</strong>
本研究针对交互式增强现实应用，探索基于指令引导的第一人称视频编辑技术。当前AI视频编辑工具虽在第三人称视角素材上表现良好，但第一人称视角因存在快速自身运动与频繁的手-物交互等独特挑战，形成了显著的领域差异。此外，现有离线编辑流程存在高延迟问题，制约了实时交互体验。为应对这些挑战，我们构建了一套完整的第一人称视频编辑生态系统。首先，我们创建了EgoEditData数据集——该数据集专为第一人称编辑场景设计，通过精细构建与人工标注，在显式保持手部完整性的同时，呈现丰富的手-物交互内容。其次，我们开发了EgoEdit模型，这是一个遵循编辑指令的第一人称视频编辑器，支持在单GPU上实现实时流式推理。最后，我们提出EgoEditBench评估体系，从指令遵循度、手部与交互保持度、自身运动下的时序稳定性三个维度建立评价标准。实验表明，在第一人称与通用编辑任务中，EgoEdit均能生成时序稳定、忠实于指令的编辑结果，并保持交互级延迟。在现有方法表现欠佳的第一人称编辑任务上，本方法取得显著性能提升，同时在通用编辑任务中保持与最优基线模型相当的性能。EgoEditData数据集与EgoEditBench评估套件将向研究社区公开，详情请访问项目网站：https://snap-research.github.io/EgoEdit</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.06065">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.06065">arXiv</a></p>
<hr />
<h3>8. 分布匹配变分自编码器</h3>
<p><strong>原文标题：</strong> Distribution Matching Variational AutoEncoder</p>
<p><strong>摘要：</strong>
大多数视觉生成模型在应用扩散或自回归建模之前，会将图像压缩至潜在空间。然而，现有方法（如变分自编码器及与基础模型对齐的编码器）往往隐式地约束潜在空间，而未显式地塑造其分布，这使得何种分布最适合建模尚不明确。本文提出分布匹配变分自编码器，其通过分布匹配约束显式地将编码器的潜在分布与任意参考分布对齐。该方法突破了传统变分自编码器高斯先验的局限，可实现与自监督特征、扩散噪声或其他先验分布所导出分布的对齐。借助分布匹配变分自编码器，我们能够系统探究何种潜在分布更有利于建模，并发现自监督学习导出的分布能在重建保真度与建模效率间取得优异平衡——仅经过64轮训练即在ImageNet数据集上达到gFID=3.2。实验结果表明：选择适宜的潜在分布结构（通过分布层级对齐实现），而非依赖固定先验，是弥合易建模潜在空间与高保真图像合成之间差距的关键。代码已发布于https://github.com/sen-ye/dmvae。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.07778">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.07778">arXiv</a></p>
<hr />
<h3>9. 关系视觉相似性</h3>
<p><strong>原文标题：</strong> Relational Visual Similarity</p>
<p><strong>摘要：</strong>
人类不仅能感知属性相似性——还能识别关系相似性。苹果与桃子相似是因为二者皆为红色水果，但地球同样与桃子相似：其地壳、地幔和地核分别对应桃子的表皮、果肉与果核。认知科学家认为，这种感知与识别关系相似性的能力正是人类区别于其他物种的关键特征。然而，当前广泛使用的视觉相似性度量方法（如LPIPS、CLIP、DINO）仅关注感知属性相似性，未能捕捉人类所感知的丰富且常令人惊奇的关系相似性。我们该如何超越图像的可见内容以捕捉其关系特性？又该如何在表征空间中使具有相同关系逻辑的图像彼此靠近？为回答这些问题，我们首先将关系图像相似性形式化为可度量问题：当两幅图像的视觉元素间存在对应的内部关系或功能关联时，即使其视觉属性不同，它们仍具有关系相似性。基于此，我们构建了包含11.4万条图像-文本对的数据集，其中文本描述经过匿名化处理——着重描述场景底层的关系逻辑而非表面内容。利用该数据集，我们对视觉-语言模型进行微调，以度量图像间的关系相似性。该模型首次实现了通过底层关系结构（而非可见外观）连接图像的研究目标。研究表明，尽管关系相似性具有广泛的实际应用价值，现有图像相似性模型均未能有效捕捉这一特性——这揭示了视觉计算领域存在的重要空白。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.07833">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.07833">arXiv</a></p>
<hr />
<h3>10. 多视角金字塔变换器：见宏窥微，视野更广</h3>
<p><strong>原文标题：</strong> Multi-view Pyramid Transformer: Look Coarser to See Broader</p>
<p><strong>摘要：</strong>
本文提出多视角金字塔变换器（MVP），这是一种可扩展的多视角变换器架构，能够在前向单次推理中直接根据数十至数百张图像重建大规模三维场景。基于“观全局以窥全貌，察细微以见真章”的设计理念，MVP建立在两个核心设计原则之上：1）局部到全局的视角间层次结构，使模型视野从局部视角逐步扩展至视角组，最终覆盖完整场景；2）精细到粗略的视角内层次结构，从详细的空间表征出发，逐步将其聚合为紧凑且信息密集的令牌。这种双重层次结构兼顾计算效率与表征丰富性，实现了对大规模复杂场景的快速重建。我们在多个数据集上验证了MVP的性能，结果表明：当以三维高斯溅射作为底层三维表征方法时，该架构在保持高效率与强可扩展性的同时，能够适应多种视角配置，并达到当前最优的泛化重建质量。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.07806">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.07806">arXiv</a></p>
<hr />
<h3>11. 论预训练、中期训练与强化学习在推理语言模型中的相互作用</h3>
<p><strong>原文标题：</strong> On the Interplay of Pre-Training, Mid-Training, and RL on Reasoning Language Models</p>
<p><strong>摘要：</strong>
尽管近期的强化学习（RL）技术显著提升了语言模型的推理能力，但后训练是否真正扩展了模型超越预训练阶段所获能力的问题尚未明确。当前训练流程的核心挑战在于缺乏可控性：大规模预训练语料库不透明，中期训练常被忽视，且强化学习目标与未知的先验知识以复杂方式相互作用。为厘清这一模糊性，本研究构建了一个完全受控的实验框架，以分离预训练、中期训练和基于强化学习的后训练之间的因果贡献。该方法采用合成推理任务，这些任务具有明确的原子操作、可解析的逐步推理轨迹，并能系统调控训练数据分布。我们从两个维度评估模型：面向更复杂组合的外推泛化能力，以及跨不同表层语境的上下文泛化能力。通过该框架，我们调和了关于强化学习有效性的对立观点。研究发现：1）仅当预训练留有足够提升空间，且强化学习数据针对模型能力边界（即困难但尚未完全超出掌握范围的任务）时，强化学习才能带来真实的能力提升（pass@128）；2）上下文泛化仅需最小但充分的预训练接触，此后强化学习可稳定实现能力迁移；3）在固定计算量下，中期训练相比仅使用强化学习能显著提升性能，揭示了其在训练流程中关键但尚未被充分探索的作用；4）过程级奖励机制能减少奖励欺骗行为并提升推理可靠性。这些结果共同阐明了预训练、中期训练与强化学习之间的相互作用，为理解和改进推理语言模型的训练策略奠定了基础。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.07783">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.07783">arXiv</a></p>
<hr />
<h3>12. LongCat-Image 技术报告</h3>
<p><strong>原文标题：</strong> LongCat-Image Technical Report</p>
<p><strong>摘要：</strong>
我们推出 LongCat-Image，这是一个开创性的开源、双语（中英）图像生成基础模型，旨在解决当前主流模型在多语言文本渲染、照片级真实感、部署效率和开发者可访问性方面的核心挑战。1）我们通过在预训练、中期训练和 SFT 阶段实施严格的数据策展策略，并在强化学习阶段协调使用精心构建的奖励模型来实现这一目标。这一策略使模型达到了新的最先进水平，提供了卓越的文本渲染能力和出色的照片级真实感，并显著提升了美学质量。2）值得注意的是，它为汉字渲染树立了新的行业标准。通过支持甚至复杂和罕见的字符，其在覆盖范围上超越了主流开源和商业解决方案，同时实现了更高的准确性。3）该模型通过紧凑的设计实现了卓越的效率。其核心扩散模型仅包含 60 亿参数，远小于该领域常见的近 200 亿或更大规模的混合专家架构。这确保了极低的显存占用和快速的推理速度，显著降低了部署成本。除了生成能力，LongCat-Image 在图像编辑方面同样表现出色，在标准基准测试中取得了最先进的结果，与其他开源作品相比，具有更优的编辑一致性。4）为了全面赋能社区，我们建立了迄今为止最全面的开源生态系统。我们不仅发布了用于文本到图像和图像编辑的多个模型版本（包括中期训练和训练后阶段的检查点），还开放了整个训练流程的工具链。我们相信，LongCat-Image 的开放性将为开发者和研究人员提供强有力的支持，推动视觉内容创作的前沿发展。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.07584">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.07584">arXiv</a></p>
<hr />
<h3>13. UnityVideo：面向增强世界感知视频生成的统一多模态多任务学习框架</h3>
<p><strong>原文标题：</strong> UnityVideo: Unified Multi-Modal Multi-Task Learning for Enhancing World-Aware Video Generation</p>
<p><strong>摘要：</strong>
当前视频生成模型虽展现出卓越的合成能力，但仍受限于单模态条件约束，制约了其对世界的整体理解能力。这一局限源于跨模态交互不足以及模态多样性有限，难以全面表征世界知识。为突破这些限制，我们提出UnityVideo——一个面向世界感知视频生成的统一框架，能够跨越多重模态（分割掩码、人体骨架、密集姿态、光流与深度图）与训练范式进行联合学习。本框架包含两大核心组件：（1）动态噪声注入机制，用于统一异构训练范式；（2）搭载上下文学习器的模态切换器，通过模块化参数与上下文学习实现统一处理。我们构建了包含130万样本的大规模统一数据集。通过联合优化，UnityVideo显著加速模型收敛，并大幅提升对未见数据的零样本泛化能力。实验表明，UnityVideo在视频质量、时序一致性及物理世界约束对齐方面均取得优越性能。代码与数据详见：https://github.com/dvlab-research/UnityVideo</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.07831">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.07831">arXiv</a></p>
<hr />
<h3>14. 视觉生成调优</h3>
<p><strong>原文标题：</strong> Visual Generation Tuning</p>
<p><strong>摘要：</strong>
大规模视觉语言模型通过广泛的预训练有效弥合了模态鸿沟，获得了与语言对齐的复杂视觉表征。然而，这些为多模态理解任务优化的表征是否蕴含视觉生成的内在潜力，目前仍未得到充分探索。本文提出视觉生成调优这一新范式，旨在激发任意视觉语言模型中潜在的视觉生成能力。通过对预训练良好的视觉语言模型进行高效的视觉生成调优，我们显著降低了对齐成本，并加速了连续空间中自回归建模的收敛速度（提速20倍）。具体而言，我们摒弃了为扩散变换器设计的纠缠像素级变分自编码器，通过将预训练视觉语言模型的语义编码器与像素解码器的潜在表征对齐，构建了视觉生成调优自编码器。在图像重建任务中，我们在28倍压缩率下实现了26.67峰值信噪比和0.50相对弗雷歇距离，性能优于专用变分自编码器；在视觉生成任务中，我们在自回归模型中取得了最先进的结果——在生成评估基准上达到0.77分，在深度感知生成基准上达到78.73分。此外，所提出的视觉生成调优展现出显著的扩展潜力，能够灵活赋能任何为多模态理解训练的视觉语言模型，使其具备视觉生成能力，这为探索下一代统一多模态基础模型开辟了新路径。模型与代码已发布于https://github.com/hustvl/VGT。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.23469">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.23469">arXiv</a></p>
<hr />
<h3>15. SPARK：面向无参考强化学习的逐步过程感知奖励机制</h3>
<p><strong>原文标题：</strong> SPARK: Stepwise Process-Aware Rewards for Reference-Free Reinforcement Learning</p>
<p><strong>摘要：</strong>
提供密集步骤级反馈的过程奖励模型（PRMs）在强化学习中展现出潜力，但其应用仍受限于对昂贵步骤级标注或真实参考答案的需求。本文提出SPARK三阶段框架：第一阶段通过生成模型产生多样化解决方案，并利用并行扩展（自洽性）与序列扩展（元批判）的验证模型进行评估；第二阶段将验证输出作为合成训练数据微调生成式过程奖励模型，使其在训练中充当奖励信号。研究表明，在步骤层面聚合多个独立验证生成的数据能训练出超越真实结果监督的过程奖励模型——在数学推理错误步骤识别基准ProcessBench上达到67.5 F1值，优于参考引导训练的66.4和GPT-4o的61.9。第三阶段将带有思维链验证的生成式过程奖励模型（PRM-CoT）作为数学推理强化学习的奖励函数，并引入格式约束防止奖励攻击。基于Qwen2.5-Math-7B模型，我们在六项数学推理基准测试中取得47.4%的平均准确率，超越基于真实答案的RLVR方法（43.9%）。本工作实现了超越真实答案方法的无参考强化学习训练，为缺乏可验证答案或可获取真实数据的领域开辟了新可能。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.03244">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.03244">arXiv</a></p>
<hr />
<h3>16. VG-Refiner：基于智能体强化学习的工具精化指称式接地推理研究</h3>
<p><strong>原文标题：</strong> VG-Refiner: Towards Tool-Refined Referring Grounded Reasoning via Agentic Reinforcement Learning</p>
<p><strong>摘要：</strong>
工具集成视觉推理（TiVR）在增强多模态问题解决方面展现出巨大潜力。然而，现有TiVR范式主要侧重于通过强化学习集成各类视觉工具，却未能设计有效的响应机制来处理不可靠或错误的工具输出。这一局限在指称与接地任务中尤为突出，其中不准确的检测工具预测常误导TiVR模型产生幻觉推理。为解决此问题，我们提出VG-Refiner——首个面向工具精化的指称式接地推理框架。技术上，我们引入两阶段“思考-再思考”机制，使模型能够显式分析并响应工具反馈，同时设计精化奖励机制以激励模型针对低质量工具结果进行有效修正。此外，我们提出两项新评估指标并建立公平评测协议，以系统衡量现有模型的精化能力。通过采用少量任务特定数据增强VG-Refiner的精化能力，我们在指称与推理接地基准测试中实现了准确率与修正能力的显著提升，同时保持了预训练模型的通用能力。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.06373">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.06373">arXiv</a></p>
<hr />
<h3>17. ReCamDriving：一种无需激光雷达的相机控制新型轨迹视频生成方法</h3>
<p><strong>原文标题：</strong> ReCamDriving: LiDAR-Free Camera-Controlled Novel Trajectory Video Generation</p>
<p><strong>摘要：</strong>
本文提出ReCamDriving，一种纯视觉的、相机控制的新型轨迹视频生成框架。现有基于修复的方法难以恢复复杂伪影，而基于激光雷达的方法依赖稀疏且不完整的线索。ReCamDriving利用密集且场景完整的3D高斯泼溅渲染结果提供显式几何引导，实现了精确的相机可控生成。为缓解基于3DGS渲染条件训练时对修复行为的过拟合问题，ReCamDriving采用两阶段训练范式：第一阶段使用相机位姿进行粗粒度控制，第二阶段引入3DGS渲染实现细粒度视角与几何引导。此外，我们提出基于3DGS的跨轨迹数据构建策略，以消除相机变换模式在训练与测试阶段的差异，从而能够从单目视频中实现可扩展的多轨迹监督。基于此策略，我们构建了ParaDrive数据集，包含超过11万组平行轨迹视频对。大量实验表明，ReCamDriving在相机控制精度与结构一致性方面达到了最先进的性能水平。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.03621">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.03621">arXiv</a></p>
<hr />
<h3>18. OmniSafeBench-MM：多模态越狱攻击-防御评估的统一基准与工具箱</h3>
<p><strong>原文标题：</strong> OmniSafeBench-MM: A Unified Benchmark and Toolbox for Multimodal Jailbreak Attack-Defense Evaluation</p>
<p><strong>摘要：</strong>
多模态大语言模型（MLLMs）的最新进展实现了统一的感知-推理能力，但这些系统仍极易受到越狱攻击，从而绕过安全对齐机制并引发有害行为。现有基准如JailBreakV-28K、MM-SafetyBench和HADES为多模态漏洞提供了有价值的见解，但它们通常局限于有限的攻击场景，缺乏标准化的防御评估，且未提供统一、可复现的工具箱。为弥补这些不足，我们提出了OmniSafeBench-MM，这是一个用于多模态越狱攻击-防御评估的综合工具箱。OmniSafeBench-MM整合了13种代表性攻击方法、15种防御策略，以及一个涵盖9个主要风险领域和50个细粒度类别的多样化数据集，并按照咨询式、指令式和陈述式查询类型进行结构化设计，以反映真实的用户意图。除数据覆盖外，该基准还建立了一个三维评估协议，用于衡量：（1）危害性，采用从低影响个体危害到灾难性社会威胁的细粒度多级尺度进行区分；（2）响应与查询之间的意图对齐度；（3）响应详细程度，从而支持细致的安全-效用分析。我们对10个开源和8个闭源MLLMs进行了广泛实验，揭示了它们对多模态越狱攻击的脆弱性。通过将数据、方法与评估统一到一个开源、可复现的平台中，OmniSafeBench-MM为未来研究提供了标准化基础。代码发布于https://github.com/jiaxiaojunQAQ/OmniSafeBench-MM。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.06589">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.06589">arXiv</a></p>
<hr />
<h3>19. 超越词元级监督：通过强化学习释放基于解码的回归潜力</h3>
<p><strong>原文标题：</strong> Beyond Token-level Supervision: Unlocking the Potential of Decoding-based Regression via Reinforcement Learning</p>
<p><strong>摘要：</strong>
基于解码的回归将回归任务重新定义为序列生成问题，已成为应用大语言模型进行数值预测的一种前景广阔的范式。然而，其发展受到离散词元级目标（如交叉熵）与连续数值之间不匹配的制约。现有依赖词元级约束的方法往往难以捕捉目标值的全局量级，限制了预测精度与泛化能力。本文提出通过强化学习释放基于解码的回归潜力。我们将生成过程建模为马尔可夫决策过程，利用序列级奖励来强化全局数值一致性。在表格数据回归与代码度量回归上的大量实验表明，本方法（特别是结合ReMax与GRPO算法）在性能上持续优于最先进的词元级基线方法与传统回归头，证明了引入序列级信号的优势。进一步分析显示，强化学习显著提升了采样效率与预测精度，从而确立了基于解码的回归作为一种鲁棒且精确的通用数值预测范式。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.06533">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.06533">arXiv</a></p>
<hr />
<h3>20. OpenSubject：利用视频衍生的身份与多样性先验知识实现主体驱动的图像生成与编辑</h3>
<p><strong>原文标题：</strong> OpenSubject: Leveraging Video-Derived Identity and Diversity Priors for Subject-driven Image Generation and Manipulation</p>
<p><strong>摘要：</strong>
尽管主体驱动的图像生成已取得显著进展，但现有模型常偏离参考身份特征，且在包含多主体的复杂场景中表现欠佳。为应对这一挑战，我们提出了OpenSubject——一个基于视频构建的大规模数据集，包含250万个样本和435万张图像，专用于主体驱动的生成与编辑任务。该数据集通过利用跨帧身份先验的四阶段流程构建：（一）视频筛选。通过分辨率与美学过滤获取高质量视频片段。（二）跨帧主体挖掘与配对。基于视觉-语言模型的类别共识、局部定位和多样性感知配对策略筛选图像对。（三）身份保持的参考图像合成。采用分割图引导的外延绘制技术合成主体驱动生成的输入图像，结合边界框引导的内绘技术生成主体驱动编辑的输入图像，并辅以几何感知增强与不规则边界侵蚀处理。（四）验证与描述生成。使用视觉-语言模型验证合成样本，对未达标样本基于第三阶段流程重新合成，最终构建简洁与详细的双重描述文本。此外，我们建立了覆盖主体驱动生成与编辑任务的评测基准，通过视觉-语言模型评估身份保真度、提示符遵从性、编辑一致性与背景一致性。大量实验表明，使用OpenSubject进行训练能显著提升生成与编辑性能，尤其在复杂场景中效果突出。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.08294">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.08294">arXiv</a></p>
<hr />
<h3>21. 一层足矣：面向图像生成任务的自适应预训练视觉编码器</h3>
<p><strong>原文标题：</strong> One Layer Is Enough: Adapting Pretrained Visual Encoders for Image Generation</p>
<p><strong>摘要：</strong>
视觉生成模型（如扩散模型）通常在压缩的潜在空间中运行，以平衡训练效率与生成质量。与此同时，利用高质量预训练视觉表征的研究日益受到关注，常见方法包括将其与变分自编码器（VAE）内部对齐或直接整合到生成模型中。然而，由于理解导向的特征与生成友好的潜在空间之间存在本质性错配，此类表征的适配仍具挑战性：表征编码器受益于高维潜在空间以捕捉掩码区域的多样化假设，而生成模型则倾向于低维潜在空间以忠实保留注入的噪声。这种差异导致先前研究不得不依赖复杂的优化目标和架构设计。本文提出特征自编码器（FAE），这是一个简洁而高效的框架，能够将预训练的视觉表征适配至适用于生成任务的低维潜在空间，仅需使用单个注意力层即可实现，同时保留足够的信息以支持重建和理解任务。其核心在于耦合两个独立的深度解码器：一个训练用于重建原始特征空间，另一个则以重建后的特征作为输入进行图像生成。FAE具有通用性，可与多种自监督编码器（如DINO、SigLIP）结合使用，并能嵌入到两类不同的生成模型家族中：扩散模型与标准化流。在类别条件生成和文本到图像生成的基准测试中，FAE均表现出强劲性能。例如，在ImageNet 256×256数据集上，我们搭载分类器引导（CFG）的扩散模型取得了接近最优的FID分数1.29（800训练周期）和1.70（80训练周期）；在不使用CFG的情况下，FAE达到了当前最优的FID分数1.48（800周期）和2.08（80周期），充分证明了其高质量生成与快速学习的能力。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.07829">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.07829">arXiv</a></p>
<hr />
<h3>22. 群表示位置编码</h3>
<p><strong>原文标题：</strong> Group Representational Position Encoding</p>
<p><strong>摘要：</strong>
本文提出GRAPE（群表示位置编码），一种基于群作用的统一位置编码框架。GRAPE整合了两类机制：（1）SO(d)群中的乘法旋转（乘法GRAPE）；（2）源于一般线性群GL中单能作用的加法逻辑偏置（加法GRAPE）。在乘法GRAPE中，Z中的位置n（或R中的t）通过G(n)=exp(n,ω,L)作用，其中L为R^{d×d}中的二阶斜对称生成元，产生具有闭式矩阵指数的相对性、复合性、保范映射。当d/2个平面为具有对数均匀谱的标准坐标对时，可精确恢复RoPE。通过学习可交换子空间和紧致非交换混合，该几何结构被严格扩展至能捕捉子空间特征耦合，每注意力头计算成本分别为O(d)和O(r d)。在加法GRAPE中，加法逻辑值以一阶（或低阶）单能作用形式产生，精确恢复ALiBi与遗忘变换器（FoX）作为特例，同时保持精确的相对性规律和流式缓存能力。总体而言，GRAPE为长上下文模型中的位置几何提供了原则性设计空间，将RoPE与ALiBi统一为特例。项目页面：https://github.com/model-architectures/GRAPE。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.07805">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.07805">arXiv</a></p>
<hr />
<h3>23. 解耦以泛化：面向数据稀缺视觉语言推理的上下文优先自演化学习</h3>
<p><strong>原文标题：</strong> Decouple to Generalize: Context-First Self-Evolving Learning for Data-Scarce Vision-Language Reasoning</p>
<p><strong>摘要：</strong>
近期视觉语言模型通过强化学习实现了卓越的推理能力，为在经验时代实现持续自我演化的大型视觉语言模型提供了可行路径。然而，视觉语言模型的强化学习需要大量高质量多模态数据，在化学、地球科学及多模态数学等专业领域尤为困难。现有策略如合成数据与自奖励机制存在分布局限与对齐难题，最终导致奖励破解现象：模型利用高奖励模式，致使策略熵崩溃并破坏训练稳定性。本文提出DoGe（解耦泛化）框架，该双解耦机制引导模型优先从上下文而非问题求解中学习，通过重新聚焦被合成数据方法忽视的问题情境场景实现突破。通过将学习过程解耦为双组件（思考器与求解器），我们合理量化该过程的奖励信号，并提出从自由探索上下文到实际解决问题的两阶段强化学习后训练方法。其次，为提升训练数据多样性，DoGe构建了动态课程学习流程：包含扩展的领域知识语料库与迭代演化的种子问题池。实验表明，本方法在多个基准测试中持续超越基线模型，为实现自演化大型视觉语言模型提供了可扩展路径。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.06835">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.06835">arXiv</a></p>
<hr />
<h3>24. VideoVLA：视频生成器可作为通用机器人操作器</h3>
<p><strong>原文标题：</strong> VideoVLA: Video Generators Can Be Generalizable Robot Manipulators</p>
<p><strong>摘要：</strong>
机器人操作中的泛化能力对于在开放世界环境中部署机器人以及迈向通用人工智能至关重要。尽管当前的视觉-语言-动作模型利用大规模预训练理解模型进行感知和指令跟随，但其在新任务、新物体和新环境中的泛化能力仍然有限。本文提出VideoVLA，这是一种探索将大型视频生成模型转化为机器人视觉-语言-动作操作器的简单方法。给定语言指令和图像，VideoVLA能够预测动作序列及未来的视觉结果。基于多模态扩散变换器架构，VideoVLA联合建模视频、语言和动作模态，利用预训练视频生成模型实现视觉与动作的协同预测。实验表明，高质量的视觉想象与可靠的动作预测及任务成功率高度相关，凸显了视觉想象在操作任务中的重要性。VideoVLA展现出强大的泛化能力，包括模仿其他智能体的技能和处理新物体。这种同时预测动作及其视觉结果的双重预测策略，探索了机器人学习范式的转变，并释放了操作系统的泛化潜力。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.06963">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.06963">arXiv</a></p>
<hr />
<h3>25. 尺度自回归生成中的训练动态机制再思考</h3>
<p><strong>原文标题：</strong> Rethinking Training Dynamics in Scale-wise Autoregressive Generation</p>
<p><strong>摘要：</strong>
自回归生成模型的最新进展催生了日益强大的媒体合成系统。其中，跨尺度预测已成为主流范式，模型通过从粗粒度到细粒度的方式生成图像。然而，尺度自回归模型存在曝光偏差问题，严重影响生成质量。我们识别出该问题的两个主要成因：（1）训练-测试失配，即模型在推理过程中必须依赖自身不完美的预测结果；（2）跨尺度学习难度失衡，某些尺度表现出不成比例的高优化复杂度。通过对训练动态的全面分析，我们提出自回归精炼方法以解决这些局限性。该方法包含两个核心机制：交错尺度展开机制通过轻量级自回归展开使模型接触自身中间预测结果，从而实现训练-测试模式对齐；以及互补的对比学生强制损失函数，为自生成语境提供充分监督以确保训练稳定性。实验结果表明，将自回归精炼应用于预训练的自回归模型后，能以最小计算开销持续提升生成质量。例如在ImageNet 256数据集上，对FlexVAR-d16模型进行10轮训练（32xA100 GPU运行5小时）即可实现FID指标5.2%的降低。鉴于该方法在效率、可扩展性和有效性方面的优势，我们预期自回归精炼将成为视觉自回归生成领域可靠的训练后优化方案。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.06421">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.06421">arXiv</a></p>
<hr />
<h3>26. 小增益纳什：可微博弈中经认证的纳什均衡收缩方法</h3>
<p><strong>原文标题：</strong> Small-Gain Nash: Certified Contraction to Nash Equilibria in Differentiable Games</p>
<p><strong>摘要：</strong>
经典博弈梯度学习收敛性证明要求伪梯度在欧几里得几何中满足（强）单调性条件（如Rosen于1965年所示），但该条件即使在具有强跨参与者耦合的简单博弈中也常不成立。本文提出小增益纳什方法，这是一种在定制块加权几何中的块小增益条件。该方法将局部曲率与跨参与者Lipschitz耦合边界转化为可处理的收缩性证明，通过构建加权块度量使伪梯度在满足这些边界的任意区域呈现强单调性——即使其在欧几里得意义下非单调。连续流在此设计几何中呈指数收缩，而投影欧拉法与RK4离散化在基于SGN裕度与局部Lipschitz常数导出的显式步长范围内收敛。我们的分析揭示了一种经认证的“时间尺度带”，这是一种基于度量的非渐近证明，其作用类似于TTUR机制：不同于通过渐近不等步长强制时间尺度分离，SGN可识别相对度量权重的有限带域，使得单步长动力学具备可证明的收缩性。我们在二次博弈中验证了该框架的有效性——传统欧几里得单调性分析在此类博弈中无法预测收敛，而SGN成功完成认证，并将该构造扩展至马尔可夫博弈中熵正则化策略梯度的镜像/费希尔几何。最终形成离线认证流程：在紧致区域估计曲率、耦合与Lipschitz参数，优化块权重以扩大SGN裕度，并为非单调博弈输出包含度量、收缩率及安全步长的结构化可计算收敛证明。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.06791">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.06791">arXiv</a></p>
<hr />
<h3>27. 基于高斯变分自编码器的向量量化方法</h3>
<p><strong>原文标题：</strong> Vector Quantization using Gaussian Variational Autoencoder</p>
<p><strong>摘要：</strong>
向量量化变分自编码器（VQ-VAE）是一种将图像压缩为离散标记的离散自编码器，其离散化特性导致模型训练困难。本文提出一种名为高斯量化（GQ）的简洁有效技术，可将具有特定约束的高斯变分自编码器无需训练即可转换为VQ-VAE。该方法通过生成随机高斯噪声构建码本，并寻找与后验均值最接近的噪声向量。理论上，我们证明当码本对数值超过高斯变分自编码器的比特回传编码率时，可确保较小的量化误差。实践中，我们提出一种针对GQ优化的高斯变分自编码器训练启发式策略——目标散度约束（TDC）。实验表明，在UNet和ViT架构上，GQ方法在性能上优于VQGAN、FSQ、LFQ和BSQ等现有VQ-VAE模型。此外，TDC策略也较TokenBridge等现有高斯变分自编码器离散化方法有所提升。源代码已发布于https://github.com/tongdaxu/VQ-VAE-from-Gaussian-VAE。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.06609">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.06609">arXiv</a></p>
<hr />
<h3>28. 人机交互中的具身指代表达理解</h3>
<p><strong>原文标题：</strong> Embodied Referring Expression Comprehension in Human-Robot Interaction</p>
<p><strong>摘要：</strong>
随着机器人进入人类工作空间，其理解具身化人类指令以实现直观流畅的人机交互（HRI）的需求日益迫切。然而，由于缺乏能够捕捉多样化HRI场景中自然具身交互的大规模数据集，准确理解仍面临挑战。现有数据集普遍存在视角偏差、单视角采集、非语言手势覆盖不足以及过度聚焦室内环境等问题。为解决这些局限性，我们提出了Refer360数据集——一个在室内外多视角环境下采集的大规模具身化语言与非语言交互数据集。此外，我们设计了多模态引导残差模块MuRes，以提升具身指代表达理解能力。该模块通过构建信息瓶颈，提取关键模态特征信号并将其强化至预训练表征中，从而为下游任务构建互补特征。我们在包括Refer360在内的四个HRI数据集上进行了广泛实验，结果表明当前多模态模型难以全面捕捉具身交互特征，而引入MuRes模块能持续提升模型性能。这些发现确立了Refer360作为重要基准数据集的价值，同时揭示了引导残差学习在推动人类环境中机器人具身指代表达理解方面的潜力。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.06558">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.06558">arXiv</a></p>
<hr />
<h3>29. 基于格式强化学习的结构化文档翻译</h3>
<p><strong>原文标题：</strong> Structured Document Translation via Format Reinforcement Learning</p>
<p><strong>摘要：</strong>
当前结构化文本翻译的研究仍局限于句子层面，难以有效处理复杂的文档级XML或HTML结构。为此，我们提出格式强化学习方法，该方法在监督微调模型基础上采用组相对策略优化，直接优化两种新型结构感知奖励函数：1）TreeSim——衡量预测XML树与参考XML树之间的结构相似度；2）Node-chrF——在XML节点层面评估翻译质量。此外，我们引入StrucAUC细粒度评估指标，以区分细微错误与重大结构缺陷。在SAP软件文档基准测试上的实验表明，该方法在六项指标上均取得提升；进一步分析揭示了不同奖励函数如何协同提升结构完整性与翻译质量。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.05100">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.05100">arXiv</a></p>
<hr />
<h3>30. DZ-TDPO：面向长上下文对话中可变状态追踪的非破坏性时序对齐方法</h3>
<p><strong>原文标题：</strong> DZ-TDPO: Non-Destructive Temporal Alignment for Mutable State Tracking in Long-Context Dialogue</p>
<p><strong>摘要：</strong>
长上下文对话系统普遍存在“状态惯性”问题，即静态约束阻碍模型在动态演变的用户意图与既定历史语境之间有效化解冲突。为解决此问题，我们提出DZ-TDPO——一种非破坏性对齐框架，该框架通过融合冲突感知的动态KL约束与校准化的时序注意力偏置实现协同优化。基于多轮会话聊天（MSC）数据集的实验表明，DZ-TDPO在Phi-3.5模型上达到55.4%的胜率，取得当前最优性能，同时保持稳健的零样本泛化能力。我们的扩展分析揭示出“容量-稳定性权衡”规律：较小模型需付出“对齐代价”（困惑度激增）以克服历史惯性，而规模更大的Qwen2.5-7B模型仅以可忽略的困惑度开销即实现50.8%的胜率。这证实了时序注意力惯性可通过精准的注意力调控（而非破坏性的权重更新）得到缓解，从而在不同模型规模下保持通用能力（如MMLU基准表现）。代码与数据已开源：https://github.com/lyj20071013/DZ-TDPO</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.03704">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.03704">arXiv</a></p>
<hr />
<h3>31. JEPA作为神经分词器：基于密度自适应注意力的鲁棒语音表征学习</h3>
<p><strong>原文标题：</strong> JEPA as a Neural Tokenizer: Learning Robust Speech Representations with Density Adaptive Attention</p>
<p><strong>摘要：</strong>
本文提出一种两阶段自监督学习框架，将联合嵌入预测架构（JEPA）与密度自适应注意力机制（DAAM）相结合，用于学习鲁棒的语音表征。第一阶段采用集成DAAM的JEPA架构，通过在隐空间进行掩码预测来学习语义音频特征，该过程完全解耦于波形重建任务。第二阶段利用有限标量化（FSQ）和混合基数打包方案，将学习到的表征高效转换为离散化符号序列，再通过HiFi-GAN解码器实现高保真波形重建。通过将基于高斯混合模型的密度自适应门控机制融入JEPA编码器，该模型能够以2.5Hz的低帧率实现自适应时序特征选择，并发现语音的层次化结构。最终生成的符号序列（47.5符号/秒）具有可逆性、高压缩性和语言模型友好性，其性能与现有神经音频编解码器相当，且通常具有更高的编码效率。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.07168">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.07168">arXiv</a></p>
<hr />
<h3>32. SAM系列模型中SAM2到SAM3的断层：基于提示的专长为何在概念驱动的图像分割中失效</h3>
<p><strong>原文标题：</strong> The SAM2-to-SAM3 Gap in the Segment Anything Model Family: Why Prompt-Based Expertise Fails in Concept-Driven Image Segmentation</p>
<p><strong>摘要：</strong>
本文研究了最新两代Segment Anything模型（SAM2与SAM3）之间的根本性断层。我们阐释了SAM2在基于提示的分割任务中所积累的专业知识为何无法迁移至SAM3的多模态概念驱动范式。SAM2通过空间提示（点、框、掩码）进行操作，产生纯几何与时间维度的分割结果；相比之下，SAM3引入了统一的视觉-语言架构，具备开放词汇推理、语义基础、对比对齐及基于范例的概念理解能力。本文通过五个核心组成部分展开分析：（1）基于提示与基于概念的分割之间的理念断层，对比SAM2的空间提示语义与SAM3的多模态融合及文本条件掩码生成机制；（2）架构差异，详述SAM2的纯视觉-时序设计与SAM3对视觉-语言编码器、几何与范例编码器、融合模块、DETR风格解码器、对象查询以及通过专家混合处理歧义等技术的整合；（3）数据集与标注差异，对比SAM2的SA-V视频掩码与SAM3的多模态概念标注语料库；（4）训练与超参数区别，说明为何SAM2的优化经验不适用于SAM3；（5）评估指标与失效模式，梳理从几何交并比指标向语义化开放词汇评估体系的转变。这些分析共同确立了SAM3作为新一代分割基础模型的地位，并为新兴的概念驱动分割时代指明了未来发展方向。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.06032">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.06032">arXiv</a></p>
<hr />
<h2>🔍 关键词云图</h2>
<p><img alt="关键词云图" src="../images/keywords_wordcloud.png" /></p>
<h2>📈 近期论文趋势</h2>
<p><img alt="论文趋势" src="../images/daily_papers.png" /></p>
<h2>🎙️ 语音播报</h2>
<ul>
<li><a href="../audio/2025-12-09_daily_papers.mp3">收听今日论文解读</a></li>
</ul>
<h2>📱 订阅渠道</h2>
<ul>
<li>GitHub: <a href="https://github.com/2404589803/hf-daily-paper-newsletter-chinese">hf-daily-paper-newsletter-chinese</a></li>
</ul>
    </div>
</body>
</html>