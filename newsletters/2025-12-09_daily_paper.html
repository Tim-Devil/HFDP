<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Hugging Face 论文日报 - 2025-12-09</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', sans-serif;
            line-height: 1.6;
            color: #333;
            background-color: #f5f5f5;
            padding: 20px;
        }
        
        .container {
            max-width: 900px;
            margin: 0 auto;
            background-color: white;
            padding: 40px;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }
        
        h1 {
            color: #2c3e50;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
            margin-bottom: 30px;
            font-size: 28px;
        }
        
        h1 img {
            vertical-align: middle;
            margin-right: 10px;
        }
        
        h2 {
            color: #34495e;
            margin-top: 40px;
            margin-bottom: 20px;
            font-size: 24px;
            border-left: 4px solid #3498db;
            padding-left: 15px;
        }
        
        h3 {
            color: #2c3e50;
            margin-top: 30px;
            margin-bottom: 15px;
            font-size: 20px;
        }
        
        p {
            margin-bottom: 15px;
            text-align: justify;
        }
        
        ul {
            margin-left: 20px;
            margin-bottom: 20px;
        }
        
        li {
            margin-bottom: 8px;
        }
        
        a {
            color: #3498db;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        strong {
            color: #2c3e50;
            font-weight: 600;
        }
        
        hr {
            border: none;
            border-top: 1px solid #e0e0e0;
            margin: 30px 0;
        }
        
        /* 关键修复:限制图片宽度 */
        img {
            max-width: 100%;
            height: auto;
            display: block;
            margin: 20px auto;
            border-radius: 4px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        
        /* 确保图片容器也有宽度限制 */
        p img {
            max-width: 100%;
        }
        
        /* 论文详情区域样式 */
        .paper-section {
            background-color: #f9f9f9;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 20px;
        }
        
        /* 统计信息样式 */
        .stats {
            background-color: #e8f4f8;
            padding: 15px;
            border-radius: 6px;
            margin-bottom: 20px;
        }
        
        /* 响应式设计 */
        @media (max-width: 768px) {
            .container {
                padding: 20px;
            }
            
            h1 {
                font-size: 24px;
            }
            
            h2 {
                font-size: 20px;
            }
            
            h3 {
                font-size: 18px;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <h1><img src="https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo.png" width="30"/> Hugging Face 2025-12-09 论文日报</h1>
<h2>📊 今日论文统计</h2>
<ul>
<li>总论文数：32</li>
<li>热门领域：LLM, RL, GPT</li>
</ul>
<h2>📝 论文详情</h2>
<h3>1. 原生并行推理器：通过自蒸馏强化学习实现并行推理</h3>
<p><strong>原文标题：</strong> Native Parallel Reasoner: Reasoning in Parallelism via Self-Distilled Reinforcement Learning</p>
<p><strong>摘要：</strong>
本文提出原生并行推理器（NPR），一种无需教师模型的框架，使大语言模型（LLM）能够自我演化出真正的并行推理能力。NPR通过三项关键创新将模型从序列化模仿转变为原生并行认知：1）自蒸馏渐进式训练范式，在无外部监督的情况下，从“冷启动”格式发现过渡到严格的拓扑约束；2）新颖的并行感知策略优化（PAPO）算法，直接在执行图中优化分支策略，使模型能够通过试错学习自适应分解；3）稳健的NPR引擎，重构SGLang的内存管理与流程控制，实现稳定的大规模并行强化学习训练。在八个推理基准测试中，基于Qwen3-4B训练的NPR实现了高达24.5%的性能提升和最高4.6倍的推理加速。与先前常退化为自回归解码的基线方法不同，NPR实现了100%的真正并行执行，为自我演化、高效且可扩展的智能体推理设立了新标准。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.07461">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.07461">arXiv</a></p>
<hr />
<h3>2. 超越实数：长上下文大语言模型中旋转位置编码的虚部扩展</h3>
<p><strong>原文标题：</strong> Beyond Real: Imaginary Extension of Rotary Position Embeddings for Long-Context LLMs</p>
<p><strong>摘要：</strong>
旋转位置编码通过将查询向量和键向量映射到复数平面进行旋转，已成为大语言模型中编码序列顺序的标准方法。然而，标准实现仅使用复数点积的实部计算注意力分数。这种简化舍弃了包含重要相位信息的虚部，可能导致对长上下文依赖建模至关重要的关系细节丢失。本文提出一种扩展方法，重新整合被舍弃的虚部信息。该方法利用完整的复数表示构建双组分注意力分数，从理论和实验上证明该方案能通过保留更多位置信息来增强长上下文依赖建模能力。在一系列长上下文语言建模基准测试中的评估表明，相较于标准旋转位置编码，本方法能持续提升模型性能，且随着上下文长度增加，改进效果愈加显著。代码已开源：https://github.com/OpenMOSS/rope_pp。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.07525">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.07525">arXiv</a></p>
<hr />
<h3>3. 基于时序推理器的统一视频编辑方法</h3>
<p><strong>原文标题：</strong> Unified Video Editing with Temporal Reasoner</p>
<p><strong>摘要：</strong>
现有视频编辑方法面临关键权衡：专家模型虽能提供精确编辑效果，但依赖掩码等任务特定先验知识，阻碍了方法统一化；反之，基于时序上下文学习的统一模型虽无需掩码，但缺乏显式空间线索，导致指令-区域映射能力薄弱与定位精度不足。为解决这一矛盾，我们受思维链推理启发，提出一种新颖的帧链式方法VideoCoF。该方法通过强制视频扩散模型在生成目标视频标记前先预测推理标记（编辑区域潜在表示），构建“观察-推理-编辑”的流程。这种显式推理步骤在无需用户提供掩码的前提下，实现了精准的指令-区域对齐与细粒度视频编辑。此外，我们提出一种RoPE对齐策略，利用推理标记确保运动连贯性，并实现超越训练时长的序列长度外推能力。实验表明，仅使用5万对视频数据的极低训练成本，VideoCoF即在VideoCoF-Bench评测中达到最先进性能，验证了本方法的高效性与有效性。相关代码、权重及数据已开源：https://github.com/knightyxp/VideoCoF。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.07469">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.07469">arXiv</a></p>
<hr />
<h3>4. Voxify3D：像素艺术与体素渲染的交汇</h3>
<p><strong>原文标题：</strong> Voxify3D: Pixel Art Meets Volumetric Rendering</p>
<p><strong>摘要：</strong>
体素艺术是一种广泛应用于游戏和数字媒体的独特风格化形式，然而，由于几何抽象、语义保持和离散色彩一致性之间的相互冲突要求，从三维网格自动生成体素艺术仍具挑战性。现有方法要么过度简化几何结构，要么无法实现体素艺术所要求的像素级精确、调色板约束的美学效果。本文提出Voxify3D，一个可微分的两阶段框架，将三维网格优化与二维像素艺术监督相结合。我们的核心创新在于协同整合了三个组件：(1) 正交像素艺术监督，消除透视畸变以实现精确的体素-像素对齐；(2) 基于分块的CLIP对齐，在离散化过程中保持语义一致性；(3) 调色板约束的Gumbel-Softmax量化，支持在离散色彩空间上进行可微分优化，并提供可控的调色板策略。该整合解决了关键挑战：极端离散化下的语义保持、通过体素渲染实现像素艺术美学，以及端到端的离散优化。实验表明，该方法在多样化角色模型和可控抽象程度（2-8种颜色，20倍至50倍分辨率）上均表现出优越性能（CLIP-IQA得分37.12，用户偏好率77.90%）。项目页面：https://yichuanh.github.io/Voxify-3D/</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.07834">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.07834">arXiv</a></p>
<hr />
<h3>5. 规模化零样本参考视频生成</h3>
<p><strong>原文标题：</strong> Scaling Zero-Shot Reference-to-Video Generation</p>
<p><strong>摘要：</strong>
参考视频生成旨在合成符合文本提示的视频，同时保持参考图像中的主体身份一致性。然而，现有方法受限于对显式参考图像-视频-文本三元组的依赖，此类数据的构建成本高昂且难以规模化。本研究通过引入Saber框架突破这一瓶颈，该可扩展的零样本框架无需显式的参考视频生成数据。Saber仅使用视频-文本对进行训练，通过掩码训练策略与定制的基于注意力的模型设计，学习身份一致且具有参考感知能力的表征。为进一步缓解参考视频生成中常见的复制粘贴伪影，本研究整合了掩码增强技术。实验表明，Saber在不同数量参考图像条件下均展现出卓越的泛化能力，并在OpenS2V-Eval基准测试中超越了依赖参考视频生成数据训练的方法，实现了更优的性能。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.06905">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.06905">arXiv</a></p>
<hr />
<h3>6. DoVer：面向大语言模型多智能体系统的干预驱动自动调试方法</h3>
<p><strong>原文标题：</strong> DoVer: Intervention-Driven Auto Debugging for LLM Multi-Agent Systems</p>
<p><strong>摘要：</strong>
基于大语言模型的多智能体系统调试难度较高，其故障往往源于冗长且分支复杂的交互轨迹。当前主流方法依赖大语言模型进行基于日志的故障定位，将错误归因于特定智能体或执行步骤。然而，该范式存在两个关键局限：（1）纯日志调试缺乏验证环节，仅生成未经检验的假设；（2）单步骤或单智能体归因常与实际情况不符，本研究发现多种不同的干预措施均可独立修复任务故障。针对第一个局限，我们提出DoVer——一种干预驱动的调试框架，通过定向干预（如编辑消息、调整计划）进行主动验证，从而增强假设生成的可信度。对于第二个局限，我们不再以归因准确性为评估核心，转而关注系统能否解决故障或在任务成功方向上取得可量化的进展，这体现了更注重实际结果的调试视角。在Magnetic-One智能体框架中，基于GAIA与AssistantBench衍生的数据集进行实验，DoVer成功将18%-28%的失败案例转化为成功案例，实现最高16%的里程碑进度突破，并能验证或推翻30%-60%的故障假设。在另一数据集（GSMPlus）与智能体框架（AG2）的测试中，DoVer同样表现优异，成功修复了49%的失败案例。这些结果凸显了干预机制在提升智能体系统可靠性方面的实用价值，并为开发更鲁棒、可扩展的大语言模型多智能体系统调试方法开辟了新路径。项目网站与代码将在 https://aka.ms/DoVer 发布。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.06749">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.06749">arXiv</a></p>
<hr />
<h3>7. EgoEdit：面向第一人称视频编辑的数据集、实时流式处理模型与基准框架</h3>
<p><strong>原文标题：</strong> EgoEdit: Dataset, Real-Time Streaming Model, and Benchmark for Egocentric Video Editing</p>
<p><strong>摘要：</strong>
本研究针对交互式增强现实应用，探索基于指令引导的第一人称视频编辑技术。当前AI视频编辑工具虽在第三人称视角素材上表现良好，但第一人称视角存在独特挑战——包括快速的自我运动与频繁的手-物交互——这导致了显著的领域差异。此外，现有离线编辑流程存在高延迟问题，限制了实时交互体验。为解决这些难题，我们构建了一套完整的第一人称视频编辑生态系统。首先，我们创建了EgoEditData数据集，该数据集专为第一人称编辑场景设计，通过精细构建与人工标注，突出丰富的手-物交互特征并明确保持手部完整性。其次，我们开发了EgoEdit模型，这是一个遵循指令的第一人称视频编辑器，支持在单GPU上实现实时流式推理。最后，我们提出EgoEditBench评估体系，针对指令遵循度、手部与交互保持能力，以及自我运动下的时序稳定性进行系统评测。实验表明，在第一人称与通用编辑任务中，EgoEdit均能生成时序稳定、忠实于指令的编辑结果，并保持交互级延迟。在现有方法表现不佳的第一人称编辑基准测试中，本方法取得显著优势，同时在通用编辑任务上保持与最强基线模型相当的性能。EgoEditData数据集与EgoEditBench评估套件将向研究社区公开。详见项目网站：https://snap-research.github.io/EgoEdit</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.06065">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.06065">arXiv</a></p>
<hr />
<h3>8. 分布匹配变分自编码器</h3>
<p><strong>原文标题：</strong> Distribution Matching Variational AutoEncoder</p>
<p><strong>摘要：</strong>
大多数视觉生成模型在应用扩散或自回归建模之前，会将图像压缩至潜在空间。然而，现有方法（如变分自编码器及与基础模型对齐的编码器）仅隐式约束潜在空间，而未显式塑造其分布，导致何种分布最适合建模尚不明确。本文提出分布匹配变分自编码器，通过分布匹配约束显式地将编码器的潜在分布与任意参考分布对齐。该方法突破了传统变分自编码器高斯先验的局限，可实现与自监督特征、扩散噪声或其他先验分布所导出分布的对齐。借助分布匹配变分自编码器，我们能够系统探究何种潜在分布更有利于建模，并发现自监督学习导出的分布能在重建保真度与建模效率间取得优异平衡——仅经过64轮训练即在ImageNet数据集上达到gFID=3.2。实验结果表明：选择适宜的潜在分布结构（通过分布层级对齐实现），而非依赖固定先验，是弥合易建模潜在空间与高保真图像合成之间差距的关键。代码已发布于https://github.com/sen-ye/dmvae。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.07778">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.07778">arXiv</a></p>
<hr />
<h3>9. 关系视觉相似性</h3>
<p><strong>原文标题：</strong> Relational Visual Similarity</p>
<p><strong>摘要：</strong>
人类不仅能识别属性相似性——还能识别关系相似性。苹果与桃子相似是因为两者都是偏红色的水果，但地球也与桃子相似：其地壳、地幔和地核分别对应桃子的表皮、果肉和果核。认知科学家认为，这种感知和识别关系相似性的能力正是人类区别于其他物种的关键特征。然而，当前广泛使用的视觉相似性度量方法（如LPIPS、CLIP、DINO）仅关注感知属性相似性，未能捕捉人类所感知的丰富且常令人惊奇的关系相似性。我们该如何超越图像的可见内容以捕捉其关系特性？又该如何在表征空间中使具有相同关系逻辑的图像彼此靠近？为回答这些问题，我们首先将关系图像相似性形式化为可度量问题：当两幅图像的视觉元素之间的内在关系或功能相互对应时，即使其视觉属性不同，它们也具有关系相似性。随后，我们构建了一个包含11.4万条图像-文本对的数据集，其中文本描述经过匿名化处理——着重描述场景底层的关系逻辑而非表面内容。基于该数据集，我们对视觉-语言模型进行微调，以度量图像间的关系相似性。该模型标志着我们朝着依据底层关系结构（而非可见外观）连接图像迈出了第一步。研究表明，尽管关系相似性具有广泛的实际应用价值，现有图像相似性模型却未能有效捕捉这一特性——这揭示了视觉计算领域存在的重要空白。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.07833">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.07833">arXiv</a></p>
<hr />
<h3>10. 多视角金字塔变换器：观其大略以见全局</h3>
<p><strong>原文标题：</strong> Multi-view Pyramid Transformer: Look Coarser to See Broader</p>
<p><strong>摘要：</strong>
本文提出多视角金字塔变换器（MVP），一种可扩展的多视角变换器架构，能够在前向单次推理中直接根据数十至数百张图像重建大规模三维场景。受“观全局以见整体，察细微以知局部”思想启发，MVP建立在两个核心设计原则之上：1）局部到全局的视角间层次结构，使模型视角从局部视图逐步扩展至视图组，最终覆盖完整场景；2）精细到粗略的视角内层次结构，从详细的空间表征出发，逐步聚合为紧凑且信息密集的令牌。这种双重层次结构兼顾计算效率与表征丰富性，实现了对大规模复杂场景的快速重建。我们在多个数据集上验证了MVP的性能，结果表明：当以三维高斯溅射作为底层三维表征方法时，该架构在保持高效性与可扩展性的同时，能够在多种视角配置下达到当前最优的泛化重建质量。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.07806">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.07806">arXiv</a></p>
<hr />
<h3>11. 论预训练、中期训练与强化学习在推理语言模型中的相互作用</h3>
<p><strong>原文标题：</strong> On the Interplay of Pre-Training, Mid-Training, and RL on Reasoning Language Models</p>
<p><strong>摘要：</strong>
近期的强化学习技术显著提升了语言模型的推理能力，但后训练是否真正扩展了模型超越预训练阶段所获能力仍不明确。核心挑战在于现代训练流程缺乏可控性：大规模预训练语料不透明，中期训练常被忽视，且强化学习目标与未知的先验知识以复杂方式相互作用。为厘清这一问题，我们构建了一个完全可控的实验框架，以分离预训练、中期训练和基于强化学习的后训练的因果贡献。该方法采用具有明确原子操作、可解析逐步推理轨迹及训练分布系统性操控的合成推理任务。我们从两个维度评估模型：面向更复杂组合的外推泛化能力，以及跨表面语境的上下文泛化能力。通过该框架，我们调和了关于强化学习有效性的对立观点。研究表明：1）仅当预训练留有足够提升空间、且强化学习数据针对模型能力边界（即困难但尚未完全无法掌握的任务）时，强化学习才能带来真实能力提升（pass@128）；2）上下文泛化需要最低限度但充分的预训练基础，此后强化学习可稳定实现能力迁移；3）在固定计算量下，中期训练较单纯强化学习能显著提升性能，揭示了其在训练流程中关键却未被充分探索的作用；4）过程级奖励能减少奖励破解现象并提升推理保真度。这些结果共同阐明了预训练、中期训练与强化学习之间的相互作用机制，为理解和改进推理语言模型的训练策略提供了理论基础。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.07783">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.07783">arXiv</a></p>
<hr />
<h3>12. LongCat-Image 技术报告</h3>
<p><strong>原文标题：</strong> LongCat-Image Technical Report</p>
<p><strong>摘要：</strong>
我们推出 LongCat-Image，这是一个开创性的开源双语（中英）图像生成基础模型，旨在解决当前主流模型在多语言文本渲染、照片真实感、部署效率和开发者可访问性方面的核心挑战。1）我们通过在预训练、中期训练和 SFT（监督微调）阶段实施严格的数据策展策略，并在强化学习阶段协调使用精选的奖励模型来实现这一目标。这一策略使模型达到了新的最优性能（SOTA），提供了卓越的文本渲染能力和出色的照片真实感，并显著提升了美学质量。2）值得注意的是，该模型为汉字渲染树立了新的行业标准。通过支持复杂和罕见字符，其在覆盖范围上超越了主流开源和商业解决方案，同时实现了更高的准确性。3）该模型凭借其紧凑设计实现了卓越的效率。其核心扩散模型仅包含 60 亿参数，远小于该领域常见的近 200 亿或更大规模的专家混合模型架构。这确保了最小的显存占用和快速的推理速度，显著降低了部署成本。除了生成能力，LongCat-Image 在图像编辑方面也表现出色，在标准基准测试中取得了 SOTA 结果，与其他开源作品相比具有更优的编辑一致性。4）为了全面赋能社区，我们建立了迄今为止最全面的开源生态系统。我们不仅发布了用于文生图和图像编辑的多个模型版本（包括中期训练和后训练阶段的检查点），还开放了整个训练流程的工具链。我们相信，LongCat-Image 的开放性将为开发者和研究人员提供强有力的支持，推动视觉内容创作的前沿发展。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.07584">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.07584">arXiv</a></p>
<hr />
<h3>13. UnityVideo：面向增强世界感知视频生成的统一多模态多任务学习框架</h3>
<p><strong>原文标题：</strong> UnityVideo: Unified Multi-Modal Multi-Task Learning for Enhancing World-Aware Video Generation</p>
<p><strong>摘要：</strong>
当前视频生成模型虽展现出卓越的合成能力，但仍受限于单模态条件约束，制约了其对世界的整体理解能力。这一局限源于跨模态交互不足以及模态多样性有限，难以支撑全面的世界知识表征。为突破这些限制，我们提出UnityVideo——一个面向世界感知视频生成的统一框架，能够跨多模态（分割掩码、人体骨架、DensePose、光流与深度图）及多训练范式进行联合学习。本框架包含两大核心组件：（1）动态噪声注入机制，用于统一异构训练范式；（2）搭载上下文学习器的模态切换器，通过模块化参数与上下文学习实现统一处理。我们构建了包含130万样本的大规模统一数据集。经联合优化，UnityVideo显著加速模型收敛，并大幅提升对未见数据的零样本泛化能力。实验表明，UnityVideo在视频质量、时序一致性及物理世界约束对齐方面均达到更优性能。代码与数据详见：https://github.com/dvlab-research/UnityVideo</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.07831">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.07831">arXiv</a></p>
<hr />
<h3>14. 视觉生成调优</h3>
<p><strong>原文标题：</strong> Visual Generation Tuning</p>
<p><strong>摘要：</strong>
大规模视觉语言模型通过广泛的预训练有效弥合了模态鸿沟，获得了与语言对齐的复杂视觉表征。然而，这些针对多模态理解任务优化的表征是否蕴含视觉生成的内在潜力，目前仍未得到充分探索。本文提出视觉生成调优这一新范式，旨在激发任意视觉语言模型中潜在的视觉生成能力。通过对预训练良好的视觉语言模型进行高效的视觉生成调优，我们显著降低了对齐成本，并加速了连续空间中自回归建模的收敛速度（提速20倍）。具体而言，我们摒弃了为扩散变换器设计的纠缠像素级变分自编码器，通过将预训练视觉语言模型的语义编码器与像素解码器的潜在表征对齐，构建了视觉生成调优自编码器。在图像重建任务中，我们在28倍压缩比下实现了26.67的峰值信噪比和0.50的相对弗雷歇距离，性能优于专用变分自编码器；在视觉生成任务中，我们在自回归模型中取得了最先进的成果——GenEval评测达到0.77分，DPG-Bench评测达到78.73分。此外，所提出的视觉生成调优展现出显著的扩展潜力，能够灵活赋能任何为多模态理解训练的视觉语言模型，使其具备视觉生成能力，这为探索下一代统一多模态基础模型开辟了新路径。模型与代码已发布于https://github.com/hustvl/VGT。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.23469">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.23469">arXiv</a></p>
<hr />
<h3>15. SPARK：面向无参考强化学习的渐进式过程感知奖励机制</h3>
<p><strong>原文标题：</strong> SPARK: Stepwise Process-Aware Rewards for Reference-Free Reinforcement Learning</p>
<p><strong>摘要：</strong>
提供密集步骤级反馈的过程奖励模型在强化学习中展现出潜力，但其应用仍受限于昂贵的步骤级标注或真实参考答案的需求。本文提出SPARK三阶段框架：第一阶段通过生成模型产生多样化解决方案，并利用并行扩展（自洽性验证）与序列扩展（元批判）机制驱动验证模型进行评估；第二阶段将验证输出作为合成训练数据，对生成式过程奖励模型进行微调，使其在训练过程中提供奖励信号。研究表明，在步骤层面聚合多个独立验证结果所生成的过程奖励模型训练数据，其效果优于真实结果监督方法——在数学推理错误步骤识别基准ProcessBench上达到67.5的F1分数，显著超越参考指导训练的66.4分与GPT-4o的61.9分。第三阶段将融合思维链验证的生成式过程奖励模型应用于数学推理强化学习实验，并通过格式约束机制防范奖励攻击。基于Qwen2.5-Math-7B模型，我们在六项数学推理基准测试中取得47.4%的平均准确率，优于基于真实结果的RLVR方法（43.9%）。本工作实现了超越真实参考方法的无参考强化学习训练，为缺乏可验证答案或难以获取真实参考的领域开辟了新路径。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.03244">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.03244">arXiv</a></p>
<hr />
<h3>16. VG-Refiner：基于智能体强化学习的工具精炼指称接地推理</h3>
<p><strong>原文标题：</strong> VG-Refiner: Towards Tool-Refined Referring Grounded Reasoning via Agentic Reinforcement Learning</p>
<p><strong>摘要：</strong>
工具集成视觉推理（TiVR）在增强多模态问题解决能力方面展现出巨大潜力。然而，现有TiVR范式主要关注通过强化学习整合各类视觉工具，却未能设计有效的响应机制来处理不可靠或错误的工具输出。这一局限在指称与接地任务中尤为突出，其中不准确的检测工具预测常误导TiVR模型产生幻觉推理。为解决此问题，我们提出VG-Refiner——首个面向工具精炼指称接地推理的框架。技术上，我们引入两阶段“思考-再思考”机制，使模型能够显式分析并响应工具反馈，同时设计精炼奖励机制以激励模型针对低质量工具结果进行有效修正。此外，我们提出两项新评估指标并建立公平评测协议，以系统衡量现有模型的精炼能力。通过采用少量任务特定数据增强VG-Refiner的精炼能力，该框架在指称与推理接地基准测试中实现了准确率与修正能力的显著提升，同时保持了预训练模型的通用能力。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.06373">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.06373">arXiv</a></p>
<hr />
<h3>17. ReCamDriving：一种无激光雷达的相机控制新轨迹视频生成方法</h3>
<p><strong>原文标题：</strong> ReCamDriving: LiDAR-Free Camera-Controlled Novel Trajectory Video Generation</p>
<p><strong>摘要：</strong>
本文提出ReCamDriving，一种纯视觉、相机控制的新轨迹视频生成框架。基于修复的方法难以恢复复杂的伪影，而基于激光雷达的方法依赖于稀疏且不完整的线索，与此不同，ReCamDriving利用密集且场景完整的3D高斯溅射渲染提供显式几何引导，实现了精确的相机可控生成。为缓解在3DGS渲染条件下对修复行为的过拟合问题，ReCamDriving采用两阶段训练范式：第一阶段使用相机位姿进行粗略控制，第二阶段则引入3DGS渲染以实现细粒度的视点与几何引导。此外，我们提出一种基于3DGS的跨轨迹数据构建策略，以消除相机变换模式在训练与测试间的差异，从而能够从单目视频中实现可扩展的多轨迹监督。基于此策略，我们构建了ParaDrive数据集，包含超过11万组平行轨迹视频对。大量实验表明，ReCamDriving在相机可控性与结构一致性方面达到了最先进的性能水平。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.03621">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.03621">arXiv</a></p>
<hr />
<h3>18. OmniSafeBench-MM：多模态越狱攻击-防御评估的统一基准与工具箱</h3>
<p><strong>原文标题：</strong> OmniSafeBench-MM: A Unified Benchmark and Toolbox for Multimodal Jailbreak Attack-Defense Evaluation</p>
<p><strong>摘要：</strong>
多模态大语言模型（MLLMs）的最新进展实现了统一的感知-推理能力，但这些系统仍极易受到越狱攻击，从而绕过安全对齐机制并诱发有害行为。现有基准如JailBreakV-28K、MM-SafetyBench和HADES为多模态漏洞提供了有价值的洞见，但它们通常局限于有限的攻击场景，缺乏标准化的防御评估，且未提供统一、可复现的工具箱。为弥补这些不足，我们提出了OmniSafeBench-MM，这是一个用于多模态越狱攻击-防御评估的综合工具箱。OmniSafeBench-MM整合了13种代表性攻击方法、15种防御策略，以及一个涵盖9个主要风险领域和50个细粒度类别的多样化数据集，并按照咨询式、命令式和陈述式查询类型进行结构化设计，以反映真实的用户意图。除了数据覆盖范围，该基准还建立了一个三维评估协议，用于衡量：（1）危害性，采用从低影响个体伤害到灾难性社会威胁的细粒度多级尺度进行区分；（2）响应与查询之间的意图对齐度；（3）响应详细程度，从而支持细致的安全-效用分析。我们对10个开源和8个闭源MLLMs进行了广泛实验，揭示了它们对多模态越狱攻击的脆弱性。通过将数据、方法与评估统一到一个开源、可复现的平台中，OmniSafeBench-MM为未来研究提供了标准化基础。代码已发布于https://github.com/jiaxiaojunQAQ/OmniSafeBench-MM。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.06589">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.06589">arXiv</a></p>
<hr />
<h3>19. 超越词元级监督：通过强化学习释放基于解码的回归潜力</h3>
<p><strong>原文标题：</strong> Beyond Token-level Supervision: Unlocking the Potential of Decoding-based Regression via Reinforcement Learning</p>
<p><strong>摘要：</strong>
基于解码的回归将回归任务重构为序列生成问题，已成为应用大语言模型进行数值预测的一种前景广阔的范式。然而，其发展受到离散词元级目标（如交叉熵）与连续数值之间错位的制约。现有依赖词元级约束的方法往往难以捕捉目标值的整体量级，限制了预测精度与泛化能力。本文提出通过强化学习释放基于解码的回归潜力。我们将生成过程建模为马尔可夫决策过程，利用序列级奖励来保证全局数值一致性。在表格回归与代码度量回归任务上的大量实验表明，本方法（特别是采用ReMax与GRPO时）在性能上持续优于当前最先进的词元级基线方法与传统回归头，证明了引入序列级信号的优势。进一步分析揭示，强化学习显著提升了采样效率与预测精度，从而确立了基于解码的回归作为一种鲁棒且精确的通用数值预测范式。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.06533">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.06533">arXiv</a></p>
<hr />
<h3>20. OpenSubject：利用视频衍生的身份与多样性先验实现主体驱动的图像生成与编辑</h3>
<p><strong>原文标题：</strong> OpenSubject: Leveraging Video-Derived Identity and Diversity Priors for Subject-driven Image Generation and Manipulation</p>
<p><strong>摘要：</strong>
尽管主体驱动的图像生成已取得显著进展，但现有模型常偏离参考身份特征，且在包含多主体的复杂场景中表现欠佳。为解决这一挑战，我们提出了OpenSubject——一个基于视频构建的大规模数据集，包含250万个样本和435万张图像，专用于主体驱动的生成与编辑任务。该数据集通过利用跨帧身份先验的四阶段流程构建：（一）视频筛选。通过分辨率与美学过滤获取高质量视频片段。（二）跨帧主体挖掘与配对。采用基于视觉语言模型的类别共识、局部定位及多样性感知配对策略筛选图像对。（三）身份保持的参考图像合成。通过分割图引导的外延绘制合成主体驱动生成的输入图像，结合边界框引导的内绘修复生成主体驱动编辑的输入图像，并辅以几何感知增强与不规则边界侵蚀技术。（四）验证与标注。使用视觉语言模型验证合成样本，对失败样本基于第三阶段流程重新合成，最终构建简短与详细描述文本。此外，我们建立了涵盖主体驱动生成与编辑任务的基准测试体系，通过视觉语言模型评估身份保真度、提示符遵从性、编辑一致性与背景一致性。大量实验表明，使用OpenSubject进行训练能显著提升生成与编辑性能，尤其在复杂场景中效果突出。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.08294">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.08294">arXiv</a></p>
<hr />
<h3>21. 一层足矣：面向图像生成任务的自适应预训练视觉编码器</h3>
<p><strong>原文标题：</strong> One Layer Is Enough: Adapting Pretrained Visual Encoders for Image Generation</p>
<p><strong>摘要：</strong>
视觉生成模型（如扩散模型）通常在压缩的潜在空间中运行，以平衡训练效率与生成样本质量。与此同时，利用高质量预训练视觉表征的研究日益受到关注，常见方法包括将其与变分自编码器（VAEs）对齐或直接整合至生成模型中。然而，由于理解导向的特征与生成友好的潜在空间之间存在本质性不匹配，此类表征的适配仍具挑战性。表征编码器受益于高维潜在空间以捕捉掩码区域的多样化假设，而生成模型则倾向于低维潜在空间以忠实保留注入的噪声。这种差异导致先前研究依赖复杂的优化目标与架构设计。本文提出特征自编码器（FAE），这是一个简洁而高效的框架，能够将预训练的视觉表征适配至适用于生成任务的低维潜在空间，仅需使用单个注意力层即可实现，同时保留足够的重建与理解信息。其关键在于耦合两个独立的深度解码器：一个训练用于重建原始特征空间，另一个则以重建特征作为输入进行图像生成。FAE具有通用性：可与多种自监督编码器（如DINO、SigLIP）结合实例化，并嵌入到扩散模型与标准化流这两类不同的生成框架中。在类别条件生成与文生图基准测试中，FAE均表现出优异性能。例如，在ImageNet 256×256数据集上，结合分类器无引导（CFG）的扩散模型实现了接近最优的FID分数1.29（800训练周期）与1.70（80训练周期）；未使用CFG时，FAE达到当前最优的FID分数1.48（800训练周期）与2.08（80训练周期），展现出高质量生成与快速学习的双重优势。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.07829">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.07829">arXiv</a></p>
<hr />
<h3>22. 群表示位置编码</h3>
<p><strong>原文标题：</strong> Group Representational Position Encoding</p>
<p><strong>摘要：</strong>
本文提出GRAPE（群表示位置编码），这是一个基于群作用的统一位置编码框架。GRAPE整合了两类机制：（1）SO(d)群中的乘法旋转（乘法GRAPE），以及（2）源于一般线性群GL中单能作用的加法对数偏置（加法GRAPE）。在乘法GRAPE中，Z中的位置n（或R中的t）通过G(n)=exp(n,ω,L)作用，其中L为R^{d×d}中的二阶斜生成元，产生具有闭式矩阵指数的相对性、复合性、保范映射。当d/2平面为具有对数均匀谱的标准坐标对时，可精确恢复RoPE。通过学习可交换子空间和紧致非交换混合，该几何结构被严格扩展至分别以每头O(d)和O(r d)代价捕获跨子空间特征耦合。在加法GRAPE中，加法对数以秩1（或低秩）单能作用形式出现，将ALiBi与遗忘变换器（FoX）作为精确特例恢复，同时保持精确的相对律与流式缓存能力。总体而言，GRAPE为长上下文模型中的位置几何提供了原则性设计空间，将RoPE与ALiBi纳入为特例。项目页面：https://github.com/model-architectures/GRAPE。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.07805">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.07805">arXiv</a></p>
<hr />
<h3>23. 解耦以泛化：面向数据稀缺视觉语言推理的上下文优先自演化学习</h3>
<p><strong>原文标题：</strong> Decouple to Generalize: Context-First Self-Evolving Learning for Data-Scarce Vision-Language Reasoning</p>
<p><strong>摘要：</strong>
当前视觉语言模型通过强化学习实现了卓越的推理能力，为在经验时代实现持续自我演化的大型视觉语言模型提供了可行路径。然而，视觉语言模型的强化学习需要大量高质量多模态数据，在化学、地球科学及多模态数学等专业领域尤为困难。现有策略如合成数据与自奖励机制存在分布局限和对齐困难，最终导致奖励破解现象：模型利用高奖励模式，致使策略熵崩溃并破坏训练稳定性。我们提出DoGe（解耦以泛化）——一种双重解耦框架，通过引导模型首先从上下文而非问题求解中学习，重新聚焦于合成数据方法所忽视的问题情境场景。通过将学习过程解耦为双组件（思考器与求解器），我们合理量化该过程的奖励信号，并提出从自由探索上下文到实际解决问题的两阶段强化学习后训练方法。其次，为提升训练数据多样性，DoGe构建了动态演化的课程学习流程：扩展的本体领域知识库与迭代演化的种子问题池。实验表明，我们的方法在多种基准测试中持续超越基线模型，为实现自演化大型视觉语言模型提供了可扩展路径。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.06835">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.06835">arXiv</a></p>
<hr />
<h3>24. VideoVLA：视频生成器可作为通用机器人操作器</h3>
<p><strong>原文标题：</strong> VideoVLA: Video Generators Can Be Generalizable Robot Manipulators</p>
<p><strong>摘要：</strong>
机器人操作的泛化能力对于在开放世界环境中部署机器人以及迈向通用人工智能至关重要。尽管近期的视觉-语言-动作模型利用大规模预训练理解模型进行感知和指令跟随，但其在新任务、新物体和新环境中的泛化能力仍然有限。本研究提出VideoVLA，这是一种探索将大型视频生成模型转化为机器人视觉-语言-动作操作器的简单方法。给定语言指令和图像，VideoVLA可预测动作序列及未来视觉结果。基于多模态扩散变换器架构，VideoVLA通过预训练视频生成模型实现视觉与动作的联合建模，同步处理视频、语言和动作模态。实验表明，高质量的视觉想象与可靠的动作预测及任务成功率呈正相关，凸显了视觉想象力在操作任务中的重要性。VideoVLA展现出强大的泛化能力，包括模仿其他实体技能和处理新异物体。这种同时预测动作及其视觉结果的双重预测策略，探索了机器人学习范式的转变，并释放了操作系统的泛化潜能。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.06963">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.06963">arXiv</a></p>
<hr />
<h3>25. 尺度自回归生成中的训练动态机制再思考</h3>
<p><strong>原文标题：</strong> Rethinking Training Dynamics in Scale-wise Autoregressive Generation</p>
<p><strong>摘要：</strong>
自回归生成模型的最新进展催生了日益强大的媒体合成系统。其中，尺度递进预测已成为主流范式，模型通过从粗到细的方式生成图像。然而，尺度自回归模型普遍存在曝光偏差问题，严重影响生成质量。我们识别出该问题的两个核心成因：（1）训练-测试失配——推理阶段模型必须依赖自身不完美的预测结果；（2）尺度学习难度失衡——特定尺度表现出不成比例的高优化复杂度。通过对训练动态的全面分析，我们提出自回归精炼框架以解决这些局限。该框架包含交错尺度展开机制——通过轻量级自回归展开使模型接触其中间预测结果，从而实现训练-测试模式对齐；以及互补的对比性强制学习损失函数——为自生成语境提供充分监督以确保训练稳定性。实验结果表明，将自回归精炼框架应用于预训练的自回归模型后，能以极小计算开销持续提升生成质量。例如在ImageNet 256数据集上训练的FlexVAR-d16模型中，自回归精炼框架仅需10个训练周期（32xA100 GPU运行5小时）即可实现FID指标5.2%的优化。鉴于其高效性、可扩展性与有效性，我们预期自回归精炼框架将成为视觉自回归生成领域可靠的训练后优化方法。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.06421">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.06421">arXiv</a></p>
<hr />
<h3>26. 小增益纳什：可微博弈中经认证的纳什均衡收缩方法</h3>
<p><strong>原文标题：</strong> Small-Gain Nash: Certified Contraction to Nash Equilibria in Differentiable Games</p>
<p><strong>摘要：</strong>
经典博弈梯度学习收敛性证明要求伪梯度在欧几里得几何中满足（强）单调性条件（如Rosen于1965年所示），但该条件即使在具有强跨参与者耦合的简单博弈中也常不成立。本文提出小增益纳什方法，这是一种在定制块加权几何中的块小增益条件。该方法将局部曲率与跨参与者Lipschitz耦合边界转化为可处理的收缩认证条件，通过构建加权块度量，使得伪梯度在满足这些边界的任意区域呈现强单调性——即使其在欧几里得意义下非单调。连续流在此设计几何中呈指数收缩，且投影欧拉法与RK4离散化在基于SGN裕度与局部Lipschitz常数导出的显式步长范围内收敛。分析揭示了一种经认证的“时间尺度带”，这是一种非渐近的、基于度量的认证机制，其作用类似于TTUR方法：不同于通过趋近零的不等步长强制渐近时间尺度分离，SGN可识别相对度量权重的有限带域，使得单一步长动力学可证收缩。我们在二次博弈中验证了该框架的有效性——传统欧几里得单调性分析无法预测收敛的情形，SGN却能成功认证收敛性，并将该构造扩展至马尔可夫博弈中熵正则化策略梯度的镜像/费希尔几何。最终形成离线认证流程：在紧致区域估计曲率、耦合与Lipschitz参数，优化块权重以扩大SGN裕度，并为非单调博弈返回包含度量、收缩率与安全步长的结构化可计算收敛认证。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.06791">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.06791">arXiv</a></p>
<hr />
<h3>27. 基于高斯变分自编码器的向量量化方法</h3>
<p><strong>原文标题：</strong> Vector Quantization using Gaussian Variational Autoencoder</p>
<p><strong>摘要：</strong>
向量量化变分自编码器（VQ-VAE）是一种将图像压缩为离散标记的离散自编码器，其离散化特性导致模型训练困难。本文提出一种简单而有效的技术——高斯量化（GQ），该方法通过特定约束将高斯变分自编码器直接转换为VQ-VAE而无需额外训练。GQ通过生成随机高斯噪声构建码本，并寻找与后验均值最接近的噪声向量。理论上，我们证明当码本对数值超过高斯变分自编码器的比特回传编码率时，该方法可保证较小的量化误差。实践中，我们提出一种启发式训练策略——目标散度约束（TDC），用于优化高斯变分自编码器以提升GQ效果。实验表明，在UNet和ViT架构上，GQ的性能优于VQGAN、FSQ、LFQ和BSQ等现有VQ-VAE方法。此外，TDC策略也改进了TokenBridge等传统高斯变分自编码器离散化方法。源代码已发布于https://github.com/tongdaxu/VQ-VAE-from-Gaussian-VAE。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.06609">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.06609">arXiv</a></p>
<hr />
<h3>28. 人机交互中的具身指代表达理解</h3>
<p><strong>原文标题：</strong> Embodied Referring Expression Comprehension in Human-Robot Interaction</p>
<p><strong>摘要：</strong>
随着机器人进入人类工作空间，其理解具身化人类指令以实现直观流畅的人机交互（HRI）的需求日益迫切。然而，由于缺乏能够捕捉多样化HRI场景中自然具身交互的大规模数据集，准确理解面临挑战。现有数据集普遍存在视角偏差、单视角采集、非语言手势覆盖不足以及过度聚焦室内环境等问题。为应对这些挑战，我们提出了Refer360数据集——一个在室内外多视角环境下采集的大规模具身化语言与非语言交互数据集。同时，我们设计了多模态引导残差模块MuRes，以提升具身指代表达理解能力。该模块通过构建信息瓶颈，提取显著模态特异性信号并将其强化至预训练表征中，从而为下游任务构建互补特征。我们在四个HRI数据集（包括Refer360）上进行了广泛实验，结果表明当前多模态模型未能全面捕捉具身交互特征，而引入MuRes模块能持续提升模型性能。这些发现确立了Refer360作为重要基准数据集的价值，并展现了引导残差学习在推动人类环境中机器人具身指代表达理解方面的潜力。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.06558">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.06558">arXiv</a></p>
<hr />
<h3>29. 基于格式强化学习的结构化文档翻译</h3>
<p><strong>原文标题：</strong> Structured Document Translation via Format Reinforcement Learning</p>
<p><strong>摘要：</strong>
当前结构化文本翻译研究仍局限于句子层面，难以有效处理复杂的文档级XML或HTML结构。为此，我们提出格式强化学习方法，该方法在监督微调模型基础上采用组相对策略优化，直接优化两种新型结构感知奖励函数：1）TreeSim——衡量预测XML树与参考XML树之间的结构相似性；2）Node-chrF——在XML节点层面评估翻译质量。此外，我们引入StrucAUC细粒度评估指标，以区分细微错误与重大结构缺陷。在SAP软件文档基准测试上的实验表明，该方法在六项指标上均取得提升；进一步分析揭示了不同奖励函数如何共同促进结构完整性与翻译质量的改进。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.05100">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.05100">arXiv</a></p>
<hr />
<h3>30. DZ-TDPO：面向长对话中可变状态追踪的非破坏性时序对齐方法</h3>
<p><strong>原文标题：</strong> DZ-TDPO: Non-Destructive Temporal Alignment for Mutable State Tracking in Long-Context Dialogue</p>
<p><strong>摘要：</strong>
长上下文对话系统普遍存在状态惯性问题，即静态约束阻碍模型在动态演变的用户意图与既定历史语境间有效化解冲突。为解决此问题，我们提出DZ-TDPO——一种融合冲突感知动态KL约束与校准时序注意力偏置的非破坏性对齐框架。在Multi-Session Chat（MSC）数据集上的实验表明，DZ-TDPO实现了最先进的胜率（Phi-3.5模型达55.4%），同时保持稳健的零样本泛化能力。我们的扩展分析揭示了“容量-稳定性权衡”现象：较小模型需付出“对齐代价”（困惑度激增）以克服历史惯性，而更大的Qwen2.5-7B模型以可忽略的困惑度开销实现了50.8%的胜率。这证实了时序注意力惯性可通过精确的注意力调控（而非破坏性权重更新）来缓解，从而在不同规模模型上保持通用能力（MMLU指标）。代码与数据已开源：https://github.com/lyj20071013/DZ-TDPO</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.03704">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.03704">arXiv</a></p>
<hr />
<h3>31. JEPA作为神经分词器：基于密度自适应注意力机制的鲁棒语音表征学习</h3>
<p><strong>原文标题：</strong> JEPA as a Neural Tokenizer: Learning Robust Speech Representations with Density Adaptive Attention</p>
<p><strong>摘要：</strong>
本文提出一种两阶段自监督学习框架，将联合嵌入预测架构（JEPA）与密度自适应注意力机制（DAAM）相结合，用于学习鲁棒的语音表征。第一阶段采用集成DAAM的JEPA架构，通过在隐空间进行掩码预测来学习语义音频特征，该过程完全与波形重构解耦。第二阶段利用有限标量化（FSQ）和混合基数打包方案，将学到的表征高效转换为分词表示，再通过HiFi-GAN解码器实现高保真波形重建。通过将基于高斯混合模型的密度自适应门控机制集成到JEPA编码器中，该模型能以2.5Hz的低帧率实现自适应时序特征选择，并发现语音的层次化结构。最终生成的分词（47.5词元/秒）具有可逆性、高压缩性和语言模型友好性，其性能与现有神经音频编解码器相当，且通常具有更高的效率。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.07168">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.07168">arXiv</a></p>
<hr />
<h3>32. SAM系列模型中的SAM2至SAM3断层：基于提示的专长为何在概念驱动的图像分割中失效</h3>
<p><strong>原文标题：</strong> The SAM2-to-SAM3 Gap in the Segment Anything Model Family: Why Prompt-Based Expertise Fails in Concept-Driven Image Segmentation</p>
<p><strong>摘要：</strong>
本文探究了最新两代Segment Anything模型（SAM2与SAM3）之间的根本性断层。我们阐释了为何SAM2在基于提示的分割任务中的专业能力无法迁移至SAM3的多模态概念驱动范式。SAM2通过空间提示（点、框、掩码）进行操作，产生纯几何与时间维度的分割结果；而SAM3引入了统一的视觉-语言架构，具备开放词汇推理、语义 grounding、对比对齐以及基于范例的概念理解能力。本研究通过五个核心组成部分展开分析：（1）基于提示与基于概念的分割模式间的理念断裂，对比SAM2的空间提示语义与SAM3的多模态融合及文本条件掩码生成机制；（2）架构差异，详述SAM2的纯视觉-时序设计与SAM3中视觉-语言编码器、几何与范例编码器、融合模块、DETR风格解码器、对象查询机制以及通过专家混合模型处理模糊性的整合架构；（3）数据集与标注差异，对比SAM2的SA-V视频掩码数据与SAM3的多模态概念标注语料库；（4）训练与超参数区别，说明SAM2的优化经验为何不适用于SAM3；（5）评估体系、指标与失效模式，梳理从几何IoU指标向语义化、开放词汇评估范式的转变。这些分析共同确立了SAM3作为新一代分割基础模型的地位，并为新兴的概念驱动分割时代指明了未来发展方向。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.06032">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.06032">arXiv</a></p>
<hr />
<h2>🔍 关键词云图</h2>
<p><img alt="关键词云图" src="../images/keywords_wordcloud.png" /></p>
<h2>📈 近期论文趋势</h2>
<p><img alt="论文趋势" src="../images/daily_papers.png" /></p>
<h2>🎙️ 语音播报</h2>
<ul>
<li><a href="../audio/2025-12-09_daily_papers.mp3">收听今日论文解读</a></li>
</ul>
<h2>📱 订阅渠道</h2>
<ul>
<li>GitHub: <a href="https://github.com/2404589803/hf-daily-paper-newsletter-chinese">hf-daily-paper-newsletter-chinese</a></li>
</ul>
    </div>
</body>
</html>