
# <img src="https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo.png" width="30"/> Hugging Face 2025-12-19 论文日报

## 📊 今日论文统计
- 总论文数：38
- 热门领域：Transformer, Vision, Diffusion, RL, LLM, GPT

## 📝 论文详情


### 1. Kling-Omni 技术报告

**原文标题：** Kling-Omni Technical Report

**摘要：**
本文介绍 Kling-Omni，一种通用的生成式框架，旨在直接从多模态视觉语言输入合成高保真度视频。该框架采用端到端视角，弥合了多样化视频生成、编辑与智能推理任务之间的功能分离，将其整合为一个整体系统。与割裂的流水线方法不同，Kling-Omni 支持包括文本指令、参考图像和视频上下文在内的多种用户输入，并将其处理为统一的多模态表示，以提供电影级质量且高度智能的视频内容创作。为支撑这些能力，我们构建了一个全面的数据系统，作为多模态视频创作的基础。该框架还通过高效的大规模预训练策略和推理基础设施优化得到进一步增强。综合评估表明，Kling-Omni 在上下文生成、基于推理的编辑以及多模态指令跟随方面展现出卓越能力。超越单纯的内容创作工具，我们认为 Kling-Omni 是实现多模态世界模拟器的关键进展，该模拟器能够感知、推理、生成并与动态复杂世界进行交互。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.16776) | [arXiv](https://arxiv.org/abs/2512.16776)



---

### 2. 智能体人工智能的适应性研究

**原文标题：** Adaptation of Agentic AI

**摘要：**
前沿的智能体人工智能系统建立在基础模型之上，这些模型可通过调整以实现规划、推理及与外部工具交互，从而执行日益复杂和专门化的任务。随着此类系统在能力与适用范围上的扩展，适应性已成为提升性能、可靠性和泛化能力的核心机制。本文通过构建一个系统化框架，将快速扩展的研究领域统一划分为智能体适应与工具适应两大维度，并进一步将其解构为工具执行信号驱动型和智能体输出信号驱动型的智能体适应方式，以及智能体无关型和智能体监督型的工具适应方式。研究表明，该框架有助于厘清智能体人工智能适应性策略的设计空间，明确其内在权衡关系，并为系统设计过程中策略的选择与切换提供实践指导。在此基础上，本文综述了各类别的代表性方法，剖析其优势与局限，并指出当前面临的关键开放挑战与未来研究方向。总体而言，本文旨在为构建更强大、高效、可靠的智能体人工智能系统的研究者和实践者提供理论基础与实践路径。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.16301) | [arXiv](https://arxiv.org/abs/2512.16301)



---

### 3. LLaDA2.0：将扩散语言模型扩展至千亿参数规模

**原文标题：** LLaDA2.0: Scaling Up Diffusion Language Models to 100B

**摘要：**
本文提出LLaDA2.0——一组通过自回归模型系统化转换构建、总参数量达千亿规模的离散扩散大语言模型，为前沿规模部署建立了新范式。该方法摒弃了成本高昂的从头训练，秉持知识继承、渐进适应与效率优先的设计原则，通过创新的三阶段块级加权序列扩散训练方案，将预训练的自回归模型无缝转换为扩散语言模型：该方案包含块扩散中逐步增大块尺寸的预热阶段、大规模全序列扩散的稳定阶段，以及回归紧凑块尺寸扩散的衰减阶段。结合监督微调与直接偏好优化的训练后对齐，我们获得了LLaDA2.0-mini（160亿参数）和LLaDA2.0-flash（千亿参数）两个经过指令优化的混合专家模型变体，专为实际部署场景优化。通过保留并行解码的优势，这些模型在前沿规模上实现了卓越的性能与效率。两款模型均已开源发布。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.15745) | [arXiv](https://arxiv.org/abs/2512.15745)



---

### 4. 下一嵌入预测构建强视觉学习器

**原文标题：** Next-Embedding Prediction Makes Strong Vision Learners

**摘要：**
受生成式预训练在自然语言领域成功的启发，我们探究相同原理能否构建强大的自监督视觉学习器。不同于训练模型输出特征供下游任务使用，我们训练模型直接生成嵌入以执行预测任务。本研究探索了这种从学习表征到学习模型的范式转变。具体而言，模型通过因果掩码和梯度截断技术，学习基于历史图像块嵌入预测未来嵌入，我们将此方法称为下一嵌入预测自回归模型。实验证明，仅以下一嵌入预测为学习目标、在ImageNet-1k上预训练的简单Transformer模型即具备显著效果——无需像素重建、离散标记、对比损失或任务特定头设计。该方案保持了架构简洁性与可扩展性，无需引入额外设计复杂度。NEPA在多项任务中表现优异：基于ViT-B和ViT-L骨干网络微调后，在ImageNet-1K上分别达到83.8%和85.3%的top-1准确率，并能有效迁移至ADE20K语义分割任务。我们相信基于嵌入的生成式预训练为视觉自监督学习提供了一种简洁、可扩展且可能模态无关的替代方案。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.16922) | [arXiv](https://arxiv.org/abs/2512.16922)



---

### 5. StereoPilot：基于生成先验学习统一高效立体转换方法

**原文标题：** StereoPilot: Learning Unified and Efficient Stereo Conversion via Generative Priors

**摘要：**
随着立体显示设备（包括VR头显与3D影院）的快速发展，市场对高质量立体视频内容的需求日益增长。然而，三维视频的制作仍面临成本高昂、流程复杂的问题，而自动单目到立体的转换技术受限于传统多阶段“深度-形变-修复”（DWI）流程的固有缺陷。该范式存在误差传播、深度歧义以及平行与汇聚立体格式间不一致等问题。为应对这些挑战，我们首次构建了UniStereo——一个覆盖两种立体格式的大规模统一立体视频转换数据集，以支持公平的性能评估与鲁棒的模型训练。基于此数据集，我们提出StereoPilot模型，该高效前馈模型无需依赖显式深度图或迭代扩散采样，可直接合成目标视角视图。通过可学习的域切换器与循环一致性损失函数，StereoPilot能够自适应不同立体格式并提升视觉一致性。大量实验表明，StereoPilot在视觉保真度与计算效率方面均显著优于当前最先进方法。项目页面：https://hit-perfect.github.io/StereoPilot/。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.16915) | [arXiv](https://arxiv.org/abs/2512.16915)



---

### 6. Seedance 1.5 pro：一种原生音视频联合生成基础模型

**原文标题：** Seedance 1.5 pro: A Native Audio-Visual Joint Generation Foundation Model

**摘要：**
视频生成领域的最新进展为统一的音视频生成铺平了道路。本研究提出了Seedance 1.5 pro，这是一个专门为原生、联合的音视频生成而设计的基础模型。该模型采用双分支扩散Transformer架构，集成了跨模态联合模块与专门的多阶段数据流水线，实现了卓越的音画同步效果与优异的生成质量。为确保其实用性，我们实施了精细的训练后优化，包括基于高质量数据集的有监督微调以及采用多维奖励模型的人类反馈强化学习。此外，我们引入了一个加速框架，将推理速度提升了超过10倍。Seedance 1.5 pro凭借其精确的多语言与方言口型同步、动态的电影级镜头控制以及增强的叙事连贯性脱颖而出，使其成为专业级内容创作的强大引擎。Seedance 1.5 pro现已通过火山引擎平台对外开放，访问地址为：https://console.volcengine.com/ark/region:ark+cn-beijing/experience/vision?type=GenVideo。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.13507) | [arXiv](https://arxiv.org/abs/2512.13507)



---

### 7. 深度任意全景：全景深度估计的基础模型

**原文标题：** Depth Any Panoramas: A Foundation Model for Panoramic Depth Estimation

**摘要：**
本研究提出了一种全景度量深度基础模型，能够泛化适应多样化的场景距离。我们从数据构建与框架设计的双重角度，探索了一种数据驱动的循环范式。通过整合公开数据集、基于UE5模拟器生成的高质量合成数据、文本到图像模型生成的数据以及从网络收集的真实全景图像，我们构建了一个大规模数据集。为减少室内/室外及合成/真实数据间的领域差异，我们引入了三阶段伪标签优化流程，为未标注图像生成可靠的基准真值。在模型设计上，我们采用具有强大预训练泛化能力的DINOv3-Large作为主干网络，并提出了即插即用的距离掩码头模块、以清晰度为中心的优化策略和以几何一致性为中心的优化方法，以增强模型对多变距离的鲁棒性并强化跨视角的几何一致性。在多个基准数据集（如Stanford2D3D、Matterport3D和Deep360）上的实验表明，该模型具有卓越的性能和零样本泛化能力，尤其在多样化真实场景中能提供鲁棒且稳定的度量深度预测。项目页面详见：https://insta360-research-team.github.io/DAP_website/

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.16913) | [arXiv](https://arxiv.org/abs/2512.16913)



---

### 8. 生成式重聚焦：基于单张图像的灵活散焦控制

**原文标题：** Generative Refocusing: Flexible Defocus Control from a Single Image

**摘要：**
景深控制在摄影中至关重要，但获得完美对焦往往需要多次尝试或特殊设备。单图像重聚焦技术仍面临挑战，其需同时恢复清晰内容并生成逼真的焦外虚化效果。现有方法存在明显局限：需要全对焦输入图像、依赖仿真器生成的合成数据，且对光圈的控制能力有限。本文提出生成式重聚焦技术，采用包含去模糊网络与虚化网络的两阶段流程，前者可从多样化输入中恢复全对焦图像，后者可生成可控虚化效果。本研究的核心创新在于半监督训练方法，通过结合合成配对数据与未配对真实虚化图像，并利用EXIF元数据捕捉仿真器无法提供的真实光学特性。实验表明，本方法在散焦去模糊、虚化合成与重聚焦基准测试中均达到最优性能。此外，所提出的生成式重聚焦技术支持文本引导的参数调整与自定义光圈形状。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.16923) | [arXiv](https://arxiv.org/abs/2512.16923)



---

### 9. DeContext 防御法：扩散变换器中的安全图像编辑

**原文标题：** DeContext as Defense: Safe Image Editing in Diffusion Transformers

**摘要：**
上下文扩散模型使用户能够以极高的便捷性和真实感修改图像。然而，这种能力也引发了严重的隐私担忧：个人图像可在未经所有者同意的情况下被轻易操纵，用于身份冒充、虚假信息传播或其他恶意用途。尽管先前的研究已探索通过输入扰动来防范个性化文本到图像生成中的滥用，但现代大规模基于上下文扩散变换器模型的鲁棒性在很大程度上仍未得到充分检验。本文提出DeContext方法，旨在保护输入图像免遭未经授权的上下文编辑。我们的核心发现是：源图像的上下文信息主要通过多模态注意力层传播至输出结果。通过注入微小且有针对性的扰动以削弱这些跨注意力路径，DeContext能够阻断这种信息流，从而有效解耦输入与输出之间的关联。这种简洁的防御机制兼具高效性与鲁棒性。我们进一步证明，早期去噪步骤和特定变换器模块主导着上下文传播过程，这使得我们可以将扰动集中在最关键的位置。在Flux Kontext和Step1X-Edit数据集上的实验表明，DeContext能持续阻断非预期的图像编辑，同时保持视觉质量。这些结果凸显了基于注意力的扰动作为抵御图像篡改的有效防御手段的优越性。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.16625) | [arXiv](https://arxiv.org/abs/2512.16625)



---

### 10. REGLUE：融合全局与局部语义的隐变量纠缠扩散方法

**原文标题：** REGLUE Your Latents with Global and Local Semantics for Entangled Diffusion

**摘要：**
隐空间扩散模型（LDMs）虽能实现最先进的图像合成，但其重建式去噪目标仅提供间接的语义监督：高层语义形成缓慢，需更长的训练时间且限制了生成质量。近期研究通过视觉基础模型（VFMs）注入语义，或采用外部表征对齐方式，或在扩散过程中仅联合建模狭窄的VFM特征片段，未能充分利用其丰富、非线性、多层级的空间语义信息。本文提出REGLUE（全局-局部统一编码的表征纠缠方法），该统一隐空间扩散框架在单个SiT主干网络中联合建模：（i）VAE图像隐变量，（ii）紧凑的局部（图像块级）VFM语义，以及（iii）全局（图像级）[CLS]标记。通过轻量级卷积语义压缩器，将多层VFM特征非线性聚合为低维空间结构化表征，并在扩散过程中与VAE隐变量进行纠缠。外部对齐损失进一步将内部表征正则化至冻结的VFM目标。在ImageNet 256×256数据集上，REGLUE相较于SiT-B/2和SiT-XL/2基线模型，以及REPA、ReDi和REG方法，持续提升FID指标并加速收敛。大量实验表明：（a）空间VFM语义至关重要，（b）非线性压缩是充分发挥其效益的关键，（c）全局标记与外部对齐在本研究提出的全局-局部-隐变量联合建模框架中起到互补的轻量化增强作用。代码已开源：https://github.com/giorgospets/reglue。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.16636) | [arXiv](https://arxiv.org/abs/2512.16636)



---

### 11. Alchemist：基于元梯度的数据选择提升文本到图像模型训练效率

**原文标题：** Alchemist: Unlocking Efficiency in Text-to-Image Model Training via Meta-Gradient Data Selection

**摘要：**
近年来，文本到图像（T2I）生成模型（如Imagen、Stable Diffusion和FLUX）在视觉质量方面取得了显著进步。然而，其性能从根本上受限于训练数据的质量。网络爬取和合成图像数据集中常包含低质量或冗余样本，这会导致视觉保真度下降、训练过程不稳定以及计算效率低下。因此，有效的数据选择对于提升数据效率至关重要。现有方法依赖于成本高昂的人工筛选或基于文本到图像数据过滤中单一维度特征的启发式评分。尽管基于元学习的方法已在大型语言模型中得到探索，但尚未适用于图像模态。为此，我们提出**Alchemist**，一种基于元梯度的框架，用于从大规模文本-图像数据对中选择合适的子集。该方法通过从数据中心的视角迭代优化模型，自动学习评估每个样本的影响。Alchemist包含两个关键阶段：数据评分与数据剪枝。我们训练一个轻量级评分器，基于梯度信息并辅以多粒度感知来估计每个样本的影响，随后采用Shift-G采样策略选择信息丰富的子集以进行高效的模型训练。Alchemist是首个面向文本到图像模型训练的自动化、可扩展、基于元梯度的数据选择框架。在合成与网络爬取数据集上的实验表明，Alchemist能够持续提升视觉质量与下游任务性能。使用Alchemist选择的50%数据进行训练，其效果可优于使用完整数据集进行训练。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.16905) | [arXiv](https://arxiv.org/abs/2512.16905)



---

### 12. 世界即画布：基于参考图像、轨迹与文本的可提示事件绘制

**原文标题：** The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text

**摘要：**
我们提出WorldCanvas框架，通过融合文本、轨迹与参考图像实现可提示的世界事件生成，支持丰富且用户导向的模拟。与纯文本方法及现有轨迹控制的图像转视频技术不同，我们的多模态方法将编码运动、时序与可见性的轨迹、表达语义意图的自然语言，以及确立物体视觉特征的参考图像相结合，能够生成包含多智能体交互、物体进出场景、参考图像引导的外观呈现及反直觉事件的连贯可控事件。生成的视频不仅具备时间连贯性，还展现出涌现一致性——即使物体暂时消失，其身份特征与场景关系仍得以保持。通过支持富有表现力的世界事件生成，WorldCanvas推动世界模型从被动预测器向用户可交互塑造的模拟器演进。项目页面详见：https://worldcanvas.github.io/。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.16924) | [arXiv](https://arxiv.org/abs/2512.16924)



---

### 13. N3D-VLM：原生三维定位赋能视觉语言模型实现精准空间推理

**原文标题：** N3D-VLM: Native 3D Grounding Enables Accurate Spatial Reasoning in Vision-Language Models

**摘要：**
尽管当前的多模态模型能够基于二维图像回答问题，但它们缺乏内在的三维物体感知能力，限制了其在三维场景中理解空间关系与深度线索的能力。本研究提出N3D-VLM，一种新颖的统一框架，将原生三维物体感知与三维感知视觉推理无缝集成，实现了精确的三维定位与可解释的空间理解。不同于传统端到端模型直接从RGB/RGB-D输入预测答案，我们的方法赋予模型原生三维物体感知能力，使其能够根据文本描述直接在三维空间中定位物体。在精确的三维物体定位基础上，模型进一步在三维空间中进行显式推理，实现更具可解释性和结构化的空间理解。为支持这些能力的鲁棒训练，我们开发了一个可扩展的数据构建流程，利用深度估计将大规模二维标注提升至三维空间，显著增加了三维物体定位数据的多样性与覆盖范围，其规模达到现有最大单图像三维检测数据集的六倍以上。此外，该流程生成了针对三维思维链推理的空间问答数据集，促进了三维物体定位与三维空间推理的联合训练。实验结果表明，我们的统一框架不仅在三维定位任务上取得了最先进的性能，而且在视觉语言模型的三维空间推理任务中持续超越现有方法。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.16561) | [arXiv](https://arxiv.org/abs/2512.16561)



---

### 14. JustRL：采用简易强化学习方案扩展15亿参数大语言模型

**原文标题：** JustRL: Scaling a 1.5B LLM with a Simple RL Recipe

**摘要：**
近期大语言模型强化学习方法呈现出日益复杂的趋势：多阶段训练流程、动态超参数调度以及课程学习策略。这引发了一个根本性问题：此类复杂性是否必要？本文提出JustRL——一种采用固定超参数单阶段训练的极简方法，在两个15亿参数推理模型上实现了最先进性能（在九项数学基准测试中平均准确率分别达到54.9%和64.3%），同时计算消耗比复杂方法减少2倍。相同超参数可在两个模型间直接迁移且无需调优，训练过程在4000余步中呈现平滑单调的改进曲线，未出现通常需要干预的崩溃或平台期。关键性消融实验表明，添加显式长度惩罚和鲁棒验证器等“标准技巧”可能因压缩探索空间而导致性能下降。这些结果表明，当前领域可能正在通过增加复杂性来解决那些在稳定扩展的基线方法中本不存在的问题。我们公开模型与代码，旨在为学界建立一个经过验证的简易基线标准。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.16649) | [arXiv](https://arxiv.org/abs/2512.16649)



---

### 15. AdaTooler-V：面向图像与视频的自适应工具调用

**原文标题：** AdaTooler-V: Adaptive Tool-Use for Images and Videos

**摘要：**
近期研究表明，多模态大语言模型（MLLMs）能够通过结合视觉工具交互的多模态交错思维链（CoT）获得性能提升。然而，现有开源模型常表现出盲目的工具调用推理模式，即使在无需工具时仍频繁调用视觉工具，这显著增加了推理开销并降低了模型性能。为此，我们提出AdaTooler-V，一种能够通过判断视觉问题是否真正需要工具来实现自适应工具调用的MLLM。首先，我们引入AT-GRPO强化学习算法，该算法基于每个样本的“工具效益评分”自适应调整奖励尺度，激励模型仅在工具能带来实质改进时进行调用。此外，我们构建了两个支持训练的数据集：用于监督微调冷启动的AdaTooler-V-CoT-100k，以及覆盖单图像、多图像和视频数据的、具有可验证奖励的强化学习数据集AdaTooler-V-300k。在十二个基准测试上的实验表明，AdaTooler-V具备强大的推理能力，在多样化的视觉推理任务中优于现有方法。值得注意的是，AdaTooler-V-7B在高分辨率基准V*上达到了89.8%的准确率，超越了商业闭源模型GPT-4o和Gemini 1.5 Pro。所有代码、模型与数据均已开源。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.16918) | [arXiv](https://arxiv.org/abs/2512.16918)



---

### 16. 探索与利用之辨：基于裁剪、熵与伪奖励的RLVR机制重思

**原文标题：** Exploration v.s. Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward

**摘要：**
本文研究了可验证奖励强化学习框架中探索与利用的权衡问题，该框架旨在提升大语言模型的推理能力。近期研究表明，RLVR可通过两种看似矛盾的机制激发大语言模型的数学推理能力：伪奖励机制通过奖励与真实答案无关的结果来抑制模型对已知策略的过度利用；熵最小化机制则通过推动模型产生更自信、更确定的输出来抑制探索行为。这两种机制共同揭示了一个令人困惑的动态现象：抑制利用与抑制探索均能提升推理性能，但协调这两种效应的内在原理尚不明确。本研究聚焦两个核心问题：（1）策略熵如何影响模型性能；（2）伪奖励是否通过裁剪偏差与模型污染的相互作用产生增益。实验结果表明，在伪奖励作用下，裁剪偏差会降低策略熵，促使模型输出更自信、更确定的结果，而单纯的熵最小化并不足以带来性能提升。我们进一步提出奖励错配模型，阐释了伪奖励为何能在非污染场景下提升模型性能。本研究阐明了伪奖励产生增益的内在机制，并为设计更有效的RLVR训练方法提供了理论依据。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.16912) | [arXiv](https://arxiv.org/abs/2512.16912)



---

### 17. 多模态奖励基准2：评估交错文本与图像的全能奖励模型

**原文标题：** Multimodal RewardBench 2: Evaluating Omni Reward Models for Interleaved Text and Image

**摘要：**
奖励模型对于训练大语言模型至关重要，但在处理交错图像与文本序列的全能模型领域仍缺乏深入探索。本文提出多模态奖励基准2，这是首个针对多模态理解与（交错）生成任务的奖励模型综合评测基准。该基准涵盖四大任务：文本到图像生成、图像编辑、交错内容生成以及多模态推理（“基于图像的思考”），每个任务包含来自23个模型与智能体在21项源任务中产生的1000组专家标注偏好对。本基准的设计具有三大特点：（1）兼具实用性与挑战性的提示词；（2）汇集前沿模型与智能体的生成响应；（3）通过集成过滤策略构建具有高度专家共识的偏好对。基于该基准，我们系统评估了各子任务的现有评判方法，包括多模态大语言模型即评判器以及基于人类偏好训练的模型。最新Gemini 3 Pro模型达到75-80%的准确率，GPT-5与Gemini 2.5 Pro达到66-75%的准确率（人类专家准确率＞90%），但仍显著超越广泛使用的GPT-4o（59%）。性能最佳的开源模型Qwen3-VL-32B达到与Gemini 2.5 Flash相当的准确率（64%）。我们进一步通过N选优抽样证明基准性能与下游任务成功率存在强相关性，并通过深入分析指出奖励模型未来改进的关键方向。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.16899) | [arXiv](https://arxiv.org/abs/2512.16899)



---

### 18. EasyV2V：一种基于指令的高质量视频编辑框架

**原文标题：** EasyV2V: A High-quality Instruction-based Video Editing Framework

**摘要：**
尽管图像编辑技术发展迅速，视频编辑领域仍相对缺乏深入探索，在一致性、控制性和泛化性方面面临诸多挑战。本研究系统探讨了数据、架构与控制机制的设计空间，提出了EasyV2V——一个简洁高效的基于指令的视频编辑框架。在数据层面，我们通过快速逆变换整合现有专家模型以构建多样化视频对，借助单帧监督与共享仿射运动的伪配对将图像编辑对提升至视频维度，挖掘密集标注视频片段以构建视频配对，并引入过渡监督机制以指导编辑过程的动态呈现。在模型架构方面，我们发现预训练的文本到视频模型本身具备编辑潜力，从而启发了简化设计思路。仅需通过简单的序列拼接进行条件控制，配合轻量级LoRA微调即可训练出高性能模型。在控制机制上，我们通过统一的单掩码机制实现时空协同控制，并支持可选参考图像输入。总体而言，EasyV2V支持灵活输入模式（如视频+文本、视频+掩码+文本、视频+掩码+参考图像+文本），在视频编辑效果上达到当前最优水平，超越了同期研究成果及商业系统。项目页面：https://snap-research.github.io/easyv2v/

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.16920) | [arXiv](https://arxiv.org/abs/2512.16920)



---

### 19. FlashPortrait：基于自适应潜在预测的6倍速无限肖像动画生成

**原文标题：** FlashPortrait: 6x Faster Infinite Portrait Animation with Adaptive Latent Prediction

**摘要：**
当前基于扩散模型的长序列肖像动画加速方法难以有效保证身份特征的一致性。本文提出FlashPortrait——一种端到端的视频扩散变换器模型，能够合成保持身份特征一致性的无限长度视频，并在推理速度上实现最高6倍的加速。具体而言，FlashPortrait首先通过现成的特征提取器计算身份无关的面部表情特征，随后引入标准化面部表情模块，通过使用各自均值与方差对特征进行归一化处理，将面部特征与扩散潜在空间对齐，从而提升面部建模中的身份稳定性。在推理阶段，模型采用动态滑动窗口机制，并在重叠区域进行加权融合，确保长序列动画的平滑过渡与身份一致性。在每个上下文窗口中，FlashPortrait基于特定时间步的潜在变量变化率及扩散层间的导数幅值比，利用当前时间步的高阶潜在导数直接预测未来时间步的潜在表示，从而跳过多个去噪步骤，实现6倍速度加速。基准测试实验从定性与定量两方面验证了FlashPortrait的有效性。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.16900) | [arXiv](https://arxiv.org/abs/2512.16900)



---

### 20. RePlan：面向复杂指令图像编辑的推理引导区域规划方法

**原文标题：** RePlan: Reasoning-guided Region Planning for Complex Instruction-based Image Editing

**摘要：**
基于指令的图像编辑技术实现了通过自然语言控制视觉内容修改，然而现有模型在面对指令-视觉复杂度（IV-Complexity）场景时表现欠佳——即当复杂指令与杂乱或模糊视觉场景交织时。本文提出RePlan（区域对齐规划框架），该“先规划后执行”框架将视觉语言规划器与扩散编辑器相结合。规划器通过逐步推理分解指令，并将其显式关联至目标区域；编辑器随后采用无需训练的注意力区域注入机制实施修改，实现了无需迭代修复的精准并行多区域编辑。为增强规划能力，我们基于GRPO强化学习方法，仅使用1000条纯指令样本进行训练，显著提升了推理准确性与格式可靠性。此外，我们构建了IV-Edit基准数据集，专注于细粒度区域定位与知识密集型编辑任务。在多种IV-Complex场景测试中，RePlan始终优于基于更大规模数据集训练的基线模型，在区域定位精度与整体编辑保真度方面均取得显著提升。项目主页：https://replan-iv-edit.github.io

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.16864) | [arXiv](https://arxiv.org/abs/2512.16864)



---

### 21. VenusBench-GD：面向多样化定位任务的多平台综合图形用户界面基准

**原文标题：** VenusBench-GD: A Comprehensive Multi-Platform GUI Benchmark for Diverse Grounding Tasks

**摘要：**
图形用户界面（GUI）定位是构建高效能GUI智能体的关键组成部分。然而，现有定位基准存在显著局限性：它们要么数据量不足、领域覆盖狭窄，要么过度聚焦单一平台且需要高度专业化的领域知识。本研究提出VenusBench-GD——一个跨越多平台的双语综合GUI定位基准，支持面向实际应用场景的层次化评估。本基准的贡献包括：（一）构建了大规模跨平台基准数据集，涵盖广泛的应用类型、多样化的UI元素及丰富的标注数据；（二）建立了高质量的定位任务数据构建流程，其标注精度超越现有基准；（三）通过提出层次化任务分类体系扩展了元素定位的范畴，将定位任务划分为基础与高级两大类别，涵盖六个设计互补的子任务以多维度评估模型性能。实验结果表明：通用多模态模型在基础定位任务上已媲美甚至超越专用GUI模型，而高级任务仍更适配GUI专用模型，但后者存在明显的过拟合与鲁棒性不足问题。这些发现凸显了建立综合性多层次评估框架的必要性。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.16501) | [arXiv](https://arxiv.org/abs/2512.16501)



---

### 22. ModelTables：关于模型的表格语料库

**原文标题：** ModelTables: A Corpus of Tables about Models

**摘要：**
本文提出ModelTables，这是一个针对模型湖中表格的基准数据集，旨在捕捉性能与配置表格的结构化语义信息，这些信息在纯文本检索中常被忽视。该语料库基于Hugging Face模型卡片、GitHub README文件及相关论文构建，将每个表格与其对应的模型及出版背景进行关联。与开放数据湖中的表格相比，模型表格规模较小但表现出更密集的表格间关联，反映了模型与基准测试紧密耦合的演化特性。当前版本涵盖超过6万个模型和9万个表格。为评估模型与表格的相关性，我们通过三种互补信号构建了多源基准真值：(1) 论文引用链接，(2) 显式模型卡片链接与继承关系，(3) 共享训练数据集。我们以表格搜索为例展示了该基准数据集的一个实证应用案例，比较了经典数据湖搜索操作（可合并、可连接、关键词搜索）与信息检索基线方法（稠密检索、稀疏检索、混合检索）在此基准上的表现。基于合并的语义表格检索整体P@1达到54.8%（引用信号54.6%，继承信号31.3%，共享数据集信号30.6%）；基于表格的稠密检索达到66.5% P@1，元数据混合检索为54.1%。评估结果表明现有表格搜索方法仍有明显改进空间。通过发布ModelTables及其构建协议，我们首次提供了描述AI模型的大规模结构化数据基准。我们在模型湖中开展的表格发现应用案例，为开发更精确的语义检索、结构化比较以及结构化模型知识的系统化组织提供了理论依据与实践证明。源代码、数据及相关材料已发布于https://github.com/RJMillerLab/ModelTables。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.16106) | [arXiv](https://arxiv.org/abs/2512.16106)



---

### 23. 听觉翻译：语音模态集成于大语言模型的有效性研究

**原文标题：** Hearing to Translate: The Effectiveness of Speech Modality Integration into LLMs

**摘要：**
随着大语言模型（LLMs）的应用范围从文本向多模态扩展，将语音作为原生模态进行集成催生了语音大语言模型（SpeechLLMs），其旨在直接翻译口语，从而绕开传统的基于转写的处理流程。然而，这种集成是否比成熟的级联架构更能提升语音到文本的翻译质量，仍是一个悬而未决的问题。本研究提出“听觉翻译”，首次构建了一个全面的测试套件，将5个前沿的SpeechLLMs与16个结合领先语音基础模型（SFMs）和多语言LLMs的强大直接及级联系统进行严格基准比较。我们的分析涵盖16个基准测试、13种语言对以及9种具有挑战性的条件，包括不流畅、含噪声和长篇幅语音。在此广泛评估中，我们发现级联系统总体上仍是最可靠的方案，而当前的SpeechLLMs仅在特定场景下与级联系统表现相当，SFMs则落后于两者。这突出表明，无论是在模型内部还是通过流程集成，引入LLM对于实现高质量的语音翻译至关重要。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.16378) | [arXiv](https://arxiv.org/abs/2512.16378)



---

### 24. 差异之重：基于能力差距发现与修正的模型审计方法

**原文标题：** Differences That Matter: Auditing Models for Capability Gap Discovery and Rectification

**摘要：**
传统多模态大语言模型（MLLMs）的评估方法缺乏可解释性，往往难以充分揭示模型间显著的能力差距。为此，我们提出AuditDM——一种通过主动审计模型分歧来发现并修正MLLM失效模式的自动化框架。该框架通过强化学习微调MLLM作为审计器，使其生成能够最大化目标模型间分歧的挑战性问题和反事实图像。训练完成后，审计器可发掘出多样化的可解释示例，这些示例既能揭示模型弱点，又可作为无需标注的修正数据。在Gemma-3和PaliGemma-2等前沿模型上的实验表明，AuditDM成功识别出超过20类不同的失效模式。基于这些发现进行微调后，所有模型在16个基准测试中均获得持续提升，并使一个30亿参数模型超越其280亿参数的对照模型。研究结果表明，当数据扩展的边际效益递减时，定向模型审计为模型诊断与性能提升提供了有效路径。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.16921) | [arXiv](https://arxiv.org/abs/2512.16921)



---

### 25. Insight Miner：面向跨领域自然语言对齐的时间序列分析数据集

**原文标题：** Insight Miner: A Time Series Analysis Dataset for Cross-Domain Alignment with Natural Language

**摘要：**
时间序列数据在环境分析、农业、交通和金融等诸多科学与工业领域中至关重要。然而，从这类数据中挖掘洞见通常需要深厚的领域专业知识，这一过程既耗时又费力。本文提出Insight Miner，这是一个旨在生成高质量、综合性时间序列描述的大规模多模态模型，其描述内容融合了领域特定知识。为实现这一目标，我们推出了TS-Insights（可通过\href{https://huggingface.co/datasets/zhykoties/time-series-language-alignment}{https://huggingface.co/datasets/zhykoties/time-series-language-alignment}访问），这是首个面向时间序列与语言对齐的通用领域数据集。TS-Insights包含从20个预测数据集中采样的10万个时间序列窗口，我们通过一种新颖的智能体工作流程构建该数据集：先使用统计工具从原始时间序列中提取特征，再借助GPT-4将其合成为连贯的趋势描述。基于TS-Insights进行指令微调后，Insight Miner在生成时间序列描述与洞见方面超越了LLaVA（liu2023llava）和GPT-4等先进多模态模型。我们的研究结果为利用多模态模型进行时间序列分析指明了可行方向，并为使大语言模型将时间序列作为原生输入模态进行解读奠定了重要基础。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.11251) | [arXiv](https://arxiv.org/abs/2512.11251)



---

### 26. Make-It-Poseable：面向三维人形角色动画的前馈式潜在姿态生成模型

**原文标题：** Make-It-Poseable: Feed-forward Latent Posing Model for 3D Humanoid Character Animation

**摘要：**
三维角色姿态生成是计算机图形学与视觉领域的一项基础任务。然而，现有方法如自动绑定技术与姿态条件生成模型常面临蒙皮权重预测不准确、拓扑结构缺陷及姿态贴合度不足等挑战，限制了其鲁棒性与泛化能力。为突破这些局限，本文提出Make-It-Poseable——一种创新的前馈式框架，将角色姿态生成重新定义为潜在空间变换问题。与传统流程中直接变形网格顶点不同，本方法通过操纵潜在表征直接重建新姿态下的角色。其核心是一个基于骨骼运动操纵形状标记的潜在姿态变换器，该过程通过密集姿态表征实现精准控制。为确保高保真几何输出并适应拓扑结构变化，我们同时提出了潜在空间监督策略与自适应补全模块。实验表明，本方法在姿态生成质量上表现优异，并能自然扩展到部件替换与精细化编辑等三维编辑任务中。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.16767) | [arXiv](https://arxiv.org/abs/2512.16767)



---

### 27. FrameDiffuser：基于G-Buffer条件扩散的神经前向帧渲染

**原文标题：** FrameDiffuser: G-Buffer-Conditioned Diffusion for Neural Forward Frame Rendering

**摘要：**
面向交互应用的神经渲染需要将几何与材质属性（G-Buffer）逐帧转换为具有真实光照效果的逼真图像。尽管近期基于扩散模型的方法在G-Buffer条件图像合成中展现出潜力，但仍存在关键局限：如RGBX等单帧模型独立生成各帧而缺乏时序一致性；而DiffusionRenderer等视频模型则因计算成本过高难以适配多数消费级游戏设备，且需预先获取完整序列，无法满足未来帧依赖用户输入的交互场景需求。本文提出FrameDiffuser——一种自回归神经渲染框架，通过融合G-Buffer数据与模型自身历史输出来生成时序一致的高真实感帧序列。在生成首帧后，FrameDiffuser仅依赖输入的几何、材质及表面属性等G-Buffer数据，同时利用已生成的前一帧提供时序引导，从而在数百至数千帧范围内保持稳定且时序一致的生成效果。我们的双条件架构结合了ControlNet的结构引导与ControlLoRA的时序连贯性控制，并通过三阶段训练策略实现稳定的自回归生成。该模型针对特定场景环境进行专门化训练，将时序一致性与推理速度置于广泛泛化能力之上。实验表明，相较于通用化方法，场景定制化训练能实现更优越的逼真度，在光照、阴影与反射等视觉效果方面表现出更高精度。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.16670) | [arXiv](https://arxiv.org/abs/2512.16670)



---

### 28. 可训练的对数线性稀疏注意力机制用于高效扩散变换器

**原文标题：** Trainable Log-linear Sparse Attention for Efficient Diffusion Transformers

**摘要：**
扩散变换器（DiTs）在视觉生成领域取得了最先进的性能，但其二次方的自注意力计算成本从根本上限制了其向长令牌序列的扩展。近期提出的Top-K稀疏注意力方法通过将令牌压缩为块状表示并选择少量相关关键块来减少DiTs的计算量，但仍存在以下问题：（1）在压缩令牌上仍需二次方选择成本；（2）随着序列增长，为保持模型质量需要不断增加K值。我们发现其效率低下的根源在于单层设计，因为单一粗粒度层级不足以有效表示全局结构。本文提出对数线性稀疏注意力（LLSA），这是一种适用于极长令牌序列的可训练稀疏注意力机制，通过利用分层结构将选择成本和注意力成本从二次方降低至对数线性复杂度。LLSA执行分层Top-K选择，基于前一层级发现的索引逐步采用稀疏Top-K选择，并引入分层键值增强机制，在注意力计算过程中使用更少不同粒度的令牌同时保持全局上下文信息。为支持高效训练，我们开发了高性能GPU实现方案，在前向和反向传播中仅使用稀疏索引，无需构建稠密注意力掩码。我们在不使用分块化和VAE编码的高分辨率像素空间图像生成任务上评估LLSA。在256×256像素令牌序列上，LLSA将注意力推理速度提升28.27倍，将DiT训练速度提升6.09倍，同时保持生成质量。实验结果表明，LLSA为高效训练长序列DiTs提供了有前景的技术路径。代码已开源：https://github.com/SingleZombie/LLSA

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.16615) | [arXiv](https://arxiv.org/abs/2512.16615)



---

### 29. 面向语言模型通用推理的耦合变分强化学习方法

**原文标题：** Coupled Variational Reinforcement Learning for Language Model General Reasoning

**摘要：**
尽管强化学习在语言模型推理领域取得了显著进展，但其发展仍受限于对可验证奖励信号的依赖。近期无需验证器的强化学习方法通过利用大语言模型生成参考答案的内在概率作为奖励信号，缓解了这一限制。然而，此类方法通常仅基于问题本身对推理轨迹进行采样。这种设计使得推理轨迹采样与答案信息相分离，导致探索效率低下以及推理轨迹与最终答案之间的不连贯性。本文提出耦合变分强化学习方法，该方法通过混合采样策略耦合先验分布与后验分布，从而建立变分推断与强化学习之间的桥梁。通过构建并优化融合这两种分布的复合分布，该方法在保持强思维-答案一致性的同时实现了高效探索。在数学推理与通用推理基准上的大量实验表明，该方法相较于基线模型性能提升12.4%，并在当前先进的无需验证器强化学习基线方法基础上再获2.3%的性能增益，为增强语言模型的通用推理能力提供了理论严谨的框架。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.12576) | [arXiv](https://arxiv.org/abs/2512.12576)



---

### 30. MomaGraph：基于视觉语言模型的状态感知统一场景图用于具身任务规划

**原文标题：** MomaGraph: State-Aware Unified Scene Graphs with Vision-Language Model for Embodied Task Planning

**摘要：**
家庭环境中的移动操作机器人需同时具备导航与操控能力，这要求一种紧凑且语义丰富的场景表征，能够捕捉物体位置、功能属性及可操作部件。场景图虽为自然选择，但现有研究常将空间关系与功能关系分离，将场景视为缺乏物体状态或时序更新的静态快照，且忽略与当前任务最相关的信息。为突破这些局限，我们提出MomaGraph——一种面向具身智能体的统一场景表征，其整合了空间-功能关联与部件级交互元素。然而，推进此类表征需要适配的数据集与严谨的评估体系，而这两者长期缺失。为此，我们构建了MomaGraph-Scenes（首个面向家庭环境、包含丰富标注的任务驱动场景图大规模数据集）与MomaGraph-Bench（涵盖从高层规划到细粒度场景理解的六类推理能力的系统化评估套件）。基于此基础，我们进一步开发了MomaGraph-R1——一个通过强化学习在MomaGraph-Scenes上训练的70亿参数视觉语言模型。该模型能够预测任务导向场景图，并在“先构图后规划”框架下实现零样本任务规划。大量实验表明，我们的模型在开源模型中达到最优性能，在基准测试中取得71.6%的准确率（较最佳基线提升11.4%），同时在公开基准上展现良好泛化能力，并能有效迁移至真实机器人实验。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.16909) | [arXiv](https://arxiv.org/abs/2512.16909)



---

### 31. TabReX：基于无参考表格的可解释性评估框架

**原文标题：** TabReX : Tabular Referenceless eXplainable Evaluation

**摘要：**
评估大型语言模型（LLMs）生成的表格质量仍是一个开放性问题：现有方法或将表格扁平化为文本而忽略其结构，或依赖固定参考标准从而限制泛化能力。本文提出TabReX，一种基于无参考、属性驱动的评估框架，通过图推理方法对表格生成质量进行评估。TabReX将源文本与生成表格均转换为规范化知识图谱，通过LLM引导的匹配过程实现对齐，并计算可解释的、基于评估量规的分数，以量化结构与事实一致性。该评估指标可在敏感性与特异性之间实现可控权衡，生成与人类判断对齐的评估结果及单元格级错误追溯。为系统评估指标鲁棒性，我们构建了TabReX-Bench大规模基准数据集，涵盖六个领域、十二种规划驱动的扰动类型及三个难度层级。实验结果表明，TabReX与专家评估结果具有最高相关性，在强扰动下保持稳定，并能支持模型与提示词的细粒度对比分析，为结构化生成系统的可信可解释评估建立了新范式。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.15907) | [arXiv](https://arxiv.org/abs/2512.15907)



---

### 32. 用于创造性连接与表达视觉概念的“氛围空间”

**原文标题：** Vibe Spaces for Creatively Connecting and Expressing Visual Concepts

**摘要：**
创造新的视觉概念通常需要通过图像最相关的共享属性——即其“氛围”——来连接不同的创意。本文提出“氛围融合”这一新颖任务，旨在生成连贯且有意义的混合图像，以揭示图像间的共享属性。现有方法在实现此类融合时面临挑战，难以识别并遍历潜在空间中连接远距离概念的非线性路径。为此，我们提出“氛围空间”——一种层次化图流形结构，能够在CLIP等特征空间中学习低维测地线，从而实现概念间平滑且语义一致的过渡。为评估创意质量，我们设计了一个受认知启发的评估框架，结合人类主观判断、大语言模型推理以及基于几何路径的难度评分。实验表明，相较于现有方法，“氛围空间”生成的融合图像在人类评估中 consistently 被评为更具创意性与连贯性。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.14884) | [arXiv](https://arxiv.org/abs/2512.14884)



---

### 33. 心智内推理：潜在空间中的动态多模态交错

**原文标题：** Reasoning Within the Mind: Dynamic Multimodal Interleaving in Latent Space

**摘要：**
多模态大语言模型（MLLM）的最新进展通过在语义空间中引入思维链（CoT）推理，显著提升了跨模态理解与推理能力。在此基础上，近期研究将CoT机制扩展至视觉模态，使模型能够借助外部工具或显式图像生成在推理过程中整合视觉信息。然而，这些方法仍依赖于显式的分步推理，存在感知-推理交互不稳定及显著计算开销的问题。受人类认知机制启发，我们认为思维并非线性展开，而是通过推理与感知在心智内的动态交错进行。基于这一视角，我们提出DMLR——一种测试时动态多模态潜在推理框架，该框架采用置信度引导的潜在策略梯度优化方法，通过精炼潜在思维标记实现深度推理。此外，我们引入动态视觉注入策略，该策略在每个潜在思维标记处检索最相关的视觉特征并更新最佳视觉补丁集合，随后将更新后的补丁注入潜在思维标记，从而实现动态的视觉-文本交错。在七个多模态推理基准及多种模型架构上的实验表明，DMLR在保持高效推理的同时，显著提升了模型的推理与感知性能。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.12623) | [arXiv](https://arxiv.org/abs/2512.12623)



---

### 34. 双向归一化流：从数据到噪声及其逆过程

**原文标题：** Bidirectional Normalizing Flow: From Data to Noise and Back

**摘要：**
归一化流已被确立为生成建模的规范化框架。标准归一化流包含前向过程与反向过程：前向过程将数据映射至噪声，反向过程则通过逆变换生成样本。典型归一化流的前向变换受显式可逆性约束，确保反向过程可作为其精确解析逆。近期TARFlow及其变体通过结合Transformer与自回归流重振了归一化流方法，但也暴露出因果解码作为主要瓶颈的问题。本研究提出双向归一化流框架，该框架无需依赖精确解析逆。双向归一化流通过学习近似底层噪声到数据逆映射的反向模型，实现了更灵活的损失函数与架构设计。在ImageNet上的实验表明，相较于因果解码方法，双向归一化流在将采样速度提升两个数量级的同时，显著提高了生成质量。该框架在基于归一化流的方法中取得了最优结果，并在单次评估方法中展现出具有竞争力的性能。随着归一化流领域近期取得的积极进展，我们希望本研究能进一步唤起对这一经典范式的关注。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.10953) | [arXiv](https://arxiv.org/abs/2512.10953)



---

### 35. EmoCaliber：通过置信度言语化与校准提升视觉情感理解的可靠性

**原文标题：** EmoCaliber: Advancing Reliable Visual Emotion Comprehension via Confidence Verbalization and Calibration

**摘要：**
视觉情感理解旨在从图像中蕴含的情感线索推断情感极性或情绪类别。近年来，多模态大语言模型凭借其泛化能力，成为统一不同情感分类体系下视觉情感理解任务的流行范式。尽管该范式取得了显著成功，但其通常将视觉情感理解视为确定性任务，要求模型为每张图像输出单一、确定的情感标签。这种设定未能充分考虑情感感知固有的主观性，忽略了对于不同观察者而言可能同样合理的其他解释。为应对这一局限，我们提出赋予多模态大语言模型对其情感预测结果进行置信度言语化的能力。这一附加信号使用户能够同时评估其他解释的合理性以及模型自我评估的胜任程度，从而在实践中提升系统可靠性。基于此洞见，我们提出一个三阶段训练框架，逐步赋予模型结构化推理能力、教授其置信度言语化方法，并对置信度表达进行校准，最终形成用于视觉情感理解的置信度感知模型EmoCaliber。通过在统一基准测试集VECBench上进行公平全面的评估，EmoCaliber在情感预测与置信度估计两方面均展现出相较于现有方法的整体优越性。这些结果验证了我们方法的有效性，标志着向更可靠的视觉情感理解系统迈出了切实一步。项目页面：https://github.com/wdqqdw/EmoCaliber。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.15528) | [arXiv](https://arxiv.org/abs/2512.15528)



---

### 36. Nemotron-Math：基于多模态监督的高效长上下文数学推理蒸馏

**原文标题：** Nemotron-Math: Efficient Long-Context Distillation of Mathematical Reasoning from Multi-Mode Supervision

**摘要：**
高质量的数学推理监督需要多样化的推理风格、长篇幅的解题过程以及有效的工具集成能力，而现有数据集仅能提供有限形式的支持。借助gpt-oss-120b的多模态生成能力，我们提出了Nemotron-Math——一个大规模数学推理数据集，包含750万条涵盖高、中、低三种推理模式的解题过程，每种模式均提供包含与不包含Python工具集成推理（TIR）的版本。该数据集整合了8.5万道精选的AoPS竞赛题与26.2万道社区来源的StackExchange-Math问题，将结构化竞赛任务与多样化的现实数学问题相结合。我们通过受控评估验证了数据集质量：在匹配的AoPS问题上，Nemotron-Math始终优于原始OpenMathReasoning数据集；引入StackExchange-Math数据显著提升了模型的鲁棒性与泛化能力（尤其在HLE-Math基准上），同时保持了数学竞赛基准的准确性。为支持高效的长上下文训练，我们开发了顺序分桶策略，使128K上下文长度的微调加速2-3倍且未造成显著精度损失。总体而言，Nemotron-Math实现了最先进的性能表现，在使用Python TIR时，在AIME 2024和2025测试中达到了100%的maj@16准确率。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.15489) | [arXiv](https://arxiv.org/abs/2512.15489)



---

### 37. 提示与程序间的状态共享

**原文标题：** Sharing State Between Prompts and Programs

**摘要：**
大型语言模型（LLM）的兴起催生了一种新型编程范式：自然语言编程。通过编写提示词来引导LLM执行自然语言处理、代码生成、推理等任务，用户实际上是在用自然语言编写代码——即自然语言代码——供LLM执行。

当前新兴的研究领域致力于实现自然语言代码与Python等形式化语言之间的互操作性。本文提出一种新颖的编程抽象概念——共享程序状态，该概念消除了实现自然语言代码与程序状态互操作性所需的人工操作。借助共享程序状态，程序员能够编写可直接写入程序变量、使用程序对象进行计算，并在程序中实现控制流的自然语言代码。我们提出一种用于定义自然函数接口的规范框架，该框架可扩展编程系统以支持自然语言代码，并基于此框架将共享程序状态定义为一种自然函数接口。

我们在Nightjar编程系统中实现了共享程序状态机制。Nightjar使程序员能够编写包含自然语言代码的Python程序，这些自然语言代码可直接与Python程序状态进行交互。实验表明，Nightjar程序在任务准确率上达到甚至超过人工编写实现的水平（提升4-19%），同时平均减少39.6%的代码量。使用Nightjar的代价是可能产生运行时开销（运行时间约为人工实现的0.4-4.3倍）。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.14805) | [arXiv](https://arxiv.org/abs/2512.14805)



---

### 38. 通过混合LoRA提升递归Transformer的性能

**原文标题：** Improving Recursive Transformers with Mixture of LoRAs

**摘要：**
递归Transformer中的参数共享虽然减少了模型规模，但会导致层间表达能力退化。我们提出混合低秩适配器（MoL），这是一种轻量级的条件计算机制，通过在共享前馈网络内部插入低秩适配器专家模块实现。与先前添加固定或外部适配器的方法不同，MoL能够在不解耦主干参数的前提下，实现基于令牌条件的共享前馈网络权重空间调制。我们预训练了一个现代化的递归架构ModernALBERT，该架构融合了旋转位置编码、GeGLU激活函数、FlashAttention注意力机制以及基于知识蒸馏的初始化方法。在GLUE、SQuAD-v2和BEIR基准测试中，ModernALBERT（参数量50M-120M）在紧凑模型中实现了最优性能，并超越了更大规模的完全参数化基线模型。我们还提出了一种专家合并方法，可在推理阶段将MoL压缩为单一适配器且保持精度，从而实现高效部署。实验结果表明，条件权重空间调制能有效恢复递归Transformer在激进参数共享策略下损失的表达能力。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.12880) | [arXiv](https://arxiv.org/abs/2512.12880)



---


## 🔍 关键词云图
![关键词云图](../images/keywords_wordcloud.png)

## 📈 近期论文趋势
![论文趋势](../images/daily_papers.png)

## 🎙️ 语音播报
- [收听今日论文解读](../audio/2025-12-19_daily_papers.mp3)

## 📱 订阅渠道
- GitHub: [hf-daily-paper-newsletter-chinese](https://github.com/2404589803/hf-daily-paper-newsletter-chinese)