<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Hugging Face 论文日报 - 2025-11-11</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', sans-serif;
            line-height: 1.6;
            color: #333;
            background-color: #f5f5f5;
            padding: 20px;
        }
        
        .container {
            max-width: 900px;
            margin: 0 auto;
            background-color: white;
            padding: 40px;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }
        
        h1 {
            color: #2c3e50;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
            margin-bottom: 30px;
            font-size: 28px;
        }
        
        h1 img {
            vertical-align: middle;
            margin-right: 10px;
        }
        
        h2 {
            color: #34495e;
            margin-top: 40px;
            margin-bottom: 20px;
            font-size: 24px;
            border-left: 4px solid #3498db;
            padding-left: 15px;
        }
        
        h3 {
            color: #2c3e50;
            margin-top: 30px;
            margin-bottom: 15px;
            font-size: 20px;
        }
        
        p {
            margin-bottom: 15px;
            text-align: justify;
        }
        
        ul {
            margin-left: 20px;
            margin-bottom: 20px;
        }
        
        li {
            margin-bottom: 8px;
        }
        
        a {
            color: #3498db;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        strong {
            color: #2c3e50;
            font-weight: 600;
        }
        
        hr {
            border: none;
            border-top: 1px solid #e0e0e0;
            margin: 30px 0;
        }
        
        /* 关键修复:限制图片宽度 */
        img {
            max-width: 100%;
            height: auto;
            display: block;
            margin: 20px auto;
            border-radius: 4px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        
        /* 确保图片容器也有宽度限制 */
        p img {
            max-width: 100%;
        }
        
        /* 论文详情区域样式 */
        .paper-section {
            background-color: #f9f9f9;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 20px;
        }
        
        /* 统计信息样式 */
        .stats {
            background-color: #e8f4f8;
            padding: 15px;
            border-radius: 6px;
            margin-bottom: 20px;
        }
        
        /* 响应式设计 */
        @media (max-width: 768px) {
            .container {
                padding: 20px;
            }
            
            h1 {
                font-size: 24px;
            }
            
            h2 {
                font-size: 20px;
            }
            
            h3 {
                font-size: 18px;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <h1><img src="https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo.png" width="30"/> Hugging Face 2025-11-11 论文日报</h1>
<h2>📊 今日论文统计</h2>
<ul>
<li>总论文数：30</li>
<li>热门领域：RL, LLM, Transformer, Diffusion</li>
</ul>
<h2>📝 论文详情</h2>
<h3>1. HaluMem：智能体记忆系统中的幻觉现象评估研究</h3>
<p><strong>原文标题：</strong> HaluMem: Evaluating Hallucinations in Memory Systems of Agents</p>
<p><strong>摘要：</strong>
记忆系统是实现大语言模型与智能体长期学习及持续交互的核心组件。然而在记忆存储与检索过程中，这些系统常出现记忆幻觉现象，具体表现为虚构、错漏、矛盾与缺失等问题。现有对记忆幻觉的评估主要采用端到端问答模式，难以准确定位幻觉产生的具体操作环节。为此，我们首次提出面向记忆系统的操作级幻觉评估基准HaluMem，通过定义记忆提取、记忆更新和记忆问答三项评估任务，系统揭示交互过程中不同操作阶段的幻觉行为特征。为支撑评估工作，我们构建了以用户为中心的多轮人机交互数据集HaluMem-Medium与HaluMem-Long，两者共包含约1.5万个记忆节点及3.5千道多类型问题。单用户平均对话轮次分别达到1.5千轮和2.6千轮，上下文长度超百万标记，可实现对不同上下文规模与任务复杂度下的幻觉评估。基于HaluMem的实证研究表明，现有记忆系统在提取与更新阶段易产生并积累幻觉，进而将误差传播至问答阶段。未来研究应致力于开发可解释的约束性记忆操作机制，系统抑制幻觉产生并提升记忆可靠性。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.03506">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.03506">arXiv</a></p>
<hr />
<h3>2. IterResearch：基于马尔可夫状态重构的长周期智能体范式再思考</h3>
<p><strong>原文标题：</strong> IterResearch: Rethinking Long-Horizon Agents via Markovian State
  Reconstruction</p>
<p><strong>摘要：</strong>
深度研究智能体的最新进展通过对外部信息源进行动态推理，为实现自主知识构建提供了可能。然而，现有方法依赖单一上下文范式，将所有信息累积在持续扩展的上下文窗口中，导致上下文窒息与噪声污染问题，限制了其在长周期任务中的效能。我们提出IterResearch——一种创新的迭代式深度研究范式，将长周期研究重新定义为具有策略性工作空间重构的马尔可夫决策过程。通过将动态演进的研究报告作为记忆体并定期整合研究洞见，该方法能在任意探索深度下保持稳定的推理能力。我们进一步开发了效率感知策略优化（EAPO），该强化学习框架通过几何奖励折现激励高效探索，并借助自适应降采样实现稳定的分布式训练。大量实验表明，IterResearch在六个基准测试中相较现有开源智能体平均提升14.5个百分点，显著缩小了与前沿私有系统的差距。值得注意的是，该范式展现出前所未有的交互扩展能力，可支持高达2048次交互且性能实现跨越式提升（从3.5%增至42.5%），同时作为有效的提示策略，在长周期任务上相较ReAct将前沿模型性能提升达19.2个百分点。这些发现确立了IterResearch作为长周期推理的通用解决方案，既能作为训练完成的智能体，也可作为前沿模型的提示范式。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.07327">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.07327">arXiv</a></p>
<hr />
<h3>3. DRIVE：面向可验证奖励的竞争性代码生成强化学习数据管理最佳实践</h3>
<p><strong>原文标题：</strong> DRIVE: Data Curation Best Practices for Reinforcement Learning with
  Verifiable Reward in Competitive Code Generation</p>
<p><strong>摘要：</strong>
近期以推理为先的模型（如OpenAI o1、DeepSeek R1）推动了RLVR技术的复兴。然而当前进展主要集中于数学领域（如AIME），竞争性编程代码生成领域探索不足，且数据管理获得的关注远少于强化学习算法设计。本研究探讨如何构建RLVR数据集（即强化学习提示），并提出在竞争性编程代码生成中实现强劲性能的实用训练技术。我们的流程始于基于强开源模型蒸馏得到的监督微调，并辅以通用型与推理密集型数据增强。强化学习阶段采用可执行、测试用例驱动的奖励机制，实施两阶段训练：首先使用组相对策略优化方法，在均匀分布的大规模竞争性编程问题集上进行训练（每提示8次 rollout，响应生成窗口较短——监督微调阶段32K，本阶段24K），以扩展熵值并缓解重复与截断问题；随后执行预GRPO阶段：在高质量挑战性问题的小型数据集上，采用大 rollout 预算（每提示64次 rollout）和硬聚焦课程策略（持续保留训练全程中最难实例）进行参数更新。我们在Qwen2.5-32B模型上实施该方法，并通过LeetCode和Codeforces周赛评估以避免数据泄露。最终模型在同等规模模型中达到最先进性能，与DeepSeek v3.1、Doubao-1.5-Thinking等领先系统表现相当。我们还分析了扩展规律，在内部大规模混合专家模型上观察到显著的强化学习扩展效应。本研究提炼出针对竞争性编程代码生成的RLVR数据管理、熵扩展与课程设计的简明最佳实践。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.06307">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.06307">arXiv</a></p>
<hr />
<h3>4. 《站台：面向人工智能驱动发现的开放世界环境》</h3>
<p><strong>原文标题：</strong> The Station: An Open-World Environment for AI-Driven Discovery</p>
<p><strong>摘要：</strong>
本文提出"站台"——一个模拟微型科研生态系统的开放世界多智能体环境。依托扩展上下文窗口，智能体可在站台中进行长周期科研探索，包括研读同行论文、提出假设、提交代码、执行分析及发表成果。值得关注的是，该系统不存在中央协调机制——智能体可自主选择行为并在环境中构建独立研究叙事。实验表明，站台中的AI智能体在数学、计算生物学、机器学习等多领域基准测试中均达到最新最优性能，尤其在圆包装问题上显著超越AlphaEvolve系统。当智能体开展自主研究、进行学术互动并累积历史成果时，会形成丰富的研究叙事脉络。这些涌现的叙事中自然衍生出创新方法，例如用于单细胞RNA测序批次整合的新型密度自适应算法。站台标志着在开放世界环境中通过涌现行为实现自主科学发现的重要突破，代表着超越刚性优化范式的新型研究范式的确立。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.06309">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.06309">arXiv</a></p>
<hr />
<h3>5. MVU-Eval：面向多模态大语言模型的多视频理解评估体系</h3>
<p><strong>原文标题：</strong> MVU-Eval: Towards Multi-Video Understanding Evaluation for Multimodal
  LLMs</p>
<p><strong>摘要：</strong>
多模态大语言模型（MLLMs）的出现将人工智能能力扩展至视觉模态，然而现有评估基准仍局限于单视频理解，忽视了现实场景（如体育赛事分析与自动驾驶）中对多视频理解的关键需求。为填补这一重要空白，我们推出MVU-Eval——首个面向MLLMs的多视频理解综合评估基准。该基准通过来自多元领域的4,959个视频构建的1,824个精编问答对，系统评估八大核心能力，涵盖基础感知任务与高阶推理任务。这些能力指标严格对标自动驾驶系统中的多传感器融合、多视角体育分析等实际应用场景。通过对前沿开源与闭源模型的广泛测试，我们揭示了当前MLLMs在多视频理解能力方面存在的显著性能差异与局限性。本基准将公开提供，以推动该领域的后续研究。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.07250">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.07250">arXiv</a></p>
<hr />
<h3>6. 路由流形对齐提升混合专家大语言模型的泛化能力</h3>
<p><strong>原文标题：</strong> Routing Manifold Alignment Improves Generalization of Mixture-of-Experts
  LLMs</p>
<p><strong>摘要：</strong>
稀疏混合专家模型因其能在不增加推理成本的前提下有效扩展模型能力，已被广泛应用于当前的大语言模型。然而，在广泛下游任务上的评估表明，现有混合专家大语言模型中的路由模块普遍存在次优问题，导致其与最优路由存在显著性能差距（例如准确率相差10-20%）。本文提出通过将路由权重流形与任务嵌入流形对齐，可有效缩小该差距并提升混合专家大语言模型的泛化性能。我们提出的"路由流形对齐方法"在训练目标中引入额外的流形正则项，仅需对路由模块进行轻量级微调（其余参数冻结）。具体而言，该正则化促使每个样本的路由权重在任务嵌入空间中接近其成功邻域样本（即路由权重能得出正确答案的样本）的权重，从而使面向相似任务的样本在不同网络层能保持一致的专家选择模式。在不同样本间建立任务与专家的绑定关系，对实现更优泛化能力至关重要。此外，该方法还展现了将任务理解（通过嵌入模型）与解决方案生成（通过混合专家大语言模型）相统一的优势。实验中，我们使用该方法对OLMoE、DeepSeekMoE和Qwen3-MoE的路由模块进行微调。在多基准测试中的评估结果以及与基线模型的广泛对比表明，该方法带来了显著的性能提升。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.07419">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.07419">arXiv</a></p>
<hr />
<h3>7. RedOne 2.0：社交网络服务中领域特定大语言模型后训练机制的重新思考</h3>
<p><strong>原文标题：</strong> RedOne 2.0: Rethinking Domain-specific LLM Post-Training in Social
  Networking Services</p>
<p><strong>摘要：</strong>
作为人类互动与信息交换的关键媒介，社交网络服务（SNS）对大语言模型（LLM）提出了独特挑战：异构工作负载、快速演变的网络规范与俚语、以及引发显著分布偏移的多语言跨文化语料。监督微调（SFT）虽能实现模型专业化，但往往会引发域内性能提升与域外鲁棒性之间的“跷跷板效应”，这一矛盾在小型模型中尤为突出。为应对这些挑战，我们提出RedOne 2.0——采用渐进式强化学习优先的后训练范式，专为快速稳定适配SNS场景而设计。该流程包含三个阶段：（1）基于精选SNS语料的探索性学习，建立初始对齐并识别系统性缺陷；（2）靶向微调阶段，针对诊断出的能力缺口选择性应用SFT，同时混入少量通用数据以缓解灾难性遗忘；（3）强化学习阶段，重新应用以SNS为核心信号的RL机制，巩固改进效果并协调多任务间的权衡关系。在涵盖三大类别的多样化任务测试中，我们的40亿参数模型相较70亿参数次优基线平均提升2.41个性能点。此外，RedOne 2.0相较基础模型实现平均8.74的性能跃升，其所需训练数据量不足以SFT为核心的方法RedOne的一半，彰显出在紧凑模型规模下卓越的数据效率与训练稳定性。总体而言，RedOne 2.0为SNS场景中的领域特定大语言模型建立了具有竞争力的成本效益基准，在保持鲁棒性的同时实现了能力突破。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.07070">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.07070">arXiv</a></p>
<hr />
<h3>8. SofT-GRPO：基于Gumbel重参数化软思考策略优化突破离散令牌大语言模型强化学习</h3>
<p><strong>原文标题：</strong> SofT-GRPO: Surpassing Discrete-Token LLM Reinforcement Learning via
  Gumbel-Reparameterized Soft-Thinking Policy Optimization</p>
<p><strong>摘要：</strong>
大语言模型的软思考推理范式在某些场景下能够超越传统的离散令牌思维链推理，彰显其研究价值与应用潜力。然而，虽然离散令牌思维链推理模式可通过群体相对策略优化等算法进行强化，但将软思考模式与强化学习相结合仍面临挑战。这一困难源于向软思考令牌注入随机性及相应策略更新的复杂性，导致先前将软思考与GRPO结合的尝试通常表现不及离散令牌GRPO方法。为充分释放软思考的潜力，本文提出新型策略优化算法SofT-GRPO，在软思考推理模式下强化大语言模型。该算法通过向logits注入Gumbel噪声，采用Gumbel-Softmax技术避免软思考令牌超出预训练嵌入空间，并在策略梯度中运用重参数化技巧。我们在1.5B至7B参数的基础大语言模型上进行实验，结果表明：SofT-GRPO使软思考大语言模型在Pass@1指标上略优于离散令牌GRPO（平均准确率提升0.13%），同时在Pass@32指标上实现显著提升（平均准确率增长2.19%）。代码与权重文件已发布于https://github.com/zz1358m/SofT-GRPO-master</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.06411">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.06411">arXiv</a></p>
<hr />
<h3>9. 基于置信度的推理：通过不确定性头部实现大语言模型推理步骤的高效验证</h3>
<p><strong>原文标题：</strong> Reasoning with Confidence: Efficient Verification of LLM Reasoning Steps
  via Uncertainty Heads</p>
<p><strong>摘要：</strong>
解决复杂任务通常需要大语言模型生成多步骤的推理链。已有研究表明，对单个推理步骤的正确性进行验证能够进一步提升大语言模型在此类任务中的表现与效率，并增强解决方案的可解释性。然而现有的验证方法（如过程奖励模型）存在计算成本高昂、适用领域受限或需要大规模人工/模型生成标注等问题。为此，我们提出一种基于数据驱动不确定性评分的轻量级步骤级推理验证方案。通过训练基于Transformer的不确定性量化头部，利用冻结参数大语言模型的内部状态来估计其生成过程中推理步骤的不确定性。该方法完全自动化：目标标签可由更大规模语言模型（如DeepSeek R1）生成，或通过原模型以自监督方式产生。不确定性头部在参数量不足1000万的情况下兼具高效性与轻量化特性。在数学推理、任务规划及常识问答等多个领域，其性能媲美甚至超越参数量达810倍的过程奖励模型。我们的研究结果表明，大语言模型的内部状态编码了其不确定性，可作为推理验证的可靠信号，为构建可扩展、可泛化的自省式大语言模型指明了前景广阔的研究方向。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.06209">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.06209">arXiv</a></p>
<hr />
<h3>10. 基于物理世界模型的机器人学习</h3>
<p><strong>原文标题：</strong> Robot Learning from a Physical World Model</p>
<p><strong>摘要：</strong>
本文提出PhysWorld框架，通过物理世界建模实现基于视频生成的机器人学习。当前视频生成模型能够根据语言指令和图像合成逼真的视觉演示，这为机器人技术提供了强大却尚未充分开发的训练信号来源。然而，直接将从生成视频中提取的像素运动映射到机器人会忽略物理规律，通常导致操作失准。PhysWorld通过耦合视频生成与物理世界重建来解决这一局限。给定单张图像和任务指令，本方法既能生成任务条件视频，又能从视频中重建底层物理世界，同时通过基于对象的残差强化学习与物理世界模型，将生成的视频运动转化为物理精确的动作。这种协同作用将隐性视觉指导转化为可物理执行的机器人轨迹，无需真实机器人数据采集即可实现零样本可泛化的机器人操作。在多样化现实任务上的实验表明，与现有方法相比，PhysWorld显著提升了操作精度。详情请访问项目网页：https://pointscoder.github.io/PhysWorld_Web/</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.07416">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.07416">arXiv</a></p>
<hr />
<h3>11. 基于追溯循环机制的预训练语言模型深度思维教学方法</h3>
<p><strong>原文标题：</strong> Teaching Pretrained Language Models to Think Deeper with Retrofitted
  Recurrence</p>
<p><strong>摘要：</strong>
深度循环语言模型的最新进展表明，循环机制能够将训练时的计算量与参数规模同测试时的计算需求分离开来。本研究探索如何将现有非循环预训练语言模型转化为深度循环模型。我们发现，通过采用渐进式循环课程学习策略，在训练过程中逐步增加模型有效深度，可以在保持性能的同时降低总体计算成本。在数学领域的实验中，相较于直接对原始非循环语言模型进行后训练，将预训练模型转化为循环结构能在相同计算预算下获得更优的性能表现。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.07384">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.07384">arXiv</a></p>
<hr />
<h3>12. NURBGen：基于大语言模型驱动的NURBS建模实现高保真文本到CAD生成</h3>
<p><strong>原文标题：</strong> NURBGen: High-Fidelity Text-to-CAD Generation through LLM-Driven NURBS
  Modeling</p>
<p><strong>摘要：</strong>
从自然语言生成可编辑的3D CAD模型仍面临挑战，现有文本到CAD系统要么生成网格模型，要么依赖稀缺的设计历史数据。我们提出NURBGen——首个通过非均匀有理B样条（NURBS）直接根据文本生成高保真3D CAD模型的框架。为实现这一目标，我们微调大语言模型，将自由格式文本转换为包含NURBS曲面参数（即控制点、节点向量、阶数和有理权重）的JSON表示，这些参数可通过Python直接转换为BRep格式。我们进一步提出混合表示法，将未修剪NURBS与解析图元相结合，以更稳健地处理修剪曲面和退化区域，同时降低标记复杂度。此外，我们推出partABC数据集——这是ABC数据集的精选子集，包含独立CAD组件，并通过自动化标注流程配以详细描述。专家评估证实，NURBGen在多样化提示词上表现优异，在几何保真度和尺寸精度方面超越现有方法。代码与数据集将公开发布。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.06194">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.06194">arXiv</a></p>
<hr />
<h3>13. 长链思维推理：大规模提炼组合式视觉推理路径</h3>
<p><strong>原文标题：</strong> Long Grounded Thoughts: Distilling Compositional Visual Reasoning Chains at Scale</p>
<p><strong>摘要：</strong>
当前多模态推理的进展主要依赖于未公开数据集和专有数据合成方案，这引发了对如何系统化构建大规模视觉中心推理数据集的开放性问题，尤其是针对超越视觉数学范畴的任务。本研究提出新型推理数据生成框架，涵盖多样化技能与复杂度层级，生成超过100万道高质量合成视觉中心问题。该数据集同时包含偏好数据与指令提示，支持离线和在线强化学习。我们的合成框架分两阶段实施：（1）规模化扩展；（2）复杂度提升。通过融合视觉语言模型与推理大语言模型的双阶段处理流程，生成适用于视觉语言模型的思维链轨迹，捕捉前沿推理模型中丰富的多样化认知行为。值得注意的是，基于我们数据微调的Qwen2.5-VL-7B模型在所有评估的视觉中心基准测试中均超越开源基线模型，甚至在V* Bench、CV-Bench和MMStar-V基准上优于MiMo-VL-7B-RL等强效闭源模型。最令人惊讶的是，尽管完全专注于视觉领域，我们的数据在纯文本推理（MMLU-Pro）和音频推理（MMAU）任务中展现出正向迁移能力。同样，在未包含视频或具身视觉数据的情况下，我们在单证据具身问答基准（NiEH）上观察到显著性能提升。最后，我们利用该数据系统分析视觉语言模型的后训练流程，实证研究表明：（i）基于含非线性推理轨迹的高质量数据进行监督微调是在线强化学习生效的关键；（ii）分阶段离线强化学习可匹配在线强化学习性能，同时降低计算需求；（iii）对高质量数据实施精细监督微调能显著提升跨领域、跨模态的迁移性能。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.05705">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.05705">arXiv</a></p>
<hr />
<h3>14. RLoop：基于迭代策略初始化的强化学习自改进框架</h3>
<p><strong>原文标题：</strong> RLoop: An Self-Improving Framework for Reinforcement Learning with
  Iterative Policy Initialization</p>
<p><strong>摘要：</strong>
尽管可验证奖励的强化学习（RLVR）在训练大型推理模型方面具有强大能力，但其训练动态存在一个关键挑战：RL过拟合，即模型获得训练奖励却丧失泛化能力。我们的分析表明，这一现象由策略过度特化及训练过程中产生的多样化解决方案的灾难性遗忘所驱动。标准优化方法会丢弃这些宝贵的跨步骤策略多样性。为解决此问题，我们提出RLoop——一个基于迭代策略初始化的自改进框架。RLoop将标准训练过程转化为良性循环：首先使用RL从给定策略出发探索解空间，随后筛选成功轨迹构建专家数据集。通过拒绝采样微调（RFT）利用该数据集优化初始策略，为下一轮迭代创建更优的起点。这种通过迭代重初始化的探索与利用循环，有效将瞬态策略变异转化为稳健的性能提升。实验表明，RLoop能有效缓解遗忘现象并显著提升泛化能力，相较于原始RL方法，平均准确率提高9%，pass@32指标提升超过15%。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.04285">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.04285">arXiv</a></p>
<hr />
<h3>15. DigiData：通用移动控制智能体的训练与评估框架</h3>
<p><strong>原文标题：</strong> DigiData: Training and Evaluating General-Purpose Mobile Control Agents</p>
<p><strong>摘要：</strong>
具备用户界面控制能力的人工智能体有望彻底改变人类与数字设备的交互模式。为加速这一变革进程，两大基础要素不可或缺：一是能够支持智能体实现复杂且符合人类需求目标的高质量数据集，二是可供研究与实践人员快速提升智能体性能的稳健评估方法。本文提出的DigiData是一个专为移动控制智能体训练设计的大规模、高质量、多模态数据集。与现有基于非结构化交互目标的数据集不同，DigiData通过系统性探索应用程序功能精心构建，具有更丰富的多样性特征和更高的目标复杂度。同时，我们推出DigiData-Bench评估基准，用于在真实世界复杂任务场景下检验移动控制智能体性能。研究表明，当前广泛采用的分步准确率指标难以可靠评估移动控制智能体，为此我们提出动态评估协议与人工智能驱动的评估方法作为更严谨的替代方案。本研究的成果将有力推动移动控制智能体发展，为构建更直观高效的人机交互模式奠定基础。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.07413">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.07413">arXiv</a></p>
<hr />
<h3>16. MPJudge：音乐诱发绘画的感知评估研究</h3>
<p><strong>原文标题：</strong> MPJudge: Towards Perceptual Assessment of Music-Induced Paintings</p>
<p><strong>摘要：</strong>
音乐诱发绘画是一种独特的艺术实践，指在音乐影响下创作视觉艺术作品。评估画作是否忠实反映其灵感来源的音乐，构成了一项具有挑战性的感知评估任务。现有方法主要依赖情感识别模型来评估音乐与绘画的相似性，但这类模型会引入显著噪声且忽略了情感之外的更广泛感知线索。为解决这些局限，我们提出了一种新颖的音乐诱发绘画评估框架，直接建模音乐与视觉艺术之间的感知连贯性。我们引入了MPD数据集——首个由领域专家基于感知连贯性标注的大规模音乐-绘画配对数据集。为更好处理模糊案例，我们进一步收集了成对偏好标注。基于该数据集，我们提出了MPJudge模型，通过基于调制的融合机制将音乐特征整合到视觉编码器中。为有效学习模糊案例，我们采用直接偏好优化方法进行训练。大量实验表明，我们的方法优于现有方法。定性结果进一步证明，我们的模型能更准确地识别绘画中与音乐相关的区域。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.07137">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.07137">arXiv</a></p>
<hr />
<h3>17. Llama-Embed-Nemotron-8B：面向多语言与跨语言任务的通用文本嵌入模型</h3>
<p><strong>原文标题：</strong> Llama-Embed-Nemotron-8B: A Universal Text Embedding Model for
  Multilingual and Cross-Lingual Tasks</p>
<p><strong>摘要：</strong>
本文推出llama-embed-nemotron-8b——一个开放权重的文本嵌入模型，截至2025年10月21日，该模型在多语言海量文本嵌入基准（MMTEB）排行榜上实现了最先进的性能。尽管现有模型展现出强劲表现，但其训练数据与方法论往往未完全公开。我们通过开发完全开源的模型来解决这一问题，不仅公开其权重参数与详尽的消融实验，还计划发布经系统整理的训练数据集。该模型在所有主流嵌入任务（包括检索、分类与语义文本相似度STS）中均表现出卓越性能，并在低资源语言与跨语言设置等具有挑战性的多语言场景中表现尤为突出。这一突破性性能得益于我们创新的数据组合策略：使用1,610万组查询-文档对，其中770万样本来自公共数据集，840万样本通过各类开放权重大语言模型合成生成。我们的核心贡献包括对关键设计选择的系统化消融研究：对比不同对比损失函数的实现方案、评估合成数据生成策略的效能，以及分析模型融合技术的影响。llama-embed-nemotron-8b作为指令感知模型，支持用户自定义指令以增强特定场景下的性能。这种顶尖的性能表现、广泛的适用性与用户可定制的灵活性相结合，使其成为通用文本嵌入解决方案的理想选择。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.07025">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.07025">arXiv</a></p>
<hr />
<h3>18. FLEX：基于经验前向学习的智能体持续进化框架</h3>
<p><strong>原文标题：</strong> FLEX: Continuous Agent Evolution via Forward Learning from Experience</p>
<p><strong>摘要：</strong>
基于大语言模型的自主智能体在推理与问题解决领域引发革命性突破，但其在训练完成后即处于静态，无法像智能生物那样在部署过程中通过经验积累实现持续成长。本文提出经验前向学习框架（FLEX），该无需梯度的学习范式使大语言模型智能体能够通过积累的经验实现持续进化。具体而言，FLEX通过持续反思与环境交互过程中的成功与失败，构建结构化经验库，从而实现可扩展、可传承的智能体进化。在数学推理、化学逆合成及蛋白质适应性预测任务中，FLEX分别取得显著提升（AIME25数据集提升23%，USPTO50k数据集提升10%，ProteinGym数据集提升14%）。研究进一步揭示了经验增长的显著缩放规律及跨智能体经验传承现象，标志着向可扩展、可传承的持续智能体进化迈出关键一步。项目页面：https://flex-gensi-thuair.github.io。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.06449">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.06449">arXiv</a></p>
<hr />
<h3>19. 引领视觉-语言-动作模型未来发展的十大开放挑战</h3>
<p><strong>原文标题：</strong> 10 Open Challenges Steering the Future of Vision-Language-Action Models</p>
<p><strong>摘要：</strong>
凭借其遵循自然语言指令的能力，视觉-语言-动作模型在具身人工智能领域日益普及，这得益于其前身——大语言模型和视觉语言模型取得的广泛成功。本文系统论述了VLA模型发展过程中的十大关键挑战：多模态融合、推理能力、数据构建、评估体系、跨机器人动作泛化、模型效率、全身协调、安全机制、智能体架构以及人机协同。同时，我们深入探讨了为实现这些突破性进展而涌现的研究趋势，包括空间理解、世界动态建模、后训练优化以及数据合成等关键技术路径。通过这些讨论，我们希望引导研究界关注那些可能加速VLA模型获得更广泛认可的关键研究方向。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.05936">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.05936">arXiv</a></p>
<hr />
<h3>20. Ariadne：一个用于探索与拓展视觉语言模型推理边界的可控框架</h3>
<p><strong>原文标题：</strong> Ariadne: A Controllable Framework for Probing and Extending VLM
  Reasoning Boundaries</p>
<p><strong>摘要：</strong>
尽管经过强化学习（RL）后训练的视觉语言模型（VLM）展现出令人印象深刻的通用推理能力，但其评估通常局限于语言主导型任务（如数学推理）。这引发了一个关键问题：对于基础VLM最初无法解决的视觉中心型空间任务，RL后训练是否真能拓展其固有能力边界？为探究此问题，我们提出Ariadne框架——通过可精确控制任务难度（如路径长度、转弯次数）的合成迷宫系统进行多步空间推理研究。我们利用这一可控环境，采用带验证奖励的强化学习（RLVR）在难度感知课程中训练VLM。令人惊讶的是，经过RLVR后训练的VLM在基础模型准确率为0%的问题集上实现了超过50%的准确率，证明我们的方法拓展了模型的初始能力边界。为评估实际应用潜力，我们在实践基准测试中评估了分布外（OOD）泛化能力。尽管仅使用合成迷宫样本进行训练，Ariadne在MapBench（如博物馆导航）和ReasonMap（地铁换乘任务）上分别实现了16%和24%的平均零样本性能提升。这些结果证实我们的方法不仅拓宽了模型的基础能力边界，还增强了其在现实世界空间推理任务中的泛化能力。我们承认由于预训练数据的不透明性，本研究仅限于后训练阶段的研究，期待我们的工作能推动面向专业能力拓展的对齐方法进一步探索。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.00710">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.00710">arXiv</a></p>
<hr />
<h3>21. DIMO：面向任意物体的多样化三维运动生成方法</h3>
<p><strong>原文标题：</strong> DIMO: Diverse 3D Motion Generation for Arbitrary Objects</p>
<p><strong>摘要：</strong>
本文提出DIMO生成式方法，能够基于单张图像为任意物体生成多样化的三维运动。本方法的核心思想是利用训练成熟的视频模型中蕴含的丰富先验知识，提取通用运动模式并将其嵌入至共享的低维潜空间。具体而言，我们首先生成具有多样化运动的同一物体的多段视频，随后将每种运动嵌入至潜向量，并通过训练共享运动解码器来学习以结构化紧凑运动表征（即神经关键点轨迹）所描述的运动分布。这些规范化的三维高斯模型随后由关键点驱动并进行融合，以建模几何形态与外观特征。在基于已学习潜空间进行推理时，我们可通过单次前向传播实时采样多样化三维运动，并支持三维运动插值与语言引导运动生成等多项创新应用。项目页面详见：https://linzhanm.github.io/dimo。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.07409">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.07409">arXiv</a></p>
<hr />
<h3>22. RLVE：基于可验证自适应环境的大规模语言模型强化学习方法</h3>
<p><strong>原文标题：</strong> RLVE: Scaling Up Reinforcement Learning for Language Models with
  Adaptive Verifiable Environments</p>
<p><strong>摘要：</strong>
本文提出基于可验证自适应环境的强化学习方法（RLVE），该方法通过可验证环境程序化生成问题并提供算法可验证的奖励机制，以实现语言模型强化学习的规模化扩展。RLVE使每个可验证环境能够根据策略模型在训练过程中的能力水平，动态调整问题难度分布。相比之下，静态数据分布在问题过于简单或困难时往往会导致学习信号消失。为实施RLVE，我们开发了RLVE-Gym——一个通过人工环境工程精心构建的、包含400个可验证环境的大规模训练套件。基于RLVE-Gym的实验表明，环境扩展（即增加训练环境集合）能持续提升模型的泛化推理能力。使用RLVE-Gym全部400个环境进行联合训练时，RLVE在六大推理基准测试中实现了3.37%的平均绝对提升（以当前最强的15亿参数推理语言模型为基线）。相比之下，延续该模型原有强化训练方案仅获得0.49%的平均提升，且消耗了3倍以上的算力。我们已公开相关代码。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.07317">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.07317">arXiv</a></p>
<hr />
<h3>23. 大语言模型具备情感感知能力吗？基于提示工程、检索机制与课程学习的情绪识别研究</h3>
<p><strong>原文标题：</strong> Do LLMs Feel? Teaching Emotion Recognition with Prompts, Retrieval, and
  Curriculum Learning</p>
<p><strong>摘要：</strong>
对话情绪识别（ERC）是理解人类情感并实现自然人机交互的关键任务。尽管大语言模型（LLMs）近期在该领域展现出巨大潜力，但其捕捉显性情绪与隐性情绪内在联系的能力仍存在局限。我们提出名为PRC-Emo的创新ERC训练框架，该框架融合提示工程、示例检索与课程学习三大模块，旨在探究LLMs能否有效感知对话情境中的情绪状态。具体而言，我们基于显性与隐性情绪线索设计情感敏感型提示模板，以更精准引导模型理解说话者的心理状态。构建了ERC领域首个专用示例检索库，包含来自广泛使用数据集的训练样本，以及由LLMs生成并经人工核验的高质量对话实例。此外，我们将课程学习策略引入LoRA微调过程，通过量化同一说话者与不同说话者话语间的加权情绪转移值来划分对话样本难度等级，并依此构建由易到难的训练序列。在IEMOCAP和MELD两个基准数据集上的实验结果表明，本方法取得了最新的最优性能，验证了所提框架在增强基于LLM的情绪理解能力方面的有效性与泛化性。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.07061">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.07061">arXiv</a></p>
<hr />
<h3>24. SWE-fficiency：语言模型能否在真实工作负载下优化现实代码仓库？</h3>
<p><strong>原文标题：</strong> SWE-fficiency: Can Language Models Optimize Real-World Repositories on
  Real Workloads?</p>
<p><strong>摘要：</strong>
优化大型软件仓库的性能需要代码推理与软件工程（SWE）领域的专业知识，在保证程序正确性的同时降低运行耗时。然而现有基准测试多聚焦于“修复目标”而非“修复方法”。本文提出SWE-fficiency——首个面向真实工作负载的仓库级性能优化评估基准。该测试套件涵盖九个广泛使用的数据科学、机器学习及高性能计算仓库（如numpy、pandas、scipy）中的498项任务：给定完整代码库与低速工作负载，智能体需解析代码语义、定位性能瓶颈及相关测试，并生成能通过同等单元测试且加速效果达到或超越专家水平的补丁。为实现这种“如何修复”的评估，我们通过自动化流水线从GitHub拉取请求中采集性能优化编辑记录，结合关键词过滤、静态分析、覆盖率工具与执行验证，既确认专家加速基准又识别相关仓库单元测试。对前沿智能体的实证评估显示其表现显著欠佳：平均加速效果仅达专家水平的0.15倍。智能体在定位优化机会、跨函数执行推理及保持编辑正确性方面存在明显不足。我们公开此基准测试及配套数据流水线，以推动自动化性能工程与长周期软件推理的研究进展。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.06090">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.06090">arXiv</a></p>
<hr />
<h3>25. Diffusion-SDPO：扩散模型的带保护直接偏好优化方法</h3>
<p><strong>原文标题：</strong> Diffusion-SDPO: Safeguarded Direct Preference Optimization for Diffusion
  Models</p>
<p><strong>摘要：</strong>
文本到图像扩散模型虽能生成高质量图像，但其与人类偏好的对齐仍具挑战性。本文重新审视基于扩散的直接偏好优化方法，发现一个关键缺陷：扩大偏好间隔未必能提升生成质量。具体而言，标准Diffusion-DPO目标函数可能同时增加优胜分支与劣汰分支的重建误差。这会导致非优选输出的退化程度加剧，即使偏好间隔扩大，优选分支也会受到不利影响。为此，我们提出Diffusion-SDPO方法，该保护性更新规则通过自适应缩放劣汰分支梯度与其优胜分支梯度的对齐程度来保持优胜分支质量。一阶分析推导出的闭式缩放系数可确保在每步优化中优选输出的误差保持非递增。本方法具有简洁性、模型无关性，可广泛兼容现有DPO式对齐框架，且仅增加边际计算开销。在标准文本到图像基准测试中，Diffusion-SDPO在自动化偏好评估、美学质量和提示对齐指标上均持续优于现有偏好学习基线。代码公开于：https://github.com/AIDC-AI/Diffusion-SDPO。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.03317">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.03317">arXiv</a></p>
<hr />
<h3>26. 千词成图：基于结构化描述增强文本到图像生成技术</h3>
<p><strong>原文标题：</strong> Generating an Image From 1,000 Words: Enhancing Text-to-Image With
  Structured Captions</p>
<p><strong>摘要：</strong>
文本到图像模型已从随意的创意工具迅速发展为专业级系统，实现了前所未有的图像质量与真实感。然而，大多数模型被训练用于将简短提示映射为精细图像，导致稀疏文本输入与丰富视觉输出之间产生鸿沟。这种不匹配降低了可控性——模型常会随意填补缺失细节，偏向普通用户偏好，限制了专业应用的精确度。为解决这一局限，我们首次基于长结构化描述训练开源文本到图像模型，每个训练样本均通过统一细粒度属性集进行标注。该设计最大化表达覆盖范围，并实现视觉要素的解耦控制。为高效处理长文本，我们提出DimFusion融合机制，在不增加标记长度的前提下整合轻量级大语言模型的中间标记。同时引入文本瓶颈重建评估协议：通过评估真实图像在标注-生成循环中的重建质量，该协议可直接衡量可控性与表达能力，即使在现有评估方法失效的超长文本场景下仍能适用。最终，我们通过训练大规模模型FIBO验证研究成果，在开源模型中实现了最先进的提示对齐效果。模型权重已公开发布于https://huggingface.co/briaai/FIBO</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.06876">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.06876">arXiv</a></p>
<hr />
<h3>27. VADER：基于关系感知大语言模型的因果视频异常理解框架</h3>
<p><strong>原文标题：</strong> VADER: Towards Causal Video Anomaly Understanding with Relation-Aware
  Large Language Models</p>
<p><strong>摘要：</strong>
视频异常理解旨在对视频中的异常事件提供精细化阐释与语义级认知，突破传统方法仅聚焦异常检测与定位的局限。然而现有研究往往忽视物体间深层的因果关系与动态交互，而这些要素对理解异常行为至关重要。本文提出VADER——一种基于大语言模型的视频异常理解框架，通过融合关键帧物体关系特征与视觉线索来增强视频异常认知能力。具体而言，VADER首先通过异常评分器计算逐帧异常分值，继而采用情境感知采样策略捕捉异常事件的因果上下文。通过关系特征提取器与对比关系编码器的协同作用，系统建模动态物体交互并生成紧凑的关系表征以支持下游推理。这些视觉与关系线索与大语言模型集成后，可生成具有因果依据的细粒度描述，并支持鲁棒的异常相关问答。在多个真实场景视频异常理解基准测试上的实验表明，VADER在异常描述、解释及因果推理任务中均取得优异性能，推动了可解释视频异常分析研究的前沿发展。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.07299">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.07299">arXiv</a></p>
<hr />
<h3>28. Omni-AVSR：基于大语言模型构建统一多模态语音识别系统</h3>
<p><strong>原文标题：</strong> Omni-AVSR: Towards Unified Multimodal Speech Recognition with Large
  Language Models</p>
<p><strong>摘要：</strong>
大语言模型近期在听觉语音识别、视觉语音识别与音视频语音识别等多模态语音识别任务中取得显著成果。然而现有基于大语言模型的方法通常独立处理各项任务，需训练独立模型导致计算与部署资源消耗增加，且未能充分利用跨任务协同潜力。这些方法还依赖固定速率的分词压缩机制，限制了精度与效率平衡的灵活性。这些局限凸显了构建统一框架的必要性——既能支持多模态语音识别任务，又可实现弹性推理。为此，我们提出Omni-AVSR这一统一音视频大语言模型，通过高效多粒度训练与参数有效性自适应相结合的方法实现突破。具体而言，我们采用套娃表示学习范式实现多粒度音视频数据的高效训练，显著降低固有训练资源消耗。此外，我们探索三种基于LoRA的骨干网络自适应策略，在共享性与任务特异性之间实现最优平衡。在LRS2和LRS3数据集上的实验表明，Omni-AVSR仅需训练单一模型即可达到或超越现有最优基准模型精度，同时大幅降低训练与部署资源消耗。该模型在噪声环境下仍保持稳健性能，我们通过分析模型规模扩展规律，为性能与效率的权衡提供了重要见解。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.07253">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.07253">arXiv</a></p>
<hr />
<h3>29. LUT-LLM：基于FPGA内存计算的高效大语言模型推理方案</h3>
<p><strong>原文标题：</strong> LUT-LLM: Efficient Large Language Model Inference with Memory-based
  Computations on FPGAs</p>
<p><strong>摘要：</strong>
大语言模型的快速发展推动了众多应用场景的进步，然而高效的单一批次推理对于设备端智能仍然至关重要。尽管FPGA具备细粒度数据控制和高能效的特性，但近期GPU优化已缩小了其优势，尤其在基于算术运算的场景下。为突破此限制，我们利用FPGA丰富的片上存储资源，通过查表操作将LLM推理从算术计算转向内存计算。本文提出LUT-LLM——首个通过向量化内存操作实现十亿参数级LLM推理的FPGA加速器。我们的分析表明激活-权重协同量化是最有效的方案，其技术支撑包括：（1）带宽感知并行质心搜索；（2）高效二维查表机制；（3）最小化数据缓存的时空混合架构。在AMD V80 FPGA平台上对定制化Qwen 3 1.7B模型的实测表明，LUT-LLM相较AMD MI210实现延迟降低1.66倍，相比NVIDIA A100能效提升1.72倍，并可扩展至320亿参数模型，较A100实现2.16倍的能效增益。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.06174">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.06174">arXiv</a></p>
<hr />
<h3>30. 强化学习提升大语言模型对层次化知识的遍历能力</h3>
<p><strong>原文标题：</strong> Reinforcement Learning Improves Traversal of Hierarchical Knowledge in LLMs</p>
<p><strong>摘要：</strong>
传统观点认为强化学习（RL）虽能提升语言模型的推理与泛化能力，却会削弱其记忆知识。我们通过实验发现，在纯粹的知识召回任务中——尤其是需要遍历层次化结构化知识（如医疗编码）的任务——经过RL增强的模型持续优于基础模型及监督微调（SFT）模型，从而对这一观点提出挑战。我们推测这种提升并非源于新获取的数据，而是源于模型在参数空间内导航和检索既有知识层次结构的流程性技能得到增强。为验证该假设，我们证明通过结构化提示显式引导SFT模型进行层次遍历后，性能差距可大幅缩小（在MedConceptsQA数据集上，DeepSeek-V3/R1模型的差距从24个百分点降至7个百分点）。进一步研究发现，虽然提示策略能提升最终答案准确率，但RL增强模型在深度检索任务中仍保持更优的正确流程路径召回能力。最后，我们通过分层内部激活分析发现：尽管事实性表征（如“编码57.95指代尿路感染”的激活状态）在SFT与RL模型间保持较高的余弦相似度，但查询表征（如“编码57.95是什么”）却呈现显著差异，这表明RL主要改变的是模型遍历知识的方式，而非知识表征本身。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.05933">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.05933">arXiv</a></p>
<hr />
<h2>🔍 关键词云图</h2>
<p><img alt="关键词云图" src="../images/keywords_wordcloud.png" /></p>
<h2>📈 近期论文趋势</h2>
<p><img alt="论文趋势" src="../images/daily_papers.png" /></p>
<h2>🎙️ 语音播报</h2>
<ul>
<li><a href="../audio/2025-11-11_daily_papers.mp3">收听今日论文解读</a></li>
</ul>
<h2>📱 订阅渠道</h2>
<ul>
<li>GitHub: <a href="https://github.com/2404589803/hf-daily-paper-newsletter-chinese">hf-daily-paper-newsletter-chinese</a></li>
</ul>
    </div>
</body>
</html>