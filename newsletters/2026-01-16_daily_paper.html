<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Hugging Face 论文日报 - 2026-01-16</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', sans-serif;
            line-height: 1.6;
            color: #333;
            background-color: #f5f5f5;
            padding: 20px;
        }
        
        .container {
            max-width: 900px;
            margin: 0 auto;
            background-color: white;
            padding: 40px;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }
        
        h1 {
            color: #2c3e50;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
            margin-bottom: 30px;
            font-size: 28px;
        }
        
        h1 img {
            vertical-align: middle;
            margin-right: 10px;
        }
        
        h2 {
            color: #34495e;
            margin-top: 40px;
            margin-bottom: 20px;
            font-size: 24px;
            border-left: 4px solid #3498db;
            padding-left: 15px;
        }
        
        h3 {
            color: #2c3e50;
            margin-top: 30px;
            margin-bottom: 15px;
            font-size: 20px;
        }
        
        p {
            margin-bottom: 15px;
            text-align: justify;
        }
        
        ul {
            margin-left: 20px;
            margin-bottom: 20px;
        }
        
        li {
            margin-bottom: 8px;
        }
        
        a {
            color: #3498db;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        strong {
            color: #2c3e50;
            font-weight: 600;
        }
        
        hr {
            border: none;
            border-top: 1px solid #e0e0e0;
            margin: 30px 0;
        }
        
        /* 关键修复:限制图片宽度 */
        img {
            max-width: 100%;
            height: auto;
            display: block;
            margin: 20px auto;
            border-radius: 4px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        
        /* 确保图片容器也有宽度限制 */
        p img {
            max-width: 100%;
        }
        
        /* 论文详情区域样式 */
        .paper-section {
            background-color: #f9f9f9;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 20px;
        }
        
        /* 统计信息样式 */
        .stats {
            background-color: #e8f4f8;
            padding: 15px;
            border-radius: 6px;
            margin-bottom: 20px;
        }
        
        /* 响应式设计 */
        @media (max-width: 768px) {
            .container {
                padding: 20px;
            }
            
            h1 {
                font-size: 24px;
            }
            
            h2 {
                font-size: 20px;
            }
            
            h3 {
                font-size: 18px;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <h1><img src="https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo.png" width="30"/> Hugging Face 2026-01-16 论文日报</h1>
<h2>📊 今日论文统计</h2>
<ul>
<li>总论文数：38</li>
<li>热门领域：Vision, GPT, LLM, Transformer, RL</li>
</ul>
<h2>📝 论文详情</h2>
<h3>1. 基于视觉-语言推理的城市社会语义分割</h3>
<p><strong>原文标题：</strong> Urban Socio-Semantic Segmentation with Vision-Language Reasoning</p>
<p><strong>摘要：</strong>
作为人类活动的枢纽，城市地表包含丰富的语义实体。从卫星图像中分割这些多样化的实体对于一系列下游应用至关重要。当前先进的语义分割模型能够可靠地分割由物理属性定义的实体（如建筑物、水体），但在处理社会属性定义的类别（如学校、公园）时仍面临挑战。本研究通过视觉-语言模型的推理能力实现社会语义分割。为此，我们构建了名为SocioSeg的城市社会语义分割数据集，该数据集包含卫星影像、数字地图以及按层级结构组织的社会语义实体像素级标注。此外，我们提出了一种新颖的视觉-语言推理框架SocioReasoner，该框架通过跨模态识别与多阶段推理模拟人类识别与标注社会语义实体的认知过程。我们采用强化学习优化这一不可微分的推理流程，从而激发视觉-语言模型的深层推理能力。实验表明，该方法在性能上超越现有先进模型，并展现出强大的零样本泛化能力。数据集与代码已公开于https://github.com/AMAP-ML/SocioReasoner。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.10477">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.10477">arXiv</a></p>
<hr />
<h3>2. STEP3-VL-10B技术报告</h3>
<p><strong>原文标题：</strong> STEP3-VL-10B Technical Report</p>
<p><strong>摘要：</strong>
本文介绍STEP3-VL-10B——一个旨在重新定义紧凑效率与前沿多模态智能之间权衡的轻量级开源基础模型。该模型通过两项战略转型实现：首先，采用基于1.2万亿多模态标记的统一全解冻预训练策略，将语言对齐的感知编码器与Qwen3-8B解码器融合，建立内在的视觉-语言协同机制；其次，构建包含超千轮强化学习的规模化后训练流程。我们创新性地引入并行协调推理（PaCoRe）机制扩展测试时计算资源，将算力动态分配至可扩展的感知推理过程，实现对多样化视觉假设的探索与综合。实验表明，尽管仅具有100亿参数的紧凑架构，STEP3-VL-10B在多项基准测试中媲美或超越规模达其10-20倍的模型（如GLM-4.6V-106B、Qwen3-VL-235B），并与Gemini 2.5 Pro、Seed-1.5-VL等顶尖专有模型旗鼓相当。该模型取得业界领先性能：在MMBench达到92.2%，MMMU获得80.11%，同时在复杂推理任务中表现卓越，AIME2025得分94.43%，MathVision得分75.95%。我们完整开源模型套件，为学界提供强大、高效且可复现的基准系统。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.09668">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.09668">arXiv</a></p>
<hr />
<h3>3. 奖励稀缺性：面向大语言模型创造性问题求解的独特性感知强化学习</h3>
<p><strong>原文标题：</strong> Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in LLMs</p>
<p><strong>摘要：</strong>
强化学习已成为大语言模型后训练的核心范式，尤其在复杂推理任务中，但该方法常受探索坍缩问题困扰：策略过早集中于少数主导推理模式，虽能提升单次通过率，却限制了轨迹层面的多样性及k次通过率的增益。我们认为这一缺陷源于对局部词元行为的常规化约束，而非对解决方案集合多样性的关注。为此，我们提出独特性感知强化学习方法，通过轨迹层目标函数显式奖励那些采用罕见高层策略的正确解决方案。该方法基于大语言模型的评判器，将同一问题的求解轨迹按其高层策略进行聚类（忽略表面差异），并依据聚类规模对策略优势进行反向加权。由此，正确但新颖的策略将比冗余策略获得更高奖励。在数学、物理和医学推理基准测试中，本方法在大规模采样预算下持续提升k次通过率，在保持单次通过率的同时提高k次通过率曲线下面积，并能维持探索过程，系统性地发掘更多样化的求解策略。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.08763">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.08763">arXiv</a></p>
<hr />
<h3>4. 面向推理任务的协作式多智能体测试时强化学习</h3>
<p><strong>原文标题：</strong> Collaborative Multi-Agent Test-Time Reinforcement Learning for Reasoning</p>
<p><strong>摘要：</strong>
多智能体系统已发展成为众多实际应用中由大语言模型驱动的实用协作框架，其通过多样性与交叉验证获得鲁棒性。然而，多智能体强化学习训练过程资源消耗大且不稳定：智能体间的协同适应会引发非平稳性问题，且奖励信号通常稀疏且具有高方差特性。为此，我们提出多智能体测试时强化学习框架，该框架在推理阶段将结构化文本经验注入多智能体协商过程。该框架构建由专业智能体组成的多专家团队进行多轮讨论，检索并整合测试时经验，最终通过共识机制形成决策。我们还研究了用于构建轮次级经验池的信用分配机制，并将其重新注入对话流程。在医学、数学与教育领域的多个挑战性基准测试中，该框架相较于多智能体基线模型平均准确率提升3.67%，较可比单智能体基线提升8.67%。消融实验检验了不同信用分配方案，并详细比较了其对训练结果的影响。该框架为无需调参即可实现分布偏移鲁棒的多智能体推理提供了一条稳定、高效且可靠的路径。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.09667">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.09667">arXiv</a></p>
<hr />
<h3>5. VIBE：基于视觉指令的编辑器</h3>
<p><strong>原文标题：</strong> VIBE: Visual Instruction Based Editor</p>
<p><strong>摘要：</strong>
基于指令的图像编辑是生成式人工智能领域发展最快的方向之一。过去一年中，该领域已达到新高度，数十个开源模型与高性能商业系统相继发布。然而，目前仅有有限数量的开源方法能够实现实际应用级别的质量。此外，作为此类流程主流选择的扩散模型主干网络通常体量庞大、计算成本高昂，广泛使用的变体通常包含60亿至200亿参数，这对许多部署和研究场景构成挑战。本文提出一种紧凑型、高吞吐的基于指令图像编辑流程，该流程采用现代化的20亿参数Qwen3-VL模型指导编辑过程，并利用16亿参数扩散模型Sana1.5进行图像生成。我们在架构设计、数据处理、训练配置和评估目标等方面均以低成本推理与严格源图像一致性为核心考量，同时在此规模可行的主要编辑类别中保持高质量输出。通过在ImgEdit和GEdit基准测试中的评估，所提方法的性能达到或显著超越了参数规模数倍、推理成本更高的基线模型，在需要保持输入图像特性的编辑任务（如属性调整、对象移除、背景编辑和定向替换）上表现尤为突出。该模型可在24GB GPU内存内运行，在未进行额外推理优化或蒸馏的情况下，于NVIDIA H100显卡上以BF16精度生成最高2K分辨率的编辑图像，耗时约4秒。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.02242">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.02242">arXiv</a></p>
<hr />
<h3>6. 超越静态工具：面向科学推理的测试时工具演化</h3>
<p><strong>原文标题：</strong> Beyond Static Tools: Test-Time Tool Evolution for Scientific Reasoning</p>
<p><strong>摘要：</strong>
科学人工智能的核心挑战不仅在于推理本身，更在于在开放的科学世界中创建计算方法的能力。现有基于大语言模型的智能体依赖于静态的、预定义的工具库，这种范式在工具稀缺、异构且本质上不完整的科学领域存在根本性缺陷。本文提出测试时工具演化这一新范式，使智能体能够在推理过程中合成、验证并演化可执行工具。通过将工具从固定资源转化为问题驱动的产物，该范式克服了静态工具库的僵化性与长尾局限性。为支持严谨评估，我们构建了SciEvo基准数据集，包含1,590项科学推理任务及支撑其实现的925个自动演化工具。大量实验表明，该范式在准确率与工具效率方面均达到最先进水平，同时实现了计算工具的有效跨领域适配。代码与基准数据集已发布于https://github.com/lujiaxuan0520/Test-Time-Tool-Evol。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.07641">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.07641">arXiv</a></p>
<hr />
<h3>7. 丹青：一个前沿的大规模中文视觉-语言预训练数据集</h3>
<p><strong>原文标题：</strong> DanQing: An Up-to-Date Large-Scale Chinese Vision-Language Pre-training Dataset</p>
<p><strong>摘要：</strong>
视觉-语言预训练模型通过对比预训练从大规模图文对中学习，在多种下游任务中展现出强大性能。大规模英文图文数据集（如COYO-700M和LAION-400M）的发布，使得CLIP、SigLIP等模型在跨模态检索、图像描述生成等任务中得到广泛应用。然而，由于高质量中文图文数据的稀缺，中文视觉-语言预训练的发展明显滞后。为填补这一空白，我们开发了一套构建高质量中文跨模态数据集的完整流程。基于此，我们提出了包含1亿个从Common Crawl收集的图文对的“丹青”数据集。与现有数据集不同，丹青通过更严格的筛选流程进行构建，数据质量显著提升。此外，丹青主要基于2024-2025年的网络数据构建，能使模型更好地捕捉语义演变趋势，从而具备更强的实际应用价值。我们通过持续预训练SigLIP2模型，将丹青与现有数据集进行了对比实验。结果表明，在包括零样本分类、跨模态检索以及基于大语言模型的评估等一系列中文下游任务中，丹青均能取得更优的性能。为促进中文视觉-语言预训练的进一步研究，我们将在知识共享CC-BY 4.0协议下开源丹青数据集。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.10305">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.10305">arXiv</a></p>
<hr />
<h3>8. 迈向超长程自主科学：面向机器学习工程的认知累积框架</h3>
<p><strong>原文标题：</strong> Toward Ultra-Long-Horizon Agentic Science: Cognitive Accumulation for Machine Learning Engineering</p>
<p><strong>摘要：</strong>
人工智能向自主科学的发展目前受限于超长程自主能力挑战——即在跨越数日或数周的实验周期中保持战略连贯性与迭代修正的能力。尽管大语言模型已在短程推理中展现卓越能力，但在现实研究的高维延迟反馈环境中，它们易被执行细节淹没，难以将稀疏反馈整合为连贯的长期指导。本文提出ML-Master 2.0自主智能体，该系统在作为科学发现典型微观范式的超长程机器学习工程任务中实现突破。通过将情境管理重构为认知累积过程，我们引入受计算机系统启发的分层认知缓存架构，该多层级设计实现了经验随时间推移的结构化区分。通过动态将瞬态执行轨迹提炼为稳定知识与跨任务智慧，该架构使智能体能够解耦即时执行与长期实验策略，有效克服静态上下文窗口的扩展限制。在OpenAI MLE-Bench平台24小时预算评估中，ML-Master 2.0获得56.44%的领先奖牌率。研究结果表明，超长程自主能力为人工智能超越人类既有复杂度的自主探索提供了可扩展的蓝图。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.10402">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.10402">arXiv</a></p>
<hr />
<h3>9. CoF-T2I：作为纯视觉推理器的视频模型用于文本到图像生成</h3>
<p><strong>原文标题：</strong> CoF-T2I: Video Models as Pure Visual Reasoners for Text-to-Image Generation</p>
<p><strong>摘要：</strong>
近期视频生成模型揭示了帧间推理链能力的涌现，实现了逐帧视觉推断。凭借这一能力，视频模型已成功应用于多种视觉任务（如迷宫求解、视觉谜题）。然而，由于文本到图像生成过程中缺乏明确定义的视觉推理起点与可解释的中间状态，其在增强文本到图像生成方面的潜力尚未得到充分探索。为弥补这一空白，我们提出CoF-T2I模型，该模型通过渐进式视觉优化将帧间推理链整合至文本到图像生成过程，其中中间帧作为显式推理步骤，最终帧作为输出结果。为构建此类显式生成流程，我们构建了CoF-Evol-Instruct数据集，该数据集包含建模从语义到美学生成过程的帧间推理轨迹。为进一步提升生成质量并避免运动伪影，我们实现了对每帧的独立编码操作。实验表明，CoF-T2I显著优于基础视频模型，并在具有挑战性的基准测试中取得具有竞争力的性能表现——在GenEval上达到0.86分，在Imagine-Bench上达到7.468分。这些结果证明了视频模型在推进高质量文本到图像生成方面的巨大潜力。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.10061">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.10061">arXiv</a></p>
<hr />
<h3>10. 先思后成：基于大语言模型编码器的推理感知文本到图像扩散方法</h3>
<p><strong>原文标题：</strong> Think-Then-Generate: Reasoning-Aware Text-to-Image Diffusion with LLM Encoders</p>
<p><strong>摘要：</strong>
文本到图像扩散模型的最新进展已能根据多样化文本提示生成高质量视觉内容。然而，现有大多数文本到图像扩散模型（即使配备基于大语言模型的文本编码器）仍停留在文本-像素映射阶段——它们仅将大语言模型用作文本编码器，未能利用其内在推理能力来推断文本提示对应的视觉呈现内容。为突破这种字面生成模式，本文提出“先思后成”范式，通过激励基于大语言模型的文本编码器对原始用户提示进行推理与重写，并将重写后的提示状态作为扩散条件。为实现这一目标，我们首先通过轻量级监督微调激活大语言模型编码器的“先思后写”模式，随后通过双重生成式强化策略优化协同优化大语言模型编码器与扩散主干网络，确保对上下文的忠实推理和语义的精确呈现。具体而言，文本编码器通过基于图像的奖励机制强化其世界知识的推断与调用能力，而扩散主干网络则被优化以生成语义一致且视觉连贯的图像。实验表明，该方法在基于推理的图像生成与编辑基准测试中，在事实一致性、语义对齐和视觉真实性方面均取得显著提升，WISE分数达到0.79，与GPT-4表现近乎持平。本研究为构建兼具推理、表达与演示能力的下一代统一模型迈出了重要一步。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.10332">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.10332">arXiv</a></p>
<hr />
<h3>11. Alterbute：图像中对象固有属性的编辑方法</h3>
<p><strong>原文标题：</strong> Alterbute: Editing Intrinsic Attributes of Objects in Images</p>
<p><strong>摘要：</strong>
本文提出Alterbute，一种基于扩散模型的图像中对象固有属性编辑方法。该方法支持修改对象的颜色、纹理、材质甚至形状，同时保持其感知身份与场景上下文。现有方法要么依赖无监督先验（往往难以保持身份一致性），要么采用过度严格的监督机制（限制了有意义的固有属性变化）。我们的方法基于两个核心设计：（一）采用宽松的训练目标，使模型能够根据身份参考图像、描述目标固有属性的文本提示、以及定义外部背景的背景图像和对象掩码，同时修改对象的固有属性与外部属性。在推理阶段，通过复用原始背景和对象掩码来限制外部变化，从而确保仅目标固有属性被修改；（二）引入视觉命名实体——一种细粒度的视觉身份类别（例如“保时捷911卡雷拉”），将具有身份定义特征但固有属性可变的对象进行分组。我们利用视觉语言模型从大规模公共图像数据集中自动提取VNE标签和固有属性描述，实现了可扩展且保持身份一致性的监督训练。实验表明，Alterbute在保持身份一致性的对象固有属性编辑任务上优于现有方法。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.10714">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.10714">arXiv</a></p>
<hr />
<h3>12. MatchTIR：通过二分图匹配实现工具集成推理的细粒度监督</h3>
<p><strong>原文标题：</strong> MatchTIR: Fine-Grained Supervision for Tool-Integrated Reasoning via Bipartite Matching</p>
<p><strong>摘要：</strong>
工具集成推理通过将推理步骤与外部工具调用交错进行，赋能大语言模型处理复杂任务。然而，现有的强化学习方法通常依赖于结果级或轨迹级奖励，对轨迹中的所有步骤赋予统一的优势值。这种粗粒度的信用分配无法区分有效工具调用与冗余或错误调用，尤其在长视野、多轮次的场景中。为此，我们提出MatchTIR框架，该框架通过基于二分图匹配的轮次级奖励分配和双层优势估计，引入了细粒度的监督机制。具体而言，我们将信用分配问题形式化为预测轨迹与真实轨迹之间的二分图匹配问题，并利用两种分配策略来生成密集的轮次级奖励。此外，为平衡局部步骤精度与全局任务成功率，我们引入了一种双层优势估计方案，该方案整合了轮次级与轨迹级信号，为每个交互轮次分配不同的优势值。在三个基准测试上进行的大量实验证明了MatchTIR的优越性。值得注意的是，我们的40亿参数模型在多数任务上超越了80亿参数的竞争模型，尤其在长视野和多轮次任务中表现突出。代码已开源：https://github.com/quchangle1/MatchTIR。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.10712">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.10712">arXiv</a></p>
<hr />
<h3>13. ToolSafe：通过主动式步骤级护栏与反馈增强基于大语言模型的智能体工具调用安全性</h3>
<p><strong>原文标题：</strong> ToolSafe: Enhancing Tool Invocation Safety of LLM-based agents via Proactive Step-level Guardrail and Feedback</p>
<p><strong>摘要：</strong>
尽管基于大语言模型的智能体能够通过调用外部工具与环境交互，但其扩展的能力也同时放大了安全风险。实时监控步骤级工具调用行为并在不安全执行前主动干预，对于智能体部署至关重要，然而相关研究仍显不足。本研究首先构建了TS-Bench——一个面向大语言模型智能体步骤级工具调用安全检测的新型基准测试集。随后，我们通过多任务强化学习开发了护栏模型TS-Guard。该模型通过分析交互历史记录，在执行前主动检测不安全的工具调用行为，评估请求的危害性及行为与攻击的关联度，并生成可解释、可泛化的安全判断与反馈。此外，我们提出了TS-Flow——一种基于护栏-反馈驱动的智能体推理框架，该框架在提示注入攻击场景下，平均将ReAct式智能体的有害工具调用减少65%，并将良性任务完成率提升约10%。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.10156">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.10156">arXiv</a></p>
<hr />
<h3>14. GPT-5.2、Gemini 3 Pro、Qwen3-VL、Doubao 1.8、Grok 4.1 Fast、Nano Banana Pro与Seedream 4.5安全评估报告</h3>
<p><strong>原文标题：</strong> A Safety Report on GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5</p>
<p><strong>摘要：</strong>
大语言模型与多模态大语言模型的快速发展，显著提升了语言与视觉领域的推理、感知及生成能力。然而，这些技术进步是否带来相应的安全性提升尚不明确，部分原因在于现有评估实践局限于单一模态或威胁模型，呈现碎片化特征。本报告对GPT-5.2、Gemini 3 Pro、Qwen3-VL、Doubao 1.8、Grok 4.1 Fast、Nano Banana Pro及Seedream 4.5等7个前沿模型开展综合性安全评估。我们采用统一评估框架——整合基准测试、对抗性评估、多语言评估与合规性评估——在纯语言、视觉-语言及图像生成三种场景中对各模型进行系统评测。通过将多维度评估结果聚合为安全性能排行榜与模型安全画像，研究揭示出高度异质化的安全格局：GPT-5.2在所有评估中均展现出持续稳定且均衡的安全性能，而其他模型则在基准安全性、对抗对齐性、多语言泛化能力及法规遵从性方面呈现显著权衡关系。尽管各模型在标准基准测试中表现良好，但在对抗性评估下，语言与视觉-语言模态均显现出显著脆弱性，所有模型性能均出现大幅下降。文生图模型在受监管视觉风险类别中表现出相对更强的对齐能力，但在对抗性提示或语义模糊提示下仍显脆弱。总体而言，研究结果表明前沿模型的安全性本质上是多维度的——其表现受模态特性、语言类型及评估方案共同塑造，这凸显了建立标准化安全评估体系的必要性，以准确评估现实风险并引导负责任的模型开发与部署。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.10527">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.10527">arXiv</a></p>
<hr />
<h3>15. Molmo2：具备视频理解与定位能力的视觉语言模型开源权重与数据集</h3>
<p><strong>原文标题：</strong> Molmo2: Open Weights and Data for Vision-Language Models with Video Understanding and Grounding</p>
<p><strong>摘要：</strong>
当前最强大的视频语言模型（VLMs）仍属于闭源系统。最优秀的开源权重模型要么依赖闭源VLMs生成的合成数据进行知识蒸馏，要么未公开其训练数据与方法。这导致开源社区缺乏改进前沿视频（及图像）语言模型所需的基础资源。尤为关键的是，许多下游应用不仅需要高层次视频理解能力，更需具备像素级定位（指向或追踪）功能，而现有闭源模型同样欠缺此能力。本文提出Molmo2系列VLMs，该系列在开源模型中达到领先水平，并在单图像、多图像及视频任务的指向驱动定位中展现出卓越的新能力。我们的核心贡献在于构建了7个新型视频数据集与2个多图像数据集，包括：用于预训练的高细节视频描述数据集、用于微调的自由形式视频问答数据集、包含复杂查询的新型目标追踪数据集，以及创新的视频指向数据集——所有数据均未使用闭源VLMs生成。同时，我们提出了基于高效数据打包与消息树编码方案的训练方法，并证明视觉令牌的双向注意力机制与新颖的令牌加权策略能有效提升性能。我们8B规模的顶尖模型在短视频理解、计数与描述任务上超越同类开源权重与数据模型，在长视频任务中表现相当，在视频定位任务中显著优于Qwen3-VL等现有开源模型（视频计数准确率35.5对29.6），并在部分任务上超越Gemini 3 Pro等闭源模型（视频指向F1分数38.4对20.0，视频追踪J&amp;F分数56.2对41.1）。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.10611">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.10611">arXiv</a></p>
<hr />
<h3>16. FlowAct-R1：迈向交互式人形视频生成</h3>
<p><strong>原文标题：</strong> FlowAct-R1: Towards Interactive Humanoid Video Generation</p>
<p><strong>摘要：</strong>
交互式人形视频生成旨在合成能够通过连续且响应式视频与人类互通的逼真视觉智能体。尽管视频合成领域近期取得了进展，现有方法仍常面临高保真合成与实时交互需求之间的权衡。本文提出FlowAct-R1，一个专为实时交互式人形视频生成设计的框架。该框架基于MMDiT架构构建，能够实现任意时长视频的流式合成，同时保持低延迟响应。我们引入了一种分块扩散强制策略，并结合新颖的自强制变体，以减轻误差累积并确保连续交互过程中的长期时序一致性。通过高效蒸馏与系统级优化，本框架在480p分辨率下实现了稳定的25fps生成速度，首帧生成时间仅约1.5秒。所提方法提供整体且细粒度的全身控制，使智能体能在交互场景中自然过渡于多种行为状态。实验结果表明，FlowAct-R1在保持跨角色风格鲁棒泛化能力的同时，实现了卓越的行为生动性与感知真实感。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.10103">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.10103">arXiv</a></p>
<hr />
<h3>17. 快速视频生成的过渡匹配蒸馏方法</h3>
<p><strong>原文标题：</strong> Transition Matching Distillation for Fast Video Generation</p>
<p><strong>摘要：</strong>
大型视频扩散与流模型已在高质量视频生成领域取得显著成功，但由于其低效的多步采样过程，在实时交互应用中的使用仍受限。本研究提出过渡匹配蒸馏（TMD），一种将视频扩散模型蒸馏为高效少步生成器的新框架。TMD的核心思想是将扩散模型的多步去噪轨迹与少步概率转移过程相匹配，其中每个转移步骤被建模为轻量级条件流。为实现高效蒸馏，我们将原始扩散主干网络分解为两个部分：（1）主主干网络（包含早期多数层），用于在外部每个转移步骤提取语义表征；（2）流头部网络（由最后若干层构成），利用这些表征执行多次内部流更新。给定预训练视频扩散模型，我们首先为其引入流头部结构，并将其适配为条件流映射。随后通过在每个转移步骤中展开流头部，对包含流头部的学生模型实施分布匹配蒸馏。基于Wan2.1 1.3B和14B文本到视频模型的蒸馏实验表明，TMD在生成速度与视觉质量之间实现了灵活而优异的平衡。特别值得注意的是，在可比较的推理成本下，TMD在视觉保真度与提示遵循度方面均优于现有蒸馏模型。项目页面：https://research.nvidia.com/labs/genair/tmd</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.09881">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.09881">arXiv</a></p>
<hr />
<h3>18. PACEvolve：实现长周期进度感知一致性进化的框架</h3>
<p><strong>原文标题：</strong> PACEvolve: Enabling Long-Horizon Progress-Aware Consistent Evolution</p>
<p><strong>摘要：</strong>
大语言模型已成为进化搜索的强大操作工具，但高效搜索框架的设计仍缺乏系统性。尽管现有的大语言模型循环系统展现出潜力，但其在管理进化过程方面缺乏系统化方法。我们识别出三种典型的失效模式：上下文污染（实验历史偏差影响后续候选生成）、模式崩溃（因探索-利用平衡不佳导致智能体陷入局部最优）以及弱协作（僵化的交叉策略无法有效利用并行搜索轨迹）。为应对这些挑战，我们提出进度感知一致性进化框架，该框架通过分层上下文管理结合剪枝机制解决上下文污染问题；采用动量回溯机制逃离局部最优；并引入自适应采样策略，将回溯与交叉统一为动态搜索协调机制，使智能体能够平衡内部优化与跨轨迹协作。实验表明，该框架为实现持续长周期自我进化提供了系统化路径，在LLM-SR和KernelBench基准测试中达到最优性能，并在Modded NanoGPT任务中发现了超越现有记录的解决方案。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.10657">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.10657">arXiv</a></p>
<hr />
<h3>19. Action100M：一个大规模视频动作数据集</h3>
<p><strong>原文标题：</strong> Action100M: A Large-scale Video Action Dataset</p>
<p><strong>摘要：</strong>
从视觉观察中推断物理动作是推动机器智能在物理世界中发展的核心能力。实现这一目标需要覆盖广泛领域的大规模、开放词汇的视频动作数据集。我们提出了Action100M，这是一个基于120万个互联网教学视频（总时长约14.6年）构建的大规模数据集，生成了约1亿个具有开放词汇动作标注和丰富文本描述的时间定位片段。Action100M通过全自动流程生成，该流程（i）利用V-JEPA 2嵌入进行分层时间分割，（ii）生成组织为“描述树”的多层级帧与片段描述，以及（iii）在多轮自我优化过程中，通过推理模型（GPT-OSS-120B）聚合证据，输出结构化标注（简洁/详细动作、执行者、简洁/详细描述）。在Action100M上训练VL-JEPA模型显示出持续的数据规模扩展效益，并在多种动作识别基准测试中实现了强大的零样本性能，从而确立了Action100M作为视频理解与世界建模可扩展研究的新基础。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.10592">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.10592">arXiv</a></p>
<hr />
<h3>20. M^4olGen：精确多属性约束下的多智能体、多阶段分子生成</h3>
<p><strong>原文标题：</strong> M^4olGen: Multi-Agent, Multi-Stage Molecular Generation under Precise Multi-Property Constraints</p>
<p><strong>摘要：</strong>
生成满足多种物理化学性质精确数值约束的分子至关重要且具有挑战性。尽管大语言模型（LLMs）表达能力强，但在缺乏外部结构和反馈的情况下，难以实现精确的多目标控制和数值推理。我们提出了M^4olGen，一个基于片段、检索增强的两阶段框架，用于多属性约束下的分子生成。第一阶段：原型生成：一个多智能体推理器执行检索锚定的片段级编辑，生成接近可行区域的候选分子。第二阶段：基于强化学习的细粒度优化：一个采用组相对策略优化（GRPO）训练的片段级优化器，通过单跳或多跳精炼，在调控编辑复杂度和原型偏离度的同时，显式最小化属性误差以逼近目标值。一个自动构建的大规模数据集支撑了两个阶段，该数据集包含片段编辑的推理链和测量的属性变化，从而实现了确定性的、可复现的监督以及可控的多跳推理。与先前工作不同，我们的框架通过利用片段更好地对分子进行推理，并支持向数值目标的可控精炼。在两组属性约束（QED、LogP、分子量以及HOMO、LUMO）下的生成实验表明，该框架在有效性和精确满足多属性目标方面持续提升，性能优于强大的LLMs和基于图的算法。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.10131">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.10131">arXiv</a></p>
<hr />
<h3>21. HeartMuLa：开源音乐基础模型系列</h3>
<p><strong>原文标题：</strong> HeartMuLa: A Family of Open Sourced Music Foundation Models</p>
<p><strong>摘要：</strong>
本文提出一系列开源音乐基础模型，旨在推动跨任务与多模态的大规模音乐理解与生成。该框架包含四大核心组件：（1）HeartCLAP——音频-文本对齐模型；（2）HeartTranscriptor——面向真实音乐场景优化的鲁棒歌词识别模型；（3）HeartCodec——低帧率（12.5 Hz）高保真音乐编解码分词器，在捕捉长程音乐结构的同时保留细粒度声学细节，并支持高效自回归建模；（4）HeartMuLa——基于大语言模型的歌曲生成模型，能够在丰富用户可控条件下（如文本风格描述、歌词及参考音频）合成高保真音乐。此外，该模型提供两种专项生成模式：（i）细粒度音乐属性控制，允许用户通过自然语言指令指定歌曲段落（如前奏、主歌、副歌）的风格；（ii）简短动感音乐生成，适用于短视频背景音乐场景。实验表明，当模型参数量扩展至70亿时，HeartMuLa性能显著提升。本研究首次证明，利用学术规模数据与GPU资源即可复现达到Suno级别的商用系统水准。我们期望该系列基础模型能为未来研究提供坚实基线，并推动多模态内容生产的实际应用。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.10547">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.10547">arXiv</a></p>
<hr />
<h3>22. 视频生成模型的推理时物理对齐与潜在世界模型</h3>
<p><strong>原文标题：</strong> Inference-time Physics Alignment of Video Generative Models with Latent World Models</p>
<p><strong>摘要：</strong>
当前最先进的视频生成模型能够生成视觉效果出色的内容，但常常违反基本物理原理，这限制了其实用性。尽管有观点认为这一缺陷源于预训练阶段对物理规律的理解不足，但我们发现物理合理性的不足也源于次优的推理策略。因此，我们提出了WMReward方法，将提升视频生成的物理合理性视为推理时的对齐问题。具体而言，我们利用潜在世界模型（此处为VJEPA-2）的强物理先验作为奖励，通过搜索和引导多条候选去噪轨迹，实现了测试时计算资源的灵活扩展以提升生成性能。实验表明，我们的方法在图像条件生成、多帧条件生成和文本条件生成等多种设置下均显著提升了物理合理性，并通过人工偏好研究验证了其有效性。值得注意的是，在ICCV 2025感知测试物理智商挑战赛中，我们以62.64%的最终得分获得第一名，较先前最优方法提升了7.42%。我们的工作证明了利用潜在世界模型提升视频生成物理合理性的可行性，其价值不仅限于特定的模型实例或参数化方案。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.10553">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.10553">arXiv</a></p>
<hr />
<h3>23. EvasionBench：基于多模型共识与LLM裁判机制的金融问答规避性回答检测</h3>
<p><strong>原文标题：</strong> EvasionBench: Detecting Evasive Answers in Financial Q&amp;A via Multi-Model Consensus and LLM-as-Judge</p>
<p><strong>摘要：</strong>
在财报电话会议中检测规避性回答对提升金融透明度至关重要，但大规模基准数据的缺乏制约了该领域进展。本文提出EvasionBench数据集，包含按三种规避程度划分的30,000个训练样本和1,000个人工标注测试样本（科恩卡帕系数0.835）。我们的核心贡献在于提出一种多模型标注框架，其关键洞见是：前沿大语言模型之间的分歧信号能识别出最具训练价值的困难样本。我们通过挖掘两个强标注模型产生冲突的边界案例，并引入裁判模型进行标签裁决。该方法相比单模型蒸馏策略提升2.4%的性能，且经裁判裁决的样本虽带来更高训练损失（0.421对比0.393），却显著提升了模型泛化能力——这证明分歧挖掘机制发挥了隐式正则化作用。最终训练的Eva-4B模型（40亿参数）达到81.3%的准确率，较基础模型提升25个百分点，并以极低的推理成本逼近前沿大语言模型性能。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.09142">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.09142">arXiv</a></p>
<hr />
<h3>24. TAG-MoE：面向统一生成式专家混合模型的任务感知门控机制</h3>
<p><strong>原文标题：</strong> TAG-MoE: Task-Aware Gating for Unified Generative Mixture-of-Experts</p>
<p><strong>摘要：</strong>
在密集扩散Transformer架构中，统一的图像生成与编辑模型存在严重的任务干扰问题，其共享参数空间必须在相互冲突的目标（如局部编辑与主体驱动生成）之间做出妥协。尽管稀疏专家混合（MoE）范式是一种颇具前景的解决方案，但其门控网络仍保持任务无关性，仅基于局部特征进行操作，无法感知全局任务意图。这种任务无关特性阻碍了有意义的专业化分工，未能从根本上解决任务干扰问题。本文提出一种新颖框架，将语义意图注入MoE路由机制。我们引入分层任务语义标注方案以构建结构化任务描述符（如作用范围、任务类型、保留要求），进而设计预测对齐正则化方法，使内部路由决策与任务高层语义保持一致。该正则化使门控网络从任务无关执行器演进为智能调度中心。实验表明，本模型能有效缓解任务干扰，在生成保真度与质量方面超越密集基线模型，分析结果证实各专家能自然形成清晰且语义关联的专业化能力。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.08881">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.08881">arXiv</a></p>
<hr />
<h3>25. PRL：过程奖励学习提升大语言模型推理能力并拓展推理边界</h3>
<p><strong>原文标题：</strong> PRL: Process Reward Learning Improves LLMs' Reasoning Ability and Broadens the Reasoning Boundary</p>
<p><strong>摘要：</strong>
提升大语言模型的推理能力是当前持续关注的研究课题。然而，现有研究大多基于轨迹层面的结果奖励，缺乏对推理过程的细粒度监督。其他尝试融合过程信号以优化大语言模型的训练框架，往往严重依赖蒙特卡洛树搜索、训练独立奖励模型等繁琐附加步骤，损害了训练效率。此外，过程信号设计背后的直觉缺乏严格的理论支撑，导致优化机制的理解仍不透明。本文提出过程奖励学习方法，该方法将熵正则化强化学习目标分解为中间步骤，并基于严格推导的过程奖励对模型进行相应分配。我们从理论动机出发，推导出与“奖励最大化目标加策略模型与参考模型间KL散度惩罚项”本质等价的过程奖励学习形式化表达。该方法能够将结果奖励转化为过程监督信号，从而在强化学习优化过程中更好地引导探索。实验结果表明，过程奖励学习不仅通过平均@n指标提升了大语言模型推理能力的整体表现，还通过提高通过@n指标拓展了推理边界。大量实验验证了该方法的有效性及其良好的泛化能力。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.10201">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.10201">arXiv</a></p>
<hr />
<h3>26. LaViT：面向多模态推理的潜在视觉思维对齐方法</h3>
<p><strong>原文标题：</strong> LaViT: Aligning Latent Visual Thoughts for Multi-modal Reasoning</p>
<p><strong>摘要：</strong>
当前多模态潜在推理方法通常依赖外部监督（如辅助图像），忽略了内在的视觉注意力动态机制。本研究揭示了知识蒸馏中的关键感知鸿沟：学生模型往往在关注完全不同的视觉区域时模仿教师的文本输出，实质上依赖于语言先验而非基于感知的推理。为弥合这一鸿沟，我们提出LaViT框架，该框架通过对齐潜在视觉思维而非静态嵌入来实现多模态对齐。LaViT强制学生在文本生成前自回归地重构教师的视觉语义与注意力轨迹，并采用课程式感知门控机制以避免捷径学习。大量实验表明，LaViT显著增强了视觉基础推理能力，在复杂推理任务上实现了最高达16.9%的性能提升，使一个紧凑的30亿参数模型能够超越更大规模的开源变体及GPT-4o等专有模型。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.10129">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.10129">arXiv</a></p>
<hr />
<h3>27. LSRIF：面向指令跟随的逻辑结构化强化学习</h3>
<p><strong>原文标题：</strong> LSRIF: Logic-Structured Reinforcement Learning for Instruction Following</p>
<p><strong>摘要：</strong>
指令跟随能力对大语言模型至关重要，但现实场景中的指令常包含顺序依赖与条件分支等逻辑结构。现有方法通常构建具有并行约束的数据集并优化平均奖励，忽视了逻辑依赖性，导致训练信号存在噪声。本文提出一种显式建模指令逻辑的训练框架LSRIF。我们首先构建包含并行、顺序、条件三类约束结构的LSRInstruct数据集，进而设计结构感知的奖励方法LSRIF：对并行结构采用平均聚合奖励，对顺序结构实施失败惩罚传递机制，对条件分支设计选择性奖励。实验表明，LSRIF在指令跟随（领域内/外）和通用推理任务上均带来显著提升。分析发现，通过显式逻辑结构学习能够驱动注意力层的参数更新，并增强模型对约束条件与逻辑运算符的词级关注度。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.06431">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.06431">arXiv</a></p>
<hr />
<h3>28. 临床文本到SQL中的患者相似性队列推理</h3>
<p><strong>原文标题：</strong> Patient-Similarity Cohort Reasoning in Clinical Text-to-SQL</p>
<p><strong>摘要：</strong>
现实世界的临床文本到SQL任务需要对异构电子健康记录表、时间窗口和患者相似性队列进行推理，以生成可执行的查询。我们提出了CLINSQL基准，该基准基于MIMIC-IV v3.1数据集构建，包含633项专家标注的任务，要求进行多表连接、具有临床意义的筛选并生成可执行的SQL语句。解决CLINSQL任务需要理解数据库模式元数据和临床编码系统、处理长上下文信息，并构建超越传统文本到SQL的多步骤查询。我们在思维链自优化框架下评估了22个专有和开源模型，并采用基于量规的SQL分析与执行检查方法，优先考虑关键的临床需求。尽管近期技术有所进展，但模型性能仍远未达到临床可靠性标准：在测试集上，GPT-5-mini的执行准确率为74.7%，DeepSeek-R1以69.2%的成绩领先开源模型，而Gemini-2.5-Pro在简单任务上虽达到85.5%的准确率，但在困难任务上则降至67.2%。CLINSQL基准的进展标志着面向真实世界电子健康记录分析的临床可靠文本到SQL技术取得了实质性进步。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.09876">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.09876">arXiv</a></p>
<hr />
<h3>29. 智能体技能在真实环境中的安全风险：大规模安全漏洞实证研究</h3>
<p><strong>原文标题：</strong> Agent Skills in the Wild: An Empirical Study of Security Vulnerabilities at Scale</p>
<p><strong>摘要：</strong>
人工智能智能体框架的兴起催生了智能体技能——一种包含指令与可执行代码的模块化封装包，能够动态扩展智能体能力。尽管这种架构实现了强大的定制功能，但技能通常在隐式信任且缺乏严格审查的环境下执行，从而形成了一个显著但尚未被充分认知的攻击面。本研究首次对这一新兴生态系统开展大规模安全实证分析：从两大主流市场收集42,447个技能样本，并运用SkillScan多阶段检测框架（整合静态分析与基于大语言模型的语义分类技术）对31,132个技能进行系统性检测。研究发现普遍存在的安全风险：26.1%的技能至少存在一种漏洞，这些漏洞涵盖提示词注入、数据窃取、权限提升和供应链风险四大类共14种具体模式。其中数据窃取（13.3%）与权限提升（11.8%）最为普遍，5.2%的技能表现出强烈暗示恶意意图的高危模式。研究进一步发现：封装可执行脚本的技能存在漏洞的概率是纯指令型技能的2.12倍（OR=2.12, p&lt;0.001）。本研究的贡献包括：（1）基于8,126个漏洞技能构建的实证漏洞分类体系；（2）达到86.7%精确率与82.5%召回率的可验证检测方法；（3）为后续研究提供的开源数据集与检测工具包。这些结果表明，在此攻击向量被进一步利用之前，亟需建立基于能力的权限控制系统和强制性的安全审查机制。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.10338">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.10338">arXiv</a></p>
<hr />
<h3>30. 从情节中推导角色逻辑：基于编码决策树的方法</h3>
<p><strong>原文标题：</strong> Deriving Character Logic from Storyline as Codified Decision Trees</p>
<p><strong>摘要：</strong>
角色扮演智能体依赖行为配置文件在不同叙事情境中保持行为一致性，但现有配置文件大多为非结构化、不可执行且验证薄弱，导致智能体行为脆弱。我们提出编码决策树（CDT）这一数据驱动框架，能够从大规模叙事数据中推导出可执行且可解释的决策结构。CDT将行为配置文件表示为条件规则树，其中内部节点对应经过验证的场景条件，叶节点编码具体行为陈述，从而在执行时实现情境适配规则的确定性检索。该决策树通过迭代推导候选场景-行动规则、依据数据进行验证，并通过层次化特化进行优化而习得，最终形成支持透明检视与原则性更新的配置文件。在涵盖16个叙事作品的85个角色的多项基准测试中，CDT显著优于人工编写的配置文件及先前的配置文件推导方法，表明经过编码与验证的行为表征能够为智能体提供更可靠的行为基础。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.10080">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.10080">arXiv</a></p>
<hr />
<h3>31. V-DPM：基于动态点图映射的四维视频重建</h3>
<p><strong>原文标题：</strong> V-DPM: 4D Video Reconstruction with Dynamic Point Maps</p>
<p><strong>摘要：</strong>
诸如DUSt3R不变点图映射等强大的三维表示方法，通过编码三维形状与相机参数，显著推进了前馈式三维重建技术的发展。传统点图映射通常假设场景为静态，而动态点图映射（DPMs）通过额外表征场景运动，将这一概念扩展至动态三维内容。然而，现有DPMs仅适用于图像对，且与DUSt3R类似，在处理超过两个视角时仍需通过优化进行后处理。我们认为DPMs在视频应用中更具实用价值，并由此提出V-DPM予以验证。首先，我们阐释如何构建适用于视频输入的DPMs表示框架，以最大化表征能力、促进神经预测并实现预训练模型复用。其次，我们在近期强大的三维重建框架VGGT基础上实现了该框架。尽管VGGT原基于静态场景训练，但我们证明仅需适量合成数据即可将其有效适配为V-DPM预测器。该方法在动态场景的三维与四维重建任务中达到业界最优性能。特别值得注意的是，相较于VGGT的近期动态扩展方法（如P3），DPMs不仅能重建动态深度，还能完整恢复场景中每个点的三维运动轨迹。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.09499">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.09499">arXiv</a></p>
<hr />
<h3>32. RigMo：统一骨骼与运动学习的生成式动画框架</h3>
<p><strong>原文标题：</strong> RigMo: Unifying Rig and Motion Learning for Generative Animation</p>
<p><strong>摘要：</strong>
尽管四维生成领域已取得显著进展，但作为动画核心结构与动态组件的骨骼与运动通常被建模为独立问题。现有流程依赖真实骨骼与蒙皮权重进行运动生成，并将自动骨骼绑定视为独立过程，这限制了方法的可扩展性与可解释性。本文提出RigMo——一个统一的生成式框架，能够直接从原始网格序列中联合学习骨骼与运动，无需任何人工标注的骨骼信息。RigMo将逐顶点变形编码至两个紧凑的潜在空间：骨骼潜在空间（解码为显式高斯骨骼与蒙皮权重）和运动潜在空间（生成随时间变化的SE(3)变换）。这些输出共同定义了具有显式结构与连贯运动的可动画网格，实现了可变形物体的前馈式骨骼与运动推断。除统一发现骨骼-运动关系外，我们进一步提出在RigMo潜在空间中运行的Motion-DiT模型，证明这种结构感知的潜在表示能自然支持下游运动生成任务。在DeformingThings4D、Objaverse-XL和TrueBones数据集上的实验表明，RigMo能够学习平滑、可解释且物理合理的骨骼系统，同时在重建效果与类别级泛化能力上优于现有自动骨骼绑定与变形基线方法。RigMo为统一、结构感知且可扩展的动态三维建模建立了新范式。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.06378">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.06378">arXiv</a></p>
<hr />
<h3>33. CaMeLs亦能驾驭计算机：计算机使用代理的系统级安全</h3>
<p><strong>原文标题：</strong> CaMeLs Can Use Computers Too: System-level Security for Computer Use Agents</p>
<p><strong>摘要：</strong>
人工智能代理易受提示注入攻击，恶意内容可通过劫持代理行为窃取凭证或造成经济损失。目前唯一已知的稳健防御方案是采用架构隔离机制，将可信任务规划与不可信环境观察严格分离。然而，将该设计应用于计算机使用代理（CUAs）——即通过观察屏幕状态并执行操作实现任务自动化的系统——面临根本性挑战：现有代理需要持续观察用户界面状态以确定每个操作步骤，这与安全所需的隔离要求相冲突。我们通过论证用户界面工作流虽具有动态性但结构可预测的特性，解决了这一矛盾。本文提出面向CUAs的单次规划方法，由可信规划器在观察任何潜在恶意内容前，生成包含条件分支的完整执行图，从而为任意指令注入攻击提供可验证的控制流完整性保障。尽管架构隔离能有效防御指令注入，但我们发现仍需额外措施防范分支导向攻击——此类攻击通过操纵界面元素触发计划中非预期的有效路径。我们在OSWorld环境中评估该设计，在保持前沿模型57%性能的同时，将小型开源模型的性能提升最高达19%，证明CUAs能够实现严格安全性与实用性的共存。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.09923">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.09923">arXiv</a></p>
<hr />
<h3>34. WildRayZer：动态环境中自监督的大视角合成</h3>
<p><strong>原文标题：</strong> WildRayZer: Self-supervised Large View Synthesis in Dynamic Environments</p>
<p><strong>摘要：</strong>
本文提出WildRayZer，一种用于相机与物体均发生运动的动态环境中的新视角合成自监督框架。动态内容会破坏静态新视角合成模型所依赖的多视角一致性，导致重影、几何结构失真及姿态估计不稳定等问题。WildRayZer通过执行分析-合成测试解决这一难题：首先通过仅考虑相机运动的静态渲染器解析刚性结构，其残差图可揭示瞬变区域。基于这些残差，我们构建伪运动掩码，蒸馏出运动估计器，并利用该估计器对输入标记进行掩码处理及损失梯度门控，从而使监督学习聚焦于跨视角背景补全任务。为支持大规模训练与评估，我们构建了Dynamic RealEstate10K（D-RE10K）数据集——包含1.5万条自然采集动态序列的真实场景数据集，以及D-RE10K-iPhone配对数据集——专为稀疏视角瞬变感知新视角合成设计的瞬变/洁净场景基准测试集。实验表明，WildRayZer在单次前向传播中，无论是瞬变区域消除还是全帧新视角合成质量，均持续优于基于优化的基线方法与前馈基线模型。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.10716">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.10716">arXiv</a></p>
<hr />
<h3>35. VQ-Seg：基于向量量化标记扰动的半监督医学图像分割方法</h3>
<p><strong>原文标题：</strong> VQ-Seg: Vector-Quantized Token Perturbation for Semi-Supervised Medical Image Segmentation</p>
<p><strong>摘要：</strong>
基于特征扰动的一致性学习是半监督医学图像分割中广泛采用的策略。然而，现有许多扰动方法依赖于随机失活技术，需要人工精细调整失活率——该超参数敏感且难以优化，易导致正则化效果欠佳。为克服这一局限，本文提出VQ-Seg方法，首次引入向量量化技术离散化特征空间，并设计了一种可替代随机失活的新型可控量化扰动模块。该模块通过重排码本索引的空间位置实现对离散表示的扰动，从而达成高效可控的正则化效果。为缓解量化可能造成的信息损失，我们设计了双分支架构，使图像重建与分割任务共享量化后的特征空间。此外，我们提出后量化特征适配器，通过引入基础模型的语义指导来补充量化过程中损失的高层语义信息。本研究还构建了包含828例中央型肺癌标注CT扫描的大规模肺癌数据集。在肺癌数据集及其他公开基准上的大量实验表明，本方法优于当前最优方案，验证了其有效性。代码已开源：https://github.com/script-Yang/VQ-Seg。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.10124">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.10124">arXiv</a></p>
<hr />
<h3>36. 通过先进提示工程技术增强大语言模型的情感分类与反讽检测能力</h3>
<p><strong>原文标题：</strong> Enhancing Sentiment Classification and Irony Detection in Large Language Models through Advanced Prompt Engineering Techniques</p>
<p><strong>摘要：</strong>
本研究探讨如何运用提示工程技术提升大语言模型（特别是GPT-4o-mini与gemini-1.5-flash）在情感分析任务中的表现。通过对比基线方法，系统评估了少样本学习、思维链提示及自我一致性等先进提示技术的效果。核心任务涵盖情感分类、基于方面的情感分析以及反讽等微妙语义的检测。研究详细阐述了理论背景、数据集与实验方法，并基于准确率、召回率、精确率和F1分数评估大语言模型的性能。结果表明，先进提示技术能显著改善情感分析效果：少样本提示在GPT-4o-mini中表现最优，而思维链提示使gemini-1.5-flash的反讽检测性能提升高达46%。由此可见，尽管先进提示技术整体提升模型性能，但针对不同模型与任务需采用差异化策略——少样本提示最适合GPT-4o-mini，而思维链提示在gemini-1.5-flash的反讽检测中更具优势。这凸显了提示设计必须同时适配大语言模型架构与任务语义复杂性的重要意义。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.08302">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.08302">arXiv</a></p>
<hr />
<h3>37. 揭秘注意力机制中的斜线模式：RoPE的作用</h3>
<p><strong>原文标题：</strong> Demystifying the Slash Pattern in Attention: The Role of RoPE</p>
<p><strong>摘要：</strong>
大型语言模型（LLM）常表现出斜线注意力模式，即注意力分数集中在某个偏移量Δ对应的第Δ条次对角线上。这些模式在跨词元传递信息中起着关键作用。但为何会出现这种模式？本文从实证与理论双重视角揭示了斜线主导注意力头（SDH）的生成机制。首先，通过对开源LLM的分析，我们发现SDH是模型固有的特性，并能泛化至分布外提示。为解释其固有生成机制，我们分析了共同决定注意力分数的查询向量、键向量及旋转位置编码（RoPE）。实证分析揭示了SDH的两个特征条件：（1）查询向量与键向量近似秩为一；（2）RoPE由中高频分量主导。在此条件下，跨词元的查询向量与键向量近乎相同，而RoPE中高频分量间的相互作用催生了SDH。除实证证据外，我们通过将上述条件形式化为建模假设，从理论上证明这些条件足以保证SDH的生成。特别地，我们分析了在此条件下配备RoPE的浅层Transformer的训练动态，并证明通过梯度下降训练的模型会呈现SDH特性，且该特性可泛化至分布外提示。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.08297">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.08297">arXiv</a></p>
<hr />
<h3>38. 面向大语言模型持续适应的记忆库压缩方法</h3>
<p><strong>原文标题：</strong> Memory Bank Compression for Continual Adaptation of Large Language Models</p>
<p><strong>摘要：</strong>
大语言模型已成为众多日常应用的核心技术。然而，随着数据动态演进，其知识体系会迅速过时。持续学习旨在通过新信息更新大语言模型，同时避免遗忘已掌握的知识。尽管全参数微调等方法能够整合新数据，但其计算成本高昂且易引发灾难性遗忘——即旧有知识被覆盖的问题。基于记忆增强的方法通过为大语言模型配备记忆库（即存储信息以供后续调用的外部记忆模块）来解决这一挑战。然而，这些方法面临关键局限：在现实场景中，当大规模数据流持续涌入时，记忆库会不断膨胀。本文提出MBC模型，该模型通过在线适应学习过程中的码本优化策略实现记忆库压缩。为确保学习稳定性，我们同时引入在线重置机制以防止码本坍缩。此外，我们在大语言模型的注意力层采用键值低秩适应技术，从而实现对压缩记忆表征的高效利用。基于基准问答数据集的实验表明：相较于最具竞争力的基线方法，MBC可将记忆库规模压缩至原大小的0.3%，同时在在线适应学习中保持高记忆保持准确率。代码已公开于https://github.com/Thomkat/MBC。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2601.00756">HuggingFace</a> | <a href="https://arxiv.org/abs/2601.00756">arXiv</a></p>
<hr />
<h2>🔍 关键词云图</h2>
<p><img alt="关键词云图" src="../images/keywords_wordcloud.png" /></p>
<h2>📈 近期论文趋势</h2>
<p><img alt="论文趋势" src="../images/daily_papers.png" /></p>
<h2>🎙️ 语音播报</h2>
<ul>
<li><a href="../audio/2026-01-16_daily_papers.mp3">收听今日论文解读</a></li>
</ul>
<h2>📱 订阅渠道</h2>
<ul>
<li>GitHub: <a href="https://github.com/2404589803/hf-daily-paper-newsletter-chinese">hf-daily-paper-newsletter-chinese</a></li>
</ul>
    </div>
</body>
</html>