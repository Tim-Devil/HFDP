
# <img src="https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo.png" width="30"/> Hugging Face 2025-12-16 论文日报

## 📊 今日论文统计
- 总论文数：41
- 热门领域：GPT, RL, LLM, Diffusion, Transformer, NLP

## 📝 论文详情


### 1. ReFusion：一种基于并行自回归解码的扩散大语言模型

**原文标题：** ReFusion: A Diffusion Large Language Model with Parallel Autoregressive Decoding

**摘要：**
自回归模型因顺序推理速度缓慢而受到限制。尽管掩码扩散模型提供了并行化的替代方案，但其存在两个关键缺陷：一是因无法使用键值缓存而导致计算开销高昂，二是在学习难以处理的词元组合空间上的依赖关系时，会产生不连贯的生成结果。为解决这些局限性，本文提出ReFusion——一种新颖的掩码扩散模型，通过将并行解码从词元层级提升至更高层级的“槽位”（每个槽位为固定长度的连续子序列），实现了更优的性能与效率。该模型采用迭代式的“规划-填充”解码流程：首先通过基于扩散的规划步骤识别出一组弱依赖的槽位，随后通过自回归填充步骤并行解码这些选定槽位。这种基于槽位的设计在统一因果框架下实现了完整的键值缓存复用，同时将学习复杂度从词元组合空间降低至可管理的槽位排列空间。在七个多样化基准测试上的大量实验表明，ReFusion不仅以34%的性能提升和平均超过18倍的加速比显著超越现有掩码扩散模型，更在保持平均2.33倍加速优势的同时，大幅缩小了与强自回归模型的性能差距。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.13586) | [arXiv](https://arxiv.org/abs/2512.13586)



---

### 2. 面向生成任务的可扩展视觉分词器预训练研究

**原文标题：** Towards Scalable Pre-training of Visual Tokenizers for Generation

**摘要：**
视觉分词器（如变分自编码器）的潜在空间质量对现代生成模型至关重要。然而，基于标准重建的训练范式产生的潜在空间偏向于低层次信息，这导致了一个根本性缺陷：更高的像素级精度并不能带来更高质量的生成结果。这意味着将大量计算资源投入视觉分词器预训练对生成性能的提升效果有限。我们将此问题定义为“预训练扩展困境”，并提出关键转变思路：要有效支持生成任务，潜在空间必须简洁地表征高层次语义信息。本文提出VTP——一个统一的视觉分词器预训练框架，率先实现了图像-文本对比损失、自监督损失与重建损失的联合优化。我们的大规模实验揭示了两项核心发现：（1）理解能力是驱动生成性能的关键因素；（2）该框架具备更优越的扩展特性，生成性能可随预训练投入的计算量、参数量和数据量有效提升。经过大规模预训练后，我们的分词器展现出卓越性能（在ImageNet数据集上达到78.2%的零样本分类准确率和0.36的rFID指标），且在生成任务上比先进蒸馏方法快4.1倍收敛。更重要的是，该框架具备显著扩展优势：在不改变标准DiT训练配置的情况下，仅通过增加VTP预训练的计算量即可实现下游生成任务65.8%的FID指标提升，而传统自编码器在仅使用1/10计算量时性能便过早停滞。预训练模型已开源：https://github.com/MiniMax-AI/VTP。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.13687) | [arXiv](https://arxiv.org/abs/2512.13687)



---

### 3. 人工智能智能体时代中的记忆研究

**原文标题：** Memory in the Age of AI Agents

**摘要：**
记忆已成为基于基础模型的智能体核心能力，并将持续发挥关键作用。随着智能体记忆研究快速扩张并受到前所未有的关注，该领域也日益呈现碎片化态势。现有关于智能体记忆的研究在动机、实现方式和评估标准上存在显著差异，而定义松散的记忆术语泛滥进一步模糊了概念清晰度。传统分类法（如长/短期记忆）已不足以涵盖当代智能体记忆系统的多样性。本研究旨在系统梳理当前智能体记忆研究的最新图景。首先明确界定智能体记忆的范畴，并将其与大型语言模型记忆、检索增强生成（RAG）及上下文工程等相关概念进行区分。随后通过形式、功能与动态演化的统一视角审视智能体记忆：在形式层面，归纳出符号级记忆、参数化记忆与潜在记忆三种主流实现方式；在功能层面，提出更细粒度的事实记忆、经验记忆与工作记忆分类体系；在动态层面，解析记忆随时间推移的形成、演化与检索机制。为支撑实际开发，本文汇编了完整的记忆基准测试集与开源框架综述。在整合现有成果基础上，进一步展望了记忆自动化、强化学习融合、多模态记忆、多智能体记忆及可信度问题等新兴前沿研究方向。本研究不仅可作为现有工作的参考指南，更希望为将记忆重新定位为未来智能体设计中的核心基础要素提供概念框架。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.13564) | [arXiv](https://arxiv.org/abs/2512.13564)



---

### 4. QwenLong-L1.5：面向长上下文推理与记忆管理的后训练方案

**原文标题：** QwenLong-L1.5: Post-Training Recipe for Long-Context Reasoning and Memory Management

**摘要：**
本文介绍QwenLong-L1.5模型，该模型通过系统性的后训练创新实现了卓越的长上下文推理能力。QwenLong-L1.5的关键技术突破如下：（1）长上下文数据合成流程：我们开发了系统性合成框架，可生成需要基于全局分布证据进行多跳溯因的复杂推理任务。通过将文档解构为原子事实及其内在关联，再以可编程方式组合可验证的推理问题，本方法实现了高质量训练数据的大规模生成，显著超越简单检索任务，真正实现了长距离推理能力。（2）面向长上下文训练的稳定强化学习：为克服长上下文强化学习中的关键不稳定性，我们提出基于任务平衡采样与任务特定优势估计的奖励偏差缓解机制，并设计自适应熵控制策略优化算法，动态调节探索与利用的平衡。（3）超长上下文记忆增强架构：针对扩展上下文窗口仍无法容纳无限长序列的瓶颈，我们开发了具有多阶段融合强化训练的记忆管理框架，通过单次推理与基于记忆的迭代处理无缝协同，可处理超过400万标记的超长任务。基于Qwen3-30B-A3B-Thinking构建的QwenLong-L1.5在长上下文推理基准测试中达到与GPT-5和Gemini-2.5-Pro相当的性能，较基线模型平均提升9.90分。在超长任务（100万至400万标记）中，其记忆智能体框架相比智能体基线获得9.48分的性能增益。此外，所获得的长上下文推理能力可迁移提升科学推理、记忆工具使用及长程对话等通用领域的表现。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.12967) | [arXiv](https://arxiv.org/abs/2512.12967)



---

### 5. LongVie 2：多模态可控超长视频世界模型

**原文标题：** LongVie 2: Multimodal Controllable Ultra-Long Video World Model

**摘要：**
在预训练视频生成系统基础上构建视频世界模型，是实现通用时空智能的重要且具有挑战性的一步。一个理想的世界模型应具备三个关键特性：可控性、长期视觉质量与时间一致性。为此，我们采用渐进式方法——先提升可控性，再向长期高质量生成扩展。我们提出LongVie 2，这是一个端到端自回归框架，通过三阶段训练实现：（1）多模态引导，融合稠密与稀疏控制信号以提供隐式世界级监督，提升可控性；（2）输入帧的退化感知训练，弥合训练与长期推理间的差距以保持高视觉质量；（3）历史上下文引导，对齐相邻片段间的上下文信息以确保时间一致性。我们进一步提出LongVGenBench，这是一个包含100段高分辨率一分钟视频的综合基准数据集，涵盖多样化的真实世界与合成场景。大量实验表明，LongVie 2在长程可控性、时间连贯性与视觉保真度方面均达到领先水平，并支持持续生成长达五分钟的视频，标志着向统一视频世界建模迈出了重要一步。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.13604) | [arXiv](https://arxiv.org/abs/2512.13604)



---

### 6. Finch：面向以电子表格为核心的企业工作流程的财务与会计基准测试

**原文标题：** Finch: Benchmarking Finance & Accounting across Spreadsheet-Centric Enterprise Workflows

**摘要：**
本文提出一个财务与会计基准测试框架（Finch），用于评估人工智能代理在真实企业级专业工作流程中的表现——这些流程交织着数据录入、结构化处理、格式调整、网络搜索、跨文件检索、计算、建模、验证、翻译、可视化及报告生成等任务。Finch的数据源自安然公司（涵盖150名员工的15,000份电子表格和50万封电子邮件）及其他金融机构的真实企业工作环境，完整保留了多模态材料（文本、表格、公式、图表、代码和图像）在真实场景中的杂乱特性，覆盖预算编制、交易执行和资产管理等多个领域。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.13168) | [arXiv](https://arxiv.org/abs/2512.13168)



---

### 7. NL2Repo-Bench：面向代码智能体长周期仓库生成能力的评估基准

**原文标题：** NL2Repo-Bench: Towards Long-Horizon Repository Generation Evaluation of Coding Agents

**摘要：**
代码智能体的最新进展表明，自主软件开发正快速发展，但现有基准测试未能严格评估构建完整软件系统所需的长周期能力。以往评估大多聚焦于局部代码生成、框架补全或短期修复任务，导致智能体能否在现实仓库构建所需的长周期中保持连贯推理、规划与执行能力仍存疑问。为填补这一空白，我们提出NL2Repo-Bench——一个专门用于评估代码智能体长周期仓库生成能力的基准测试。该测试仅提供一个自然语言需求文档和空白工作空间，要求智能体自主完成架构设计、依赖管理、多模块逻辑实现，并最终生成可完整安装的Python库。通过对当前最先进的开源与闭源模型进行实验，我们发现长周期仓库生成任务仍远未解决：即使性能最强的智能体平均测试通过率也低于40%，且极少能完整正确地生成整个仓库。深入分析揭示了长周期任务中的根本性失效模式，包括过早终止、全局一致性丧失、脆弱的跨文件依赖关系，以及在数百个交互步骤中规划能力不足等问题。NL2Repo-Bench为衡量智能体持续自主能力建立了严谨可验证的测试平台，并揭示长周期推理能力是下一代自主代码智能体发展的核心瓶颈。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.12730) | [arXiv](https://arxiv.org/abs/2512.12730)



---

### 8. 无误差线性注意力是免费午餐：基于连续时间动力学的精确解

**原文标题：** Error-Free Linear Attention is a Free Lunch: Exact Solution from Continuous-Time Dynamics

**摘要：**
线性时间注意力与状态空间模型有望解决采用softmax注意力的长上下文语言模型中的二次计算成本瓶颈。本文提出无误差线性注意力——一种数值稳定、完全并行化且具有普适性的增量规则形式化方法。具体而言，我们将在线学习更新过程构建为连续时间动力系统，并证明其精确解不仅可获取，还能以线性时间复杂度和完全并行化方式计算。通过利用动力学矩阵的秩-1结构特性，我们直接推导出等效于无限阶龙格-库塔方法的精确闭式解。该注意力机制理论上不存在误差累积，在保持线性时间复杂度的同时完美捕捉连续动力学特性。通过大量实验验证，EFLA在噪声环境中表现出鲁棒性能，在不引入额外参数的情况下，相比DeltaNet实现了更低的语言建模困惑度和更优的下游基准性能。本研究为构建高保真、可扩展的线性时间注意力模型提供了新的理论基础。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.12602) | [arXiv](https://arxiv.org/abs/2512.12602)



---

### 9. KlingAvatar 2.0 技术报告

**原文标题：** KlingAvatar 2.0 Technical Report

**摘要：**
近年来，虚拟形象视频生成模型取得了显著进展。然而，现有方法在生成长时长高分辨率视频时效率有限，随着视频长度增加，常出现时间漂移、质量下降及提示跟随能力弱等问题。为应对这些挑战，我们提出 KlingAvatar 2.0——一种在空间分辨率与时间维度上实现双重升级的时空级联框架。该框架首先生成捕捉全局语义与运动的低分辨率蓝图视频关键帧，随后通过首尾帧策略将其细化为高分辨率、时间连贯的视频片段，同时保持长视频中平滑的时间过渡。为增强长视频中的跨模态指令融合与对齐，我们引入了由三个模态专用大语言模型专家组成的协同推理指导器。这些专家通过多轮对话推理模态优先级并推断用户潜在意图，将输入转化为详细的故事线。负面指导器进一步优化负面提示以提升指令对齐效果。基于这些组件，我们将框架扩展至支持特定身份的多角色控制。大量实验表明，我们的模型能有效应对高效、多模态对齐的长时长高分辨率视频生成挑战，在视觉清晰度、具有精准唇形同步的逼真唇齿渲染、强身份保持力以及连贯的多模态指令跟随方面均表现出显著提升。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.13313) | [arXiv](https://arxiv.org/abs/2512.13313)



---

### 10. MentraSuite：面向心理健康推理与评估的大语言模型后训练框架

**原文标题：** MentraSuite: Post-Training Large Language Models for Mental Health Reasoning and Assessment

**摘要：**
心理健康障碍影响着全球数亿人口，而网络已成为获取支持、信息与评估的主要渠道。大语言模型能够提供可扩展且易于获取的辅助支持，但当其推理过程存在不完整、不一致或缺乏依据时，在心理健康场景中的部署仍存在风险。现有心理学大语言模型侧重于情感理解或知识复现，却忽视了评估、诊断、干预规划、抽象归纳及验证所需的渐进式、临床导向的推理能力。为解决这些问题，我们提出MentraSuite——一个推进可靠心理健康推理的统一框架。我们构建了MentraBench综合评估基准，涵盖五个核心推理维度、六类任务及13个数据集，从简洁性、连贯性、幻觉规避、任务理解与内部一致性五个维度系统评估任务表现与推理质量。进一步，我们提出通过混合SFT-RL框架后训练优化的Mindora模型，该框架采用不一致性检测奖励机制以确保忠实连贯的推理。为支持训练过程，我们通过创新的推理轨迹生成策略构建高质量轨迹数据，该策略通过筛选困难样本并实施结构化、一致性导向的重写流程，生成简洁可读且均衡优化的推理轨迹。在评估的20个大语言模型中，Mindora在MentraBench上取得最高平均性能，并在推理可靠性方面表现突出，证明了其在复杂心理健康场景中的有效性。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.09636) | [arXiv](https://arxiv.org/abs/2512.09636)



---

### 11. Openpi Comet：2025年BEHAVIOR挑战赛竞赛方案

**原文标题：** Openpi Comet: Competition Solution For 2025 BEHAVIOR Challenge

**摘要：**
2025年BEHAVIOR挑战赛旨在严格追踪物理智能体在仿真环境中解决长程任务的研究进展。BEHAVIOR-1K聚焦于人们最期望机器人协助完成的日常家庭任务，这些任务在真实场景中引入了长程移动操作挑战，从而弥合了当前研究与现实世界、以人为中心的应用之间的差距。本报告介绍了我们在2025年BEHAVIOR挑战赛中获得亚军（与冠军成绩极为接近）的解决方案，其性能显著优于其他参赛方案。我们在π_{0.5}的基础上，通过系统研究训练技术与数据的影响来构建解决方案。经过细致的消融实验，我们证明了预训练与后训练阶段的扩展能力对提升竞赛性能的关键作用。我们总结了实践心得与设计建议，期望能为更广泛的具身智能社区在将强大基础模型适配复杂具身场景时提供可操作的参考。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.10071) | [arXiv](https://arxiv.org/abs/2512.10071)



---

### 12. 基于人类视频视觉-物理对齐的空间感知视觉-语言-动作预训练

**原文标题：** Spatial-Aware VLA Pretraining through Visual-Physical Alignment from Human Videos

**摘要：**
视觉-语言-动作模型通过将视觉感知与语言引导的策略学习相结合，为机器人学习提供了有前景的范式。然而，现有方法大多依赖二维视觉输入在三维物理环境中执行动作，导致感知与动作落地之间存在显著鸿沟。为弥合这一差距，本文提出一种空间感知的视觉-语言-动作预训练范式，通过在预训练阶段显式对齐视觉空间与物理空间，使模型在机器人策略学习前即可获得三维空间理解能力。基于预训练的视觉-语言模型，我们利用大规模人类示范视频提取三维视觉标注与三维动作标注，构建出能够对齐二维视觉观测与三维空间推理的新型监督信号。基于该范式，我们构建了VIPA-VLA模型——采用双编码器架构，通过引入三维视觉编码器将空间感知特征融入语义视觉表征。在下游机器人任务适配中，VIPA-VLA显著提升了二维视觉与三维动作的对接能力，从而产生更具鲁棒性与泛化性的机器人策略。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.13080) | [arXiv](https://arxiv.org/abs/2512.13080)



---

### 13. WebOperator：面向网络环境中自主智能体的动作感知树搜索方法

**原文标题：** WebOperator: Action-Aware Tree Search for Autonomous Agents in Web Environment

**摘要：**
基于大语言模型的智能体通常以贪婪的逐步方式运行，仅根据当前观察选择动作，而不考虑长期后果或替代路径。这种前瞻性的缺失在网络环境中尤为突出——由于环境仅部分可观测（仅限于浏览器可见内容，如DOM和UI元素），单个失误往往需要通过复杂且脆弱的导航操作才能撤销。若缺乏显式的回溯机制，智能体难以纠正错误或系统性地探索替代路径。树搜索方法为此类结构化探索提供了原则性框架，但现有方法缺乏安全回溯机制，容易引发非预期的副作用。同时，这些方法假设所有动作皆可逆，忽略了实际网络任务中不可逆动作的存在，从而降低了其在真实场景中的有效性。为应对这些挑战，我们提出了WebOperator——一种支持可靠回溯与策略性探索的树搜索框架。该方法融合了最佳优先搜索策略，通过奖励估计与安全性评估对动作进行排序，并配备鲁棒的回溯机制，在重放历史路径前验证其可行性，从而避免非预期副作用。为进一步引导探索过程，WebOperator从多样化推理语境中生成候选动作集以保证探索的多样性与鲁棒性，随后通过预执行过滤无效动作、合并语义等价动作的方式，构建高质量动作集合。在WebArena和WebVoyager平台上的实验结果表明了WebOperator的有效性。在WebArena测试中，WebOperator结合gpt-4o实现了54.6%的最优成功率，凸显了策略性前瞻与安全执行相结合的关键优势。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.12692) | [arXiv](https://arxiv.org/abs/2512.12692)



---

### 14. DrivePI：面向统一自动驾驶理解、感知、预测与规划的空间感知四维多模态大语言模型

**原文标题：** DrivePI: Spatial-aware 4D MLLM for Unified Autonomous Driving Understanding, Perception, Prediction and Planning

**摘要：**
尽管多模态大语言模型（MLLMs）已在多个领域展现出强大能力，但其在自动驾驶中生成细粒度三维感知与预测输出方面的应用仍待深入探索。本文提出DrivePI，一种新颖的空间感知四维MLLM，它作为一个统一的视觉-语言-动作框架，同时兼容视觉-动作模型。我们的方法通过端到端优化并行实现空间理解、三维感知（即三维占据）、预测（即占据流）与规划（即动作输出）。为同时获取精确的几何信息与丰富的视觉外观，本方法将点云、多视角图像及语言指令整合于统一的MLLM架构中。我们进一步开发了数据引擎，用于生成面向四维空间理解的文本-占据与文本-流问答对。值得注意的是，仅以0.5B参数的Qwen2.5模型作为MLLM骨干，DrivePI作为单一统一模型在性能上达到或超越了现有视觉-语言-动作模型及专用视觉-动作模型。具体而言，相较于视觉-语言-动作模型，DrivePI在nuScenes-QA数据集上的平均准确率较OpenDriveVLA-7B提升2.5%，在nuScenes数据集上的碰撞率较ORION降低70%（从0.37%降至0.11%）。与专用视觉-动作模型相比，DrivePI在OpenOcc数据集上的三维占据任务中RayIoU指标超越FB-OCC达10.3，在占据流任务中将mAVE从0.591降至0.509，并在nuScenes规划任务中较VAD实现32%的L2误差降低（从0.72米降至0.49米）。代码将在https://github.com/happinesslz/DrivePI 公开。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.12799) | [arXiv](https://arxiv.org/abs/2512.12799)



---

### 15. V-REX：基于问题链的探索性视觉推理基准测试

**原文标题：** V-REX: Benchmarking Exploratory Visual Reasoning via Chain-of-Questions

**摘要：**
尽管许多视觉语言模型（VLM）被开发用于回答定义明确、目标具体的直接问题（如大多数基准测试所示），但在实践中，它们往往难以应对复杂的开放式任务。这类任务通常需要在视觉空间中进行多轮探索与推理。此类视觉思维路径不仅能够像AI侦探一样提供逐步探索与验证，还能对最终答案产生更好的解释。然而，由于中间步骤的探索空间巨大，这些路径的评估极具挑战性。为弥合这一差距，我们开发了一个评估套件——“多步探索视觉推理（V-REX）”，该套件包含一个需要原生多步探索的挑战性视觉推理任务基准以及相应的评估协议。V-REX涵盖了跨领域的丰富应用场景。它将多步探索性推理转化为“问题链”（CoQ），并解构了VLM的两方面能力：（1）规划能力：通过选择一系列探索性问题来分解开放式任务；（2）执行能力：依次回答精心设计的问题链，以收集信息并推导最终答案。通过为每个步骤设计有限的问题与答案选项，V-REX实现了对中间步骤可靠、定量且细粒度的分析。通过对当前最先进的专有及开源VLM进行评估，我们揭示了其一致的能力扩展趋势、规划与执行能力间的显著差异，以及多步探索性推理方面存在的巨大改进空间。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.11995) | [arXiv](https://arxiv.org/abs/2512.11995)



---

### 16. 迈向动态视觉：基于视觉感知的主动视角选择学习

**原文标题：** Toward Ambulatory Vision: Learning Visually-Grounded Active View Selection

**摘要：**
视觉语言模型在视觉问答任务中表现出色，但其仍局限于静态视觉感知，仅能基于单张图像进行推理。相比之下，具身智能体需要动态视觉能力，通过主动移动获取信息量更丰富的观察视角。本文提出视觉感知的主动视角选择任务，该任务仅利用当前图像的视觉信息选择信息量最大的下一观察视角，无需依赖场景记忆或外部知识。为支持该任务，我们构建了包含自动生成的配对查询-目标视角及问答提示的合成数据集。同时提出一种通过监督微调与基于强化学习的策略优化相结合、对预训练视觉语言模型进行微调的框架。该方法在基于视角选择的问答任务中表现出色，并能稳健地泛化至未见过的合成场景与真实场景。此外，将学习得到的视觉感知主动视角选择框架整合至现有基于场景探索的具身问答系统中，可有效提升下游问答任务的准确率。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.13250) | [arXiv](https://arxiv.org/abs/2512.13250)



---

### 17. 基于一致性求解器的图像扩散预览方法

**原文标题：** Image Diffusion Preview with Consistency Solver

**摘要：**
图像扩散模型的缓慢推理过程严重影响了交互式用户体验。为此，我们提出扩散预览这一新范式，通过快速低步数采样生成初步输出供用户评估，仅在预览结果达到要求后才进行全步数精细化处理。现有加速方法（包括免训练求解器与训练后蒸馏技术）难以同时实现高质量预览并确保预览与最终输出的一致性。我们提出一致性求解器——一种源自通用线性多步方法的轻量级可训练高阶求解器，通过强化学习进行优化，以提升预览质量与一致性。实验结果表明，一致性求解器在低步数场景下显著提升了生成质量与一致性，使其成为高效预览-优化工作流程的理想选择。该方法仅需减少47%的步数即可达到与多步DPM-Solver相当的FID分数，同时优于蒸馏基线方法。此外，用户研究表明我们的方法在保持生成质量的同时，将整体用户交互时间减少了近50%。代码已发布于https://github.com/G-U-N/consolver。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.13592) | [arXiv](https://arxiv.org/abs/2512.13592)



---

### 18. VLSA：具备即插即用安全约束层的视觉-语言-动作模型

**原文标题：** VLSA: Vision-Language-Action Models with Plug-and-Play Safety Constraint Layer

**摘要：**
视觉-语言-动作（VLA）模型在多样化机器人操作任务中展现出卓越的泛化能力。然而，由于需同时满足任务执行与安全保障的严格要求（特别是在物理交互中防止潜在碰撞），在非结构化环境中部署此类模型仍面临挑战。本研究提出一种名为AEGIS的视觉-语言-安全动作（VLSA）架构，该架构通过控制屏障函数构建了即插即用的安全约束层。AEGIS可直接与现有VLA模型集成，在保持其原始指令跟随性能的同时，以理论保证提升系统安全性。为评估该架构效能，我们构建了涵盖不同空间复杂度与障碍物干预特征的综合性安全关键基准测试集SafeLIBERO。大量实验证明，本方法显著优于现有先进基线模型。值得注意的是，AEGIS在障碍物规避率上实现59.16%的提升，同时将任务执行成功率大幅提高17.25%。为促进可复现性与未来研究，我们已将代码、模型及基准数据集公开于https://vlsa-aegis.github.io/。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.11891) | [arXiv](https://arxiv.org/abs/2512.11891)



---

### 19. 审美对齐风险同化：图像生成与奖励模型如何强化审美偏见与意识形态“审查”

**原文标题：** Aesthetic Alignment Risks Assimilation: How Image Generation and Reward Models Reinforce Beauty Bias and Ideological "Censorship"

**摘要：**
将图像生成模型过度对齐于广义审美偏好会与用户意图产生冲突，尤其在用户出于艺术或批判目的请求“反审美”输出时。这种对齐机制以开发者中心价值观为优先，损害了用户自主性与审美多元性。我们通过构建广谱审美数据集并评估前沿生成模型与奖励模型来检验这一偏见。研究发现，经过审美对齐的生成模型常默认输出符合传统审美的图像，无法有效响应关于低质量或负面意象的生成指令。关键问题在于，奖励模型会对反审美图像施加惩罚，即使这些图像完全符合用户的显式指令。我们通过图像编辑实验以及与真实抽象艺术作品的对比评估，证实了这一系统性偏见的存在。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.11883) | [arXiv](https://arxiv.org/abs/2512.11883)



---

### 20. 迈向数字人的交互智能

**原文标题：** Towards Interactive Intelligence for Digital Humans

**摘要：**
本文提出“交互智能”这一数字人新范式，其具备人格对齐表达、自适应交互与自主进化能力。为实现该目标，我们构建了Mio（多模态交互全能化身）——一个由五大专业模块构成的端到端框架：思维模块、语音模块、面部动画模块、身体动画模块与渲染模块。该统一架构将认知推理与实时多模态具身化相结合，实现了流畅且连贯的交互体验。此外，我们建立了全新基准体系以系统评估交互智能的核心能力。大量实验表明，本框架在所有评估维度上均优于当前最先进方法。这些成果共同推动数字人从表层模仿迈向智能交互的新阶段。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.13674) | [arXiv](https://arxiv.org/abs/2512.13674)



---

### 21. GenieDrive：基于四维占据栅格引导视频生成的物理感知驾驶世界模型

**原文标题：** GenieDrive: Towards Physics-Aware Driving World Model with 4D Occupancy Guided Video Generation

**摘要：**
物理感知的驾驶世界模型对于驾驶规划、分布外数据合成以及闭环评估至关重要。然而，现有方法通常依赖单一扩散模型直接将驾驶动作映射为视频，这导致学习困难并产生物理不一致的输出。为克服这些挑战，我们提出GenieDrive——一个专为物理感知驾驶视频生成设计的新型框架。我们的方法首先生成四维占据栅格，以此为后续视频生成提供物理信息基础。四维占据栅格包含丰富的物理信息，包括高分辨率三维结构与动态特性。为有效压缩此类高分辨率占据栅格，我们提出一种变分自编码器，将占据栅格编码为潜在三平面表示，将潜在空间尺寸降至先前方法的58%。我们进一步引入互控注意力机制，以精确建模控制信号对占据栅格演化的影响，并以端到端方式联合训练变分自编码器与后续预测模块，从而最大化预测精度。这些设计共同实现了推理速度41 FPS下预测mIoU指标7.2%的提升，且仅使用347万参数。此外，我们在视频生成模型中引入归一化多视角注意力机制，通过四维占据栅格引导生成多视角驾驶视频，使FVD指标降低20.7%，显著提升视频质量。实验表明，GenieDrive能够实现高度可控、多视角一致且具有物理感知的驾驶视频生成。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.12751) | [arXiv](https://arxiv.org/abs/2512.12751)



---

### 22. 表征对齐的关键：全局信息还是空间结构？

**原文标题：** What matters for Representation Alignment: Global Information or Spatial Structure?

**摘要：**
表征对齐（REPA）通过将预训练的强视觉编码器中的表征蒸馏至扩散模型的中间特征，从而指导生成式模型的训练。本文探究一个核心问题：目标表征的哪个方面对生成任务更为关键——是其全局语义信息（例如通过ImageNet-1K准确率衡量），还是其空间结构（即图像块标记之间的成对余弦相似度）？普遍观点认为，作为目标表征，更强的全局语义性能会带来更好的生成效果。为研究此问题，我们首先对27种不同的视觉编码器及不同模型规模进行了大规模实证分析。结果令人意外：驱动目标表征生成性能的主要因素是空间结构，而非全局性能。为进一步探究，我们引入了两种直接改进方法，专门强化空间信息的传递：将REPA中标准的MLP投影层替换为简单卷积层，并为外部表征引入空间归一化层。令人惊讶的是，我们提出的简洁方法（代码实现少于4行），称为iREPA，在多种视觉编码器、模型规模和训练变体（如REPA、REPA-E、Meanflow、JiT等）中均能持续提升REPA的收敛速度。本研究促使我们重新审视表征对齐的基本工作机制，并探索如何利用其改进生成模型的训练。代码与项目页面详见：https://end2end-diffusion.github.io/irepa

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.10794) | [arXiv](https://arxiv.org/abs/2512.10794)



---

### 23. RecTok：沿整流流的重建蒸馏

**原文标题：** RecTok: Reconstruction Distillation along Rectified Flow

**摘要：**
视觉分词器在扩散模型中起着关键作用。潜在空间的维度同时决定了重建保真度和潜在特征的语义表达能力。然而，维度与生成质量之间存在着固有的权衡，这限制了现有方法只能采用低维潜在空间。尽管近期研究利用视觉基础模型来增强视觉分词器的语义并加速收敛，但高维分词器的性能仍逊于低维版本。本研究提出RecTok方法，通过两项关键创新克服高维视觉分词器的局限性：流语义蒸馏和重建对齐蒸馏。我们的核心思路是使流匹配中的前向流具有丰富的语义，并将其作为扩散变换器的训练空间，而非如以往研究那样聚焦于潜在空间。具体而言，我们的方法将视觉基础模型中的语义信息蒸馏至流匹配的前向流轨迹中，并通过引入掩码特征重建损失进一步增强语义表达能力。RecTok在图像重建、生成质量和判别性能方面均表现优异，在有无分类器引导的gFID-50K基准测试中均取得最先进的结果，同时保持了语义丰富的潜在空间结构。此外，随着潜在维度的增加，我们观察到性能的持续提升。代码与模型已发布于https://shi-qingyu.github.io/rectok.github.io。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.13421) | [arXiv](https://arxiv.org/abs/2512.13421)



---

### 24. 文本到图像生成的少步蒸馏：实用指南

**原文标题：** Few-Step Distillation for Text-to-Image Generation: A Practical Guide

**摘要：**
扩散蒸馏技术已显著加速了类别条件图像合成，但其在开放式文本到图像生成中的应用前景仍不明朗。本文首次系统性地研究并比较了在强大的文本到图像教师模型FLUX.1-lite上适配的先进蒸馏技术。通过将现有方法纳入统一框架，我们揭示了从离散类别标签转向自由形式语言提示时出现的关键障碍。除深入的方法论分析外，我们还提供了关于输入缩放、网络架构和超参数的实用指南，并同步开源了实现代码与预训练学生模型。本研究为在实际文本到图像应用中部署快速、高保真且资源高效的扩散生成器奠定了坚实基础。代码发布于github.com/alibaba-damo-academy/T2I-Distill。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.13006) | [arXiv](https://arxiv.org/abs/2512.13006)



---

### 25. AutoMV：一种用于音乐视频生成的自动化多智能体系统

**原文标题：** AutoMV: An Automatic Multi-Agent System for Music Video Generation

**摘要：**
针对完整歌曲的音乐到视频（M2V）生成面临重大挑战。现有方法生成的视频片段短且不连贯，无法将视觉内容与音乐结构、节拍或歌词对齐，并缺乏时间一致性。本文提出AutoMV，一种能够直接从歌曲生成完整音乐视频（MV）的多智能体系统。AutoMV首先应用音乐处理工具提取音乐属性（如结构、人声音轨和时间对齐的歌词），并将这些特征构建为后续智能体的上下文输入。随后，编剧智能体与导演智能体利用这些信息设计简短剧本、在共享外部库中定义角色档案，并指定镜头指令。接着，这些智能体调用图像生成器生成关键帧，并调用不同的视频生成器分别生成“故事”场景和“歌手”场景。验证智能体对输出结果进行评估，通过多智能体协作生成连贯的长篇音乐视频。为评估M2V生成效果，我们进一步提出包含四大高级类别（音乐内容、技术、后期制作、艺术）和十二项细粒度指标的基准体系。应用该基准对商业产品、AutoMV及人工执导的MV进行专家人工评分比较：AutoMV在全部四个类别上均显著优于现有基线方法，缩小了与专业MV的差距。最后，我们探索使用大型多模态模型作为自动MV评估工具；虽然该方法前景可观，但仍落后于人类专家，这为未来研究指明了改进方向。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.12196) | [arXiv](https://arxiv.org/abs/2512.12196)



---

### 26. Flowception：面向视频生成的时序扩展流匹配方法

**原文标题：** Flowception: Temporally Expansive Flow Matching for Video Generation

**摘要：**
本文提出Flowception，一种新颖的非自回归可变长度视频生成框架。该框架通过学习交织离散帧插入与连续帧去噪的概率路径实现视频生成。相较于自回归方法，Flowception通过采样过程中的帧插入机制有效缓解误差累积/漂移问题，该机制可作为处理长期上下文的高效压缩策略。与全序列流方法相比，本方法将训练浮点运算量降低至三分之一，同时更适配局部注意力变体，并能实现视频时长与内容的联合学习。定量实验结果显示，本方法在FVD和VBench指标上均优于自回归与全序列基线模型，定性分析结果进一步验证了其有效性。通过序列中的帧插入与去噪协同学习机制，Flowception可无缝集成图像到视频生成、视频插帧等多样化任务。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.11438) | [arXiv](https://arxiv.org/abs/2512.11438)



---

### 27. CAPTAIN：面向文本到图像扩散模型中记忆缓解的语义特征注入方法

**原文标题：** CAPTAIN: Semantic Feature Injection for Memorization Mitigation in Text-to-Image Diffusion Models

**摘要：**
扩散模型可能无意中复现训练数据，随着此类系统的大规模部署，引发了隐私与版权方面的担忧。现有的推理阶段缓解方法通常通过操纵无分类器引导机制或扰动提示嵌入来实现，但这些方法往往难以在降低记忆复现的同时保持与条件提示的良好对齐。本文提出CAPTAIN，一种无需重新训练即可缓解记忆复现的框架，其通过在去噪过程中直接修改潜在特征来实现。CAPTAIN首先应用基于频率的噪声初始化，以降低去噪早期阶段复制记忆模式的倾向；随后识别特征注入的最佳去噪时间步并定位记忆区域；最后，将非记忆参考图像中语义对齐的特征注入定位的潜在区域，在抑制记忆复现的同时保持提示忠实度与视觉质量。实验表明，相较于基于无分类器引导的基线方法，CAPTAIN在维持与目标提示强对齐的前提下，实现了记忆复现现象的显著减少。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.10655) | [arXiv](https://arxiv.org/abs/2512.10655)



---

### 28. DiffusionBrowser：基于多分支解码器的交互式扩散预览

**原文标题：** DiffusionBrowser: Interactive Diffusion Previews via Multi-Branch Decoders

**摘要：**
视频扩散模型已彻底改变了生成式视频合成技术，但其生成过程存在不精确、速度慢且透明度低的问题——导致用户在生成期间长时间处于未知状态。本研究提出DiffusionBrowser，一个与模型无关的轻量级解码器框架，允许用户在去噪过程中的任意节点（时间步或Transformer模块）交互式生成预览。该模型能以超过实时速度4倍（4秒视频生成时间低于1秒）生成包含RGB与场景本征特征的多模态预览表征，这些预览与最终视频保持外观与运动的一致性。通过训练后的解码器，我们证明了可通过随机性重注入与模态引导在中间噪声步骤中实现交互式生成控制，从而解锁新的调控能力。此外，我们利用学习到的解码器对模型进行系统性探查，揭示了在原本黑箱化的去噪过程中场景、物体及其他细节是如何逐步组合构建的。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.13690) | [arXiv](https://arxiv.org/abs/2512.13690)



---

### 29. LitePT：更轻量且更强大的点云Transformer

**原文标题：** LitePT: Lighter Yet Stronger Point Transformer

**摘要：**
现代三维点云处理神经网络架构同时包含卷积层和注意力模块，但如何最优组合这些模块仍不明确。本文分析了三维点云网络中不同计算模块的作用，发现一种直观规律：卷积层适合在早期高分辨率阶段提取低层次几何特征，此时注意力机制因计算代价高昂而未带来明显优势；而注意力机制在低分辨率的深层网络中能更高效地捕获高层次语义和上下文信息。基于此设计原则，我们提出一种改进的三维点云骨干网络，在浅层采用卷积运算，在深层切换至注意力机制。为避免丢弃冗余卷积层时损失空间布局信息，我们引入了一种无需训练的三维位置编码方法PointROPE。最终构建的LitePT模型与当前最先进的Point Transformer V3相比，参数量减少3.6倍，运行速度提升2倍，内存消耗降低2倍，同时在一系列任务和数据集上达到相当甚至更优的性能。代码与模型已开源：https://github.com/prs-eth/LitePT。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.13689) | [arXiv](https://arxiv.org/abs/2512.13689)



---

### 30. I-Scene：三维实例模型作为隐式可泛化空间学习器

**原文标题：** I-Scene: 3D Instance Models are Implicit Generalizable Spatial Learners

**摘要：**
泛化能力仍是交互式三维场景生成的核心挑战。现有基于学习的方法将空间理解局限于有限场景数据集，限制了其对新布局的泛化能力。本研究通过重构预训练的三维实例生成器，使其作为场景级学习器，将以数据集为中心的监督替换为以模型为中心的空间监督。这种重构释放了生成器的可迁移空间知识，使其能够泛化至未见布局及新颖物体组合。值得注意的是，即使训练场景由随机组合的物体构成，空间推理能力依然能够涌现。这表明生成器的可迁移场景先验为从纯几何线索推断邻近性、支撑关系和对称性提供了丰富的学习信号。我们摒弃广泛使用的规范空间，采用以视角为中心的场景空间构建方法实例化这一思路，从而构建了一个完全前馈、可泛化的场景生成器，能够直接从实例模型中学习空间关系。定量与定性实验表明，三维实例生成器可作为隐式空间学习与推理器，为交互式三维场景理解与生成的基座模型指明了方向。项目页面：https://luling06.github.io/I-Scene-project/

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.13683) | [arXiv](https://arxiv.org/abs/2512.13683)



---

### 31. 面向个性化文本到图像生成的方向性文本反演

**原文标题：** Directional Textual Inversion for Personalized Text-to-Image Generation

**摘要：**
文本反演是一种高效的文本到图像个性化方法，但在复杂提示词上常表现不佳。我们发现其失败根源在于嵌入范数膨胀：学习得到的词元偏离至分布外尺度，导致预归一化Transformer中的提示条件作用退化。实证研究表明，CLIP词元空间中的语义信息主要由方向编码，而膨胀的范数会损害上下文关联性；理论分析揭示，过大的幅度会削弱位置信息并阻碍预归一化模块中的残差更新。为此，我们提出方向性文本反演方法，该方法将嵌入幅度固定于分布内尺度，并通过黎曼随机梯度下降在单位超球面上仅优化方向。我们将方向学习建模为具有冯·米塞斯-费希尔先验的最大后验估计，由此产生恒定方向先验梯度，该方法简单高效。在各类个性化任务中，方向性文本反演在保持主体相似性的同时，较传统文本反演及其变体显著提升了文本忠实度。关键的是，该方法的超球面参数化支持学习概念间的平滑语义连贯插值，这是标准文本反演所不具备的能力。我们的研究结果表明，纯方向优化是实现提示忠实个性化的稳健且可扩展路径。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.13672) | [arXiv](https://arxiv.org/abs/2512.13672)



---

### 32. FoundationMotion：视频空间运动的自动标注与推理

**原文标题：** FoundationMotion: Auto-Labeling and Reasoning about Spatial Movement in Videos

**摘要：**
运动理解是物理推理的基础，它使模型能够推断动态并预测未来状态。然而，现有先进模型在近期的运动基准测试中仍面临困难，主要原因是缺乏大规模、细粒度的运动数据集。现有的运动数据集通常依赖成本高昂的人工标注构建，严重限制了其可扩展性。为应对这一挑战，我们提出了FoundationMotion，一种全自动的数据构建流程，用于创建大规模运动数据集。该方法首先在视频中检测并跟踪物体以提取其运动轨迹，随后利用这些轨迹、视频帧以及大型语言模型（LLLMs）生成关于运动与空间推理的细粒度描述文本和多样化问答对。通过使用该流程构建的数据集，我们对包括NVILA-Video-15B和Qwen2.5-7B在内的开源模型进行微调，在保持其他任务性能的同时，实现了运动理解能力的显著提升。值得注意的是，在多种运动理解数据集和基准测试中，我们的模型表现优于Gemini-2.5 Flash等强大的闭源基线模型以及Qwen2.5-VL-72B等大型开源模型。因此，FoundationMotion为构建细粒度运动数据集提供了一个可扩展的解决方案，能够有效微调多样化模型，从而增强其运动理解与空间推理能力。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.10927) | [arXiv](https://arxiv.org/abs/2512.10927)



---

### 33. START：面向图表理解的空间与文本学习

**原文标题：** START: Spatial and Textual Learning for Chart Understanding

**摘要：**
图表理解对于在多模态大语言模型（MLLMs）中部署至现实场景（如分析科学论文与技术报告）至关重要。与自然图像不同，图表同时包含结构化的视觉布局（空间属性）与底层数据表征（文本属性）——准确理解二者是实现精确、细粒度图表推理的关键。基于此观察，我们提出START（面向图表理解的空间与文本学习）。具体而言，我们引入（1）图表元素定位与（2）图表到代码生成两项任务，以增强MLLM对图表视觉布局与数据细节的双重理解。为促进空间与文本学习，我们构建了START数据集，该数据集通过创新的数据生成流程构建：首先利用MLLM将真实图表图像转换为可执行的图表代码，在还原底层数据表征的同时保持真实图表的视觉分布特征；随后通过大语言模型（LLM）对代码进行演化，以确定捕捉图表视觉结构的关键元素位置，从而解决现有方法难以应对的挑战。为评估模型对图表空间结构的理解能力，我们提出了图表空间理解基准（CS-Bench），填补了当前图表综合理解评估的关键空白。依托空间与文本学习机制，START在不同模型规模与基准测试中均较基线模型取得稳定提升，并以显著优势超越现有最优方法。代码、数据与模型将公开发布。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.07186) | [arXiv](https://arxiv.org/abs/2512.07186)



---

### 34. 无需观测即可推断组合式四维场景

**原文标题：** Inferring Compositional 4D Scenes without Ever Seeing One

**摘要：**
现实世界中的场景通常由多个静态与动态物体组合而成。捕捉这些物体在自然状态下的四维结构、组合方式及时空配置虽然极具研究价值，却也异常困难。现有研究往往依赖特定类别的参数化动态物体模型，每次仅聚焦单个物体进行分析。这种方法不仅受限于已建模的物体类别，还可能导致场景配置不一致。我们提出COM4D（组合式四维场景重建）方法，该方法仅需静态多物体或动态单物体的监督数据，即可一致性地联合预测四维/三维物体的结构及时空配置。我们通过对二维视频输入的空间与时间注意力机制进行精心设计的训练实现这一目标：训练过程被解耦为物体组合学习与视频中单物体动态学习两个独立阶段，从而完全避免对四维组合训练数据的依赖。在推理阶段，我们提出的注意力混合机制能够融合这些独立学习的注意力权重，且无需任何四维组合示例。通过交替进行空间推理与时间推理，COM4D可直接从单目视频中重建包含多个交互物体的完整且持续的四维场景。此外，尽管采用纯数据驱动方法，COM4D在现有四维物体重建与组合式三维重建这两个独立问题上均取得了最先进的研究成果。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.05272) | [arXiv](https://arxiv.org/abs/2512.05272)



---

### 35. FIN-bench-v2：用于评估芬兰语大语言模型的统一鲁棒基准套件

**原文标题：** FIN-bench-v2: A Unified and Robust Benchmark Suite for Evaluating Finnish Large Language Models

**摘要：**
本文介绍FIN-bench-v2——一个用于评估芬兰语大语言模型的统一基准套件。该套件将广泛使用的基准测试芬兰语版本与原始FIN-bench的更新扩展版整合为格式统一的数据集合，涵盖阅读理解、常识推理、情感分析、世界知识和对齐任务中的选择题与生成式任务。所有数据集均转换为HuggingFace Datasets格式，包含完形填空和选择题两种提示模板（每项任务设五种变体），并对GoldenSwag、XED等机器翻译资源进行了人工标注或审核。为筛选鲁棒性任务，我们预训练了一组21.5亿参数的仅解码器模型，通过其学习曲线计算单调性、信噪比、非随机性能及模型排序一致性指标，仅保留满足全部标准的任务。进一步通过指令微调的大规模模型评估，系统刻画了不同任务与提示模板下的性能表现。所有数据集、提示模板及评估配置已通过我们分叉的Language Model Evaluation Harness开源（https://github.com/LumiOpen/lm-evaluation-harness），补充资源发布于独立存储库（https://github.com/TurkuNLP/FIN-bench-v2）。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.13330) | [arXiv](https://arxiv.org/abs/2512.13330)



---

### 36. 状态于词元之上：推理词元的作用特征分析

**原文标题：** State over Tokens: Characterizing the Role of Reasoning Tokens

**摘要：**
大型语言模型（LLM）能够在生成最终答案前输出推理词元，以提升复杂任务的表现。尽管这些序列看似与人类思维过程相似，但实证研究表明它们并非对模型实际推理过程的真实解释。为弥合表象与功能之间的鸿沟，本文提出“状态于词元之上”（SoT）概念框架。SoT将推理词元重新定义为一种外化的计算状态——而非语言叙述，它是模型在无状态生成周期中唯一持续存在的信息载体。这一框架解释了为何这些词元在驱动正确推理的同时，若被视作文本解读则无法成为真实解释，并揭示了此前被忽视的关于此类词元的研究问题。我们认为，要真正理解大型语言模型的运行机制，研究必须超越将推理词元作为文本解读的范式，转而聚焦于将其作为状态进行解码。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.12777) | [arXiv](https://arxiv.org/abs/2512.12777)



---

### 37. CoRe3D：作为三维智能基础的协同推理框架

**原文标题：** CoRe3D: Collaborative Reasoning as a Foundation for 3D Intelligence

**摘要：**
近期大规模多模态模型的研究进展表明，显式推理机制对于提升模型可靠性、可解释性以及跨模态对齐能力具有关键作用。尽管此类以推理为核心的方法已在语言与视觉任务中被证明有效，但其向三维领域的拓展仍处于初级阶段。CoRe3D提出了一种统一的三维理解与生成推理框架，该框架在语义与空间抽象层面进行协同运算，使得从语言中推断出的高层意图能够直接指导底层三维内容的生成。该设计的核心在于一种基于空间锚定的推理表征，它将三维潜在空间分解为局部化区域，使模型能够以组合化、程序化的方式对几何结构进行推理。通过将语义思维链推理与结构化空间推理紧密耦合，CoRe3D生成的三维输出展现出显著的局部一致性，并与语言描述保持高度忠实对齐。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.12768) | [arXiv](https://arxiv.org/abs/2512.12768)



---

### 38. 重新思考专家轨迹在大语言模型后训练中的利用方式

**原文标题：** Rethinking Expert Trajectory Utilization in LLM Post-training

**摘要：**
尽管有效的后训练整合了监督微调（SFT）与强化学习（RL），但利用专家轨迹的最优机制仍未明确。我们提出“可塑性-上限”框架为此领域提供理论依据，将性能分解为基础SFT性能与后续RL可塑性。通过广泛的基准测试，我们确立了“先SFT后RL”的序列化流程为更优标准，克服了同步方法的稳定性缺陷。此外，我们推导出精确的扩展准则：（1）在SFT稳定期或轻度过拟合子阶段转向RL，可通过确保基础SFT性能且不损害RL可塑性，最大化最终性能上限；（2）在“先SFT后RL”的扩展背景下驳斥“少即是多”的观点，我们证明数据规模决定后训练的主要潜力，而轨迹难度则作为性能倍增器；（3）发现最小SFT验证损失可作为筛选专家轨迹的稳健指标，以最大化最终性能上限。本研究为从专家轨迹中提取最大价值提供了可操作的指导原则。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.11470) | [arXiv](https://arxiv.org/abs/2512.11470)



---

### 39. 基于音频世界模型的机器人操作学习

**原文标题：** Learning Robot Manipulation from Audio World Models

**摘要：**
世界模型在机器人学习任务中展现出卓越性能。此类任务往往天然需要多模态推理能力；例如，仅凭视觉信息判断水瓶注水过程可能存在模糊性或信息缺失，因此需要结合音频信号的时序演变进行推理，并考量其底层物理特性与音高模式。本文提出一种生成式潜在流匹配模型，用于预测未来音频观测数据，使系统在融入机器人策略时能够推理长期行为后果。通过两项需要感知真实环境音频或音乐信号的操作任务，我们证明了本系统相较于无前瞻预测的方法具有更优越的性能。我们进一步强调，成功实现此类任务的机器人动作学习不仅依赖于多模态输入，更关键的是对未来音频状态的精准预测——这些状态本质上承载着内在的节奏模式。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.08405) | [arXiv](https://arxiv.org/abs/2512.08405)



---

### 40. 基于细粒度分类的渔业电子监控鱼类视觉重识别研究

**原文标题：** Towards Visual Re-Identification of Fish using Fine-Grained Classification for Electronic Monitoring in Fisheries

**摘要：**
准确的渔业数据对实现有效且可持续的海洋资源管理至关重要。随着电子监控系统近年来的推广应用，当前采集的视频数据量已远超人工审阅的可行范围。本文通过构建基于新型AutoFish数据集（模拟配备传送带的电子监控系统，包含六种形态相似的鱼类）的优化深度学习流程，以应对鱼类自动重识别任务中的挑战。研究表明，结合针对数据集特点设计的归一化图像预处理流程，并采用困难三元组挖掘策略，可显著提升重识别关键指标（R1与mAP@k）。应用该策略后，基于视觉Transformer的Swin-T架构持续优于基于卷积神经网络的ResNet-50模型，最高达到41.65%的mAP@k与90.43%的Rank-1准确率。深入分析表明，主要挑战在于区分同物种中视觉特征相似的个体（种内误差），其中视角不一致问题的影响显著大于局部遮挡。源代码及技术文档详见：https://github.com/msamdk/Fish_Re_Identification.git

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.08400) | [arXiv](https://arxiv.org/abs/2512.08400)



---

### 41. KD-OCT：面向临床级视网膜OCT分类的高效知识蒸馏方法

**原文标题：** KD-OCT: Efficient Knowledge Distillation for Clinical-Grade Retinal OCT Classification

**摘要：**
年龄相关性黄斑变性（AMD）和脉络膜新生血管（CNV）相关疾病是全球范围内导致视力丧失的主要原因，而光学相干断层扫描（OCT）是其早期检测与管理的基石。然而，在临床环境中部署如ConvNeXtV2-Large等先进深度学习模型受到其高计算需求的限制。因此，开发能够在保持高诊断性能的同时实现实时部署的高效模型具有重要意义。本研究提出了一种新颖的知识蒸馏框架KD-OCT，旨在将经过高级数据增强、随机权重平均和焦点损失优化的高性能ConvNeXtV2-Large教师模型，压缩为轻量级的EfficientNet-B2学生模型，用于分类正常、玻璃膜疣及CNV病例。KD-OCT采用实时蒸馏策略，通过结合软教师知识迁移与硬真实标签监督的混合损失函数实现平衡优化。该方法的有效性在Noor眼科医院（NEH）数据集上采用患者级交叉验证进行评估。实验结果表明，KD-OCT在效率与准确性的平衡上优于同类多尺度或特征融合OCT分类器，在模型大小和推理时间大幅降低的同时实现了接近教师模型的性能。尽管经过压缩，学生模型仍超越了大多数现有框架，为AMD筛查的边缘部署提供了便利。代码公开于：https://github.com/erfan-nourbakhsh/KD-OCT。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.09069) | [arXiv](https://arxiv.org/abs/2512.09069)



---


## 🔍 关键词云图
![关键词云图](../images/keywords_wordcloud.png)

## 📈 近期论文趋势
![论文趋势](../images/daily_papers.png)

## 🎙️ 语音播报
- [收听今日论文解读](../audio/2025-12-16_daily_papers.mp3)

## 📱 订阅渠道
- GitHub: [hf-daily-paper-newsletter-chinese](https://github.com/2404589803/hf-daily-paper-newsletter-chinese)