
# <img src="https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo.png" width="30"/> Hugging Face 2025-11-21 论文日报

## 📊 今日论文统计
- 总论文数：22
- 热门领域：Audio, RL, Transformer

## 📝 论文详情


### 1. 首帧：视频内容定制化的关键所在

**原文标题：** First Frame Is the Place to Go for Video Content Customization

**摘要：**
首帧在视频生成模型中究竟扮演着何种角色？传统观点将其视为视频时空序列的起始点，仅作为后续动画生成的种子。本研究提出了一个根本性不同的视角：视频模型隐式地将首帧作为概念记忆缓冲区，用于存储视觉实体以供后续生成过程重复调用。基于这一发现，我们证明仅需20-50个训练样本即可在多样化场景中实现鲁棒且泛化性强的视频内容定制，无需调整模型架构或进行大规模微调。这一研究揭示了视频生成模型在基于参考视频的内容定制方面被长期忽视的强大能力。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.15700) | [arXiv](https://arxiv.org/abs/2511.15700)



---

### 2. V-ReasonBench：面向视频生成模型的统一推理基准测试套件

**原文标题：** V-ReasonBench: Toward Unified Reasoning Benchmark Suite for Video Generation Models

**摘要：**
随着Veo-3等生成式视频模型的最新进展展现出惊人的零样本推理能力，对系统化可靠评估的需求日益增长。我们推出V-ReasonBench这一基准测试框架，旨在从四个关键维度评估视频推理能力：结构化问题解决、空间认知、基于模式的推理和物理动态理解。该基准集成了合成与真实世界图像序列，提供一系列可验证答案的多样化任务，具备可复现、可扩展和无歧义的特点。对六款前沿视频模型的评估显示出明显的维度差异，在结构化、空间、模式化及物理推理方面存在显著波动。我们进一步将视频模型与强图像模型进行对比，分析常见的幻觉生成行为，并研究视频时长对帧序列推理链的影响。总体而言，V-ReasonBench为衡量视频推理能力提供了统一可复现的框架，旨在推动开发具有更可靠、更符合人类思维的推理能力的视频生成模型。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.16668) | [arXiv](https://arxiv.org/abs/2511.16668)



---

### 3. Step-Audio-R1技术报告

**原文标题：** Step-Audio-R1 Technical Report

**摘要：**
推理模型通过扩展的思维链推演在文本和视觉领域取得了显著成功。然而音频语言模型领域始终存在一个令人困惑的现象：模型在极少或无需推理的情况下表现更优，这引发了一个根本性问题——音频智能能否真正受益于深度思考？我们推出Step-Audio-R1，这是首个成功在音频领域解锁推理能力的音频推理模型。通过我们提出的模态锚定推理蒸馏框架，Step-Audio-R1学会了生成与音频相关的推理链，这些推理链能真正植根于声学特征，而非产生脱离实际的推演。我们的模型展现出强大的音频推理能力，在涵盖语音、环境声和音乐的综合音频理解与推理基准测试中，不仅超越了Gemini 2.5 Pro，更达到了与最先进的Gemini 3 Pro相媲美的性能。这些结果表明，当推理过程被恰当锚定时，推理能力可成为跨模态的可迁移能力，从而将扩展推演从音频智能的负担转化为强大优势。通过建立首个成功的音频推理模型，Step-Audio-R1为构建真正跨感官模态深度思考的多模态推理系统开辟了新路径。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.15848) | [arXiv](https://arxiv.org/abs/2511.15848)



---

### 4. 基于多模态基础模型的空间智能规模化研究

**原文标题：** Scaling Spatial Intelligence with Multimodal Foundation Models

**摘要：**
尽管取得了显著进展，多模态基础模型在空间智能方面仍存在明显不足。本研究通过扩展多模态基础模型规模，在SenseNova-SI系列中培育空间智能能力。该系列建立在成熟的多模态基础之上，包括视觉理解模型（如Qwen3-VL和InternVL3）以及统一理解与生成模型（如Bagel）。我们采用系统化方法构建了包含800万多样化数据样本的SenseNova-SI-8M数据集，并基于严格的空间能力分类体系进行数据筛选，旨在建立高性能且稳健的空间智能模型。SenseNova-SI在广泛的空间智能基准测试中展现出卓越性能：VSI-Bench达68.7%，MMSI达43.3%，MindCube达85.6%，ViewSpatial达54.6%，SITE达50.1%，同时保持强大的通用多模态理解能力（如MMBench-En达84.9%）。更重要的是，我们分析了数据规模化的影响，探讨了多样化数据训练带来的涌现泛化能力早期迹象，研究了过拟合和语言捷径风险，提出了空间思维链推理的初步研究，并验证了潜在的下游应用价值。SenseNova-SI是持续发展的研究项目，本报告将定期更新。所有新训练的多模态基础模型均已公开发布，以推动该领域的深入研究。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.13719) | [arXiv](https://arxiv.org/abs/2511.13719)



---

### 5. SAM 3D：图像三维化通用框架

**原文标题：** SAM 3D: 3Dfy Anything in Images

**摘要：**
本文提出SAM 3D——一种基于视觉感知的三维物体重建生成模型，能够通过单张图像预测几何结构、纹理特征和空间布局。该模型在自然场景图像中表现卓越，能有效处理常见遮挡与场景杂乱问题，并充分利用上下文环境中的视觉识别线索。我们通过构建人机协同标注流程，实现了物体形状、纹理及位姿的精准标注，从而提供了规模空前的视觉基三维重建数据集。采用结合合成预训练与真实场景对齐的现代多阶段训练框架，成功突破了三维数据的"资源壁垒"。实验表明，本方法相较现有研究取得显著提升，在真实场景物体与环境的用户偏好测试中获胜率超过5:1。我们将公开核心代码与模型权重，提供在线演示系统，并建立具有挑战性的真实场景三维物体重建新基准。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.16624) | [arXiv](https://arxiv.org/abs/2511.16624)



---

### 6. 视频即答案：基于联合GRPO的下一事件预测与生成方法

**原文标题：** Video-as-Answer: Predict and Generate Next Video Event with Joint-GRPO

**摘要：**
尽管语言模型已在众多现实应用中产生重要影响，视频生成领域仍主要局限于娱乐用途。受视频与生俱来的物理世界信息呈现能力启发（例如仅通过文本描述系领带的教学场景），我们发现将视频拓展为下一代事件预测的新型答案模态存在未被充分利用的潜力，由此提出视频化下一代事件预测任务框架。传统NEP任务通过输入流程性视频与预测性问题来生成文本形式的下一事件预测，而VNEP要求以动态视频作为响应。这种从“讲述”到“呈现”的范式转变，为流程化学习和创意探索提供了更直观、可定制的解答方案。然而，该任务对现有模型仍具挑战性，因其需要理解多模态输入、完成指令条件推理，并生成具备视觉与语义一致性的视频。为此，我们提出VANS模型，通过强化学习将视觉语言模型与视频扩散模型对齐以解决VNEP任务。该模型核心是我们设计的联合GRPO机制，可协调VLM与VDM作为整体运行：基于各自输出的共享奖励信号，该机制在优化VLM生成兼具准确性与可视化友好性描述的同时，引导VDM生成符合描述内容与输入视觉语境的视频。为支撑此学习过程，我们构建了专用于VNEP任务的VANS-Data-100K数据集。在流程性与预测性基准测试上的实验表明，VANS在视频事件预测与可视化方面均达到最先进性能。代码已发布于https://github.com/KlingTeam/VANS。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.16669) | [arXiv](https://arxiv.org/abs/2511.16669)



---

### 7. MiMo-Embodied：跨具身基础模型技术报告

**原文标题：** MiMo-Embodied: X-Embodied Foundation Model Technical Report

**摘要：**
我们开源了MiMo-Embodied——首个成功整合自动驾驶与具身人工智能两大领域并实现最先进性能的跨具身基础模型。该模型在任务规划、功能可供性预测与空间理解等17项具身AI基准测试中创下新纪录，同时在环境感知、状态预测与驾驶规划等12项自动驾驶基准测试中表现卓越。在所有任务中，MiMo-Embodied显著超越了现有开源、闭源及专业基线模型。研究表明，通过多阶段学习、精选数据构建以及思维链/强化学习微调，这两个领域展现出显著的积极迁移效应并形成相互增强。我们详细解析了模型设计与训练方法，以促进后续研究。代码与模型详见：https://github.com/XiaomiMiMo/MiMo-Embodied

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.16518) | [arXiv](https://arxiv.org/abs/2511.16518)



---

### 8. Agent0：通过工具集成推理实现零数据自演进智能体

**原文标题：** Agent0: Unleashing Self-Evolving Agents from Zero Data via Tool-Integrated Reasoning

**摘要：**
基于强化学习训练的大语言模型智能体长期受限于对人类标注数据的依赖，这不仅制约了系统扩展性，也将人工智能束缚在人类知识范畴内。现有自演进框架虽提供替代方案，但通常受限于模型固有能力和单轮交互机制，难以发展涉及工具使用与动态推理的复杂课程体系。我们提出Agent0——完全自主的智能体演进框架，通过多步协同进化与无缝工具集成，无需外部数据即可培育高性能智能体。该框架在相同基座大语言模型上构建两个智能体的共生竞争机制：课程智能体负责提出日益挑战的前沿任务，执行智能体则学习解决这些任务。我们集成外部工具以增强执行者的问题解决能力，这种提升反过来迫使课程智能体构建更具复杂性、工具感知的新型任务。通过此迭代过程，Agent0建立起自我强化的循环体系，持续生成高质量课程。实验表明，Agent0显著提升推理能力，在数学推理基准上使Qwen3-8B-Base模型提升18%，通用推理基准提升24%。代码已开源：https://github.com/aiming-lab/Agent0。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.16043) | [arXiv](https://arxiv.org/abs/2511.16043)



---

### 9. Nemotron Elastic：迈向高效多合一推理大语言模型

**原文标题：** Nemotron Elastic: Towards Efficient Many-in-One Reasoning LLMs

**摘要：**
针对多尺度部署目标训练大语言模型家族的成本极其高昂，需要为不同规模模型分别进行独立训练。近期通过剪枝和知识蒸馏实现的模型压缩方法虽降低了成本，但每个压缩模型仍需消耗数千亿训练令牌。本文提出Nemotron Elastic框架，用于构建面向推理的混合Mamba-Attention架构大语言模型，该框架可在单一父模型中嵌入多个嵌套子模型，每个子模型针对不同部署配置和预算进行优化。这些子模型与父模型共享权重，无需额外训练或微调即可在部署时实现零样本提取。我们通过端到端训练的路由器实现此功能，该路由器与专为推理模型设计的两阶段训练课程紧密耦合。我们还提出了保持Mamba结构约束的分组感知SSM弹性化机制、异构MLP弹性化技术、基于归一化MSE的层重要性评估以改进深度选择，以及支持多预算同步优化的知识蒸馏方法。将Nemotron Elastic应用于Nemotron Nano V2 12B模型，仅使用1100亿训练令牌即可同步生成90亿和60亿参数模型：相比从头训练模型家族可实现360倍以上的成本降低，相较现有压缩技术也有约7倍的提升。所有嵌套模型在准确度上均达到或超越现有最优水平。更重要的是，与其他压缩方法不同，我们的嵌套特性可实现多合一推理模型，其部署内存需求在模型家族数量增加时保持恒定。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.16664) | [arXiv](https://arxiv.org/abs/2511.16664)



---

### 10. 通用基础模型在医院运营场景中的临床适用性不足

**原文标题：** Generalist Foundation Models Are Not Clinical Enough for Hospital Operations

**摘要：**
医院与医疗系统的运营决策直接影响患者流、成本控制及医疗质量。尽管通用文本训练的基础模型在医学知识和对话基准测试中表现优异，但其可能缺乏医疗运营决策所需的专业知识。我们推出Lang1模型系列（参数量1亿至70亿），其预训练语料融合了纽约大学朗格尼医疗中心电子健康记录的800亿临床标记符及互联网来源的6270亿标记符。为在真实场景中严格评估Lang1，我们开发了现实医疗评估基准（ReMedE），该基准基于668,331份电子健康记录笔记，涵盖五大关键任务：30天再入院预测、30天死亡率预测、住院时长预测、共病编码及保险拒赔预测。在零样本场景下，通用模型与专业模型在五项任务中有四项表现不佳（AUROC值36.6%-71.7%），仅死亡率预测例外。经微调后，Lang1-1B模型的表现优于参数量达其70倍的微调通用模型及参数量达其671倍的零样本模型，AUROC指标分别提升3.64%-6.75%和1.66%-23.66%。我们还观察到跨任务扩展效应——多任务联合微调可提升其他任务表现。Lang1-1B能有效迁移至分布外场景，包括其他临床任务及外部医疗系统。研究表明，医院运营的预测能力需要显式监督微调，而基于电子健康记录的领域内预训练可提升微调效率。这些发现印证了新兴观点：专业大语言模型可在特定任务中与通用模型竞争，同时表明构建高效医疗系统人工智能需要结合领域内预训练、监督微调及超越代理基准的真实场景评估。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.13703) | [arXiv](https://arxiv.org/abs/2511.13703)



---

### 11. 边生成边思考：视觉生成过程中的文本推理交错机制

**原文标题：** Thinking-while-Generating: Interleaving Textual Reasoning throughout Visual Generation

**摘要：**
视觉生成领域的最新进展逐渐探索推理能力的整合。现有方法通常在生成前（作为预规划）或生成后（作为后优化）引入文本推理，但缺乏生成过程中实时多模态交互。在本初步研究中，我们提出“边生成边思考”框架，这是首个在视觉生成全过程中实现文本推理协同演进的交错式架构。随着视觉内容的渐进生成，文本推理被交错运用于指导即将生成的局部区域，并对已合成内容进行反思。这种动态交互产生了更具上下文感知能力和语义丰富的视觉输出。为挖掘该框架潜力，我们研究了三类实现策略：零样本提示法、基于自建TwiG-50K数据集的有监督微调法，以及通过定制化TwiG-GRPO策略的强化学习法，每种策略都为交错推理的动态机制提供了独特见解。我们期待这项工作能推动文本推理交错技术助力视觉生成优化的深入研究。代码将发布于：https://github.com/ZiyuGuo99/Thinking-while-Generating。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.16671) | [arXiv](https://arxiv.org/abs/2511.16671)



---

### 12. TurkColBERT：土耳其语信息检索的稠密与延迟交互模型基准测试

**原文标题：** TurkColBERT: A Benchmark of Dense and Late-Interaction Models for Turkish Information Retrieval

**摘要：**
神经信息检索系统在高资源语言中表现卓越，但对土耳其语这类形态丰富、资源相对匮乏的语言研究仍显不足。当前土耳其语信息检索主要采用稠密双编码器，而保留词元级表示以进行细粒度匹配的延迟交互模型尚未得到系统评估。我们推出TurkColBERT——首个针对土耳其语检索的稠密编码器与延迟交互模型综合基准。通过两阶段适配流程：先在土耳其语NLI/STS任务上微调英语和多语言编码器，再使用基于MS MARCO-TR训练的PyLate将其转换为ColBERT式检索器。我们在涵盖科学、金融及论证领域的五个土耳其语BEIR数据集上评估了10个模型。结果显示卓越的参数效率：仅含1.0M参数的colbert-hash-nano-tr比600M参数的turkish-e5-large稠密编码器缩小600倍，同时保持其平均mAP值的71%以上。参数量比稠密编码器小3-5倍的延迟交互模型显著优于后者：ColmmBERT-base-TR在特定领域任务中mAP提升最高达+13.8%。针对生产就绪需求，我们比较了索引算法：MUVERA+重排序比PLAID快3.33倍，并实现+1.7%的相对mAP提升。这使得ColmmBERT-base-TR在MUVERA架构下可实现0.54毫秒查询延迟。我们公开了所有检查点、配置及评估脚本。局限性包括对中等规模数据集（≤5万文档）和翻译基准的依赖，这可能无法完全反映真实场景的土耳其语检索条件；更大规模的MUVERA评估仍有待开展。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.16528) | [arXiv](https://arxiv.org/abs/2511.16528)



---

### 13. SRPO：视觉-语言-动作模型的自参照策略优化方法

**原文标题：** SRPO: Self-Referential Policy Optimization for Vision-Language-Action Models

**摘要：**
视觉-语言-动作模型在机器人操作任务中表现出色，但其性能受限于对专家示范数据的严重依赖，导致存在示范偏差问题。强化学习作为克服这些局限的关键后训练策略，在现有VLA-RL方法（包括基于群体的优化方法）中却受困于严重的奖励稀疏性。仅依赖二元成功指标会浪费失败轨迹中的宝贵信息，导致训练效率低下。为此，我们提出自参照策略优化框架——一种创新的VLA-RL方法。该框架通过利用当前训练批次中模型自身生成的成功轨迹作为参照基准，无需外部示范数据或人工奖励工程，即可为失败尝试分配渐进式奖励。其核心创新在于采用潜在世界表征来稳健度量行为进展：通过世界模型潜在空间获得的压缩化、可迁移编码，无需依赖原始像素或领域特定微调，即可自然捕获跨环境进展模式，实现精准的通用化轨迹比较。在LIBERO基准测试中的实证研究表明，SRPO从监督基线48.9%的成功率起步，仅通过200步强化学习训练就将成功率提升至99.2%，相对改进幅度达103%且无需额外监督。在LIBERO-Plus基准上更实现了167%的性能提升，展现出卓越的鲁棒性。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.15605) | [arXiv](https://arxiv.org/abs/2511.15605)



---

### 14. SAM2S：通过语义长期追踪实现手术视频中的任意目标分割

**原文标题：** SAM2S: Segment Anything in Surgical Videos via Semantic Long-term Tracking

**摘要：**
手术视频分割对计算机辅助手术至关重要，能够实现器械与组织的精确定位与追踪。基于预定义类别的方法存在局限性，而交互式视频目标分割（iVOS）模型（如SAM2）通过提示机制提供了更高灵活性，但在手术场景中因领域差异和长期追踪能力不足面临挑战。为突破这些限制，我们构建了SA-SV——目前规模最大的手术iVOS基准数据集，包含跨越8种手术类型的实例级时空标注（61千帧，1.6千个掩码片段），为长期追踪与零样本泛化研究提供全面支撑。基于该数据集，我们提出SAM2S基础模型，通过三大创新增强SAM2在手术iVOS中的性能：（1）DiveMem可训练多样性记忆机制，实现鲁棒长期追踪；（2）面向器械理解的时序语义学习；（3）抗标注歧义学习以缓解多源数据标注不一致问题。大量实验表明，在SA-SV上进行微调可显著提升性能，SAM2的平均J&F指标较原始版本提升12.99。SAM2S进一步将平均J&F提升至80.42，分别超越原始版与微调版SAM2达17.10和4.11个点，同时保持68 FPS的实时推理速度与强大的零样本泛化能力。代码与数据集将在https://jinlab-imvr.github.io/SAM2S 发布。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.16618) | [arXiv](https://arxiv.org/abs/2511.16618)



---

### 15. NaTex：作为潜在颜色扩散的无缝纹理生成方法

**原文标题：** NaTex: Seamless Texture Generation as Latent Color Diffusion

**摘要：**
本文提出NaTex——一种在三维空间中直接预测纹理颜色的原生纹理生成框架。与现有基于几何条件多视图扩散模型（MVD）合成二维多视图图像进行烘焙的方法不同，NaTex规避了MVD流程的若干固有局限：包括处理需修复的遮挡区域、实现边界处网格与纹理的精确对齐、保持跨视图内容与色彩强度的一致性等难题。NaTex采用创新范式，将纹理视为稠密颜色点云，通过潜在颜色扩散技术解决上述问题。该技术包含几何感知的颜色点云变分自编码器（VAE）与多控制扩散变换器（DiT），全部基于三维数据从头训练，实现纹理重建与生成。为实现精确对齐，我们引入原生几何控制机制，通过位置嵌入与几何潜在变量将直接三维空间信息作为DiT的条件输入。我们协同设计VAE-DiT架构：通过专设几何分支与颜色VAE紧密耦合提取几何潜在变量，提供与纹理保持强对应关系的细粒度表面引导。实验表明，NaTex在纹理一致性与对齐精度上显著优于现有方法，并展现出强大的泛化能力——无需训练或仅需简单调参即可应用于材质生成、纹理优化、部件分割与纹理化等多种下游任务。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.16317) | [arXiv](https://arxiv.org/abs/2511.16317)



---

### 16. PartUV：基于部件划分的三维网格UV展开方法

**原文标题：** PartUV: Part-Based UV Unwrapping of 3D Meshes

**摘要：**
UV展开技术将三维曲面以最小失真度展开为二维平面，通常需要将复杂曲面分解为多个图块。尽管该技术已被广泛研究，现有UV展开方法在处理AI生成网格时仍面临诸多挑战，这类网格通常存在噪点、凹凸不平和条件不良等问题。现有方法往往产生高度碎片化的图块和欠优的边界划分，导致伪影产生并影响下游任务。本文提出PartUV——一种基于部件划分的UV展开流程，在保持低失真度的同时能生成数量显著减少且与部件对齐的图块。该方案基于最新基于学习的部件分解方法PartField构建，通过自上而下的递归框架将高层语义部件分解与新型几何启发式算法相结合，在确保每个图块失真度低于用户设定阈值的同时，最小化图块总数。该流程集成并扩展了参数化与排布算法，包含对非流形与退化网格的专门处理，并采用大规模并行化以提升效率。在涵盖人造物体、CAD模型、AI生成网格和通用形状的四个数据集上的评估表明，PartUV在图块数量和接缝长度指标上优于现有工具与近期神经方法，达到可比拟的失真度，在挑战性网格上呈现高成功率，并支持部件级多贴图排布等新应用。项目主页详见：https://www.zhaoningwang.com/PartUV。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.16659) | [arXiv](https://arxiv.org/abs/2511.16659)



---

### 17. TimeViper：面向高效长视频理解的混合Mamba-Transformer视觉语言模型

**原文标题：** TimeViper: A Hybrid Mamba-Transformer Vision-Language Model for Efficient Long Video Understanding

**摘要：**
本文提出TimeViper混合视觉语言模型，旨在解决长视频理解中的关键挑战。处理长视频既需要高效的模型架构，又需要有效的长时序上下文处理机制。为此，TimeViper采用混合Mamba-Transformer主干网络，将状态空间模型的高效性与注意力机制的强表达能力相结合。通过这种混合设计，我们揭示了视觉到文本的信息聚合现象：随着大语言模型层深增加，信息持续从视觉标记向文本标记流动，导致视觉标记出现严重冗余。基于此发现，我们提出TransV模块——一种在保持多模态理解能力的同时，将视觉标记转移并压缩至指令标记的令牌信息传输机制。该设计使TimeViper能够处理超过10,000帧的时长一小时视频。在多个基准测试上的广泛实验表明，TimeViper在显著扩展处理帧数的同时，仍可与最先进模型保持竞争力。我们进一步分析了Mamba与Transformer层的注意力机制特性，为混合模型的可解释性研究提供了新视角。本工作代表了在开发、解析和压缩混合Mamba-Transformer架构方向上的初步探索。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.16595) | [arXiv](https://arxiv.org/abs/2511.16595)



---

### 18. EntroPIC：基于比例-积分控制的熵稳定方法实现大语言模型的长期稳定训练

**原文标题：** EntroPIC: Towards Stable Long-Term Training of LLMs via Entropy Stabilization with Proportional-Integral Control

**摘要：**
大语言模型的长期训练需要保持稳定的探索性，以防止模型坍缩至次优行为。熵在此过程中具有关键作用，它既能控制探索强度，又有助于避免过早收敛到次优解。然而现有强化学习方法难以维持恰当的熵水平，因为训练过程同时包含正负样本，且不同样本在训练步长中对熵的影响机制存在差异。为此，我们提出基于比例-积分控制的熵稳定方法（EntroPIC），该方法通过动态调整正负样本的损失系数来自适应调节其影响，从而在整个训练过程中实现熵稳定，确保高效探索与稳定进展。我们为同策略与异策略学习场景提供了完整的理论分析，证明EntroPIC能够有效控制大规模语言模型训练中的熵变化。实验结果表明，本方法能成功维持目标熵水平，为大语言模型实现稳定且最优的强化学习训练。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.15248) | [arXiv](https://arxiv.org/abs/2511.15248)



---

### 19. FinTRec：基于Transformer的金融应用统一上下文广告定向与个性化框架

**原文标题：** FinTRec: Transformer Based Unified Contextual Ads Targeting and Personalization for Financial Applications

**摘要：**
基于Transformer的架构虽已广泛应用于序列推荐系统，但其在金融服务领域的实时推荐应用中仍面临独特的实践与建模挑战。这些挑战包括：a) 用户跨数字与实体渠道产生的长周期交互行为（隐式与显式）形成时序异构上下文；b) 多类关联产品并存需协同建模以支持多样化广告位投放与个性化信息流，同时平衡相互竞争的业务目标。我们提出FinTRec这一基于Transformer的框架，旨在解决金融服务领域的这些挑战及运营目标。尽管传统上基于树形模型的方案因可解释性及符合监管要求而更受金融服务领域青睐，但本研究证明FinTRec为转向基于Transformer的架构提供了可行有效的路径。通过历史模拟与线上A/B测试关联分析，我们验证了FinTRec持续优于生产级树形基线模型。该统一架构经过产品适配微调后，可实现跨产品信号共享，降低训练成本与技术负债，同时提升所有产品的离线性能。据我们所知，这是首个在金融服务领域兼顾技术实现与商业考量的统一序列推荐建模综合性研究。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.14865) | [arXiv](https://arxiv.org/abs/2511.14865)



---

### 20. BioBench：超越ImageNet的科学机器学习基准测试蓝图

**原文标题：** BioBench: A Blueprint to Move Beyond ImageNet for Scientific ML Benchmarks

**摘要：**
ImageNet-1K线性探针迁移准确度仍是衡量视觉表征质量的标准指标，但其已无法有效预测科学影像的性能表现。通过对46个现代视觉模型检查点的测试，ImageNet top-1准确度仅能解释生态学任务中34%的方差差异，且在准确度超过75%的模型中存在30%的误判。我们推出BioBench——一个开源的生态视觉基准测试集，旨在捕捉ImageNet所遗漏的关键维度。该基准整合了9项公开发布的应用驱动型任务，涵盖4个生物分类界和6种采集模态（无人机RGB影像、网络视频、显微图像、原位与标本照片、相机陷阱帧），总计310万张图像。通过统一的Python接口可实现数据下载、轻量级分类器与冻结主干网络的适配，并输出类别均衡的宏观F1值（同时提供FishNet和FungiCLEF的领域特定指标）；在A6000 GPU上评估ViT-L模型仅需6小时。BioBench不仅为生态计算机视觉研究提供了新的衡量标准，更为构建跨领域可靠人工智能科学基准建立了可复用的模板方案。代码及预测结果详见https://github.com/samuelstevens/biobench，完整结果可访问https://samuelstevens.me/biobench。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.16315) | [arXiv](https://arxiv.org/abs/2511.16315)



---

### 21. 基于多粒度语言学习的医学视觉理解增强方法

**原文标题：** Boosting Medical Visual Understanding From Multi-Granular Language Learning

**摘要：**
图像-文本预训练技术通过对齐视觉与文本表征，显著提升了视觉理解能力。对比语言-图像预训练（CLIP）在多模态学习中发挥了关键作用。然而该方法专注于单标签单粒度对齐，在医学影像等复杂领域存在局限性——该类图像通常对应多个高层级标签（如疾病类别）且具有不同注释粒度（如诊断描述、临床解释）。为此，我们提出多粒度语言学习（MGLL）框架，该对比学习框架旨在同时提升多标签与跨粒度对齐能力。MGLL通过结构化多标签监督机制，整合多粒度文本描述，并引入带逐点约束的软标签监督来增强对齐效果。该方法采用平滑KL散度确保跨粒度一致性，同时保持即插即用模块的计算效率，可适配各类视觉语言模型。基于构建的大规模多粒度数据集进行预训练，并在多个数据集上验证表明，MGLL在下游任务中性能优于现有先进方法。代码已开源：https://github.com/HUANGLIZI/MGLL。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.15943) | [arXiv](https://arxiv.org/abs/2511.15943)



---

### 22. 基于视觉专家的草拟与优化框架

**原文标题：** Draft and Refine with Visual Experts

**摘要：**
当前的大型视觉语言模型虽展现出强大的多模态推理能力，但由于过度依赖语言先验而非视觉证据，常产生缺乏依据的幻觉响应。这一局限凸显出现有方法缺乏对模型在推理过程中实际使用视觉信息程度的量化评估。我们提出草拟与优化框架，该智能体框架由问题条件化利用度指标驱动。该指标通过构建查询条件化关联图来定位问题相关线索，再通过关联引导的概率掩码测量依赖程度，从而量化模型对视觉证据的依赖水平。在此指标引导下，DnR智能体通过外部视觉专家的针对性反馈优化初始回答。每位专家输出（如检测框或掩码）以视觉线索形式呈现在图像上，通过重新查询模型选择能最大程度提升利用度的响应。该过程无需重新训练或改变架构即可增强视觉基础。在视觉问答和图像描述基准测试中的实验表明，该方法持续提升准确率并降低幻觉现象，证明视觉利用度测量为构建更可解释、证据驱动的多模态智能体系统提供了原理性路径。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.11005) | [arXiv](https://arxiv.org/abs/2511.11005)



---


## 🔍 关键词云图
![关键词云图](../images/keywords_wordcloud.png)

## 📈 近期论文趋势
![论文趋势](../images/daily_papers.png)

## 🎙️ 语音播报
- [收听今日论文解读](../audio/2025-11-21_daily_papers.mp3)

## 📱 订阅渠道
- GitHub: [hf-daily-paper-newsletter-chinese](https://github.com/2404589803/hf-daily-paper-newsletter-chinese)