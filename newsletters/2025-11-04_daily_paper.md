
# <img src="https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo.png" width="30"/> Hugging Face 2025-11-04 论文日报

## 📊 今日论文统计
- 总论文数：30
- 热门领域：LLM, Transformer, RL, GPT

## 📝 论文详情


### 1. 每次激活皆提升：将通用推理模型扩展至万亿级开放语言基础

**原文标题：** Every Activation Boosted: Scaling General Reasoner to 1 Trillion Open
  Language Foundation

**摘要：**
我们推出Ling 2.0系列面向推理的语言基础模型，其核心设计理念是每次激活都能增强推理能力。该系列在统一的混合专家范式下，实现了从百亿到万亿参数规模的扩展，并基于经验缩放法则重点优化了高稀疏性、跨尺度一致性与运行效率。该系列包含三款非思维型（指令式）模型——Ling-mini-2.0、Ling-flash-2.0和Ling-1T，总参数量覆盖160亿至1万亿范围，与稠密模型相比最高可实现7倍激活计算效率。Ling 2.0在模型架构、预训练、后训练及基础设施层面实现了协同创新：采用配备混合张量并行的超稀疏MoE架构以提升推理效率，引入面向推理的数据与训练中程思维链激活机制，实施基于强化学习的精细调优（直接反馈训练、进化式思维链），并实现全量程FP8训练与细粒度异构流水线。在万亿参数规模上，Ling-1T确立了推理精度与计算效率的新帕累托前沿，证明当稀疏激活与推理目标精准对齐时，可实现可扩展的高效智能。整体而言，Ling 2.0为推进未来推理与思维模型（包括基于同底座的Ring系列）提供了连贯、开放且高效的基础架构。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2510.22115) | [arXiv](https://arxiv.org/abs/2510.22115)



---

### 2. 将测试时计算最优缩放推广为可优化图结构

**原文标题：** Generalizing Test-time Compute-optimal Scaling as an Optimizable Graph

**摘要：**
测试时缩放技术通过分配额外计算资源提升大语言模型性能，通常采用并行、串行或混合缩放模式。然而既有研究常预设固定的协作架构（如拓扑结构）和单一模型使用，忽视了最优架构与模型组合会随任务动态变化的特性。为此，我们研究在固定计算预算下寻找测试时缩放中最优模型组合与架构的新课题。本文将其形式化为多LLM协作图优化问题：节点编码角色与LLM模型分配，边捕捉信息流动。该问题面临双重挑战：（1）组合搜索空间呈指数级增长；（2）任务特定需求需定制化设计。我们通过将问题重构为概率图优化，并借助预实验总结出测试时协作图的三项经验性发现。基于这些发现，提出Agent-REINFORCE框架——通过将“采样-梯度-更新”映射为“采样-反馈-更新”流程，其中文本化反馈作为梯度更新概率图，以此高效搜索最优多LLM协作图。实验表明，该方法在样本效率和搜索性能上均优于传统基线与LLM基线，能有效平衡准确率与推理延迟的双重目标，精准识别最优图结构。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.00086) | [arXiv](https://arxiv.org/abs/2511.00086)



---

### 3. 视觉模型在图结构理解中被低估的能力

**原文标题：** The Underappreciated Power of Vision Models for Graph Structural
  Understanding

**摘要：**
图神经网络通过自底向上的消息传递机制运作，这与人类视觉感知存在根本差异——后者能够直觉性地先捕捉全局结构。我们研究了视觉模型在图理解任务中被低估的潜力，发现其在经典基准测试中达到了与图神经网络相当的性能，同时展现出截然不同的学习模式。这些差异化的行为特征，加之现有基准测试将领域特征与拓扑理解相互混淆的局限性，促使我们开发了GraphAbstract基准。该基准通过识别组织原型、检测对称性、感知连接强度及识别关键元素等维度，评估模型像人类一样感知全局图属性的能力。实验结果表明：在需要整体结构理解的任务中，视觉模型显著优于图神经网络，并能在不同图规模下保持泛化能力；而图神经网络则难以进行全局模式抽象，且性能随图规模增大而下降。本研究证实视觉模型具有卓越但未被充分利用的图结构理解能力，特别是在需要全局拓扑感知和尺度不变推理的问题上。这些发现为开发更有效的图基础模型开辟了新途径，尤其适用于以整体模式识别为主导的任务场景。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2510.24788) | [arXiv](https://arxiv.org/abs/2510.24788)



---

### 4. UniLumos：基于物理可信反馈的快速统一图像视频重照明框架

**原文标题：** UniLumos: Fast and Unified Image and Video Relighting with
  Physics-Plausible Feedback

**摘要：**
重照明技术兼具实际应用需求与艺术价值，近期扩散模型通过实现丰富可控的照明效果展现出强大潜力。然而，由于这些模型通常在语义隐空间中进行优化，其邻近性无法保证视觉空间中的物理正确性，常产生过度曝光的高光、错位阴影及错误遮挡等非真实效果。我们提出UniLumos这一面向图像与视频的统一重照明框架，通过将RGB空间的几何反馈引入流匹配主干网络来解决该问题。借助从模型输出中提取的深度图与法向图进行监督，我们显式地将照明效果与场景结构对齐，增强物理可信度。然而，这种反馈机制需要视觉空间中的高质量输出进行监督，使得标准多步去噪过程计算成本高昂。为此，我们采用路径一致性学习，确保即使在少步数训练机制下监督依然有效。为实现细粒度重照明控制与监督，我们设计了结构化的六维标注协议以捕捉核心光照属性。在此基础上提出LumosBench——一个解耦的属性级基准测试集，通过大视觉语言模型评估光照可控性，实现对各个维度重照明精度的自动化可解释评估。大量实验表明，UniLumos在显著提升物理一致性的同时实现了最先进的重照明质量，并为图像和视频重照明带来20倍加速。代码已开源：https://github.com/alibaba-damo-academy/Lumos-Custom。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.01678) | [arXiv](https://arxiv.org/abs/2511.01678)



---

### 5. ROVER：面向全模态生成的逆向跨模态推理基准测试

**原文标题：** ROVER: Benchmarking Reciprocal Cross-Modal Reasoning for Omnimodal
  Generation

**摘要：**
统一多模态模型已成为无缝整合文本与图像理解与生成的强大范式。然而主流评估方法往往孤立对待这些能力，导致多模态输入输出任务的评分主要基于单模态推理——文本基准强调语言推理能力，而视觉基准则关注像素层面的推理结果。为应对测试逆向跨模态推理的迫切需求，我们推出ROVER基准。逆向跨模态推理指运用一种模态来引导、验证或优化另一种模态输出的能力，这是实现统一多模态智能愿景的核心能力。ROVER作为人工标注的基准测试集，明确针对逆向跨模态推理设计，包含基于1876张图像构建的1312个任务，涵盖两个互补场景：视觉生成中的语言增强推理评估模型能否运用语言提示和推理链指导精确的图像合成；语言生成中的视觉增强推理评估模型能否生成中间可视化结果以强化问答任务的推理过程。通过对17个统一模型的实验，我们获得两个关键发现：(i)跨模态推理决定视觉生成质量，交错式模型显著优于非交错式模型，值得注意的是，组合强单模态模型无法实现可比推理能力；(ii)模型在物理推理与符号推理间存在解离现象：能成功解读具象感知概念，却难以构建符号任务的视觉抽象表征，这种缺陷会损害推理性能。这些结果表明逆向跨模态推理是实现真正全模态生成的关键前沿领域。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.01163) | [arXiv](https://arxiv.org/abs/2511.01163)



---

### 6. PHUMA：基于物理的人形机器人运动数据集

**原文标题：** PHUMA: Physically-Grounded Humanoid Locomotion Dataset

**摘要：**
运动模仿是实现人形机器人拟人化运动的重要方法。现有研究多依赖AMASS等高质量动作捕捉数据集，但这些数据稀缺且成本高昂，限制了方法的可扩展性与运动多样性。近期研究尝试通过转换互联网视频扩大数据规模（如Humanoid-X），但这类方法常产生漂浮、穿模、足部滑动等物理失真现象，影响运动稳定性。为此，我们提出PHUMA——基于物理验证的人形机器人运动数据集，该数据集在规模化利用人类视频数据的同时，通过精细化数据清洗与物理约束的重定向技术解决物理失真问题。PHUMA严格遵循关节活动限度，确保足部接地约束，消除足部滑动现象，最终生成兼具大规模与物理可靠性的运动数据。我们在两类场景中评估PHUMA：（1）对自采集测试视频中未见运动的模仿能力；（2）仅基于骨盆引导的路径跟踪任务。实验表明，基于PHUMA训练的策略在两项任务中均显著优于Humanoid-X和AMASS，在多样化运动模仿方面取得突破性进展。项目代码详见：https://davian-robotics.github.io/PHUMA。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2510.26236) | [arXiv](https://arxiv.org/abs/2510.26236)



---

### 7. UniREditBench：基于统一推理机制的图像编辑基准测试框架

**原文标题：** UniREditBench: A Unified Reasoning-based Image Editing Benchmark

**摘要：**
多模态生成模型的最新进展显著推动了图像编辑技术的发展。然而，当前生成模型在处理需要隐式推理的多样化复杂图像编辑任务时仍面临挑战，这凸显了建立系统性评估各类推理场景下模型性能的综合基准的必要性。现有基准主要关注现实场景中的单目标属性转换，虽具实效性却存在两大关键局限：（1）普遍忽视多目标交互关系及涉及人为规则的虚拟场景，而这些要素在实际应用中十分常见；（2）仅依赖文本参照评估生成图像，可能导致系统性误判，尤其在复杂推理场景中。为此，本研究提出UniREditBench——基于统一推理机制的图像编辑评估基准。该基准包含2,700个精心构建的样本，涵盖现实与虚拟两大场景，涉及8个主维度和18个子维度。为提升评估可靠性，我们引入多模态双参照评估机制，为每个样本提供文本与真实图像双重参照。此外，我们设计了自动化多场景数据合成流程，构建了包含高质量思维链推理标注的大规模合成数据集UniREdit-Data-100K。基于该数据集对Bagel模型进行微调后开发的UniREdit-Bagel，在域内与域外场景均展现出显著性能提升。通过对开源与闭源图像编辑模型进行全面基准测试，系统揭示了各类模型在不同维度上的优势与不足。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.01295) | [arXiv](https://arxiv.org/abs/2511.01295)



---

### 8. 基于视频基础模型的物理人工智能世界仿真

**原文标题：** World Simulation with Video Foundation Models for Physical AI

**摘要：**
我们推出新一代物理人工智能世界基础模型[Cosmos-Predict2.5]。该模型基于流式架构构建，在单一模型中实现了文本生成世界、图像生成世界与视频生成世界的统一，并利用物理人工智能视觉语言模型[Cosmos-Reason1]提供更丰富的文本语义基础与更精细的世界仿真控制。通过2亿条精选视频片段训练及基于强化学习的训练后优化，[Cosmos-Predict2.5]在视频质量与指令对齐方面较[Cosmos-Predict1]实现显著提升，同步发布20亿与140亿参数规模的模型。这些能力为机器人与自主系统领域提供了更可靠的合成数据生成、策略评估与闭环仿真方案。我们进一步推出控制网络框架[Cosmos-Transfer2.5]，实现仿真到现实及现实到现实的世界转换。尽管其参数量较[Cosmos-Transfer1]减少3.5倍，仍能提供更高保真度与鲁棒性的长时序视频生成。这些突破共同确立了[Cosmos-Predict2.5]与[Cosmos-Transfer2.5]作为扩展具身智能的通用工具地位。为加速物理人工智能领域的研究部署，我们在NVIDIA开放模型许可下于https://github.com/nvidia-cosmos/cosmos-predict2.5 与 https://github.com/nvidia-cosmos/cosmos-transfer2.5 发布源代码、预训练模型与精选基准测试集。期待这些开放资源能降低技术应用门槛，推动新一代具身智能建设的创新发展。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.00062) | [arXiv](https://arxiv.org/abs/2511.00062)



---

### 9. ToolScope：一种面向视觉引导与长周期工具使用的自主智能框架

**原文标题：** ToolScope: An Agentic Framework for Vision-Guided and Long-Horizon Tool
  Use

**摘要：**
近年来，大型语言模型通过自主集成外部工具进行协同推理，展现出卓越的问题解决能力。然而由于多模态信息固有的复杂性和多样性，使多模态大语言模型在推理过程中灵活高效地利用外部工具仍是一个尚未充分探索的挑战。本研究提出ToolScope——一个将全局规划与局部多模态感知相统一的自主智能框架，通过专用感知工具缓解长周期视觉问答任务中的视觉语境退化问题。该框架包含三大核心组件：全局导航器作为"望远镜"提供高层策略指导；自主执行器通过集成搜索、代码和感知三类外部工具，以迭代方式增强模型的局部感知能力；响应合成器则将推理过程整合为连贯的用户友好型输出。我们在涵盖VQA 2.0、ScienceQA、MAT-Search和MathVista的四个跨领域VQA基准测试中评估ToolScope，该框架展现出强大的泛化能力，在所有数据集上平均性能提升最高达+6.69%。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2510.27363) | [arXiv](https://arxiv.org/abs/2510.27363)



---

### 10. EBT-策略：能量模型解锁涌现的物理推理能力

**原文标题：** EBT-Policy: Energy Unlocks Emergent Physical Reasoning Capabilities

**摘要：**
由生成模型参数化的隐式策略（如扩散策略）已成为机器人领域策略学习与视觉-语言-动作模型的标准范式。然而，这类方法常面临计算成本高、曝光偏差和推理动态不稳定等问题，导致其在分布偏移下出现性能退化。基于能量的模型通过端到端学习能量景观并建模平衡动力学，能有效提升鲁棒性并减少曝光偏差。但传统基于能量的策略参数化方法始终难以实现规模化应用。近期基于能量的Transformer研究虽证明了该类模型在高维空间的扩展能力，但其在物理实体模型中解决核心挑战的潜力尚未被充分探索。我们提出新型能量架构EBT-策略，可有效解决机器人及现实场景中的核心问题。在仿真与真实任务中，EBT-策略持续超越基于扩散的策略，同时显著降低训练与推理计算量。值得注意的是，在某些任务中仅需两次推理迭代即可收敛，相较扩散策略所需的100次迭代减少50倍计算量。更突出的是，EBT-策略展现出前所未有的涌现能力：仅通过行为克隆而无需显式重试训练，即可实现失败动作序列的零样本恢复。通过利用标量能量实现不确定性感知推理与动态计算资源分配，EBT-策略为构建分布偏移下鲁棒、可泛化的机器人行为提供了可行路径。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2510.27545) | [arXiv](https://arxiv.org/abs/2510.27545)



---

### 11. OpenSIR：开放式自我改进推理系统

**原文标题：** OpenSIR: Open-Ended Self-Improving Reasoner

**摘要：**
当前基于强化学习的大语言模型推理方法依赖于带标注数据集的可验证奖励机制，这种模式可能限制模型超越人类水平的能力。虽然自我博弈机制提供了有前景的替代方案，但现有方法仍需依赖外部验证器或无法实现开放式学习。我们提出开放式自我改进推理系统（OpenSIR），该自博弈框架使大语言模型通过交替扮演教师与学生角色，在无外部监督条件下自主生成并解决新型问题。为实现问题创新，OpenSIR同步优化难度系数与多样性指标，通过奖励那些既能形成适度挑战又能探索独特概念的问题生成，最终实现开放式数学发现。从单个简单种子问题出发，OpenSIR显著提升了指令模型的性能：Llama-3.2-3B-Instruct在GSM8K上的得分从73.9提升至78.3，在大学数学数据集上从28.8提升至34.4；Gemma-2-2B-Instruct在GSM8K上的得分从38.5跃升至58.7。分析表明，OpenSIR通过协同进化的师生角色实现开放式学习，这种机制能自适应校准问题难度并驱动多样化探索，使系统实现从基础数学到高等数学的自主演进。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.00602) | [arXiv](https://arxiv.org/abs/2511.00602)



---

### 12. MR-Align：基于元推理认知的大规模推理模型事实性校准框架

**原文标题：** MR-Align: Meta-Reasoning Informed Factuality Alignment for Large
  Reasoning Models

**摘要：**
大规模推理模型在复杂推理任务中展现出强大能力，但其在证据依赖型事实问题上的边际提升有限。我们发现该局限部分源于推理-答案的命中差距：模型在推理过程中能识别正确事实，却未能将其有效整合至最终响应，从而导致事实保真度下降。为解决此问题，我们提出MR-ALIGN——一种基于元推理认知的校准框架，该框架无需依赖外部验证器即可增强事实准确性。MR-ALIGN通过量化模型思维过程中的状态转移概率，构建具有转移感知能力的隐式奖励机制，该机制在原子化思维片段层面强化有效推理模式的同时抑制缺陷推理模式。这种重加权策略将词元级信号重构为概率感知的片段评分，从而促生更有利于事实准确性的连贯推理轨迹。在四个事实问答数据集和一个长文本事实基准上的实证评估表明，MR-ALIGN能持续提升准确性与真实性，同时减少误导性推理。这些结果凸显出：对推理过程本身（而非仅对输出结果）进行校准，是推进大规模推理模型事实性进步的关键所在。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2510.24794) | [arXiv](https://arxiv.org/abs/2510.24794)



---

### 13. LongCat-Flash-Omni技术报告

**原文标题：** LongCat-Flash-Omni Technical Report

**摘要：**
本文介绍LongCat-Flash-Omni——一个拥有5600亿参数的前沿开源全模态模型，在实时音视频交互方面表现卓越。通过采用课程式渐进训练策略，从简单到复杂逐步推进模态序列建模任务，该模型在保持强大单模态能力的同时获得了全面的多模态能力。基于采用高性能零计算专家捷径连接混合专家架构的LongCat-Flash基础，LongCat-Flash-Omni集成了高效的多模态感知与语音重建模块。尽管参数量高达5600亿（激活参数270亿），该模型仍能实现低延迟的实时音视频交互。在训练基础设施方面，我们开发了专门应对大规模多模态训练中数据与模型异构性的模态解耦并行方案，这一创新方法通过维持纯文本训练90%以上的吞吐量，展现出卓越的效率优势。大量评估表明，LongCat-Flash-Omni在开源模型中实现了全模态基准测试的最优性能。此外，该模型在包括文本、图像、视频理解以及音频理解与生成等广泛模态专项任务中均展现出高度竞争力。我们全面阐述了模型架构设计、训练流程与数据策略，并将模型开源以促进学界未来的研究与发展。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.00279) | [arXiv](https://arxiv.org/abs/2511.00279)



---

### 14. 迈向通用视频检索：基于合成多模态金字塔课程学习的视频嵌入泛化方法

**原文标题：** Towards Universal Video Retrieval: Generalizing Video Embedding via
  Synthesized Multimodal Pyramid Curriculum

**摘要：**
当前主流视频检索范式存在结构性偏差，由于狭窄的基准测试催生了相应的有限数据和单任务训练模式，导致系统通用能力受到抑制——这源于缺乏能够定义并要求多维泛化能力的诊断性评估。为突破这一局限，我们提出了评估体系、数据构建与模型设计协同创新的框架。首先创建了通用视频检索基准（UVRB），该测试集包含16个数据集，不仅能衡量性能表现，更能诊断跨任务与跨领域的关键能力缺陷。其次，基于UVRB的诊断指导，我们开发了可扩展的合成工作流，生成155万高质量数据对以充实实现通用性所需的语义空间。最后设计了模态金字塔课程学习策略，通过显式利用异构数据间的潜在关联，训练出通用视频嵌入模型（GVE）。大量实验表明，GVE在UVRB上实现了零样本泛化的最先进性能。特别值得注意的是，我们的分析揭示了流行基准测试对通用能力的预测性较差，且部分相关检索是主流但被长期忽视的应用场景。总体而言，本协同设计框架为突破现有局限、迈向真正通用的视频检索提供了可行路径。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2510.27571) | [arXiv](https://arxiv.org/abs/2510.27571)



---

### 15. TIR-Bench：面向具身图像思维推理的综合评测基准

**原文标题：** TIR-Bench: A Comprehensive Benchmark for Agentic Thinking-with-Images
  Reasoning

**摘要：**
视觉推理的研究前沿正转向OpenAI o3等模型，这类模型能智能创建并操作工具对图像进行转换以解决问题，即思维链中的图像思维推理。然而现有评测基准难以完整评估这种进阶能力。即便是当前图像思维方法最常用的评测基准Visual Search，也仅测试定位与裁剪等基础操作，无法深入探究更复杂、动态且依赖工具的推理能力。我们推出TIR-Bench这一综合评测基准，通过涵盖13类差异化任务来系统评估具身图像思维推理能力，每个任务均要求在思维链中运用创新工具进行图像处理与编辑。我们对22个多模态大语言模型（MLLMs）展开评估，涵盖领先的开源/闭源模型及经过显式工具增强的模型。实验表明TIR-Bench具有普适挑战性，优异表现需依赖真正的图像思维能力。最后，我们通过对照实验比较了直接微调与具身微调的效果差异。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.01833) | [arXiv](https://arxiv.org/abs/2511.01833)



---

### 16. NaviTrace：视觉语言模型具身导航能力评估

**原文标题：** NaviTrace: Evaluating Embodied Navigation of Vision-Language Models

**摘要：**
视觉语言模型在多样化任务与场景中展现出前所未有的性能表现与泛化能力。将这些基础模型集成到机器人导航系统中，为构建通用机器人开辟了新路径。然而，当前对这些模型导航能力的评估仍受限于昂贵的真实环境测试、过度简化的仿真环境以及有限的基准数据集。我们提出NaviTrace——一个高质量的视觉问答基准测试框架，模型在接收指令与具身类型（人类、足式机器人、轮式机器人、自行车）后，需在图像空间输出二维导航轨迹。基于1000个测试场景和3000余条专家轨迹，我们采用新提出的语义感知轨迹评分标准，对八种前沿视觉语言模型进行系统评估。该指标融合动态时间规整距离、目标端点误差，以及基于像素级语义与具身类型的惩罚项，并与人类偏好保持相关性。评估结果表明，由于空间定位与目标识别能力不足，现有模型与人类表现存在系统性差距。NaviTrace为真实环境机器人导航建立了可扩展、可复现的评估基准。测试集与排行榜详见：https://leggedrobotics.github.io/navitrace_webpage/。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2510.26909) | [arXiv](https://arxiv.org/abs/2510.26909)



---

### 17. Trove：面向稠密检索的灵活工具包

**原文标题：** Trove: A Flexible Toolkit for Dense Retrieval

**摘要：**
本文推出Trove——一个易用的开源检索工具包，该工具能在保持灵活性与速度的同时简化研究实验。我们首次引入高效的数据管理功能，仅需少量代码即可动态加载和处理（筛选、选择、转换及合并）检索数据集。这使得用户能够灵活尝试不同数据集配置，无需计算和存储大型数据集的多个副本。Trove具备高度可定制性：除内置多种选项外，还允许用户自由修改现有组件或完全替换为自定义对象。该工具包同时提供支持多节点零代码改动的低代码统一评估流程与困难负例挖掘流程。Trove的数据管理功能可将内存消耗降低至原需求的2.6分之一，其易用的推理流程不会产生额外开销，且推理时间随可用节点数量呈线性下降。最重要的是，我们通过实验证明Trove能有效简化检索实验流程并支持任意定制需求，从而为探索性研究提供有力支持。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.01857) | [arXiv](https://arxiv.org/abs/2511.01857)



---

### 18. 左|,↻,文本{BUS},右|：一个用于评估视觉语言模型理解画谜能力的大规模多样化多模态基准

**原文标题：** left|,circlearrowright,text{BUS},right|: A Large and
  Diverse Multimodal Benchmark for evaluating the ability of Vision-Language
  Models to understand Rebus Puzzles

**摘要：**
理解画谜（画谜通过图片、符号和字母来创造性地表达词语或短语）需要多种技能，如图像识别、认知能力、常识推理、多步推理、基于图像的文字游戏等，这使得即使对当前的视觉语言模型而言也是一项具有挑战性的任务。本文提出左|,↻,文本{BUS},右|，一个包含1,333个英语画谜的大规模多样化基准，涵盖不同艺术风格和难度级别，分布于18个类别（如食物、习语、体育、金融、娱乐等）。我们还提出了RebusDescProgICE，一个模型无关的框架，结合非结构化描述和基于代码的结构化推理，并采用更优的基于推理的上下文示例选择方法，将视觉语言模型在左|,↻,文本{BUS},右|上的性能相比思维链推理分别提升了2.1-4.1%（闭源模型）和20-30%（开源模型）。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.01340) | [arXiv](https://arxiv.org/abs/2511.01340)



---

### 19. 视觉语言模型能否胜任测量任务？基于MeasureBench的视觉测量读数基准测试

**原文标题：** Do Vision-Language Models Measure Up? Benchmarking Visual Measurement
  Reading with MeasureBench

**摘要：**
对人类而言，读取测量仪器读数轻松自如且所需领域专业知识较少，但我们在初步评估中发现，这项任务对当前视觉语言模型（VLMs）仍具有惊人挑战性。本研究推出MeasureBench——一个涵盖真实场景与合成图像的综合性视觉测量读数基准，同时提供可扩展的数据合成流程。该流程能程序化生成具有可控视觉特征的指定类型仪表，实现对指针、刻度、字体、光照及背景干扰等关键细节的大规模参数化调整。对主流专有模型和开源权重的VLMs评估表明，即使最先进的尖端模型在测量读数任务中仍普遍表现不佳。模型存在一致的失败模式：虽然能够识别数字或标签，但无法准确定位指针或对齐标记的关键位置，导致尽管文本推理看似合理却产生巨大数值误差。我们还在合成数据上开展了强化学习的初步实验，发现在域内合成子集上获得鼓舞人心的结果，但对真实场景图像的泛化能力仍有不足。本分析揭示了当前VLMs在细粒度空间定位方面的根本性局限。我们期望该资源能推动视觉基础计算能力与精确空间感知方面的研究进展，弥合数字识别与物理世界测量之间的能力鸿沟。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2510.26865) | [arXiv](https://arxiv.org/abs/2510.26865)



---

### 20. Actial：激活多模态大语言模型的空间推理能力

**原文标题：** Actial: Activate Spatial Reasoning Ability of Multimodal Large Language
  Models

**摘要：**
多模态大语言模型（MLLMs）在二维视觉理解方面取得显著进展，这促使研究者探索其在复杂三维推理任务中的应用潜力。然而，这些模型是否能有效捕捉现实场景中稳健性能所需的精细空间信息——尤其是跨视角一致性这一三维推理的关键要求——仍不明确。针对这一问题，我们提出视角学习任务，旨在评估和提升MLLMs的空间推理能力。我们构建了包含10万组以物体为中心的多视角图像及对应问答对的Viewpoint-100K数据集。该方法采用两阶段微调策略：首先通过监督微调（SFT）将基础空间知识注入基线MLLM，使其在多项任务中取得显著提升；随后基于群组相对策略优化（GRPO）算法，在更广泛的问题集上通过强化学习增强模型泛化能力。此外，我们提出混合冷启动初始化方法，可同步学习视角表征并保持连贯推理思维。实验结果表明，我们的方法显著激活了MLLMs的空间推理能力，在领域内和跨领域推理任务中均表现出性能提升。本研究凸显了培养MLLMs基础空间技能的重要价值，为机器人技术、自主系统和三维场景理解等领域的未来发展提供支撑。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.01618) | [arXiv](https://arxiv.org/abs/2511.01618)



---

### 21. 迈向稳健的数学推理

**原文标题：** Towards Robust Mathematical Reasoning

**摘要：**
确立正确的北极星指标对提升基础模型的数学推理能力至关重要，尤其是考虑到现有评估方法要么过于简单，要么仅关注获取简答题的正确答案。为解决这些问题，我们提出IMO-Bench——一套经顶尖专家团队审核的高级推理基准测试集，专门针对年轻数学家最高殿堂国际数学奥林匹克（IMO）的难度级别设计。IMO-AnswerBench首阶段包含400道可验证简短答案的多样化奥数题目，IMO-ProofBench则针对证明能力进行进阶评估，涵盖基础与高级IMO难度题目，并配备详细评分标准以实现自动化评分。这些基准在我们通过Gemini Deep Think模型（Luong与Lockhart，2025）实现IMO 2025金奖的历史性突破中发挥了关键作用。该模型在IMO-AnswerBench达到80.0%准确率，在高级IMO-ProofBench获得65.7%得分，分别以6.9%和42.4%的显著优势超越非Gemini最佳模型。我们还证实基于Gemini推理构建的自动评分器与人工评估高度相关，并构建包含1000份证明人工评分的IMO-GradingBench，以推动长文本答案自动评估的发展。我们期待IMO-Bench能助力学界推进稳健数学推理研究，相关资源已发布于https://imobench.github.io/。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.01846) | [arXiv](https://arxiv.org/abs/2511.01846)



---

### 22. 基于离线策略影响指导的数据高效RLVR方法

**原文标题：** Data-Efficient RLVR via Off-Policy Influence Guidance

**摘要：**
数据选择在可验证奖励强化学习（RLVR）中对于提升大语言模型（LLM）的推理能力具有关键作用。当前的数据选择方法主要基于启发式策略，缺乏理论保证和普适性。本研究提出一种基于影响函数的理论方法，通过量化每个数据点对学习目标的贡献度来指导数据选择。为解决在线影响评估中策略推演所需的过高计算成本，我们引入了离线策略影响评估方法，利用预先收集的离线轨迹高效近似数据影响力。针对大语言模型的高维梯度特性，采用稀疏随机投影技术降低维度，提升存储和计算效率。基于这些技术，我们开发了具有离线策略影响指导的课程强化学习框架（CROPI）——一种多阶段强化学习框架，能够迭代选择对当前策略最具影响力的数据。在参数量达70亿的模型实验表明，CROPI能显著加速训练过程。在15亿参数模型上，相较于全数据集训练，该方法仅使用每阶段10%的数据就实现了2.66倍的步骤级加速。我们的研究成果充分证明了基于影响度的数据选择方法在高效RLVR领域的巨大潜力。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2510.26491) | [arXiv](https://arxiv.org/abs/2510.26491)



---

### 23. 外科医生距离手术世界模型还有多远？基于专家评估的零样本手术视频生成试点研究

**原文标题：** How Far Are Surgeons from Surgical World Models? A Pilot Study on
  Zero-shot Surgical Video Generation with Expert Assessment

**摘要：**
视频生成基础模型作为模拟物理世界的潜在世界模型，正展现出卓越的能力。然而，其在手术等高风险领域的应用仍存在关键空白——这些领域需要深度专业化的因果知识而非通用物理规则。为系统应对这一挑战，我们提出SurgVeo（首个经专家审定的手术视频生成模型评估基准）及手术合理性金字塔（SPP），这是一个新颖的四层级评估框架，专门用于从基础表象到复杂手术策略的模型输出评估。基于SurgVeo基准，我们让先进的Veo-3模型对腹腔镜与神经外科手术片段进行零样本预测任务，并由四位认证外科专家委员会根据SPP框架对生成视频进行评估。研究结果揭示出显著的“合理性鸿沟”：虽然Veo-3在视觉感知合理性层面表现卓越，但在SPP更高层级（包括器械操作合理性、环境反馈合理性与手术意图合理性）存在严重缺陷。这项研究首次为手术AI领域提供了视觉逼真模仿与因果理解之间存在鸿沟的量化证据。通过SurgVeo与SPP获得的发现，为开发能够驾驭专业现实医疗领域复杂性的未来模型奠定了关键基础并指明了发展路径。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.01775) | [arXiv](https://arxiv.org/abs/2511.01775)



---

### 24. 统一扩散视觉语言行动模型：基于联合离散去噪扩散过程的多模态协同框架

**原文标题：** Unified Diffusion VLA: Vision-Language-Action Model via Joint Discrete
  Denoising Diffusion Process

**摘要：**
视觉-语言-行动模型旨在理解自然语言指令与视觉观测信息，并作为具身智能体执行相应动作。近期研究将未来图像预测纳入感知-行动循环，构建出能协同理解、生成与行动的统一模型——这些模型可同时处理文本与图像输入，并输出未来图像与动作序列。然而，现有方法或依赖外部专家进行模态对齐，或将图像生成与动作预测视为独立过程，限制了任务间直接协同的效益。本研究的核心理念是通过同步去噪过程实现生成与行动的联合优化，在持续充分的视觉引导下，通过迭代优化使动作从初始状态逐步演进。基于此，我们提出统一扩散视觉语言行动模型与联合离散去噪扩散过程：该联合扩散框架将多模态信息融入单一去噪轨迹，使理解、生成与行动形成本质性协同。我们的模型与理论建立于统一的多模态令牌空间与混合注意力机制之上，进一步提出两阶段训练流程及多项推理优化技术以平衡性能与效率。在CALVIN、LIBERO和SimplerEnv等基准测试中，本方法以比自回归模型快4倍的推理速度达到最优性能，并通过深度分析与真实场景验证了其有效性。项目页面详见：https://irpn-eai.github.io/UD-VLA.github.io/。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.01718) | [arXiv](https://arxiv.org/abs/2511.01718)



---

### 25. MotionStream：基于交互式运动控制的实时视频生成技术

**原文标题：** MotionStream: Real-Time Video Generation with Interactive Motion
  Controls

**摘要：**
当前基于运动条件的视频生成方法存在延迟过高（每分钟生成数帧）及非因果性处理的问题，导致无法实现实时交互。本文提出的MotionStream框架在单GPU上实现了亚秒级延迟与最高29 FPS的流式生成。我们首先通过运动控制增强文本到视频模型，使其生成既符合全局文本提示又遵循局部运动引导的高质量视频，但该模型无法实现实时推理。为此，我们通过基于分布匹配蒸馏的自强制学习，将这种双向教师模型蒸馏为因果性学生模型，从而实现实时流式推理。在生成长时间乃至无限时长视频时面临三大挑战：（1）如何弥合有限时长训练与无限时长外推之间的领域差异；（2）如何通过抑制误差累积保持生成质量；（3）如何在不断增长的上下文窗口下维持快速推理而不增加计算成本。本方法的核心在于引入精心设计的滑动窗口因果注意力机制与注意力锚点技术。通过训练过程中结合注意力锚点的自展开策略和KV缓存滚动机制，我们以固定上下文窗口准确模拟推理时的外推过程，实现任意长度视频的恒速生成。该模型在运动跟随性与视频质量方面达到最先进水平，同时将生成速度提升两个数量级，独树一帜地实现了无限长度流式生成。借助MotionStream，用户可通过绘制轨迹、控制摄像机或迁移运动等方式实时观察生成结果，获得真正的交互体验。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.01266) | [arXiv](https://arxiv.org/abs/2511.01266)



---

### 26. UME-R1：探索推理驱动的生成式多模态嵌入方法

**原文标题：** UME-R1: Exploring Reasoning-Driven Generative Multimodal Embeddings

**摘要：**
多模态大语言模型（MLLMs）取得的显著成功推动了多模态嵌入技术的发展，然而现有模型本质上仍属于判别式模型，限制了其从推理驱动生成范式中获益的能力。本研究开创性地探索生成式嵌入方法，将嵌入任务统一于生成范式之中。我们提出UME-R1——一个通用多模态嵌入框架，采用两阶段训练策略：通过冷启动监督微调阶段赋予模型推理能力，使其能同时生成判别式与生成式嵌入；后续强化学习阶段则增强推理能力并进一步优化生成式嵌入质量。这项开创性研究揭示了四个关键发现：1）通过利用MLLMs强大的生成推理能力，生成式嵌入相较传统判别式嵌入实现了显著性能提升；2）判别式与生成式嵌入具有互补性，二者组合的预言机性能远超单一模式；3）强化学习能有效增强生成式嵌入，建立了可扩展的优化范式；4）推理阶段的重采样策略可提升下游任务覆盖度（pass@k），彰显了生成式嵌入在推理时的可扩展潜力。在涵盖视频、图像及视觉文档的78个任务MMEB-V2基准测试中，UME-R1显著优于传统判别式嵌入模型，为构建更具可解释性的推理驱动生成式多模态嵌入奠定了坚实基础。相关代码、模型及数据集将在https://github.com/XMUDeepLIT/UME-R1 公开。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.00405) | [arXiv](https://arxiv.org/abs/2511.00405)



---

### 27. 上下文投票机制：将视觉语言模型转化为零样本排序融合器

**原文标题：** Vote-in-Context: Turning VLMs into Zero-Shot Rank Fusers

**摘要：**
在检索领域，异构检索器的候选结果融合长期面临挑战，尤其在处理视频等复杂多模态数据时更为突出。传统融合技术虽无需训练，但仅依赖排序或分数信号，忽略了候选表征本身。本研究提出上下文投票机制（ViC），这一通用化免训练框架将列表重排序与融合重新定义为视觉语言模型的零样本推理任务。其核心创新在于将内容证据与检索器元数据直接序列化嵌入VLM提示中，使模型能自适应权衡检索器共识与视觉语言内容。我们通过跨模态视频检索这一挑战性任务验证该框架的通用性，为此引入S-Grid紧凑序列化图谱——将每个视频表示为图像网格，并可选择性搭配字幕以实现视频候选的列表推理。实验表明，ViC作为单列表重排序器能显著提升个体检索器精度，作为集成融合器则持续优于CombSUM等强基线方法。在ActivityNet、VATEX等视频检索基准测试中，该框架创造了零样本检索性能的新标杆，展现出处理复杂视觉、时序信号与文本的卓越能力。在零样本设定下，ViC于MSR-VTT数据集获得87.1%（文本到视频）/89.0%（视频到文本）的Recall@1分数，在VATEX数据集实现99.6%（视频到文本）的Recall@1分数，较先前最优基线提升高达+40个Recall@1百分点。本研究提出的ViC为将现代VLM转化为强大零样本重排序与融合器提供了简洁、可复现的高效方案。代码与资源已开源：https://github.com/mohammad2012191/ViC

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.01617) | [arXiv](https://arxiv.org/abs/2511.01617)



---

### 28. GUI-AIMA：基于上下文锚点的图形用户界面多模态注意力对齐方法

**原文标题：** GUI-AIMA: Aligning Intrinsic Multimodal Attention with a Context Anchor
  for GUI Grounding

**摘要：**
图形用户界面（GUI）定位是计算机使用代理的核心功能，其将自然语言指令映射至可操作的屏幕区域。现有基于多模态大语言模型（MLLM）的方法通常将其建模为基于文本的坐标生成任务，但直接从视觉输入生成精确坐标仍存在挑战且计算成本高昂。一种直观的GUI定位实现方式是先筛选与指令相关的视觉区块，再在这些区块内确定精确点击位置。基于通用MLLM的注意力机制中天然蕴含基础定位能力的观察，我们提出GUI-AIMA——一种基于注意力机制且无需坐标监督的高效GUI定位微调框架。该框架通过将MLLM固有的多模态注意力与区块级定位信号对齐，采用多头聚合算法在简化的查询-视觉注意力矩阵上自适应计算多样化用户指令的定位信号。此外，其无坐标特性可轻松集成即插即用的局部放大模块。仅使用8.5万张屏幕截图训练的GUI-AIMA-3B模型展现出卓越的数据效率，验证了轻量训练即可激发MLLM的先天定位能力。该模型在3B参数规模中达到最先进性能，在ScreenSpot-Pro和OSWorld-G数据集上分别取得58.6%和62.2%的平均准确率。项目主页：https://github.com/sjz5202/GUI-AIMA

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.00810) | [arXiv](https://arxiv.org/abs/2511.00810)



---

### 29. 基于秩-2子空间解耦的多步知识交互分析

**原文标题：** Multi-Step Knowledge Interaction Analysis via Rank-2 Subspace
  Disentanglement

**摘要：**
自然语言解释通过结合外部语境知识与存储在模型权重中的参数知识，描述大语言模型的决策机制。理解这两种知识的交互作用是评估自然语言解释可靠性的关键，但目前研究尚不充分。现有工作大多仅考察单步生成（通常是最终答案），并将参数知识与语境知识的交互建模为秩-1子空间中的二元选择，这忽略了更丰富的交互形式（如知识互补或协同支持）。我们提出了一种新颖的秩-2投影子空间方法，能够更精确地解耦参数知识与语境知识的贡献，并首次实现针对长序列自然语言解释的多步知识交互分析。在四个问答数据集和三个开源指令微调大语言模型上的实验表明：秩-1子空间难以有效表征多样化的知识交互，而我们的秩-2模型能准确捕捉这些交互特征。多步分析揭示：产生幻觉的自然语言解释显著偏向参数知识方向，上下文可信的解释则平衡参数知识与语境知识，而思维链提示通过降低对参数知识的依赖使生成结果向语境知识偏移。本研究通过构建更丰富的秩-2子空间解耦框架，为系统研究大语言模型中的多步知识交互提供了首个方法论基础。代码与数据详见：https://github.com/copenlu/pk-ck-knowledge-disentanglement

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.01706) | [arXiv](https://arxiv.org/abs/2511.01706)



---

### 30. AthenaBench：面向网络威胁情报领域大语言模型评估的动态基准框架

**原文标题：** AthenaBench: A Dynamic Benchmark for Evaluating LLMs in Cyber Threat
  Intelligence

**摘要：**
大语言模型在自然语言推理方面展现出强大能力，但其在网络威胁情报领域的应用仍存在局限。CTI分析需要从海量非结构化报告中提炼可操作知识，这一过程本可通过大语言模型显著减轻分析人员工作负荷。CTIBench曾提出用于评估大语言模型在多任务CTI场景下性能的综合基准。本研究通过开发AthenaBench对CTIBench进行扩展，该增强版基准具备以下特征：改进的数据集构建流程、重复数据消除机制、优化的评估指标体系，以及新增专注于风险缓解策略的分析任务。我们评估了十二个大语言模型，包括GPT-5和Gemini-2.5 Pro等尖端专有模型，以及来自LLaMA和Qwen系列的七个开源模型。实验表明，虽然专有模型整体表现更优，但在威胁行为归因和风险缓解等需要深度推理的任务中仍显不足，开源模型则存在更大差距。这些发现揭示了当前大语言模型推理能力的根本局限，强调需要专门针对CTI工作流和自动化需求进行模型定制。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2511.01144) | [arXiv](https://arxiv.org/abs/2511.01144)



---


## 🔍 关键词云图
![关键词云图](../images/keywords_wordcloud.png)

## 📈 近期论文趋势
![论文趋势](../images/daily_papers.png)

## 🎙️ 语音播报
- [收听今日论文解读](../audio/2025-11-04_daily_papers.mp3)

## 📱 订阅渠道
- GitHub: [hf-daily-paper-newsletter-chinese](https://github.com/2404589803/hf-daily-paper-newsletter-chinese)