
# <img src="https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo.png" width="30"/> Hugging Face 2025-12-30 论文日报

## 📊 今日论文统计
- 总论文数：31
- 热门领域：LLM, RL, Transformer, GPT

## 📝 论文详情


### 1. 通过辅助损失实现专家混合模型中专家与路由器的耦合

**原文标题：** Coupling Experts and Routers in Mixture-of-Experts via an Auxiliary Loss

**摘要：**
专家混合模型缺乏明确的约束机制来确保路由器的决策与专家能力充分匹配，这最终限制了模型性能。为解决这一问题，我们提出专家-路由器耦合损失——一种轻量级辅助损失函数，能够将路由器的决策与专家能力紧密耦合。该方法将每个专家的路由器嵌入向量视为分配给该专家的标记的代理标记，并通过专家网络输入扰动后的路由器嵌入向量以获取内部激活值。ERC损失对这些激活值施加双重约束：（1）每个专家对自身代理标记的激活强度必须高于对其他专家代理标记的激活；（2）每个代理标记在其对应专家中激发的激活强度必须高于在其他专家中的激活。这些约束共同确保每个路由器嵌入向量能准确表征对应专家的能力特征，同时使每个专家专注于处理实际被路由至该专家的标记。ERC损失具有计算高效性，仅需处理n²个激活值（n为专家数量），这种固定成本与批次大小无关，而现有耦合方法的计算量通常随标记数量（每批次常达数百万）线性增长。通过对3B至15B参数的MoE-LLMs进行预训练，并在数万亿标记上进行广泛分析，我们验证了ERC损失的有效性。此外，ERC损失能够在训练过程中灵活控制并量化追踪专家专业化程度，为理解专家混合模型提供了重要洞见。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.23447) | [arXiv](https://arxiv.org/abs/2512.23447)



---

### 2. LiveTalk：通过改进的在线策略蒸馏实现实时多模态交互式视频扩散

**原文标题：** LiveTalk: Real-Time Multimodal Interactive Video Diffusion via Improved On-Policy Distillation

**摘要：**
通过扩散模型实现实时视频生成对于构建通用多模态交互式人工智能系统至关重要。然而，扩散模型中通过迭代过程对所有视频帧进行双向注意力的同步去噪阻碍了实时交互。虽然现有的蒸馏方法可以使模型具备自回归特性并减少采样步骤以缓解此问题，但它们主要关注文本到视频生成，导致人机交互不自然且效率较低。本文旨在实现基于多模态上下文（包括文本、图像和音频）的实时交互式视频扩散，以弥合这一差距。鉴于领先的在线策略蒸馏方法Self Forcing在多模态条件下遇到挑战（如闪烁、黑帧和质量下降等视觉伪影），我们研究了一种改进的蒸馏方案，重点关注条件输入的质量以及在线策略优化的初始化和调度策略。在包括HDTF、AVSpeech和CelebV-HQ在内的多模态条件（音频、图像和文本）虚拟形象视频生成基准测试中，我们蒸馏出的模型在推理成本和延迟降低20倍的情况下，达到了与全步骤双向基线模型（规模相似或更大）相当的视觉质量。此外，我们将模型与音频语言模型及长视频推理技术Anchor-Heavy Identity Sinks集成，构建了LiveTalk——一个实时多模态交互式虚拟形象系统。在我们构建的多轮交互基准上进行系统级评估表明，LiveTalk在多轮视频连贯性和内容质量上优于最先进模型（Sora2、Veo3），同时将响应延迟从1-2分钟缩短至实时生成，实现了无缝的人机多模态交互。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.23576) | [arXiv](https://arxiv.org/abs/2512.23576)



---

### 3. Yume-1.5：一种文本可控的交互式世界生成模型

**原文标题：** Yume-1.5: A Text-Controlled Interactive World Generation Model

**摘要：**
近期研究已证明利用扩散模型生成可交互与可探索世界的潜力。然而，现有方法大多面临参数量过大、依赖冗长推理步骤以及历史上下文快速增长等关键挑战，严重限制了实时性能并缺乏文本可控的生成能力。为应对这些挑战，我们提出\method，这是一种新颖的框架，旨在从单张图像或文本提示生成逼真、可交互且连续的世界。\method通过精心设计的框架实现这一目标，支持基于键盘对生成世界进行探索。该框架包含三个核心组件：（1）融合统一上下文压缩与线性注意力的长视频生成框架；（2）基于双向注意力蒸馏与增强文本嵌入方案的实时流式加速策略；（3）用于生成世界事件的文本可控方法。相关代码库已附于补充材料中。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.22096) | [arXiv](https://arxiv.org/abs/2512.22096)



---

### 4. SmartSnap：面向自验证智能体的主动证据寻求范式

**原文标题：** SmartSnap: Proactive Evidence Seeking for Self-Verifying Agents

**摘要：**
智能体强化学习在复杂图形用户界面任务下的自主智能体开发中展现出巨大潜力，但其可扩展性仍受限于任务完成验证的瓶颈。现有验证方法普遍采用被动的事后处理模式：验证器（如基于规则的评分脚本、奖励或评判模型，以及大语言模型即法官）通过分析智能体完整交互轨迹来判断任务成功与否。这种处理包含无关噪声历史的冗长上下文的方式，给验证机制带来挑战，导致高昂成本与低可靠性。为突破此瓶颈，我们提出SmartSnap范式，将验证模式从被动事后验证转变为智能体主动实施的现场自验证。我们设计了一种具有双重使命的新型自验证智能体：其不仅需要完成任务，还需通过精心筛选的快照证据证明任务完成度。基于我们提出的3C原则（完整性、简洁性、创造性），该智能体利用在线环境可访问性，在最小化决定性快照集上进行自验证。此类证据将作为通用大语言模型法官验证器判断有效性与相关性的唯一材料。跨模型系列与规模的移动端任务实验表明，SmartSnap范式能够以可扩展方式训练大语言模型驱动的智能体，为80亿和300亿参数模型分别带来26.08%和16.66%的性能提升。解决方案探索与证据寻求的协同作用，培育出具有高效自验证能力的智能体，其性能可与DeepSeek V3.1及Qwen3-235B-A22B相媲美。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.22322) | [arXiv](https://arxiv.org/abs/2512.22322)



---

### 5. 扩散模型通晓透明度：利用视频扩散模型实现透明物体深度与法向估计

**原文标题：** Diffusion Knows Transparency: Repurposing Video Diffusion for Transparent Object Depth and Normal Estimation

**摘要：**
透明物体始终是感知系统面临的难题：折射、反射和透射现象破坏了立体视觉、飞行时间法以及纯判别式单目深度估计的基本假设，导致深度估计结果存在空洞且时序不稳定。我们的核心发现是，现代视频扩散模型已能合成逼真的透明现象，表明其内部已学习到光学规律。为此，我们构建了TransPhy3D合成视频数据集，包含1.1万段透明/反射场景序列，采用Blender/Cycles渲染引擎生成。场景由精心筛选的类别丰富静态资产与形状多样程序化资产组合而成，并配以玻璃/塑料/金属材质。通过基于物理的光线追踪与OptiX降噪技术，我们同步渲染RGB图像、深度图与法向图。基于大规模视频扩散模型，我们通过轻量级LoRA适配器学习从视频到深度（及法向）的转换器。训练过程中，我们在DiT主干网络中拼接RGB与（含噪）深度潜在特征，并在TransPhy3D与现有逐帧合成数据集上协同训练，从而实现对任意长度输入视频的时序一致预测。所得模型DKT在涉及透明度的真实与合成视频基准测试（ClearPose、DREDS的CatKnown/CatNovel子集及TransPhy3D-Test）中实现了零样本state-of-the-art性能。相较于强图像/视频基线方法，该模型在精度与时序一致性上均有提升，其法向估计变体在ClearPose上取得了最佳视频法向估计结果。紧凑的13亿参数版本运行速度约0.17秒/帧。集成至抓取系统后，DKT的深度估计显著提升了半透明、反射及漫反射表面的抓取成功率，优于现有估计器。这些成果共同印证了一个更广泛的论断：“扩散模型通晓透明度。”生成式视频先验能够被高效、无标注地转化为针对复杂现实操控任务的鲁棒且时序连贯的感知能力。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.23705) | [arXiv](https://arxiv.org/abs/2512.23705)



---

### 6. Stream-DiffVSR：基于自回归扩散的低延迟流式视频超分辨率方法

**原文标题：** Stream-DiffVSR: Low-Latency Streamable Video Super-Resolution via Auto-Regressive Diffusion

**摘要：**
基于扩散模型的视频超分辨率方法虽能实现优异的感知质量，但由于其依赖未来帧及昂贵的多步去噪过程，在延迟敏感场景中仍不实用。本文提出Stream-DiffVSR，一种基于因果条件扩散的高效在线视频超分辨率框架。该方法严格基于历史帧进行处理，融合了四项关键技术：用于快速推理的四步蒸馏去噪器、在潜在去噪过程中注入运动对齐线索的自回归时序引导模块，以及配备时序处理模块的轻量级时序感知解码器，以增强细节与时序一致性。在RTX4090 GPU上，Stream-DiffVSR处理720p帧仅需0.328秒，性能显著优于现有基于扩散的方法。与当前在线状态下的最优方法TMP相比，本方法在提升感知质量（LPIPS指标改善0.095）的同时，将延迟降低了130倍以上。Stream-DiffVSR实现了目前基于扩散的视频超分辨率方法中最低的延迟，将初始处理延迟从超过4600秒缩短至0.328秒，从而成为首款适用于低延迟在线部署的扩散式视频超分辨率方法。项目页面：https://jamichss.github.io/stream-diffvsr-project-page/

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.23709) | [arXiv](https://arxiv.org/abs/2512.23709)



---

### 7. Dream-VL与Dream-VLA：基于扩散语言模型架构的开放视觉-语言及视觉-语言-动作模型

**原文标题：** Dream-VL & Dream-VLA: Open Vision-Language and Vision-Language-Action Models with Diffusion Language Model Backbone

**摘要：**
尽管自回归式大型视觉-语言模型（VLMs）已取得显著成就，但其序列生成特性常限制其在复杂视觉规划与动态机器人控制任务中的效能。本研究探索基于扩散式大语言模型（dLLMs）构建视觉-语言模型的潜力以突破这些局限。我们提出Dream-VL——一种基于扩散架构的开放视觉-语言模型（dVLM），该模型在现有dVLM中实现了最先进的性能表现。Dream-VL在多项基准测试中与基于开放数据训练的一流自回归视觉-语言模型性能相当，且在视觉规划任务中展现出更优潜力。基于Dream-VL，我们进一步提出Dream-VLA——通过开放机器人数据集持续预训练开发的基于dLLM的视觉-语言-动作模型（dVLA）。研究证明，该扩散架构固有的双向特性为视觉-语言-动作任务提供了更优基础，其天然适配动作分块与并行生成机制，使得下游微调收敛速度显著提升。Dream-VLA在LIBERO基准测试中取得97.2%的平均成功率，在SimplerEnv-Bridge和SimplerEnv-Fractal测试中分别实现71.4%与60.5%的综合平均成绩，超越了π_0、GR00T-N1等领先模型。我们同时验证了在不同训练目标的下游任务中，dVLM模型均优于自回归基线模型。现公开发布Dream-VL与Dream-VLA，以推动相关领域的深入研究。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.22615) | [arXiv](https://arxiv.org/abs/2512.22615)



---

### 8. SpotEdit：扩散变换器中的选择性区域编辑

**原文标题：** SpotEdit: Selective Region Editing in Diffusion Transformers

**摘要：**
扩散变换器模型通过编码条件图像并将其整合到变换器层中，显著推动了图像编辑技术的发展。然而，大多数编辑仅涉及修改小范围区域，而现有方法在每一时间步均对所有标记进行统一处理与去噪，这不仅导致冗余计算，还可能使未改变区域的质量下降。这引发了一个根本性问题：在编辑过程中，是否真的有必要重新生成每个区域？为此，我们提出了SpotEdit，一种无需训练的选择性扩散编辑框架，其仅更新被修改的区域。SpotEdit包含两个核心组件：SpotSelector通过感知相似性识别稳定区域，并复用条件图像特征以跳过其计算；SpotFusion则通过动态融合机制，自适应地将这些特征与已编辑标记进行混合，从而保持上下文连贯性与编辑质量。通过减少不必要的计算并确保未修改区域的高保真度，SpotEdit实现了高效且精确的图像编辑。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.22323) | [arXiv](https://arxiv.org/abs/2512.22323)



---

### 9. GRAN-TED：为扩散模型生成鲁棒、对齐且细腻的文本嵌入

**原文标题：** GRAN-TED: Generating Robust, Aligned, and Nuanced Text Embedding for Diffusion Models

**摘要：**
文本编码器是文生图与文生视频扩散模型的核心组件，从根本上决定了生成内容的语义保真度。然而，其发展长期受限于两大挑战：一是缺乏能够可靠预测下游生成性能的高效评估框架，二是难以将预训练语言模型有效适配于视觉合成任务。为解决这些问题，我们提出了GRAN-TED范式，旨在为扩散模型生成鲁棒、对齐且细腻的文本嵌入。我们的贡献包含两方面：首先，我们提出了TED-6K——一个全新的纯文本基准数据集，它通过轻量化的统一适配器实现标准化评估，无需昂贵的端到端模型训练即可高效、稳健地衡量编码器的表征质量。实验表明，TED-6K上的表现与编码器在下游生成任务中的效能具有强相关性。值得注意的是，在我们的实验设置下，相较于从头训练扩散模型，使用TED-6K进行评估的速度提升约750倍。其次，在此验证框架的指导下，我们通过一种新颖的两阶段训练范式开发出更优的文本编码器：第一阶段在多模态大语言模型上进行微调以提升视觉表征能力，随后采用分层加权方法提取更细腻、更有效的文本特征。实验表明，所得GRAN-TED编码器不仅在TED-6K基准上达到最优性能，同时在文生图与文生视频任务中带来显著性能提升。TED-6K数据集及评估代码已公开于：https://anonymous.4open.science/r/GRAN-TED-4FCC/。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.15560) | [arXiv](https://arxiv.org/abs/2512.15560)



---

### 10. Act2Goal：从世界模型到通用目标条件策略

**原文标题：** Act2Goal: From World Model To General Goal-conditioned Policy

**摘要：**
如何以既具表达力又精确的方式定义机器人操作任务，仍是核心挑战。视觉目标提供了紧凑且明确的任务描述，但现有目标条件策略通常因依赖单步动作预测而缺乏对任务进展的显式建模，难以应对长时程操作任务。本文提出Act2Goal，一种通用目标条件操作策略，它将目标条件视觉世界模型与多尺度时序控制相结合。给定当前观测与目标视觉状态，世界模型生成一组捕捉长时程结构的合理中间视觉状态序列。为实现视觉规划向鲁棒执行的转化，我们提出多尺度时序哈希（MSTH）方法，将预测轨迹分解为密集近端帧（用于细粒度闭环控制）和稀疏远端帧（用于保持全局任务一致性）。策略通过端到端交叉注意力机制将这些表征与运动控制耦合，在保持对局部干扰响应能力的同时实现连贯的长时程行为。Act2Goal在新物体、空间布局及环境场景中展现出优异的零样本泛化能力。我们进一步通过基于LoRA微调的后视目标重标注技术实现无奖励在线自适应，使系统能在无外部监督条件下快速自主改进。真实机器人实验表明，在具有挑战性的分布外任务中，Act2Goal通过数分钟自主交互即可将成功率从30%提升至90%，验证了融合多尺度时序控制的目标条件世界模型能为鲁棒的长时程操作提供必要的结构化引导。项目页面：https://act2goal.github.io/

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.23541) | [arXiv](https://arxiv.org/abs/2512.23541)



---

### 11. Web世界模型

**原文标题：** Web World Models

**摘要：**
语言智能体日益需要在能够执行行动、存储记忆并进行学习的持久化世界中运行。现有方法处于两个极端：传统网络框架通过数据库提供可靠但固定的上下文环境，而完全生成式世界模型则以牺牲可控性和工程实用性为代价追求无限环境。本研究提出Web世界模型（WWM）作为中间路径，其世界状态与“物理规则”通过常规网络代码实现以确保逻辑一致性，而大型语言模型则在此结构化潜在状态之上生成上下文、叙事和高层决策。我们在现实网络技术栈上构建了一系列WWM系统，包括基于真实地理的无限旅行地图、虚构星系探索系统、网络级百科全书与叙事世界，以及模拟与游戏化环境。通过这些系统，我们总结出WWM的实用设计原则：将代码定义的规则与模型驱动的想象分离，将潜在状态表示为类型化网络接口，并利用确定性生成实现无限但有结构的探索。研究结果表明，网络技术栈本身可作为世界模型的可扩展基础，实现可控且开放的环境。项目页面：https://github.com/Princeton-AI2-Lab/Web-World-Models。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.23676) | [arXiv](https://arxiv.org/abs/2512.23676)



---

### 12. DiRL：一种高效的扩散语言模型后训练框架

**原文标题：** DiRL: An Efficient Post-Training Framework for Diffusion Language Models

**摘要：**
扩散语言模型已成为自回归模型的有前景的替代方案。尽管近期研究验证了其预训练潜力并提升了推理速度，但扩散语言模型的后训练体系仍不成熟。现有方法存在计算效率低下、训练与推理目标不匹配等问题，严重限制了模型在数学等复杂推理任务上的性能。为此，我们提出DiRL——一种高效的后训练框架，该框架将FlexAttention加速的分块训练与LMDeploy优化的推理紧密集成。该架构实现了简化的在线模型更新循环，支持高效的两阶段后训练（监督微调后接强化学习）。基于此框架，我们提出DiPO，这是首个专为扩散语言模型设计的无偏分组相对策略优化实现。我们通过高质量数学数据训练DiRL-8B-Instruct验证了该方法。实验表明，该模型在扩散语言模型中取得了最先进的数学性能，并在多个基准测试中超越了Qwen2.5系列的同规模模型。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.22234) | [arXiv](https://arxiv.org/abs/2512.22234)



---

### 13. 基于评分准则奖励的人工智能协研员训练方法研究

**原文标题：** Training AI Co-Scientists Using Rubric Rewards

**摘要：**
人工智能协研员正逐渐成为协助人类研究者实现科研目标的重要工具。这类系统的核心功能在于能够根据既定研究目标与约束条件生成科研方案。生成的方案既可用于研究者头脑风暴，亦可在进一步优化后付诸实践。然而，当前语言模型在生成完全符合约束条件与隐性要求的研究方案方面仍面临挑战。本研究探索如何利用海量现有科研文献训练语言模型以生成更优质的研究方案。我们通过从多领域论文中自动提取研究目标及目标导向的评分准则，构建了可扩展的多样化训练语料库。随后采用基于自我评分的强化学习方法训练研究方案生成模型：训练过程中由初始策略的冻结副本担任评分器，评分准则形成的生成器-验证器差异使模型无需外部人工监督即可持续优化。为验证该方法，我们组织机器学习领域专家开展了225小时的人工评估实验。结果显示，针对70%的研究目标，专家更倾向于选择经微调的Qwen3-30B-A3B模型生成的方案；同时84%的自动提取目标评分准则获得专家认可。为评估方法普适性，我们将该方法扩展至医学论文及arXiv预印本的研究目标，并采用前沿模型陪审团进行评估。微调后的模型实现了12-22%的相对性能提升，展现出显著的跨领域泛化能力，即使在医学研究等难以获得执行反馈的问题场景中仍保持有效性。这些发现共同证明，这种可扩展的自动化训练方案具有提升通用人工智能协研员能力的潜力，标志着该领域向前迈出了重要一步。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.23707) | [arXiv](https://arxiv.org/abs/2512.23707)



---

### 14. YOLO-Master：基于专家混合与专用Transformer增强的实时检测加速框架

**原文标题：** YOLO-Master: MOE-Accelerated with Specialized Transformers for Enhanced Real-time Detection

**摘要：**
现有实时目标检测方法普遍采用类YOLO架构，以平衡精度与速度。然而，这些模型依赖静态密集计算机制，对所有输入进行统一处理，导致表征能力与计算资源分配失当——例如在简单场景中过度分配资源，而在复杂场景中分配不足。这种不匹配既造成计算冗余，又导致检测性能欠佳。为突破此局限，本文提出YOLO-Master，一种创新的类YOLO框架，首次为实时目标检测引入实例条件自适应计算机制。该框架通过高效稀疏专家混合模块实现动态计算资源分配，能够依据输入场景复杂度自适应调配资源。其核心在于轻量化动态路由网络，该网络通过多样性增强目标在训练过程中引导专家专业化，促进专家间形成互补性专长。此外，路由网络能自适应学习激活最相关的专家，从而在提升检测性能的同时最小化推理计算开销。在五个大规模基准测试上的综合实验验证了YOLO-Master的优越性：在MS COCO数据集上，该模型以1.62毫秒延迟取得42.4%平均精度，较YOLOv13-N提升0.8%平均精度且推理速度加快17.8%。值得注意的是，该模型在挑战性密集场景中提升尤为显著，同时在常规输入上保持高效性，并维持实时推理速度。代码将公开提供。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.23273) | [arXiv](https://arxiv.org/abs/2512.23273)



---

### 15. Video-BrowseComp：开放网络环境下智能体视频研究基准评测

**原文标题：** Video-BrowseComp: Benchmarking Agentic Video Research on Open Web

**摘要：**
自主智能体的发展正在重塑信息获取方式，从被动检索转向主动、开放式的网络研究。然而，尽管文本与静态多模态智能体已取得快速进展，但在处理网络中最具动态性的模态——视频时，仍存在显著的能力缺口。现有视频基准主要聚焦于被动感知，即向模型提供精选片段而无需外部检索，未能评估智能体视频研究所需的核心能力：主动探查视频时间线、交叉参照分散的证据，以及在开放网络中验证信息主张。为填补这一空白，我们提出Video-BrowseComp基准，该基准包含210个专为开放网络环境下智能体视频推理设计的挑战性问题。与以往基准不同，Video-BrowseComp强制要求模型依赖时序视觉证据，确保答案无法仅通过文本搜索获得，而必须通过导航视频时间线来验证外部信息主张。我们对前沿模型的评估揭示了一个关键瓶颈：即使如GPT-5.1（搭载搜索功能）等先进的搜索增强模型，准确率也仅为15.24%。分析表明，这些模型严重依赖文本代理信息，在元数据丰富的领域（如有剧情摘要的电视剧）表现良好，但在元数据稀缺的动态环境（如体育赛事、游戏实况）中则完全失效，而这些场景恰恰需要视觉 grounding 能力。作为首个开放网络视频研究基准，Video-BrowseComp推动该领域从被动感知迈向主动视频推理的新阶段。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.23044) | [arXiv](https://arxiv.org/abs/2512.23044)



---

### 16. 面向智能信息检索的嵌套式浏览器使用学习

**原文标题：** Nested Browser-Use Learning for Agentic Information Seeking

**摘要：**
信息检索智能体已在广泛而深入的搜索任务中展现出卓越性能，但其工具使用仍主要局限于API级片段检索和基于URL的页面获取，限制了通过真实浏览器访问更丰富信息的能力。尽管完整的浏览器交互可解锁更深层功能，但其细粒度控制与冗长页面内容反馈为ReAct式函数调用智能体带来了显著复杂性。为弥合这一差距，本研究提出嵌套式浏览器使用学习框架，通过引入极简而完整的浏览器操作框架，采用嵌套结构将交互控制与页面探索解耦。该设计在简化智能体推理过程的同时，实现了对深层网络信息的有效获取。在具有挑战性的深度信息检索基准测试中的实证结果表明，该框架在实践中具有显著优势。进一步的深入分析印证了其高效性与灵活性。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.23647) | [arXiv](https://arxiv.org/abs/2512.23647)



---

### 17. OmniAgent：面向全模态音视频理解的音频引导主动感知智能体

**原文标题：** OmniAgent: Audio-Guided Active Perception Agent for Omnimodal Audio-Video Understanding

**摘要：**
全模态大语言模型在统一音频与视觉模态方面已取得显著进展，但其仍常缺乏细粒度的跨模态理解能力，并在多模态对齐方面存在困难。为突破这些限制，本文提出OmniAgent——一种完全由音频引导的主动感知智能体，它通过动态协调专用工具来实现更细粒度的音视频推理。与以往依赖僵化静态工作流和密集帧描述的方法不同，本文展示了从被动响应生成到主动多模态查询的范式转变。OmniAgent采用动态规划机制，按需自主协调工具调用，策略性地将感知注意力集中于任务相关线索。本方法的核心创新在于提出了一种新颖的由粗到细的音频引导感知范式，该范式利用音频线索定位时序事件并引导后续推理过程。在三个音视频理解基准测试上的大量实验评估表明，OmniAgent取得了最先进的性能表现，以10%-20%的准确率优势显著超越当前领先的开源与专有模型。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.23646) | [arXiv](https://arxiv.org/abs/2512.23646)



---

### 18. SurgWorld：通过世界建模从视频中学习手术机器人策略

**原文标题：** SurgWorld: Learning Surgical Robot Policies from Videos via World Modeling

**摘要：**
数据稀缺仍是实现全自主手术机器人的根本障碍。尽管大规模视觉语言动作模型通过利用跨领域的配对视频动作数据，在家庭与工业操作任务中展现出卓越的泛化能力，但手术机器人领域仍缺乏同时包含视觉观测与精确机器人运动学的数据集。相比之下，海量手术视频资源虽然存在，却缺少对应的动作标注，导致模仿学习或视觉语言动作模型训练无法直接应用。本研究旨在通过SurgWorld缓解这一困境——这是一个专为手术物理人工智能设计的世界模型。我们构建了面向手术机器人精细动作描述的“手术动作文本对齐数据集”，并基于最先进的物理人工智能世界模型与该数据集开发了SurgWorld系统。该系统能够生成多样化、可泛化且高度逼真的手术视频。我们首次引入逆动力学模型，从合成手术视频中推断伪运动学数据，从而生成合成的配对视频-动作数据。实验证明，在真实手术机器人平台上，利用这些增强数据训练的视觉语言动作策略模型，其性能显著优于仅使用真实示范数据训练的模型。本研究通过整合未标注手术视频与生成式世界建模，为手术自主技能习得提供了可扩展的路径，从而为开发泛化性强、数据高效的手术机器人策略开辟了新方向。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.23162) | [arXiv](https://arxiv.org/abs/2512.23162)



---

### 19. VL-LN 基准：面向长视野目标导航的主动对话研究

**原文标题：** VL-LN Bench: Towards Long-horizon Goal-oriented Navigation with Active Dialogs

**摘要：**
在现有的大多数具身导航任务中，指令通常被设定为明确且无歧义的，例如指令跟随和物体搜索。在这种理想化设定下，智能体仅需根据视觉与语言输入生成有效的导航输出。然而，现实世界中的导航指令往往具有模糊性和不确定性，要求智能体通过主动对话来消解歧义并推断用户意图。为填补这一研究空白，本文提出交互式实例物体导航任务，该任务不仅要求智能体生成导航动作，还需通过主动对话产生语言输出，从而更贴近实际应用场景。该任务在实例物体导航的基础上，允许智能体在导航过程中以自然语言形式自由向信息源发起咨询。基于此任务，我们构建了视觉语言-语言导航基准，该基准提供了大规模自动生成的数据集及完整的评估协议，用于训练和评估支持对话的导航模型。该基准包含超过4.1万条用于训练的长视野对话增强轨迹，以及配备自动应答能力的评估协议。基于此基准，我们训练了具备对话能力的导航模型，实验表明该模型相较于基线方法取得显著提升。大量实验与分析进一步验证了该基准在推动具身对话导航研究方面的有效性与可靠性。代码与数据集：https://0309hws.github.io/VL-LN.github.io/

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.22342) | [arXiv](https://arxiv.org/abs/2512.22342)



---

### 20. 单子上下文工程

**原文标题：** Monadic Context Engineering

**摘要：**
大型语言模型（LLM）的普及推动了能够进行复杂推理与工具使用的自主智能体的发展。然而，当前智能体架构常采用命令式的临时模式构建，导致系统脆弱，普遍存在状态管理、错误处理和并发控制等方面的困难。本文提出**单子上下文工程（Monadic Context Engineering，MCE）**，这是一种新型架构范式，它利用函子、应用函子与单子的代数结构，为智能体设计提供形式化基础。MCE将智能体工作流视为计算上下文，其中横切关注点（如状态传递、短路错误处理和异步执行）通过抽象的代数性质进行内在管理。我们阐述了单子如何实现稳健的顺序组合，应用函子如何为并行执行提供原则性结构，并重点说明了单子变换器如何系统性地组合这些能力。这种分层方法使开发者能够从简单、可独立验证的组件构建复杂、鲁棒且高效的人工智能体。我们进一步扩展该框架以描述**元智能体**，其利用MCE实现生成式编排，通过元编程动态创建并管理子智能体工作流。项目页面：https://github.com/yifanzhang-pro/monadic-context-engineering。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.22431) | [arXiv](https://arxiv.org/abs/2512.22431)



---

### 21. 智能体系统设计的信息论视角

**原文标题：** An Information Theoretic Perspective on Agentic System Design

**摘要：**
智能体语言模型系统驱动着“深度研究”与“Claude代码”等现代应用，其通过多语言模型架构突破上下文长度限制。尽管表面形态多样，这些系统普遍遵循一种核心模式：较小的“压缩器”语言模型（甚至可在本地运行）将原始上下文提炼为紧凑文本，再由较大的“预测器”语言模型进行处理。尽管此类系统应用广泛，其压缩器-预测器的设计仍多依赖于临时方案，关于二者选择如何影响下游性能缺乏系统性指导。实践中，区分性能提升源于压缩优化还是预测增强需耗费高昂的任务特异性配对实验。我们认为，这些智能体系统设计问题本质上是信息论问题。通过将压缩器语言模型视为含噪信道，我们提出一种基于上下文与其压缩结果间互信息的简易估计量，从而以任务无关的方式量化压缩质量。研究表明，互信息能独立于具体任务强预测下游性能。基于信息论框架，我们在五个数据集与三个模型系列上展开全面实证分析。结果显示：更大规模的压缩器不仅更精确，同时具备更高的标记效率——每标记可传递更多比特信息。例如，70亿参数的Qwen-2.5压缩器相较于其15亿参数版本，准确度提升1.6倍，表达简洁度提高4.6倍，单标记传递的互信息比特量增加5.5倍。跨数据集实验表明，扩展压缩器规模比扩展预测器规模效果更为显著，这使得部署于本地的较大压缩器可与较小的云端预测器协同工作。将上述原则应用于深度研究系统时，仅需30亿参数的本地压缩器即可恢复前沿语言模型99%的准确度，同时将API成本降低至26%。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.21720) | [arXiv](https://arxiv.org/abs/2512.21720)



---

### 22. 分位数渲染：在3D高斯泼溅中高效嵌入高维特征

**原文标题：** Quantile Rendering: Efficiently Embedding High-dimensional Feature on 3D Gaussian Splatting

**摘要：**
计算机视觉领域的最新进展通过利用3D高斯泼溅技术，成功将开放词汇分割扩展至三维领域。尽管取得了这一进展，如何高效渲染开放词汇查询所需的高维特征仍面临重大挑战。现有方法采用码本或特征压缩技术，导致信息损失并降低分割质量。为突破此局限，我们提出分位数渲染——一种针对3D高斯泼溅的新型渲染策略，能在保持高保真度的同时高效处理高维特征。与传统体渲染需对每条光线相交的所有3D高斯进行密集采样不同，分位数渲染仅稀疏采样沿光线方向具有主导影响的高斯分布。通过将分位数渲染集成至可泛化的三维神经网络，我们进一步提出高斯泼溅网络，该网络能以可泛化方式预测高斯特征。在ScanNet和LeRF数据集上的大量实验表明，本框架在512维特征图上实现约43.7倍的加速比并支持实时渲染的同时，其性能优于现有最优方法。相关代码将公开发布。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.20927) | [arXiv](https://arxiv.org/abs/2512.20927)



---

### 23. Robo-Dopamine：面向高精度机器人操作的通用工序奖励建模

**原文标题：** Robo-Dopamine: General Process Reward Modeling for High-Precision Robotic Manipulation

**摘要：**
将强化学习应用于现实世界机器人技术的主要障碍在于有效奖励函数的设计。尽管近期基于学习的工序奖励模型展现出良好前景，但其通常受限于两个根本性缺陷：奖励模型缺乏对操作步骤的感知理解，且依赖单视角感知，导致对细粒度操作进程的评估不可靠；其奖励塑形过程在理论上缺乏严谨性，往往引发误导策略优化的语义陷阱。为解决这些问题，我们提出Dopamine-Reward——一种从多视角输入中学习通用、步骤感知的工序奖励模型的新方法。其核心是我们基于超过3400小时数据集训练的通⽤奖励模型，该模型通过步骤化奖励离散化实现结构化理解，并采用多视角奖励融合克服感知局限。基于Dopamine-Reward，我们进一步提出Dopamine-RL鲁棒策略学习框架，该框架采用理论完备的策略不变奖励塑形方法，使智能体能够在不改变最优策略的前提下利用密集奖励实现高效自我改进，从而从根本上规避语义陷阱。我们在多样化仿真与真实任务中的大量实验验证了本方法的有效性：通用奖励模型在奖励评估准确率上达到最优水平，基于该模型构建的Dopamine-RL显著提升了策略学习效率。例如，当通用奖励模型通过单条专家轨迹以一次性适应方式迁移至新任务后，所得奖励模型能使Dopamine-RL仅通过150次在线采样（约1小时真实机器人交互）便将策略成功率从接近零提升至95%，同时保持优秀的跨任务泛化能力。项目网站：https://robo-dopamine.github.io

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.23703) | [arXiv](https://arxiv.org/abs/2512.23703)



---

### 24. ProGuard：迈向主动式多模态安全防护

**原文标题：** ProGuard: Towards Proactive Multimodal Safeguard

**摘要：**
生成模型的快速发展导致多模态安全风险持续涌现，凸显出现有防御方法的局限性。为应对这些挑战，我们提出ProGuard——一种视觉语言主动防护系统，能够在无需传统被动方法所需模型调整的情况下，识别并描述分布外（OOD）安全风险。我们首先构建了一个包含8.7万个样本的模态平衡数据集，每个样本均通过分层多模态安全分类体系标注了二元安全标签与风险类别，有效缓解了模态偏差，确保了对文本、图像及图文混合输入的一致性审核。基于该数据集，我们通过纯强化学习（RL）训练视觉语言基础模型，以实现高效简洁的推理。为在受控环境中模拟主动安全场景，我们进一步引入OOD安全类别推断任务，并采用基于同义词库的相似性奖励增强RL目标，激励模型为未见过的非安全类别生成简洁描述。实验结果表明，ProGuard在二元安全分类任务上达到与闭源大模型相当的性能，在非安全内容分类任务上显著优于现有开源防护模型。尤为突出的是，ProGuard展现出强大的主动审核能力，将OOD风险检测性能提升52.6%，OOD风险描述性能提升64.8%。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.23573) | [arXiv](https://arxiv.org/abs/2512.23573)



---

### 25. 通过统一导演模型连接想象与音视频生成

**原文标题：** Bridging Your Imagination with Audio-Video Generation via a Unified Director

**摘要：**
现有AI驱动的视频创作系统通常将剧本草拟与关键镜头设计视为两个独立任务：前者依赖大语言模型，后者则依托图像生成模型。我们认为这两个任务应当统一在单一框架内，因为逻辑推理与想象思维均是电影导演的核心素养。本研究提出UniMAGE——一个连接用户提示与结构化剧本的统一导演模型，使非专业用户能借助现有音视频生成模型创作长上下文、多镜头的影片。为实现这一目标，我们采用混合Transformer架构来统一文本与图像生成。为进一步增强叙事逻辑与关键帧一致性，我们提出“先交织后解耦”的训练范式：首先进行交织概念学习，利用交错排列的图文数据促进模型对剧本的深度理解与想象诠释；随后实施解耦专家学习，将剧本写作与关键帧生成解耦，以提升故事叙述的灵活性与创造性。大量实验表明，UniMAGE在开源模型中达到领先水平，能够生成逻辑连贯的视频剧本与视觉一致的关键帧图像。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.23222) | [arXiv](https://arxiv.org/abs/2512.23222)



---

### 26. 节点强制：驯服自回归视频扩散模型以实现实时无限交互式肖像动画

**原文标题：** Knot Forcing: Taming Autoregressive Video Diffusion Models for Real-time Infinite Interactive Portrait Animation

**摘要：**
实时肖像动画对于虚拟助手和实时虚拟形象等交互应用至关重要，它要求具备高视觉保真度、时序连贯性、超低延迟，并能根据参考图像与驱动信号等动态输入进行即时响应控制。基于扩散的模型虽能实现高质量生成，但其非因果特性阻碍了流式部署。因果自回归视频生成方法支持高效的逐帧生成，但存在误差累积、片段边界处运动不连续以及长期一致性退化等问题。本研究提出一种名为“节点强制”的新型流式框架，用于实时肖像动画，通过三项关键设计应对上述挑战：（1）采用分块生成策略，通过缓存参考图像的键值状态实现全局身份保持，并利用滑动窗口注意力进行局部时序建模；（2）设计时序节点模块，通过重叠相邻片段并借助图像到视频的条件传递时空线索，以平滑片段间的运动过渡；（3）引入“超前运行”机制，在推理过程中动态更新参考帧的时序坐标，使其语义语境始终领先于当前生成帧，从而维持长期连贯性。节点强制能够在消费级GPU上实现高保真、时序连贯且可交互的无限序列肖像动画，达到实时性能并保持出色的视觉稳定性。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.21734) | [arXiv](https://arxiv.org/abs/2512.21734)



---

### 27. KernelEvolve：面向Meta异构AI加速器的可扩展智能内核编码框架

**原文标题：** KernelEvolve: Scaling Agentic Kernel Coding for Heterogeneous AI Accelerators at Meta

**摘要：**
实现深度学习推荐模型（DLRM）训练与推理的高效快速至关重要，但这面临三大系统挑战：模型架构多样性、内核原语多样性，以及硬件代际与架构异构性。本文提出KernelEvolve——一种智能内核编码框架，以应对DLRM的大规模异构性挑战。该框架以内核规范为输入，旨在自动化实现跨异构硬件架构的推荐模型内核生成与优化过程。其通过多层级编程抽象（从Triton和CuTe领域专用语言到底层硬件无关语言）覆盖完整的软硬件优化栈。内核优化过程被建模为基于图的搜索，结合选择策略、通用算子、适应度函数与终止规则，并通过检索增强的提示合成技术动态适应运行时执行环境。我们设计、实现并部署了KernelEvolve，用于优化跨代NVIDIA与AMD GPU以及Meta自研AI加速器上的多种生产级推荐模型。在公开测试集KernelBench上的验证表明：该框架在三个难度级别共250个测试问题中实现100%通过率，并在三种异构硬件平台上对160个PyTorch ATen算子达成100%正确性。实际生产用例中，KernelEvolve将开发周期从数周缩短至数小时，在多样化应用场景及大规模异构AI系统上均显著超越PyTorch基线性能。除性能提升外，该框架通过为内部研发的AI硬件提供自动化内核生成能力，有效降低了新型AI硬件的编程门槛。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.23236) | [arXiv](https://arxiv.org/abs/2512.23236)



---

### 28. TrGLUE与SentiTurca的提出：土耳其语通用语言理解与情感分析综合基准

**原文标题：** Introducing TrGLUE and SentiTurca: A Comprehensive Benchmark for Turkish General Language Understanding and Sentiment Analysis

**摘要：**
评估Transformer、大语言模型及其他自然语言处理系统的性能需要能够多维度衡量表现的综合基准。其中，自然语言理解能力的评估尤为关键，因其是衡量模型能力的根本标准。因此，建立能够从多角度深入评估分析自然语言理解能力的基准至关重要。尽管GLUE基准已为英语自然语言理解评估树立了标准，其他语言也相继开发了类似基准，如中文的CLUE、法语的FLUE和日语的JGLUE，但土耳其语目前尚缺乏可比拟的基准体系。为填补这一空白，我们提出了涵盖多种土耳其语自然语言理解任务的综合基准TrGLUE，并同步推出面向情感分析的专业基准SentiTurca。为支持研究者，我们还提供了基于Transformer模型的微调与评估代码，以促进这些基准的有效使用。TrGLUE包含精心构建的土耳其语原生语料库，其设计遵循GLUE式评估的领域划分与任务框架，标签通过半自动化流程获取——该流程融合了基于强LLM的自动标注、跨模型一致性校验及后续人工验证。这一设计优先保障语言自然度，最大限度减少直接翻译带来的失真，并形成了可扩展、可复现的工作流程。通过TrGLUE，我们旨在为土耳其语自然语言理解建立稳健的评估框架，为研究者提供宝贵资源，并为生成高质量半自动化数据集的实践提供方法论参考。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.22100) | [arXiv](https://arxiv.org/abs/2512.22100)



---

### 29. 自评估解锁任意步数文本到图像生成

**原文标题：** Self-Evaluation Unlocks Any-Step Text-to-Image Generation

**摘要：**
本文提出自评估模型（Self-E），这是一种全新的、从零开始训练的文本到图像生成方法，支持任意步数推理。Self-E 以类似于流匹配模型的方式从数据中学习，同时采用创新的自评估机制：它利用当前分数估计值评估自身生成的样本，实质上充当了动态的自监督教师。与传统扩散模型或流模型不同，该方法不单纯依赖通常需要大量推理步数的局部监督；与基于蒸馏的方法相比，它无需预训练的教师模型。这种即时局部学习与自驱动全局匹配的结合，弥合了两种范式间的鸿沟，使得能够从零开始训练出在极低步数下仍表现卓越的高质量文本到图像模型。在大规模文本到图像基准上的广泛实验表明，Self-E 不仅在少步数生成中表现优异，在50步推理时也可与最先进的流匹配模型竞争。我们进一步发现，其性能随推理步数增加呈单调提升趋势，从而在单一统一模型内实现了超快速少步生成与高质量长轨迹采样。据我们所知，Self-E 是首个从零训练、支持任意步数的文本到图像模型，为高效可扩展的生成提供了统一框架。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.22374) | [arXiv](https://arxiv.org/abs/2512.22374)



---

### 30. 思维形态：推理任务中分布特性比答案正确性更重要的现象

**原文标题：** Shape of Thought: When Distribution Matters More than Correctness in Reasoning Tasks

**摘要：**
本研究揭示了一个令人惊奇的发现：通过使用来自更强能力模型生成的思维链合成数据集进行训练，即使这些思维链最终都推导出错误答案，语言模型的推理能力仍能得到提升。实验表明，这种方法在推理任务上的表现优于基于人工标注数据集的训练。我们提出两个关键假设解释此现象：首先，合成数据的分布本质上更接近语言模型自身的分布特征，从而更易于被模型学习；其次，这些“错误”思维链往往仅存在部分缺陷，其中包含的有效推理步骤仍可为模型提供学习价值。为验证第一个假设，我们使用语言模型对人工标注的思维链进行复述处理——使其分布更接近模型自身分布——实验证明该方法能有效提升模型性能。针对第二个假设，我们通过引入缺陷程度递增的思维链，系统探究了模型对这些缺陷的容忍限度。我们在数学推理、算法推理和代码生成等多个领域验证了这一发现，使用MATH、GSM8K、Countdown和MBPP数据集，并在Qwen、Llama和Gemma系列模型中测试了1.5B至9B不同规模的模型。研究表明：构建与模型分布特征契合的数据集是至关重要的考量因素。同时我们证明，最终答案的正确性并不能始终可靠地反映推理过程的忠实性。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.22255) | [arXiv](https://arxiv.org/abs/2512.22255)



---

### 31. 逆向个性化

**原文标题：** Reverse Personalization

**摘要：**
近期基于文本到图像的扩散模型在根据文本提示和人类身份生成逼真人脸图像方面展现出卓越能力，实现了个性化面部图像的创建。然而，现有基于提示的方法在移除或修改身份特征时，要么依赖于预训练模型已充分学习目标主体特征，要么需要对特定身份进行模型微调。本研究通过分析身份特征的生成过程，提出了一种面向人脸匿名化的逆向个性化框架。该方法利用条件扩散反演技术，无需文本提示即可直接操作图像。为提升对模型训练数据外主体的泛化能力，我们引入了身份引导的条件分支。与先前缺乏面部属性控制的匿名化方法不同，本框架支持属性可控的匿名化处理。实验表明，我们的方法在身份移除、属性保留和图像质量三者间达到了最优平衡。源代码与数据详见 https://github.com/hanweikung/reverse-personalization。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.22984) | [arXiv](https://arxiv.org/abs/2512.22984)



---


## 🔍 关键词云图
![关键词云图](../images/keywords_wordcloud.png)

## 📈 近期论文趋势
![论文趋势](../images/daily_papers.png)

## 🎙️ 语音播报
- [收听今日论文解读](../audio/2025-12-30_daily_papers.mp3)

## 📱 订阅渠道
- GitHub: [hf-daily-paper-newsletter-chinese](https://github.com/2404589803/hf-daily-paper-newsletter-chinese)