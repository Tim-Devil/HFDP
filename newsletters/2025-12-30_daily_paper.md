
# <img src="https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo.png" width="30"/> Hugging Face 2025-12-30 论文日报

## 📊 今日论文统计
- 总论文数：31
- 热门领域：LLM, RL, Transformer, GPT

## 📝 论文详情


### 1. 基于辅助损失的专家混合模型中专家与路由器的耦合机制

**原文标题：** Coupling Experts and Routers in Mixture-of-Experts via an Auxiliary Loss

**摘要：**
专家混合模型缺乏明确的约束机制来确保路由器的决策与专家能力相匹配，这最终限制了模型性能。为解决这一问题，我们提出专家-路由器耦合损失——一种轻量级辅助损失函数，可将路由器的决策与专家能力紧密耦合。该方法将每个专家的路由器嵌入向量视为分配给该专家的标记的代理标记，并通过专家网络输入扰动后的路由器嵌入向量以获取内部激活值。ERC损失对这些激活值施加双重约束：(1) 每个专家对其自身代理标记的激活强度必须高于对其他专家代理标记的激活；(2) 每个代理标记在其对应专家中激发的激活强度必须高于在其他专家中的激活。这些约束共同确保每个路由器嵌入向量能准确表征对应专家的能力特征，同时使每个专家专注于处理实际被路由至该专家的标记。ERC损失具有计算高效性，仅需处理n²个激活值（n为专家数量），这种固定成本与批次大小无关，而现有耦合方法的计算量通常随标记数量（每批次常达数百万）线性增长。通过对3B至15B参数的MoE-LLMs进行预训练，并在数万亿标记上进行广泛分析，我们验证了ERC损失的有效性。此外，ERC损失能够在训练过程中灵活控制并量化追踪专家专业化程度，为MoE模型研究提供了重要分析视角。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.23447) | [arXiv](https://arxiv.org/abs/2512.23447)



---

### 2. LiveTalk：基于改进策略蒸馏的实时多模态交互视频扩散模型

**原文标题：** LiveTalk: Real-Time Multimodal Interactive Video Diffusion via Improved On-Policy Distillation

**摘要：**
通过扩散模型实现实时视频生成对于构建通用多模态交互式人工智能系统至关重要。然而，扩散模型通过迭代过程对所有视频帧进行双向注意力同步去噪的方式阻碍了实时交互。虽然现有蒸馏方法可使模型具备自回归特性并减少采样步骤以缓解此问题，但这些方法主要关注文本到视频生成，导致人机交互仍显生硬且效率低下。本文旨在实现基于多模态上下文（包括文本、图像和音频）的实时交互式视频扩散，以弥合这一差距。通过观察发现，当前领先的策略蒸馏方法Self Forcing在多模态条件输入下存在挑战（如闪烁、黑帧等视觉伪影及质量下降），我们提出一种改进的蒸馏方案，重点关注条件输入质量以及策略优化初始化和调度策略。在HDTF、AVSpeech和CelebV-HQ等多模态条件（音频、图像及文本）驱动的虚拟形象视频生成基准测试中，我们蒸馏后的模型在推理成本和延迟降低20倍的情况下，视觉质量仍能达到同等或更大规模全步骤双向基线模型的水平。进一步地，我们将模型与音频语言模型及长视频推理技术Anchor-Heavy Identity Sinks集成，构建了LiveTalk实时多模态交互虚拟形象系统。在我们构建的多轮交互基准测试中进行的系统级评估表明，LiveTalk在多轮视频连贯性与内容质量上优于前沿模型（Sora2、Veo3），同时将响应延迟从1-2分钟缩短至实时生成水平，实现了无缝的人机多模态交互。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.23576) | [arXiv](https://arxiv.org/abs/2512.23576)



---

### 3. Yume-1.5：一种文本控制的交互式世界生成模型

**原文标题：** Yume-1.5: A Text-Controlled Interactive World Generation Model

**摘要：**
近期研究已证明利用扩散模型生成交互式可探索世界的潜力。然而，现有方法大多面临参数量过大、依赖冗长推理步骤、历史上下文快速增长等关键挑战，严重限制了实时性能且缺乏文本控制生成能力。为应对这些挑战，本文提出\method，这是一种新颖的框架，能够通过单张图像或文本提示生成逼真、交互且连续的世界。该框架通过精心设计的架构实现基于键盘操作的生成世界探索，其核心包含三个组成部分：（1）融合统一上下文压缩与线性注意力的长视频生成框架；（2）通过双向注意力蒸馏与增强型文本嵌入方案驱动的实时流式加速策略；（3）面向世界事件生成的文本控制方法。相关代码库已附于补充材料中。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.22096) | [arXiv](https://arxiv.org/abs/2512.22096)



---

### 4. SmartSnap：自验证智能体的主动证据寻求机制

**原文标题：** SmartSnap: Proactive Evidence Seeking for Self-Verifying Agents

**摘要：**
智能体强化学习在复杂图形用户界面任务下的自主智能体开发中展现出巨大潜力，但其可扩展性仍受限于任务完成验证的瓶颈。现有验证方法普遍采用被动的事后处理模式：验证器（如基于规则的评分脚本、奖励或评判模型，以及基于大语言模型的裁判机制）通过分析智能体完整的交互轨迹来判断任务成功与否。这种处理包含无关噪声历史的冗长上下文的方式，对验证协议构成挑战，导致验证成本高昂且可靠性不足。为突破此限制，我们提出SmartSnap范式，将验证模式从被动事后验证转变为智能体主动实施的现场自验证。我们设计了一种新型自验证智能体，其具备双重使命：不仅完成任务，还需通过精心筛选的快照证据证明任务完成度。基于我们提出的3C原则（完整性、简洁性、创造性），该智能体利用在线环境可访问性，在最小化决定性快照集合上进行自验证。这些证据将作为通用大语言模型裁判验证器判断有效性与相关性的唯一依据。跨模型系列与规模的移动端任务实验表明，SmartSnap范式能够以可扩展方式训练大语言模型驱动的智能体，使80亿参数模型和300亿参数模型分别获得26.08%和16.66%的性能提升。解决方案探索与证据寻求的协同机制，成功培育出性能高效的自验证智能体，其竞争力与DeepSeek V3.1及Qwen3-235B-A22B模型相当。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.22322) | [arXiv](https://arxiv.org/abs/2512.22322)



---

### 5. 扩散模型通晓透明度：基于视频扩散的透明物体深度与法向估计新范式

**原文标题：** Diffusion Knows Transparency: Repurposing Video Diffusion for Transparent Object Depth and Normal Estimation

**摘要：**
透明物体始终是感知系统面临的经典难题：折射、反射与透射现象破坏了立体视觉、飞行时间法及纯判别式单目深度估计的基本假设，导致预测结果存在空洞且时序不稳定。本研究的核心发现在于，现代视频扩散模型已能合成逼真的透明现象，表明其内部已隐式学习光学物理规律。为此，我们构建了TransPhy3D合成视频数据集，包含1.1万段采用Blender/Cycles渲染的透明/反射场景序列。场景由经过筛选的类别丰富静态资产与几何形态丰富的程序化资产组合而成，并配以玻璃/塑料/金属材质。通过基于物理的光线追踪与OptiX降噪技术，我们同步渲染RGB图像、深度图与法向图。基于大规模视频扩散模型，我们通过轻量级LoRA适配器训练视频到视频的深度（及法向）转换器。训练过程中，我们在DiT主干网络中拼接RGB与（含噪）深度潜在特征，并在TransPhy3D与现有逐帧合成数据集上进行协同训练，从而实现对任意长度输入视频的时序一致性预测。所得模型DKT在涉及透明物体的真实与合成视频基准测试中实现零样本最优性能：包括ClearPose、DREDS（CatKnown/CatNovel）及TransPhy3D-Test。该模型在精度与时序一致性上均超越现有强图像/视频基线模型，其法向估计变体在ClearPose上创下视频法向估计最佳纪录。紧凑的13亿参数版本可实现约0.17秒/帧的推理速度。在抓取任务集成实验中，DKT的深度估计显著提升了半透明、反射与漫反射表面的抓取成功率，优于现有估计器。这些成果共同印证了一个更广泛的论断：“扩散模型通晓透明度”。生成式视频先验能够以高效、无标注的方式转化为鲁棒且时序连贯的感知系统，为复杂现实场景下的机器人操作提供新可能。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.23705) | [arXiv](https://arxiv.org/abs/2512.23705)



---

### 6. Stream-DiffVSR：基于自回归扩散的低延迟可流式视频超分辨率方法

**原文标题：** Stream-DiffVSR: Low-Latency Streamable Video Super-Resolution via Auto-Regressive Diffusion

**摘要：**
基于扩散模型的视频超分辨率方法虽能实现优异的感知质量，但由于其依赖未来帧以及计算成本高昂的多步去噪过程，在延迟敏感的实际应用中仍不具可行性。本文提出Stream-DiffVSR，一种基于因果条件扩散的高效在线视频超分辨率框架。该框架严格基于历史帧进行处理，融合了以下核心组件：一个用于快速推理的四步蒸馏去噪器；一个在潜在去噪过程中注入运动对齐线索的自回归时序引导模块；以及一个轻量级时序感知解码器，其配备的时序处理模块可有效增强细节与时序一致性。在RTX4090 GPU上，Stream-DiffVSR处理720p帧仅需0.328秒，性能显著优于现有基于扩散的方法。与当前在线视频超分辨率最优方法TMP相比，本方法在提升感知质量的同时（LPIPS指标提升0.095），将延迟降低了超过130倍。Stream-DiffVSR实现了目前基于扩散模型的视频超分辨率方法中最低的延迟，将初始处理延迟从超过4600秒大幅缩减至0.328秒，从而成为首个适用于低延迟在线部署的扩散视频超分辨率方法。项目页面：https://jamichss.github.io/stream-diffvsr-project-page/

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.23709) | [arXiv](https://arxiv.org/abs/2512.23709)



---

### 7. Dream-VL与Dream-VLA：基于扩散语言模型架构的开放视觉-语言及视觉-语言-动作模型

**原文标题：** Dream-VL & Dream-VLA: Open Vision-Language and Vision-Language-Action Models with Diffusion Language Model Backbone

**摘要：**
尽管自回归大型视觉-语言模型（VLMs）已取得显著成就，但其序列化生成特性常限制其在复杂视觉规划与动态机器人控制任务中的效能。本研究探索基于扩散大语言模型（dLLMs）构建视觉-语言模型的潜力以突破这些局限。我们提出Dream-VL——一种基于扩散架构的开放视觉-语言模型（dVLM），其在现有dVLM中达到最先进的性能水平。Dream-VL在多项基准测试中与基于开放数据训练的自回归视觉-语言模型性能相当，同时在视觉规划任务中展现出更优潜力。基于Dream-VL架构，我们进一步提出Dream-VLA：通过对开放机器人数据集进行持续预训练开发的基于dLLM的视觉-语言-动作模型（dVLA）。我们证明该扩散架构天然的 bidirectional 特性为视觉-语言-动作任务提供了更优基础，其内在适用于动作分块与并行生成，从而在下游微调中实现显著加速收敛。Dream-VLA在LIBERO基准测试中取得97.2%的平均成功率，在SimplerEnv-Bridge和SimplerEnv-Fractal基准测试中分别达到71.4%和60.5%的综合平均成绩，超越π_0、GR00T-N1等领先模型。我们还验证了在不同训练目标的下游任务中，dVLM模型均优于自回归基线模型。我们公开释放Dream-VL与Dream-VLA模型，以促进学界进一步研究。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.22615) | [arXiv](https://arxiv.org/abs/2512.22615)



---

### 8. SpotEdit：扩散变换器中的选择性区域编辑

**原文标题：** SpotEdit: Selective Region Editing in Diffusion Transformers

**摘要：**
扩散变换器模型通过编码条件图像并将其整合到变换器层中，显著推动了图像编辑技术的发展。然而，大多数编辑仅涉及修改小范围区域，而现有方法在每一步均对所有令牌进行统一处理与去噪，这不仅导致冗余计算，还可能使未改动区域的质量下降。这引发了一个根本性问题：在编辑过程中，是否真的有必要重新生成每一个区域？为解决此问题，我们提出了SpotEdit，一种无需训练的选择性扩散编辑框架，该框架仅更新被修改的区域。SpotEdit包含两个核心组件：SpotSelector通过感知相似性识别稳定区域，并复用条件图像特征以跳过其计算过程；SpotFusion则通过动态融合机制，自适应地将这些特征与已编辑令牌进行混合，从而保持上下文连贯性与编辑质量。通过减少不必要的计算并确保未修改区域的高保真度，SpotEdit实现了高效且精确的图像编辑。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.22323) | [arXiv](https://arxiv.org/abs/2512.22323)



---

### 9. GRAN-TED：为扩散模型生成鲁棒、对齐且细腻的文本嵌入

**原文标题：** GRAN-TED: Generating Robust, Aligned, and Nuanced Text Embedding for Diffusion Models

**摘要：**
文本编码器是文生图与文生视频扩散模型的核心组件，从根本上决定了生成内容的语义保真度。然而，其发展长期受到两大挑战的制约：一是缺乏能够可靠预测下游生成性能的高效评估框架，二是难以将预训练语言模型有效适配于视觉合成任务。为解决这些问题，我们提出了GRAN-TED范式，旨在为扩散模型生成鲁棒、对齐且细腻的文本嵌入。我们的贡献包含两个方面。首先，我们提出了TED-6K——一个全新的纯文本基准数据集，它通过轻量化的统一适配器实现标准化评估，无需昂贵的端到端模型训练即可高效、稳健地衡量编码器的表征质量。我们证明，TED-6K上的表现与编码器在下游生成任务中的效能具有强相关性。值得注意的是，在我们的实验设置下，相较于从头训练扩散模型，使用TED-6K进行评估的速度提升约750倍。其次，在这一经过验证的框架指导下，我们通过一种新颖的两阶段训练范式开发了性能更优的文本编码器。该过程包含：第一阶段在多模态大语言模型上进行微调以提升视觉表征能力，第二阶段采用分层加权方法提取更细腻、更有效的文本特征。实验表明，所得GRAN-TED编码器不仅在TED-6K上达到领先性能，同时在文生图与文生视频任务中实现了显著的性能提升。我们的TED-6K数据集与评估代码已公开于：https://anonymous.4open.science/r/GRAN-TED-4FCC/。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.15560) | [arXiv](https://arxiv.org/abs/2512.15560)



---

### 10. Act2Goal：从世界模型到通用目标条件策略

**原文标题：** Act2Goal: From World Model To General Goal-conditioned Policy

**摘要：**
如何以兼具表达力与精确性的方式定义机器人操作任务，仍是当前的核心挑战。视觉目标能够提供紧凑且明确的任务描述，但现有的目标条件策略通常因依赖单步动作预测而缺乏对任务进展的显式建模，难以应对长时序操作任务。本文提出Act2Goal，一种通用目标条件操作策略，它将目标条件视觉世界模型与多尺度时序控制相结合。给定当前观测与目标视觉状态，世界模型能够生成一系列合理的中间视觉状态序列，以捕捉长时序任务结构。为实现视觉规划到鲁棒执行的转化，我们提出了多尺度时序哈希（MSTH）方法，将预测轨迹分解为密集近端帧（用于细粒度闭环控制）和稀疏远端帧（用于保持全局任务一致性）。策略通过端到端交叉注意力机制将这些表征与运动控制相耦合，在保持对局部干扰实时响应的同时，实现连贯的长时序行为。Act2Goal在新物体、空间布局及环境场景中展现出优异的零样本泛化能力。此外，我们通过基于LoRA微调的后视目标重标注技术，实现了无需奖励信号的在线自适应，使系统能在无外部监督条件下快速自主改进。真实机器人实验表明，在具有挑战性的分布外任务中，Act2Goal通过数分钟自主交互即可将成功率从30%提升至90%，验证了结合多尺度时序控制的目标条件世界模型能为鲁棒的长时序操作提供必要的结构化引导。项目页面：https://act2goal.github.io/

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.23541) | [arXiv](https://arxiv.org/abs/2512.23541)



---

### 11. Web世界模型

**原文标题：** Web World Models

**摘要：**
语言智能体日益需要在能够行动、记忆和学习的持久化世界中运行。现有方法处于两个极端：传统Web框架通过数据库支持提供可靠但固定的上下文环境，而完全生成式世界模型以牺牲可控性和工程实用性为代价追求无限环境。本研究提出Web世界模型（WWM）作为中间方案，其世界状态与“物理规则”通过常规Web代码实现以确保逻辑一致性，同时由大语言模型在此结构化潜在状态之上生成上下文、叙事和高层决策。我们基于现实Web技术栈构建了一系列WWM系统，包括基于真实地理的无限旅行图册、虚构星系探索系统、网络级百科全书与叙事世界，以及模拟与游戏化环境。通过这些系统，我们总结出WWM的实用设计原则：分离代码定义规则与模型驱动想象，将潜在状态表示为类型化Web接口，利用确定性生成实现无限但有结构的探索。研究结果表明，Web技术栈本身可作为世界模型的可扩展基础，实现可控且开放的环境。项目页面：https://github.com/Princeton-AI2-Lab/Web-World-Models。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.23676) | [arXiv](https://arxiv.org/abs/2512.23676)



---

### 12. DiRL：一种高效的扩散语言模型后训练框架

**原文标题：** DiRL: An Efficient Post-Training Framework for Diffusion Language Models

**摘要：**
扩散语言模型已成为自回归模型的有前景的替代方案。尽管近期研究验证了其预训练潜力并提升了推理速度，但扩散语言模型的后训练体系仍不完善。现有方法存在计算效率低下、训练与推理目标不匹配等问题，严重限制了模型在数学等复杂推理任务上的性能。为此，我们提出DiRL，一种高效的后训练框架，将FlexAttention加速的分块训练与LMDeploy优化的推理机制紧密集成。该架构实现了流畅的在线模型更新循环，支持高效的两阶段后训练（监督微调与强化学习）。基于此框架，我们提出了DiPO——首个专为扩散语言模型设计的无偏分组相对策略优化实现。我们通过在高质量数学数据上训练DiRL-8B-Instruct验证了方法的有效性。该模型在扩散语言模型中取得了领先的数学性能，并在多个基准测试中超越了Qwen2.5系列的同规模模型。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.22234) | [arXiv](https://arxiv.org/abs/2512.22234)



---

### 13. 基于评分标准奖励的人工智能协研员训练方法研究

**原文标题：** Training AI Co-Scientists Using Rubric Rewards

**摘要：**
人工智能协研员正逐渐成为协助人类研究者实现科研目标的重要工具。其核心功能在于能够根据既定研究目标与约束条件生成研究方案。此类方案既可用于研究者头脑风暴，亦可在进一步优化后付诸实施。然而，当前语言模型在生成完全符合约束条件与隐性要求的研究方案方面仍面临挑战。本研究探索如何利用海量现有科研文献训练语言模型以生成更优质的研究方案。我们通过跨学科论文自动提取研究目标及目标特异性评分标准，构建了可扩展的多元化训练语料库。随后采用带自评分机制的强化学习方法训练研究方案生成模型：训练过程中由初始策略的冻结副本担任评分器，评分标准在生成器与验证器之间形成性能差距，从而实现无需外部人工监督的持续优化。为验证该方法，我们组织机器学习领域专家开展了225小时的人工评估实验。结果显示，针对70%的研究目标，专家更倾向于选择经微调的Qwen3-30B-A3B模型生成的研究方案，且对84%的自动提取目标特异性评分标准表示认可。为检验方法普适性，我们将该方法扩展至医学论文及arXiv预印本的研究目标，并采用前沿模型评审团进行评估。微调后的模型实现了12-22%的相对性能提升，展现出显著的跨领域泛化能力，即使在医学研究等难以获取执行反馈的问题场景中仍保持有效性。这些发现共同证明，这种可扩展的自动化训练方法具有提升通用人工智能协研员性能的潜力，为相关领域发展提供了可行路径。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.23707) | [arXiv](https://arxiv.org/abs/2512.23707)



---

### 14. YOLO-Master：基于专家混合与专用Transformer增强的实时检测加速框架

**原文标题：** YOLO-Master: MOE-Accelerated with Specialized Transformers for Enhanced Real-time Detection

**摘要：**
现有实时目标检测方法普遍采用类YOLO架构，以权衡精度与速度。然而，这些模型依赖静态密集计算机制，对所有输入采用统一处理方式，导致表征能力与计算资源分配失当——例如在简单场景中过度分配资源，而在复杂场景中资源不足。这种不匹配既造成计算冗余，也导致检测性能未达最优。为突破此局限，本文提出YOLO-Master，一种新型类YOLO框架，通过实例条件自适应计算机制实现实时目标检测。该框架核心为高效稀疏专家混合模块，能够依据输入场景复杂度动态分配计算资源。其轻量级动态路由网络通过多样性增强目标引导专家在训练过程中实现专业化，促进专家间形成互补能力。此外，路由网络自适应学习仅激活最相关专家，从而在提升检测性能的同时最小化推理计算开销。在五个大规模基准测试上的综合实验验证了YOLO-Master的优越性：在MS COCO数据集上，本模型以1.62毫秒延迟取得42.4%平均精度，较YOLOv13-N提升0.8%平均精度且推理速度加快17.8%。值得注意的是，该模型在挑战性密集场景中提升尤为显著，同时能在典型输入中保持高效性并维持实时推理速度。代码将公开提供。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.23273) | [arXiv](https://arxiv.org/abs/2512.23273)



---

### 15. Video-BrowseComp：开放网络环境下智能体视频研究基准测试

**原文标题：** Video-BrowseComp: Benchmarking Agentic Video Research on Open Web

**摘要：**
自主智能体的发展正在重塑信息获取方式，从被动检索转向主动、开放式的网络研究。然而，尽管文本与静态多模态智能体已取得快速进展，在处理网络最具动态性的模态——视频时，仍存在显著的模态鸿沟。现有视频基准主要聚焦于被动感知，即向模型提供精选片段而无需外部检索，未能评估需要主动查询视频时间线、交叉参照分散证据并基于开放网络验证主张的智能体视频研究能力。为填补这一空白，我们提出Video-BrowseComp基准测试，该基准包含210个专为开放网络智能体视频推理设计的挑战性问题。与先前基准不同，Video-BrowseComp强制要求对时序视觉证据的依赖，确保答案无法仅通过文本搜索获得，而必须通过导航视频时间线来验证外部主张。我们对前沿模型的评估揭示了一个关键瓶颈：即使如GPT-5.1（搭载搜索功能）等先进的搜索增强模型，其准确率也仅为15.24%。分析表明，这些模型主要依赖文本代理信息，在元数据丰富的领域（如附带剧情摘要的电视剧）表现良好，但在元数据稀缺、动态变化的环境（如体育赛事、游戏实况）中则严重失效，而这些场景恰恰需要视觉基础验证。作为首个开放网络视频研究基准，Video-BrowseComp推动该领域从被动感知向主动视频推理迈进。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.23044) | [arXiv](https://arxiv.org/abs/2512.23044)



---

### 16. 面向智能信息检索的嵌套式浏览器使用学习

**原文标题：** Nested Browser-Use Learning for Agentic Information Seeking

**摘要：**
信息检索智能体在广泛而深入的搜索任务中已展现出卓越性能，但其工具使用仍主要局限于API级片段检索与基于URL的页面获取，难以通过真实浏览行为获取更丰富的信息。尽管完整的浏览器交互可释放更深层能力，但其细粒度控制与冗长页面内容反馈为ReAct式函数调用智能体带来了显著复杂性。为弥合此鸿沟，我们提出嵌套式浏览器使用学习框架，通过嵌套结构引入极简而完整的浏览器操作框架，将交互控制与页面探索解耦。该设计在简化智能体推理过程的同时，实现了对深层网络信息的有效获取。在具有挑战性的深度信息检索基准测试中的实证结果表明，该框架在实践中具有显著优势。进一步的深入分析印证了其高效性与灵活性。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.23647) | [arXiv](https://arxiv.org/abs/2512.23647)



---

### 17. OmniAgent：面向全模态音视频理解的音频引导主动感知智能体

**原文标题：** OmniAgent: Audio-Guided Active Perception Agent for Omnimodal Audio-Video Understanding

**摘要：**
全模态大语言模型在统一音频与视觉模态方面已取得显著进展，但其仍常缺乏细粒度的跨模态理解能力，并在多模态对齐方面存在困难。为突破这些局限，本文提出OmniAgent——一种完全由音频引导的主动感知智能体，它通过动态协调专用工具来实现更细粒度的音视频推理。与以往依赖僵化静态工作流和密集帧描述的方法不同，本文展示了从被动响应生成到主动多模态查询的范式转变。OmniAgent采用动态规划机制，按需自主协调工具调用，策略性地将感知注意力集中于任务相关线索。本方法的核心在于一种新颖的由粗到精的音频引导感知范式，该范式利用音频线索定位时序事件并引导后续推理。在三个音视频理解基准上的大量实验评估表明，OmniAgent取得了最先进的性能表现，以10%-20%的准确率优势显著超越当前领先的开源模型与专有模型。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.23646) | [arXiv](https://arxiv.org/abs/2512.23646)



---

### 18. SurgWorld：通过世界建模从视频中学习手术机器人策略

**原文标题：** SurgWorld: Learning Surgical Robot Policies from Videos via World Modeling

**摘要：**
数据稀缺仍然是实现全自主手术机器人的根本障碍。尽管大规模视觉语言动作模型通过利用来自不同领域的配对视频动作数据，在家庭和工业操作中展现出卓越的泛化能力，但手术机器人领域却因缺乏同时包含视觉观察和精确机器人运动学的数据集而受限。相比之下，虽然存在大量手术视频资源，但它们缺乏相应的动作标签，导致无法直接应用模仿学习或视觉语言动作模型训练。本研究旨在通过从SurgWorld（一个专为手术物理人工智能设计的世界模型）中学习策略模型来缓解这一问题。我们构建了专门针对手术机器人的手术动作文本对齐数据集，其中包含详细的动作描述。随后，基于最先进的物理人工智能世界模型和该数据集，我们开发了SurgWorld系统。该系统能够生成多样化、可泛化且逼真的手术视频。我们首次采用逆动力学模型从合成手术视频中推断伪运动学数据，从而生成合成的配对视频动作数据。实验证明，在真实手术机器人平台上，使用这些增强数据训练的手术视觉语言动作策略模型，其性能显著优于仅基于真实演示数据训练的模型。本研究通过利用大量未标注手术视频和生成式世界建模，为自主手术技能获取提供了一条可扩展的路径，从而为开发泛化性强且数据高效的手术机器人策略开辟了新途径。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.23162) | [arXiv](https://arxiv.org/abs/2512.23162)



---

### 19. VL-LN基准：面向长视野目标导航与主动对话的研究

**原文标题：** VL-LN Bench: Towards Long-horizon Goal-oriented Navigation with Active Dialogs

**摘要：**
在现有的大多数具身导航任务中，指令通常被设定为明确且无歧义的，例如指令跟随和物体搜索。在这种理想化设定下，智能体仅需根据视觉与语言输入生成有效的导航输出。然而，现实世界中的导航指令往往具有模糊性和不确定性，要求智能体通过主动对话来消除歧义并推断用户意图。为填补这一研究空白，本文提出交互式实例物体导航任务，该任务不仅要求智能体生成导航动作，还需通过主动对话产生语言输出，从而更贴近实际应用场景。该任务在实例物体导航的基础上，允许智能体在导航过程中以自然语言形式向信息源自由咨询。基于此任务，我们构建了视觉语言-语言导航基准，该基准提供了大规模自动生成的数据集和完整的评估协议，用于训练和评估支持对话的导航模型。该基准包含超过4.1万条用于训练的长视野对话增强轨迹，以及配备可响应智能体查询信息源的自动评估协议。利用该基准，我们训练了具备对话能力的导航模型，实验表明该模型相较于基线方法取得了显著提升。大量实验与分析进一步验证了该基准在推动对话式具身导航研究方面的有效性和可靠性。代码与数据集：https://0309hws.github.io/VL-LN.github.io/

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.22342) | [arXiv](https://arxiv.org/abs/2512.22342)



---

### 20. 单子上下文工程

**原文标题：** Monadic Context Engineering

**摘要：**
大型语言模型（LLM）的普及推动了能够进行复杂推理和工具使用的自主智能体发展。然而，当前的智能体架构通常采用命令式的临时模式构建，导致系统脆弱，普遍存在状态管理、错误处理和并发控制等难题。本文提出单子上下文工程（MCE），这是一种利用函子、应用函子与单子的代数结构为智能体设计提供形式化基础的新型架构范式。MCE将智能体工作流视为计算上下文，其中状态传递、短路错误处理和异步执行等横切关注点通过抽象的代数性质实现内化管理。我们论证了单子如何实现稳健的顺序组合，应用函子如何为并行执行提供结构化原则，并重点阐释了单子变换器如何系统化组合这些能力。这种分层架构使开发者能够从简单且可独立验证的组件中构建复杂、鲁棒且高效的人工智能体。我们进一步扩展该框架以描述元智能体——其通过元编程动态创建并管理子智能体工作流，利用MCE实现生成式编排。项目页面：https://github.com/yifanzhang-pro/monadic-context-engineering。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.22431) | [arXiv](https://arxiv.org/abs/2512.22431)



---

### 21. 智能体系统设计的信息论视角

**原文标题：** An Information Theoretic Perspective on Agentic System Design

**摘要：**
智能体语言模型系统支撑着"深度研究"和"克劳德代码"等现代应用，通过多语言模型架构突破上下文限制。这些系统表面差异之下存在共性模式：较小的"压缩器"语言模型（甚至可在本地运行）将原始上下文提炼为紧凑文本，再由较大的"预测器"语言模型处理。尽管此类系统应用广泛，其设计仍多依赖经验法则，缺乏关于压缩器与预测器选择如何影响下游性能的理论指导。实践中，区分压缩与预测的贡献需要耗费大量资源的任务特定配对实验。本文认为这些智能体系统设计问题本质上是信息论问题。通过将压缩器语言模型视为噪声信道，我们提出一种基于上下文与其压缩之间互信息的简易估计器，以任务无关的方式量化压缩质量。研究表明，互信息能独立于具体任务强预测下游性能。基于信息论框架，我们在五个数据集和三个模型系列上展开全面实证分析。结果显示：更大规模的压缩器不仅更精确，而且更具标记效率——每个标记能传递更多比特信息。例如，70亿参数的Qwen-2.5压缩器相较于其15亿参数版本，准确度提升1.6倍，压缩简洁度提高4.6倍，单标记互信息传输量增加5.5倍。跨数据集实验表明，扩展压缩器规模比扩展预测器规模效果更为显著，这使得大型设备端压缩器可与小型云端预测器协同工作。将上述原理应用于深度研究系统时，仅需30亿参数的本地压缩器即可恢复前沿语言模型99%的准确度，同时将API成本降低至26%。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.21720) | [arXiv](https://arxiv.org/abs/2512.21720)



---

### 22. 分位数渲染：在3D高斯泼溅中高效嵌入高维特征

**原文标题：** Quantile Rendering: Efficiently Embedding High-dimensional Feature on 3D Gaussian Splatting

**摘要：**
计算机视觉领域的最新进展通过利用3D高斯泼溅技术，成功将开放词汇分割扩展到三维领域。尽管取得了这一进展，如何高效渲染开放词汇查询所需的高维特征仍面临重大挑战。现有方法采用码本或特征压缩技术，导致信息损失，从而降低了分割质量。为突破此局限，我们提出了分位数渲染——一种面向3D高斯泼溅的新型渲染策略，能够在保持高保真度的同时高效处理高维特征。与传统体渲染方法需对每条射线相交的所有3D高斯分布进行密集采样不同，分位数渲染仅沿射线稀疏采样具有主导影响的高斯分布。通过将分位数渲染集成到可泛化的三维神经网络中，我们进一步提出了高斯泼溅网络，该网络能以可泛化方式预测高斯特征。在ScanNet和LeRF数据集上的大量实验表明，本框架性能优于现有最优方法，同时能以约43.7倍的加速比实现512维特征图的实时渲染。相关代码将公开提供。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.20927) | [arXiv](https://arxiv.org/abs/2512.20927)



---

### 23. Robo-Dopamine：面向高精度机器人操作的通用工序奖励建模

**原文标题：** Robo-Dopamine: General Process Reward Modeling for High-Precision Robotic Manipulation

**摘要：**
将强化学习应用于现实世界机器人技术的主要障碍在于有效奖励函数的设计。尽管近期基于学习的工序奖励模型展现出良好前景，但其常受限于两个根本性缺陷：奖励模型缺乏对操作步骤的感知理解，且依赖单视角感知，导致对细粒度操作进程的评估不可靠；其奖励塑形过程在理论上缺乏严谨性，往往引发误导策略优化的语义陷阱。为解决这些问题，我们提出Dopamine-Reward——一种从多视角输入中学习通用、步骤感知型工序奖励模型的新方法。其核心是我们基于超过3400小时数据集训练的通⽤奖励模型，该模型通过步骤化奖励离散化实现结构化理解，并采用多视角奖励融合机制突破感知局限。基于Dopamine-Reward，我们进一步提出Dopamine-RL鲁棒策略学习框架，该框架采用理论严密的策略不变奖励塑形方法，使智能体能利用密集奖励实现高效自我提升，同时保持最优策略不变，从而从根本上规避语义陷阱。在多样化仿真与真实任务中的大量实验验证了本方法的有效性：通用奖励模型在奖励评估准确率上达到最先进水平，基于该模型构建的Dopamine-RL显著提升了策略学习效率。例如，当通用奖励模型通过单条专家轨迹以单次适应方式迁移至新任务后，所得奖励模型能使Dopamine-RL仅通过150次在线采样（约1小时真实机器人交互）便将策略成功率从接近零提升至95%，同时保持跨任务的强泛化能力。项目网站：https://robo-dopamine.github.io

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.23703) | [arXiv](https://arxiv.org/abs/2512.23703)



---

### 24. ProGuard：面向主动式多模态安全防护

**原文标题：** ProGuard: Towards Proactive Multimodal Safeguard

**摘要：**
生成模型的快速发展导致多模态安全风险持续涌现，暴露出现有防御方法的局限性。为应对这些挑战，我们提出ProGuard——一种视觉语言主动防护系统，能够在无需传统被动方法所需模型调整的情况下，识别并描述分布外（OOD）安全风险。我们首先构建了一个包含8.7万样本的模态平衡数据集，每个样本均通过分层多模态安全分类体系标注了二元安全标签与风险类别，有效缓解了模态偏差，确保了对文本、图像及图文混合输入的一致性审核。基于此数据集，我们通过纯强化学习（RL）训练视觉语言基础模型，以实现高效简洁的推理。为在受控环境中模拟主动安全场景，我们进一步引入OOD安全类别推断任务，并采用基于同义词库的相似性奖励增强RL目标，激励模型为未见过的风险类别生成简洁描述。实验结果表明，ProGuard在二元安全分类任务上达到与闭源大模型相当的性能，在不安全内容分类任务上显著优于现有开源防护模型。尤为突出的是，ProGuard展现出强大的主动审核能力，将OOD风险检测性能提升52.6%，OOD风险描述性能提升64.8%。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.23573) | [arXiv](https://arxiv.org/abs/2512.23573)



---

### 25. 通过统一导演模型实现音视频生成与用户想象的桥梁构建

**原文标题：** Bridging Your Imagination with Audio-Video Generation via a Unified Director

**摘要：**
现有的人工智能驱动视频创作系统通常将剧本草拟与关键镜头设计视为两个独立任务：前者依赖大型语言模型，后者则依托图像生成模型。我们认为这两个任务应当统一在单一框架内，因为逻辑推理与想象思维均是电影导演的核心素养。本研究提出UniMAGE——一个能够将用户提示与结构化剧本相衔接的统一导演模型，使非专业用户能够借助现有音视频生成模型创作长上下文、多镜头影片。为实现这一目标，我们采用统一文本与图像生成的混合变换器架构。为进一步增强叙事逻辑与关键帧一致性，我们提出“先交错学习，后解耦训练”的范式：首先进行交错概念学习，利用交错排列的文本-图像数据促进模型对剧本的深度理解与想象诠释；随后实施解耦专家学习，将剧本写作与关键帧生成分离，从而提升故事叙述的灵活性与创造性。大量实验表明，UniMAGE在开源模型中达到领先水平，能够生成逻辑连贯的视频剧本与视觉一致的关键帧图像。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.23222) | [arXiv](https://arxiv.org/abs/2512.23222)



---

### 26. Knot Forcing：驯服自回归视频扩散模型以实现实时无限交互式肖像动画

**原文标题：** Knot Forcing: Taming Autoregressive Video Diffusion Models for Real-time Infinite Interactive Portrait Animation

**摘要：**
实时肖像动画对于虚拟助手和实时化身等交互应用至关重要，需要高视觉保真度、时序连贯性、超低延迟，并能响应参考图像和驱动信号等动态输入的控制。基于扩散的模型虽能实现高质量生成，但其非因果特性阻碍了流式部署。因果自回归视频生成方法支持高效的逐帧生成，但存在误差累积、片段边界处的运动不连续以及长期一致性退化等问题。本研究提出了一种名为Knot Forcing的新型流式框架，用于实时肖像动画，通过三项关键设计应对上述挑战：（1）采用分块生成策略，通过缓存参考图像的KV状态实现全局身份保持，并利用滑动窗口注意力进行局部时序建模；（2）设计时序结模块，通过重叠相邻片段并借助图像到视频的条件传递时空线索，以平滑片段间的运动过渡；（3）引入“超前运行”机制，在推理过程中动态更新参考帧的时序坐标，使其语义语境始终领先于当前生成帧，从而支持长期连贯性。Knot Forcing能够在消费级GPU上实现高保真、时序连贯且可交互的无限序列肖像动画，达到实时性能并保持出色的视觉稳定性。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.21734) | [arXiv](https://arxiv.org/abs/2512.21734)



---

### 27. KernelEvolve：面向Meta异构AI加速器的可扩展智能内核编程框架

**原文标题：** KernelEvolve: Scaling Agentic Kernel Coding for Heterogeneous AI Accelerators at Meta

**摘要：**
实现深度学习推荐模型（DLRM）训练与推理的高效快速至关重要，但这一目标面临三大系统挑战：模型架构多样性、内核原语多样性，以及硬件代际与架构异构性。本文提出KernelEvolve——一种智能内核编程框架，旨在应对DLRM的大规模异构性挑战。该框架以内核规范为输入，通过自动化内核生成与优化流程，适配异构硬件架构下的推荐模型需求。KernelEvolve在多层次编程抽象上运行，涵盖从Triton与CuTe领域专用语言到底层硬件无关语言的完整软硬件优化栈。内核优化过程被建模为基于图的搜索，通过选择策略、通用算子、适应度函数与终止规则构成的动态机制，并借助检索增强的提示合成技术实时适配运行时执行环境。我们设计、实现并部署了KernelEvolve，用于优化跨代NVIDIA与AMD GPU以及Meta自研AI加速器上的多种生产级推荐模型。在公开测试集KernelBench上的验证表明：该框架在三个难度级别的250个测试问题中实现100%通过率，在三种异构硬件平台上对160个PyTorch ATen算子达成100%正确性。实际应用显示，KernelEvolve将开发周期从数周缩短至数小时，在多样化生产场景及大规模异构AI系统中较PyTorch基线实现显著性能提升。除性能优化外，该框架通过为内部研发的AI硬件提供自动化内核生成能力，显著降低了新型AI硬件的编程门槛。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.23236) | [arXiv](https://arxiv.org/abs/2512.23236)



---

### 28. TrGLUE与SentiTurca的提出：土耳其语通用语言理解与情感分析综合基准

**原文标题：** Introducing TrGLUE and SentiTurca: A Comprehensive Benchmark for Turkish General Language Understanding and Sentiment Analysis

**摘要：**
评估Transformer、大语言模型及其他自然语言处理系统的性能需要能够多维度衡量表现的综合基准。其中，自然语言理解能力的评估尤为关键，因其是衡量模型能力的核心标准。因此，建立能够从多视角全面评估分析NLU能力的基准体系至关重要。尽管GLUE基准已为英语NLU评估树立了规范，其他语言也相继开发了类似基准（如中文CLUE、法语FLUE、日文JGLUE），但土耳其语目前尚缺乏可比拟的基准体系。为填补这一空白，本文提出TrGLUE——一个涵盖多类型土耳其语NLU任务的综合基准，同时推出面向情感分析的专业基准SentiTurca。为支持研究者，我们还提供了基于Transformer模型的微调与评估代码，以促进基准的有效使用。TrGLUE包含精心构建的土耳其语原生语料库，其设计遵循GLUE式评估的领域划分与任务框架，标签通过半自动化流程生成：该流程融合了基于强LLM的自动标注、跨模型一致性校验及人工验证环节。此设计优先保障语言自然度，最大程度减少直接翻译带来的失真效应，并形成可扩展、可复现的工作流程。通过TrGLUE，我们旨在为土耳其语NLU建立稳健的评估框架，为学界提供高质量研究资源，并为生成优质半自动化数据集的方法论提供实践洞见。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.22100) | [arXiv](https://arxiv.org/abs/2512.22100)



---

### 29. 自评估解锁任意步数文本到图像生成

**原文标题：** Self-Evaluation Unlocks Any-Step Text-to-Image Generation

**摘要：**
本文提出自评估模型（Self-E），一种全新的、从零开始训练的文本到图像生成方法，支持任意步数推理。Self-E 采用与流匹配模型类似的数据学习方式，同时引入创新的自评估机制：模型利用当前分数估计值评估自身生成的样本，有效充当动态自监督教师。与传统扩散模型或流模型不同，该方法不依赖通常需要大量推理步数的局部监督；与基于蒸馏的方法不同，它无需预训练教师模型。这种即时局部学习与自驱动全局匹配的结合，弥合了两种范式间的鸿沟，使得从零开始训练的高质量文本到图像模型即使在极低推理步数下也能表现出色。在大规模文本到图像基准上的广泛实验表明，Self-E 不仅在少步数生成中表现优异，在50步推理时也可与最先进的流匹配模型竞争。我们进一步发现其性能随推理步数增加呈单调提升趋势，从而在单一统一模型内实现了超快速少步生成与高质量长轨迹采样。据我们所知，Self-E 是首个从零开始训练、支持任意步数的文本到图像生成模型，为高效可扩展的生成任务提供了统一框架。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.22374) | [arXiv](https://arxiv.org/abs/2512.22374)



---

### 30. 思维形态：推理任务中分布特性比答案正确性更重要的现象研究

**原文标题：** Shape of Thought: When Distribution Matters More than Correctness in Reasoning Tasks

**摘要：**
本研究揭示了一个令人意外的发现：即使所有思维链轨迹都导向错误答案，通过使用从更强模型中生成的合成思维链轨迹数据集进行训练，语言模型的推理能力仍能得到提升。实验表明，这种方法在推理任务上的表现优于基于人工标注数据集的训练。我们提出两个关键假设解释此现象：首先，合成数据的分布本质上更接近语言模型自身的分布特征，从而更易于被模型学习；其次，这些“错误”轨迹往往仅存在部分缺陷，其中包含的有效推理步骤仍可供模型学习。为验证第一个假设，我们使用语言模型对人工标注轨迹进行复述——使其分布更接近模型自身分布——并证明该方法能提升性能。针对第二个假设，我们引入缺陷程度递增的思维链轨迹，探究模型对这些缺陷的容忍限度。我们在数学推理、算法推理和代码生成等多个领域（使用MATH、GSM8K、Countdown和MBPP数据集），基于Qwen、Llama和Gemma系列中1.5B至9B参数规模的语言模型验证了上述发现。研究表明：构建更贴近模型分布特征的数据集是值得关注的关键维度；同时，正确答案并不总能可靠反映推理过程的忠实性。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.22255) | [arXiv](https://arxiv.org/abs/2512.22255)



---

### 31. 逆向个性化

**原文标题：** Reverse Personalization

**摘要：**
近期基于文本到图像的扩散模型在根据文本提示和人类身份生成逼真人脸图像方面展现出卓越能力，实现了个性化面部图像的创建。然而，现有基于提示的方法在移除或修改身份特征时，要么依赖预训练模型已充分学习目标主体，要么需要对特定身份进行模型微调。本研究通过分析身份特征的生成过程，提出了一种面向人脸匿名化的逆向个性化框架。该方法利用条件扩散反演技术，无需文本提示即可直接操作图像。为推广至模型训练数据之外的主体，我们引入了身份引导的条件分支。与先前缺乏面部属性控制的匿名化方法不同，本框架支持属性可控的匿名化处理。实验表明，该方法在身份移除、属性保留和图像质量三者间达到了当前最优的平衡状态。源代码与数据详见 https://github.com/hanweikung/reverse-personalization。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.22984) | [arXiv](https://arxiv.org/abs/2512.22984)



---


## 🔍 关键词云图
![关键词云图](../images/keywords_wordcloud.png)

## 📈 近期论文趋势
![论文趋势](../images/daily_papers.png)

## 🎙️ 语音播报
- [收听今日论文解读](../audio/2025-12-30_daily_papers.mp3)

## 📱 订阅渠道
- GitHub: [hf-daily-paper-newsletter-chinese](https://github.com/2404589803/hf-daily-paper-newsletter-chinese)