<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Hugging Face 论文日报 - 2025-12-02</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', sans-serif;
            line-height: 1.6;
            color: #333;
            background-color: #f5f5f5;
            padding: 20px;
        }
        
        .container {
            max-width: 900px;
            margin: 0 auto;
            background-color: white;
            padding: 40px;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }
        
        h1 {
            color: #2c3e50;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
            margin-bottom: 30px;
            font-size: 28px;
        }
        
        h1 img {
            vertical-align: middle;
            margin-right: 10px;
        }
        
        h2 {
            color: #34495e;
            margin-top: 40px;
            margin-bottom: 20px;
            font-size: 24px;
            border-left: 4px solid #3498db;
            padding-left: 15px;
        }
        
        h3 {
            color: #2c3e50;
            margin-top: 30px;
            margin-bottom: 15px;
            font-size: 20px;
        }
        
        p {
            margin-bottom: 15px;
            text-align: justify;
        }
        
        ul {
            margin-left: 20px;
            margin-bottom: 20px;
        }
        
        li {
            margin-bottom: 8px;
        }
        
        a {
            color: #3498db;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        strong {
            color: #2c3e50;
            font-weight: 600;
        }
        
        hr {
            border: none;
            border-top: 1px solid #e0e0e0;
            margin: 30px 0;
        }
        
        /* 关键修复:限制图片宽度 */
        img {
            max-width: 100%;
            height: auto;
            display: block;
            margin: 20px auto;
            border-radius: 4px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        
        /* 确保图片容器也有宽度限制 */
        p img {
            max-width: 100%;
        }
        
        /* 论文详情区域样式 */
        .paper-section {
            background-color: #f9f9f9;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 20px;
        }
        
        /* 统计信息样式 */
        .stats {
            background-color: #e8f4f8;
            padding: 15px;
            border-radius: 6px;
            margin-bottom: 20px;
        }
        
        /* 响应式设计 */
        @media (max-width: 768px) {
            .container {
                padding: 20px;
            }
            
            h1 {
                font-size: 24px;
            }
            
            h2 {
                font-size: 20px;
            }
            
            h3 {
                font-size: 18px;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <h1><img src="https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo.png" width="30"/> Hugging Face 2025-12-02 论文日报</h1>
<h2>📊 今日论文统计</h2>
<ul>
<li>总论文数：47</li>
<li>热门领域：LLM, Transformer, GPT, Vision, Audio, RL</li>
</ul>
<h2>📝 论文详情</h2>
<h3>1. 从代码基础模型到智能体与应用：代码智能实践指南</h3>
<p><strong>原文标题：</strong> From Code Foundation Models to Agents and Applications: A Practical Guide to Code Intelligence</p>
<p><strong>摘要：</strong>
大型语言模型（LLM）通过实现自然语言描述到功能代码的直接转换，从根本上改变了自动化软件开发的面貌，并借助GitHub Copilot（微软）、Cursor（Anysphere）、Trae（字节跳动）和Claude Code（Anthropic）等工具推动了商业应用。该领域已从基于规则的系统显著演进至基于Transformer的架构，在HumanEval等基准测试中的性能从个位数成功率提升至超过95%。本文针对代码大型语言模型，提供了全面的综述与实践指南（包含一系列分析与探测实验），系统性地审视了从数据构建到后训练的完整模型生命周期，涵盖高级提示范式、代码预训练、监督微调、强化学习及自主编码智能体。我们分析了通用大型语言模型（GPT-4、Claude、LLaMA）与代码专用大型语言模型（StarCoder、Code LLaMA、DeepSeek-Coder、QwenCoder）的代码能力，并对相关技术、设计决策与权衡进行了批判性考察。进一步地，我们阐明了学术研究（如基准测试与任务）与实际部署（如软件相关代码任务）之间的研究-实践差距，涉及代码正确性、安全性、大型代码库的上下文感知以及与开发流程的集成，并将有前景的研究方向与实际需求进行映射。最后，我们通过一系列实验对代码预训练、监督微调与强化学习进行了综合分析，涵盖缩放定律、框架选择、超参数敏感性、模型架构及数据集比较。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.18538">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.18538">arXiv</a></p>
<hr />
<h3>2. LongVT：通过原生工具调用激励“长视频思考”</h3>
<p><strong>原文标题：</strong> LongVT: Incentivizing "Thinking with Long Videos" via Native Tool Calling</p>
<p><strong>摘要：</strong>
大型多模态模型（LMMs）在结合文本思维链进行视频推理方面展现出巨大潜力。然而，它们仍易产生幻觉，尤其在处理证据稀疏且时间分散的长视频时。受人类理解长视频方式（先全局浏览，再细查相关片段）的启发，我们提出了LongVT——一个端到端的智能体框架，通过交错的多模态工具-思维链实现“长视频思考”。具体而言，我们利用LMMs固有的时序定位能力作为原生视频裁剪工具，以聚焦特定视频片段并重采样更细粒度的视频帧。这种从全局到局部的推理循环持续进行，直至答案基于检索到的视觉证据得到验证。针对长视频推理任务中细粒度问答数据稀缺的问题，我们构建并将发布名为VideoSIAH的数据套件，以支持训练与评估。具体来说，我们的训练数据集包含24.79万个样本用于工具集成的冷启动监督微调、1.6千个样本用于智能体强化学习，以及15.4千个样本用于智能体强化微调。评估基准包含1280个经过半自动化数据流程精心构建、并经过人机协同验证的问答对。通过精心设计的三阶段训练策略和大量实证验证，LongVT在四个具有挑战性的长视频理解与推理基准测试中均持续超越现有强基线模型。我们的代码、数据及模型检查点已公开于https://github.com/EvolvingLMMs-Lab/LongVT。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.20785">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.20785">arXiv</a></p>
<hr />
<h3>3. Envision：面向因果世界过程洞察的统一理解与生成基准评测</h3>
<p><strong>原文标题：</strong> Envision: Benchmarking Unified Understanding &amp; Generation for Causal World Process Insights</p>
<p><strong>摘要：</strong>
当前多模态模型旨在通过统一理解与生成能力突破单模态表征的局限，常采用文本到图像（T2I）任务校准语义一致性。然而，其在训练与评估中对静态单图像生成的依赖，导致模型过度拟合静态模式匹配与语义融合，从根本上制约了对随时间演化的动态过程建模能力。为突破这些限制，我们提出Envision——一个面向链式文本到多图像生成的因果事件演进基准。该基准以世界知识为基础，通过时空因果结构进行组织，重构了现有评估维度，涵盖六大科学与人文领域的1000个四阶段提示。为实现从单图像到序列帧的评估转型，并检验模型是否在遵循因果时序约束的同时真正内化了世界知识，我们提出Envision-Score综合评估指标，整合了多维一致性、物理合理性与美学表现。对15个模型（10个专用T2I模型，5个统一模型）的系统评估发现：专用T2I模型虽在美学渲染方面表现熟练，但缺乏内在世界知识；统一多模态模型弥补了这一差距，在因果叙事连贯性上持续优于专用模型。然而，即使这些统一架构仍逊色于闭源模型，且在克服时空一致性的核心挑战方面存在困难。这表明，对因果孤立单图像的关注会阻碍多帧推理与生成，促使模型偏向静态模式匹配而非动态世界建模，最终限制了世界知识的内化与生成能力。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.01816">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.01816">arXiv</a></p>
<hr />
<h3>4. 基于大语言模型的强化学习稳定性：理论框架与实践方法</h3>
<p><strong>原文标题：</strong> Stabilizing Reinforcement Learning with LLMs: Formulation and Practices</p>
<p><strong>摘要：</strong>
本文提出了一种基于大语言模型的强化学习新框架，从理论层面阐释了在REINFORCE等策略梯度方法中，为何及在何种条件下可通过代理词级目标函数优化真实序列级奖励。具体而言，通过一阶近似分析，我们证明仅当训练-推断差异与策略陈旧性同时最小化时，该代理目标的有效性才会显著提升。这一发现为多项广泛采用的强化学习稳定技术提供了理论依据，包括重要性采样校正、梯度裁剪，以及特别针对专家混合模型的路由重放机制。通过对参数量达300亿的专家混合模型开展累计数十万GPU小时的实验，我们发现：在在线策略训练中，结合重要性采样校正的基础策略梯度算法能实现最高的训练稳定性；当引入离线策略更新以加速收敛时，梯度裁剪与路由重放技术的结合对于缓解策略陈旧性引起的不稳定性至关重要。值得注意的是，一旦训练趋于稳定，无论采用何种冷启动初始化方式，持续优化最终都能获得相当的性能表现。我们期望所分享的理论洞见与开发的稳定训练方案能为未来研究提供参考。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.01374">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.01374">arXiv</a></p>
<hr />
<h3>5. 我们距离真正实用的深度研究智能体还有多远？</h3>
<p><strong>原文标题：</strong> How Far Are We from Genuinely Useful Deep Research Agents?</p>
<p><strong>摘要：</strong>
深度研究智能体旨在通过迭代式信息检索与综合，自动生成分析师级别的报告。然而，现有大多数深度研究智能体仅在问答基准测试中得到验证，而针对生成综合性报告的研究仍被忽视。更严重的是，当前报告综合的基准测试存在任务复杂度高与评估指标主观性强的问题——这既无法反映用户真实需求，也限制了生成报告的实际效用。为填补这些空白，我们提出了细粒度深度研究基准测试，这是一个包含100项人工策划研究任务的增强型基准，涵盖419项结构化检查清单条目，用于标准化报告结构、分析深度与事实依据。基于主流深度研究智能体生成的约1000份报告，我们进一步提出了深度研究失败分类法，这是首个针对深度研究智能体的失败类型学体系。该分类法包含推理、检索与生成三大维度下的14种细粒度失败模式，并基于扎根理论构建，采用人机协同标注与标注者间信度验证。实验结果表明，当前深度研究智能体的主要瓶颈不在于任务理解，而在于证据整合、事实核查以及具备推理韧性的规划能力。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.01948">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.01948">arXiv</a></p>
<hr />
<h3>6. 视频生成中的重力问题如何解决？基于可验证奖励的后训练牛顿定律应用</h3>
<p><strong>原文标题：</strong> What about gravity in video generation? Post-Training Newton's Laws with Verifiable Rewards</p>
<p><strong>摘要：</strong>
当前视频扩散模型虽能合成视觉上逼真的片段，却常违反基本物理定律——物体漂浮、加速度漂移、碰撞行为不一致——这揭示了视觉真实感与物理真实感之间的持续差距。本文提出NewtonRewards，首个基于可验证奖励的物理基础视频生成后训练框架。该方法不依赖人类或视觉语言模型反馈，而是通过冻结的效用模型从生成视频中提取可测量代理：光流作为速度代理，高层外观特征作为质量代理。这些代理通过两种互补奖励实现牛顿力学结构的显式强化：牛顿运动学约束确保恒定加速度动力学，质量守恒奖励防止平凡退化解。我们在新构建的大规模基准数据集NewtonBench-60K上，针对五种牛顿运动基本形式（自由落体、水平/抛物线抛射、斜面下滑/上滑）进行评估。在视觉与物理指标上，NewtonRewards在所有基本形式中均持续提升物理合理性、运动平滑度与时间连贯性，优于现有后训练方法。该框架在高度、速度与摩擦力的分布外变化下仍保持强劲性能。实验结果表明，基于物理的可验证奖励为物理感知的视频生成提供了可扩展的路径。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.00425">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.00425">arXiv</a></p>
<hr />
<h3>7. Infinity-RoPE：自回归自展开中涌现的动作可控无限视频生成</h3>
<p><strong>原文标题：</strong> Infinity-RoPE: Action-Controllable Infinite Video Generation Emerges From Autoregressive Self-Rollout</p>
<p><strong>摘要：</strong>
当前的自回归视频扩散模型受限于三个核心瓶颈：(i) 基础模型的三维旋转位置编码（3D-RoPE）所施加的有限时间跨度，(ii) 在长序列展开过程中维持细粒度动作控制的缓慢提示响应性，以及(iii) 无法在单一生成流中实现非连续的影视化转场。我们提出了Infinity-RoPE，一个统一的推理时框架，通过三个相互关联的组件——块相对论RoPE、KV刷新与RoPE截断——共同解决了上述所有限制。块相对论RoPE将时间编码重新表述为一个移动的局部参考系，其中每个新生成的潜在块相对于基础模型的最大帧跨度进行旋转，而较早的块则向后旋转以保持相对的时间几何关系。这种相对论表述消除了固定的时间位置，使得视频生成能够持续进行，远超基础位置编码的限制。为了在不重新编码的情况下实现细粒度的动作控制，KV刷新通过仅保留两个潜在帧（全局汇点与最后生成的潜在帧）来更新KV缓存，从而确保即时的提示响应性。最后，RoPE截断在时间RoPE坐标中引入受控的不连续性，使得在单一连续展开中能够实现多镜头场景转场。这些组件共同构成了Infinity-RoPE，作为一个无需训练的基础框架，支持无限跨度、可控且具有影视化效果的视频扩散。综合实验表明，Infinity-RoPE在整体VBench评分上持续超越先前的自回归模型。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.20649">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.20649">arXiv</a></p>
<hr />
<h3>8. 一致性评判器：通过参考引导的注意力对齐修正生成图像中的不一致性</h3>
<p><strong>原文标题：</strong> The Consistency Critic: Correcting Inconsistencies in Generated Images via Reference-Guided Attentive Alignment</p>
<p><strong>摘要：</strong>
先前研究已探索了基于参考图像的多种定制化生成任务，但这些方法在生成具有一致性的细粒度细节方面仍存在局限。本文旨在通过采用参考引导的后编辑方法解决生成图像的不一致性问题，并提出我们的ImageCritic系统。我们首先通过基于视觉语言模型的选择与显式退化处理，构建了包含参考图像-退化图像-目标图像的三元组数据集，该数据集有效模拟了现有生成模型中常见的细节偏差与不一致现象。在此基础上，通过对模型注意力机制与内在表征的深入分析，我们相应设计了注意力对齐损失函数与细节编码器，以实现对不一致区域的精准修正。ImageCritic可集成至智能体框架中，在复杂场景下通过多轮局部编辑自动检测并修正不一致区域。大量实验表明，ImageCritic能在多种定制化生成场景中有效解决细节相关问题，相较于现有方法取得了显著提升。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.20614">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.20614">arXiv</a></p>
<hr />
<h3>9. TUNA：面向原生统一多模态模型的统一视觉表征驯化</h3>
<p><strong>原文标题：</strong> TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models</p>
<p><strong>摘要：</strong>
统一多模态模型旨在单一框架内联合实现多模态理解与生成任务。本文提出TUNA——一种原生统一多模态模型，通过级联变分自编码器编码器与表征编码器构建统一的连续视觉表征空间。该统一表征空间支持对图像与视频进行端到端的理解与生成处理。相较于采用解耦表征的现有统一多模态模型，TUNA的统一视觉空间避免了因独立编码器引入的表征格式失配问题，在理解与生成任务上均优于解耦方案。此外，我们发现更强的预训练表征编码器能在所有多模态任务中持续提升性能，这凸显了表征编码器的重要性。最终，在此统一框架下，联合训练理解与生成数据能使两项任务相互促进而非相互干扰。我们在多模态理解与生成基准测试中的大量实验表明，TUNA在图像/视频理解、图像/视频生成及图像编辑任务上均取得最先进性能，验证了其统一表征设计的有效性与可扩展性。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.02014">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.02014">arXiv</a></p>
<hr />
<h3>10. LFM2技术报告</h3>
<p><strong>原文标题：</strong> LFM2 Technical Report</p>
<p><strong>摘要：</strong>
本文提出LFM2系列液态基础模型，该模型家族专为高效的设备端部署与强大的任务能力而设计。通过在边缘延迟和内存约束下进行硬件在环架构搜索，我们获得了一个紧凑的混合主干网络，该网络将门控短卷积与少量分组查询注意力模块相结合，在CPU上相比同等规模模型实现了最高2倍的预填充和解码速度提升。LFM2系列涵盖3.5亿至83亿参数规模，包括稠密模型（3.5亿/7亿/12亿/26亿参数）和专家混合变体（83亿总参数/15亿激活参数），所有模型均支持32K上下文长度。LFM2的训练流程包含：采用避免支持失配的温和解耦Top-K知识蒸馏目标、基于难度排序数据的课程学习，以及包含监督微调、长度归一化偏好优化和模型融合的三阶段后训练方案。基于10-12万亿token预训练的LFM2模型在多样化基准测试中表现优异，例如LFM2-26B在IFEval达到79.56%，在GSM8K达到82.41%。我们进一步构建了多模态与检索变体：面向视觉语言任务的LFM2-VL、面向语音的LFM2-Audio，以及面向检索的LFM2-ColBERT。LFM2-VL通过令牌高效的视觉处理支持可调节的精度-延迟权衡；LFM2-Audio分离音频输入输出路径，可实现与三倍规模模型相媲美的实时语音交互；LFM2-ColBERT提供面向查询和文档的低延迟编码器，支持跨语言高性能检索。所有模型均以开放权重形式发布，并提供适用于ExecuTorch、llama.cpp和vLLM的部署套件，使LFM2成为需要快速、内存高效推理与强大任务能力的边缘应用的实用基础框架。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.23404">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.23404">arXiv</a></p>
<hr />
<h3>11. Wikontic：利用大语言模型构建与Wikidata对齐、本体感知的知识图谱</h3>
<p><strong>原文标题：</strong> Wikontic: Constructing Wikidata-Aligned, Ontology-Aware Knowledge Graphs with Large Language Models</p>
<p><strong>摘要：</strong>
知识图谱为大型语言模型提供了结构化、可验证的基础，但当前基于大语言模型的系统通常仅将知识图谱作为文本检索的辅助结构，其内在质量尚未得到充分探索。本研究提出Wikontic，一个多阶段处理流程，能够从开放域文本中构建知识图谱。该流程通过提取带限定条件的候选三元组、实施基于Wikidata的类型与关系约束，并对实体进行规范化以减少重复，最终生成紧凑、符合本体论且连接性良好的知识图谱。在MuSiQue数据集上，正确答案实体出现在96%的生成三元组中。在HotpotQA任务中，仅使用三元组的设置取得了76.0的F1分数，在MuSiQue上达到59.8 F1，匹配或超越了多个仍需依赖文本上下文的检索增强生成基线方法。此外，Wikontic在MINE-1基准测试中取得了最先进的信息保留性能（86%），优于先前的知识图谱构建方法。Wikontic在构建阶段也表现出高效性：知识图谱构建消耗少于1,000个输出标记，约为AriGraph的三分之一，不足GraphRAG的二十分之一。所提出的流程提升了生成知识图谱的质量，并为在大语言模型中利用结构化知识提供了可扩展的解决方案。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.00590">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.00590">arXiv</a></p>
<hr />
<h3>12. 基于优化视角的大语言模型思维校正</h3>
<p><strong>原文标题：</strong> Rectifying LLM Thought from Lens of Optimization</p>
<p><strong>摘要：</strong>
大语言模型（LLM）的最新进展主要得益于其涌现的推理能力，尤其是通过长链思维（CoT）提示实现的深入探索与思考。尽管取得这些进步，采用长链CoT的LLM仍常表现出次优推理行为，例如过度思考与推理链过长等问题，这可能损害模型性能。本文从优化视角分析推理过程，将CoT框架化为梯度下降过程，其中每个推理步骤构成向问题解决的迭代更新。基于此视角，我们提出RePro（校正过程级奖励）方法，这是一种在训练后阶段优化LLM推理的新途径。RePro通过定义代理目标函数来评估CoT背后的优化过程，采用双重评分机制量化其强度与稳定性。这些评分被整合为复合过程级奖励，无缝集成至带可验证奖励的强化学习（RLVR）流程中以优化LLM。通过在数学、科学与编程等多领域基准测试上，对多种强化学习算法及不同LLM开展的大量实验表明，RePro能持续提升推理性能并有效缓解次优推理行为。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.01925">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.01925">arXiv</a></p>
<hr />
<h3>13. Flash-DMD：通过高效蒸馏与联合强化学习实现高保真少步图像生成</h3>
<p><strong>原文标题：</strong> Flash-DMD: Towards High-Fidelity Few-Step Image Generation with Efficient Distillation and Joint Reinforcement Learning</p>
<p><strong>摘要：</strong>
扩散模型已成为生成模型中的主导类别，但其迭代采样过程计算成本高昂。时间步蒸馏是一种有前景的加速生成技术，但通常需要大量训练并导致图像质量下降。此外，使用强化学习针对特定目标（如审美吸引力或用户偏好）对这些蒸馏模型进行微调，存在众所周知的训练不稳定问题，且极易陷入奖励破解。本文提出Flash-DMD，一种通过蒸馏实现快速收敛并结合基于强化学习的联合优化的新型框架。具体而言，我们首先提出一种高效的时间步感知蒸馏策略，该策略在显著降低训练成本的同时增强了生成真实性，仅需DMD2模型2.1%的训练成本即可实现更优性能。其次，我们引入一种联合训练方案，在模型通过强化学习目标进行微调的同时，时间步蒸馏训练持续并行进行。我们证明，来自持续蒸馏过程的稳定、定义明确的损失函数可作为强大的正则化器，有效稳定强化学习训练过程并防止策略崩溃。在基于分数和流匹配模型上的大量实验表明，我们所提出的Flash-DMD不仅收敛速度显著加快，而且在少步采样机制下实现了最先进的生成质量，在视觉质量、人类偏好和图文对齐指标上均优于现有方法。本研究为训练高效、高保真且稳定的生成模型提供了一种有效范式。代码即将发布。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.20549">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.20549">arXiv</a></p>
<hr />
<h3>14. VLASH：通过未来状态感知异步推理实现实时视觉语言动作模型</h3>
<p><strong>原文标题：</strong> VLASH: Real-Time VLAs via Future-State-Aware Asynchronous Inference</p>
<p><strong>摘要：</strong>
视觉-语言-动作模型（VLAs）在多样化机器人任务中正展现出日益强大的能力。然而，其在实际部署中仍存在延迟高、效率低的问题：演示视频常需加速5-10倍才能呈现流畅效果，且存在明显的动作停滞与环境响应延迟。异步推理通过使机器人能够同时执行动作与进行推理，为实现连续低延迟控制提供了可行方案。但受推理过程中机器人与环境持续演化的影响，预测区间与执行区间会产生时序错位，导致显著的动作失稳现象。现有方法往往以牺牲准确性或增加运行时开销为代价来缓解该问题。本文提出VLASH——一种通用的VLA异步推理框架，无需额外开销或架构修改即可实现平滑、准确、快速的反应控制。VLASH通过将机器人状态与先前生成的动作片段进行前向推演，预估未来执行时刻的状态，从而弥合预测与执行间的时序鸿沟。实验表明，相较于同步推理，VLASH在完全保持原始准确性的同时，最高可实现2.03倍的加速效果，并将反应延迟降低达17.4倍。此外，该框架使VLAs能够胜任乒乓球对战、打地鼠等需要快速反应与高精度操控的任务，而传统同步推理方法在此类任务中均告失效。代码已开源：https://github.com/mit-han-lab/vlash</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.01031">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.01031">arXiv</a></p>
<hr />
<h3>15. GR-RL：面向长时序机器人灵巧操控的精细控制框架</h3>
<p><strong>原文标题：</strong> GR-RL: Going Dexterous and Precise for Long-Horizon Robotic Manipulation</p>
<p><strong>摘要：</strong>
本文提出GR-RL——一种机器人学习框架，可将通用视觉-语言-动作策略转化为擅长长时序灵巧操控的专用策略。现有视觉-语言-动作策略的核心假设是人类演示具有最优性，但我们指出在高度灵巧且需精密控制的任务中，人类演示存在噪声且非最优。GR-RL设计了一个多阶段训练流程，通过强化学习对演示数据进行筛选、增强与优化：首先学习视觉-语言条件化的任务进度函数，筛选演示轨迹并仅保留对进度有积极贡献的状态转移。具体而言，我们证明通过直接应用稀疏奖励的离线强化学习，所得Q值可作为鲁棒的进度函数。其次，引入形态对称增强方法，显著提升GR-RL的泛化能力与性能。最后，为更好地对齐视觉-语言-动作策略与高精度控制场景下的部署行为，我们通过隐空间噪声预测器进行在线强化学习。该框架使GR-RL成为首个基于学习的策略，能够以83.3%的成功率自主完成穿鞋带任务——该任务需贯穿多个鞋孔，要求长时序推理、毫米级精度及柔性体顺应交互。我们希望GR-RL能为通用机器人基础模型向可靠现实场景专家系统的转化提供技术路径。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.01801">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.01801">arXiv</a></p>
<hr />
<h3>16. InternVideo-Next：迈向无需视频-文本监督的通用视频基础模型</h3>
<p><strong>原文标题：</strong> InternVideo-Next: Towards General Video Foundation Models without Video-Text Supervision</p>
<p><strong>摘要：</strong>
大规模视频-文本预训练虽能实现强劲性能，但依赖于语义覆盖有限且带有噪声的合成描述，往往忽略了物体运动、三维几何与物理线索等隐含世界知识。相比之下，掩码视频建模方法直接利用时空结构，但在通用任务上仍落后于文本监督方法。我们发现这一差距源于被忽视的架构问题：像素级重建面临收敛困难，其低层次要求常与语义目标冲突；而潜在特征预测则易引发捷径学习。为解决这些问题，我们将传统编码器-解码器架构解耦为编码器-预测器-解码器框架，其中预测器充当潜在世界模型，并提出InternVideo-Next——一个两阶段预训练方案，为该世界模型构建语义一致且细节保留的潜在空间。首先，像素级掩码视频建模中传统的线性解码器强制预测器输出特征需线性映射至像素空间，导致其与语义抽象目标产生冲突。我们的第一阶段提出条件扩散解码器，并注入可靠的图像级语义先验以增强语义表达与收敛性，从而在像素级保真度与高层语义抽象间建立桥梁。第二阶段通过在该空间内预测冻结的第一阶段目标来进一步学习世界知识，有效缓解捷径学习问题。在公开无标注视频数据上训练的InternVideo-Next在多项基准测试中取得最先进成果，为通用视频表征学习提供了可扩展的技术路径。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.01342">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.01342">arXiv</a></p>
<hr />
<h3>17. SpeContext：通过推测性上下文稀疏性实现大语言模型的高效长上下文推理</h3>
<p><strong>原文标题：</strong> SpeContext: Enabling Efficient Long-context Reasoning with Speculative Context Sparsity in LLMs</p>
<p><strong>摘要：</strong>
本文指出，检索算法的目标与大语言模型（LLM）对齐，这与大语言模型中知识蒸馏的目标相似。我们从信息论角度分析了蒸馏语言模型（DLM）与原始大语言模型在信息关注点上的相似性，进而提出一种利用DLM作为检索算法的新范式。基于这一洞见，我们提出了SpeContext——一种面向长上下文推理的算法与系统协同设计框架。（1）在算法层面，SpeContext基于DLM的头部注意力权重设计了轻量级检索头，通过剪枝冗余参数实现了超过90%的参数压缩。（2）在系统层面，SpeContext通过弹性加载策略设计了异步预取数据流，有效实现了KV缓存检索与大语言模型计算的重叠执行。（3）在编译层面，SpeContext构建了理论内存模型并实现了自适应内存管理系统，通过最大化GPU内存利用率实现加速。我们在云端和边缘两种资源受限环境中部署并评估了SpeContext。大量实验表明，与Huggingface框架相比，SpeContext在云端实现了最高24.89倍的吞吐量提升，在边缘端实现了10.06倍的加速，且精度损失可忽略不计，从而推动了精度与吞吐量的帕累托前沿边界。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.00722">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.00722">arXiv</a></p>
<hr />
<h3>18. 流更直更快：基于整流轨迹MeanFlow的高效一步生成建模</h3>
<p><strong>原文标题：</strong> Flow Straighter and Faster: Efficient One-Step Generative Modeling via MeanFlow on Rectified Trajectories</p>
<p><strong>摘要：</strong>
基于流的生成模型近期展现出强大性能，但其采样过程通常依赖于昂贵的常微分方程数值积分。整流流方法通过学习近似直线的概率路径实现一步采样，但获得这种直线性需要多次计算密集的整流迭代。MeanFlow通过直接建模时间平均速度实现一步生成，然而在高度弯曲的流上训练时，其收敛速度缓慢且监督信号存在噪声。为克服这些局限，我们提出整流平均流框架，该框架仅需单次整流步骤即可沿整流轨迹建模平均速度场。这种方法在避免完美直线轨迹需求的同时实现了高效训练。此外，我们引入一种简单有效的截断启发式方法，旨在减少残余曲率并进一步提升性能。在64×64、256×256和512×512分辨率ImageNet数据集上的大量实验表明，Re-MeanFlow在样本质量和训练效率方面均优于现有的一步流蒸馏方法与整流流方法。代码已发布于https://github.com/Xinxi-Zhang/Re-MeanFlow。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.23342">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.23342">arXiv</a></p>
<hr />
<h3>19. 基于分层令牌压缩的流式视频大语言模型加速方法</h3>
<p><strong>原文标题：</strong> Accelerating Streaming Video Large Language Models via Hierarchical Token Compression</p>
<p><strong>摘要：</strong>
流式视频大语言模型（VideoLLMs）在各种视频理解任务中展现出卓越性能，但由于处理连续视频流中密集视觉令牌的高计算成本，其在实时部署中面临重大挑战。在流式视频场景中，主要瓶颈在于视觉变换器（ViT）编码阶段——对时间相似帧的冗余处理导致效率低下。此外，大语言模型预填充阶段膨胀的令牌序列进一步加剧了延迟和内存开销。为应对这些挑战，我们提出流式令牌压缩（STC），一种即插即用的分层框架，可无缝集成到现有流式VideoLLMs中，通过优化ViT编码和大语言模型预填充阶段实现处理加速。STC引入两个令牌级加速器：STC-Cacher通过缓存并重用时间相似帧的特征降低ViT编码开销；STC-Pruner在视觉令牌序列输入大语言模型前对其进行压缩，基于时空相关性仅保留最显著的令牌。在五个基准测试中对四种主流流式VideoLLMs的广泛实验表明，STC性能优于其他压缩方法。值得注意的是，STC在ReKV框架上保持高达99%的准确率，同时将ViT编码延迟和大语言模型预填充延迟分别降低24.5%和45.3%。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.00891">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.00891">arXiv</a></p>
<hr />
<h3>20. 文化褪色之处：揭示文本到图像生成中的文化鸿沟</h3>
<p><strong>原文标题：</strong> Where Culture Fades: Revealing the Cultural Gap in Text-to-Image Generation</p>
<p><strong>摘要：</strong>
多语言文本到图像生成模型在视觉真实性与语义对齐方面进展迅速，现已得到广泛应用。然而，其输出结果在不同文化语境中存在显著差异：由于语言承载着文化内涵，基于多语言提示生成的图像应当保持跨语言的文化一致性。我们通过综合分析发现，当前文本到图像模型在处理多语言提示时，常产生文化中性或偏向英语文化的结果。对两个代表性模型的分析表明，该问题并非源于文化知识的缺失，而是由文化相关表征激活不足所致。我们提出一种探测方法，可将文化敏感信号定位至少数固定层中的特定神经元集合。基于这一发现，我们引入两种互补的对齐策略：（1）无需微调主干模型的推理时文化激活技术，通过增强已识别神经元的响应实现文化表征强化；（2）针对文化相关层的定向增强方法，仅更新与文化关联密切的模型层。在自建文化评估基准CultureBench上的实验表明，该方法在保持生成质量与多样性的同时，相较于现有基线模型实现了文化一致性的持续提升。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.17282">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.17282">arXiv</a></p>
<hr />
<h3>21. PromptBridge：面向大语言模型的跨模型提示迁移框架</h3>
<p><strong>原文标题：</strong> PromptBridge: Cross-Model Prompt Transfer for Large Language Models</p>
<p><strong>摘要：</strong>
大语言模型已成为代码生成、数学推理和智能体工作流等应用的核心支撑。在实际应用中，系统通常通过商业API或开源部署调用大语言模型，而模型生态（如GPT、Claude、Llama等）正经历快速迭代。这种快速演进使得用户需要频繁切换模型，切换动因包括性能差异、成本考量、部署限制及隐私需求等。然而，提示词具有显著的模型敏感性：针对特定模型优化的提示词直接迁移至其他模型时，其性能往往远低于针对目标模型专门优化的提示词。我们将此现象称为“模型漂移”。通过对多种大语言模型配置的广泛实证分析，我们发现模型漂移现象普遍存在且影响显著。为应对这一挑战，本文提出PromptBridge——一种无需训练的框架，旨在保持模型切换时提示词的有效性，实现无需针对每项任务或每个模型进行高成本重新优化的跨模型提示迁移。该框架仅需少量对齐任务进行校准：首先通过“模型自适应反射式提示演进”方法，借助迭代式反射优化与量化评估，获得面向特定任务与模型的最优提示词；随后利用源模型与目标模型校准后的提示词对，学习跨模型提示映射关系。在测试阶段（即面对未见任务时），给定源模型提示词，该映射可直接生成针对目标模型的优化提示词。在单智能体与多智能体场景下的实验表明，PromptBridge能持续提升下游任务准确率，同时显著降低迁移成本。相关代码即将公开。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.01420">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.01420">arXiv</a></p>
<hr />
<h3>22. SCALE：选择性资源分配以克服数学测试时扩展中的性能瓶颈</h3>
<p><strong>原文标题：</strong> SCALE: Selective Resource Allocation for Overcoming Performance Bottlenecks in Mathematical Test-time Scaling</p>
<p><strong>摘要：</strong>
测试时计算扩展已成为一种强大的范式，通过在推理过程中分配额外计算资源来增强大型语言模型（LLM）的数学推理能力。然而，现有方法对所有推理子问题采用均匀的资源分配，这造成了根本性瓶颈：具有挑战性的子问题未能获得足够关注，而常规操作却消耗了不成比例的资源。这种均匀分配导致性能瓶颈，使得额外计算资源的投入产生边际效益递减。受双过程理论启发，我们提出SCALE（选择性资源分配）框架，该框架根据子问题难度选择性分配计算资源。SCALE通过四个阶段运行：（1）将问题分解为顺序推理子问题；（2）评估每个子问题的难度，以区分常规操作与计算密集型挑战性子问题；（3）为简单子问题分配系统1处理模式，为复杂子问题分配系统2处理模式；（4）结合上下文传递的顺序执行。通过将资源集中于挑战性子问题，同时高效处理常规操作，SCALE在显著提升性能的同时实现了更优的资源利用率。大量实验表明，SCALE显著优于均匀扩展基线方法，在AIME25数据集上准确率最高提升13.75个百分点（从57.50%提升至71.25%），同时降低33%-53%的计算成本。这代表了测试时扩展领域的重要进展，有效解决了现有方法的根本性局限。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.00466">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.00466">arXiv</a></p>
<hr />
<h3>23. MultiBanana：一个面向多参考文本到图像生成的挑战性基准</h3>
<p><strong>原文标题：</strong> MultiBanana: A Challenging Benchmark for Multi-Reference Text-to-Image Generation</p>
<p><strong>摘要：</strong>
近期的文本到图像生成模型已具备多参考生成与编辑的能力，即能够从多张参考图像中继承目标对象的外观特征，并在新情境下进行重新渲染。然而，现有基准数据集通常侧重于单张或少量参考图像的生成任务，这限制了我们在不同多参考条件下评估模型性能进展或指认其弱点的能力。此外，现有任务定义仍较为模糊，通常局限于“编辑内容”或“参考数量”等单一维度，未能充分捕捉多参考场景的内在复杂性。为填补这一空白，我们提出了MultiBanana基准，其通过广泛覆盖大规模多参考特有问题来系统评估模型的能力边界：（1）参考图像数量的变化，（2）参考图像间的领域不匹配（例如照片与动漫风格），（3）参考场景与目标场景的尺度差异，（4）包含罕见概念的参考图像（例如红色香蕉），以及（5）多语言文本参考的渲染需求。通过对多种文本到图像模型的分析，我们揭示了其优势表现、典型失败模式及改进方向。MultiBanana将作为开放基准发布，旨在推动多参考图像生成领域的发展，并为公平比较建立标准化基础。相关数据与代码已公开于https://github.com/matsuolab/multibanana。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.22989">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.22989">arXiv</a></p>
<hr />
<h3>24. Script：面向多模态大语言模型的图结构及查询条件语义令牌剪枝方法</h3>
<p><strong>原文标题：</strong> Script: Graph-Structured and Query-Conditioned Semantic Token Pruning for Multimodal Large Language Models</p>
<p><strong>摘要：</strong>
多模态大语言模型（MLLMs）中视觉令牌数量的快速增长导致内存消耗和推理延迟显著增加，尤其是在处理高分辨率图像和视频时。令牌剪枝技术通过消除冗余来缓解这一问题，但现有方法往往忽略与用户查询的相关性，或受限于注意力机制的缺陷，从而降低了其适应性和有效性。为应对这些挑战，我们提出Script——一种无需重新训练且可泛化至多种MLLMs的即插即用式剪枝方法。Script包含两个核心模块：图结构剪枝模块用于去除视觉冗余令牌，以及查询条件语义剪枝模块用于保留与查询相关的视觉信息。二者协同工作，显著提升了多模态任务性能。在涵盖图像与视频理解任务的十四个基准测试上的实验表明，相较于现有剪枝方法，Script在模型效率和预测准确性方面均表现更优。在LLaVA-NeXT-7B模型上，该方法实现了最高6.8倍的预填充加速和10倍的浮点运算量削减，同时保持了原模型96.88%的性能表现。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.01949">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.01949">arXiv</a></p>
<hr />
<h3>25. Lotus-2：基于强大图像生成模型的几何密集预测进展</h3>
<p><strong>原文标题：</strong> Lotus-2: Advancing Geometric Dense Prediction with Powerful Image Generative Model</p>
<p><strong>摘要：</strong>
从单张图像中恢复逐像素的几何属性本质上是病态问题，其原因在于外观的模糊性以及二维观测与三维结构之间的非单射映射关系。尽管判别式回归模型通过大规模监督学习取得了优异性能，但其成功受限于可用数据的规模、质量与多样性，以及有限的物理推理能力。近期扩散模型展现出强大的世界先验，能够编码从海量图文数据中学到的几何与语义信息，然而直接沿用其随机生成范式对于确定性几何推理并非最优：前者旨在实现多样且高保真的图像生成，而后者则需要稳定且精确的预测。本研究提出Lotus-2——一个用于实现稳定、精确且细粒度几何密集预测的两阶段确定性框架，旨在提供一种最优适配方案以充分挖掘预训练生成先验的潜力。具体而言，在第一阶段，核心预测器采用基于干净数据目标的单步确定性建模，并配备轻量级局部连续性模块，以生成全局连贯且无网格伪影的结构。在第二阶段，细节锐化器在核心预测器定义的流形内执行约束性多步修正流优化，通过无噪声的确定性流匹配增强细粒度几何细节。仅使用5.9万训练样本（不足现有大规模数据集的1%），Lotus-2在单目深度估计任务中取得了全新的最优性能，并在表面法线预测中达到高度竞争力。这些结果表明，扩散模型能够作为确定性世界先验，实现超越传统判别式与生成范式的高质量几何推理。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.01030">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.01030">arXiv</a></p>
<hr />
<h3>26. StreamGaze：流式视频中的视线引导时序推理与前瞻性理解</h3>
<p><strong>原文标题：</strong> StreamGaze: Gaze-Guided Temporal Reasoning and Proactive Understanding in Streaming Videos</p>
<p><strong>摘要：</strong>
流式视频理解不仅要求模型能够处理时序输入的帧序列，还需要在如增强现实眼镜等实际应用中预测用户意图。尽管现有的流式基准测试评估了时序推理能力，但尚未有研究衡量多模态大语言模型（MLLMs）是否能在流式场景中解读或利用人类视线信号。为填补这一空白，我们提出了StreamGaze——首个专门用于评估MLLMs在流式视频中如何利用视线进行时序与前瞻性推理的基准测试。StreamGaze引入了视线引导的过去、现在与前瞻性任务，全面评估流式视频理解能力。这些任务检验模型能否利用实时视线追踪注意力转移，并仅基于过去及当前观测帧推断用户意图。为构建StreamGaze，我们开发了一套视线-视频问答生成流程，通过注视点提取、区域特异性视觉提示和扫描路径构建，将第一人称视频与原始视线轨迹对齐。该流程生成具有时空 grounded 性的问答对，紧密贴合人类感知动态。在所有StreamGaze任务中，我们发现当前最先进的MLLMs与人类表现之间存在显著性能差距，揭示了基于视线的时序推理、意图建模和前瞻预测方面的根本性局限。我们进一步对视线提示策略、推理行为及任务特定失败模式进行了详细分析，深入探讨了当前MLLMs的不足及未来模型需发展的能力。所有数据与代码将公开发布，以支持视线引导流式视频理解领域的持续研究。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.01707">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.01707">arXiv</a></p>
<hr />
<h3>27. POLARIS：基于投影正交最小二乘的扩散模型鲁棒自适应反演方法</h3>
<p><strong>原文标题：</strong> POLARIS: Projection-Orthogonal Least Squares for Robust and Adaptive Inversion in Diffusion Models</p>
<p><strong>摘要：</strong>
基于扩散模型的反演-去噪范式在多种图像编辑与修复任务中表现卓越。本文重新审视其工作机制，揭示了一个导致重建质量退化的关键但常被忽视的因素：近似噪声误差。该误差源于使用第t-1步的预测值来近似第t步的噪声，从而在整个反演过程中引发严重的误差累积。我们提出一种基于投影正交最小二乘的鲁棒自适应反演方法（POLARIS），该方法将反演问题从误差补偿问题重新定义为误差溯源问题。POLARIS不通过优化嵌入或潜码来抵消累积漂移，而是将引导尺度ω视为步进变量，并推导出具有数学依据的公式以最小化每一步的反演误差。值得注意的是，POLARIS仅需一行代码即可显著提升反演潜码质量。在性能开销可忽略不计的前提下，该方法能有效抑制噪声近似误差，并持续提升下游任务的准确性。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.00369">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.00369">arXiv</a></p>
<hr />
<h3>28. ORION：在思维语言中高效训练语言模型推理能力</h3>
<p><strong>原文标题：</strong> ORION: Teaching Language Models to Reason Efficiently in the Language of Thought</p>
<p><strong>摘要：</strong>
大型推理模型在数学、代码生成和任务规划方面表现出色，但其依赖冗长的“思维”标记链导致高延迟、冗余和推理路径不连贯。受思维语言假说启发——该假说认为人类推理基于一种称为“心理语”的符号化、组合性心理语言——我们提出了一个训练模型以类似紧凑风格进行推理的框架。心理语将抽象推理编码为超压缩的结构化标记，使模型能够以更少的步骤解决复杂问题。为提升效率与准确性，我们提出<strong>短长度偏好优化</strong>方法，这是一种强化学习方法，奖励保持正确的简洁解决方案，同时允许必要时进行更长的推理。应用于心理语对齐模型后，该方法通过实现简洁推理显著提高了压缩率，既保留了详细思维的优势，又避免了计算开销。在AIME 2024/2025、MinervaMath、OlympiadBench、Math500和AMC等基准测试中，我们的ORION模型生成的推理轨迹标记数减少4-16倍，推理延迟降低最高达5倍，训练成本相比DeepSeek R1 Distilled模型降低7-9倍，同时保持其90-98%的准确率。ORION的准确率较Claude和ChatGPT-4o最高提升5%，且保持2倍压缩率。这些结果表明，心理语式压缩推理向类人认知效率迈进了一步，实现了在不牺牲准确性的前提下进行实时、高性价比的推理。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.22891">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.22891">arXiv</a></p>
<hr />
<h3>29. 如苏格拉底般提问：苏格拉底助力视觉语言模型理解遥感图像</h3>
<p><strong>原文标题：</strong> Asking like Socrates: Socrates helps VLMs understand remote sensing images</p>
<p><strong>摘要：</strong>
受DeepSeek-R1启发的多模态推理模型近期显著推动了视觉语言系统的发展。然而在遥感任务中，我们观察到普遍存在的伪推理现象：模型倾向于描述推理过程，而非基于视觉证据进行真正的推理以得出正确答案。我们将此归因于"一瞥效应"，即对大规模遥感图像的单次粗略感知导致理解不完整，并促使模型依赖语言自洽性而非视觉证据进行推理。为解决该问题，我们提出RS-EoT（遥感思维证据链），一种语言驱动的迭代式视觉证据搜寻范式。为植入该范式，我们设计了SocraticAgent——一个通过推理与视觉检查交替循环来合成推理轨迹的自博弈多代理系统。为增强并泛化这些模式，我们提出两阶段渐进式强化学习策略：首先通过细粒度定位任务的强化学习提升RS-EoT能力，再通过遥感视觉问答的强化学习泛化至更广泛的理解场景。实验表明RS-EoT在多个遥感视觉问答与定位基准测试中达到最先进性能。分析显示清晰的推理与证据搜寻迭代循环，证实RS-EoT能有效缓解一瞥效应，实现真正基于证据的推理。代码、数据及模型已开源：https://geox-lab.github.io/Asking_like_Socrates</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.22396">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.22396">arXiv</a></p>
<hr />
<h3>30. 基于指令-策略协同进化的智能体策略优化</h3>
<p><strong>原文标题：</strong> Agentic Policy Optimization via Instruction-Policy Co-Evolution</p>
<p><strong>摘要：</strong>
可验证奖励强化学习（RLVR）显著提升了大型语言模型（LLM）的推理能力，使其能够构建可进行高效多轮交互与工具整合推理的自主智能体。尽管指令是定义智能体的核心协议，但传统RLVR通常依赖于静态的人工设计指令。然而，这些指令对基础模型可能并非最优，且最优指令会随着智能体策略的改进及其与环境交互探索的深入而动态变化。为弥合这一差距，本文提出INSPO——一种创新的指令-策略协同进化框架，将指令优化作为强化学习（RL）循环的动态组成部分。INSPO维护一个动态的指令候选集合，这些指令与问题共同采样；RL循环中的奖励信号会自动归因于每条指令，并定期淘汰低效指令。新指令通过基于策略的反思机制生成与验证：一个基于LLM的优化器分析回放缓冲区中的历史经验，并针对当前策略演化出更有效的策略指导。我们在多轮检索与推理任务上进行了广泛实验，结果表明INSPO显著优于依赖静态指令的强基线方法。INSPO能够发现创新性指令，引导智能体走向更具战略性的推理路径，在仅略微增加计算开销的情况下实现显著的性能提升。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.01945">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.01945">arXiv</a></p>
<hr />
<h3>31. HiconAgent：面向图形用户界面智能体的历史上下文感知策略优化</h3>
<p><strong>原文标题：</strong> HiconAgent: History Context-aware Policy Optimization for GUI Agents</p>
<p><strong>摘要：</strong>
图形用户界面（GUI）智能体需要有效利用历史上下文以执行序列化导航任务。虽然整合过往动作与观察信息能够提升决策质量，但直接使用完整历史记录会导致计算开销过大并引入无关信息干扰。为此，我们提出HiconAgent——一种采用历史上下文感知策略优化（HCPO）方法训练的GUI智能体，旨在实现历史信息的高效精准利用。HCPO通过两个互补组件优化历史信息在采样与策略更新阶段的使用：（1）动态上下文采样（DCS）在采样阶段为智能体提供可变长度的历史记录，使其能自适应选择最相关上下文；（2）锚点引导的历史压缩（AHC）采用双分支策略改进策略更新阶段，其中压缩分支在移除历史观察信息的同时保留历史动作作为信息流锚点。压缩分支与未压缩分支通过历史增强对齐损失进行耦合，在保证效率的同时强化历史使用的一致性。主流GUI导航基准测试表明，该方法具有卓越性能：尽管参数量更小，HiconAgent-3B在GUI-Odyssey数据集上的定位准确率与步骤成功率分别超越GUI-R1-7B模型8.46%和11.32%，同时在AndroidControl与AITW数据集上取得可比结果，并实现最高2.47倍的计算加速与60%的浮点运算量削减。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.01763">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.01763">arXiv</a></p>
<hr />
<h3>32. 非结构化数据流形特征结构学习</h3>
<p><strong>原文标题：</strong> Learning Eigenstructures of Unstructured Data Manifolds</p>
<p><strong>摘要：</strong>
本文提出一种新颖框架，可直接从非结构化数据中学习用于形状与流形分析的谱基，无需传统算子选择、离散化及特征求解过程。基于最优逼近理论，我们通过训练网络在选定探测函数分布上最小化学习基中的重构误差，从而分解隐式逼近算子。对于合适的分布，该框架可视为拉普拉斯算子及其特征分解的近似——这两者在几何处理中具有基础性地位。此外，我们的方法能以统一方式同时恢复谱基、隐式度量的采样密度以及底层算子的特征值。值得注意的是，这种无监督方法无需对数据流形（如网格化或流形维度）进行任何假设，可扩展至任意维度的数据集。针对三维空间表面点云与高维图像流形，本方法无需显式构建算子即可生成有意义的谱基，其性质可与拉普拉斯算子的谱基相媲美。通过用基于学习的方法替代传统算子选择、构建及特征分解流程，本框架为传统处理管线提供了原理清晰、数据驱动的替代方案。这为非结构化数据（特别是高维空间数据）的几何处理开辟了新的可能性。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.01103">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.01103">arXiv</a></p>
<hr />
<h3>33. 大型语言模型测试时计算扩展的艺术</h3>
<p><strong>原文标题：</strong> The Art of Scaling Test-Time Compute for Large Language Models</p>
<p><strong>摘要：</strong>
测试时扩展（TTS）——在推理过程中动态分配计算资源——是提升大型语言模型（LLMs）推理能力的一个前景广阔的方向。然而，目前尚缺乏在相同条件下对主流TTS策略的系统性比较，且模型类型与问题难度对性能的影响仍不明确。为填补这些空白，我们开展了首次大规模的TTS研究，涵盖八个开源LLM（参数量从70亿到2350亿）在四个推理数据集上生成的超过三百亿个标记。我们观察到三个一致趋势：（1）没有单一的TTS策略能在所有情况下占优；（2）推理模型在不同问题难度和推理轨迹长度上表现出明显的轨迹质量差异，可划分为短视野与长视野两类；（3）对于给定模型类型，最优TTS性能随计算预算增加呈单调提升。基于这些发现，我们提出了一个综合考虑问题难度、模型类型和计算预算的实用方案，用于选择最佳TTS策略，为高效的推理时扩展提供了实践指导。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.02008">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.02008">arXiv</a></p>
<hr />
<h3>34. OpenREAD：基于LLM作为评判者的端到端自动驾驶强化开放式推理框架</h3>
<p><strong>原文标题：</strong> OpenREAD: Reinforced Open-Ended Reasoing for End-to-End Autonomous Driving with LLM-as-Critic</p>
<p><strong>摘要：</strong>
近期，两阶段微调策略（例如通过监督微调获取关键驾驶知识，再通过强化微调提升决策与规划能力）在推动知识驱动的自动驾驶范式发展中展现出巨大潜力。然而，监督微调的学习本质仍限制了推理能力的泛化，从而制约了驾驶性能的全面提升。同时，由于场景理解属于开放式问题，其对应奖励难以量化，现有强化微调方法主要应用于下游任务。为突破这些局限，本文提出OpenREAD，一种基于强化开放式推理的视觉语言模型自动驾驶框架，能够实现从高层推理到低层轨迹规划的端到端全流程强化微调。具体而言，我们首先在开源驾驶知识数据集上构建大规模思维链标注，并利用强大的Qwen3大语言模型作为强化微调中的评判者，在奖励建模过程中对开放式问题的推理质量进行量化评估。大量实验证实，联合端到端强化微调在上下游任务中均带来显著提升，使OpenREAD在推理与规划基准测试中达到了最先进的性能水平。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.01830">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.01830">arXiv</a></p>
<hr />
<h3>35. ChronosObserver：基于超空间扩散采样的四维世界构建方法</h3>
<p><strong>原文标题：</strong> ChronosObserver: Taming 4D World with Hyperspace Diffusion Sampling</p>
<p><strong>摘要：</strong>
尽管当前主流的相机控制视频生成模型能够产生电影级效果，但将其直接应用于生成三维一致且高保真的时间同步多视角视频仍面临挑战，而这正是驾驭四维世界的关键能力。现有研究多采用数据增强或测试时优化策略，但这些方法受限于模型泛化能力不足与可扩展性问题。为此，我们提出ChronosObserver——一种无需训练的方法，其核心包含用于表征四维世界场景时空约束的“世界状态超空间”，以及通过超空间同步多视角扩散采样轨迹的“超空间引导采样”机制。实验结果表明，该方法无需对扩散模型进行训练或微调，即可实现高保真、三维一致的时间同步多视角视频生成。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.01481">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.01481">arXiv</a></p>
<hr />
<h3>36. 通用大语言模型在医学基准测试中表现优于临床专用工具</h3>
<p><strong>原文标题：</strong> Generalist Large Language Models Outperform Clinical Tools on Medical Benchmarks</p>
<p><strong>摘要：</strong>
专业临床人工智能助手正快速进入医疗实践领域，常被宣传为比通用大语言模型更安全可靠。然而与前沿模型不同，这些临床工具很少接受独立的定量评估，尽管其对诊断、分诊和指南解读的影响日益增强，这一证据缺口仍构成关键问题。本研究选取两个广泛部署的临床AI系统（OpenEvidence与UpToDate Expert AI）与三种前沿通用大语言模型（GPT-5、Gemini 3 Pro和Claude Sonnet 4.5），通过整合MedQA（医学知识）与HealthBench（临床对齐）任务的千项微型基准测试进行对比评估。通用模型在所有评估维度上均稳定超越临床工具，其中GPT-5获得最高评分；而OpenEvidence与UpToDate在回答完整性、沟通质量、情境感知及系统化安全推理方面存在明显缺陷。研究结果表明，当前市场上推广的临床决策支持工具可能普遍落后于前沿大语言模型，这凸显了在面向患者的临床工作流部署前，建立透明独立评估机制的紧迫性。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.01191">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.01191">arXiv</a></p>
<hr />
<h3>37. 从落叶中看见风：基于视频的不可见力场推断</h3>
<p><strong>原文标题：</strong> Seeing the Wind from a Falling Leaf</p>
<p><strong>摘要：</strong>
计算机视觉领域的一个长期目标是从视频中建模物体运动，然而运动背后的表征——即导致物体形变与移动的不可见物理相互作用——在很大程度上仍未得到充分探索。本文研究如何从视觉观测中恢复不可见的力场，例如通过观察一片落叶的运动来估计风场。我们的核心创新在于提出了一种端到端可微分的逆向图形学框架，该框架能够直接从视频中联合建模物体几何结构、物理属性与相互作用关系。通过反向传播机制，该方法实现了从物体运动中恢复力场表征。我们在合成场景与真实场景中验证了方法的有效性，实验结果表明该方法能够从视频中推断出合理的力场分布。此外，我们展示了该方法在基于物理的视频生成与编辑等领域的潜在应用价值。本研究为理解与建模像素背后的物理过程提供了新思路，有助于弥合视觉感知与物理规律之间的认知鸿沟。更多视频结果请访问项目页面：https://chaoren2357.github.io/seeingthewind/。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.00762">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.00762">arXiv</a></p>
<hr />
<h3>38. WiseEdit：面向认知与创意驱动型图像编辑的基准评测框架</h3>
<p><strong>原文标题：</strong> WiseEdit: Benchmarking Cognition- and Creativity-Informed Image Editing</p>
<p><strong>摘要：</strong>
当前图像编辑模型展现出新一代智能特性，能够支持基于认知与创意的图像编辑。然而，现有评测基准的评估范围过于局限，难以全面衡量这些高级能力。为此，我们提出WiseEdit——一个知识密集型的综合性评测基准，用于系统评估认知与创意驱动型图像编辑能力。该基准具有任务深度大、知识覆盖面广的特点。借鉴人类认知创作过程，WiseEdit将图像编辑解构为感知、解析与想象三个级联步骤，每个步骤对应特定任务，旨在检验模型在该环节的处理能力。同时，基准还包含需要综合处理三个步骤的复合型复杂任务。此外，WiseEdit整合了陈述性知识、程序性知识和元认知知识三大基础知识类型。最终构建的评测集包含1,220个测试案例，客观揭示了当前前沿图像编辑模型在基于知识的认知推理与创意构图能力方面的局限性。本基准框架、评估代码及各模型生成图像将于近期公开发布。项目主页：https://qnancy.github.io/wiseedit_project_page/。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.00387">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.00387">arXiv</a></p>
<hr />
<h3>39. CauSight：面向视觉因果发现的学习式超感知</h3>
<p><strong>原文标题：</strong> CauSight: Learning to Supersense for Visual Causal Discovery</p>
<p><strong>摘要：</strong>
因果思维使人类不仅能理解所见之物，更能洞悉其发生的原因。为在现代人工智能系统中复现这种能力，我们提出了视觉因果发现这一任务。该任务要求模型在多样场景中推断视觉实体间的因果关系，而非仅仅感知其存在。为此，我们首先构建了视觉因果图数据集（VCG-32K），这是一个包含超过32,000张标注了实体级因果图的大规模图像集合；进而开发了CauSight——一种通过因果感知推理进行视觉因果发现的新型视觉语言模型。我们的训练方案整合了三个核心部分：（1）基于VCG-32K的训练数据构建，（2）用于合成推理轨迹的因果思维树（ToCT），以及（3）通过设计的因果奖励进行强化学习以优化推理策略。实验表明，CauSight在视觉因果发现任务上显著优于GPT-4.1，实现了超过三倍的性能提升（绝对增益达21%）。我们的代码、模型及数据集已在项目页面全面开源：https://github.com/OpenCausaLab/CauSight。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.01827">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.01827">arXiv</a></p>
<hr />
<h3>40. DreamingComics：基于视频模型的主体与布局定制化生成的故事可视化流程</h3>
<p><strong>原文标题：</strong> DreamingComics: A Story Visualization Pipeline via Subject and Layout Customized Generation using Video Models</p>
<p><strong>摘要：</strong>
现有故事可视化方法通常仅依赖文本来定位主体，且在维持艺术风格一致性方面面临挑战。为克服这些局限，本文提出DreamingComics——一种具备布局感知能力的故事可视化框架。该框架基于预训练的视频扩散变换器（DiT）模型构建，利用其时空先验知识增强角色身份与风格一致性。为实现基于布局的位置控制，我们提出RegionalRoPE区域感知位置编码方案，该方案根据目标布局对嵌入表示进行重索引。此外，我们引入掩码条件损失函数，进一步将每个主体的视觉特征约束至其指定区域。为从自然语言脚本推断布局，我们集成基于大语言模型的布局生成器，该生成器经训练可生成漫画风格布局，从而实现灵活可控的布局条件控制。通过系统性评估表明：相较于现有方法，本方法在角色一致性指标上提升29.2%，在风格相似度指标上提升36.2%，同时展现出较高的空间准确性。项目页面详见：https://yj7082126.github.io/dreamingcomics/</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.01686">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.01686">arXiv</a></p>
<hr />
<h3>41. IndicParam：面向低资源印度语言的大语言模型评估基准</h3>
<p><strong>原文标题：</strong> IndicParam: Benchmark to evaluate LLMs on low-resource Indic Languages</p>
<p><strong>摘要：</strong>
尽管大语言模型在高资源多语言任务中表现优异，但针对低资源及极低资源印度语言的系统性评估仍严重不足。本文提出IndicParam——一个包含超过13,000道人工标注选择题的评估基准，涵盖11种印度语言（低资源语言：尼泊尔语、古吉拉特语、马拉地语、奥里亚语；极低资源语言：多格拉语、迈蒂利语、拉贾斯坦语、梵语、博多语、桑塔利语、孔卡尼语）以及梵英混合语料集。通过对19个开源与闭源大语言模型的评估，我们发现即使表现最佳的GPT-5平均准确率也仅达45.0%，其次为DeepSeek-3.2（43.1%）和Claude-4.5（42.7%）。我们进一步将问题标注为知识导向型或纯语言型，以区分事实记忆能力与语法掌握能力。此外，除了传统选择题外，我们还评估了大语言模型处理多样化题型的能力，包括列表匹配题、论断推理对题和序列排序题。IndicParam揭示了跨语言迁移的局限性，为印度语言建立了具有挑战性的评估基准。数据集发布于https://huggingface.co/datasets/bharatgenai/IndicParam，基准测试脚本可通过https://github.com/ayushbits/IndicParam获取。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.00333">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.00333">arXiv</a></p>
<hr />
<h3>42. 一种用于超限肢体人形机器人步态的分层控制框架</h3>
<p><strong>原文标题：</strong> A Hierarchical Framework for Humanoid Locomotion with Supernumerary Limbs</p>
<p><strong>摘要：</strong>
在人形机器人上集成超限肢体（SLs）会引入动态扰动，从而带来显著的稳定性挑战。本研究通过设计一种新颖的分层控制架构来解决这一问题，以提升配备超限肢体的人形机器人的步态稳定性。该框架的核心是一种解耦策略，将基于学习的步态控制与基于模型的平衡控制相结合。底层组件通过模仿学习与课程学习，为Unitree H1人形机器人生成行走步态。高层组件则主动利用超限肢体进行动态平衡调节。该系统在基于物理的仿真环境中通过三种条件进行评估：无负载人形机器人的基准步态（基准行走）、携带静态超限肢体负载行走（静态负载）以及使用主动动态平衡控制器行走（动态平衡）。评估结果表明，动态平衡控制器有效提升了稳定性。与静态负载条件相比，该平衡策略产生的步态模式更接近基准步态，并将质心轨迹的动态时间规整（DTW）距离降低了47%。此外，平衡控制器改善了步态周期内的再稳定能力，并实现了更协调的反相地面反作用力（GRF）模式。研究结果证明，这种解耦的分层设计能够有效缓解由超限肢体的质量与运动产生的内部动态干扰，从而使配备功能化超限肢体的人形机器人实现稳定运动。代码与演示视频可见：https://github.com/heyzbw/HuSLs。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.00077">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.00077">arXiv</a></p>
<hr />
<h3>43. 基于三维点轨迹的生成式视频运动编辑</h3>
<p><strong>原文标题：</strong> Generative Video Motion Editing with 3D Point Tracks</p>
<p><strong>摘要：</strong>
相机与物体运动是视频叙事的关键要素。然而，对这些已捕捉运动进行精确编辑仍面临重大挑战，尤其在复杂物体运动场景下更为突出。现有运动控制的图像到视频方法常因缺乏完整场景上下文而难以实现一致性视频编辑，而视频到视频方法虽能提供视角变换或基础物体位移，却对细粒度物体运动的控制能力有限。本文提出一种轨迹条件化的视频到视频框架，能够实现相机与物体运动的联合编辑。该框架通过将视频生成模型约束于源视频及其配对的、表征源运动与目标运动的三维点轨迹来实现编辑功能。这些三维轨迹建立的稀疏对应关系，可在保持时空连贯性的同时，将丰富上下文信息从源视频迁移至新运动状态。关键之处在于，相较于二维轨迹，三维轨迹能提供显式深度线索，使模型能够解析深度顺序并处理遮挡问题，从而实现精确运动编辑。通过合成数据与真实数据的双阶段训练，本模型支持多种运动编辑任务，包括相机/物体联合操控、运动迁移及非刚性形变，为视频编辑领域开拓了新的创作可能性。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.02015">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.02015">arXiv</a></p>
<hr />
<h3>44. MEGConformer：基于Conformer的稳健语音与音素分类MEG解码器</h3>
<p><strong>原文标题：</strong> MEGConformer: Conformer-Based MEG Decoder for Robust Speech and Phoneme Classification</p>
<p><strong>摘要：</strong>
本文针对LibriBrain 2025 PNPL竞赛提出基于Conformer架构的解码器，聚焦于脑磁图（MEG）信号处理的两项基础任务：语音检测与音素分类。该方法将轻量化Conformer模型适配于原始306通道MEG信号，通过轻量卷积投影层与任务专用输出头实现特征提取。在语音检测任务中，我们首次探索了面向MEG信号的SpecAugment数据增强策略；在音素分类任务中，采用逆平方根类别加权与动态分组加载器处理百样本平均数据。此外，简单的实例级归一化策略对抑制预留数据集分布偏移具有关键作用。基于官方标准赛道划分方案与宏平均F1分数进行模型选择，我们的最优系统在排行榜上分别取得88.9%（语音检测）与65.8%（音素分类）的评估结果，超越竞赛基线模型并在两项任务中均位列前十。完整技术文档、源代码与模型检查点可通过https://github.com/neural2speech/libribrain-experiments获取。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.01443">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.01443">arXiv</a></p>
<hr />
<h3>45. 多普勒增强深度学习：基于YOLOv5实例分割的甲状腺结节分割性能提升研究</h3>
<p><strong>原文标题：</strong> Doppler-Enhanced Deep Learning: Improving Thyroid Nodule Segmentation with YOLOv5 Instance Segmentation</p>
<p><strong>摘要：</strong>
全球甲状腺癌发病率的持续上升推动了各类计算机辅助检测方法的发展。在人工智能辅助临床决策支持系统的开发中，甲状腺结节的精确分割是关键的第一步。本研究聚焦于利用YOLOv5算法对超声图像进行甲状腺结节实例分割。我们在包含与不包含多普勒图像的两个数据集版本上评估了多种YOLOv5变体（Nano、Small、Medium、Large及XLarge）。实验结果表明，在包含多普勒图像的数据集上，YOLOv5-Large算法取得了最佳性能，其Dice相似系数达91%，平均精度（mAP）为0.87。值得注意的是，通常被医师排除在外的多普勒图像能显著提升分割性能：当排除多普勒图像时，YOLOv5-Small模型的Dice系数为79%，而引入多普勒图像后所有模型变体的性能均得到提升。这些发现表明，基于YOLOv5的实例分割为甲状腺结节检测提供了有效的实时解决方案，在自动化诊断系统中具有临床转化潜力。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.00639">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.00639">arXiv</a></p>
<hr />
<h3>46. OmniFusion：基于模块化融合的多语言多模态同步翻译模型</h3>
<p><strong>原文标题：</strong> OmniFusion: Simultaneous Multilingual Multimodal Translations via Modular Fusion</p>
<p><strong>摘要：</strong>
开源纯文本翻译大语言模型（LLMs）在语言覆盖范围与翻译质量方面已取得显著进展。然而，这些模型仅能通过级联流程应用于语音翻译（ST），即先进行自动语音识别再进行文本翻译。这种处理方式会引入额外延迟，在同步语音翻译（SimulST）场景中尤为关键，且无法利用多模态上下文（如图像）来辅助消歧。预训练多模态基础模型（MMFMs）已具备跨模态的强感知与推理能力，但通常缺乏专用翻译大语言模型的多语言覆盖能力与专业化翻译性能。为构建高效的多模态翻译系统，本文提出一种端到端方法，将多模态基础模型与翻译大语言模型进行融合。我们设计了一种新颖的融合策略，将预训练多模态基础模型多个隐藏层的状态连接至翻译大语言模型，从而实现联合端到端训练。基于Omni 2.5-7B作为多模态基础模型、SeedX PPO-7B作为翻译大语言模型构建的OmniFusion模型，能够执行语音到文本、语音与图像到文本、以及文本与图像到文本的翻译任务。实验表明，OmniFusion能有效利用音频与视觉输入，在同步语音翻译任务中相比级联流程降低1秒延迟，同时提升整体翻译质量。代码已开源：https://github.com/saikoneru/OmniFusion。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2512.00234">HuggingFace</a> | <a href="https://arxiv.org/abs/2512.00234">arXiv</a></p>
<hr />
<h3>47. 基于视觉语言模型的业务流程图表结构化信息提取</h3>
<p><strong>原文标题：</strong> Structured Extraction from Business Process Diagrams Using Vision-Language Models</p>
<p><strong>摘要：</strong>
业务流程模型与标注（BPMN）是广泛采用的复杂业务工作流表示标准。尽管BPMN图表通常以视觉图像形式交换，现有分析方法主要依赖XML表示进行计算处理。本研究提出一种处理流程，利用视觉语言模型（VLMs）直接从图像中提取BPMN图表的结构化JSON表示，无需源模型文件或文本标注。我们整合光学字符识别（OCR）技术进行文本增强，并通过源XML文件生成的真实数据评估所生成元素列表的准确性。该方法能够在原始源文件不可获取的场景下实现稳健的组件提取。我们对多种VLM模型进行基准测试，发现使用OCR进行文本增强时多个模型性能得到提升。此外，我们对基于OCR的增强方法进行了深入的统计分析及提示消融研究，从而更清晰地理解这些方法对模型性能的影响。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2511.22448">HuggingFace</a> | <a href="https://arxiv.org/abs/2511.22448">arXiv</a></p>
<hr />
<h2>🔍 关键词云图</h2>
<p><img alt="关键词云图" src="../images/keywords_wordcloud.png" /></p>
<h2>📈 近期论文趋势</h2>
<p><img alt="论文趋势" src="../images/daily_papers.png" /></p>
<h2>🎙️ 语音播报</h2>
<ul>
<li><a href="../audio/2025-12-02_daily_papers.mp3">收听今日论文解读</a></li>
</ul>
<h2>📱 订阅渠道</h2>
<ul>
<li>GitHub: <a href="https://github.com/2404589803/hf-daily-paper-newsletter-chinese">hf-daily-paper-newsletter-chinese</a></li>
</ul>
    </div>
</body>
</html>