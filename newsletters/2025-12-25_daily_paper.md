
# <img src="https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo.png" width="30"/> Hugging Face 2025-12-25 论文日报

## 📊 今日论文统计
- 总论文数：17
- 热门领域：Diffusion, Transformer, GPT, RL, LLM

## 📝 论文详情


### 1. TurboDiffusion：将视频扩散模型加速100-200倍

**原文标题：** TurboDiffusion: Accelerating Video Diffusion Models by 100-200 Times

**摘要：**
本文提出TurboDiffusion，一种视频生成加速框架，能够在保持视频质量的同时将端到端扩散生成过程加速100-200倍。TurboDiffusion主要依赖以下组件实现加速：（1）注意力加速：采用低比特SageAttention与可训练稀疏线性注意力（SLA）来加速注意力计算；（2）步数蒸馏：通过改进的rCM方法实现高效的步数蒸馏；（3）W8A8量化：将模型参数和激活值量化为8位，以加速线性层计算并压缩模型规模。此外，TurboDiffusion还融合了多项工程优化技术。我们在Wan2.2-I2V-14B-720P、Wan2.1-T2V-1.3B-480P、Wan2.1-T2V-14B-720P及Wan2.1-T2V-14B-480P模型上进行了实验验证。结果表明，即使在单张RTX 5090 GPU上，TurboDiffusion仍能实现100-200倍的视频生成加速，同时保持可比的视频生成质量。相关GitHub仓库已开源，包含模型检查点与易用代码，访问地址为：https://github.com/thu-ml/TurboDiffusion。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.16093) | [arXiv](https://arxiv.org/abs/2512.16093)



---

### 2. 面向视觉语言模型的四维推理学习：动态空间理解研究

**原文标题：** Learning to Reason in 4D: Dynamic Spatial Understanding for Vision Language Models

**摘要：**
视觉语言模型在通用理解任务中表现出色，但在动态空间推理方面仍存在明显不足——即对三维空间中物体几何属性与相互关系随时间演变过程的推理能力较弱，这主要源于可扩展的四维感知训练资源的匮乏。为从数据集、基准测试和模型三个维度系统性地弥合这一差距，我们提出了动态空间推理套件。首先，我们设计了一种自动化流程，能够从真实场景视频中生成针对动态空间推理的多选题问答对。该流程通过整合现代视觉基础模型，提取丰富的几何与运动信息，包括相机位姿、局部点云、物体掩码、空间朝向以及三维运动轨迹。这些几何线索既支撑了用于模型训练的DSR-Train数据集构建，也形成了经人工精修的评估基准DSR-Bench。与已有研究相比，我们的数据强调以下特性：（1）真实场景视频源；（2）物体与场景层级的三维空间约束；（3）视角变换；（4）多物体交互；（5）细粒度、过程化的答案设计。在数据资源之外，我们提出了一种轻量化的几何选择模块，该模块能够将几何先验知识无缝集成至视觉语言模型中。其核心机制是通过压缩问题语义，从预训练的四维重建先验中提取与问题相关的知识，并将其编码为紧凑的几何标记集合。这种定向提取策略有效避免了无关知识对模型造成的干扰。实验表明，将DSR-Train数据集与几何选择模块集成至Qwen2.5-VL-7B模型后，其动态空间推理能力得到显著提升，同时在通用视频理解基准测试中保持了原有的性能水平。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.20557) | [arXiv](https://arxiv.org/abs/2512.20557)



---

### 3. DreaMontage：基于任意帧引导的单镜头视频生成框架

**原文标题：** DreaMontage: Arbitrary Frame-Guided One-Shot Video Generation

**摘要：**
"单镜头"技术代表了电影制作中一种独特而精妙的美学风格。然而，其实践应用常受限于高昂成本与复杂的现实条件制约。尽管新兴的视频生成模型提供了虚拟化解决方案，但现有方法通常依赖于简单的片段拼接，往往难以保持视觉流畅性与时序连贯性。本文提出DreaMontage——一个专为任意帧引导生成设计的综合框架，能够基于用户提供的多样化输入，合成无缝、富有表现力且时长长的单镜头视频。为实现这一目标，我们通过三个核心维度解决技术挑战：（一）在DiT架构中集成轻量级中间条件调节机制。通过采用能有效利用基础训练数据的自适应调优策略，我们实现了鲁棒的任意帧控制能力。（二）为提升视觉保真度与电影表现力，我们构建了高质量数据集并实施视觉表达监督微调阶段。针对主体运动合理性与转场平滑性等关键问题，我们采用定制化的直接偏好优化方案，显著提升了生成内容的成功率与可用性。（三）为支持长序列生成，我们设计了分段自回归推理策略，以内存高效的方式实现扩展生成。大量实验表明，我们的方法在保持计算效率的同时，能够实现视觉惊艳且无缝连贯的单镜头效果，使用户能够将碎片化的视觉素材转化为生动、连贯的单镜头电影体验。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.21252) | [arXiv](https://arxiv.org/abs/2512.21252)



---

### 4. T2AV-Compass：迈向文本-音频-视频生成的统一评估

**原文标题：** T2AV-Compass: Towards Unified Evaluation for Text-to-Audio-Video Generation

**摘要：**
文本-音频-视频（T2AV）生成旨在从自然语言合成时序连贯的视频与语义同步的音频，然而其评估仍处于碎片化状态，常依赖单模态指标或范围狭窄的基准，难以捕捉复杂提示下的跨模态对齐、指令跟随及感知真实性。为应对此局限，本文提出T2AV-Compass——一个用于全面评估T2AV系统的统一基准。该基准包含500个多样化且复杂的提示，通过分类学驱动的流程构建，以确保语义丰富性与物理合理性。同时，T2AV-Compass引入双层评估框架，整合了针对视频质量、音频质量及跨模态对齐的客观信号级指标，以及用于指令跟随和真实性评估的主观“大语言模型即评判者”协议。对11个代表性T2AV系统的广泛评估表明，即使最强模型仍远未达到人类水平的真实性与跨模态一致性，在音频真实性、细粒度同步、指令跟随等方面存在持续缺陷。这些结果揭示了未来模型的显著改进空间，并凸显了T2AV-Compass作为推动文本-音频-视频生成发展的挑战性诊断测试平台的价值。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.21094) | [arXiv](https://arxiv.org/abs/2512.21094)



---

### 5. 超越记忆：揭示视觉语言模型中流行度偏差的多模态序数回归基准

**原文标题：** Beyond Memorization: A Multi-Modal Ordinal Regression Benchmark to Expose Popularity Bias in Vision-Language Models

**摘要：**
我们在前沿视觉语言模型中发现显著的流行度偏差，这些模型在著名建筑上的准确率比普通建筑高出34%，表明其依赖记忆而非可泛化的理解能力。为系统研究此问题，我们构建了该任务规模最大的开放基准：YearGuessr数据集，包含来自157个国家的55,546张建筑图像，每张图像均标注了连续序数标签（建造年份1001-2024年）、GPS数据，并以页面浏览量作为流行度代理指标。基于该数据集，我们将建造年份预测任务构建为序数回归问题，并提出融合流行度感知的区间准确率度量方法以量化此类偏差。通过对30余个模型（包括我们提出的YearCLIP模型）的基准测试证实，视觉语言模型擅长处理流行且被记忆的实体，而对非知名对象的识别能力显著不足，这揭示了其推理能力存在根本缺陷。项目页面：https://sytwu.github.io/BeyondMemo/

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.21337) | [arXiv](https://arxiv.org/abs/2512.21337)



---

### 6. Nemotron 3 Nano：面向智能体推理的开放高效混合专家Mamba-Transformer模型

**原文标题：** Nemotron 3 Nano: Open, Efficient Mixture-of-Experts Hybrid Mamba-Transformer Model for Agentic Reasoning

**摘要：**
本文介绍Nemotron 3 Nano 30B-A3B模型，这是一种采用混合专家架构的Mamba-Transformer混合语言模型。该模型在25万亿文本标记（其中包含超过3万亿相较于Nemotron 2新增的独特标记）上进行预训练，随后在多类环境中进行监督微调与大规模强化学习。Nemotron 3 Nano在每次前向传播中激活的参数量不足半数的情况下，仍实现了比前代Nemotron 2 Nano更优的准确率。与GPT-OSS-20B、Qwen3-30B-A3B-Thinking-2507等规模相近的开源模型相比，其推理吞吐量最高可提升3.3倍，同时在主流基准测试中表现更精准。Nemotron 3 Nano展现出增强的智能体交互、推理及对话能力，并支持高达100万标记的上下文长度。我们已在Hugging Face平台发布预训练版本Nemotron 3 Nano 30B-A3B Base与训练后版本Nemotron 3 Nano 30B-A3B的模型检查点。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.20848) | [arXiv](https://arxiv.org/abs/2512.20848)



---

### 7. HiStream：通过冗余消除流式处理实现高效高分辨率视频生成

**原文标题：** HiStream: Efficient High-Resolution Video Generation via Redundancy-Eliminated Streaming

**摘要：**
高分辨率视频生成对数字媒体和电影制作至关重要，但扩散模型的二次计算复杂度导致其成为计算瓶颈，使得实际推理难以实现。为解决这一问题，我们提出了HiStream——一种高效的自回归框架，该系统性地从三个维度消除冗余：i) 空间压缩：在低分辨率下进行去噪，再利用缓存特征进行高分辨率细化；ii) 时间压缩：采用固定尺寸锚点缓存的逐块处理策略，确保稳定的推理速度；iii) 时间步压缩：对后续基于缓存条件生成的视频块应用更少的去噪步骤。在1080p基准测试中，我们的核心HiStream模型（i+ii）在实现最先进视觉质量的同时，相比Wan2.1基线模型去噪速度提升最高达76.2倍，且质量损失可忽略不计。我们的加速变体HiStream+整合全部三项优化（i+ii+iii），相比基线实现107.5倍加速，在速度与质量间取得了卓越的平衡，从而使高分辨率视频生成兼具实用性与可扩展性。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.21338) | [arXiv](https://arxiv.org/abs/2512.21338)



---

### 8. NVIDIA Nemotron 3：高效开放的智能模型

**原文标题：** NVIDIA Nemotron 3: Efficient and Open Intelligence

**摘要：**
我们推出Nemotron 3系列模型——Nano、Super和Ultra。这些模型具备强大的智能体交互、推理和对话能力。该系列采用混合专家（Mixture-of-Experts）与Mamba-Transformer融合架构，实现了业界领先的吞吐量，并支持高达100万标记的上下文长度。Super和Ultra模型使用NVFP4精度进行训练，并引入创新性方法LatentMoE以提升模型质量。两款较大模型还搭载了MTP层以加速文本生成。所有Nemotron 3模型均通过多环境强化学习进行后训练，使其具备复杂推理、多步骤工具调用能力，并支持细粒度推理资源控制。最小模型Nano在保持极高推理成本效益的同时，其准确率优于同类模型；Super专为协作智能体和高负载场景（如IT工单自动化）优化；最大模型Ultra则提供了最先进的准确率与推理性能。Nano模型已与其技术报告及本白皮书同步发布，Super和Ultra模型将于未来数月内陆续推出。我们将公开模型权重、训练前后软件工具、训练方案以及所有具备再分发权的数据。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.20856) | [arXiv](https://arxiv.org/abs/2512.20856)



---

### 9. TokSuite：衡量分词器选择对语言模型行为的影响

**原文标题：** TokSuite: Measuring the Impact of Tokenizer Choice on Language Model Behavior

**摘要：**
分词器为语言模型（LMs）处理与表示文本提供了基础支撑。尽管分词过程至关重要，但由于难以单独衡量分词选择的影响，其在语言模型性能与行为中的作用尚未得到充分理解。为应对这一需求，我们提出了TokSuite——一个支持分词对语言模型影响研究的模型集合与基准测试框架。具体而言，我们训练了十四种采用不同分词器但其他条件完全一致的模型，这些模型在架构、数据集、训练预算和初始化方式上均保持一致。此外，我们构建并发布了一个新的基准测试集，专门用于衡量模型在可能影响分词效果的真实场景扰动下的性能表现。TokSuite通过系统化设计实现了模型分词器影响的稳健解耦分析，进而支持了一系列创新发现，揭示了多种主流分词器各自的优势与局限。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.20757) | [arXiv](https://arxiv.org/abs/2512.20757)



---

### 10. 基于下一帧预测的学习：自回归视频建模编码有效表征

**原文标题：** Learning from Next-Frame Prediction: Autoregressive Video Modeling Encodes Effective Representations

**摘要：**
通用基础模型预训练的最新进展显著提升了各类下游任务的性能。尽管如GPT等自回归生成模型已在自然语言处理领域引发革命，但大多数视觉生成式预训练方法仍依赖BERT风格的掩码建模，这种方法常忽略视频分析所必需的时间信息。现有的少数自回归视觉预训练方法存在语义定位不准、生成质量欠佳等问题，导致语义表征能力不足。本研究提出NExT-Vid——一种新颖的自回归视觉生成式预训练框架，通过掩码下一帧预测实现对图像与视频的联合建模。该框架引入上下文隔离的自回归预测器以解耦语义表征与目标解码过程，并采用条件流匹配解码器以提升生成质量与多样性。通过上下文隔离的流匹配预训练，本方法获得了强大的表征能力。基于大规模预训练模型的广泛实验表明，通过下游分类任务中的注意力探针评估，我们提出的方法在视觉表征学习方面持续优于以往的生成式预训练方法。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.21004) | [arXiv](https://arxiv.org/abs/2512.21004)



---

### 11. 从词语到世界：大型语言模型能否成为隐式的基于文本的世界模型？

**原文标题：** From Word to World: Can Large Language Models be Implicit Text-based World Models?

**摘要：**
智能体强化学习日益依赖经验驱动的规模化扩展，然而现实世界环境仍存在非适应性、覆盖范围有限和难以扩展的问题。世界模型通过模拟经验提供了提升学习效率的潜在路径，但大型语言模型能否可靠承担这一角色，以及在何种条件下能实质性地提升智能体性能，目前尚未明确。本研究在基于文本的环境中探讨这些问题——该环境为将语言建模重新诠释为交互情境下的状态预测提供了受控实验场。我们提出三层评估框架用于检验基于大型语言模型的世界模型：（一）保真度与一致性，（二）可扩展性与鲁棒性，（三）智能体效用。在五个典型环境中的实验表明，经过充分训练的世界模型能够保持连贯的潜在状态，其性能随数据量与模型规模呈现可预测的扩展，并能通过动作验证、合成轨迹生成以及强化学习预热启动等方式提升智能体表现。同时，这些增益效果严格依赖于行为覆盖度与环境复杂度，由此明确了世界模型有效支持智能体学习的边界条件。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.18832) | [arXiv](https://arxiv.org/abs/2512.18832)



---

### 12. DramaBench：面向剧本续写的六维评估框架

**原文标题：** DramaBench: A Six-Dimensional Evaluation Framework for Drama Script Continuation

**摘要：**
剧本续写任务要求模型保持角色一致性、推进情节连贯性并保留戏剧结构，而现有基准测试未能全面评估这些能力。本文提出DramaBench——首个面向剧本续写任务的大规模评估基准，涵盖六个独立维度：格式规范、叙事效率、角色一致性、情感深度、逻辑一致性与冲突处理。本框架结合基于规则的分析、大语言模型标注与统计度量，确保评估过程的客观性与可复现性。我们在1,103个剧本样本（总计8,824次评估）上对8个前沿语言模型进行综合评估，采用严格的统计显著性检验（252组配对比较，65.9%具统计显著性）及人工验证（188个剧本，其中3/5维度达到显著一致性）。消融实验证实六个维度均捕捉独立的文本质量特征（平均|r|=0.020）。DramaBench可为模型改进提供可操作的维度特异性反馈，并为创造性写作评估建立严谨标准。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.19012) | [arXiv](https://arxiv.org/abs/2512.19012)



---

### 13. SWE-EVO：在长周期软件演化场景中对编码智能体进行基准测试

**原文标题：** SWE-EVO: Benchmarking Coding Agents in Long-Horizon Software Evolution Scenarios

**摘要：**
现有针对AI编码智能体的基准测试主要集中于孤立、单一问题的任务，例如修复错误或实现小型功能。然而，现实世界的软件工程本质上是一项长周期任务：开发者必须理解高层次需求，规划跨多个文件的协调变更，并在保持现有功能的同时通过多次迭代演进代码库。我们提出了SWE-EVO，这是一个用于评估智能体在长周期软件演化挑战中表现的基准测试。该基准基于七个成熟开源Python项目的发布说明和版本历史构建，包含48个演化任务，要求智能体实现平均涉及21个文件的多步骤修改，并通过平均每个实例包含874个测试的全面测试套件进行验证。对前沿模型的实验揭示了一个显著的能力差距：即使是配备OpenHands的GPT-5，在SWE-EVO上的解决率也仅为21%，而在单一问题基准SWE-Bench Verified上则达到65%。这表明当前智能体在持续、多文件的推理方面存在困难。我们还提出了修复率这一细粒度指标，用于捕捉解决这些复杂长周期任务过程中的部分进展。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.18470) | [arXiv](https://arxiv.org/abs/2512.18470)



---

### 14. 流式视频指令调优

**原文标题：** Streaming Video Instruction Tuning

**摘要：**
本文提出Streamo——一个作为通用交互助手的实时流式视频大语言模型。与现有专注于问答或描述等单一功能的在线视频模型不同，Streamo能够执行广泛的流式视频任务，包括实时叙述、动作理解、事件描述、时序事件定位及时间敏感型问答。为实现这种多功能性，我们构建了Streamo-Instruct-465K——一个专为流式视频理解定制的大规模指令遵循数据集。该数据集涵盖多样化时序语境与多任务监督信号，支持跨异构流式任务的统一训练。通过端到端的精简训练流程，Streamo在指令数据集上训练后展现出强大的时序推理能力、即时响应特性以及在多种流式基准测试中的广泛泛化性能。大量实验表明，Streamo成功弥合了离线视频感知模型与实时多模态助手之间的鸿沟，为连续视频流中的统一智能视频理解迈出了关键一步。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.21334) | [arXiv](https://arxiv.org/abs/2512.21334)



---

### 15. 基于早期知识对齐的多跳推理方法

**原文标题：** Multi-hop Reasoning via Early Knowledge Alignment

**摘要：**
检索增强生成技术已成为大语言模型处理需要领域专业知识或最新信息的密集知识型查询的重要范式。为应对单步检索难以处理的复杂多跳问题，研究者提出了结合强化学习的迭代式检索增强生成方法。然而，现有迭代系统通常在规划问题分解时未能充分利用检索语料库的元信息，导致检索效率低下，推理链中的误差累积最终影响整体性能。本文提出早期知识对齐模块——一种简单而有效的解决方案，通过在迭代系统中引入上下文相关的检索知识，使大语言模型在规划阶段前与检索集合实现对齐。在六个标准检索增强生成数据集上的实验表明，该模块通过建立更坚实的推理基础，显著提升了检索精度，减少了误差传递，同时改善了模型性能与效率。从信息熵视角的分析证明，早期知识的引入能够减少推理过程中不必要的探索，使模型更聚焦于相关信息的子集。此外，该模块作为无需训练的通用推理策略，可无缝扩展至大规模模型。跨数据集与检索语料的泛化测试验证了方法的鲁棒性。总体而言，早期知识对齐模块推动了迭代式检索增强生成技术的发展，同时揭示了强化学习增强框架中结构化推理与高效探索的关键交互机制。代码已发布于 https://github.com/yxzwang/EarlyKnowledgeAlignment{Github}。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.20144) | [arXiv](https://arxiv.org/abs/2512.20144)



---

### 16. LLM瑞士轮：基于竞争性瑞士制动态的多基准性能聚合

**原文标题：** LLM Swiss Round: Aggregating Multi-Benchmark Performance via Competitive Swiss-System Dynamics

**摘要：**
大语言模型（LLM）的快速涌现与多样化专业基准的激增，要求评估体系从碎片化的任务特定指标转向能够有效聚合多维度能力的整体性竞争排名系统。当前评估方法主要依赖静态评分，存在根本性局限：既难以确定跨异质基准的合理混合比例，更无法捕捉模型在连续高压力任务环境中的动态竞争适应性及其脆弱性。为此，我们提出创新的竞争性瑞士制动态框架。该框架通过模拟多轮次序列化竞赛，使模型根据累积胜负记录在精心设计的基准序列中动态配对竞技。我们采用蒙特卡洛模拟进行十万次迭代，以计算统计稳健的期望胜率得分，从而消除随机配对与早期轮次运气带来的噪声干扰。此外，我们通过参数化每轮淘汰数量实施失败敏感性分析，从而根据模型的风险偏好进行画像区分——识别稳健的通才型模型与激进的专才型模型。实验表明，相较于传统聚合评分与静态配对模型，竞争性瑞士制动态框架能提供更精细且情境感知的排名结果，标志着向风险感知的下一代LLM评估迈出关键一步。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.21010) | [arXiv](https://arxiv.org/abs/2512.21010)



---

### 17. PhononBench：面向晶体生成动态稳定性评估的大规模声子基准测试框架

**原文标题：** PhononBench:A Large-Scale Phonon-Based Benchmark for Dynamical Stability in Crystal Generation

**摘要：**
本研究提出了PhononBench——首个面向人工智能生成晶体的动态稳定性评估大规模基准测试框架。基于近期开发的MatterSim原子间势函数（在超过10,000种材料的声子预测中达到密度泛函理论精度），该框架对六种主流晶体生成模型产生的108,843个晶体结构实现了高效的大规模声子计算与动态稳定性分析。PhononBench揭示了当前生成模型在保障动态稳定性方面存在的普遍局限：所有生成结构的平均动态稳定率仅为25.83%，其中表现最优的MatterGen模型也仅达到41.0%。进一步的案例研究表明，在面向特定物性的生成任务中（以MatterGen的带隙条件生成为例），即使在0.5 eV的最优带隙条件下，动态稳定率仍低至23.5%。在空间群受控的生成任务中，高对称性晶体表现出更好的稳定性（如立方晶系稳定率可达49.2%），但所有受控生成的平均稳定率仍仅为34.4%。本研究的重要附加成果是识别出28,119个在全布里渊区内声子稳定的晶体结构，为未来材料探索提供了可靠的候选材料库。通过建立首个大规模动态稳定性基准，本工作系统揭示了当前晶体生成模型的局限性，并为推动其朝着设计物理可行材料的方向发展提供了关键评估标准与指导。所有模型生成的晶体结构、声子计算结果及PhononBench开发的高通量评估工作流将在https://github.com/xqh19970407/PhononBench 公开释放。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.21227) | [arXiv](https://arxiv.org/abs/2512.21227)



---


## 🔍 关键词云图
![关键词云图](../images/keywords_wordcloud.png)

## 📈 近期论文趋势
![论文趋势](../images/daily_papers.png)

## 🎙️ 语音播报
- [收听今日论文解读](../audio/2025-12-25_daily_papers.mp3)

## 📱 订阅渠道
- GitHub: [hf-daily-paper-newsletter-chinese](https://github.com/2404589803/hf-daily-paper-newsletter-chinese)