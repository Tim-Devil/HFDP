
# <img src="https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo.png" width="30"/> Hugging Face 2025-12-25 论文日报

## 📊 今日论文统计
- 总论文数：17
- 热门领域：Diffusion, Transformer, GPT, RL, LLM

## 📝 论文详情


### 1. TurboDiffusion：将视频扩散模型加速100-200倍

**原文标题：** TurboDiffusion: Accelerating Video Diffusion Models by 100-200 Times

**摘要：**
本文提出TurboDiffusion，一种视频生成加速框架，能够在保持视频质量的同时，将端到端扩散生成过程加速100-200倍。TurboDiffusion主要依赖以下组件实现加速：（1）注意力加速：采用低比特SageAttention与可训练稀疏线性注意力（SLA）以加速注意力计算；（2）步数蒸馏：通过rCM方法实现高效的步数蒸馏；（3）W8A8量化：将模型参数与激活值量化为8比特，以加速线性层运算并压缩模型。此外，TurboDiffusion还融合了多项工程优化技术。我们在Wan2.2-I2V-14B-720P、Wan2.1-T2V-1.3B-480P、Wan2.1-T2V-14B-720P及Wan2.1-T2V-14B-480P模型上进行了实验。结果表明，即使在单张RTX 5090 GPU上，TurboDiffusion仍能实现100-200倍的视频生成加速，同时保持可比的视频生成质量。相关GitHub仓库已开源，包含模型检查点与易用代码，访问地址为：https://github.com/thu-ml/TurboDiffusion。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.16093) | [arXiv](https://arxiv.org/abs/2512.16093)



---

### 2. 面向视觉语言模型的四维动态空间推理学习

**原文标题：** Learning to Reason in 4D: Dynamic Spatial Understanding for Vision Language Models

**摘要：**
视觉语言模型在通用理解任务中表现出色，但在动态空间推理方面仍存在明显不足——即对三维空间中物体几何属性与相互关系随时间演变的推理能力较弱，这主要源于可扩展的四维感知训练资源的稀缺。为从数据集、基准测试和模型三个层面系统性地弥合这一差距，我们提出了动态空间推理套件。首先，我们设计了一套自动化流程，能够从真实场景视频中生成面向动态空间推理的多选题问答对。该流程通过整合现代视觉基础模型，提取丰富的几何与运动信息，包括相机位姿、局部点云、物体掩码、空间朝向以及三维运动轨迹。基于这些几何线索，我们构建了用于模型训练的DSR-Train数据集，并进一步通过人工精校形成了用于评估的DSR-Bench基准。与已有工作相比，我们的数据特别强调：（一）真实场景视频源；（二）物体与场景层级的三维信息需求；（三）视角变换；（四）多物体交互；（五）细粒度、过程化的答案形式。在数据构建之外，我们提出了一种轻量化的几何选择模块，该模块能够将几何先验知识无缝集成到视觉语言模型中。它通过压缩问题语义，从预训练的四维重建先验中提取与问题相关的知识，并将其编码为紧凑的几何标记集合。这种定向提取机制避免了无关知识对模型的干扰。实验表明，将DSR-Train数据集与几何选择模块集成至Qwen2.5-VL-7B模型中，能显著提升其动态空间推理能力，同时保持在通用视频理解基准测试上的准确度。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.20557) | [arXiv](https://arxiv.org/abs/2512.20557)



---

### 3. DreaMontage：基于任意帧引导的单镜头视频生成框架

**原文标题：** DreaMontage: Arbitrary Frame-Guided One-Shot Video Generation

**摘要：**
“单镜头”技术代表了电影制作中一种独特而精妙的美学风格。然而，其实践应用常受限于高昂成本与复杂的现实条件制约。尽管新兴的视频生成模型提供了虚拟化替代方案，但现有方法通常依赖于简单的片段拼接，往往难以保持视觉流畅性与时间连贯性。本文提出DreaMontage，一个专为任意帧引导生成设计的综合框架，能够基于用户提供的多样化输入，合成无缝、富有表现力且时长长的单镜头视频。为实现这一目标，我们从三个核心维度应对挑战：（一）我们在DiT架构中引入轻量级中间条件调节机制。通过采用能有效利用基础训练数据的自适应调优策略，实现了强大的任意帧控制能力。（二）为提升视觉保真度与电影表现力，我们构建了高质量数据集并实施视觉表达监督微调阶段。针对主体运动合理性与转场平滑性等关键问题，我们采用定制化的直接偏好优化方案，显著提高了生成内容的成功率与可用性。（三）为支持长序列生成，我们设计了分段自回归推理策略，以高效内存管理方式运行。大量实验表明，我们的方法在保持计算效率的同时，能实现视觉表现突出且连贯无缝的单镜头效果，使用户能够将碎片化的视觉素材转化为生动、连贯的单镜头电影体验。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.21252) | [arXiv](https://arxiv.org/abs/2512.21252)



---

### 4. T2AV-Compass：迈向文本-音频-视频生成的统一评估框架

**原文标题：** T2AV-Compass: Towards Unified Evaluation for Text-to-Audio-Video Generation

**摘要：**
文本-音频-视频（T2AV）生成旨在从自然语言合成时序连贯的视频与语义同步的音频，然而其评估体系仍处于碎片化状态，常依赖单模态指标或范围狭窄的基准测试，难以全面衡量复杂提示下的跨模态对齐、指令跟随及感知真实感。为应对这一局限，本文提出T2AV-Compass——一个面向T2AV系统综合评估的统一基准，包含通过分类学驱动流程构建的500个多样化复杂提示，确保语义丰富性与物理合理性。此外，T2AV-Compass设计了双层评估框架：一方面整合客观信号级指标以评估视频质量、音频质量及跨模态对齐；另一方面引入基于多模态大语言模型的“评委”协议进行主观评估，涵盖指令跟随与真实感判断。通过对11个代表性T2AV系统的广泛评测发现，即使当前最优模型仍与人类水平的真实感及跨模态一致性存在显著差距，在音频真实感、细粒度同步、指令跟随等方面存在持续缺陷。这些结果表明未来模型仍有巨大改进空间，同时凸显了T2AV-Compass作为推动文本-音频-视频生成技术发展的挑战性诊断测试平台的重要价值。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.21094) | [arXiv](https://arxiv.org/abs/2512.21094)



---

### 5. 超越记忆：揭示视觉语言模型中流行度偏差的多模态序数回归基准

**原文标题：** Beyond Memorization: A Multi-Modal Ordinal Regression Benchmark to Expose Popularity Bias in Vision-Language Models

**摘要：**
我们揭示了当前最先进的视觉语言模型存在显著的流行度偏差：与普通建筑相比，这些模型对著名建筑的识别准确率最高可提升34%，表明其依赖记忆而非可泛化的理解能力。为系统研究此问题，我们构建了该任务领域规模最大的开放基准数据集——YearGuessr。该数据集包含来自157个国家的55,546张建筑图像，配备多模态属性标注，包括连续序数形式的建造年份标签（1001-2024年）、GPS数据以及作为流行度代理指标的页面浏览量。基于此数据集，我们将建造年份预测任务构建为序数回归问题，并引入考虑流行度的区间准确率度量指标以量化此类偏差。我们对包括YearCLIP模型在内的30余个模型进行基准测试，结果证实视觉语言模型在流行度高、可记忆的项目上表现优异，但对未被识别的对象则显著困难，这暴露了其推理能力的关键缺陷。项目页面：https://sytwu.github.io/BeyondMemo/

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.21337) | [arXiv](https://arxiv.org/abs/2512.21337)



---

### 6. Nemotron 3 Nano：面向智能体推理的开放高效混合专家Mamba-Transformer模型

**原文标题：** Nemotron 3 Nano: Open, Efficient Mixture-of-Experts Hybrid Mamba-Transformer Model for Agentic Reasoning

**摘要：**
本文介绍Nemotron 3 Nano 30B-A3B模型，这是一种采用混合专家架构的Mamba-Transformer混合语言模型。该模型在25万亿文本标记上进行预训练，其中包含超过3万亿个相较于Nemotron 2新增的独特标记，随后在多类环境中进行了监督微调与大规模强化学习。Nemotron 3 Nano在每次前向传播中激活的参数数量不足半数的情况下，实现了比前代Nemotron 2 Nano更优的准确率。与GPT-OSS-20B、Qwen3-30B-A3B-Thinking-2507等规模相近的开源模型相比，其推理吞吐量最高可提升3.3倍，同时在主流基准测试中展现出更高的准确性。该模型表现出增强的智能体交互、推理及对话能力，并支持高达100万标记的上下文长度。我们已在Hugging Face平台发布预训练版本的Nemotron 3 Nano 30B-A3B基础模型及后训练版本的完整检查点。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.20848) | [arXiv](https://arxiv.org/abs/2512.20848)



---

### 7. HiStream：通过冗余消除流式处理实现高效高分辨率视频生成

**原文标题：** HiStream: Efficient High-Resolution Video Generation via Redundancy-Eliminated Streaming

**摘要：**
高分辨率视频生成对于数字媒体和电影制作至关重要，但扩散模型的二次计算复杂度造成了计算瓶颈，使得实际推理难以实现。为解决这一问题，我们提出了HiStream——一种高效的自回归框架，该系统性地从三个维度消除冗余：i) 空间压缩：在低分辨率下进行去噪，再利用缓存特征进行高分辨率细化；ii) 时间压缩：采用基于固定大小锚点缓存的逐块处理策略，确保稳定的推理速度；iii) 时间步压缩：对后续基于缓存条件的视频块应用更少的去噪步骤。在1080p基准测试中，我们的核心HiStream模型（i+ii）在实现最先进视觉质量的同时，相比Wan2.1基线模型去噪速度提升最高达76.2倍，且质量损失可忽略不计。我们的加速变体HiStream+融合三项优化（i+ii+iii），实现了相比基线107.5倍的加速，在速度与质量间取得了卓越的平衡，从而使高分辨率视频生成兼具实用性与可扩展性。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.21338) | [arXiv](https://arxiv.org/abs/2512.21338)



---

### 8. 英伟达Nemotron 3：高效开放的智能模型

**原文标题：** NVIDIA Nemotron 3: Efficient and Open Intelligence

**摘要：**
本文介绍Nemotron 3系列模型——包含Nano、Super与Ultra三个版本。该系列模型具备强大的智能体交互、推理及对话能力，采用混合专家（Mixture-of-Experts）与Mamba-Transformer融合架构，实现了业界领先的吞吐性能，并支持高达100万标记的上下文长度。其中Super与Ultra模型采用NVFP4训练框架，并引入创新的LatentMoE技术以提升模型质量。两款大型模型还搭载MTP层以加速文本生成。全系列模型均通过多环境强化学习进行后训练，具备复杂推理、多步骤工具调用能力，并支持细粒度推理资源控制。最小规模的Nano模型在保持极高推理成本效益的同时，其准确率超越同类竞品；Super模型专为协作智能体与高负载任务（如IT工单自动化）优化；Ultra模型作为最大规模版本，提供了顶尖的准确率与推理性能。Nano模型已随技术报告及本白皮书同步发布，Super与Ultra模型将于未来数月内陆续公开。我们将开源模型权重、训练与后训练软件、配置方案及所有具备再分发权的数据。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.20856) | [arXiv](https://arxiv.org/abs/2512.20856)



---

### 9. TokSuite：评估分词器选择对语言模型行为的影响

**原文标题：** TokSuite: Measuring the Impact of Tokenizer Choice on Language Model Behavior

**摘要：**
分词器为语言模型（LMs）的文本表示与处理提供了基础支撑。尽管分词过程至关重要，但由于难以独立衡量分词策略的具体影响，其在语言模型性能与行为中的作用尚未得到充分理解。为应对这一需求，我们提出了TokSuite——一个集成模型与基准测试的工具集，旨在支持分词机制对语言模型影响的系统性研究。具体而言，我们训练了十四组模型，这些模型在保持架构、数据集、训练预算与初始化条件完全一致的前提下，仅采用不同的分词器。此外，我们构建并发布了一套新型基准测试，专门用于衡量模型在可能影响分词结果的现实扰动场景下的性能表现。TokSuite通过上述设计实现了模型分词器影响的稳健解耦分析，进而支撑了一系列创新发现，系统阐明了多种主流分词器各自的优势与局限性。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.20757) | [arXiv](https://arxiv.org/abs/2512.20757)



---

### 10. 基于下一帧预测的学习：自回归视频建模编码有效表征

**原文标题：** Learning from Next-Frame Prediction: Autoregressive Video Modeling Encodes Effective Representations

**摘要：**
通用基础模型预训练的最新进展显著提升了各类下游任务的性能。虽然GPT等自回归生成模型已在自然语言处理领域引发革命性变革，但当前大多数视觉生成式预训练方法仍依赖于BERT风格的掩码建模范式，往往忽视了视频分析所必需的时间信息。现有的少数自回归视觉预训练方法存在语义定位不准确、生成质量欠佳等问题，导致语义表征能力不足。本研究提出NExT-Vid——一种创新的自回归视觉生成式预训练框架，通过掩码下一帧预测联合建模图像与视频数据。该框架引入上下文隔离的自回归预测器以解耦语义表征与目标解码过程，并采用条件流匹配解码器提升生成质量与多样性。通过上下文隔离流匹配预训练，本方法获得了强大的表征能力。基于大规模预训练模型的实验表明，通过下游分类任务的注意力探测评估，我们提出的方法在视觉表征学习方面持续优于现有生成式预训练方法。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.21004) | [arXiv](https://arxiv.org/abs/2512.21004)



---

### 11. 从词语到世界：大语言模型能否成为隐式的文本世界模型？

**原文标题：** From Word to World: Can Large Language Models be Implicit Text-based World Models?

**摘要：**
智能体强化学习日益依赖经验驱动的规模化扩展，然而现实世界环境仍存在非适应性、覆盖范围有限及难以规模化等问题。世界模型通过模拟经验为提升学习效率提供了潜在路径，但大语言模型能否可靠承担这一角色，以及在何种条件下能为智能体带来实质性效益，目前尚不明确。本研究在文本环境中探讨这些问题——该环境为将语言建模重新诠释为交互情境下的状态预测提供了受控实验场。我们提出三层评估框架以检验基于大语言模型的世界模型：（一）保真度与一致性，（二）可扩展性与鲁棒性，（三）智能体效用。通过在五个典型环境中的实验发现：经过充分训练的世界模型能够保持连贯的潜在状态，其性能随数据量与模型规模呈现可预测的扩展，并能通过动作验证、合成轨迹生成以及强化学习预热启动等方式提升智能体表现。同时，这些收益高度依赖于行为覆盖度与环境复杂度，由此明确了世界模型有效支持智能体学习的能力边界。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.18832) | [arXiv](https://arxiv.org/abs/2512.18832)



---

### 12. DramaBench：剧本续写的六维评估框架

**原文标题：** DramaBench: A Six-Dimensional Evaluation Framework for Drama Script Continuation

**摘要：**
剧本续写任务要求模型能够保持角色一致性、推进情节连贯性并维持戏剧结构，而现有基准测试未能全面评估这些能力。本文提出DramaBench，这是首个针对剧本续写任务的大规模评估基准，涵盖六个独立维度：格式规范、叙事效率、角色一致性、情感深度、逻辑一致性与冲突处理。本框架结合基于规则的分析、大语言模型标注与统计度量方法，确保评估过程的客观性与可复现性。我们在1,103个剧本样本（总计8,824次评估）上对8个前沿语言模型进行了全面评估，通过严格的统计显著性检验（252组配对比较，65.9%具显著性）和人工验证（188个剧本，在3/5维度上达成实质性一致）。消融实验证实六个维度均捕捉独立的文本质量特征（平均|r| = 0.020）。DramaBench为模型改进提供可操作的维度特异性反馈，并为创造性写作评估建立了严谨的标准。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.19012) | [arXiv](https://arxiv.org/abs/2512.19012)



---

### 13. SWE-EVO：在长周期软件演化场景中评估代码智能体的基准测试

**原文标题：** SWE-EVO: Benchmarking Coding Agents in Long-Horizon Software Evolution Scenarios

**摘要：**
现有的人工智能代码智能体基准测试主要关注孤立、单一问题的任务，例如修复错误或实现小型功能。然而，现实世界的软件工程本质上是一项长周期的工作：开发者必须解读高层级需求，规划跨多个文件的协调变更，并在保持现有功能的同时，通过多次迭代演进代码库。我们提出了SWE-EVO，这是一个评估智能体应对长周期软件演化挑战的基准测试。该基准基于七个成熟的Python开源项目的发布说明和版本历史构建，包含48个演化任务，要求智能体实施平均涉及21个文件的多步骤修改，并通过平均每个实例包含874个测试的全面测试套件进行验证。使用最先进模型进行的实验揭示了一个显著的能力差距：即使是配备OpenHands的GPT-5，在SWE-EVO上的解决率也仅为21%，而在单一问题基准SWE-Bench Verified上的解决率为65%。这表明当前智能体在持续、多文件的推理方面存在困难。我们还提出了修复率这一细粒度指标，用于捕捉解决这些复杂长周期任务过程中的部分进展。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.18470) | [arXiv](https://arxiv.org/abs/2512.18470)



---

### 14. 流式视频指令调优

**原文标题：** Streaming Video Instruction Tuning

**摘要：**
本文提出Streamo——一种作为通用交互助手的实时流式视频大语言模型。与现有专注于问答或描述等单一功能的在线视频模型不同，Streamo能够执行广泛的流式视频任务，包括实时叙述、动作理解、事件描述、时序事件定位及时间敏感型问答。为实现这种多功能性，我们构建了Streamo-Instruct-465K——一个专为流式视频理解定制的大规模指令遵循数据集。该数据集涵盖多样化时序语境与多任务监督信号，支持跨异构流式任务的统一训练。通过端到端的简化训练流程，模型在指令数据集上训练后展现出强大的时序推理能力、实时响应特性以及在多种流式基准测试中的广泛泛化性能。大量实验表明，Streamo弥合了离线视频感知模型与实时多模态助手之间的鸿沟，为连续视频流中的统一智能视频理解迈出了重要一步。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.21334) | [arXiv](https://arxiv.org/abs/2512.21334)



---

### 15. 基于早期知识对齐的多跳推理方法

**原文标题：** Multi-hop Reasoning via Early Knowledge Alignment

**摘要：**
检索增强生成已成为大型语言模型处理需要领域特定或最新信息的知识密集型查询的强大范式。为应对单步检索难以处理的复杂多跳问题，研究者提出了结合强化学习的迭代式检索增强生成方法。然而，现有迭代式检索增强生成系统通常在规划问题分解时未充分利用检索语料库的元信息，导致检索效率低下，且推理链中的误差会逐级累积，最终影响整体性能。本文提出早期知识对齐模块——一种简单而有效的创新设计，通过在迭代式检索增强生成系统中引入上下文相关的检索知识，使大型语言模型在规划阶段前与检索集合实现对齐。在六个标准检索增强生成数据集上的大量实验表明，通过建立更坚实的推理基础，早期知识对齐模块显著提升了检索精度，减少了误差累积，同时改善了系统性能与效率。从信息熵视角的分析证明，早期知识的引入能够减少推理过程中不必要的探索，使模型更聚焦于相关信息子集。此外，早期知识对齐模块被证实可作为无需训练的通用推理策略，并能无缝扩展至大型模型。跨多样化数据集与检索语料的泛化测试验证了该方法的鲁棒性。总体而言，早期知识对齐模块推动了迭代式检索增强生成技术的发展，同时揭示了强化学习增强框架中结构化推理与高效探索之间的关键互动机制。代码已发布于 https://github.com/yxzwang/EarlyKnowledgeAlignment{Github}。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.20144) | [arXiv](https://arxiv.org/abs/2512.20144)



---

### 16. LLM瑞士轮：通过竞争性瑞士制动态聚合多基准测试性能

**原文标题：** LLM Swiss Round: Aggregating Multi-Benchmark Performance via Competitive Swiss-System Dynamics

**摘要：**
大型语言模型（LLM）的快速涌现与多样化专业基准测试的激增，要求我们从碎片化的任务特定评估指标转向能够有效聚合多维度能力的整体性竞争排名系统。当前评估方法主要依赖静态评分，存在根本性局限：既难以确定跨异质基准的合理混合比例，更无法捕捉模型在面临连续高风险任务时的动态竞争适应性及其脆弱性。为此，我们提出创新的竞争性瑞士制动态框架。该框架模拟多轮次序列化竞赛，模型将根据其累积胜负记录，在精心设计的基准测试序列中进行动态配对对抗。我们采用蒙特卡洛模拟（N=100,000次迭代）来逼近统计稳健的期望胜率得分，从而消除随机配对与早期轮次运气带来的噪声干扰。进一步，我们通过参数化每轮淘汰数量实施失败敏感性分析，从而依据模型的风险偏好进行画像区分——甄别稳健的通才型模型与激进的专才型模型。实验表明，相较于传统聚合评分与静态配对模型，竞争性瑞士制动态框架能提供更精细且情境感知的排名结果，标志着向风险感知的下一代LLM评估迈出关键一步。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.21010) | [arXiv](https://arxiv.org/abs/2512.21010)



---

### 17. PhononBench：面向晶体生成动态稳定性评估的大规模声子基准数据集

**原文标题：** PhononBench:A Large-Scale Phonon-Based Benchmark for Dynamical Stability in Crystal Generation

**摘要：**
本研究提出了PhononBench——首个面向人工智能生成晶体的动态稳定性评估大规模基准数据集。基于近期开发的MatterSim原子间势函数（在超过10,000种材料的声子预测中达到密度泛函理论精度），该数据集对六种主流晶体生成模型产生的108,843个晶体结构实现了高效的大规模声子计算与动态稳定性分析。PhononBench揭示了当前生成模型在确保动态稳定性方面存在的普遍局限：所有生成结构的平均动态稳定率仅为25.83%，其中表现最优的MatterGen模型也仅达到41.0%。进一步案例研究表明，在面向特定物性的生成任务中（以MatterGen的带隙条件生成为例），即使在0.5 eV的最优带隙条件下，动态稳定率仍低至23.5%。在空间群受控的生成任务中，高对称性晶体表现出更好的稳定性（如立方晶系稳定率可达49.2%），但所有受控生成的平均稳定率仍仅为34.4%。本研究的重要附加成果是发现了28,119个在全布里渊区内声子稳定的晶体结构，为未来材料探索提供了大量可靠候选材料。通过建立首个大规模动态稳定性基准，本工作系统揭示了当前晶体生成模型的局限性，并为推动其朝着设计物理可实存材料的方向发展提供了关键评估标准与指导。所有模型生成的晶体结构、声子计算结果及PhononBench开发的高通量评估工作流程均将通过https://github.com/xqh19970407/PhononBench公开。

**论文链接：** [HuggingFace](https://huggingface.co/papers/2512.21227) | [arXiv](https://arxiv.org/abs/2512.21227)



---


## 🔍 关键词云图
![关键词云图](../images/keywords_wordcloud.png)

## 📈 近期论文趋势
![论文趋势](../images/daily_papers.png)

## 🎙️ 语音播报
- [收听今日论文解读](../audio/2025-12-25_daily_papers.mp3)

## 📱 订阅渠道
- GitHub: [hf-daily-paper-newsletter-chinese](https://github.com/2404589803/hf-daily-paper-newsletter-chinese)