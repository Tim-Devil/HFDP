[
  {
    "title": "Step-GUI Technical Report",
    "summary": "Recent advances in multimodal large language models unlock unprecedented opportunities for GUI automation. However, a fundamental challenge remains: how to efficiently acquire high-quality training data while maintaining annotation reliability? We introduce a self-evolving training pipeline powered by the Calibrated Step Reward System, which converts model-generated trajectories into reliable training signals through trajectory-level calibration, achieving >90% annotation accuracy with 10-100x lower cost. Leveraging this pipeline, we introduce Step-GUI, a family of models (4B/8B) that achieves state-of-the-art GUI performance (8B: 80.2% AndroidWorld, 48.5% OSWorld, 62.6% ScreenShot-Pro) while maintaining robust general capabilities. As GUI agent capabilities improve, practical deployment demands standardized interfaces across heterogeneous devices while protecting user privacy. To this end, we propose GUI-MCP, the first Model Context Protocol for GUI automation with hierarchical architecture that combines low-level atomic operations and high-level task delegation to local specialist models, enabling high-privacy execution where sensitive data stays on-device. Finally, to assess whether agents can handle authentic everyday usage, we introduce AndroidDaily, a benchmark grounded in real-world mobile usage patterns with 3146 static actions and 235 end-to-end tasks across high-frequency daily scenarios (8B: static 89.91%, end-to-end 52.50%). Our work advances the development of practical GUI agents and demonstrates strong potential for real-world deployment in everyday digital interactions.",
    "translation": "标题：Step-GUI技术报告\n\n摘要：多模态大语言模型的最新进展为图形用户界面自动化带来了前所未有的机遇。然而，一个根本性挑战依然存在：如何在保持标注可靠性的同时高效获取高质量训练数据？我们提出了一种由校准步骤奖励系统驱动的自进化训练流程，该系统通过轨迹级校准将模型生成的轨迹转化为可靠的训练信号，以降低10-100倍的成本实现了超过90%的标注准确率。基于此流程，我们推出了Step-GUI系列模型（4B/8B），在保持强大通用能力的同时实现了最先进的GUI性能（8B模型：AndroidWorld 80.2%，OSWorld 48.5%，ScreenShot-Pro 62.6%）。随着GUI智能体能力提升，实际部署需要跨异构设备的标准化接口，同时保护用户隐私。为此，我们提出了GUI-MCP——首个面向GUI自动化的分层架构模型上下文协议，该协议结合底层原子操作与向本地专业模型的高层任务委派机制，实现了敏感数据全程驻留设备的高隐私执行方案。最后，为评估智能体处理真实日常使用场景的能力，我们构建了AndroidDaily基准测试，该基准基于真实移动端使用模式，涵盖高频日常场景中的3146项静态操作与235项端到端任务（8B模型：静态任务89.91%，端到端任务52.50%）。本研究推动了实用化GUI智能体的发展，并展现出在日常数字交互中实际部署的强大潜力。",
    "url": "https://huggingface.co/papers/2512.15431",
    "arxiv_url": "https://arxiv.org/abs/2512.15431"
  },
  {
    "title": "DEER: Draft with Diffusion, Verify with Autoregressive Models",
    "summary": "Efficiency, as a critical practical challenge for LLM-driven agentic and reasoning systems, is increasingly constrained by the inherent latency of autoregressive (AR) decoding. Speculative decoding mitigates this cost through a draft-verify scheme, yet existing approaches rely on AR draft models (a.k.a., drafters), which introduce two fundamental issues: (1) step-wise uncertainty accumulation leads to a progressive collapse of trust between the target model and the drafter, and (2) inherently sequential decoding of AR drafters. Together, these factors cause limited speedups. In this paper, we show that a diffusion large language model (dLLM) drafters can naturally overcome these issues through its fundamentally different probabilistic modeling and efficient parallel decoding strategy. Building on this insight, we introduce DEER, an efficient speculative decoding framework that drafts with diffusion and verifies with AR models. To enable high-quality drafting, DEER employs a two-stage training pipeline to align the dLLM-based drafters with the target AR model, and further adopts single-step decoding to generate long draft segments. Experiments show DEER reaches draft acceptance lengths of up to 32 tokens, far surpassing the 10 tokens achieved by EAGLE-3. Moreover, on HumanEval with Qwen3-30B-A3B, DEER attains a 5.54x speedup, while EAGLE-3 achieves only 2.41x. Code, model, demo, etc, will be available at https://czc726.github.io/DEER/",
    "translation": "标题：DEER：基于扩散模型的草稿生成与自回归模型的验证\n\n摘要：效率作为大语言模型驱动的智能体与推理系统面临的关键实践挑战，正日益受到自回归解码固有延迟的限制。推测式解码通过草稿-验证机制缓解这一成本，但现有方法依赖自回归草稿模型（即草案器），这带来两个根本性问题：（1）逐步累积的不确定性导致目标模型与草案器之间的信任度持续衰减；（2）自回归草案器固有的串行解码特性。这些因素共同导致加速效果有限。本文提出扩散大语言模型草案器能够通过其根本不同的概率建模机制与高效并行解码策略，自然克服上述问题。基于此洞见，我们提出DEER——一种高效的推测式解码框架，采用扩散模型生成草稿，并通过自回归模型进行验证。为实现高质量草稿生成，DEER采用两阶段训练流程使基于扩散大语言模型的草案器与目标自回归模型对齐，并进一步采用单步解码策略生成长片段草稿。实验表明，DEER的草稿接受长度最高可达32个词元，远超EAGLE-3的10个词元。在Qwen3-30B-A3B模型上进行的HumanEval测试中，DEER实现5.54倍加速，而EAGLE-3仅实现2.41倍加速。代码、模型及演示等资源将在https://czc726.github.io/DEER/发布。",
    "url": "https://huggingface.co/papers/2512.15176",
    "arxiv_url": "https://arxiv.org/abs/2512.15176"
  },
  {
    "title": "Fast and Accurate Causal Parallel Decoding using Jacobi Forcing",
    "summary": "Multi-token generation has emerged as a promising paradigm for accelerating transformer-based large model inference. Recent efforts primarily explore diffusion Large Language Models (dLLMs) for parallel decoding to reduce inference latency. To achieve AR-level generation quality, many techniques adapt AR models into dLLMs to enable parallel decoding. However, they suffer from limited speedup compared to AR models due to a pretrain-to-posttrain mismatch. Specifically, the masked data distribution in post-training deviates significantly from the real-world data distribution seen during pretraining, and dLLMs rely on bidirectional attention, which conflicts with the causal prior learned during pretraining and hinders the integration of exact KV cache reuse. To address this, we introduce Jacobi Forcing, a progressive distillation paradigm where models are trained on their own generated parallel decoding trajectories, smoothly shifting AR models into efficient parallel decoders while preserving their pretrained causal inference property. The models trained under this paradigm, Jacobi Forcing Model, achieves 3.8x wall-clock speedup on coding and math benchmarks with minimal loss in performance. Based on Jacobi Forcing Models' trajectory characteristics, we introduce multi-block decoding with rejection recycling, which enables up to 4.5x higher token acceptance count per iteration and nearly 4.0x wall-clock speedup, effectively trading additional compute for lower inference latency. Our code is available at https://github.com/hao-ai-lab/JacobiForcing.",
    "translation": "标题：基于雅可比强迫的快速准确因果并行解码方法\n\n摘要：多令牌生成已成为加速基于Transformer的大模型推理的前沿范式。近期研究主要探索扩散大语言模型（dLLMs）的并行解码能力以降低推理延迟。为达到自回归模型的生成质量，现有技术通常将自回归模型适配为扩散大语言模型以实现并行解码。然而，由于预训练与后训练之间的分布失配问题，这些方法相较于自回归模型的加速效果有限。具体而言，后训练中的掩码数据分布与预训练阶段接触的真实数据分布存在显著偏差，且扩散大语言模型依赖的双向注意力机制与预训练习得的因果先验相冲突，阻碍了精确键值缓存重用的实现。为解决这一问题，我们提出雅可比强迫——一种渐进式蒸馏范式，该范式通过在模型自身生成的并行解码轨迹上进行训练，使自回归模型平滑过渡为高效的并行解码器，同时保持其预训练的因果推理特性。基于此范式训练的雅可比强迫模型在代码与数学基准测试中实现了3.8倍的实际加速，且性能损失极小。依托雅可比强迫模型的轨迹特性，我们进一步提出带拒绝回收机制的多块解码策略，使单次迭代的令牌接受数量提升至4.5倍，实际加速比接近4.0倍，实现了计算资源与推理延迟的有效权衡。代码已开源：https://github.com/hao-ai-lab/JacobiForcing。",
    "url": "https://huggingface.co/papers/2512.14681",
    "arxiv_url": "https://arxiv.org/abs/2512.14681"
  },
  {
    "title": "HyperVL: An Efficient and Dynamic Multimodal Large Language Model for Edge Devices",
    "summary": "Current multimodal large lanauge models possess strong perceptual and reasoning capabilities, however high computational and memory requirements make them difficult to deploy directly on on-device environments. While small-parameter models are progressively endowed with strong general capabilities, standard Vision Transformer (ViT) encoders remain a critical bottleneck, suffering from excessive latency and memory consumption when processing high-resolution inputs.To address these challenges, we introduce HyperVL, an efficient multimodal large language model tailored for on-device inference. HyperVL adopts an image-tiling strategy to cap peak memory usage and incorporates two novel techniques: (1) a Visual Resolution Compressor (VRC) that adaptively predicts optimal encoding resolutions to eliminate redundant computation, and (2) Dual Consistency Learning (DCL), which aligns multi-scale ViT encoders within a unified framework, enabling dynamic switching between visual branches under a shared LLM. Extensive experiments demonstrate that HyperVL achieves state-of-the-art performance among models of comparable size across multiple benchmarks. Furthermore, it significantly significantly reduces latency and power consumption on real mobile devices, demonstrating its practicality for on-device multimodal inference.",
    "translation": "标题：HyperVL：面向边缘设备的高效动态多模态大语言模型\n\n摘要：当前多模态大语言模型虽具备强大的感知与推理能力，但其高昂的计算与内存需求使其难以直接部署在设备端环境中。尽管小参数量模型正逐步被赋予强大的通用能力，标准的视觉Transformer编码器在处理高分辨率输入时仍存在延迟过高和内存消耗过大的瓶颈问题。为应对这些挑战，本文提出HyperVL——一种专为设备端推理设计的高效多模态大语言模型。该模型采用图像分块策略以限制峰值内存使用，并引入两项创新技术：（1）视觉分辨率压缩器，可自适应预测最优编码分辨率以消除冗余计算；（2）双重一致性学习，通过在统一框架中对齐多尺度视觉Transformer编码器，实现在共享大语言模型下视觉分支的动态切换。大量实验表明，HyperVL在多个基准测试中取得了同规模模型的领先性能。此外，其在真实移动设备上显著降低了延迟与功耗，证明了该模型在设备端多模态推理场景中的实用价值。",
    "url": "https://huggingface.co/papers/2512.14052",
    "arxiv_url": "https://arxiv.org/abs/2512.14052"
  },
  {
    "title": "Puzzle Curriculum GRPO for Vision-Centric Reasoning",
    "summary": "Recent reinforcement learning (RL) approaches like outcome-supervised GRPO have advanced chain-of-thought reasoning in Vision Language Models (VLMs), yet key issues linger: (i) reliance on costly and noisy hand-curated annotations or external verifiers; (ii) flat and sparse reward schemes in GRPO; and (iii) logical inconsistency between a chain's reasoning and its final answer. We present Puzzle Curriculum GRPO (PC-GRPO), a supervision-free recipe for RL with Verifiable Rewards (RLVR) that strengthens visual reasoning in VLMs without annotations or external verifiers. PC-GRPO replaces labels with three self-supervised puzzle environments: PatchFit, Rotation (with binary rewards) and Jigsaw (with graded partial credit mitigating reward sparsity). To counter flat rewards and vanishing group-relative advantages, we introduce a difficulty-aware curriculum that dynamically weights samples and peaks at medium difficulty. We further monitor Reasoning-Answer Consistency (RAC) during post-training: mirroring reports for vanilla GRPO in LLMs, RAC typically rises early then degrades; our curriculum delays this decline, and consistency-enforcing reward schemes further boost RAC. RAC correlates with downstream accuracy. Across diverse benchmarks and on Qwen-7B and Qwen-3B backbones, PC-GRPO improves reasoning quality, training stability, and end-task accuracy, offering a practical path to scalable, verifiable, and interpretable RL post-training for VLMs.",
    "translation": "标题：面向视觉中心推理的谜题课程GRPO方法\n\n摘要：近期如结果监督GRPO等强化学习方法在视觉语言模型的思维链推理方面取得进展，但关键问题依然存在：(i)依赖昂贵且存在噪声的人工标注或外部验证器；(ii)GRPO中奖励机制呈现扁平化与稀疏性特征；(iii)推理链条与最终答案间存在逻辑不一致性。本文提出谜题课程GRPO方法，这是一种基于可验证奖励的强化学习无监督方案，可在无需标注或外部验证器的前提下增强视觉语言模型的视觉推理能力。PC-GRPO通过三个自监督谜题环境替代人工标注：PatchFit、旋转任务（采用二元奖励）和拼图任务（通过分级部分奖励缓解奖励稀疏问题）。为应对扁平化奖励与群体相对优势衰减问题，我们设计了难度感知课程机制，动态调整样本权重并使训练峰值集中于中等难度区间。在训练后阶段，我们持续监控推理-答案一致性指标：与大型语言模型中基础GRPO的观测结果相似，RAC通常呈现先升后降趋势；我们的课程设计延缓了这种衰减，而一致性强化奖励方案进一步提升了RAC指标。实验表明RAC与下游任务准确率具有相关性。在多样化基准测试中，基于Qwen-7B和Qwen-3B架构的PC-GRPO显著提升了推理质量、训练稳定性及终端任务准确率，为视觉语言模型的可扩展、可验证、可解释强化学习后训练提供了实用路径。",
    "url": "https://huggingface.co/papers/2512.14944",
    "arxiv_url": "https://arxiv.org/abs/2512.14944"
  },
  {
    "title": "Universal Reasoning Model",
    "summary": "Universal transformers (UTs) have been widely used for complex reasoning tasks such as ARC-AGI and Sudoku, yet the specific sources of their performance gains remain underexplored. In this work, we systematically analyze UTs variants and show that improvements on ARC-AGI primarily arise from the recurrent inductive bias and strong nonlinear components of Transformer, rather than from elaborate architectural designs. Motivated by this finding, we propose the Universal Reasoning Model (URM), which enhances the UT with short convolution and truncated backpropagation. Our approach substantially improves reasoning performance, achieving state-of-the-art 53.8% pass@1 on ARC-AGI 1 and 16.0% pass@1 on ARC-AGI 2. Our code is avaliable at https://github.com/zitian-gao/URM.",
    "translation": "标题：通用推理模型\n\n摘要：通用Transformer（UT）已在ARC-AGI和数独等复杂推理任务中得到广泛应用，但其性能提升的具体来源仍未得到充分探究。本研究系统分析了通用Transformer的变体，发现其在ARC-AGI上的改进主要源于Transformer的循环归纳偏置和强非线性组件，而非复杂的架构设计。基于此发现，我们提出通用推理模型（URM），通过引入短卷积和截断反向传播机制增强通用Transformer。该方法显著提升了推理性能，在ARC-AGI 1上达到53.8% pass@1，在ARC-AGI 2上达到16.0% pass@1的当前最优结果。代码已开源：https://github.com/zitian-gao/URM。",
    "url": "https://huggingface.co/papers/2512.14693",
    "arxiv_url": "https://arxiv.org/abs/2512.14693"
  },
  {
    "title": "Qwen-Image-Layered: Towards Inherent Editability via Layer Decomposition",
    "summary": "Recent visual generative models often struggle with consistency during image editing due to the entangled nature of raster images, where all visual content is fused into a single canvas. In contrast, professional design tools employ layered representations, allowing isolated edits while preserving consistency. Motivated by this, we propose Qwen-Image-Layered, an end-to-end diffusion model that decomposes a single RGB image into multiple semantically disentangled RGBA layers, enabling inherent editability, where each RGBA layer can be independently manipulated without affecting other content. To support variable-length decomposition, we introduce three key components: (1) an RGBA-VAE to unify the latent representations of RGB and RGBA images; (2) a VLD-MMDiT (Variable Layers Decomposition MMDiT) architecture capable of decomposing a variable number of image layers; and (3) a Multi-stage Training strategy to adapt a pretrained image generation model into a multilayer image decomposer. Furthermore, to address the scarcity of high-quality multilayer training images, we build a pipeline to extract and annotate multilayer images from Photoshop documents (PSD). Experiments demonstrate that our method significantly surpasses existing approaches in decomposition quality and establishes a new paradigm for consistent image editing. Our code and models are released on https://github.com/QwenLM/Qwen-Image-Layered{https://github.com/QwenLM/Qwen-Image-Layered}",
    "translation": "标题：Qwen-Image-Layered：通过图层分解实现内在可编辑性\n\n摘要：当前的可视化生成模型在图像编辑过程中常因栅格图像的纠缠特性而难以保持一致性，即所有视觉内容均被融合至单一画布中。相比之下，专业设计工具采用分层表示法，允许在保持一致性的同时进行独立编辑。受此启发，我们提出了Qwen-Image-Layered——一种端到端的扩散模型，能够将单张RGB图像分解为多个语义解耦的RGBA图层，从而实现内在可编辑性，其中每个RGBA图层均可独立操作而不影响其他内容。为支持可变长度的分解，我们引入了三个关键组件：（1）RGBA-VAE，用于统一RGB与RGBA图像的潜在表示；（2）VLD-MMDiT（可变图层分解MMDiT）架构，能够分解可变数量的图像图层；（3）多阶段训练策略，将预训练的图像生成模型适配为多层图像分解器。此外，针对高质量多层训练图像稀缺的问题，我们构建了一套从Photoshop文档（PSD）中提取并标注多层图像的流程。实验表明，我们的方法在分解质量上显著超越现有方法，并为一致性图像编辑建立了新范式。相关代码与模型已发布于https://github.com/QwenLM/Qwen-Image-Layered。",
    "url": "https://huggingface.co/papers/2512.15603",
    "arxiv_url": "https://arxiv.org/abs/2512.15603"
  },
  {
    "title": "IC-Effect: Precise and Efficient Video Effects Editing via In-Context Learning",
    "summary": "We propose IC-Effect, an instruction-guided, DiT-based framework for few-shot video VFX editing that synthesizes complex effects (\\eg flames, particles and cartoon characters) while strictly preserving spatial and temporal consistency. Video VFX editing is highly challenging because injected effects must blend seamlessly with the background, the background must remain entirely unchanged, and effect patterns must be learned efficiently from limited paired data. However, existing video editing models fail to satisfy these requirements. IC-Effect leverages the source video as clean contextual conditions, exploiting the contextual learning capability of DiT models to achieve precise background preservation and natural effect injection. A two-stage training strategy, consisting of general editing adaptation followed by effect-specific learning via Effect-LoRA, ensures strong instruction following and robust effect modeling. To further improve efficiency, we introduce spatiotemporal sparse tokenization, enabling high fidelity with substantially reduced computation. We also release a paired VFX editing dataset spanning 15 high-quality visual styles. Extensive experiments show that IC-Effect delivers high-quality, controllable, and temporally consistent VFX editing, opening new possibilities for video creation.",
    "translation": "标题：IC-Effect：基于上下文学习的精准高效视频特效编辑方法\n\n摘要：本文提出IC-Effect，一种基于指令引导与扩散Transformer（DiT）的少样本视频视觉特效编辑框架。该框架能够合成复杂特效（如火焰、粒子与卡通角色），同时严格保持空间与时间一致性。视频特效编辑面临三重挑战：注入的特效需与背景无缝融合、背景必须完全保持不变，且需从有限的配对数据中高效学习特效模式。然而，现有视频编辑模型难以同时满足这些要求。IC-Effect将源视频作为干净的上下文条件，利用DiT模型的上下文学习能力，实现精准的背景保持与自然的特效注入。通过两阶段训练策略——先进行通用编辑适配，再通过特效低秩自适应模块（Effect-LoRA）进行特效专项学习——本框架确保了强大的指令跟随能力与鲁棒的特效建模。为进一步提升效率，我们引入时空稀疏令牌化技术，在显著降低计算量的同时保持高保真度。此外，我们发布了涵盖15种高质量视觉风格的配对特效编辑数据集。大量实验表明，IC-Effect能够实现高质量、可控且时序一致的视觉特效编辑，为视频创作开辟了新的可能性。",
    "url": "https://huggingface.co/papers/2512.15635",
    "arxiv_url": "https://arxiv.org/abs/2512.15635"
  },
  {
    "title": "Skyra: AI-Generated Video Detection via Grounded Artifact Reasoning",
    "summary": "The misuse of AI-driven video generation technologies has raised serious social concerns, highlighting the urgent need for reliable AI-generated video detectors. However, most existing methods are limited to binary classification and lack the necessary explanations for human interpretation. In this paper, we present Skyra, a specialized multimodal large language model (MLLM) that identifies human-perceivable visual artifacts in AI-generated videos and leverages them as grounded evidence for both detection and explanation. To support this objective, we construct ViF-CoT-4K for Supervised Fine-Tuning (SFT), which represents the first large-scale AI-generated video artifact dataset with fine-grained human annotations. We then develop a two-stage training strategy that systematically enhances our model's spatio-temporal artifact perception, explanation capability, and detection accuracy. To comprehensively evaluate Skyra, we introduce ViF-Bench, a benchmark comprising 3K high-quality samples generated by over ten state-of-the-art video generators. Extensive experiments demonstrate that Skyra surpasses existing methods across multiple benchmarks, while our evaluation yields valuable insights for advancing explainable AI-generated video detection.",
    "translation": "标题：Skyra：基于具象化伪影推理的AI生成视频检测方法\n\n摘要：AI驱动的视频生成技术滥用已引发严重的社会关切，凸显了对可靠AI生成视频检测器的迫切需求。然而，现有方法大多局限于二元分类，且缺乏可供人类理解的必要解释。本文提出Skyra——一种专门的多模态大语言模型（MLLM），该模型能够识别AI生成视频中人类可感知的视觉伪影，并将其作为检测与解释的具象化证据。为实现这一目标，我们构建了首个包含细粒度人工标注的大规模AI生成视频伪影数据集ViF-CoT-4K，用于监督微调训练。进而开发了一种两阶段训练策略，系统性地提升模型在时空伪影感知、解释能力与检测精度方面的性能。为全面评估Skyra，我们构建了ViF-Bench基准测试集，该数据集包含由十余种前沿视频生成模型产生的3000个高质量样本。大量实验表明，Skyra在多项基准测试中均超越现有方法，同时我们的评估结果为推进可解释AI生成视频检测研究提供了重要启示。",
    "url": "https://huggingface.co/papers/2512.15693",
    "arxiv_url": "https://arxiv.org/abs/2512.15693"
  },
  {
    "title": "Robust and Calibrated Detection of Authentic Multimedia Content",
    "summary": "Generative models can synthesize highly realistic content, so-called deepfakes, that are already being misused at scale to undermine digital media authenticity. Current deepfake detection methods are unreliable for two reasons: (i) distinguishing inauthentic content post-hoc is often impossible (e.g., with memorized samples), leading to an unbounded false positive rate (FPR); and (ii) detection lacks robustness, as adversaries can adapt to known detectors with near-perfect accuracy using minimal computational resources. To address these limitations, we propose a resynthesis framework to determine if a sample is authentic or if its authenticity can be plausibly denied. We make two key contributions focusing on the high-precision, low-recall setting against efficient (i.e., compute-restricted) adversaries. First, we demonstrate that our calibrated resynthesis method is the most reliable approach for verifying authentic samples while maintaining controllable, low FPRs. Second, we show that our method achieves adversarial robustness against efficient adversaries, whereas prior methods are easily evaded under identical compute budgets. Our approach supports multiple modalities and leverages state-of-the-art inversion techniques.",
    "translation": "标题：鲁棒且可校准的真实多媒体内容检测\n\n摘要：生成模型能够合成高度逼真的内容（即深度伪造内容），此类技术已被大规模滥用，严重威胁数字媒体的真实性。当前深度伪造检测方法存在两大不可靠因素：（一）事后鉴别非真实内容往往不可行（例如面对记忆样本），导致误报率理论上无界；（二）检测缺乏鲁棒性，攻击者仅需极少计算资源即可针对已知检测器实现近乎完美的规避。为应对这些局限，我们提出一种重合成框架，用于判定样本是否真实或其真实性是否可被合理质疑。本研究聚焦于高效（即计算受限）攻击场景下的高精度、低召回率设定，作出两项核心贡献：首先，我们证明经过校准的重合成方法在保持可控低误报率的同时，成为验证真实样本最可靠的途径；其次，我们验证该方法能有效抵御高效攻击，而现有方法在相同计算预算下极易被规避。该框架支持多模态数据，并融合了前沿的反演技术。",
    "url": "https://huggingface.co/papers/2512.15182",
    "arxiv_url": "https://arxiv.org/abs/2512.15182"
  },
  {
    "title": "SAGE: Training Smart Any-Horizon Agents for Long Video Reasoning with Reinforcement Learning",
    "summary": "As humans, we are natural any-horizon reasoners, i.e., we can decide whether to iteratively skim long videos or watch short ones in full when necessary for a given task. With this in mind, one would expect video reasoning models to reason flexibly across different durations. However, SOTA models are still trained to predict answers in a single turn while processing a large number of frames, akin to watching an entire long video, requiring significant resources. This raises the question: Is it possible to develop performant any-horizon video reasoning systems? Inspired by human behavior, we first propose SAGE, an agent system that performs multi-turn reasoning on long videos while handling simpler problems in a single turn. Secondly, we introduce an easy synthetic data generation pipeline using Gemini-2.5-Flash to train the orchestrator, SAGE-MM, which lies at the core of SAGE. We further propose an effective RL post-training recipe essential for instilling any-horizon reasoning ability in SAGE-MM. Thirdly, we curate SAGE-Bench with an average duration of greater than 700 seconds for evaluating video reasoning ability in real-world entertainment use cases. Lastly, we empirically validate the effectiveness of our system, data, and RL recipe, observing notable improvements of up to 6.1% on open-ended video reasoning tasks, as well as an impressive 8.2% improvement on videos longer than 10 minutes.",
    "translation": "标题：SAGE：基于强化学习的智能任意时长智能体训练及其在长视频推理中的应用\n\n摘要：人类天生具备任意时长的推理能力，即能够根据任务需求灵活选择快速浏览长视频或在必要时完整观看短视频。基于此，视频推理模型理应在不同时长范围内进行灵活推理。然而，现有前沿模型仍采用单轮预测答案的训练方式，需同时处理大量视频帧（类似于完整观看长视频），消耗大量计算资源。这引发了一个关键问题：能否开发出高性能的任意时长视频推理系统？受人类行为启发，我们首先提出SAGE智能体系统，该系统既能对长视频进行多轮推理，也能对简单问题实施单轮处理。其次，我们设计了基于Gemini-2.5-Flash的简易合成数据生成流程，用于训练SAGE的核心协调器SAGE-MM。进一步提出有效的强化学习后训练方案，该方案对培养SAGE-MM的任意时长推理能力至关重要。第三，我们构建了平均时长超过700秒的SAGE-Bench评测集，用于评估现实娱乐场景中的视频推理能力。最后，我们通过实证验证了系统架构、数据生成方法和强化学习方案的有效性：在开放式视频推理任务中取得最高6.1%的性能提升，在超过10分钟的长视频任务中更是获得8.2%的显著改进。",
    "url": "https://huggingface.co/papers/2512.13874",
    "arxiv_url": "https://arxiv.org/abs/2512.13874"
  },
  {
    "title": "MMSI-Video-Bench: A Holistic Benchmark for Video-Based Spatial Intelligence",
    "summary": "Spatial understanding over continuous visual input is crucial for MLLMs to evolve into general-purpose assistants in physical environments. Yet there is still no comprehensive benchmark that holistically assesses the progress toward this goal. In this work, we introduce MMSI-Video-Bench, a fully human-annotated benchmark for video-based spatial intelligence in MLLMs. It operationalizes a four-level framework, Perception, Planning, Prediction, and Cross-Video Reasoning, through 1,106 questions grounded in 1,278 clips from 25 datasets and in-house videos. Each item is carefully designed and reviewed by 3DV experts with explanatory rationales to ensure precise, unambiguous grounding. Leveraging its diverse data sources and holistic task coverage, MMSI-Video-Bench also supports three domain-oriented sub-benchmarks (Indoor Scene Perception Bench, Robot Bench and Grounding Bench) for targeted capability assessment. We evaluate 25 strong open-source and proprietary MLLMs, revealing a striking human--AI gap: many models perform near chance, and the best reasoning model lags humans by nearly 60%. We further find that spatially fine-tuned models still fail to generalize effectively on our benchmark. Fine-grained error analysis exposes systematic failures in geometric reasoning, motion grounding, long-horizon prediction, and cross-video correspondence. We also show that typical frame-sampling strategies transfer poorly to our reasoning-intensive benchmark, and that neither 3D spatial cues nor chain-of-thought prompting yields meaningful gains. We expect our benchmark to establish a solid testbed for advancing video-based spatial intelligence.",
    "translation": "标题：MMSI-Video-Bench：面向视频空间智能的综合基准测试框架\n\n摘要：对连续视觉输入的空间理解能力是多模态大语言模型发展为物理环境中通用助手的关键。然而，目前仍缺乏能够全面评估该目标进展的综合基准。本研究提出MMSI-Video-Bench——首个针对多模态大语言模型视频空间智能的全人工标注基准。该基准通过感知、规划、预测和跨视频推理的四层评估框架，基于来自25个公开数据集及自建视频的1,278个片段构建了1,106个精细化问题。每个问题项均由三维视觉专家精心设计并复核，配备解释性依据以确保精准无歧义的空间 grounding。凭借其多样化的数据来源与全维度任务覆盖，本基准还支持三个领域导向的子基准（室内场景感知基准、机器人操作基准与空间 grounding 基准）以实现针对性能力评估。通过对25个开源与商业多模态大语言模型的系统性评测，我们揭示了显著的人机能力差距：多数模型表现接近随机猜测，最优推理模型仍落后人类水平近60%。进一步研究发现，经过空间微调的模型在本基准上仍存在泛化失效问题。细粒度错误分析揭示了模型在几何推理、运动 grounding、长时程预测及跨视频关联等方面存在系统性缺陷。实验还表明，传统帧采样策略在本推理密集型基准上迁移效果不佳，三维空间线索与思维链提示技术均未带来显著性能提升。我们期望该基准能为推进视频空间智能研究建立坚实的测试平台。",
    "url": "https://huggingface.co/papers/2512.10863",
    "arxiv_url": "https://arxiv.org/abs/2512.10863"
  },
  {
    "title": "Can LLMs Guide Their Own Exploration? Gradient-Guided Reinforcement Learning for LLM Reasoning",
    "summary": "Reinforcement learning has become essential for strengthening the reasoning abilities of large language models, yet current exploration mechanisms remain fundamentally misaligned with how these models actually learn. Entropy bonuses and external semantic comparators encourage surface level variation but offer no guarantee that sampled trajectories differ in the update directions that shape optimization. We propose G2RL, a gradient guided reinforcement learning framework in which exploration is driven not by external heuristics but by the model own first order update geometry. For each response, G2RL constructs a sequence level feature from the model final layer sensitivity, obtainable at negligible cost from a standard forward pass, and measures how each trajectory would reshape the policy by comparing these features within a sampled group. Trajectories that introduce novel gradient directions receive a bounded multiplicative reward scaler, while redundant or off manifold updates are deemphasized, yielding a self referential exploration signal that is naturally aligned with PPO style stability and KL control. Across math and general reasoning benchmarks (MATH500, AMC, AIME24, AIME25, GPQA, MMLUpro) on Qwen3 base 1.7B and 4B models, G2RL consistently improves pass@1, maj@16, and pass@k over entropy based GRPO and external embedding methods. Analyzing the induced geometry, we find that G2RL expands exploration into substantially more orthogonal and often opposing gradient directions while maintaining semantic coherence, revealing that a policy own update space provides a far more faithful and effective basis for guiding exploration in large language model reinforcement learning.",
    "translation": "标题：大语言模型能否引导自身探索？基于梯度引导的强化学习用于大语言模型推理\n\n摘要：强化学习已成为增强大语言模型推理能力的关键手段，然而当前的探索机制本质上仍与这些模型的实际学习方式存在错配。熵奖励和外部语义比较器虽能鼓励表层多样性，但无法保证采样轨迹在影响优化过程的更新方向上存在差异。本文提出G2RL，一种梯度引导的强化学习框架，其探索驱动力并非来自外部启发式规则，而是源于模型自身的一阶更新几何结构。对于每个响应，G2RL从模型最终层的敏感度中构建序列级特征（该特征可通过标准前向传播以可忽略的成本获取），并通过在采样组内比较这些特征来衡量每条轨迹将如何重塑策略。引入新颖梯度方向的轨迹会获得有界的乘性奖励缩放因子，而冗余或偏离流形的更新则会被弱化，从而产生一种自然对齐PPO式稳定性与KL控制的自参照探索信号。在Qwen3基础版1.7B和4B模型上，针对数学与通用推理基准测试（MATH500、AMC、AIME24、AIME25、GPQA、MMLUpro），G2RL在pass@1、maj@16和pass@k指标上均持续优于基于熵的GRPO及外部嵌入方法。通过对诱导几何结构的分析，我们发现G2RL在保持语义连贯性的同时，将探索范围扩展至显著更多正交且常相对立的梯度方向，这表明策略自身的更新空间为大语言模型强化学习中的探索引导提供了更为可靠且有效的基准。",
    "url": "https://huggingface.co/papers/2512.15687",
    "arxiv_url": "https://arxiv.org/abs/2512.15687"
  },
  {
    "title": "FiNERweb: Datasets and Artifacts for Scalable Multilingual Named Entity Recognition",
    "summary": "Recent multilingual named entity recognition (NER) work has shown that large language models (LLMs) can provide effective synthetic supervision, yet such datasets have mostly appeared as by-products of broader experiments rather than as systematic, reusable resources. We introduce FiNERweb, a dataset-creation pipeline that scales the teacher-student paradigm to 91 languages and 25 scripts. Building on FineWeb-Edu, our approach trains regression models to identify NER-relevant passages and annotates them with multilingual LLMs, resulting in about 225k passages with 235k distinct entity labels. Our experiments show that the regression model achieves more than 84 F1, and that models trained on FiNERweb obtain comparable or improved performance in zero shot transfer settings on English, Thai, and Swahili, despite being trained on 19x less data than strong baselines. In addition, we assess annotation quality using LLM-as-a-judge and observe consistently high scores for both faithfulness (3.99 out of 5) and completeness (4.05 out of 5), indicating reliable and informative annotations. Further, we release the dataset with both English labels and translated label sets in the respective target languages because we observe that the performance of current state-of-the-art models drops by 0.02 to 0.09 F1 when evaluated using target language labels instead of English ones. We release FiNERweb together with all accompanying artifacts to the research community in order to facilitate more effective student-teacher training for multilingual named entity recognition.",
    "translation": "标题：FiNERweb：面向可扩展多语言命名实体识别的数据集与资源\n\n摘要：近期多语言命名实体识别（NER）研究表明，大语言模型（LLMs）能够提供有效的合成监督数据，但此类数据集大多作为广泛实验的副产品出现，而非系统化、可复用的资源。本文提出FiNERweb——一个将师生范式扩展至91种语言和25种文字体系的数据集构建流程。该方法基于FineWeb-Edu，通过训练回归模型识别NER相关文本段落，并利用多语言大语言模型进行标注，最终生成约22.5万条文本段落及23.5万个独立实体标签。实验表明：回归模型F1值超过84；基于FiNERweb训练的模型在英语、泰语和斯瓦希里语的零样本迁移场景中，仅使用基线模型1/19的数据量即可获得相当或更优的性能。此外，我们采用LLM-as-a-judge方法评估标注质量，在忠实度（5分制得3.99分）和完整性（5分制得4.05分）方面均保持稳定高分，表明标注结果可靠且信息丰富。值得注意的是，由于当前最优模型使用目标语言标签评估时F1值会下降0.02至0.09，我们同时发布了包含英文标签及对应目标语言翻译标签的数据集。我们将FiNERweb数据集及其全套资源向研究社区公开，以促进多语言命名实体识别领域更高效的师生训练范式发展。",
    "url": "https://huggingface.co/papers/2512.13884",
    "arxiv_url": "https://arxiv.org/abs/2512.13884"
  },
  {
    "title": "DiffusionVL: Translating Any Autoregressive Models into Diffusion Vision Language Models",
    "summary": "In recent multimodal research, the diffusion paradigm has emerged as a promising alternative to the autoregressive paradigm (AR), owing to its unique decoding advantages. However, due to the capability limitations of the base diffusion language model, the performance of the diffusion vision language model (dVLM) still lags significantly behind that of mainstream models. This leads to a simple yet fundamental question: Is it possible to construct dVLMs based on existing powerful AR models? In response, we propose DiffusionVL, a dVLM family that could be translated from any powerful AR models. Through simple fine-tuning, we successfully adapt AR pre-trained models into the diffusion paradigm. This approach yields two key observations: (1) The paradigm shift from AR-based multimodal models to diffusion is remarkably effective. (2) Direct conversion of an AR language model to a dVLM is also feasible, achieving performance competitive with LLaVA-style visual-instruction-tuning. Further, we introduce a block-decoding design into dVLMs that supports arbitrary-length generation and KV cache reuse, achieving a significant inference speedup. We conduct a large number of experiments. Despite training with less than 5% of the data required by prior methods, DiffusionVL achieves a comprehensive performance improvement-a 34.4% gain on the MMMU-Pro (vision) bench and 37.5% gain on the MME (Cog.) bench-alongside a 2x inference speedup. The model and code are released at https://github.com/hustvl/DiffusionVL.",
    "translation": "标题：DiffusionVL：将任意自回归模型转化为扩散视觉语言模型\n\n摘要：近年来在多模态研究中，扩散范式因其独特的解码优势，已成为自回归范式的有前景替代方案。然而，由于基础扩散语言模型的能力限制，扩散视觉语言模型的性能仍显著落后于主流模型。这引出了一个简单而根本的问题：能否基于现有强大的自回归模型构建扩散视觉语言模型？为此，我们提出了DiffusionVL，一个可从任意强大自回归模型转化而来的扩散视觉语言模型系列。通过简单的微调，我们成功将自回归预训练模型适配至扩散范式。该方法带来两个关键发现：（1）从基于自回归的多模态模型向扩散范式的转换效果显著；（2）将自回归语言模型直接转化为扩散视觉语言模型同样可行，其性能可与LLaVA风格的视觉指令微调模型相竞争。此外，我们在扩散视觉语言模型中引入了支持任意长度生成和KV缓存复用的块解码设计，实现了显著的推理加速。我们进行了大量实验：尽管训练数据量不足先前方法所需数据的5%，DiffusionVL仍实现了全面的性能提升——在MMMU-Pro（视觉）基准上提升34.4%，在MME（认知）基准上提升37.5%，同时推理速度提升2倍。模型与代码已发布于https://github.com/hustvl/DiffusionVL。",
    "url": "https://huggingface.co/papers/2512.15713",
    "arxiv_url": "https://arxiv.org/abs/2512.15713"
  },
  {
    "title": "VOYAGER: A Training Free Approach for Generating Diverse Datasets using LLMs",
    "summary": "Large language models (LLMs) are increasingly being used to generate synthetic datasets for the evaluation and training of downstream models. However, prior work has noted that such generated data lacks diversity. In this paper, we propose Voyager, a novel principled approach to generate diverse datasets. Our approach is iterative and directly optimizes a mathematical quantity that optimizes the diversity of the dataset using the machinery of determinantal point processes. Furthermore, our approach is training-free, applicable to closed-source models, and scalable. In addition to providing theoretical justification for the working of our method, we also demonstrate through comprehensive experiments that Voyager significantly outperforms popular baseline approaches by providing a 1.5-3x improvement in diversity.",
    "translation": "标题：VOYAGER：一种基于大语言模型的无训练多样化数据集生成方法\n\n摘要：大语言模型正日益广泛地用于生成合成数据集，以支持下游模型的评估与训练。然而，已有研究指出，此类生成的数据往往缺乏多样性。本文提出Voyager，一种基于原理的新型方法，用于生成多样化数据集。该方法采用迭代策略，并借助行列式点过程的理论工具，直接优化一个旨在提升数据集多样性的数学指标。此外，该方法无需训练、适用于闭源模型，且具有良好的可扩展性。我们不仅从理论上论证了本方法的有效性，还通过综合实验证明，Voyager在多样性指标上显著优于现有主流基线方法，实现了1.5至3倍的提升。",
    "url": "https://huggingface.co/papers/2512.12072",
    "arxiv_url": "https://arxiv.org/abs/2512.12072"
  },
  {
    "title": "End-to-End Training for Autoregressive Video Diffusion via Self-Resampling",
    "summary": "Autoregressive video diffusion models hold promise for world simulation but are vulnerable to exposure bias arising from the train-test mismatch. While recent works address this via post-training, they typically rely on a bidirectional teacher model or online discriminator. To achieve an end-to-end solution, we introduce Resampling Forcing, a teacher-free framework that enables training autoregressive video models from scratch and at scale. Central to our approach is a self-resampling scheme that simulates inference-time model errors on history frames during training. Conditioned on these degraded histories, a sparse causal mask enforces temporal causality while enabling parallel training with frame-level diffusion loss. To facilitate efficient long-horizon generation, we further introduce history routing, a parameter-free mechanism that dynamically retrieves the top-k most relevant history frames for each query. Experiments demonstrate that our approach achieves performance comparable to distillation-based baselines while exhibiting superior temporal consistency on longer videos owing to native-length training.",
    "translation": "标题：基于自重采样的自回归视频扩散模型端到端训练方法\n\n摘要：自回归视频扩散模型在环境模拟方面具有潜力，但容易受到训练-测试不匹配导致的曝光偏差影响。现有研究主要通过后训练方法解决此问题，但通常依赖于双向教师模型或在线判别器。为实现端到端解决方案，本文提出重采样强制训练框架——一种无需教师模型的架构，支持从零开始进行大规模自回归视频模型训练。该方法的核心理念是自重采样机制，通过在训练过程中模拟历史帧在推理阶段可能出现的模型误差。基于这些降质历史帧，稀疏因果掩码在保持时序因果性的同时，实现了帧级扩散损失的并行训练。为提升长序列生成效率，我们进一步提出历史路由机制——这是一种无需参数的动态检索方法，能够为每个查询帧自适应选取最相关的k个历史帧。实验结果表明，本方法在性能上可与基于知识蒸馏的基线模型相媲美，同时得益于原生长度训练策略，在长视频生成中展现出更优越的时序一致性。",
    "url": "https://huggingface.co/papers/2512.15702",
    "arxiv_url": "https://arxiv.org/abs/2512.15702"
  },
  {
    "title": "VABench: A Comprehensive Benchmark for Audio-Video Generation",
    "summary": "Recent advances in video generation have been remarkable, enabling models to produce visually compelling videos with synchronized audio. While existing video generation benchmarks provide comprehensive metrics for visual quality, they lack convincing evaluations for audio-video generation, especially for models aiming to generate synchronized audio-video outputs. To address this gap, we introduce VABench, a comprehensive and multi-dimensional benchmark framework designed to systematically evaluate the capabilities of synchronous audio-video generation. VABench encompasses three primary task types: text-to-audio-video (T2AV), image-to-audio-video (I2AV), and stereo audio-video generation. It further establishes two major evaluation modules covering 15 dimensions. These dimensions specifically assess pairwise similarities (text-video, text-audio, video-audio), audio-video synchronization, lip-speech consistency, and carefully curated audio and video question-answering (QA) pairs, among others. Furthermore, VABench covers seven major content categories: animals, human sounds, music, environmental sounds, synchronous physical sounds, complex scenes, and virtual worlds. We provide a systematic analysis and visualization of the evaluation results, aiming to establish a new standard for assessing video generation models with synchronous audio capabilities and to promote the comprehensive advancement of the field.",
    "translation": "标题：VABench：面向音视频生成的综合基准测试框架\n\n摘要：近期视频生成技术取得了显著进展，使得模型能够生成具有同步音频的视觉吸引力视频。虽然现有视频生成基准测试为视觉质量提供了全面的评估指标，但其缺乏对音视频生成（尤其是旨在生成同步音视频输出的模型）具有说服力的评估体系。为弥补这一空白，我们提出了VABench——一个综合性的多维基准测试框架，旨在系统评估同步音视频生成能力。VABench涵盖三大任务类型：文本到音视频生成、图像到音视频生成以及立体声音视频生成。该框架进一步构建了两大评估模块，覆盖15个评估维度，专门评估文本-视频、文本-音频、视频-音频的成对相似性，音视频同步性，唇语一致性，以及精心设计的音视频问答对等。此外，VABench涵盖七大内容类别：动物、人声、音乐、环境声音、同步物理声音、复杂场景和虚拟世界。我们通过系统化分析与可视化呈现评估结果，旨在为评估具备同步音频能力的视频生成模型建立新标准，并推动该领域的全面进步。",
    "url": "https://huggingface.co/papers/2512.09299",
    "arxiv_url": "https://arxiv.org/abs/2512.09299"
  },
  {
    "title": "In Pursuit of Pixel Supervision for Visual Pre-training",
    "summary": "At the most basic level, pixels are the source of the visual information through which we perceive the world. Pixels contain information at all levels, ranging from low-level attributes to high-level concepts. Autoencoders represent a classical and long-standing paradigm for learning representations from pixels or other raw inputs. In this work, we demonstrate that autoencoder-based self-supervised learning remains competitive today and can produce strong representations for downstream tasks, while remaining simple, stable, and efficient. Our model, codenamed \"Pixio\", is an enhanced masked autoencoder (MAE) with more challenging pre-training tasks and more capable architectures. The model is trained on 2B web-crawled images with a self-curation strategy with minimal human curation. Pixio performs competitively across a wide range of downstream tasks in the wild, including monocular depth estimation (e.g., Depth Anything), feed-forward 3D reconstruction (i.e., MapAnything), semantic segmentation, and robot learning, outperforming or matching DINOv3 trained at similar scales. Our results suggest that pixel-space self-supervised learning can serve as a promising alternative and a complement to latent-space approaches.",
    "translation": "标题：追求像素监督的视觉预训练\n\n摘要：在最基本的层面上，像素是我们感知世界的视觉信息源头。像素包含从低级属性到高级概念的所有层次信息。自编码器代表了从像素或其他原始输入中学习表征的经典且历史悠久的范式。在本研究中，我们证明基于自编码器的自监督学习至今仍具竞争力，能够为下游任务生成强大的表征，同时保持简单、稳定和高效。我们的模型代号为“Pixio”，是一种增强型掩码自编码器（MAE），具有更具挑战性的预训练任务和更强大的架构。该模型通过自筛选策略在20亿张网络爬取图像上进行训练，仅需极少量人工筛选。Pixio在广泛的现实下游任务中表现优异，包括单目深度估计（如Depth Anything）、前馈式三维重建（即MapAnything）、语义分割和机器人学习，其性能优于或匹配相似规模训练的DINOv3模型。我们的研究结果表明，像素空间自监督学习可作为潜在空间方法的有力替代和补充方案。",
    "url": "https://huggingface.co/papers/2512.15715",
    "arxiv_url": "https://arxiv.org/abs/2512.15715"
  },
  {
    "title": "Vision-Language-Action Models for Autonomous Driving: Past, Present, and Future",
    "summary": "Autonomous driving has long relied on modular \"Perception-Decision-Action\" pipelines, where hand-crafted interfaces and rule-based components often break down in complex or long-tailed scenarios. Their cascaded design further propagates perception errors, degrading downstream planning and control. Vision-Action (VA) models address some limitations by learning direct mappings from visual inputs to actions, but they remain opaque, sensitive to distribution shifts, and lack structured reasoning or instruction-following capabilities. Recent progress in Large Language Models (LLMs) and multimodal learning has motivated the emergence of Vision-Language-Action (VLA) frameworks, which integrate perception with language-grounded decision making. By unifying visual understanding, linguistic reasoning, and actionable outputs, VLAs offer a pathway toward more interpretable, generalizable, and human-aligned driving policies. This work provides a structured characterization of the emerging VLA landscape for autonomous driving. We trace the evolution from early VA approaches to modern VLA frameworks and organize existing methods into two principal paradigms: End-to-End VLA, which integrates perception, reasoning, and planning within a single model, and Dual-System VLA, which separates slow deliberation (via VLMs) from fast, safety-critical execution (via planners). Within these paradigms, we further distinguish subclasses such as textual vs. numerical action generators and explicit vs. implicit guidance mechanisms. We also summarize representative datasets and benchmarks for evaluating VLA-based driving systems and highlight key challenges and open directions, including robustness, interpretability, and instruction fidelity. Overall, this work aims to establish a coherent foundation for advancing human-compatible autonomous driving systems.",
    "translation": "标题：自动驾驶中的视觉-语言-动作模型：过去、现在与未来\n\n摘要：自动驾驶长期依赖模块化的“感知-决策-动作”流程，其中人工设计的接口与基于规则的组件在复杂或长尾场景中常出现失效。其级联式设计会进一步传播感知误差，导致下游规划与控制性能下降。视觉-动作模型通过从视觉输入到动作的直接映射学习，部分解决了上述局限，但仍存在可解释性差、对分布偏移敏感、缺乏结构化推理或指令跟随能力等问题。近年来，大语言模型与多模态学习的进展推动了视觉-语言-动作框架的兴起，该框架将感知与基于语言的决策相结合。通过统一视觉理解、语言推理与可执行输出，视觉-语言-动作模型为实现更具可解释性、泛化性及符合人类预期的驾驶策略提供了路径。本文对自动驾驶领域新兴的视觉-语言-动作研究体系进行了系统性梳理。我们追溯了从早期视觉-动作方法到现代视觉-语言-动作框架的演进历程，并将现有方法归纳为两大主要范式：端到端视觉-语言-动作模型（在单一模型中整合感知、推理与规划）与双系统视觉-语言-动作模型（通过视觉语言模型进行慢速决策，通过规划器执行快速安全关键任务）。在此分类基础上，我们进一步区分了文本型与数值型动作生成器、显式与隐式引导机制等子类别。同时，本文总结了用于评估基于视觉-语言-动作的驾驶系统的代表性数据集与基准测试，并着重指出了包括鲁棒性、可解释性与指令忠实度在内的关键挑战与未来方向。整体而言，本研究旨在为推动符合人类需求的自动驾驶系统建立清晰的理论基础。",
    "url": "https://huggingface.co/papers/2512.16760",
    "arxiv_url": "https://arxiv.org/abs/2512.16760"
  },
  {
    "title": "FrontierCS: Evolving Challenges for Evolving Intelligence",
    "summary": "We introduce FrontierCS, a benchmark of 156 open-ended problems across diverse areas of computer science, designed and reviewed by experts, including CS PhDs and top-tier competitive programming participants and problem setters. Unlike existing benchmarks that focus on tasks with known optimal solutions, FrontierCS targets problems where the optimal solution is unknown, but the quality of a solution can be objectively evaluated. Models solve these tasks by implementing executable programs rather than outputting a direct answer. FrontierCS includes algorithmic problems, which are often NP-hard variants of competitive programming problems with objective partial scoring, and research problems with the same property. For each problem we provide an expert reference solution and an automatic evaluator. Combining open-ended design, measurable progress, and expert curation, FrontierCS provides a benchmark at the frontier of computer-science difficulty. Empirically, we find that frontier reasoning models still lag far behind human experts on both the algorithmic and research tracks, that increasing reasoning budgets alone does not close this gap, and that models often over-optimize for generating merely workable code instead of discovering high-quality algorithms and system designs.",
    "translation": "标题：FrontierCS：进化智能面临的演进挑战\n\n摘要：本文提出FrontierCS基准测试集，该集合包含156个跨计算机科学多领域的开放式问题，由计算机科学博士、顶级竞赛编程参与者及命题专家共同设计与评审。与现有聚焦已知最优解任务的基准不同，FrontierCS针对那些最优解未知但解的质量可被客观评估的问题。模型需通过实现可执行程序（而非直接输出答案）来解决这些任务。该基准包含两类问题：一是具有客观部分评分机制的竞赛编程NP难变体算法问题，二是具备相同特性的研究型问题。每个问题均配备专家参考解决方案与自动评估器。通过融合开放式设计、可量化进展与专家评审机制，FrontierCS构建了处于计算机科学难度前沿的评估基准。实证研究表明：前沿推理模型在算法与研究两类任务上仍远落后于人类专家；仅增加推理预算无法弥合该差距；模型常过度优化生成仅可运行的代码，而非探索高质量算法与系统设计。",
    "url": "https://huggingface.co/papers/2512.15699",
    "arxiv_url": "https://arxiv.org/abs/2512.15699"
  },
  {
    "title": "VTCBench: Can Vision-Language Models Understand Long Context with Vision-Text Compression?",
    "summary": "The computational and memory overheads associated with expanding the context window of LLMs severely limit their scalability. A noteworthy solution is vision-text compression (VTC), exemplified by frameworks like DeepSeek-OCR and Glyph, which convert long texts into dense 2D visual representations, thereby achieving token compression ratios of 3x-20x. However, the impact of this high information density on the core long-context capabilities of vision-language models (VLMs) remains under-investigated. To address this gap, we introduce the first benchmark for VTC and systematically assess the performance of VLMs across three long-context understanding settings: VTC-Retrieval, which evaluates the model's ability to retrieve and aggregate information; VTC-Reasoning, which requires models to infer latent associations to locate facts with minimal lexical overlap; and VTC-Memory, which measures comprehensive question answering within long-term dialogue memory. Furthermore, we establish the VTCBench-Wild to simulate diverse input scenarios.We comprehensively evaluate leading open-source and proprietary models on our benchmarks. The results indicate that, despite being able to decode textual information (e.g., OCR) well, most VLMs exhibit a surprisingly poor long-context understanding ability with VTC-compressed information, failing to capture long associations or dependencies in the context.This study provides a deep understanding of VTC and serves as a foundation for designing more efficient and scalable VLMs.",
    "translation": "标题：VTCBench：视觉语言模型能否理解视觉文本压缩下的长上下文？\n\n摘要：扩展大型语言模型（LLM）上下文窗口所带来的计算与内存开销严重限制了其可扩展性。视觉文本压缩（VTC）作为一种值得关注的解决方案，以DeepSeek-OCR和Glyph等框架为代表，通过将长文本转换为密集的二维视觉表示，实现了3倍至20倍的标记压缩率。然而，这种高信息密度对视觉语言模型（VLM）核心长上下文理解能力的影响尚未得到充分研究。为填补这一空白，我们提出了首个针对VTC的基准测试，并系统评估了VLM在三种长上下文理解场景下的性能：VTC-检索，评估模型检索与整合信息的能力；VTC-推理，要求模型通过推断潜在关联来定位词汇重叠度极低的事实；以及VTC-记忆，衡量模型在长期对话记忆中进行综合问答的能力。此外，我们构建了VTCBench-Wild以模拟多样化的输入场景。我们在基准测试中对主流的开源与商业模型进行了全面评估。结果表明，尽管大多数VLM能够较好地解码文本信息（如OCR），但在处理VTC压缩信息时，其长上下文理解能力却表现出令人惊讶的不足，难以捕捉上下文中的长程关联或依赖关系。本研究深化了对VTC的理解，并为设计更高效、可扩展的VLM奠定了基础。",
    "url": "https://huggingface.co/papers/2512.15649",
    "arxiv_url": "https://arxiv.org/abs/2512.15649"
  },
  {
    "title": "SCOPE: Prompt Evolution for Enhancing Agent Effectiveness",
    "summary": "Large Language Model (LLM) agents are increasingly deployed in environments that generate massive, dynamic contexts. However, a critical bottleneck remains: while agents have access to this context, their static prompts lack the mechanisms to manage it effectively, leading to recurring Corrective and Enhancement failures. To address this capability gap, we introduce SCOPE (Self-evolving Context Optimization via Prompt Evolution). SCOPE frames context management as an online optimization problem, synthesizing guidelines from execution traces to automatically evolve the agent's prompt. We propose a Dual-Stream mechanism that balances tactical specificity (resolving immediate errors) with strategic generality (evolving long-term principles). Furthermore, we introduce Perspective-Driven Exploration to maximize strategy coverage, increasing the likelihood that the agent has the correct strategy for any given task. Experiments on the HLE benchmark show that SCOPE improves task success rates from 14.23\\% to 38.64\\% without human intervention. We make our code publicly available at https://github.com/JarvisPei/SCOPE.",
    "translation": "标题：SCOPE：基于提示演化的智能体效能增强方法\n\n摘要：大语言模型智能体正日益部署于产生海量动态上下文的环境中。然而，一个关键瓶颈依然存在：尽管智能体能够访问这些上下文，但其静态提示缺乏有效管理机制，导致纠正性错误与增强性失效反复出现。为弥补这一能力缺陷，本文提出SCOPE（基于提示演化的自进化上下文优化框架）。该框架将上下文管理构建为在线优化问题，通过综合分析执行轨迹生成指导原则，实现智能体提示的自动化演进。我们提出双流机制，在战术特异性（解决即时错误）与战略通用性（演化长期原则）之间实现动态平衡。此外，我们引入视角驱动探索方法以最大化策略覆盖范围，提升智能体针对任意任务具备正确策略的可能性。在HLE基准测试上的实验表明，SCOPE在无需人工干预的情况下将任务成功率从14.23%提升至38.64%。相关代码已开源：https://github.com/JarvisPei/SCOPE。",
    "url": "https://huggingface.co/papers/2512.15374",
    "arxiv_url": "https://arxiv.org/abs/2512.15374"
  },
  {
    "title": "Is Nano Banana Pro a Low-Level Vision All-Rounder? A Comprehensive Evaluation on 14 Tasks and 40 Datasets",
    "summary": "The rapid evolution of text-to-image generation models has revolutionized visual content creation. While commercial products like Nano Banana Pro have garnered significant attention, their potential as generalist solvers for traditional low-level vision challenges remains largely underexplored. In this study, we investigate the critical question: Is Nano Banana Pro a Low-Level Vision All-Rounder? We conducted a comprehensive zero-shot evaluation across 14 distinct low-level tasks spanning 40 diverse datasets. By utilizing simple textual prompts without fine-tuning, we benchmarked Nano Banana Pro against state-of-the-art specialist models. Our extensive analysis reveals a distinct performance dichotomy: while Nano Banana Pro demonstrates superior subjective visual quality, often hallucinating plausible high-frequency details that surpass specialist models, it lags behind in traditional reference-based quantitative metrics. We attribute this discrepancy to the inherent stochasticity of generative models, which struggle to maintain the strict pixel-level consistency required by conventional metrics. This report identifies Nano Banana Pro as a capable zero-shot contender for low-level vision tasks, while highlighting that achieving the high fidelity of domain specialists remains a significant hurdle.",
    "translation": "标题：Nano Banana Pro是低层次视觉全能手吗？基于14项任务与40个数据集的全方位评估\n\n摘要：文本到图像生成模型的快速发展正在彻底改变视觉内容创作领域。尽管如Nano Banana Pro等商业产品已获得广泛关注，但其作为传统低层次视觉问题通用解决方案的潜力仍未得到充分探索。本研究针对核心问题展开探讨：Nano Banana Pro是否具备低层次视觉全能处理能力？我们通过涵盖40个多样化数据集的14类低层次视觉任务，进行了全面的零样本性能评估。在不进行微调的情况下仅使用简单文本提示，我们将Nano Banana Pro与当前最先进的专用模型进行系统对比。深入分析揭示出显著的性能二分现象：Nano Banana Pro在主观视觉质量方面表现优异，常能生成超越专用模型的合理高频细节，但在传统基于参考图像的定量指标上存在不足。我们认为这种差异源于生成模型固有的随机性特征，使其难以满足传统度量标准对像素级一致性的严格要求。本报告确认Nano Banana Pro在低层次视觉任务中具备零样本竞争潜力，同时指出要达到领域专用模型的高保真度仍面临重大挑战。",
    "url": "https://huggingface.co/papers/2512.15110",
    "arxiv_url": "https://arxiv.org/abs/2512.15110"
  },
  {
    "title": "WAY: Estimation of Vessel Destination in Worldwide AIS Trajectory",
    "summary": "The Automatic Identification System (AIS) enables data-driven maritime surveillance but suffers from reliability issues and irregular intervals. We address vessel destination estimation using global-scope AIS data by proposing a differentiated approach that recasts long port-to-port trajectories as a nested sequence structure. Using spatial grids, this method mitigates spatio-temporal bias while preserving detailed resolution. We introduce a novel deep learning architecture, WAY, designed to process these reformulated trajectories for long-term destination estimation days to weeks in advance. WAY comprises a trajectory representation layer and Channel-Aggregative Sequential Processing (CASP) blocks. The representation layer generates multi-channel vector sequences from kinematic and non-kinematic features. CASP blocks utilize multi-headed channel- and self-attention for aggregation and sequential information delivery. Additionally, we propose a task-specialized Gradient Dropout (GD) technique to enable many-to-many training on single labels, preventing biased feedback surges by stochastically blocking gradient flow based on sample length. Experiments on 5-year AIS data demonstrate WAY's superiority over conventional spatial grid-based approaches regardless of trajectory progression. Results further confirm that adopting GD leads to performance gains. Finally, we explore WAY's potential for real-world application through multitask learning for ETA estimation.",
    "translation": "标题：WAY：全球AIS轨迹中的船舶目的地估计\n\n摘要：自动识别系统（AIS）为数据驱动的海事监控提供了可能，但其存在可靠性问题与数据间隔不规则等局限性。本研究针对全球范围AIS数据提出一种差异化船舶目的地估计方法，通过将长距离港到港轨迹重构为嵌套序列结构，在空间网格框架下缓解时空偏差的同时保持细节分辨率。我们提出名为WAY的新型深度学习架构，专门处理重构后的轨迹以实现提前数天至数周的长时目的地预测。该架构包含轨迹表征层与通道聚合序列处理模块：表征层从运动学与非运动学特征生成多通道向量序列；通道聚合序列处理模块采用多头通道注意力与自注意力机制实现特征聚合与序列信息传递。此外，我们提出任务专用的梯度丢弃技术，通过基于样本长度的随机梯度流阻断机制，在单标签数据上实现多对多训练，避免偏差反馈激增。基于五年AIS数据的实验表明，无论轨迹进展阶段如何，WAY均优于传统空间网格方法；结果同时验证梯度丢弃技术能有效提升模型性能。最后，我们通过到达时间估计的多任务学习框架，探讨了WAY在实际应用中的潜力。",
    "url": "https://huggingface.co/papers/2512.13190",
    "arxiv_url": "https://arxiv.org/abs/2512.13190"
  },
  {
    "title": "Understanding and Improving Hyperbolic Deep Reinforcement Learning",
    "summary": "The performance of reinforcement learning (RL) agents depends critically on the quality of the underlying feature representations. Hyperbolic feature spaces are well-suited for this purpose, as they naturally capture hierarchical and relational structure often present in complex RL environments. However, leveraging these spaces commonly faces optimization challenges due to the nonstationarity of RL. In this work, we identify key factors that determine the success and failure of training hyperbolic deep RL agents. By analyzing the gradients of core operations in the Poincaré Ball and Hyperboloid models of hyperbolic geometry, we show that large-norm embeddings destabilize gradient-based training, leading to trust-region violations in proximal policy optimization (PPO). Based on these insights, we introduce Hyper++, a new hyperbolic PPO agent that consists of three components: (i) stable critic training through a categorical value loss instead of regression; (ii) feature regularization guaranteeing bounded norms while avoiding the curse of dimensionality from clipping; and (iii) using a more optimization-friendly formulation of hyperbolic network layers. In experiments on ProcGen, we show that Hyper++ guarantees stable learning, outperforms prior hyperbolic agents, and reduces wall-clock time by approximately 30%. On Atari-5 with Double DQN, Hyper++ strongly outperforms Euclidean and hyperbolic baselines. We release our code at https://github.com/Probabilistic-and-Interactive-ML/hyper-rl .",
    "translation": "标题：理解与改进双曲深度强化学习\n\n摘要：强化学习智能体的性能关键取决于其底层特征表示的质量。双曲特征空间非常适合这一目的，因为它们能够自然地捕捉复杂强化学习环境中普遍存在的层次化与关系化结构。然而，由于强化学习的非平稳性，利用这些空间通常面临优化挑战。本研究确定了决定双曲深度强化学习智能体训练成败的关键因素。通过分析双曲几何的庞加莱球模型和双曲面模型中核心操作的梯度，我们发现大范数嵌入会破坏基于梯度的训练稳定性，导致近端策略优化算法中的信任域约束被违反。基于这些发现，我们提出了Hyper++——一种新型双曲近端策略优化智能体，它包含三个核心组件：（1）通过分类价值损失替代回归损失实现稳定的评论家训练；（2）特征正则化在保证范数有界的同时，避免了裁剪操作引发的维度灾难；（3）采用优化友好的双曲网络层表达形式。在ProcGen基准测试中，Hyper++能保证稳定的学习过程，其性能超越现有双曲智能体，并将实际训练时间缩短约30%。在基于Double DQN的Atari-5测试中，Hyper++显著优于欧几里得空间和双曲空间的基线方法。代码已发布于https://github.com/Probabilistic-and-Interactive-ML/hyper-rl。",
    "url": "https://huggingface.co/papers/2512.14202",
    "arxiv_url": "https://arxiv.org/abs/2512.14202"
  },
  {
    "title": "Hybrid Attribution Priors for Explainable and Robust Model Training",
    "summary": "Small language models (SLMs) are widely used in tasks that require low latency and lightweight deployment, particularly classification. As interpretability and robustness gain increasing importance, explanation-guided learning has emerged as an effective framework by introducing attribution-based supervision during training; however, deriving general and reliable attribution priors remains a significant challenge. Through an analysis of representative attribution methods in classification settings, we find that although these methods can reliably highlight class-relevant tokens, they often focus on common keywords shared by semantically similar classes. Because such classes are already difficult to distinguish under standard training, these attributions provide insufficient discriminative cues, limiting their ability to improve model differentiation. To overcome this limitation, we propose Class-Aware Attribution Prior (CAP), a novel attribution prior extraction framework that guides language models toward capturing fine-grained class distinctions and producing more salient, discriminative attribution priors. Building on this idea, we further introduce CAP Hybrid, which combines priors from CAP with those from existing attribution techniques to form a more comprehensive and balanced supervisory signal. By aligning a model's self-attribution with these enriched priors, our approach encourages the learning of diverse, decision-relevant features. Extensive experiments in full-data, few-shot, and adversarial scenarios demonstrate that our method consistently enhances both interpretability and robustness.",
    "translation": "标题：用于可解释与鲁棒模型训练的混合归因先验\n\n摘要：小型语言模型因其低延迟与轻量部署的优势，在分类等任务中被广泛使用。随着可解释性与鲁棒性日益受到重视，解释引导学习通过引入基于归因的监督机制，已成为一种有效的训练框架；然而，如何获取通用且可靠的归因先验仍面临重大挑战。通过对分类场景中代表性归因方法的分析，我们发现尽管这些方法能可靠地突出类别相关词元，却常聚焦于语义相似类别间共有的通用关键词。由于此类类别在标准训练下本就难以区分，此类归因提供的判别性线索不足，限制了其提升模型区分能力的效果。为突破这一局限，我们提出**类别感知归因先验**——一种新颖的归因先验提取框架，可引导语言模型捕捉细粒度类别差异，并生成更显著、更具判别力的归因先验。基于此，我们进一步提出**CAP混合框架**，将CAP生成的先验与现有归因技术的先验相结合，形成更全面、均衡的监督信号。通过使模型的自归因与这些增强后的先验对齐，我们的方法促进了多样化、决策相关特征的学习。在全数据、少样本及对抗场景中的大量实验表明，该方法能持续提升模型的可解释性与鲁棒性。",
    "url": "https://huggingface.co/papers/2512.14719",
    "arxiv_url": "https://arxiv.org/abs/2512.14719"
  },
  {
    "title": "SonicMoE: Accelerating MoE with IO and Tile-aware Optimizations",
    "summary": "Mixture of Experts (MoE) models have emerged as the de facto architecture for scaling up language models without significantly increasing the computational cost. Recent MoE models demonstrate a clear trend towards high expert granularity (smaller expert intermediate dimension) and higher sparsity (constant number of activated experts with higher number of total experts), which improve model quality per FLOP. However, fine-grained MoEs suffer from increased activation memory footprint and reduced hardware efficiency due to higher IO costs, while sparser MoEs suffer from wasted computations due to padding in Grouped GEMM kernels. In response, we propose a memory-efficient algorithm to compute the forward and backward passes of MoEs with minimal activation caching for the backward pass. We also design GPU kernels that overlap memory IO with computation benefiting all MoE architectures. Finally, we propose a novel \"token rounding\" method that minimizes the wasted compute due to padding in Grouped GEMM kernels. As a result, our method SonicMoE reduces activation memory by 45% and achieves a 1.86x compute throughput improvement on Hopper GPUs compared to ScatterMoE's BF16 MoE kernel for a fine-grained 7B MoE. Concretely, SonicMoE on 64 H100s achieves a training throughput of 213 billion tokens per day comparable to ScatterMoE's 225 billion tokens per day on 96 H100s for a 7B MoE model training with FSDP-2 using the lm-engine codebase. Under high MoE sparsity settings, our tile-aware token rounding algorithm yields an additional 1.16x speedup on kernel execution time compared to vanilla top-K routing while maintaining similar downstream performance. We open-source all our kernels to enable faster MoE model training.",
    "translation": "标题：SonicMoE：通过IO感知与分块感知优化加速混合专家模型\n\n摘要：混合专家模型已成为扩展语言模型规模而不显著增加计算成本的事实标准架构。近期研究显示，MoE模型呈现出高专家粒度化（更小的专家中间维度）与高稀疏化（激活专家数量恒定而专家总数增加）的明显趋势，从而提升单位浮点运算的模型质量。然而，细粒度MoE因更高的IO开销导致激活内存占用增加与硬件效率降低，而稀疏化MoE则因分组通用矩阵乘法内核中的填充操作产生计算浪费。为此，我们提出一种内存高效算法，以前向传播与反向传播过程中最小化激活缓存的方式计算MoE的前向与反向传播。同时，我们设计了可重叠内存IO与计算的GPU内核，使所有MoE架构受益。此外，我们创新性地提出\"令牌舍入\"方法，以最小化分组通用矩阵乘法内核中填充导致的无效计算。实验结果表明，对于细粒度70亿参数MoE模型，我们的SonicMoE方法在Hopper GPU上相比ScatterMoE的BF16 MoE内核减少45%的激活内存占用，并实现1.86倍的计算吞吐量提升。具体而言，在使用lm-engine代码库配合FSDP-2进行70亿参数MoE模型训练时，SonicMoE在64张H100显卡上达到每日2130亿令牌的训练吞吐量，与ScatterMoE在96张H100显卡上每日2250亿令牌的吞吐量相当。在高MoE稀疏度场景下，我们提出的分块感知令牌舍入算法相比传统Top-K路由机制，在保持相近下游性能的同时可获得额外1.16倍的内核执行加速。我们将所有内核代码开源，以促进更高效的MoE模型训练。\n\n请按照以下格式返回：\n标题：SonicMoE：通过IO感知与分块感知优化加速混合专家模型\n摘要：混合专家模型已成为扩展语言模型规模而不显著增加计算成本的事实标准架构。近期研究显示，MoE模型呈现出高专家粒度化（更小的专家中间维度）与高稀疏化（激活专家数量恒定而专家总数增加）的明显趋势，从而提升单位浮点运算的模型质量。然而，细粒度MoE因更高的IO开销导致激活内存占用增加与硬件效率降低，而稀疏化MoE则因分组通用矩阵乘法内核中的填充操作产生计算浪费。为此，我们提出一种内存高效算法，以前向传播与反向传播过程中最小化激活缓存的方式计算MoE的前向与反向传播。同时，我们设计了可重叠内存IO与计算的GPU内核，使所有MoE架构受益。此外，我们创新性地提出\"令牌舍入\"方法，以最小化分组通用矩阵乘法内核中填充导致的无效计算。实验结果表明，对于细粒度70亿参数MoE模型，我们的SonicMoE方法在Hopper GPU上相比ScatterMoE的BF16 MoE内核减少45%的激活内存占用，并实现1.86倍的计算吞吐量提升。具体而言，在使用lm-engine代码库配合FSDP-2进行70亿参数MoE模型训练时，SonicMoE在64张H100显卡上达到每日2130亿令牌的训练吞吐量，与ScatterMoE在96张H100显卡上每日2250亿令牌的吞吐量相当。在高MoE稀疏度场景下，我们提出的分块感知令牌舍入算法相比传统Top-K路由机制，在保持相近下游性能的同时可获得额外1.16倍的内核执行加速。我们将所有内核代码开源，以促进更高效的MoE模型训练。",
    "url": "https://huggingface.co/papers/2512.14080",
    "arxiv_url": "https://arxiv.org/abs/2512.14080"
  },
  {
    "title": "LikeBench: Evaluating Subjective Likability in LLMs for Personalization",
    "summary": "A personalized LLM should remember user facts, apply them correctly, and adapt over time to provide responses that the user prefers. Existing LLM personalization benchmarks are largely centered on two axes: accurately recalling user information and accurately applying remembered information in downstream tasks. We argue that a third axis, likability, is both subjective and central to user experience, yet under-measured by current benchmarks. To measure likability holistically, we introduce LikeBench, a multi-session, dynamic evaluation framework that measures likability across multiple dimensions by how much an LLM can adapt over time to a user's preferences to provide more likable responses. In LikeBench, the LLMs engage in conversation with a simulated user and learn preferences only from the ongoing dialogue. As the interaction unfolds, models try to adapt to responses, and after each turn, they are evaluated for likability across seven dimensions by the same simulated user. To the best of our knowledge, we are the first to decompose likability into multiple diagnostic metrics: emotional adaptation, formality matching, knowledge adaptation, reference understanding, conversation length fit, humor fit, and callback, which makes it easier to pinpoint where a model falls short. To make the simulated user more realistic and discriminative, LikeBench uses fine-grained, psychologically grounded descriptive personas rather than the coarse high/low trait rating based personas used in prior work. Our benchmark shows that strong memory performance does not guarantee high likability: DeepSeek R1, with lower memory accuracy (86%, 17 facts/profile), outperformed Qwen3 by 28% on likability score despite Qwen3's higher memory accuracy (93%, 43 facts/profile). Even SOTA models like GPT-5 adapt well in short exchanges but show only limited robustness in longer, noisier interactions.",
    "translation": "标题：LikeBench：评估大型语言模型个性化中的主观喜好度\n\n摘要：个性化大型语言模型应能记忆用户信息、正确应用这些信息，并随时间推移适应用户偏好以提供更受欢迎的回复。现有的大型语言模型个性化基准主要围绕两个维度展开：准确回忆用户信息以及在后续任务中准确应用已记忆信息。我们认为，第三个维度——喜好度——既是主观的，又是用户体验的核心，却在当前基准中未能得到充分衡量。为全面评估喜好度，我们提出了LikeBench，这是一个多会话动态评估框架，通过衡量大型语言模型随时间推移适应用户偏好以提供更受欢迎回复的能力，从多个维度评估喜好度。在LikeBench中，大型语言模型与模拟用户进行对话，仅通过持续对话学习用户偏好。随着交互的展开，模型尝试调整回复策略，并在每轮对话后由同一模拟用户从七个维度评估其喜好度表现。据我们所知，我们首次将喜好度分解为七个诊断性指标：情感适应性、正式度匹配、知识适应性、指代理解、对话长度适配度、幽默适配度及话题呼应能力，这有助于精准定位模型短板。为使模拟用户更具真实性与区分度，LikeBench采用基于心理学理论的细粒度描述性人物设定，而非以往研究中基于粗粒度高/低特征评级的设定。我们的基准测试表明，强大的记忆性能并不能保证高喜好度：尽管Qwen3的记忆准确率更高（93%，每档案43条信息），但DeepSeek R1在记忆准确率较低（86%，每档案17条信息）的情况下，其喜好度得分仍超出Qwen3达28%。即使是GPT-5等前沿模型，在简短交互中表现良好，但在更长、更嘈杂的交互中仅展现出有限的鲁棒性。",
    "url": "https://huggingface.co/papers/2512.13077",
    "arxiv_url": "https://arxiv.org/abs/2512.13077"
  },
  {
    "title": "Simultaneous Tactile-Visual Perception for Learning Multimodal Robot Manipulation",
    "summary": "Robotic manipulation requires both rich multimodal perception and effective learning frameworks to handle complex real-world tasks. See-through-skin (STS) sensors, which combine tactile and visual perception, offer promising sensing capabilities, while modern imitation learning provides powerful tools for policy acquisition. However, existing STS designs lack simultaneous multimodal perception and suffer from unreliable tactile tracking. Furthermore, integrating these rich multimodal signals into learning-based manipulation pipelines remains an open challenge. We introduce TacThru, an STS sensor enabling simultaneous visual perception and robust tactile signal extraction, and TacThru-UMI, an imitation learning framework that leverages these multimodal signals for manipulation. Our sensor features a fully transparent elastomer, persistent illumination, novel keyline markers, and efficient tracking, while our learning system integrates these signals through a Transformer-based Diffusion Policy. Experiments on five challenging real-world tasks show that TacThru-UMI achieves an average success rate of 85.5%, significantly outperforming the baselines of alternating tactile-visual (66.3%) and vision-only (55.4%). The system excels in critical scenarios, including contact detection with thin and soft objects and precision manipulation requiring multimodal coordination. This work demonstrates that combining simultaneous multimodal perception with modern learning frameworks enables more precise, adaptable robotic manipulation.",
    "translation": "标题：用于学习多模态机器人操作的同时触觉-视觉感知\n\n摘要：机器人操作需要丰富的多模态感知和有效的学习框架以处理复杂的现实世界任务。结合触觉与视觉感知的透皮（STS）传感器提供了有前景的感知能力，而现代模仿学习为策略获取提供了强大工具。然而，现有STS设计缺乏同步多模态感知能力，且触觉跟踪可靠性不足。此外，如何将这些丰富的多模态信号整合到基于学习的操作流程中仍是开放挑战。本文提出TacThru传感器——一种能实现同步视觉感知与鲁棒触觉信号提取的STS传感器，以及TacThru-UMI模仿学习框架——该框架利用多模态信号进行机械操作。我们的传感器采用全透明弹性体、持久照明、新型关键线标记和高效跟踪技术，学习系统则通过基于Transformer的扩散策略整合多模态信号。在五项具有挑战性的现实任务实验中，TacThru-UMI平均成功率达到85.5%，显著优于交替触觉-视觉感知（66.3%）和纯视觉感知（55.4%）的基线方法。该系统在关键场景中表现优异，包括对细薄柔软物体的接触检测，以及需要多模态协调的精密操作。本研究表明，将同步多模态感知与现代学习框架相结合，能够实现更精确、适应性更强的机器人操作。",
    "url": "https://huggingface.co/papers/2512.09851",
    "arxiv_url": "https://arxiv.org/abs/2512.09851"
  },
  {
    "title": "Towards Seamless Interaction: Causal Turn-Level Modeling of Interactive 3D Conversational Head Dynamics",
    "summary": "Human conversation involves continuous exchanges of speech and nonverbal cues such as head nods, gaze shifts, and facial expressions that convey attention and emotion. Modeling these bidirectional dynamics in 3D is essential for building expressive avatars and interactive robots. However, existing frameworks often treat talking and listening as independent processes or rely on non-causal full-sequence modeling, hindering temporal coherence across turns. We present TIMAR (Turn-level Interleaved Masked AutoRegression), a causal framework for 3D conversational head generation that models dialogue as interleaved audio-visual contexts. It fuses multimodal information within each turn and applies turn-level causal attention to accumulate conversational history, while a lightweight diffusion head predicts continuous 3D head dynamics that captures both coordination and expressive variability. Experiments on the DualTalk benchmark show that TIMAR reduces Fréchet Distance and MSE by 15-30% on the test set, and achieves similar gains on out-of-distribution data. The source code will be released in the GitHub repository https://github.com/CoderChen01/towards-seamleass-interaction.",
    "translation": "标题：迈向无缝交互：交互式三维会话头部动态的因果轮次建模\n\n摘要：人类会话涉及言语与非语言线索（如传达注意力和情感的点头、视线转移及面部表情）的持续交换。在三维空间中建模这种双向动态对于构建富有表现力的虚拟化身和交互式机器人至关重要。然而，现有框架常将说话与聆听视为独立过程，或依赖于非因果的全序列建模，导致跨轮次的时间连贯性受限。本文提出TIMAR（轮次交错掩码自回归）框架，这是一种用于三维会话头部生成的因果建模框架，将对话建模为交错的多模态视听上下文。该框架在每轮对话中融合多模态信息，并应用轮次级因果注意力机制以累积会话历史，同时通过轻量级扩散头部预测连续的三维头部动态，从而捕捉协调性与表达变异性。在DualTalk基准测试上的实验表明，TIMAR在测试集上将弗雷歇距离与均方误差降低了15-30%，并在分布外数据上取得相近的增益。源代码将在GitHub仓库 https://github.com/CoderChen01/towards-seamleass-interaction 中公开。",
    "url": "https://huggingface.co/papers/2512.15340",
    "arxiv_url": "https://arxiv.org/abs/2512.15340"
  }
]