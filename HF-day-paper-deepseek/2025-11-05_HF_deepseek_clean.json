[
  {
    "title": "Don't Blind Your VLA: Aligning Visual Representations for OOD\n  Generalization",
    "summary": "The growing success of Vision-Language-Action (VLA) models stems from the\npromise that pretrained Vision-Language Models (VLMs) can endow agents with\ntransferable world knowledge and vision-language (VL) grounding, laying a\nfoundation for action models with broader generalization. Yet when these VLMs\nare adapted to the action modality, it remains unclear to what extent their\noriginal VL representations and knowledge are preserved. In this work, we\nconduct a systematic study of representation retention during VLA fine-tuning,\nshowing that naive action fine-tuning leads to degradation of visual\nrepresentations. To characterize and measure these effects, we probe VLA's\nhidden representations and analyze attention maps, further, we design a set of\ntargeted tasks and methods that contrast VLA models with their counterpart\nVLMs, isolating changes in VL capabilities induced by action fine-tuning. We\nfurther evaluate a range of strategies for aligning visual representations and\nintroduce a simple yet effective method that mitigates degradation and yields\nimproved generalization to out-of-distribution (OOD) scenarios. Taken together,\nour analysis clarifies the trade-off between action fine-tuning and the\ndegradation of VL representations and highlights practical approaches to\nrecover inherited VL capabilities. Code is publicly available:\nhttps://blind-vla-paper.github.io",
    "translation": "标题：勿使视觉语言动作模型失明：面向分布外泛化的视觉表征对齐\n\n摘要：视觉-语言-动作模型日益成功的根源在于，经过预训练的视觉语言模型能够赋予智能体可迁移的世界知识与视觉语言 grounding 能力，为构建具有更广泛泛化能力的动作模型奠定基础。然而当这些视觉语言模型被适配至动作模态时，其原始视觉语言表征与知识保留程度仍不明确。本研究系统探讨了视觉语言动作模型微调过程中的表征保持问题，发现直接进行动作微调会导致视觉表征的退化。为量化表征变化，我们通过探测隐藏表征与分析注意力图谱，设计了一套对比评估框架，将视觉语言动作模型与其对应的视觉语言模型进行多维度对比，从而分离出动作微调对视觉语言能力的特定影响。我们进一步评估了多种视觉表征对齐策略，并提出一种简单有效的方法来缓解表征退化，显著提升模型在分布外场景的泛化能力。综合而言，本文阐明了动作微调与视觉语言表征退化之间的权衡关系，并提出了恢复继承性视觉语言能力的实用方案。代码已公开：https://blind-vla-paper.github.io",
    "url": "https://huggingface.co/papers/2510.25616",
    "arxiv_url": "https://arxiv.org/abs/2510.25616"
  },
  {
    "title": "VCode: a Multimodal Coding Benchmark with SVG as Symbolic Visual\n  Representation",
    "summary": "Code has emerged as a precise and executable medium for reasoning and action\nin the agent era. Yet, progress has largely focused on language-centric tasks\nsuch as program synthesis and debugging, leaving visual-centric coding\nunderexplored. Inspired by how humans reason over sketches, we advocate SVG\ncode as a compact, interpretable, and executable visual representation. We\nintroduce VCode, a benchmark that reframes multimodal understanding as code\ngeneration: given an image, a model must produce SVG that preserves symbolic\nmeaning for downstream reasoning. VCode covers three domains - general\ncommonsense (MM-Vet), professional disciplines (MMMU), and visual-centric\nperception (CV-Bench). To assess symbolic fidelity, we propose CodeVQA, a novel\nevaluation protocol in which a policy model answers questions over rendered\nSVGs; correct answers indicate faithful symbolic preservation. Empirically,\nfrontier VLMs struggle to generate faithful SVGs, revealing a persistent gap\nbetween language-centric and visual-centric coding. To close this gap, we\nintroduce VCoder, an agentic framework that augments VLMs along two axes: (i)\nThinking with Revision, which iteratively analyzes discrepancies and refines\nSVG code; and (ii) Acting with Visual Tools, where detectors and parsers supply\nstructured cues such as objects, shapes, and text beyond the model's intrinsic\ncapacity. Across benchmarks, frontier VLMs with strong reasoning capabilities\nscore well overall yet remain limited in professional knowledge and 3D\nreasoning. VCoder delivers a 12.3-point overall gain over the top-performing\nClaude-4-Opus. Human studies show that both humans and VLMs perform worse on\nrendered SVGs, their consistency reveals the promise of symbolic visual\nrepresentation. The benchmark and code are available at\nhttps://github.com/CSU-JPG/VCode.",
    "translation": "标题：VCode：以SVG作为符号化视觉表征的多模态编程基准\n\n摘要：在智能体时代，代码已成为一种精确且可执行的推理与行动媒介。然而当前进展主要集中于以语言为中心的任务（如程序合成与调试），致使以视觉为中心的编码研究探索不足。受人类通过草图进行推理的启发，我们主张将SVG代码作为一种紧凑、可解释且可执行的视觉表征。本文提出VCode基准，将多模态理解重新定义为代码生成任务：给定图像，模型需生成能保持符号意义以支持下游推理的SVG代码。VCode涵盖三大领域——通用常识（MM-Vet）、专业学科（MMMU）和视觉中心感知（CV-Bench）。为评估符号保真度，我们提出CodeVQA创新评估协议：策略模型在渲染后的SVG上回答问题，正确答案表明符号得到了忠实保留。实验表明，前沿视觉语言模型在生成忠实SVG方面存在困难，揭示了以语言为中心和以视觉为中心的编码之间存在持续差距。为弥合这一差距，我们提出VCoder智能体框架，从两个维度增强视觉语言模型：（1）修订思维：迭代分析差异并优化SVG代码；（2）视觉工具行动：通过检测器和解析器提供模型内在能力之外的结构化线索（如物体、形状和文本）。跨基准测试表明，具有强推理能力的前沿视觉语言模型总体得分良好，但在专业知识和3D推理方面仍存在局限。VCoder相较性能最优的Claude-4-Opus实现12.3分的综合提升。人类研究表明，人类与视觉语言模型在渲染SVG上的表现均有所下降，但二者表现的一致性揭示了符号化视觉表征的潜力。基准与代码已发布于https://github.com/CSU-JPG/VCode。",
    "url": "https://huggingface.co/papers/2511.02778",
    "arxiv_url": "https://arxiv.org/abs/2511.02778"
  },
  {
    "title": "When Visualizing is the First Step to Reasoning: MIRA, a Benchmark for\n  Visual Chain-of-Thought",
    "summary": "We propose MIRA, a new benchmark designed to evaluate models in scenarios\nwhere generating intermediate visual images is essential for successful\nreasoning. Unlike traditional CoT methods that rely solely on text, tasks in\nMIRA require models to generate and utilize intermediate images - such as\nsketches, structural diagrams, or path drawings - to guide their reasoning\nprocess. This setup closely mirrors how humans solve complex problems through\n\"drawing to think\". To solve this, MIRA focuses on tasks that are intrinsically\nchallenging and involve complex structures, spatial relationships, or reasoning\nsteps that are difficult to express through language alone. To ensure that our\nevaluation data is of high-quality, we include 546 multimodal problems,\nannotated with intermediate visual images and final answers. We also propose a\nunified evaluation protocol for MIRA that spans three levels of evaluation\ninput: direct input with image and question only, text-only CoT input with\nimage and thinking prompts, and Visual-CoT input with both annotated image\nclues and textual thinking prompts. To probe the upper bound of model capacity\non our benchmark, we also report pass@k and majority voting accuracies under\ndifferent k settings. Experimental results show that existing multimodal large\nlanguage models, including strongest private models as well as strong\nopen-weight models, perform poorly when relying solely on textual prompts.\nHowever, when intermediate visual cues are provided, model performance improves\nconsistently, yielding an average relative gain of 33.7% across all models and\ntasks. We also probe the upper bound by expanding the search space and\ndesigning textual prompts aligned with Visual-CoT, but both yield only limited\nimprovements compared to our Visual-CoT setting. These results underscore the\ncritical role of imagined visual information in enabling successful reasoning\non MIRA.",
    "translation": "标题：当可视化成为推理的第一步：MIRA——视觉思维链基准测试集\n\n摘要：我们提出MIRA这一新型基准测试集，旨在评估模型在需要生成中间视觉图像以完成推理任务的场景中的表现。与传统仅依赖文本的思维链方法不同，MIRA中的任务要求模型生成并利用中间图像——如草图、结构图或路径示意图——来引导推理过程。这种设置高度模拟了人类通过“绘图思考”解决复杂问题的方式。该测试集聚焦于本质上具有挑战性的任务，涉及复杂结构、空间关系或难以仅用语言表达的推理步骤。为确保评估数据质量，我们收录了546个多模态问题，并标注了中间视觉图像与最终答案。我们还为MIRA设计了统一的评估协议，涵盖三个层次的评估输入：仅含图像和问题的直接输入、附带图像与思维提示的纯文本思维链输入、以及同时包含标注图像线索与文本思维提示的视觉思维链输入。为探索模型在本测试集上的能力上限，我们还报告了不同k值设置下的pass@k和多数投票准确率。实验结果表明，现有多模态大语言模型（包括最强的私有模型和优秀的开源模型）在仅依赖文本提示时表现不佳，但当提供中间视觉线索后，模型性能获得持续提升，在所有模型和任务中平均相对增益达33.7%。我们通过扩展搜索空间和设计与视觉思维链对齐的文本提示来探索性能上限，但这两者相较于我们的视觉思维链设置仅带来有限改进。这些发现凸显了想象视觉信息在MIRA测试集上实现成功推理的关键作用。",
    "url": "https://huggingface.co/papers/2511.02779",
    "arxiv_url": "https://arxiv.org/abs/2511.02779"
  },
  {
    "title": "When Modalities Conflict: How Unimodal Reasoning Uncertainty Governs\n  Preference Dynamics in MLLMs",
    "summary": "Multimodal large language models (MLLMs) must resolve conflicts when\ndifferent modalities provide contradictory information, a process we term\nmodality following. Prior work measured this behavior only with coarse\ndataset-level statistics, overlooking the influence of model's confidence in\nunimodal reasoning. In this paper, we introduce a new framework that decomposes\nmodality following into two fundamental factors: relative reasoning uncertainty\n(the case-specific confidence gap between unimodal predictions) and inherent\nmodality preference( a model's stable bias when uncertainties are balanced). To\nvalidate this framework, we construct a controllable dataset that\nsystematically varies the reasoning difficulty of visual and textual inputs.\nUsing entropy as a fine-grained uncertainty metric, we uncover a universal law:\nthe probability of following a modality decreases monotonically as its relative\nuncertainty increases. At the relative difficulty level where the model tends\nto follow both modalities with comparable probability what we call the balance\npoint, a practical indicator of the model's inherent preference. Unlike\ntraditional macro-level ratios, this measure offers a more principled and less\nconfounded way to characterize modality bias, disentangling it from unimodal\ncapabilities and dataset artifacts. Further, by probing layer-wise predictions,\nwe reveal the internal mechanism of oscillation: in ambiguous regions near the\nbalance point, models vacillate between modalities across layers, explaining\nexternally observed indecision. Together, these findings establish relative\nuncertainty and inherent preference as the two governing principles of modality\nfollowing, offering both a quantitative framework and mechanistic insight into\nhow MLLMs resolve conflicting information.",
    "translation": "标题：模态冲突的解决机制：多模态大语言模型中单模态推理不确定性如何主导偏好动态\n\n摘要：当不同模态提供相互矛盾的信息时，多模态大语言模型必须进行冲突消解，这一过程我们称之为模态追随。现有研究仅通过粗糙的数据集级统计量衡量该行为，忽视了模型在单模态推理中置信度的影响。本文提出一个新框架，将模态追随分解为两个基本要素：相对推理不确定性（单模态预测间针对具体案例的置信度差异）与内在模态偏好（不确定性平衡时模型的稳定偏向）。为验证此框架，我们构建了可控制系统调节视觉与文本输入推理难度的实验数据集。通过以信息熵作为细粒度不确定性指标，我们发现了一个普适规律：追随某一模态的概率随其相对不确定性的增加而单调递减。在模型以相近概率追随双模态的相对难度水平——即平衡点处，可有效衡量模型的内在偏好。与传统宏观比率不同，该指标提供了更原理性且更少混杂的模态偏向表征方式，使其与单模态能力及数据集伪影解耦。进一步通过分层预测探测，我们揭示了决策振荡的内在机制：在平衡点附近的模糊区域中，模型会在不同层级间交替选择模态，这解释了外部观察到的决策犹豫现象。这些发现共同确立了相对不确定性与内在偏好作为模态追随的两大支配原则，为理解多模态大语言模型如何解决信息冲突提供了量化框架与机制解释。",
    "url": "https://huggingface.co/papers/2511.02243",
    "arxiv_url": "https://arxiv.org/abs/2511.02243"
  },
  {
    "title": "The Collaboration Gap",
    "summary": "The trajectory of AI development suggests that we will increasingly rely on\nagent-based systems composed of independently developed agents with different\ninformation, privileges, and tools. The success of these systems will\ncritically depend on effective collaboration among these heterogeneous agents,\neven under partial observability. Despite intense interest, few empirical\nstudies have evaluated such agent-agent collaboration at scale. We propose a\ncollaborative maze-solving benchmark that (i) isolates collaborative\ncapabilities, (ii) modulates problem complexity, (iii) enables scalable\nautomated grading, and (iv) imposes no output-format constraints, preserving\necological plausibility. Using this framework, we evaluate 32 leading open- and\nclosed-source models in solo, homogeneous, and heterogeneous pairings. Our\nresults reveal a \"collaboration gap\": models that perform well solo often\ndegrade substantially when required to collaborate. Collaboration can break\ndown dramatically; for instance, small distilled models that solve mazes well\nalone may fail almost completely in certain pairings. We find that starting\nwith the stronger agent often improves outcomes, motivating a \"relay inference\"\napproach where the stronger agent leads before handing off to the weaker one,\nclosing much of the gap. Our findings argue for (1) collaboration-aware\nevaluation, (2) training strategies developed to enhance collaborative\ncapabilities, and (3) interaction design that reliably elicits agents' latent\nskills, guidance that applies to AI-AI and human-AI collaboration.",
    "translation": "标题：协作鸿沟\n\n摘要：人工智能的发展轨迹表明，我们将日益依赖由具有不同信息、权限和工具的独立智能体构成的代理系统。这些系统的成功关键取决于异质智能体间的有效协作——即使在部分可观测条件下。尽管备受关注，目前仍缺乏对这类智能体间协作的大规模实证研究。我们提出一个协作式迷宫求解基准测试框架，其具备以下特性：（1）隔离协作能力评估；（2）可调节问题复杂度；（3）支持可扩展的自动化评分；（4）不设输出格式限制，保持生态效度。基于该框架，我们评估了32个领先的开源与闭源模型在独立运行、同构配对及异构配对三种模式下的表现。研究结果揭示了“协作鸿沟”现象：独立表现优异的模型在需要协作时性能显著下降。协作崩溃可能极为严重，例如某些独立求解表现良好的蒸馏小模型，在特定配对中几乎完全失效。我们发现由较强智能体主导协作往往能改善结果，由此提出“接力推理”方法——强智能体先行处理再移交弱智能体，该方法可大幅缩小协作鸿沟。我们的研究主张：（1）建立协作感知的评估体系；（2）开发增强协作能力的训练策略；（3）设计能可靠激发智能体潜在技能的交互机制，这些指导原则同时适用于AI-AI与人类-AI协作场景。",
    "url": "https://huggingface.co/papers/2511.02687",
    "arxiv_url": "https://arxiv.org/abs/2511.02687"
  },
  {
    "title": "Step-Audio-EditX Technical Report",
    "summary": "We present Step-Audio-EditX, the first open-source LLM-based audio model\nexcelling at expressive and iterative audio editing encompassing emotion,\nspeaking style, and paralinguistics alongside robust zero-shot text-to-speech\n(TTS) capabilities.Our core innovation lies in leveraging only large-margin\nsynthetic data, which circumvents the need for embedding-based priors or\nauxiliary modules. This large-margin learning approach enables both iterative\ncontrol and high expressivity across voices, and represents a fundamental pivot\nfrom the conventional focus on representation-level disentanglement. Evaluation\nresults demonstrate that Step-Audio-EditX surpasses both MiniMax-2.6-hd and\nDoubao-Seed-TTS-2.0 in emotion editing and other fine-grained control tasks.",
    "translation": "标题：Step-Audio-EditX技术报告\n\n摘要：本文提出Step-Audio-EditX——首个基于大语言模型的开源音频系统，在保持强大零样本文本转语音能力的同时，擅长处理包含情感、说话风格与副语言特征的表现力音频编辑任务。我们的核心创新在于仅采用大间隔合成数据进行训练，无需依赖基于嵌入的先验知识或辅助模块。这种大间隔学习方法既实现了对声音的迭代控制，又保障了高表现力，标志着从传统表征级解耦研究范式的根本性转变。评估结果表明，在情感编辑及其他细粒度控制任务中，Step-Audio-EditX的表现均优于MiniMax-2.6-hd和Doubao-Seed-TTS-2.0系统。",
    "url": "https://huggingface.co/papers/2511.03601",
    "arxiv_url": "https://arxiv.org/abs/2511.03601"
  },
  {
    "title": "Brain-IT: Image Reconstruction from fMRI via Brain-Interaction\n  Transformer",
    "summary": "Reconstructing images seen by people from their fMRI brain recordings\nprovides a non-invasive window into the human brain. Despite recent progress\nenabled by diffusion models, current methods often lack faithfulness to the\nactual seen images. We present \"Brain-IT\", a brain-inspired approach that\naddresses this challenge through a Brain Interaction Transformer (BIT),\nallowing effective interactions between clusters of functionally-similar\nbrain-voxels. These functional-clusters are shared by all subjects, serving as\nbuilding blocks for integrating information both within and across brains. All\nmodel components are shared by all clusters & subjects, allowing efficient\ntraining with a limited amount of data. To guide the image reconstruction, BIT\npredicts two complementary localized patch-level image features: (i)high-level\nsemantic features which steer the diffusion model toward the correct semantic\ncontent of the image; and (ii)low-level structural features which help to\ninitialize the diffusion process with the correct coarse layout of the image.\nBIT's design enables direct flow of information from brain-voxel clusters to\nlocalized image features. Through these principles, our method achieves image\nreconstructions from fMRI that faithfully reconstruct the seen images, and\nsurpass current SotA approaches both visually and by standard objective\nmetrics. Moreover, with only 1-hour of fMRI data from a new subject, we achieve\nresults comparable to current methods trained on full 40-hour recordings.",
    "translation": "标题：Brain-IT：基于脑交互Transformer的功能磁共振成像图像重建\n\n摘要：通过功能磁共振成像（fMRI）脑记录重建人眼所见图像，为研究人脑认知提供了非侵入式观测窗口。尽管扩散模型推动了该领域的发展，现有方法仍难以准确还原真实视觉图像。本文提出受脑启发的\"Brain-IT\"方法，通过脑交互Transformer（BIT）实现功能相似脑体素簇间的有效交互。这些功能簇作为跨被试共享的基础单元，构成脑内与脑间信息整合的构建模块。所有模型组件均实现跨簇群与被试的共享，从而在有限数据条件下实现高效训练。为引导图像重建，BIT预测两种互补的局部块级图像特征：（1）高层语义特征——引导扩散模型生成正确的图像语义内容；（2）底层结构特征——为扩散过程提供准确的图像粗粒度布局初始化。BIT的设计实现了从脑体素簇到局部图像特征的直接信息流传递。基于这些原理，我们的方法实现了从fMRI信号中忠实还原视觉图像，在视觉观感和客观指标上均超越现有先进方法。此外，仅需新被试1小时的fMRI数据，即可达到当前基于40小时完整数据训练方法的同等效果。",
    "url": "https://huggingface.co/papers/2510.25976",
    "arxiv_url": "https://arxiv.org/abs/2510.25976"
  },
  {
    "title": "Can Visual Input Be Compressed? A Visual Token Compression Benchmark for\n  Large Multimodal Models",
    "summary": "Large multimodal models (LMMs) often suffer from severe inference\ninefficiency due to the large number of visual tokens introduced by image\nencoders. While recent token compression methods, such as pruning and merging,\nhave shown promise in reducing redundancy, their evaluation remains fragmented\nand inconsistent. In this work, we present UniPruneBench, a unified and\nextensible benchmark for visual token pruning in multimodal LLMs. UniPruneBench\nprovides standardized protocols across six ability dimensions and ten datasets,\ncovering ten representative compression algorithms and three families of LMMs\n(LLaVA-v1.5, Intern-VL3, and Qwen2.5-VL). Beyond task accuracy, it incorporates\nsystem-level metrics such as runtime and prefilling latency to provide a\nholistic view. Our experiments uncover several key findings: (1) random pruning\nis a surprisingly strong baseline, (2) no single method consistently\noutperforms others across scenarios, (3) pruning sensitivity varies\nsignificantly across tasks, with OCR being most vulnerable, and (4) pruning\nratio is the dominant factor governing performance degradation. We believe\nUniPruneBench will serve as a reliable foundation for future research on\nefficient multimodal modeling.",
    "translation": "标题：视觉输入能否被压缩？面向大型多模态模型的视觉令牌压缩基准  \n\n摘要：大型多模态模型常因图像编码器引入的大量视觉令牌而存在严重的推理效率问题。尽管剪枝、融合等新兴令牌压缩方法已展现出降低冗余的潜力，但其评估体系仍存在碎片化与不一致性。本研究提出UniPruneBench——一个统一且可扩展的多模态大模型视觉令牌剪枝基准。该基准在六大能力维度与十大数据集上建立标准化评估协议，涵盖十种代表性压缩算法及三类主流LMM架构（LLaVA-v1.5、Intern-VL与Qwen2.5-VL）。除任务精度外，基准创新性引入运行时与预填充延迟等系统级指标，形成全景评估体系。实验揭示关键发现：（1）随机剪枝作为基线方法表现出超预期的鲁棒性；（2）尚无单一方法能在所有场景中持续领先；（3）不同任务对剪枝的敏感度差异显著，其中OCR任务最易受损；（4）压缩比率是主导性能衰减的核心因素。我们相信UniPruneBench将为高效多模态建模的未来研究提供可靠基础。",
    "url": "https://huggingface.co/papers/2511.02650",
    "arxiv_url": "https://arxiv.org/abs/2511.02650"
  },
  {
    "title": "LTD-Bench: Evaluating Large Language Models by Letting Them Draw",
    "summary": "Current evaluation paradigms for large language models (LLMs) represent a\ncritical blind spot in AI research--relying on opaque numerical metrics that\nconceal fundamental limitations in spatial reasoning while providing no\nintuitive understanding of model capabilities. This deficiency creates a\ndangerous disconnect between reported performance and practical abilities,\nparticularly for applications requiring physical world understanding. We\nintroduce LTD-Bench, a breakthrough benchmark that transforms LLM evaluation\nfrom abstract scores to directly observable visual outputs by requiring models\nto generate drawings through dot matrices or executable code. This approach\nmakes spatial reasoning limitations immediately apparent even to non-experts,\nbridging the fundamental gap between statistical performance and intuitive\nassessment. LTD-Bench implements a comprehensive methodology with complementary\ngeneration tasks (testing spatial imagination) and recognition tasks (assessing\nspatial perception) across three progressively challenging difficulty levels,\nmethodically evaluating both directions of the critical language-spatial\nmapping. Our extensive experiments with state-of-the-art models expose an\nalarming capability gap: even LLMs achieving impressive results on traditional\nbenchmarks demonstrate profound deficiencies in establishing bidirectional\nmappings between language and spatial concept--a fundamental limitation that\nundermines their potential as genuine world models. Furthermore, LTD-Bench's\nvisual outputs enable powerful diagnostic analysis, offering a potential\napproach to investigate model similarity.",
    "translation": "标题：LTD-Bench：通过绘图能力评估大语言模型\n\n摘要：当前大语言模型的评估范式存在关键盲点——依赖不透明的数值指标，既掩盖了空间推理的根本缺陷，又无法直观呈现模型能力。这种缺陷导致报告性能与实际应用能力间产生严重脱节，在需要物理世界理解的应用场景中尤为突出。我们提出突破性基准测试LTD-Bench，通过要求模型以点阵图或可执行代码生成绘图，将LLM评估从抽象分数转化为可直接观察的可视化输出。该方法即使对非专业人士也能即时呈现空间推理缺陷，有效弥合统计性能与直觉评估之间的根本差距。LTD-Bench采用包含互补生成任务（测试空间想象力）与识别任务（评估空间感知力）的完整方法体系，设置三个渐进难度级别，系统化评估语言-空间映射关键环节的双向能力。通过对前沿模型的大规模实验，我们揭示出令人警醒的能力断层：即便在传统基准测试中表现优异的LLM，在建立语言与空间概念双向映射方面仍存在严重缺陷——这一根本局限削弱了其作为真实世界模型的潜力。此外，LTD-Bench的可视化输出支持深度诊断分析，为探究模型相似性提供了潜在路径。",
    "url": "https://huggingface.co/papers/2511.02347",
    "arxiv_url": "https://arxiv.org/abs/2511.02347"
  },
  {
    "title": "Shorter but not Worse: Frugal Reasoning via Easy Samples as Length\n  Regularizers in Math RLVR",
    "summary": "Large language models (LLMs) trained for step-by-step reasoning often become\nexcessively verbose, raising inference cost. Standard Reinforcement Learning\nwith Verifiable Rewards (RLVR) pipelines filter out ``easy'' problems for\ntraining efficiency, leaving the model to train primarily on harder problems\nthat require longer reasoning chains. This skews the output length distribution\nupward, resulting in a model that conflates ``thinking longer'' with\n``thinking better''. In this work, we show that retaining and modestly\nup-weighting moderately easy problems acts as an implicit length regularizer.\nExposing the model to solvable short-chain tasks constrains its output\ndistribution and prevents runaway verbosity. The result is\n\\emph{emergent brevity for free}: the model learns to solve harder\nproblems without inflating the output length,  despite the absence of\nany explicit length penalization. RLVR experiments using this approach on\nQwen3-4B-Thinking-2507 (with a 16k token limit) achieve baseline\npass@1 AIME25 accuracy while generating solutions that are, on average, nearly\ntwice as short. The code is available at\nhttps://github.com/MBZUAI-Paris/Frugal-AI{GitHub}, with datasets and\nmodels on\nhttps://huggingface.co/collections/MBZUAI-Paris/k2-think-mini-68dcfa8b114686a4bd3dc2bc{Hugging\nFace}.",
    "translation": "标题：更短而非更差：基于简易样本的节俭推理作为数学RLVR中的长度正则器\n\n摘要：针对逐步推理训练的大语言模型常产生过度冗长的输出，导致推理成本攀升。传统的可验证奖励强化学习流程为提升训练效率会过滤\"简易\"问题，使模型主要训练于需要更长推理链的难题。这种处理导致输出长度分布向上偏移，使模型混淆\"更长思考\"与\"更好思考\"的关系。本研究证明，保留并适度加权中等难度问题可形成隐式长度正则器。让模型接触可解决的短链任务能约束其输出分布，防止冗长失控。由此实现无需代价的涌现性简洁：尽管未采用任何显式长度惩罚，模型仍能学会解决复杂问题而不增加输出长度。在Qwen3-4B-Thinking-2507模型上进行的RLVR实验表明，该方法在保持基准AIME25准确率的同时，生成解决方案平均缩短近50%。代码发布于https://github.com/MBZUAI-Paris/Frugal-AI，数据集与模型详见https://huggingface.co/collections/MBZUAI-Paris/k2-think-mini-68dcfa8b114686a4bd3dc2bc。",
    "url": "https://huggingface.co/papers/2511.01937",
    "arxiv_url": "https://arxiv.org/abs/2511.01937"
  },
  {
    "title": "CodeClash: Benchmarking Goal-Oriented Software Engineering",
    "summary": "Current benchmarks for coding evaluate language models (LMs) on concrete,\nwell-specified tasks such as fixing specific bugs or writing targeted tests.\nHowever, human programmers do not spend all day incessantly addressing isolated\ntasks. Instead, real-world software development is grounded in the pursuit of\nhigh-level goals, like improving user retention or reducing costs. Evaluating\nwhether LMs can also iteratively develop code to better accomplish open-ended\nobjectives without any explicit guidance remains an open challenge. To address\nthis, we introduce CodeClash, a benchmark where LMs compete in multi-round\ntournaments to build the best codebase for achieving a competitive objective.\nEach round proceeds in two phases: agents edit their code, then their codebases\ncompete head-to-head in a code arena that determines winners based on\nobjectives like score maximization, resource acquisition, or survival. Whether\nit's writing notes, scrutinizing documentation, analyzing competition logs, or\ncreating test suites, models must decide for themselves how to improve their\ncodebases both absolutely and against their opponents. We run 1680 tournaments\n(25,200 rounds total) to evaluate 8 LMs across 6 arenas. Our results reveal\nthat while models exhibit diverse development styles, they share fundamental\nlimitations in strategic reasoning. Models also struggle with long-term\ncodebase maintenance, as repositories become progressively messy and redundant.\nThese limitations are stark: top models lose every round against expert human\nprogrammers. We open-source CodeClash to advance the study of autonomous,\ngoal-oriented code development.",
    "translation": "标题：CodeClash：面向目标的软件工程基准测试框架\n\n摘要：现有代码生成基准测试主要针对具体明确的任务评估语言模型，例如修复特定错误或编写目标测试用例。然而人类程序员并非终日处理孤立任务，实际软件开发始终围绕高层目标展开，如提升用户留存率或降低运营成本。如何评估语言模型在无明确指导的情况下，通过迭代开发逐步实现开放式目标仍属前沿挑战。为此我们提出CodeClash基准测试框架，通过多轮锦标赛形式使语言模型围绕竞争性目标展开代码库优化竞赛。每轮比赛包含两个阶段：智能体编辑代码，随后在代码竞技场中进行对抗性评估，根据得分最大化、资源获取或生存时长等目标判定胜负。无论是编写注释、研读文档、分析对战记录还是创建测试套件，模型需要自主决策如何从绝对质量和相对优势两个维度优化代码库。我们通过1680场锦标赛（总计25200轮）对8个语言模型在6种竞技场景中进行评估。结果表明：虽然模型展现出多样化的开发风格，但在策略推理方面存在根本性局限。随着代码库逐渐变得混乱冗余，模型在长期维护方面也表现不佳。这些局限十分显著：顶尖模型在与人类专家程序员的对抗中全盘皆输。我们开源CodeClash框架以推动自主化、目标导向的代码开发研究。",
    "url": "https://huggingface.co/papers/2511.00839",
    "arxiv_url": "https://arxiv.org/abs/2511.00839"
  },
  {
    "title": "TWIST2: Scalable, Portable, and Holistic Humanoid Data Collection System",
    "summary": "Large-scale data has driven breakthroughs in robotics, from language models\nto vision-language-action models in bimanual manipulation. However, humanoid\nrobotics lacks equally effective data collection frameworks. Existing humanoid\nteleoperation systems either use decoupled control or depend on expensive\nmotion capture setups. We introduce TWIST2, a portable, mocap-free humanoid\nteleoperation and data collection system that preserves full whole-body control\nwhile advancing scalability. Our system leverages PICO4U VR for obtaining\nreal-time whole-body human motions, with a custom 2-DoF robot neck (cost around\n$250) for egocentric vision, enabling holistic human-to-humanoid control. We\ndemonstrate long-horizon dexterous and mobile humanoid skills and we can\ncollect 100 demonstrations in 15 minutes with an almost 100% success rate.\nBuilding on this pipeline, we propose a hierarchical visuomotor policy\nframework that autonomously controls the full humanoid body based on egocentric\nvision. Our visuomotor policy successfully demonstrates whole-body dexterous\nmanipulation and dynamic kicking tasks. The entire system is fully reproducible\nand open-sourced at https://yanjieze.com/TWIST2 . Our collected dataset is also\nopen-sourced at https://twist-data.github.io .",
    "translation": "标题：TWIST2：可扩展、便携式、整体化的人形机器人数据采集系统\n\n摘要：大规模数据推动了机器人技术领域的突破，从语言模型到双手操作中的视觉-语言-动作模型。然而，人形机器人领域目前缺乏同等高效的数据采集框架。现有的人形遥操作系统要么采用解耦控制方案，要么依赖昂贵的动作捕捉设备。我们提出TWIST2——一种无需动作捕捉的便携式人形遥操作与数据采集系统，在保持完整全身控制能力的同时实现了可扩展性突破。该系统基于PICO4U VR设备实时获取人体全身运动数据，通过自主研发的2自由度机器人颈部装置（成本约250美元）实现以自我为中心的视觉感知，最终达成从人类操作者到人形机器人的整体化控制。我们成功展示了机器人执行长时序精细操作与移动技能的能力，在15分钟内可采集100组演示数据且成功率接近100%。基于此技术框架，我们进一步提出分层视觉运动策略架构，能够基于机器人第一视角视觉自主控制完整人形躯体。该视觉运动策略已成功完成全身精细操作与动态踢球等任务。整套系统完全可复现并已在https://yanjieze.com/TWIST2 开源。我们采集的数据集也同步公开于https://twist-data.github.io 。",
    "url": "https://huggingface.co/papers/2511.02832",
    "arxiv_url": "https://arxiv.org/abs/2511.02832"
  },
  {
    "title": "iFlyBot-VLA Technical Report",
    "summary": "We introduce iFlyBot-VLA, a large-scale Vision-Language-Action (VLA) model\ntrained under a novel framework. The main contributions are listed as follows:\n(1) a latent action model thoroughly trained on large-scale human and robotic\nmanipulation videos; (2) a dual-level action representation framework that\njointly supervises both the Vision-Language Model (VLM) and the action expert\nduring training; (3) a mixed training strategy that combines robot trajectory\ndata with general QA and spatial QA datasets, effectively enhancing the 3D\nperceptual and reasoning capabilities of the VLM backbone. Specifically, the\nVLM is trained to predict two complementary forms of actions: latent actions,\nderived from our latent action model pretrained on cross-embodiment\nmanipulation data, which capture implicit high-level intentions; and structured\ndiscrete action tokens, obtained through frequency-domain transformations of\ncontinuous control signals, which encode explicit low-level dynamics. This dual\nsupervision aligns the representation spaces of language, vision, and action,\nenabling the VLM to directly contribute to action generation. Experimental\nresults on the LIBERO Franka benchmark demonstrate the superiority of our\nframe-work, while real-world evaluations further show that iFlyBot-VLA achieves\ncompetitive success rates across diverse and challenging manipulation tasks.\nFurthermore, we plan to open-source a portion of our self-constructed dataset\nto support future research in the community",
    "translation": "标题：iFlyBot-VLA技术报告\n\n摘要：本文提出iFlyBot-VLA——基于创新框架训练的大规模视觉-语言-动作模型。主要贡献包括：（1）基于大规模人类与机器人操作视频完整训练的潜在动作模型；（2）在训练过程中同时对视觉语言模型与动作专家进行联合监督的双层级动作表征框架；（3）融合机器人轨迹数据与通用问答、空间问答数据集的混合训练策略，有效增强视觉语言模型骨干网络的3D感知与推理能力。具体而言，该模型通过预测两种互补动作形式实现监督：其一是基于跨具身操作数据预训练的潜在动作模型所推导的潜在动作，用于捕捉隐式高层意图；其二是通过对连续控制信号进行频域转换获得的结构化离散动作标记，用于编码显式底层动态特征。这种双重监督机制实现了语言、视觉与动作表征空间的对齐，使视觉语言模型能直接参与动作生成。在LIBERO Franka基准测试中的实验结果表明我们框架的优越性，而真实环境评估进一步证明iFlyBot-VLA在多样复杂操作任务中均达到具有竞争力的成功率。此外，我们计划开源部分自建数据集以支持学界后续研究。",
    "url": "https://huggingface.co/papers/2511.01914",
    "arxiv_url": "https://arxiv.org/abs/2511.01914"
  },
  {
    "title": "BRAINS: A Retrieval-Augmented System for Alzheimer's Detection and\n  Monitoring",
    "summary": "As the global burden of Alzheimer's disease (AD) continues to grow, early and\naccurate detection has become increasingly critical, especially in regions with\nlimited access to advanced diagnostic tools. We propose BRAINS (Biomedical\nRetrieval-Augmented Intelligence for Neurodegeneration Screening) to address\nthis challenge. This novel system harnesses the powerful reasoning capabilities\nof Large Language Models (LLMs) for Alzheimer's detection and monitoring.\nBRAINS features a dual-module architecture: a cognitive diagnostic module and a\ncase-retrieval module. The Diagnostic Module utilizes LLMs fine-tuned on\ncognitive and neuroimaging datasets -- including MMSE, CDR scores, and brain\nvolume metrics -- to perform structured assessments of Alzheimer's risk.\nMeanwhile, the Case Retrieval Module encodes patient profiles into latent\nrepresentations and retrieves similar cases from a curated knowledge base.\nThese auxiliary cases are fused with the input profile via a Case Fusion Layer\nto enhance contextual understanding. The combined representation is then\nprocessed with clinical prompts for inference. Evaluations on real-world\ndatasets demonstrate BRAINS effectiveness in classifying disease severity and\nidentifying early signs of cognitive decline. This system not only shows strong\npotential as an assistive tool for scalable, explainable, and early-stage\nAlzheimer's disease detection, but also offers hope for future applications in\nthe field.",
    "translation": "标题：BRAINS：一种用于阿尔茨海默病检测与监测的检索增强系统\n\n摘要：随着阿尔茨海默病（AD）的全球负担持续加重，早期精准检测变得尤为关键，特别是在缺乏先进诊断工具的地区。我们提出BRAINS（神经退行性疾病筛查的生物医学检索增强智能系统）以应对这一挑战。该系统创新性地利用大语言模型（LLMs）的强大推理能力进行阿尔茨海默病的检测与监测。BRAINS采用双模块架构：认知诊断模块和案例检索模块。诊断模块运用经过认知与神经影像数据集（包括MMSE量表、CDR评分及脑容量指标）微调的LLMs，对阿尔茨海默病风险进行结构化评估；案例检索模块则将患者档案编码为潜在表征，并从经过筛选的知识库中检索相似病例。这些辅助案例通过案例融合层与输入档案进行整合，以增强上下文理解。最终结合临床提示对融合表征进行推理分析。真实世界数据集上的评估表明，BRAINS在疾病严重程度分类和认知衰退早期迹象识别方面具有显著效果。该系统不仅展现出作为可扩展、可解释的早期阿尔茨海默病检测辅助工具的强大潜力，更为该领域的未来应用提供了新的希望。",
    "url": "https://huggingface.co/papers/2511.02490",
    "arxiv_url": "https://arxiv.org/abs/2511.02490"
  },
  {
    "title": "ChartM^3: A Multi-Stage Code-Driven Pipeline for Constructing\n  Multi-Dimensional and Multi-Step Visual Reasoning Data in Chart Comprehension",
    "summary": "Complex chart understanding tasks demand advanced visual recognition and\nreasoning capabilities from multimodal large language models (MLLMs). However,\ncurrent research provides limited coverage of complex chart scenarios and\ncomputation-intensive reasoning tasks prevalent in real-world applications.\nThis study proposes an automated multi-stage code-driven pipeline for\nsystematically generating visual reasoning datasets to address these\nlimitations. The pipeline integrates retrieval-augmented generation (RAG) to\nretrieve professional chart templates and employs chain-of-thought (CoT)\nstrategies to generate reasoning codes that simulate real data distributions,\nthereby driving chart rendering and question-related statistical computations.\nThrough model-based evaluation, the pipeline enhances chart diversity and data\nquality. Using this framework, we construct ChartM^3, a multi-dimensional and\nmulti-step dataset containing 38K charts and 142K Q&A pairs for training, along\nwith 2,871 high-quality evaluation samples for enabling practical performance\nassessment. Supervised fine-tuning (SFT) and reinforcement learning (RL)\nexperiments demonstrate that our dataset significantly improves reasoning\ncapabilities and cross-domain generalization performance, enabling smaller\nmodels to achieve performance comparable to larger-scale models in complex\nchart comprehension.",
    "translation": "标题：ChartM^3：一种用于构建图表理解中多维多步视觉推理数据的多阶段代码驱动流程\n\n摘要：复杂图表理解任务要求多模态大语言模型具备先进的视觉识别与推理能力。然而当前研究对实际应用中普遍存在的复杂图表场景及计算密集型推理任务的覆盖范围有限。本研究提出一种自动化多阶段代码驱动流程，通过系统化生成视觉推理数据集以解决这些局限。该流程集成检索增强生成技术以获取专业图表模板，并采用思维链策略生成模拟真实数据分布的推理代码，从而驱动图表渲染及问题相关统计计算。通过基于模型的评估，该流程有效提升了图表的多样性与数据质量。基于此框架，我们构建了ChartM^3数据集——包含3.8万张图表和14.2万组问答对的训练数据，以及2,871个高质量评估样本，为实际性能评估提供支撑。监督微调与强化学习实验表明，我们的数据集显著提升了模型的推理能力与跨领域泛化性能，使较小规模模型在复杂图表理解任务中能达到与更大规模模型相媲美的性能表现。",
    "url": "https://huggingface.co/papers/2511.02415",
    "arxiv_url": "https://arxiv.org/abs/2511.02415"
  },
  {
    "title": "Forget BIT, It is All about TOKEN: Towards Semantic Information Theory\n  for LLMs",
    "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\nnumerous real-world applications. While the vast majority of research conducted\nfrom an experimental perspective is progressing rapidly, it demands substantial\ncomputational power, data, and other resources. Therefore, how to open the\nblack-box of LLMs from a theoretical standpoint has become a critical\nchallenge. This paper takes the theory of rate-distortion function, directed\ninformation, and Granger causality as its starting point to investigate the\ninformation-theoretic principles behind LLMs, leading to the development of\nsemantic information theory for LLMs, where the fundamental unit is token,\nrather than bits that lacks any semantic meaning. By defining the probabilistic\nmodel of LLMs, we discuss structure-agnostic information-theoretic measures,\nsuch as the directed rate-distortion function in pre-training, the directed\nrate-reward function in post-training, and the semantic information flow in\ninference phase. This paper also delves deeply into the theory of token-level\nsemantic embedding and the information-theoretically optimal vectorization\nmethod. Thereafter, we propose a general definition of autoregression LLM,\nwhere the Transformer architecture and its performance such as ELBO,\ngeneralization error bound, memory capacity, and semantic information measures\ncan be derived theoretically. Other architectures, such as Mamba/Mamba2 and\nLLaDA, are also discussed in our framework. Consequently, this paper provides a\ntheoretical framework for understanding LLMs from the perspective of semantic\ninformation theory, which also offers the necessary theoretical tools for\nfurther in-depth research.",
    "translation": "标题：告别比特，迎接标记：面向大语言模型的语义信息理论构建\n\n摘要：大语言模型在众多实际应用中展现出卓越能力。尽管基于实验视角的研究正快速推进，但其需要消耗大量算力、数据及其他资源。因此从理论视角破解大语言模型的黑箱机制已成为关键挑战。本文以率失真函数理论、定向信息与格兰杰因果理论为起点，探究大语言模型背后的信息论原理，进而构建以标记为基本单元的大语言模型语义信息理论——相较于缺乏语义内涵的比特，标记更具本质意义。通过定义大语言模型的概率模型，我们探讨了结构无关的信息论度量方法，包括预训练阶段的定向率失真函数、后训练阶段的定向率奖励函数以及推理阶段的语义信息流。本文还深入研究了标记级语义嵌入理论及信息论最优向量化方法，进而提出自回归大语言模型的通用定义，从理论上推导出Transformer架构及其相关性能指标（如证据下界、泛化误差界、记忆容量和语义信息度量）。Mamba/Mamba2和LLaDA等其他架构也在本框架中得到讨论。最终，本文构建了从语义信息理论视角理解大语言模型的理论框架，为后续深入研究提供了必要的理论工具。",
    "url": "https://huggingface.co/papers/2511.01202",
    "arxiv_url": "https://arxiv.org/abs/2511.01202"
  },
  {
    "title": "RoboChallenge: Large-scale Real-robot Evaluation of Embodied Policies",
    "summary": "Testing on real machines is indispensable for robotic control algorithms. In\nthe context of learning-based algorithms, especially VLA models, demand for\nlarge-scale evaluation, i.e. testing a large number of models on a large number\nof tasks, is becoming increasingly urgent. However, doing this right is highly\nnon-trivial, especially when scalability and reproducibility is taken into\naccount. In this report, we describe our methodology for constructing\nRoboChallenge, an online evaluation system to test robotic control algorithms,\nand our survey of recent state-of-the-art VLA models using our initial\nbenchmark Table30.",
    "translation": "标题：RoboChallenge：具身策略的大规模实机评估体系  \n\n摘要：实机测试对机器人控制算法至关重要。针对基于学习的算法（尤其是视觉语言动作模型），开展大规模评估——即在大量任务中测试大量模型——的需求日益迫切。然而，实现可扩展且可复现的高质量评估仍面临巨大挑战。本报告阐述了构建RoboChallenge在线评估系统的方法论，该系统用于测试机器人控制算法，并基于初始基准Table30对当前最先进的视觉语言动作模型进行了系统性评估。",
    "url": "https://huggingface.co/papers/2510.17950",
    "arxiv_url": "https://arxiv.org/abs/2510.17950"
  },
  {
    "title": "VidEmo: Affective-Tree Reasoning for Emotion-Centric Video Foundation\n  Models",
    "summary": "Understanding and predicting emotion from videos has gathered significant\nattention in recent studies, driven by advancements in video large language\nmodels (VideoLLMs). While advanced methods have made progress in video emotion\nanalysis, the intrinsic nature of emotions poses significant challenges.\nEmotions are characterized by dynamic and cues-dependent properties, making it\ndifficult to understand complex and evolving emotional states with reasonable\nrationale. To tackle these challenges, we propose a novel affective cues-guided\nreasoning framework that unifies fundamental attribute perception, expression\nanalysis, and high-level emotional understanding in a stage-wise manner. At the\ncore of our approach is a family of video emotion foundation models (VidEmo),\nspecifically designed for emotion reasoning and instruction-following. These\nmodels undergo a two-stage tuning process: first, curriculum emotion learning\nfor injecting emotion knowledge, followed by affective-tree reinforcement\nlearning for emotion reasoning. Moreover, we establish a foundational data\ninfrastructure and introduce a emotion-centric fine-grained dataset (Emo-CFG)\nconsisting of 2.1M diverse instruction-based samples. Emo-CFG includes\nexplainable emotional question-answering, fine-grained captions, and associated\nrationales, providing essential resources for advancing emotion understanding\ntasks. Experimental results demonstrate that our approach achieves competitive\nperformance, setting a new milestone across 15 face perception tasks.",
    "translation": "标题：VidEmo：面向情感中心化视频基础模型的情感树推理框架\n\n摘要：随着视频大语言模型的快速发展，视频情感理解与预测在近期研究中受到广泛关注。尽管现有方法在视频情感分析方面取得进展，但情感固有的动态性和线索依赖性特质仍带来重大挑战——难以通过合理推理解读复杂且持续演变的情感状态。为解决这些问题，我们提出了一种新颖的情感线索引导推理框架，以分阶段方式统一基础属性感知、表情分析和高级情感理解。该方案的核心是专为情感推理与指令跟随设计的视频情感基础模型系列（VidEmo），其经过两阶段调优：首先通过课程式情感学习注入情感知识，随后采用情感树强化学习进行情感推理。此外，我们构建了基础数据基础设施，推出了包含210万条多样化指令样本的情感中心化细粒度数据集（Emo-CFG）。该数据集涵盖可解释的情感问答、细粒度描述及相关推理依据，为推进情感理解任务提供了关键资源。实验结果表明，我们的方法在15项面部感知任务中展现出卓越性能，树立了新的技术里程碑。",
    "url": "https://huggingface.co/papers/2511.02712",
    "arxiv_url": "https://arxiv.org/abs/2511.02712"
  },
  {
    "title": "AyurParam: A State-of-the-Art Bilingual Language Model for Ayurveda",
    "summary": "Current large language models excel at broad, general-purpose tasks, but\nconsistently underperform when exposed to highly specialized domains that\nrequire deep cultural, linguistic, and subject-matter expertise. In particular,\ntraditional medical systems such as Ayurveda embody centuries of nuanced\ntextual and clinical knowledge that mainstream LLMs fail to accurately\ninterpret or apply. We introduce AyurParam-2.9B, a domain-specialized,\nbilingual language model fine-tuned from Param-1-2.9B using an extensive,\nexpertly curated Ayurveda dataset spanning classical texts and clinical\nguidance. AyurParam's dataset incorporates context-aware, reasoning, and\nobjective-style Q&A in both English and Hindi, with rigorous annotation\nprotocols for factual precision and instructional clarity. Benchmarked on\nBhashaBench-Ayur, AyurParam not only surpasses all open-source\ninstruction-tuned models in its size class (1.5--3B parameters), but also\ndemonstrates competitive or superior performance compared to much larger\nmodels. The results from AyurParam highlight the necessity for authentic domain\nadaptation and high-quality supervision in delivering reliable, culturally\ncongruent AI for specialized medical knowledge.",
    "translation": "标题：AyurParam：面向阿育吠陀医学的尖端双语语言模型\n\n摘要：当前主流大语言模型虽在通用领域表现出色，但当涉及需要深厚文化背景、语言知识及专业领域经验的垂直学科时，其表现往往不尽如人意。以阿育吠陀医学体系为例，这一蕴含数百年精妙文献与临床智慧的传统医学系统，现有模型难以实现准确解析与应用。本文推出AyurParam-2.9B模型——基于Param-1-2.9B架构进行领域专业化微调的双语模型，其训练数据涵盖经专家系统化整理的阿育吠陀经典著作与临床指南。该数据集融合语境感知、逻辑推理及客观题型问答机制，包含英语与印地语双语的精准标注体系，确保事实准确性与指导明晰性。在BhashaBench-Ayur基准测试中，AyurParam不仅显著超越同参数规模（1.5-30亿）的所有开源指令微调模型，更在多项指标上媲美或超越参数量更大的模型。本研究实证表明，要实现专业医学领域可靠且文化适配的人工智能系统，必须进行深度的领域自适应优化与高质量监督训练。",
    "url": "https://huggingface.co/papers/2511.02374",
    "arxiv_url": "https://arxiv.org/abs/2511.02374"
  },
  {
    "title": "LiveSecBench: A Dynamic and Culturally-Relevant AI Safety Benchmark for\n  LLMs in Chinese Context",
    "summary": "In this work, we propose LiveSecBench, a dynamic and continuously updated\nsafety benchmark specifically for Chinese-language LLM application scenarios.\nLiveSecBench evaluates models across six critical dimensions (Legality, Ethics,\nFactuality, Privacy, Adversarial Robustness, and Reasoning Safety) rooted in\nthe Chinese legal and social frameworks. This benchmark maintains relevance\nthrough a dynamic update schedule that incorporates new threat vectors, such as\nthe planned inclusion of Text-to-Image Generation Safety and Agentic Safety in\nthe next update. For now, LiveSecBench (v251030) has evaluated 18 LLMs,\nproviding a landscape of AI safety in the context of Chinese language. The\nleaderboard is publicly accessible at https://livesecbench.intokentech.cn/.",
    "translation": "标题：LiveSecBench：面向中文语境大语言模型的动态文化适配型AI安全基准\n\n摘要：本研究提出LiveSecBench——一个专为中文大语言模型应用场景设计的动态持续更新的安全基准。该基准立足中国法律与社会框架，从合法性、伦理道德、事实准确性、隐私保护、对抗鲁棒性和推理安全性六个核心维度对模型进行全面评估。通过动态更新机制，本基准持续纳入新型威胁向量（如下次更新计划增加的文生图安全性与智能体安全性），保持评估体系的时效性。当前版本LiveSecBench（v251030）已完成对18个大语言模型的评估，清晰呈现了中文语境下AI安全的发展态势。评估排行榜可通过 https://livesecbench.intokentech.cn/ 公开访问。",
    "url": "https://huggingface.co/papers/2511.02366",
    "arxiv_url": "https://arxiv.org/abs/2511.02366"
  },
  {
    "title": "TabDSR: Decompose, Sanitize, and Reason for Complex Numerical Reasoning\n  in Tabular Data",
    "summary": "Complex reasoning over tabular data is crucial in real-world data analysis,\nyet large language models (LLMs) often underperform due to complex queries,\nnoisy data, and limited numerical capabilities. To address these issues, we\npropose \\method, a framework consisting of: (1) a query decomposer that breaks\ndown complex questions, (2) a table sanitizer that cleans and filters noisy\ntables, and (3) a program-of-thoughts (PoT)-based reasoner that generates\nexecutable code to derive the final answer from the sanitized table. To ensure\nunbiased evaluation and mitigate data leakage, we introduce a new dataset,\nCalTab151, specifically designed for complex numerical reasoning over tables.\nExperimental results demonstrate that \\method consistently outperforms existing\nmethods, achieving state-of-the-art (SOTA) performance with 8.79%, 6.08%, and\n19.87% accuracy improvement on TAT-QA, TableBench, and \\method, respectively.\nMoreover, our framework integrates seamlessly with mainstream LLMs, providing a\nrobust solution for complex tabular numerical reasoning. These findings\nhighlight the effectiveness of our framework in enhancing LLM performance for\ncomplex tabular numerical reasoning. Data and code are available upon request.",
    "translation": "标题：TabDSR：面向表格数据复杂数值推理的分解、净化与推演框架\n\n摘要：表格数据的复杂推理在现实数据分析中至关重要，然而大型语言模型因复杂查询、噪声数据和有限数值能力往往表现不佳。为解决这些问题，我们提出TabDSR框架，其包含三个核心组件：（1）查询分解器：将复杂问题拆解为子问题；（2）表格净化器：对含噪表格进行清洗过滤；（3）基于程序化思维（PoT）的推理器：通过生成可执行代码从净化表中推导最终答案。为确保无偏评估并防止数据泄露，我们专门构建了CalTab151数据集，针对表格复杂数值推理任务设计。实验结果表明，本方法持续超越现有技术，在TAT-QA、TableBench及本数据集上分别实现8.79%、6.08%和19.87%的准确率提升，达到最先进水平。该框架可与主流大语言模型无缝集成，为复杂表格数值推理提供稳健解决方案。这些发现凸显了本框架在提升大语言模型表格数值推理性能方面的有效性。数据与代码可根据需求提供。",
    "url": "https://huggingface.co/papers/2511.02219",
    "arxiv_url": "https://arxiv.org/abs/2511.02219"
  },
  {
    "title": "Reg-DPO: SFT-Regularized Direct Preference Optimization with GT-Pair for\n  Improving Video Generation",
    "summary": "Recent studies have identified Direct Preference Optimization (DPO) as an\nefficient and reward-free approach to improving video generation quality.\nHowever, existing methods largely follow image-domain paradigms and are mainly\ndeveloped on small-scale models (approximately 2B parameters), limiting their\nability to address the unique challenges of video tasks, such as costly data\nconstruction, unstable training, and heavy memory consumption. To overcome\nthese limitations, we introduce a GT-Pair that automatically builds\nhigh-quality preference pairs by using real videos as positives and\nmodel-generated videos as negatives, eliminating the need for any external\nannotation. We further present Reg-DPO, which incorporates the SFT loss as a\nregularization term into the DPO objective to enhance training stability and\ngeneration fidelity. Additionally, by combining the FSDP framework with\nmultiple memory optimization techniques, our approach achieves nearly three\ntimes higher training capacity than using FSDP alone. Extensive experiments on\nboth I2V and T2V tasks across multiple datasets demonstrate that our method\nconsistently outperforms existing approaches, delivering superior video\ngeneration quality.",
    "translation": "标题：Reg-DPO：基于GT-Pair的SFT正则化直接偏好优化方法及其在视频生成质量提升中的应用\n\n摘要：近期研究证实直接偏好优化（DPO）是一种无需奖励机制即可有效提升视频生成质量的方法。然而现有方法主要沿袭图像领域范式，且多基于小规模模型（约20亿参数）开发，难以应对视频任务特有的三大挑战：高昂的数据构建成本、训练过程不稳定及显存消耗过大。为此，我们提出GT-Pair方法，通过将真实视频作为正样本、模型生成视频作为负样本，自动构建高质量偏好对，无需任何外部标注。进一步提出Reg-DPO方法，将监督微调（SFT）损失作为正则化项融入DPO目标函数，有效增强训练稳定性与生成保真度。通过将完全分片数据并行（FSDP）框架与多重显存优化技术相结合，我们的方法相比单独使用FSDP实现了近三倍的训练容量提升。在多个数据集上进行的图像到视频与文本到视频任务实验表明，本方法持续优于现有方案，可生成更高质量的视频内容。",
    "url": "https://huggingface.co/papers/2511.01450",
    "arxiv_url": "https://arxiv.org/abs/2511.01450"
  },
  {
    "title": "RiddleBench: A New Generative Reasoning Benchmark for LLMs",
    "summary": "Large Language Models have demonstrated strong performance on many\nestablished reasoning benchmarks. However, these benchmarks primarily evaluate\nstructured skills like quantitative problem-solving, leaving a gap in assessing\nflexible, multifaceted reasoning abilities that are central to human\nintelligence. These abilities require integrating logical deduction with\nspatial awareness and constraint satisfaction, which current evaluations do not\nmeasure well. To address this, we introduce RiddleBench, a benchmark of 1,737\nchallenging puzzles in English designed to probe these core reasoning\ncapabilities. Evaluation of state-of-the-art models on RiddleBench shows\nfundamental weaknesses. Even top proprietary models like Gemini 2.5 Pro, o3,\nand Claude 4 Sonnet achieve accuracy just above 60% (60.30%, 63.37%, and\n63.16%). Analysis further reveals deep failures, including hallucination\ncascades (accepting flawed reasoning from other models) and poor\nself-correction due to a strong self-confirmation bias. Their reasoning is also\nfragile, with performance degrading significantly when constraints are\nreordered or irrelevant information is introduced. RiddleBench functions as a\ndiagnostic tool for these issues and as a resource for guiding the development\nof more robust and reliable language models.",
    "translation": "标题：RiddleBench：面向大语言模型的全新生成式推理基准测试\n\n摘要：大语言模型在现有推理基准测试中展现出强劲性能，但这些基准主要评估定量问题求解等结构化技能，对衡量人类智能核心的灵活性多维度推理能力存在明显不足。此类能力要求融合逻辑推理、空间感知与约束满足，而当前评估体系尚难有效捕捉。为此，我们推出RiddleBench——一个包含1,737道英语谜题的基准测试集，专门用于探究这些核心推理能力。在RiddleBench上对前沿模型的评估揭示了根本性缺陷：即使顶尖专有模型如Gemini 2.5 Pro、o3和Claude 4 Sonnet的准确率也仅略超60%（分别为60.30%、63.37%和63.16%）。深度分析进一步暴露出严重问题，包括幻觉级联（盲目接受其他模型的错误推理）以及因强烈自我确认偏见导致的纠错能力薄弱。这些模型的推理过程亦显脆弱，当约束条件重排或引入无关信息时，性能会出现显著下滑。RiddleBench既可作为诊断这些问题的检测工具，也能为开发更稳健可靠的语言模型提供指引资源。",
    "url": "https://huggingface.co/papers/2510.24932",
    "arxiv_url": "https://arxiv.org/abs/2510.24932"
  },
  {
    "title": "D2D: Detector-to-Differentiable Critic for Improved Numeracy in\n  Text-to-Image Generation",
    "summary": "Text-to-image (T2I) diffusion models have achieved strong performance in\nsemantic alignment, yet they still struggle with generating the correct number\nof objects specified in prompts. Existing approaches typically incorporate\nauxiliary counting networks as external critics to enhance numeracy. However,\nsince these critics must provide gradient guidance during generation, they are\nrestricted to regression-based models that are inherently differentiable, thus\nexcluding detector-based models with superior counting ability, whose\ncount-via-enumeration nature is non-differentiable. To overcome this\nlimitation, we propose Detector-to-Differentiable (D2D), a novel framework that\ntransforms non-differentiable detection models into differentiable critics,\nthereby leveraging their superior counting ability to guide numeracy\ngeneration. Specifically, we design custom activation functions to convert\ndetector logits into soft binary indicators, which are then used to optimize\nthe noise prior at inference time with pre-trained T2I models. Our extensive\nexperiments on SDXL-Turbo, SD-Turbo, and Pixart-DMD across four benchmarks of\nvarying complexity (low-density, high-density, and multi-object scenarios)\ndemonstrate consistent and substantial improvements in object counting accuracy\n(e.g., boosting up to 13.7% on D2D-Small, a 400-prompt, low-density benchmark),\nwith minimal degradation in overall image quality and computational overhead.",
    "translation": "标题：D2D：基于检测器到可微分判别器的文本到图像生成数值能力提升方法\n\n摘要：文本到图像扩散模型在语义对齐方面已取得显著成效，但在生成符合提示词中指定数量对象的能力上仍存在不足。现有方法通常引入辅助计数网络作为外部判别器来增强数值能力。然而由于这些判别器需在生成过程中提供梯度指导，其必须采用本质可微分的回归模型，从而排除了具有更强计数能力但基于枚举计数机制而不可微分的检测器模型。为突破此限制，我们提出检测器到可微分框架，通过将不可微分的检测模型转化为可微分判别器，有效利用其卓越的计数能力来指导数值生成。具体而言，我们设计了定制激活函数将检测器逻辑值转换为软二元指示器，进而结合预训练文本到图像模型在推理阶段优化噪声先验。通过在SDXL-Turbo、SD-Turbo和Pixart-DMD模型上的大量实验，覆盖四个不同复杂度的基准测试（包括低密度、高密度及多对象场景），结果表明该方法在物体计数准确率上实现了持续显著提升（例如在包含400条提示词的低密度基准D2D-Small上最高提升13.7%），同时图像整体质量与计算开销仅受轻微影响。",
    "url": "https://huggingface.co/papers/2510.19278",
    "arxiv_url": "https://arxiv.org/abs/2510.19278"
  },
  {
    "title": "Discriminately Treating Motion Components Evolves Joint Depth and\n  Ego-Motion Learning",
    "summary": "Unsupervised learning of depth and ego-motion, two fundamental 3D perception\ntasks, has made significant strides in recent years. However, most methods\ntreat ego-motion as an auxiliary task, either mixing all motion types or\nexcluding depth-independent rotational motions in supervision. Such designs\nlimit the incorporation of strong geometric constraints, reducing reliability\nand robustness under diverse conditions. This study introduces a discriminative\ntreatment of motion components, leveraging the geometric regularities of their\nrespective rigid flows to benefit both depth and ego-motion estimation. Given\nconsecutive video frames, network outputs first align the optical axes and\nimaging planes of the source and target cameras. Optical flows between frames\nare transformed through these alignments, and deviations are quantified to\nimpose geometric constraints individually on each ego-motion component,\nenabling more targeted refinement. These alignments further reformulate the\njoint learning process into coaxial and coplanar forms, where depth and each\ntranslation component can be mutually derived through closed-form geometric\nrelationships, introducing complementary constraints that improve depth\nrobustness. DiMoDE, a general depth and ego-motion joint learning framework\nincorporating these designs, achieves state-of-the-art performance on multiple\npublic datasets and a newly collected diverse real-world dataset, particularly\nunder challenging conditions. Our source code will be publicly available at\nmias.group/DiMoDE upon publication.",
    "translation": "标题：区分性处理运动分量推动深度与自运动联合学习演进\n\n摘要：深度与自运动这两个基础三维感知任务的非监督学习近年来取得显著进展。然而现有方法大多将自运动视为辅助任务，要么混合所有运动类型，要么在监督中排除与深度无关的旋转运动。此类设计限制了强几何约束的引入，降低了算法在多样化条件下的可靠性与鲁棒性。本研究提出对运动分量进行区分性处理，利用各自刚性光流的几何规律性来提升深度与自运动估计性能。给定连续视频帧，网络输出首先对齐源相机与目标相机的光轴和成像平面。通过该对齐关系转换帧间光流，并量化偏差以分别对每个自运动分量施加几何约束，从而实现更具针对性的优化。这些对齐操作进一步将联合学习过程重构为共轴与共面形式，其中深度与各平移分量可通过闭式几何关系相互推导，引入的互补约束有效提升了深度鲁棒性。集成这些设计的通用深度与自运动联合学习框架DiMoDE，在多个公开数据集及新采集的多样化真实场景数据集上实现了最优性能，尤其在挑战性条件下表现突出。我们的源代码将在论文发表后公开于mias.group/DiMoDE。",
    "url": "https://huggingface.co/papers/2511.01502",
    "arxiv_url": "https://arxiv.org/abs/2511.01502"
  }
]