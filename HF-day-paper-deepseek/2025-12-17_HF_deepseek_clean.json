[
  {
    "title": "MMGR: Multi-Modal Generative Reasoning",
    "summary": "Video foundation models generate visually realistic and temporally coherent content, but their reliability as world simulators depends on whether they capture physical, logical, and spatial constraints. Existing metrics such as Frechet Video Distance (FVD) emphasize perceptual quality and overlook reasoning failures, including violations of causality, physics, and global consistency. We introduce MMGR (Multi-Modal Generative Reasoning Evaluation and Benchmark), a principled evaluation framework based on five reasoning abilities: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal. MMGR evaluates generative reasoning across three domains: Abstract Reasoning (ARC-AGI, Sudoku), Embodied Navigation (real-world 3D navigation and localization), and Physical Commonsense (sports and compositional interactions). MMGR applies fine-grained metrics that require holistic correctness across both video and image generation. We benchmark leading video models (Veo-3, Sora-2, Wan-2.2) and image models (Nano-banana, Nano-banana Pro, GPT-4o-image, Qwen-image), revealing strong performance gaps across domains. Models show moderate success on Physical Commonsense tasks but perform poorly on Abstract Reasoning (below 10 percent accuracy on ARC-AGI) and struggle with long-horizon spatial planning in embodied settings. Our analysis highlights key limitations in current models, including overreliance on perceptual data, weak global state consistency, and objectives that reward visual plausibility over causal correctness. MMGR offers a unified diagnostic benchmark and a path toward reasoning-aware generative world models.",
    "translation": "标题：MMGR：多模态生成式推理评估框架\n\n摘要：视频基础模型能够生成视觉逼真且时序连贯的内容，但其作为世界模拟器的可靠性取决于其是否能够捕捉物理、逻辑与空间约束。现有评估指标（如弗雷歇视频距离）侧重于感知质量，却忽视了推理层面的缺陷，包括对因果性、物理规律及全局一致性的违背。本文提出MMGR（多模态生成式推理评估基准），这是一个基于五大推理能力构建的原则性评估框架：物理推理、逻辑推理、三维空间推理、二维空间推理与时序推理。MMGR在三大领域对生成式推理进行系统评估：抽象推理（ARC-AGI、数独任务）、具身导航（真实世界三维导航与定位）以及物理常识（运动场景与组合交互）。该框架采用细粒度评估指标，要求视频与图像生成在整体上均保持正确性。通过对主流视频模型（Veo-3、Sora-2、Wan-2.2）和图像模型（Nano-banana、Nano-banana Pro、GPT-4o-image、Qwen-image）进行基准测试，本研究揭示了各模型在不同领域存在的显著性能差距。实验表明，模型在物理常识任务上表现尚可，但在抽象推理领域表现欠佳（ARC-AGI任务准确率低于10%），且在具身环境中的长程空间规划任务上存在明显困难。分析进一步指出当前模型的核心局限：过度依赖感知数据、全局状态一致性薄弱，以及优化目标偏向视觉合理性而忽视因果正确性。MMGR为生成式世界模型提供了一个统一的诊断性评估基准，并指明了构建具备推理意识的生成模型的发展路径。",
    "url": "https://huggingface.co/papers/2512.14691",
    "arxiv_url": "https://arxiv.org/abs/2512.14691"
  },
  {
    "title": "Video Reality Test: Can AI-Generated ASMR Videos fool VLMs and Humans?",
    "summary": "Recent advances in video generation have produced vivid content that are often indistinguishable from real videos, making AI-generated video detection an emerging societal challenge. Prior AIGC detection benchmarks mostly evaluate video without audio, target broad narrative domains, and focus on classification solely. Yet it remains unclear whether state-of-the-art video generation models can produce immersive, audio-paired videos that reliably deceive humans and VLMs. To this end, we introduce Video Reality Test, an ASMR-sourced video benchmark suite for testing perceptual realism under tight audio-visual coupling, featuring the following dimensions: (i) Immersive ASMR video-audio sources. Built on carefully curated real ASMR videos, the benchmark targets fine-grained action-object interactions with diversity across objects, actions, and backgrounds. (ii) Peer-Review evaluation. An adversarial creator-reviewer protocol where video generation models act as creators aiming to fool reviewers, while VLMs serve as reviewers seeking to identify fakeness. Our experimental findings show: The best creator Veo3.1-Fast even fools most VLMs: the strongest reviewer (Gemini 2.5-Pro) achieves only 56\\% accuracy (random 50\\%), far below that of human experts (81.25\\%). Adding audio improves real-fake discrimination, yet superficial cues such as watermarks can still significantly mislead models. These findings delineate the current boundary of video generation realism and expose limitations of VLMs in perceptual fidelity and audio-visual consistency. Our code is available at https://github.com/video-reality-test/video-reality-test.",
    "translation": "标题：视频真实性测试：AI生成的ASMR视频能否欺骗视觉语言模型与人类？\n\n摘要：近期视频生成技术的进步已能产出与真实视频难以区分的生动内容，使得AI生成视频检测成为新兴的社会挑战。现有AIGC检测基准大多针对无音频视频、面向宽泛叙事领域，且仅聚焦于分类任务。然而，当前最先进的视频生成模型能否产出具有沉浸感、音画同步且足以可靠欺骗人类和视觉语言模型（VLMs）的视频，仍不明确。为此，我们提出“视频真实性测试”——一个基于ASMR音视频源的基准测试套件，用于在紧密音画耦合条件下检验感知真实性，其特点包括：（一）沉浸式ASMR音视频源。基于精心筛选的真实ASMR视频构建，该基准针对细粒度的动作-物体交互，涵盖多样化的物体、动作与背景。（二）同行评审式评估。采用对抗性创作者-评审者协议：视频生成模型作为试图欺骗评审者的创作者，而VLMs则作为旨在识别伪造内容的评审者。实验结果表明：最佳创作者模型Veo3.1-Fast甚至能欺骗大多数VLMs——最强评审模型（Gemini 2.5-Pro）仅达到56%的准确率（随机基准为50%），远低于人类专家水平（81.25%）。音频的加入提升了真假判别能力，但水印等表面线索仍会显著误导模型。这些发现划定了当前视频生成真实性的边界，并揭示了VLMs在感知保真度与音画一致性方面的局限。代码已开源：https://github.com/video-reality-test/video-reality-test。",
    "url": "https://huggingface.co/papers/2512.13281",
    "arxiv_url": "https://arxiv.org/abs/2512.13281"
  },
  {
    "title": "WorldPlay: Towards Long-Term Geometric Consistency for Real-Time Interactive World Modeling",
    "summary": "This paper presents WorldPlay, a streaming video diffusion model that enables real-time, interactive world modeling with long-term geometric consistency, resolving the trade-off between speed and memory that limits current methods. WorldPlay draws power from three key innovations. 1) We use a Dual Action Representation to enable robust action control in response to the user's keyboard and mouse inputs. 2) To enforce long-term consistency, our Reconstituted Context Memory dynamically rebuilds context from past frames and uses temporal reframing to keep geometrically important but long-past frames accessible, effectively alleviating memory attenuation. 3) We also propose Context Forcing, a novel distillation method designed for memory-aware model. Aligning memory context between the teacher and student preserves the student's capacity to use long-range information, enabling real-time speeds while preventing error drift. Taken together, WorldPlay generates long-horizon streaming 720p video at 24 FPS with superior consistency, comparing favorably with existing techniques and showing strong generalization across diverse scenes. Project page and online demo can be found: https://3d-models.hunyuan.tencent.com/world/ and https://3d.hunyuan.tencent.com/sceneTo3D.",
    "translation": "标题：WorldPlay：面向实时交互式世界建模的长时几何一致性方法\n\n摘要：本文提出WorldPlay，一种流式视频扩散模型，能够实现具有长时几何一致性的实时交互式世界建模，解决了当前方法在速度与内存之间的权衡限制。WorldPlay的创新性主要体现在三个方面：1）采用双重动作表征机制，实现对用户键盘与鼠标输入的鲁棒动作控制；2）为实现长时一致性，提出重构上下文记忆模块，动态重建历史帧的上下文信息，并通过时序重构技术保持几何重要性高但时间久远的帧的可访问性，有效缓解记忆衰减问题；3）提出面向记忆感知模型的上下文强制蒸馏方法，通过对齐师生模型间的记忆上下文，保持学生模型利用长程信息的能力，在实现实时生成速度的同时避免误差漂移。综合以上技术，WorldPlay能够以24帧/秒的速率生成720p长序列流式视频，在保持卓越一致性的同时优于现有技术，并在多样场景中展现出强大的泛化能力。项目页面与在线演示详见：https://3d-models.hunyuan.tencent.com/world/ 与 https://3d.hunyuan.tencent.com/sceneTo3D。",
    "url": "https://huggingface.co/papers/2512.14614",
    "arxiv_url": "https://arxiv.org/abs/2512.14614"
  },
  {
    "title": "Scone: Bridging Composition and Distinction in Subject-Driven Image Generation via Unified Understanding-Generation Modeling",
    "summary": "Subject-driven image generation has advanced from single- to multi-subject composition, while neglecting distinction, the ability to identify and generate the correct subject when inputs contain multiple candidates. This limitation restricts effectiveness in complex, realistic visual settings. We propose Scone, a unified understanding-generation method that integrates composition and distinction. Scone enables the understanding expert to act as a semantic bridge, conveying semantic information and guiding the generation expert to preserve subject identity while minimizing interference. A two-stage training scheme first learns composition, then enhances distinction through semantic alignment and attention-based masking. We also introduce SconeEval, a benchmark for evaluating both composition and distinction across diverse scenarios. Experiments demonstrate that Scone outperforms existing open-source models in composition and distinction tasks on two benchmarks. Our model, benchmark, and training data are available at: https://github.com/Ryann-Ran/Scone.",
    "translation": "标题：Scone：通过统一理解-生成建模桥接主题驱动图像生成中的组合与区分\n\n摘要：主题驱动图像生成已从单一主题组合发展到多主题组合，但忽视了区分能力——即在输入包含多个候选主题时准确识别并生成正确主题的能力。这一局限限制了模型在复杂真实视觉场景中的有效性。我们提出Scone，一种统一的理解-生成方法，整合了组合与区分能力。Scone使理解专家充当语义桥梁，传递语义信息并引导生成专家在保持主题身份的同时最小化干扰。采用两阶段训练方案：先学习组合能力，再通过语义对齐和基于注意力的掩码机制增强区分能力。我们还提出了SconeEval基准测试，用于评估多样化场景下的组合与区分性能。实验表明，在两个基准测试中，Scone在组合与区分任务上均优于现有开源模型。我们的模型、基准测试及训练数据已开源：https://github.com/Ryann-Ran/Scone。",
    "url": "https://huggingface.co/papers/2512.12675",
    "arxiv_url": "https://arxiv.org/abs/2512.12675"
  },
  {
    "title": "RoboTracer: Mastering Spatial Trace with Reasoning in Vision-Language Models for Robotics",
    "summary": "Spatial tracing, as a fundamental embodied interaction ability for robots, is inherently challenging as it requires multi-step metric-grounded reasoning compounded with complex spatial referring and real-world metric measurement. However, existing methods struggle with this compositional task. To this end, we propose RoboTracer, a 3D-aware VLM that first achieves both 3D spatial referring and measuring via a universal spatial encoder and a regression-supervised decoder to enhance scale awareness during supervised fine-tuning (SFT). Moreover, RoboTracer advances multi-step metric-grounded reasoning via reinforcement fine-tuning (RFT) with metric-sensitive process rewards, supervising key intermediate perceptual cues to accurately generate spatial traces. To support SFT and RFT training, we introduce TraceSpatial, a large-scale dataset of 30M QA pairs, spanning outdoor/indoor/tabletop scenes and supporting complex reasoning processes (up to 9 steps). We further present TraceSpatial-Bench, a challenging benchmark filling the gap to evaluate spatial tracing. Experimental results show that RoboTracer surpasses baselines in spatial understanding, measuring, and referring, with an average success rate of 79.1%, and also achieves SOTA performance on TraceSpatial-Bench by a large margin, exceeding Gemini-2.5-Pro by 36% accuracy. Notably, RoboTracer can be integrated with various control policies to execute long-horizon, dynamic tasks across diverse robots (UR5, G1 humanoid) in cluttered real-world scenes.",
    "translation": "标题：RoboTracer：面向机器人学的视觉语言模型空间轨迹推理技术\n\n摘要：空间轨迹追踪作为机器人的基础具身交互能力，其实现面临本质性挑战，需要融合多步骤度量推理、复杂空间指代与现实世界度量测量。然而，现有方法难以应对此类组合式任务。为此，我们提出RoboTracer——一种具有三维感知能力的视觉语言模型，首次通过通用空间编码器与回归监督解码器，在监督微调阶段实现三维空间指代与测量，从而增强模型对尺度信息的感知能力。此外，RoboTracer通过引入度量敏感过程奖励的强化微调机制，监督关键中间感知线索以精准生成空间轨迹，进而推进多步骤度量推理能力。为支撑监督微调与强化微调训练，我们构建了包含3000万问答对的大规模数据集TraceSpatial，涵盖室外/室内/桌面场景，并支持多达9步的复杂推理流程。同时，我们提出评测基准TraceSpatial-Bench，填补了空间轨迹追踪系统性评估的空白。实验结果表明，RoboTracer在空间理解、测量与指代任务中全面超越基线模型，平均成功率达79.1%，并在TraceSpatial-Bench基准上以显著优势取得最先进性能，准确率较Gemini-2.5-Pro提升36%。值得注意的是，RoboTracer可与多种控制策略集成，在杂乱的真实场景中驱动多样化机器人（UR5、G1人形机器人）执行长时程动态任务。",
    "url": "https://huggingface.co/papers/2512.13660",
    "arxiv_url": "https://arxiv.org/abs/2512.13660"
  },
  {
    "title": "OpenDataArena: A Fair and Open Arena for Benchmarking Post-Training Dataset Value",
    "summary": "The rapid evolution of Large Language Models (LLMs) is predicated on the quality and diversity of post-training datasets. However, a critical dichotomy persists: while models are rigorously benchmarked, the data fueling them remains a black box--characterized by opaque composition, uncertain provenance, and a lack of systematic evaluation. This opacity hinders reproducibility and obscures the causal link between data characteristics and model behaviors. To bridge this gap, we introduce OpenDataArena (ODA), a holistic and open platform designed to benchmark the intrinsic value of post-training data. ODA establishes a comprehensive ecosystem comprising four key pillars: (i) a unified training-evaluation pipeline that ensures fair, open comparisons across diverse models (e.g., Llama, Qwen) and domains; (ii) a multi-dimensional scoring framework that profiles data quality along tens of distinct axes; (iii) an interactive data lineage explorer to visualize dataset genealogy and dissect component sources; and (iv) a fully open-source toolkit for training, evaluation, and scoring to foster data research. Extensive experiments on ODA--covering over 120 training datasets across multiple domains on 22 benchmarks, validated by more than 600 training runs and 40 million processed data points--reveal non-trivial insights. Our analysis uncovers the inherent trade-offs between data complexity and task performance, identifies redundancy in popular benchmarks through lineage tracing, and maps the genealogical relationships across datasets. We release all results, tools, and configurations to democratize access to high-quality data evaluation. Rather than merely expanding a leaderboard, ODA envisions a shift from trial-and-error data curation to a principled science of Data-Centric AI, paving the way for rigorous studies on data mixing laws and the strategic composition of foundation models.",
    "translation": "标题：OpenDataArena：一个用于基准测试训练后数据集价值的公平开放平台\n\n摘要：大语言模型的快速发展依赖于训练后数据集的质量与多样性。然而，一个关键矛盾持续存在：尽管模型经过了严格的基准测试，但支撑这些模型的数据却仍是一个“黑箱”——其构成不透明、来源不明确，且缺乏系统性评估。这种不透明性阻碍了研究的可复现性，并模糊了数据特征与模型行为之间的因果关系。为弥补这一差距，我们推出了OpenDataArena，这是一个旨在对训练后数据的内在价值进行基准测试的全面开放平台。ODA构建了一个包含四大支柱的综合生态系统：（一）统一的训练-评估流程，确保在不同模型（如Llama、Qwen）和领域间进行公平、开放的比较；（二）多维评分框架，从数十个不同维度刻画数据质量；（三）交互式数据谱系探索器，用于可视化数据集谱系并解析其构成来源；（四）完全开源的训练、评估与评分工具包，以促进数据研究。基于ODA开展的大规模实验——涵盖多个领域的120多个训练数据集、22项基准测试，并通过超过600次训练运行和4000万个已处理数据点进行验证——揭示了多项重要发现。我们的分析揭示了数据复杂度与任务性能之间的内在权衡，通过谱系追溯识别了流行基准中的冗余数据，并绘制了数据集间的谱系关系图。我们公开了所有结果、工具与配置，以推动高质量数据评估的普及。ODA不仅旨在扩展排行榜，更期望推动从试错式的数据整理转向以数据为中心的人工智能的规范科学，从而为数据混合规律与基础模型战略构成的严谨研究铺平道路。",
    "url": "https://huggingface.co/papers/2512.14051",
    "arxiv_url": "https://arxiv.org/abs/2512.14051"
  },
  {
    "title": "Vector Prism: Animating Vector Graphics by Stratifying Semantic Structure",
    "summary": "Scalable Vector Graphics (SVG) are central to modern web design, and the demand to animate them continues to grow as web environments become increasingly dynamic. Yet automating the animation of vector graphics remains challenging for vision-language models (VLMs) despite recent progress in code generation and motion planning. VLMs routinely mis-handle SVGs, since visually coherent parts are often fragmented into low-level shapes that offer little guidance of which elements should move together. In this paper, we introduce a framework that recovers the semantic structure required for reliable SVG animation and reveals the missing layer that current VLM systems overlook. This is achieved through a statistical aggregation of multiple weak part predictions, allowing the system to stably infer semantics from noisy predictions. By reorganizing SVGs into semantic groups, our approach enables VLMs to produce animations with far greater coherence. Our experiments demonstrate substantial gains over existing approaches, suggesting that semantic recovery is the key step that unlocks robust SVG animation and supports more interpretable interactions between VLMs and vector graphics.",
    "translation": "标题：向量棱镜：通过分层语义结构实现矢量图形动画化\n\n摘要：可缩放矢量图形（SVG）是现代网页设计的核心要素，随着网络环境日益动态化，对其动画化的需求持续增长。尽管在代码生成与运动规划领域已取得进展，但实现矢量图形动画的自动化对于视觉语言模型（VLM）而言仍具挑战性。由于视觉连贯的图形组件常被分割为低层级几何形状，难以提示哪些元素应协同运动，当前VLMs在处理SVG时频繁出现错误。本文提出一种框架，通过恢复SVG动画化所需的语义结构，揭示当前VLM系统所忽视的关键层级。该框架通过对多个弱部件预测结果进行统计聚合，使系统能够从噪声预测中稳定推断语义信息。通过将SVG重组为语义群组，本方法使VLMs能够生成连贯性显著提升的动画。实验结果表明，相较于现有方法，本框架取得实质性突破，证明语义重建是实现稳健SVG动画化的关键步骤，并为VLMs与矢量图形之间建立更可解释的交互机制提供支持。",
    "url": "https://huggingface.co/papers/2512.14336",
    "arxiv_url": "https://arxiv.org/abs/2512.14336"
  },
  {
    "title": "Reveal Hidden Pitfalls and Navigate Next Generation of Vector Similarity Search from Task-Centric Views",
    "summary": "Vector Similarity Search (VSS) in high-dimensional spaces is rapidly emerging as core functionality in next-generation database systems for numerous data-intensive services -- from embedding lookups in large language models (LLMs), to semantic information retrieval and recommendation engines. Current benchmarks, however, evaluate VSS primarily on the recall-latency trade-off against a ground truth defined solely by distance metrics, neglecting how retrieval quality ultimately impacts downstream tasks. This disconnect can mislead both academic research and industrial practice.\n  We present Iceberg, a holistic benchmark suite for end-to-end evaluation of VSS methods in realistic application contexts. From a task-centric view, Iceberg uncovers the Information Loss Funnel, which identifies three principal sources of end-to-end performance degradation: (1) Embedding Loss during feature extraction; (2) Metric Misuse, where distances poorly reflect task relevance; (3) Data Distribution Sensitivity, highlighting index robustness across skews and modalities. For a more comprehensive assessment, Iceberg spans eight diverse datasets across key domains such as image classification, face recognition, text retrieval, and recommendation systems. Each dataset, ranging from 1M to 100M vectors, includes rich, task-specific labels and evaluation metrics, enabling assessment of retrieval algorithms within the full application pipeline rather than in isolation. Iceberg benchmarks 13 state-of-the-art VSS methods and re-ranks them based on application-level metrics, revealing substantial deviations from traditional rankings derived purely from recall-latency evaluations. Building on these insights, we define a set of task-centric meta-features and derive an interpretable decision tree to guide practitioners in selecting and tuning VSS methods for their specific workloads.",
    "translation": "标题：从任务中心视角揭示向量相似性搜索的潜在缺陷与下一代导航路径\n\n摘要：高维空间中的向量相似性搜索正迅速成为下一代数据库系统的核心功能，支撑着从大语言模型中的嵌入检索到语义信息检索与推荐引擎等众多数据密集型服务。然而，当前基准测试主要围绕基于距离度量定义的召回率-延迟权衡进行评估，忽视了检索质量如何最终影响下游任务，这种脱节可能误导学术研究与工业实践。\n\n本文提出Iceberg——一个面向实际应用场景的向量相似性搜索方法端到端评估的综合性基准测试套件。从任务中心视角出发，Iceberg揭示了“信息损失漏斗”现象，识别出导致端到端性能下降的三个主要来源：(1) 特征提取过程中的嵌入损失；(2) 距离度量误用导致其难以反映任务相关性；(3) 数据分布敏感性，凸显索引在不同数据偏态与模态下的鲁棒性问题。为进行全面评估，Iceberg涵盖图像分类、人脸识别、文本检索和推荐系统等关键领域的八个多样化数据集。每个数据集包含100万至1亿条向量，并配备丰富的任务特定标签与评估指标，使得检索算法能在完整应用流程中而非孤立环境下接受评估。\n\n通过对13种前沿向量相似性搜索方法进行基准测试，并依据应用级指标重新排序，Iceberg揭示了传统仅依赖召回率-延迟的评估排名与实际应用效果间的显著偏差。基于这些发现，我们定义了一组任务中心元特征，并推导出可解释的决策树模型，为实践者根据具体工作负载选择与调优向量相似性搜索方法提供指导。",
    "url": "https://huggingface.co/papers/2512.12980",
    "arxiv_url": "https://arxiv.org/abs/2512.12980"
  },
  {
    "title": "MemFlow: Flowing Adaptive Memory for Consistent and Efficient Long Video Narratives",
    "summary": "The core challenge for streaming video generation is maintaining the content consistency in long context, which poses high requirement for the memory design. Most existing solutions maintain the memory by compressing historical frames with predefined strategies. However, different to-generate video chunks should refer to different historical cues, which is hard to satisfy with fixed strategies. In this work, we propose MemFlow to address this problem. Specifically, before generating the coming chunk, we dynamically update the memory bank by retrieving the most relevant historical frames with the text prompt of this chunk. This design enables narrative coherence even if new event happens or scenario switches in future frames. In addition, during generation, we only activate the most relevant tokens in the memory bank for each query in the attention layers, which effectively guarantees the generation efficiency. In this way, MemFlow achieves outstanding long-context consistency with negligible computation burden (7.9% speed reduction compared with the memory-free baseline) and keeps the compatibility with any streaming video generation model with KV cache.",
    "translation": "标题：MemFlow：用于一致高效长视频叙事的自适应记忆流\n\n摘要：流式视频生成的核心挑战在于保持长上下文中的内容一致性，这对记忆设计提出了较高要求。现有方案大多通过预定义策略压缩历史帧来维护记忆。然而，不同待生成视频片段应参考不同的历史线索，固定策略难以满足这一需求。本研究提出MemFlow以解决该问题。具体而言，在生成后续片段前，我们通过检索与该片段文本提示最相关的历史帧来动态更新记忆库。这一设计使得即使后续帧中出现新事件或场景切换，叙事仍能保持连贯性。此外，在生成过程中，我们仅激活记忆库中与注意力层每个查询最相关的标记，从而有效保障生成效率。MemFlow通过这种方式实现了卓越的长上下文一致性，其计算开销可忽略不计（与无记忆基线相比仅降低7.9%生成速度），并保持了对所有支持KV缓存的流式视频生成模型的兼容性。",
    "url": "https://huggingface.co/papers/2512.14699",
    "arxiv_url": "https://arxiv.org/abs/2512.14699"
  },
  {
    "title": "RecGPT-V2 Technical Report",
    "summary": "Large language models (LLMs) have demonstrated remarkable potential in transforming recommender systems from implicit behavioral pattern matching to explicit intent reasoning. While RecGPT-V1 successfully pioneered this paradigm by integrating LLM-based reasoning into user interest mining and item tag prediction, it suffers from four fundamental limitations: (1) computational inefficiency and cognitive redundancy across multiple reasoning routes; (2) insufficient explanation diversity in fixed-template generation; (3) limited generalization under supervised learning paradigms; and (4) simplistic outcome-focused evaluation that fails to match human standards.\n  To address these challenges, we present RecGPT-V2 with four key innovations. First, a Hierarchical Multi-Agent System restructures intent reasoning through coordinated collaboration, eliminating cognitive duplication while enabling diverse intent coverage. Combined with Hybrid Representation Inference that compresses user-behavior contexts, our framework reduces GPU consumption by 60% and improves exclusive recall from 9.39% to 10.99%. Second, a Meta-Prompting framework dynamically generates contextually adaptive prompts, improving explanation diversity by +7.3%. Third, constrained reinforcement learning mitigates multi-reward conflicts, achieving +24.1% improvement in tag prediction and +13.0% in explanation acceptance. Fourth, an Agent-as-a-Judge framework decomposes assessment into multi-step reasoning, improving human preference alignment. Online A/B tests on Taobao demonstrate significant improvements: +2.98% CTR, +3.71% IPV, +2.19% TV, and +11.46% NER. RecGPT-V2 establishes both the technical feasibility and commercial viability of deploying LLM-powered intent reasoning at scale, bridging the gap between cognitive exploration and industrial utility.",
    "translation": "标题：RecGPT-V2技术报告\n\n摘要：大型语言模型在将推荐系统从隐式行为模式匹配转变为显式意图推理方面展现出显著潜力。尽管RecGPT-V1通过将基于大语言模型的推理融入用户兴趣挖掘与物品标签预测，成功开创了这一范式，但其存在四个根本性局限：（1）多推理路径下的计算效率低下与认知冗余；（2）固定模板生成中解释多样性的不足；（3）监督学习范式下泛化能力有限；（4）结果导向的评估方式过于简化，难以匹配人类标准。\n\n为应对这些挑战，我们提出RecGPT-V2，其包含四项关键创新。首先，分层多智能体系统通过协同协作重构意图推理，在消除认知重复的同时实现多样意图覆盖。结合压缩用户行为上下文的混合表征推理技术，该框架将GPU消耗降低60%，并将独占召回率从9.39%提升至10.99%。其次，元提示框架动态生成情境自适应提示，使解释多样性提升7.3%。第三，约束强化学习缓解多奖励冲突，实现标签预测准确率提升24.1%，解释接受度提升13.0%。第四，智能体即裁判框架将评估分解为多步推理，显著提升人类偏好对齐度。在淘宝平台的在线A/B测试中取得显著提升：点击率+2.98%、商品详情页浏览量+3.71%、交易额+2.19%、新体验留存率+11.46%。RecGPT-V2从技术可行性与商业价值两个维度，证实了大规模部署基于大语言模型的意图推理系统的现实意义，成功弥合了认知探索与工业应用之间的鸿沟。",
    "url": "https://huggingface.co/papers/2512.14503",
    "arxiv_url": "https://arxiv.org/abs/2512.14503"
  },
  {
    "title": "ShowTable: Unlocking Creative Table Visualization with Collaborative Reflection and Refinement",
    "summary": "While existing generation and unified models excel at general image generation, they struggle with tasks requiring deep reasoning, planning, and precise data-to-visual mapping abilities beyond general scenarios. To push beyond the existing limitations, we introduce a new and challenging task: creative table visualization, requiring the model to generate an infographic that faithfully and aesthetically visualizes the data from a given table. To address this challenge, we propose ShowTable, a pipeline that synergizes MLLMs with diffusion models via a progressive self-correcting process. The MLLM acts as the central orchestrator for reasoning the visual plan and judging visual errors to provide refined instructions, the diffusion execute the commands from MLLM, achieving high-fidelity results. To support this task and our pipeline, we introduce three automated data construction pipelines for training different modules. Furthermore, we introduce TableVisBench, a new benchmark with 800 challenging instances across 5 evaluation dimensions, to assess performance on this task. Experiments demonstrate that our pipeline, instantiated with different models, significantly outperforms baselines, highlighting its effective multi-modal reasoning, generation, and error correction capabilities.",
    "translation": "标题：ShowTable：通过协同反思与优化解锁创意表格可视化\n\n摘要：尽管现有生成模型与统一模型在通用图像生成方面表现优异，但其在需要深度推理、规划以及超越通用场景的精确数据到视觉映射能力的任务上仍面临挑战。为突破现有局限，我们提出一项新颖且具有挑战性的任务：创意表格可视化，要求模型根据给定表格数据生成既忠实反映数据又具备美学价值的信息图。为应对这一挑战，我们提出ShowTable流程，该流程通过渐进式自我修正过程将多模态大语言模型与扩散模型协同整合。其中，多模态大语言模型作为核心协调者，负责推理视觉方案并判断视觉误差以提供优化指令，扩散模型则执行多模态大语言模型的指令，从而实现高保真度的生成结果。为支持该任务及流程，我们设计了三条自动化数据构建流程用于训练不同模块。此外，我们提出TableVisBench新基准，该基准包含800个涵盖5个评估维度的挑战性实例，用于系统评估模型在此任务上的性能。实验表明，基于不同模型实例化的流程显著优于基线方法，凸显了其在多模态推理、生成与纠错方面的卓越能力。",
    "url": "https://huggingface.co/papers/2512.13303",
    "arxiv_url": "https://arxiv.org/abs/2512.13303"
  },
  {
    "title": "Feedforward 3D Editing via Text-Steerable Image-to-3D",
    "summary": "Recent progress in image-to-3D has opened up immense possibilities for design, AR/VR, and robotics. However, to use AI-generated 3D assets in real applications, a critical requirement is the capability to edit them easily. We present a feedforward method, Steer3D, to add text steerability to image-to-3D models, which enables editing of generated 3D assets with language. Our approach is inspired by ControlNet, which we adapt to image-to-3D generation to enable text steering directly in a forward pass. We build a scalable data engine for automatic data generation, and develop a two-stage training recipe based on flow-matching training and Direct Preference Optimization (DPO). Compared to competing methods, Steer3D more faithfully follows the language instruction and maintains better consistency with the original 3D asset, while being 2.4x to 28.5x faster. Steer3D demonstrates that it is possible to add a new modality (text) to steer the generation of pretrained image-to-3D generative models with 100k data. Project website: https://glab-caltech.github.io/steer3d/",
    "translation": "标题：基于文本可引导图像到三维的前馈式编辑方法\n\n摘要：图像到三维生成技术的最新进展为设计、增强现实/虚拟现实及机器人领域开辟了广阔前景。然而，要将人工智能生成的三维资产应用于实际场景，关键需求在于能够便捷地对其进行编辑。本文提出一种前馈式方法Steer3D，通过为图像到三维模型增加文本引导能力，实现用自然语言编辑生成的三维资产。该方法受ControlNet架构启发，将其适配于图像到三维生成任务，从而在前向传播中直接实现文本引导。我们构建了可扩展的自动数据生成引擎，并开发了基于流匹配训练与直接偏好优化的两阶段训练方案。与现有方法相比，Steer3D能更准确地遵循语言指令，同时保持与原始三维资产更好的一致性，且处理速度提升2.4至28.5倍。实验表明，仅需十万量级数据即可为预训练的图像到三维生成模型增加新模态（文本）的引导能力。项目网站：https://glab-caltech.github.io/steer3d/",
    "url": "https://huggingface.co/papers/2512.13678",
    "arxiv_url": "https://arxiv.org/abs/2512.13678"
  },
  {
    "title": "Nemotron-Cascade: Scaling Cascaded Reinforcement Learning for General-Purpose Reasoning Models",
    "summary": "Building general-purpose reasoning models with reinforcement learning (RL) entails substantial cross-domain heterogeneity, including large variation in inference-time response lengths and verification latency. Such variability complicates the RL infrastructure, slows training, and makes training curriculum (e.g., response length extension) and hyperparameter selection challenging. In this work, we propose cascaded domain-wise reinforcement learning (Cascade RL) to develop general-purpose reasoning models, Nemotron-Cascade, capable of operating in both instruct and deep thinking modes. Departing from conventional approaches that blend heterogeneous prompts from different domains, Cascade RL orchestrates sequential, domain-wise RL, reducing engineering complexity and delivering state-of-the-art performance across a wide range of benchmarks. Notably, RLHF for alignment, when used as a pre-step, boosts the model's reasoning ability far beyond mere preference optimization, and subsequent domain-wise RLVR stages rarely degrade the benchmark performance attained in earlier domains and may even improve it (see an illustration in Figure 1). Our 14B model, after RL, outperforms its SFT teacher, DeepSeek-R1-0528, on LiveCodeBench v5/v6/Pro and achieves silver-medal performance in the 2025 International Olympiad in Informatics (IOI). We transparently share our training and data recipes.",
    "translation": "标题：Nemotron-Cascade：面向通用推理模型的级联强化学习规模化方法\n\n摘要：使用强化学习构建通用推理模型面临显著的跨领域异质性挑战，包括推理时响应长度和验证延迟的巨大差异。这种变异性使强化学习基础设施复杂化，延缓训练进程，并为训练课程设计（如响应长度扩展）和超参数选择带来困难。本研究提出级联领域强化学习方法，用于开发具备指令执行与深度思考双模式的通用推理模型Nemotron-Cascade。与传统混合不同领域异构提示的方法不同，级联强化学习通过按领域顺序执行强化学习来协调训练流程，既降低了工程复杂度，又在广泛基准测试中实现了最先进的性能。值得注意的是，作为前置步骤的对齐强化学习从人类反馈不仅超越了单纯的偏好优化，更显著提升了模型的推理能力；后续按领域进行的强化学习验证与修正阶段几乎不会降低模型在早期领域已取得的基准性能，甚至可能带来提升（如图1示例所示）。经过强化学习的140亿参数模型在LiveCodeBench v5/v6/Pro基准上超越其监督微调教师模型DeepSeek-R1-0528，并在2025年国际信息学奥林匹克竞赛中达到银牌水平。我们公开分享了完整的训练方案与数据构建方法。",
    "url": "https://huggingface.co/papers/2512.13607",
    "arxiv_url": "https://arxiv.org/abs/2512.13607"
  },
  {
    "title": "Olmo 3",
    "summary": "We introduce Olmo 3, a family of state-of-the-art, fully-open language models at the 7B and 32B parameter scales. Olmo 3 model construction targets long-context reasoning, function calling, coding, instruction following, general chat, and knowledge recall. This release includes the entire model flow, i.e., the full lifecycle of the family of models, including every stage, checkpoint, data point, and dependency used to build it. Our flagship model, Olmo 3 Think 32B, is the strongest fully-open thinking model released to-date.",
    "translation": "标题：Olmo 3\n\n摘要：本文介绍Olmo 3系列模型，这是一组参数规模分别为70亿和320亿的顶尖全开源语言模型。Olmo 3的构建目标涵盖长上下文推理、函数调用、代码生成、指令跟随、通用对话及知识检索等能力。本次发布包含完整的模型构建流程，即该系列模型从数据准备到最终成型的全生命周期，涵盖每个构建阶段、检查点、数据点及所有相关依赖项。我们的旗舰模型Olmo 3 Think 32B，是迄今为止发布的最强大的全开源思维模型。",
    "url": "https://huggingface.co/papers/2512.13961",
    "arxiv_url": "https://arxiv.org/abs/2512.13961"
  },
  {
    "title": "Differentiable Evolutionary Reinforcement Learning",
    "summary": "The design of effective reward functions presents a central and often arduous challenge in reinforcement learning (RL), particularly when developing autonomous agents for complex reasoning tasks. While automated reward optimization approaches exist, they typically rely on derivative-free evolutionary heuristics that treat the reward function as a black box, failing to capture the causal relationship between reward structure and task performance. To bridge this gap, we propose Differentiable Evolutionary Reinforcement Learning (DERL), a bilevel framework that enables the autonomous discovery of optimal reward signals. In DERL, a Meta-Optimizer evolves a reward function (i.e., Meta-Reward) by composing structured atomic primitives, guiding the training of an inner-loop policy. Crucially, unlike previous evolution, DERL is differentiable in its metaoptimization: it treats the inner-loop validation performance as a signal to update the Meta-Optimizer via reinforcement learning. This allows DERL to approximate the \"meta-gradient\" of task success, progressively learning to generate denser and more actionable feedback. We validate DERL across three distinct domains: robotic agent (ALFWorld), scientific simulation (ScienceWorld), and mathematical reasoning (GSM8k, MATH). Experimental results show that DERL achieves state-of-the-art performance on ALFWorld and ScienceWorld, significantly outperforming methods relying on heuristic rewards, especially in out-of-distribution scenarios. Analysis of the evolutionary trajectory demonstrates that DERL successfully captures the intrinsic structure of tasks, enabling selfimproving agent alignment without human intervention.",
    "translation": "标题：可微分进化强化学习\n\n摘要：在强化学习（RL）中，设计有效的奖励函数是一个核心且往往艰巨的挑战，尤其是在为复杂推理任务开发自主智能体时。尽管存在自动化的奖励优化方法，但它们通常依赖于将奖励函数视为黑盒的无导数进化启发式算法，未能捕捉奖励结构与任务性能之间的因果关系。为弥合这一差距，我们提出了可微分进化强化学习（DERL），这是一个双层框架，能够自主发现最优奖励信号。在DERL中，元优化器通过组合结构化的原子基元来演化奖励函数（即元奖励），从而指导内层策略的训练。关键的是，与以往的进化方法不同，DERL在其元优化过程中是可微分的：它将内层验证性能视为信号，通过强化学习来更新元优化器。这使得DERL能够近似任务成功的“元梯度”，逐步学习生成更密集且更具可操作性的反馈。我们在三个不同领域验证了DERL：机器人智能体（ALFWorld）、科学模拟（ScienceWorld）和数学推理（GSM8k, MATH）。实验结果表明，DERL在ALFWorld和ScienceWorld上实现了最先进的性能，显著优于依赖启发式奖励的方法，尤其是在分布外场景中。对进化轨迹的分析表明，DERL成功捕捉了任务的内在结构，使得智能体能够在无需人工干预的情况下实现自我改进的对齐。",
    "url": "https://huggingface.co/papers/2512.13399",
    "arxiv_url": "https://arxiv.org/abs/2512.13399"
  },
  {
    "title": "VersatileFFN: Achieving Parameter Efficiency in LLMs via Adaptive Wide-and-Deep Reuse",
    "summary": "The rapid scaling of Large Language Models (LLMs) has achieved remarkable performance, but it also leads to prohibitive memory costs. Existing parameter-efficient approaches such as pruning and quantization mainly compress pretrained models without enhancing architectural capacity, thereby hitting the representational ceiling of the base model. In this work, we propose VersatileFFN, a novel feed-forward network (FFN) that enables flexible reuse of parameters in both width and depth dimensions within a fixed parameter budget. Inspired by the dual-process theory of cognition, VersatileFFN comprises two adaptive pathways: a width-versatile path that generates a mixture of sub-experts from a single shared FFN, mimicking sparse expert routing without increasing parameters, and a depth-versatile path that recursively applies the same FFN to emulate deeper processing for complex tokens. A difficulty-aware gating dynamically balances the two pathways, steering \"easy\" tokens through the efficient width-wise route and allocating deeper iterative refinement to \"hard\" tokens. Crucially, both pathways reuse the same parameters, so all additional capacity comes from computation rather than memory. Experiments across diverse benchmarks and model scales demonstrate the effectiveness of the method. The code will be available at https://github.com/huawei-noah/noah-research/tree/master/VersatileFFN.",
    "translation": "标题：VersatileFFN：通过自适应宽深复用实现大语言模型的参数高效性\n\n摘要：大语言模型（LLMs）规模的快速扩展已取得显著性能，但也导致了高昂的内存成本。现有的参数高效方法（如剪枝和量化）主要对预训练模型进行压缩，而未增强其架构能力，因此触及了基础模型的表示能力上限。本文提出VersatileFFN，一种新颖的前馈网络（FFN），能够在固定参数预算内灵活复用宽度和深度维度的参数。受认知双过程理论启发，VersatileFFN包含两条自适应路径：一条宽度自适应路径，从单个共享FFN生成混合子专家，模拟稀疏专家路由而不增加参数；另一条深度自适应路径，递归应用同一FFN以模拟对复杂标记的更深层处理。一个难度感知门控机制动态平衡两条路径，引导“简单”标记通过高效的宽度路径，并为“困难”标记分配更深层的迭代细化。关键在于两条路径复用相同参数，因此所有额外能力均来自计算而非内存。在不同基准测试和模型规模上的实验验证了该方法的有效性。代码将在https://github.com/huawei-noah/noah-research/tree/master/VersatileFFN 公开。",
    "url": "https://huggingface.co/papers/2512.14531",
    "arxiv_url": "https://arxiv.org/abs/2512.14531"
  },
  {
    "title": "A4-Agent: An Agentic Framework for Zero-Shot Affordance Reasoning",
    "summary": "Affordance prediction, which identifies interaction regions on objects based on language instructions, is critical for embodied AI. Prevailing end-to-end models couple high-level reasoning and low-level grounding into a single monolithic pipeline and rely on training over annotated datasets, which leads to poor generalization on novel objects and unseen environments. In this paper, we move beyond this paradigm by proposing A4-Agent, a training-free agentic framework that decouples affordance prediction into a three-stage pipeline. Our framework coordinates specialized foundation models at test time: (1) a Dreamer that employs generative models to visualize how an interaction would look; (2) a Thinker that utilizes large vision-language models to decide what object part to interact with; and (3) a Spotter that orchestrates vision foundation models to precisely locate where the interaction area is. By leveraging the complementary strengths of pre-trained models without any task-specific fine-tuning, our zero-shot framework significantly outperforms state-of-the-art supervised methods across multiple benchmarks and demonstrates robust generalization to real-world settings.",
    "translation": "标题：A4-Agent：一种用于零样本可供性推理的智能体框架\n\n摘要：可供性预测旨在根据语言指令识别物体上的交互区域，是实现具身人工智能的关键技术。当前主流端到端模型将高层推理与低层定位耦合为单一整体流程，并依赖标注数据集进行训练，导致其在新型物体和未见环境中的泛化能力较差。本文突破此范式，提出A4-Agent——一种无需训练的智能体框架，将可供性预测解耦为三阶段流程。该框架在测试时协调多个专用基础模型：(1) 运用生成模型可视化交互场景的“构想者”；(2) 利用大型视觉语言模型确定交互部位的“思考者”；(3) 调度视觉基础模型精确定位交互区域的“定位者”。通过融合预训练模型的互补优势且无需任务特定微调，本零样本框架在多个基准测试中显著超越现有监督方法，并展现出对真实场景的强健泛化能力。",
    "url": "https://huggingface.co/papers/2512.14442",
    "arxiv_url": "https://arxiv.org/abs/2512.14442"
  },
  {
    "title": "SS4D: Native 4D Generative Model via Structured Spacetime Latents",
    "summary": "We present SS4D, a native 4D generative model that synthesizes dynamic 3D objects directly from monocular video. Unlike prior approaches that construct 4D representations by optimizing over 3D or video generative models, we train a generator directly on 4D data, achieving high fidelity, temporal coherence, and structural consistency. At the core of our method is a compressed set of structured spacetime latents. Specifically, (1) To address the scarcity of 4D training data, we build on a pre-trained single-image-to-3D model, preserving strong spatial consistency. (2) Temporal consistency is enforced by introducing dedicated temporal layers that reason across frames. (3) To support efficient training and inference over long video sequences, we compress the latent sequence along the temporal axis using factorized 4D convolutions and temporal downsampling blocks. In addition, we employ a carefully designed training strategy to enhance robustness against occlusion",
    "translation": "标题：SS4D：基于结构化时空隐变量的原生4D生成模型\n\n摘要：本文提出SS4D，一种原生4D生成模型，能够直接从单目视频合成动态三维物体。与现有方法通过优化三维或视频生成模型来构建4D表示不同，我们直接在4D数据上训练生成器，实现了高保真度、时间连贯性和结构一致性。本方法的核心是一组经过压缩的结构化时空隐变量。具体而言：（1）针对4D训练数据稀缺的问题，我们在预训练的单图像转三维模型基础上构建，保持了强大的空间一致性；（2）通过引入跨帧推理的专用时间层来保证时间连贯性；（3）为支持长视频序列的高效训练与推理，我们采用分解式4D卷积和时间下采样模块沿时间轴压缩隐变量序列。此外，我们采用精心设计的训练策略以增强对遮挡场景的鲁棒性。",
    "url": "https://huggingface.co/papers/2512.14284",
    "arxiv_url": "https://arxiv.org/abs/2512.14284"
  },
  {
    "title": "Sparse-LaViDa: Sparse Multimodal Discrete Diffusion Language Models",
    "summary": "Masked Discrete Diffusion Models (MDMs) have achieved strong performance across a wide range of multimodal tasks, including image understanding, generation, and editing. However, their inference speed remains suboptimal due to the need to repeatedly process redundant masked tokens at every sampling step. In this work, we propose Sparse-LaViDa, a novel modeling framework that dynamically truncates unnecessary masked tokens at each inference step to accelerate MDM sampling. To preserve generation quality, we introduce specialized register tokens that serve as compact representations for the truncated tokens. Furthermore, to ensure consistency between training and inference, we design a specialized attention mask that faithfully matches the truncated sampling procedure during training. Built upon the state-of-the-art unified MDM LaViDa-O, Sparse-LaViDa achieves up to a 2x speedup across diverse tasks including text-to-image generation, image editing, and mathematical reasoning, while maintaining generation quality.",
    "translation": "标题：Sparse-LaViDa：稀疏多模态离散扩散语言模型\n\n摘要：掩码离散扩散模型（MDMs）在图像理解、生成与编辑等广泛多模态任务中展现出卓越性能。然而，由于每个采样步骤均需重复处理冗余的掩码标记，其推理速度仍存在优化空间。本研究提出Sparse-LaViDa——一种新颖的建模框架，通过在每步推理中动态截断非必要的掩码标记以加速MDM采样。为保持生成质量，我们引入专用寄存器标记作为被截断标记的紧凑表征。此外，为确保训练与推理的一致性，我们设计了特殊的注意力掩码机制，使训练过程能精确模拟截断采样流程。基于当前最先进的统一MDM框架LaViDa-O构建的Sparse-LaViDa，在文本到图像生成、图像编辑和数学推理等多样化任务中实现了最高达2倍的加速，同时完全保持生成质量。",
    "url": "https://huggingface.co/papers/2512.14008",
    "arxiv_url": "https://arxiv.org/abs/2512.14008"
  },
  {
    "title": "Spherical Leech Quantization for Visual Tokenization and Generation",
    "summary": "Non-parametric quantization has received much attention due to its efficiency on parameters and scalability to a large codebook. In this paper, we present a unified formulation of different non-parametric quantization methods through the lens of lattice coding. The geometry of lattice codes explains the necessity of auxiliary loss terms when training auto-encoders with certain existing lookup-free quantization variants such as BSQ. As a step forward, we explore a few possible candidates, including random lattices, generalized Fibonacci lattices, and densest sphere packing lattices. Among all, we find the Leech lattice-based quantization method, which is dubbed as Spherical Leech Quantization (Λ_{24}-SQ), leads to both a simplified training recipe and an improved reconstruction-compression tradeoff thanks to its high symmetry and even distribution on the hypersphere. In image tokenization and compression tasks, this quantization approach achieves better reconstruction quality across all metrics than BSQ, the best prior art, while consuming slightly fewer bits. The improvement also extends to state-of-the-art auto-regressive image generation frameworks.",
    "translation": "标题：用于视觉标记化与生成的球形里奇量化\n\n摘要：非参数量化方法因其参数效率高且能扩展至大规模码本而备受关注。本文通过格编码的视角，提出了不同非参数量化方法的统一表述框架。格编码的几何特性揭示了在训练自编码器时，对于某些现有免查表量化变体（如BSQ）引入辅助损失项的必要性。在此基础上，我们探索了若干可能的格结构候选方案，包括随机格、广义斐波那契格以及最密球堆积格。研究发现，基于里奇格的量化方法（称为球形里奇量化Λ_{24}-SQ）凭借其高对称性与超球面上的均匀分布特性，既能简化训练流程，又能改善重建与压缩的权衡关系。在图像标记化与压缩任务中，该方法在所有评估指标上均优于当前最佳基准BSQ，同时消耗更少的比特数。该优势同样延伸至当前最先进的自回归图像生成框架中。",
    "url": "https://huggingface.co/papers/2512.14697",
    "arxiv_url": "https://arxiv.org/abs/2512.14697"
  },
  {
    "title": "CRISP: Contact-Guided Real2Sim from Monocular Video with Planar Scene Primitives",
    "summary": "We introduce CRISP, a method that recovers simulatable human motion and scene geometry from monocular video. Prior work on joint human-scene reconstruction relies on data-driven priors and joint optimization with no physics in the loop, or recovers noisy geometry with artifacts that cause motion tracking policies with scene interactions to fail. In contrast, our key insight is to recover convex, clean, and simulation-ready geometry by fitting planar primitives to a point cloud reconstruction of the scene, via a simple clustering pipeline over depth, normals, and flow. To reconstruct scene geometry that might be occluded during interactions, we make use of human-scene contact modeling (e.g., we use human posture to reconstruct the occluded seat of a chair). Finally, we ensure that human and scene reconstructions are physically-plausible by using them to drive a humanoid controller via reinforcement learning. Our approach reduces motion tracking failure rates from 55.2\\% to 6.9\\% on human-centric video benchmarks (EMDB, PROX), while delivering a 43\\% faster RL simulation throughput. We further validate it on in-the-wild videos including casually-captured videos, Internet videos, and even Sora-generated videos. This demonstrates CRISP's ability to generate physically-valid human motion and interaction environments at scale, greatly advancing real-to-sim applications for robotics and AR/VR.",
    "translation": "标题：CRISP：基于平面场景基元与接触引导的单目视频真实到仿真转换方法\n\n摘要：本文提出CRISP方法，该技术能够从单目视频中重建可仿真的运动轨迹与场景几何结构。现有的人-场景联合重建方法通常依赖数据驱动的先验知识，采用无物理约束的联合优化方案，或重建出存在噪声与伪影的几何结构，导致涉及场景交互的运动追踪策略失效。与此不同，我们的核心思路是通过对场景点云进行深度、法向量和光流信息的聚类处理，拟合平面几何基元，从而重建出凸性、清洁且可直接用于仿真的几何模型。为重建可能被交互过程遮挡的场景几何，我们引入人-场景接触建模机制（例如利用人体姿态重建被遮挡的椅面）。最后，通过强化学习驱动的人形控制器验证重建结果，确保人体与场景重建的物理合理性。在以人为中心的视频基准测试集（EMDB、PROX）上，本方法将运动追踪失败率从55.2%降低至6.9%，同时强化学习仿真吞吐量提升43%。我们进一步在多样化真实场景视频（包括日常拍摄视频、网络视频乃至Sora生成视频）中验证了方法的有效性。实验表明，CRISP能够大规模生成物理有效的人体运动与交互环境，显著推进机器人及AR/VR领域的真实到仿真应用。\n\n请按照以下格式返回：\n标题：CRISP：基于平面场景基元与接触引导的单目视频真实到仿真转换方法\n摘要：本文提出CRISP方法，该技术能够从单目视频中重建可仿真的运动轨迹与场景几何结构。现有的人-场景联合重建方法通常依赖数据驱动的先验知识，采用无物理约束的联合优化方案，或重建出存在噪声与伪影的几何结构，导致涉及场景交互的运动追踪策略失效。与此不同，我们的核心思路是通过对场景点云进行深度、法向量和光流信息的聚类处理，拟合平面几何基元，从而重建出凸性、清洁且可直接用于仿真的几何模型。为重建可能被交互过程遮挡的场景几何，我们引入人-场景接触建模机制（例如利用人体姿态重建被遮挡的椅面）。最后，通过强化学习驱动的人形控制器验证重建结果，确保人体与场景重建的物理合理性。在以人为中心的视频基准测试集（EMDB、PROX）上，本方法将运动追踪失败率从55.2%降低至6.9%，同时强化学习仿真吞吐量提升43%。我们进一步在多样化真实场景视频（包括日常拍摄视频、网络视频乃至Sora生成视频）中验证了方法的有效性。实验表明，CRISP能够大规模生成物理有效的人体运动与交互环境，显著推进机器人及AR/VR领域的真实到仿真应用。",
    "url": "https://huggingface.co/papers/2512.14696",
    "arxiv_url": "https://arxiv.org/abs/2512.14696"
  },
  {
    "title": "TimeLens: Rethinking Video Temporal Grounding with Multimodal LLMs",
    "summary": "This paper does not introduce a novel method but instead establishes a straightforward, incremental, yet essential baseline for video temporal grounding (VTG), a core capability in video understanding. While multimodal large language models (MLLMs) excel at various video understanding tasks, the recipes for optimizing them for VTG remain under-explored. In this paper, we present TimeLens, a systematic investigation into building MLLMs with strong VTG ability, along two primary dimensions: data quality and algorithmic design. We first expose critical quality issues in existing VTG benchmarks and introduce TimeLens-Bench, comprising meticulously re-annotated versions of three popular benchmarks with strict quality criteria. Our analysis reveals dramatic model re-rankings compared to legacy benchmarks, confirming the unreliability of prior evaluation standards. We also address noisy training data through an automated re-annotation pipeline, yielding TimeLens-100K, a large-scale, high-quality training dataset. Building on our data foundation, we conduct in-depth explorations of algorithmic design principles, yielding a series of meaningful insights and effective yet efficient practices. These include interleaved textual encoding for time representation, a thinking-free reinforcement learning with verifiable rewards (RLVR) approach as the training paradigm, and carefully designed recipes for RLVR training. These efforts culminate in TimeLens models, a family of MLLMs with state-of-the-art VTG performance among open-source models and even surpass proprietary models such as GPT-5 and Gemini-2.5-Flash. All codes, data, and models will be released to facilitate future research.",
    "translation": "标题：TimeLens：基于多模态大语言模型重新思考视频时序定位\n\n摘要：本文并未提出一种新颖的方法，而是为视频理解的核心能力——视频时序定位（VTG）建立了一个直接、渐进但至关重要的基准。尽管多模态大语言模型（MLLMs）在多种视频理解任务中表现出色，但如何针对VTG任务优化这些模型的策略仍未得到充分探索。本文提出TimeLens，围绕数据质量和算法设计两个主要维度，系统性地研究了如何构建具备强大VTG能力的MLLMs。我们首先揭示了现有VTG基准数据集中存在的关键质量问题，并引入了TimeLens-Bench——一个包含三个流行基准数据集经过严格质量标准重新标注的版本。我们的分析表明，与旧有基准相比，模型评估排名发生了显著变化，证实了先前评估标准的不可靠性。同时，我们通过自动化重新标注流程处理了训练数据中的噪声，构建了TimeLens-100K，这是一个大规模、高质量的训练数据集。基于此数据基础，我们深入探索了算法设计原则，获得了一系列有意义的见解以及高效且有效的实践方法。这些方法包括：用于时间表示的交替文本编码、一种无需复杂推理且具有可验证奖励的强化学习（RLVR）训练范式，以及精心设计的RLVR训练策略。这些努力最终催生了TimeLens模型系列——一组在开源模型中具备最先进VTG性能的MLLMs，其表现甚至超越了GPT-5和Gemini-2.5-Flash等专有模型。所有代码、数据和模型都将公开发布，以促进未来研究。",
    "url": "https://huggingface.co/papers/2512.14698",
    "arxiv_url": "https://arxiv.org/abs/2512.14698"
  },
  {
    "title": "EVOLVE-VLA: Test-Time Training from Environment Feedback for Vision-Language-Action Models",
    "summary": "Achieving truly adaptive embodied intelligence requires agents that learn not just by imitating static demonstrations, but by continuously improving through environmental interaction, which is akin to how humans master skills through practice. Vision-Language-Action (VLA) models have advanced robotic manipulation by leveraging large language models, yet remain fundamentally limited by Supervised Finetuning (SFT): requiring hundreds of demonstrations per task, rigidly memorizing trajectories, and failing to adapt when deployment conditions deviate from training. We introduce EVOLVE-VLA, a test-time training framework enabling VLAs to continuously adapt through environment interaction with minimal or zero task-specific demonstrations. The key technical challenge is replacing oracle reward signals (unavailable at test time) with autonomous feedback. We address this through a learned progress estimator providing dense feedback, and critically, we design our framework to ``tame'' this inherently noisy signal via two mechanisms: (1) an accumulative progress estimation mechanism smoothing noisy point-wise estimates, and (2) a progressive horizon extension strategy enabling gradual policy evolution. EVOLVE-VLA achieves substantial gains: +8.6\\% on long-horizon tasks, +22.0\\% in 1-shot learning, and enables cross-task generalization -- achieving 20.8\\% success on unseen tasks without task-specific demonstrations training (vs. 0\\% for pure SFT). Qualitative analysis reveals emergent capabilities absent in demonstrations, including error recovery and novel strategies. This work represents a critical step toward VLAs that truly learn and adapt, moving beyond static imitation toward continuous self-improvements.",
    "translation": "标题：EVOLVE-VLA：基于环境反馈的视觉-语言-动作模型测试时训练框架\n\n摘要：实现真正自适应的具身智能需要智能体不仅通过模仿静态演示进行学习，更能通过环境交互持续改进，这类似于人类通过实践掌握技能的方式。视觉-语言-动作模型通过利用大型语言模型推动了机器人操作的发展，但其根本上仍受限于监督微调范式：每个任务需要数百次演示、僵化地记忆轨迹，且在部署条件偏离训练时无法适应。本文提出EVOLVE-VLA，一种测试时训练框架，使视觉-语言-动作模型能够通过环境交互持续适应，且仅需极少甚至无需任务特定演示。核心技术挑战在于用自主反馈替代测试时无法获取的预设奖励信号。我们通过设计可提供密集反馈的学习型进度估计器来解决此问题，并特别设计双重机制来“驯服”这种固有噪声信号：（1）累积进度估计机制平滑噪声点估计；（2）渐进式规划范围扩展策略实现策略逐步演化。EVOLVE-VLA取得显著性能提升：长时序任务提升8.6%，单样本学习提升22.0%，并实现跨任务泛化——在未见过且未经任务特定演示训练的任务上达到20.8%成功率（纯监督微调方法为0%）。定性分析揭示了演示中未出现的新兴能力，包括错误恢复和新策略生成。这项研究标志着视觉-语言-动作模型向真正学习与适应能力迈出关键一步，从静态模仿转向持续自我改进。",
    "url": "https://huggingface.co/papers/2512.14666",
    "arxiv_url": "https://arxiv.org/abs/2512.14666"
  },
  {
    "title": "TAT: Task-Adaptive Transformer for All-in-One Medical Image Restoration",
    "summary": "Medical image restoration (MedIR) aims to recover high-quality medical images from their low-quality counterparts. Recent advancements in MedIR have focused on All-in-One models capable of simultaneously addressing multiple different MedIR tasks. However, due to significant differences in both modality and degradation types, using a shared model for these diverse tasks requires careful consideration of two critical inter-task relationships: task interference, which occurs when conflicting gradient update directions arise across tasks on the same parameter, and task imbalance, which refers to uneven optimization caused by varying learning difficulties inherent to each task. To address these challenges, we propose a task-adaptive Transformer (TAT), a novel framework that dynamically adapts to different tasks through two key innovations. First, a task-adaptive weight generation strategy is introduced to mitigate task interference by generating task-specific weight parameters for each task, thereby eliminating potential gradient conflicts on shared weight parameters. Second, a task-adaptive loss balancing strategy is introduced to dynamically adjust loss weights based on task-specific learning difficulties, preventing task domination or undertraining. Extensive experiments demonstrate that our proposed TAT achieves state-of-the-art performance in three MedIR tasks--PET synthesis, CT denoising, and MRI super-resolution--both in task-specific and All-in-One settings. Code is available at https://github.com/Yaziwel/TAT.",
    "translation": "标题：TAT：面向一体化医学图像复原的任务自适应Transformer\n\n摘要：医学图像复原（MedIR）旨在从低质量医学图像中恢复高质量图像。当前MedIR领域的研究进展主要集中于能够同时处理多种不同MedIR任务的一体化模型。然而，由于模态类型与退化类型存在显著差异，使用共享模型处理这些多样化任务时，必须审慎考虑两种关键的任务间关系：其一是任务干扰，即不同任务对同一参数产生冲突的梯度更新方向；其二是任务失衡，即各任务固有学习难度差异导致的优化不均衡问题。为应对这些挑战，我们提出一种任务自适应Transformer（TAT）框架，该创新框架通过两项关键技术实现动态任务适应。首先，我们引入任务自适应权重生成策略，通过为每个任务生成专属权重参数，消除共享权重参数上潜在的梯度冲突，从而缓解任务干扰。其次，我们提出任务自适应损失平衡策略，根据任务特定学习难度动态调整损失权重，防止某些任务主导训练过程或训练不足。大量实验表明，我们提出的TAT模型在PET合成、CT去噪和MRI超分辨率三项MedIR任务中，无论是针对特定任务还是一体化设置，均取得了最先进的性能。代码已发布于https://github.com/Yaziwel/TAT。",
    "url": "https://huggingface.co/papers/2512.14550",
    "arxiv_url": "https://arxiv.org/abs/2512.14550"
  },
  {
    "title": "Zoom-Zero: Reinforced Coarse-to-Fine Video Understanding via Temporal Zoom-in",
    "summary": "Grounded video question answering (GVQA) aims to localize relevant temporal segments in videos and generate accurate answers to a given question; however, large video-language models (LVLMs) exhibit limited temporal awareness. Although existing approaches based on Group Relative Policy Optimization (GRPO) attempt to improve temporal grounding, they still struggle to faithfully ground their answers in the relevant video evidence, leading to temporal mislocalization and hallucinations. In this work, we present Zoom-Zero, a coarse-to-fine framework that first localizes query-relevant segments and then temporally zooms into the most salient frames for finer-grained visual verification. Our method addresses the limits of GRPO for the GVQA task with two key innovations: (i) a zoom-in accuracy reward that validates the fidelity of temporal grounding prediction and facilitates fine-grained visual verification on grounded frames; (ii) token-selective credit assignment, which attributes rewards to the tokens responsible for temporal localization or answer generation, mitigating GRPO's issue in handling multi-faceted reward signals. Our proposed method advances grounded video question answering, improving temporal grounding by 5.2\\% on NExT-GQA and 4.6\\% on ReXTime, while also enhancing average answer accuracy by 2.4\\%. Additionally, the coarse-to-fine zoom-in during inference further benefits long-form video understanding by preserving critical visual details without compromising global context, yielding an average improvement of 6.4\\% on long-video benchmarks.",
    "translation": "标题：Zoom-Zero：通过时序局部放大实现从粗到细的强化视频理解\n\n摘要：基于视频的问答任务旨在定位视频中的相关时序片段并针对给定问题生成准确答案；然而，现有的大规模视频-语言模型在时序感知能力上存在局限。尽管基于群体相对策略优化的现有方法尝试改进时序定位效果，但其答案仍难以忠实锚定于相关视频证据，导致时序定位偏差与幻觉生成。本研究提出Zoom-Zero框架，采用从粗到细的处理流程：首先定位查询相关片段，继而通过时序放大机制聚焦于最具信息量的关键帧进行细粒度视觉验证。本方法通过两项关键创新突破群体相对策略优化在基于视频的问答任务中的局限：（1）引入放大精度奖励机制，验证时序定位预测的可靠性，并促进对定位帧的细粒度视觉验证；（2）设计基于词元的选择性信用分配策略，将奖励精准归因于负责时序定位或答案生成的关键词元，从而缓解群体相对策略优化在处理多维度奖励信号时的固有缺陷。实验表明，所提方法显著推进了基于视频的问答任务的发展：在NExT-GQA数据集上时序定位精度提升5.2%，在ReXTime数据集上提升4.6%，同时平均答案准确率提高2.4%。此外，推理过程中采用的从粗到细放大机制，能在保持全局上下文的前提下保留关键视觉细节，显著提升长视频理解能力，在长视频基准测试中平均获得6.4%的性能提升。",
    "url": "https://huggingface.co/papers/2512.14273",
    "arxiv_url": "https://arxiv.org/abs/2512.14273"
  },
  {
    "title": "Efficient-DLM: From Autoregressive to Diffusion Language Models, and Beyond in Speed",
    "summary": "Diffusion language models (dLMs) have emerged as a promising paradigm that enables parallel, non-autoregressive generation, but their learning efficiency lags behind that of autoregressive (AR) language models when trained from scratch. To this end, we study AR-to-dLM conversion to transform pretrained AR models into efficient dLMs that excel in speed while preserving AR models' task accuracy. We achieve this by identifying limitations in the attention patterns and objectives of existing AR-to-dLM methods and then proposing principles and methodologies for more effective AR-to-dLM conversion. Specifically, we first systematically compare different attention patterns and find that maintaining pretrained AR weight distributions is critical for effective AR-to-dLM conversion. As such, we introduce a continuous pretraining scheme with a block-wise attention pattern, which remains causal across blocks while enabling bidirectional modeling within each block. We find that this approach can better preserve pretrained AR models' weight distributions than fully bidirectional modeling, in addition to its known benefit of enabling KV caching, and leads to a win-win in accuracy and efficiency. Second, to mitigate the training-test gap in mask token distributions (uniform vs. highly left-to-right), we propose a position-dependent token masking strategy that assigns higher masking probabilities to later tokens during training to better mimic test-time behavior. Leveraging this framework, we conduct extensive studies of dLMs' attention patterns, training dynamics, and other design choices, providing actionable insights into scalable AR-to-dLM conversion. These studies lead to the Efficient-DLM family, which outperforms state-of-the-art AR models and dLMs, e.g., our Efficient-DLM 8B achieves +5.4%/+2.7% higher accuracy with 4.5x/2.7x higher throughput compared to Dream 7B and Qwen3 4B, respectively.",
    "translation": "标题：Efficient-DLM：从自回归到扩散语言模型，以及更快的速度\n\n摘要：扩散语言模型（dLM）作为一种支持并行、非自回归生成的新兴范式展现出巨大潜力，但其从头开始训练时的学习效率仍落后于自回归（AR）语言模型。为此，我们研究AR到dLM的转换方法，旨在将预训练的AR模型转化为高效的dLM，在保持AR模型任务准确性的同时实现更快的生成速度。我们通过分析现有AR-to-dLM方法在注意力模式和训练目标上的局限性，提出了更有效的转换原则与方法。具体而言，我们首先系统比较了不同的注意力模式，发现保持预训练AR模型的权重分布对有效转换至关重要。因此，我们引入了一种采用块状注意力模式的持续预训练方案，该方案在块间保持因果性，同时在块内实现双向建模。我们发现，与完全双向建模相比，这种方法不仅能更好地保留预训练AR模型的权重分布，还具备已知的KV缓存优势，从而在准确性和效率上实现双赢。其次，为缓解掩码标记分布（训练时的均匀分布与推理时的高度从左到右分布）之间的训练-测试差距，我们提出了一种位置相关的标记掩码策略，在训练中对靠后的标记赋予更高的掩码概率，以更好地模拟推理时的行为。基于此框架，我们对dLM的注意力模式、训练动态及其他设计选择进行了广泛研究，为可扩展的AR-to-dLM转换提供了实用见解。这些研究最终形成了Efficient-DLM模型系列，其在性能上超越了当前最先进的AR模型和dLM。例如，我们的Efficient-DLM 8B模型相比Dream 7B和Qwen3 4B，在准确率上分别提升5.4%和2.7%，同时吞吐量提高4.5倍和2.7倍。",
    "url": "https://huggingface.co/papers/2512.14067",
    "arxiv_url": "https://arxiv.org/abs/2512.14067"
  },
  {
    "title": "Janus: Disaggregating Attention and Experts for Scalable MoE Inference",
    "summary": "Large Mixture-of-Experts (MoE) model inference is challenging due to high resource demands and dynamic workloads. Existing solutions often deploy the entire model as a single monolithic unit, which applies a unified resource configuration to both attention and expert modules despite their different requirements, leading to limited scalability and resource inefficiency. In this paper, we propose Janus, a scalable MoE inference system that disaggregates attention and experts on separate GPU sub-clusters, enabling each module to be managed and scaled independently. Janus incorporates three key designs for efficient, disaggregated MoE inference. First, it proposes an adaptive two-phase communication scheme that exploits intra- and inter-node bandwidth hierarchies for low-latency data exchange. Second, motivated by the memory-bound nature of MoE modules, Janus introduces a lightweight scheduler and implements it as a GPU kernel to balance the number of activated experts across GPUs at minimal overhead, thereby reducing inference latency. Third, Janus performs fine-grained resource management to dynamically adjust expert placement and independently scale attention and MoE resources to improve overall efficiency. Evaluation shows Janus achieves up to 3.9 higher perGPU throughput than state-of-the-art systems while meeting per-token latency requirements.",
    "translation": "标题：Janus：面向可扩展MoE推理的注意力与专家模块解耦系统\n\n摘要：大规模专家混合模型推理因高资源需求与动态工作负载而面临挑战。现有方案通常将整个模型部署为单一整体单元，对注意力模块和专家模块采用统一的资源配置，忽视了二者不同的计算特性，导致可扩展性受限与资源利用率低下。本文提出Janus——一种可扩展的MoE推理系统，通过将注意力模块与专家模块解耦部署至独立的GPU子集群，实现各模块的独立管理与弹性扩展。Janus包含三项关键设计以实现高效解耦推理：首先，提出自适应两阶段通信机制，利用节点内与节点间带宽层级实现低延迟数据交换；其次，针对MoE模块的内存访问密集型特性，设计轻量级调度器并以GPU内核形式实现，以最小开销在GPU间均衡激活专家数量，从而降低推理延迟；最后，实施细粒度资源管理，动态调整专家分布并独立扩展注意力与MoE计算资源，提升整体效率。实验表明，在满足单令牌延迟要求的前提下，Janus相比现有最优系统可实现最高3.9倍的每GPU吞吐量提升。\n\n摘要：大规模专家混合模型推理因高资源需求与动态工作负载而面临挑战。现有方案通常将整个模型部署为单一整体单元，对注意力模块和专家模块采用统一的资源配置，忽视了二者不同的计算特性，导致可扩展性受限与资源利用率低下。本文提出Janus——一种可扩展的MoE推理系统，通过将注意力模块与专家模块解耦部署至独立的GPU子集群，实现各模块的独立管理与弹性扩展。Janus包含三项关键设计以实现高效解耦推理：首先，提出自适应两阶段通信机制，利用节点内与节点间带宽层级实现低延迟数据交换；其次，针对MoE模块的内存访问密集型特性，设计轻量级调度器并以GPU内核形式实现，以最小开销在GPU间均衡激活专家数量，从而降低推理延迟；最后，实施细粒度资源管理，动态调整专家分布并独立扩展注意力与MoE计算资源，提升整体效率。实验表明，在满足单令牌延迟要求的前提下，Janus相比现有最优系统可实现最高3.9倍的每GPU吞吐量提升。",
    "url": "https://huggingface.co/papers/2512.13525",
    "arxiv_url": "https://arxiv.org/abs/2512.13525"
  },
  {
    "title": "RePo: Language Models with Context Re-Positioning",
    "summary": "In-context learning is fundamental to modern Large Language Models (LLMs); however, prevailing architectures impose a rigid and fixed contextual structure by assigning linear or constant positional indices. Drawing on Cognitive Load Theory (CLT), we argue that this uninformative structure increases extraneous cognitive load, consuming finite working memory capacity that should be allocated to deep reasoning and attention allocation. To address this, we propose RePo, a novel mechanism that reduces extraneous load via context re-positioning. Unlike standard approaches, RePo utilizes a differentiable module, f_φ, to assign token positions that capture contextual dependencies, rather than replying on pre-defined integer range. By continually pre-training on the OLMo-2 1B backbone, we demonstrate that RePo significantly enhances performance on tasks involving noisy contexts, structured data, and longer context length, while maintaining competitive performance on general short-context tasks. Detailed analysis reveals that RePo successfully allocate higher attention to distant but relevant information, assign positions in dense and non-linear space, and capture the intrinsic structure of the input context. Our code is available at https://github.com/SakanaAI/repo.",
    "translation": "标题：RePo：基于上下文重定位的语言模型\n\n摘要：上下文学习是现代大语言模型（LLM）的基础能力，然而主流架构通过分配线性或恒定的位置索引，强制设定了僵化且固定的上下文结构。基于认知负荷理论（CLT），我们认为这种缺乏信息量的结构会增加外部认知负荷，消耗本应用于深度推理与注意力分配的有限工作记忆容量。为解决这一问题，我们提出RePo——一种通过上下文重定位来降低外部认知负荷的新机制。与标准方法不同，RePo采用可微分模块 f_φ 来分配能够捕捉上下文依赖关系的词元位置，而非依赖预定义的整数范围。通过在OLMo-2 1B骨干模型上进行持续预训练，我们证明RePo在处理含噪声上下文、结构化数据及长上下文任务时性能显著提升，同时在通用短上下文任务上保持竞争力。深入分析表明，RePo能成功对远距离相关信息分配更高注意力，在稠密非线性空间中定位位置，并有效捕捉输入上下文的内在结构。代码已开源：https://github.com/SakanaAI/repo。",
    "url": "https://huggingface.co/papers/2512.14391",
    "arxiv_url": "https://arxiv.org/abs/2512.14391"
  },
  {
    "title": "JMMMU-Pro: Image-based Japanese Multi-discipline Multimodal Understanding Benchmark via Vibe Benchmark Construction",
    "summary": "This paper introduces JMMMU-Pro, an image-based Japanese Multi-discipline Multimodal Understanding Benchmark, and Vibe Benchmark Construction, a scalable construction method. Following the evolution from MMMU to MMMU-Pro, JMMMU-Pro extends JMMMU by composing the question image and question text into a single image, thereby creating a benchmark that requires integrated visual-textual understanding through visual perception. To build JMMMU-Pro, we propose Vibe Benchmark Construction, a methodology in which an image generative model (e.g., Nano Banana Pro) produces candidate visual questions, and humans verify the outputs and, when necessary, regenerate with adjusted prompts to ensure quality. By leveraging Nano Banana Pro's highly realistic image generation capabilities and its ability to embed clean Japanese text, we construct a high-quality benchmark at low cost, covering a wide range of background and layout designs. Experimental results show that all open-source LMMs struggle substantially with JMMMU-Pro, underscoring JMMMU-Pro as an important benchmark for guiding future efforts in the open-source community. We believe that JMMMU-Pro provides a more rigorous evaluation tool for assessing the Japanese capabilities of LMMs and that our Vibe Benchmark Construction also offers an efficient guideline for future development of image-based VQA benchmarks.",
    "translation": "标题：JMMMU-Pro：基于图像的日本多学科多模态理解基准与Vibe基准构建方法\n\n摘要：本文提出JMMMU-Pro——一个基于图像的日本多学科多模态理解基准，以及可扩展的构建方法Vibe Benchmark Construction。延续从MMMU到MMMU-Pro的演进路径，JMMMU-Pro通过将问题图像与问题文本融合为单一图像的方式扩展了JMMMU基准，从而构建出需要借助视觉感知实现图文融合理解的评估体系。为构建JMMMU-Pro，我们提出了Vibe Benchmark Construction方法：该方法利用图像生成模型（如Nano Banana Pro）生成候选视觉问题，由人工验证输出结果并在必要时通过调整提示词重新生成，以此保障数据质量。借助Nano Banana Pro高度逼真的图像生成能力及其对日文文本的清晰嵌入特性，我们以较低成本构建了覆盖多样化背景与版式设计的高质量基准。实验结果表明，所有开源大型多模态模型在JMMMU-Pro上均表现欠佳，这凸显了该基准对开源社区未来发展的指导价值。我们相信JMMMU-Pro为评估大型多模态模型的日语能力提供了更严谨的工具，同时Vibe Benchmark Construction方法也为未来基于图像的视觉问答基准开发提供了高效的建设指南。",
    "url": "https://huggingface.co/papers/2512.14620",
    "arxiv_url": "https://arxiv.org/abs/2512.14620"
  },
  {
    "title": "MobileWorldBench: Towards Semantic World Modeling For Mobile Agents",
    "summary": "World models have shown great utility in improving the task performance of embodied agents. While prior work largely focuses on pixel-space world models, these approaches face practical limitations in GUI settings, where predicting complex visual elements in future states is often difficult. In this work, we explore an alternative formulation of world modeling for GUI agents, where state transitions are described in natural language rather than predicting raw pixels. First, we introduce MobileWorldBench, a benchmark that evaluates the ability of vision-language models (VLMs) to function as world models for mobile GUI agents. Second, we release MobileWorld, a large-scale dataset consisting of 1.4M samples, that significantly improves the world modeling capabilities of VLMs. Finally, we propose a novel framework that integrates VLM world models into the planning framework of mobile agents, demonstrating that semantic world models can directly benefit mobile agents by improving task success rates. The code and dataset is available at https://github.com/jacklishufan/MobileWorld",
    "translation": "标题：MobileWorldBench：面向移动智能体的语义世界建模研究\n\n摘要：世界模型在提升具身智能体任务性能方面展现出显著效用。现有研究主要集中于像素空间世界模型，但此类方法在图形用户界面环境中面临实际限制——预测未来状态中的复杂视觉元素往往较为困难。本研究探索了面向图形用户界面智能体的世界建模新范式，通过自然语言描述状态转换而非直接预测原始像素。首先，我们提出MobileWorldBench基准测试，用于评估视觉语言模型作为移动图形用户界面智能体世界模型的性能表现。其次，我们发布了包含140万样本的大规模数据集MobileWorld，该数据集显著提升了视觉语言模型的世界建模能力。最后，我们提出一种创新框架，将视觉语言世界模型集成到移动智能体的规划架构中，证明语义世界模型可通过提高任务成功率直接赋能移动智能体。代码与数据集已公开于https://github.com/jacklishufan/MobileWorld",
    "url": "https://huggingface.co/papers/2512.14014",
    "arxiv_url": "https://arxiv.org/abs/2512.14014"
  },
  {
    "title": "Comparative Analysis of LLM Abliteration Methods: A Cross-Architecture Evaluation",
    "summary": "Safety alignment mechanisms in large language models prevent responses to harmful queries through learned refusal behavior, yet these same mechanisms impede legitimate research applications including cognitive modeling, adversarial testing, and security analysis. While abliteration techniques enable surgical removal of refusal representations through directional orthogonalization, the relative effectiveness of available implementations remains uncharacterized. This study evaluates four abliteration tools (Heretic, DECCP, ErisForge, FailSpy) across sixteen instruction-tuned models (7B-14B parameters), reporting tool compatibility on all 16 models and quantitative metrics on subsets dictated by tool support. Single-pass methods demonstrated superior capability preservation on the benchmarked subset (avg GSM8K change across three models: ErisForge -0.28 pp; DECCP -0.13 pp), while Bayesian-optimized abliteration produced variable distribution shift (KL divergence: 0.043-1.646) with model-dependent capability impact. These findings provide researchers with evidence-based selection criteria for abliteration tool deployment across diverse model architectures. The principal finding indicates that mathematical reasoning capabilities exhibit the highest sensitivity to abliteration interventions, with GSM8K change ranging from +1.51 pp to -18.81 pp (-26.5% relative) depending on tool selection and model architecture.",
    "translation": "标题：大语言模型拒绝机制消除方法的比较分析：跨架构评估\n\n摘要：大语言模型中的安全对齐机制通过习得的拒绝行为防止对有害查询作出回应，但这些机制同时也阻碍了包括认知建模、对抗性测试与安全分析在内的合法研究应用。虽然消除技术能够通过定向正交化手术式移除拒绝表征，但现有实施方案的相对有效性尚未得到系统评估。本研究在十六个指令微调模型（70亿至140亿参数）上评估了四种消除工具（Heretic、DECCP、ErisForge、FailSpy），报告了全部16个模型的工具兼容性及工具支持子集的量化指标。在基准测试子集中，单次消除方法展现出更优的能力保持性（三个模型的GSM8K平均变化：ErisForge -0.28个百分点；DECCP -0.13个百分点），而贝叶斯优化消除则产生可变的分布偏移（KL散度：0.043-1.646）及模型依赖的能力影响。这些发现为研究者在不同模型架构中部署消除工具提供了基于证据的选择标准。核心结果表明，数学推理能力对消除干预表现出最高敏感性，GSM8K变化范围从+1.51个百分点到-18.81个百分点（相对变化-26.5%），具体取决于工具选择与模型架构。",
    "url": "https://huggingface.co/papers/2512.13655",
    "arxiv_url": "https://arxiv.org/abs/2512.13655"
  },
  {
    "title": "TraPO: A Semi-Supervised Reinforcement Learning Framework for Boosting LLM Reasoning",
    "summary": "Reinforcement learning with verifiable rewards (RLVR) has proven effective in training large reasoning models (LRMs) by leveraging answer-verifiable signals to guide policy optimization, which, however, suffers from high annotation costs. To alleviate this problem, recent work has explored unsupervised RLVR methods that derive rewards solely from the model's internal consistency, such as through entropy and majority voting. While seemingly promising, these methods often suffer from model collapse in the later stages of training, which may arise from the reinforcement of incorrect reasoning patterns in the absence of external supervision. In this work, we investigate a novel semi-supervised RLVR paradigm that utilizes a small labeled set to guide RLVR training on unlabeled samples. Our key insight is that supervised rewards are essential for stabilizing consistency-based training on unlabeled samples, ensuring that only reasoning patterns verified on labeled instances are incorporated into RL training. Technically, we propose an effective policy optimization algorithm, TraPO, that identifies reliable unlabeled samples by matching their learning trajectory similarity to labeled ones. Building on this, TraPO achieves remarkable data efficiency and strong generalization on six widely used mathematical reasoning benchmarks (AIME24/25, AMC, MATH-500, Minerva, and Olympiad) and three out-of-distribution tasks (ARC-c, GPQA-diamond, and MMLU-pro). With only 1K labeled and 3K unlabeled samples, TraPO reaches 42.6% average accuracy, surpassing the best unsupervised method trained on 45K unlabeled samples (38.3%). Notably, when using 4K labeled and 12K unlabeled samples, TraPO even outperforms the fully supervised model trained on the full 45K labeled samples on all benchmarks, while using only 10% of the labeled data. The code is available via https://github.com/ShenzhiYang2000/TRAPO.",
    "translation": "标题：TraPO：一种提升大语言模型推理能力的半监督强化学习框架\n\n摘要：基于可验证奖励的强化学习（RLVR）通过利用答案可验证信号指导策略优化，在训练大型推理模型（LRMs）方面已被证明是有效的，但这种方法通常面临高昂的标注成本。为缓解此问题，近期研究探索了无监督RLVR方法，仅通过模型内部一致性（如熵和多数投票）推导奖励。尽管这些方法看似前景良好，但在训练后期常出现模型崩溃现象，这可能源于缺乏外部监督时错误推理模式被强化。本研究提出一种新颖的半监督RLVR范式，利用少量标注样本来指导对未标注样本的RLVR训练。我们的核心观点是：监督奖励对于稳定基于一致性的未标注样本训练至关重要，可确保仅将在标注实例上验证过的推理模式纳入强化学习训练。技术上，我们提出一种高效的策略优化算法TraPO，该算法通过匹配未标注样本与标注样本的学习轨迹相似性来识别可靠的未标注样本。在此基础上，TraPO在六个广泛使用的数学推理基准（AIME24/25、AMC、MATH-500、Minerva和Olympiad）和三个分布外任务（ARC-c、GPQA-diamond和MMLU-pro）上实现了显著的数据效率和强大的泛化能力。仅使用1K标注样本和3K未标注样本，TraPO即达到42.6%的平均准确率，超越了在45K未标注样本上训练的最佳无监督方法（38.3%）。值得注意的是，当使用4K标注样本和12K未标注样本时，TraPO在所有基准测试中甚至优于使用全部45K标注样本训练的完全监督模型，而标注数据用量仅为其10%。代码可通过 https://github.com/ShenzhiYang2000/TRAPO 获取。",
    "url": "https://huggingface.co/papers/2512.13106",
    "arxiv_url": "https://arxiv.org/abs/2512.13106"
  },
  {
    "title": "UAGLNet: Uncertainty-Aggregated Global-Local Fusion Network with Cooperative CNN-Transformer for Building Extraction",
    "summary": "Building extraction from remote sensing images is a challenging task due to the complex structure variations of the buildings. Existing methods employ convolutional or self-attention blocks to capture the multi-scale features in the segmentation models, while the inherent gap of the feature pyramids and insufficient global-local feature integration leads to inaccurate, ambiguous extraction results. To address this issue, in this paper, we present an Uncertainty-Aggregated Global-Local Fusion Network (UAGLNet), which is capable to exploit high-quality global-local visual semantics under the guidance of uncertainty modeling. Specifically, we propose a novel cooperative encoder, which adopts hybrid CNN and transformer layers at different stages to capture the local and global visual semantics, respectively. An intermediate cooperative interaction block (CIB) is designed to narrow the gap between the local and global features when the network becomes deeper. Afterwards, we propose a Global-Local Fusion (GLF) module to complementarily fuse the global and local representations. Moreover, to mitigate the segmentation ambiguity in uncertain regions, we propose an Uncertainty-Aggregated Decoder (UAD) to explicitly estimate the pixel-wise uncertainty to enhance the segmentation accuracy. Extensive experiments demonstrate that our method achieves superior performance to other state-of-the-art methods. Our code is available at https://github.com/Dstate/UAGLNet",
    "translation": "标题：UAGLNet：基于CNN-Transformer协同机制的不确定性聚合全局-局部融合网络用于建筑物提取\n\n摘要：由于建筑物结构复杂多变，从遥感图像中提取建筑物是一项具有挑战性的任务。现有方法通常在分割模型中采用卷积或自注意力模块来捕获多尺度特征，但特征金字塔之间的固有差异以及全局与局部特征融合不足，导致提取结果存在不准确和模糊的问题。为解决这一问题，本文提出一种不确定性聚合的全局-局部融合网络（UAGLNet），该网络能够在不确定性建模的指导下有效利用高质量的全局与局部视觉语义。具体而言，我们设计了一种新型协同编码器，在不同阶段分别采用混合CNN层与Transformer层以捕获局部和全局视觉语义。同时设计了中间协同交互模块（CIB），以在网络加深时缩小局部与全局特征之间的差异。随后，我们提出全局-局部融合模块（GLF），以互补方式融合全局与局部特征表示。此外，为降低不确定区域的分割模糊性，本文提出不确定性聚合解码器（UAD），通过显式估计像素级不确定性来提升分割精度。大量实验表明，本方法相比现有先进方法具有更优越的性能。代码已开源：https://github.com/Dstate/UAGLNet",
    "url": "https://huggingface.co/papers/2512.12941",
    "arxiv_url": "https://arxiv.org/abs/2512.12941"
  },
  {
    "title": "S2D: Sparse-To-Dense Keymask Distillation for Unsupervised Video Instance Segmentation",
    "summary": "In recent years, the state-of-the-art in unsupervised video instance segmentation has heavily relied on synthetic video data, generated from object-centric image datasets such as ImageNet. However, video synthesis by artificially shifting and scaling image instance masks fails to accurately model realistic motion in videos, such as perspective changes, movement by parts of one or multiple instances, or camera motion. To tackle this issue, we propose an unsupervised video instance segmentation model trained exclusively on real video data. We start from unsupervised instance segmentation masks on individual video frames. However, these single-frame segmentations exhibit temporal noise and their quality varies through the video. Therefore, we establish temporal coherence by identifying high-quality keymasks in the video by leveraging deep motion priors. The sparse keymask pseudo-annotations are then used to train a segmentation model for implicit mask propagation, for which we propose a Sparse-To-Dense Distillation approach aided by a Temporal DropLoss. After training the final model on the resulting dense labelset, our approach outperforms the current state-of-the-art across various benchmarks.",
    "translation": "标题：S2D：基于稀疏到稠密关键掩码蒸馏的无监督视频实例分割方法\n\n摘要：近年来，无监督视频实例分割领域的最先进方法严重依赖于从ImageNet等以物体为中心的图像数据集生成的合成视频数据。然而，通过人工平移和缩放图像实例掩码生成的视频合成方法，难以准确模拟视频中真实的运动模式，例如视角变化、单个或多个实例部件的运动或相机运动。为解决这一问题，我们提出了一种完全基于真实视频数据训练的无监督视频实例分割模型。该方法从单帧视频的无监督实例分割掩码出发，但这些单帧分割结果存在时序噪声，且其质量在视频序列中波动较大。为此，我们通过利用深度运动先验识别视频中的高质量关键掩码，从而建立时序一致性。随后，这些稀疏的关键掩码伪标注被用于训练一个隐式掩码传播的分割模型，为此我们提出了一种结合时序丢弃损失（Temporal DropLoss）的稀疏到稠密蒸馏方法。在生成的稠密标签集上训练最终模型后，本方法在多项基准测试中均超越了当前最先进的技术水平。",
    "url": "https://huggingface.co/papers/2512.14440",
    "arxiv_url": "https://arxiv.org/abs/2512.14440"
  },
  {
    "title": "Unveiling User Perceptions in the Generative AI Era: A Sentiment-Driven Evaluation of AI Educational Apps' Role in Digital Transformation of e-Teaching",
    "summary": "The rapid integration of generative artificial intelligence into education has driven digital transformation in e-teaching, yet user perceptions of AI educational apps remain underexplored. This study performs a sentiment-driven evaluation of user reviews from top AI ed-apps on the Google Play Store to assess efficacy, challenges, and pedagogical implications. Our pipeline involved scraping app data and reviews, RoBERTa for binary sentiment classification, GPT-4o for key point extraction, and GPT-5 for synthesizing top positive/negative themes. Apps were categorized into seven types (e.g., homework helpers, math solvers, language tools), with overlaps reflecting multifunctional designs. Results indicate predominantly positive sentiments, with homework apps like Edu AI (95.9% positive) and Answer.AI (92.7%) leading in accuracy, speed, and personalization, while language/LMS apps (e.g., Teacher AI at 21.8% positive) lag due to instability and limited features. Positives emphasize efficiency in brainstorming, problem-solving, and engagement; negatives center on paywalls, inaccuracies, ads, and glitches. Trends show that homework helpers outperform specialized tools, highlighting AI's democratizing potential amid risks of dependency and inequity. The discussion proposes future ecosystems with hybrid AI-human models, VR/AR for immersive learning, and a roadmap for developers (adaptive personalization) and policymakers (monetization regulation for inclusivity). This underscores generative AI's role in advancing e-teaching by enabling ethical refinements that foster equitable, innovative environments. The full dataset is available here(https://github.com/erfan-nourbakhsh/GenAI-EdSent).",
    "translation": "标题：生成式AI时代用户感知的揭示：基于情感分析的AI教育应用在数字化教学转型中的作用评估\n\n摘要：生成式人工智能在教育领域的快速融合推动了数字化教学转型，但用户对AI教育应用的感知仍待深入探究。本研究通过对Google Play商店热门AI教育应用的用户评论进行情感驱动式评估，以考察其效能、挑战及教学启示。研究流程包括爬取应用数据与评论、使用RoBERTa进行二元情感分类、GPT-4o提取关键观点，以及GPT-5综合归纳积极/消极主题。应用被分为七类（如作业助手、数学解题工具、语言学习工具等），类别重叠反映了多功能设计趋势。结果显示用户情感以积极为主，其中作业类应用（如Edu AI积极率95.9%、Answer.AI积极率92.7%）在准确性、响应速度与个性化方面表现突出，而语言学习/LMS类应用（如Teacher AI积极率仅21.8%）因系统不稳定和功能局限评价较低。积极评价集中于激发创意、问题解决效率及学习参与度提升；消极评价则聚焦付费门槛、内容误差、广告干扰及技术故障。趋势表明，作业助手类应用表现优于专项工具，凸显了AI在促进教育普惠性的同时，也伴随着依赖性与公平性风险。讨论部分提出未来应发展人机协同混合模型、结合VR/AR的沉浸式学习生态，并为开发者（适应性个性化设计）和政策制定者（促进包容性的商业化监管）规划行动路径。本研究强调生成式AI可通过伦理化改进推动公平创新的教学环境，从而加速数字化教学转型进程。完整数据集详见：https://github.com/erfan-nourbakhsh/GenAI-EdSent。",
    "url": "https://huggingface.co/papers/2512.11934",
    "arxiv_url": "https://arxiv.org/abs/2512.11934"
  },
  {
    "title": "Hierarchical Dataset Selection for High-Quality Data Sharing",
    "summary": "The success of modern machine learning hinges on access to high-quality training data. In many real-world scenarios, such as acquiring data from public repositories or sharing across institutions, data is naturally organized into discrete datasets that vary in relevance, quality, and utility. Selecting which repositories or institutions to search for useful datasets, and which datasets to incorporate into model training are therefore critical decisions, yet most existing methods select individual samples and treat all data as equally relevant, ignoring differences between datasets and their sources. In this work, we formalize the task of dataset selection: selecting entire datasets from a large, heterogeneous pool to improve downstream performance under resource constraints. We propose Dataset Selection via Hierarchies (DaSH), a dataset selection method that models utility at both dataset and group (e.g., collections, institutions) levels, enabling efficient generalization from limited observations. Across two public benchmarks (Digit-Five and DomainNet), DaSH outperforms state-of-the-art data selection baselines by up to 26.2% in accuracy, while requiring significantly fewer exploration steps. Ablations show DaSH is robust to low-resource settings and lack of relevant datasets, making it suitable for scalable and adaptive dataset selection in practical multi-source learning workflows.",
    "translation": "标题：面向高质量数据共享的层次化数据集选择方法\n\n摘要：现代机器学习的成功依赖于高质量训练数据的获取。在许多实际场景中，例如从公共存储库获取数据或跨机构共享数据时，数据通常被组织成离散的数据集，这些数据集在相关性、质量和效用上存在差异。因此，选择搜索哪些存储库或机构以获取有用数据集，以及将哪些数据集纳入模型训练，是至关重要的决策。然而，现有方法大多仅选择单个样本，并将所有数据视为同等相关，忽略了数据集及其来源之间的差异。本研究形式化定义了数据集选择任务：从大规模异构数据池中选择完整的数据集，以在资源约束下提升下游任务性能。我们提出了基于层次结构的数据集选择方法，该方法在数据集和群组（如数据集合、机构）两个层面建模数据效用，从而能够从有限观测中实现高效泛化。在两个公开基准测试（Digit-Five 和 DomainNet）上，DaSH 在准确率上优于现有数据选择基线方法最高达 26.2%，同时所需探索步骤显著减少。消融实验表明，DaSH 在低资源环境和相关数据集缺失的情况下仍保持鲁棒性，使其适用于实际多源学习工作流程中可扩展且自适应的数据集选择。",
    "url": "https://huggingface.co/papers/2512.10952",
    "arxiv_url": "https://arxiv.org/abs/2512.10952"
  },
  {
    "title": "MeViS: A Multi-Modal Dataset for Referring Motion Expression Video Segmentation",
    "summary": "This paper proposes a large-scale multi-modal dataset for referring motion expression video segmentation, focusing on segmenting and tracking target objects in videos based on language description of objects' motions. Existing referring video segmentation datasets often focus on salient objects and use language expressions rich in static attributes, potentially allowing the target object to be identified in a single frame. Such datasets underemphasize the role of motion in both videos and languages. To explore the feasibility of using motion expressions and motion reasoning clues for pixel-level video understanding, we introduce MeViS, a dataset containing 33,072 human-annotated motion expressions in both text and audio, covering 8,171 objects in 2,006 videos of complex scenarios. We benchmark 15 existing methods across 4 tasks supported by MeViS, including 6 referring video object segmentation (RVOS) methods, 3 audio-guided video object segmentation (AVOS) methods, 2 referring multi-object tracking (RMOT) methods, and 4 video captioning methods for the newly introduced referring motion expression generation (RMEG) task. The results demonstrate weaknesses and limitations of existing methods in addressing motion expression-guided video understanding. We further analyze the challenges and propose an approach LMPM++ for RVOS/AVOS/RMOT that achieves new state-of-the-art results. Our dataset provides a platform that facilitates the development of motion expression-guided video understanding algorithms in complex video scenes. The proposed MeViS dataset and the method's source code are publicly available at https://henghuiding.com/MeViS/",
    "translation": "标题：MeViS：面向指代性运动表达视频分割的多模态数据集\n\n摘要：本文提出一个面向指代性运动表达视频分割的大规模多模态数据集，其核心在于依据物体运动语言描述实现视频中目标物体的分割与追踪。现有指代视频分割数据集多聚焦于显著物体，且使用的语言表达富含静态属性特征，使得目标物体可能在单帧图像中即可被识别。此类数据集未能充分强调运动信息在视频与语言中的关键作用。为探索利用运动表达与运动推理线索实现像素级视频理解的可行性，我们提出了MeViS数据集，该数据集包含33,072条人工标注的文本与音频双模态运动表达，涵盖2,006个复杂场景视频中的8,171个目标物体。我们在MeViS支持的4项任务中对15种现有方法进行基准测试，包括6种指代视频目标分割（RVOS）方法、3种音频引导视频目标分割（AVOS）方法、2种指代多目标跟踪（RMOT）方法，以及针对新提出的指代性运动表达生成（RMEG）任务的4种视频描述生成方法。实验结果揭示了现有方法在处理运动表达引导的视频理解任务时存在的不足与局限。我们进一步分析了技术挑战，并提出一种面向RVOS/AVOS/RMOT任务的LMPM++方法，该方法取得了当前最优性能。本数据集为复杂视频场景中运动表达引导的视频理解算法研发提供了平台。MeViS数据集及相关方法源代码已公开于https://henghuiding.com/MeViS/。",
    "url": "https://huggingface.co/papers/2512.10945",
    "arxiv_url": "https://arxiv.org/abs/2512.10945"
  },
  {
    "title": "CoSPlan: Corrective Sequential Planning via Scene Graph Incremental Updates",
    "summary": "Large-scale Vision-Language Models (VLMs) exhibit impressive complex reasoning capabilities but remain largely unexplored in visual sequential planning, i.e., executing multi-step actions towards a goal. Additionally, practical sequential planning often involves non-optimal (erroneous) steps, challenging VLMs to detect and correct such steps. We propose Corrective Sequential Planning Benchmark (CoSPlan) to evaluate VLMs in error-prone, vision-based sequential planning tasks across 4 domains: maze navigation, block rearrangement, image reconstruction,and object reorganization. CoSPlan assesses two key abilities: Error Detection (identifying non-optimal action) and Step Completion (correcting and completing action sequences to reach the goal). Despite using state-of-the-art reasoning techniques such as Chain-of-Thought and Scene Graphs, VLMs (e.g. Intern-VLM and Qwen2) struggle on CoSPlan, failing to leverage contextual cues to reach goals. Addressing this, we propose a novel training-free method, Scene Graph Incremental updates (SGI), which introduces intermediate reasoning steps between the initial and goal states. SGI helps VLMs reason about sequences, yielding an average performance gain of 5.2%. In addition to enhancing reliability in corrective sequential planning, SGI generalizes to traditional planning tasks such as Plan-Bench and VQA.",
    "translation": "标题：CoSPlan：基于场景图增量更新的纠错式序列规划\n\n摘要：大规模视觉语言模型（VLMs）展现出令人瞩目的复杂推理能力，但在视觉序列规划领域——即执行多步动作以实现目标——仍鲜有探索。此外，实际的序列规划常包含非最优（错误）步骤，这对VLM检测与纠正此类步骤的能力提出了挑战。我们提出纠错式序列规划基准（CoSPlan），用于评估VLM在四大领域易出错的视觉序列规划任务中的表现：迷宫导航、积木重排、图像重建和物体重组。CoSPlan评估两项关键能力：错误检测（识别非最优动作）与步骤补全（纠正并完成动作序列以实现目标）。尽管采用了思维链和场景图等先进推理技术，现有VLM（如Intern-VLM与Qwen2）在CoSPlan上表现仍不理想，难以有效利用上下文线索达成目标。为此，我们提出一种无需训练的新方法——场景图增量更新（SGI），该方法在初始状态与目标状态之间引入中间推理步骤。SGI能帮助VLM进行序列推理，平均性能提升达5.2%。除提升纠错式序列规划的可靠性外，SGI还可泛化至传统规划任务（如Plan-Bench和视觉问答任务）。",
    "url": "https://huggingface.co/papers/2512.10342",
    "arxiv_url": "https://arxiv.org/abs/2512.10342"
  },
  {
    "title": "ContextAnyone: Context-Aware Diffusion for Character-Consistent Text-to-Video Generation",
    "summary": "Text-to-video (T2V) generation has advanced rapidly, yet maintaining consistent character identities across scenes remains a major challenge. Existing personalization methods often focus on facial identity but fail to preserve broader contextual cues such as hairstyle, outfit, and body shape, which are critical for visual coherence. We propose ContextAnyone, a context-aware diffusion framework that achieves character-consistent video generation from text and a single reference image. Our method jointly reconstructs the reference image and generates new video frames, enabling the model to fully perceive and utilize reference information. Reference information is effectively integrated into a DiT-based diffusion backbone through a novel Emphasize-Attention module that selectively reinforces reference-aware features and prevents identity drift across frames. A dual-guidance loss combines diffusion and reference reconstruction objectives to enhance appearance fidelity, while the proposed Gap-RoPE positional embedding separates reference and video tokens to stabilize temporal modeling. Experiments demonstrate that ContextAnyone outperforms existing reference-to-video methods in identity consistency and visual quality, generating coherent and context-preserving character videos across diverse motions and scenes. Project page: https://github.com/ziyang1106/ContextAnyone{https://github.com/ziyang1106/ContextAnyone}.",
    "translation": "标题：ContextAnyone：面向角色一致性的文本到视频生成中的上下文感知扩散方法\n\n摘要：文本到视频（T2V）生成技术发展迅速，但在不同场景中保持角色身份一致性仍是一个主要挑战。现有的个性化方法通常侧重于面部特征，却难以保留发型、服装和体型等更广泛的上下文线索，而这些线索对于视觉连贯性至关重要。本文提出ContextAnyone，一种上下文感知扩散框架，能够基于文本和单张参考图像实现角色一致性的视频生成。我们的方法联合重建参考图像并生成新的视频帧，使模型能够充分感知并利用参考信息。通过一种新颖的“强调-注意力”模块，参考信息被有效整合到基于DiT的扩散主干网络中，该模块选择性地增强参考感知特征并防止跨帧身份漂移。双重引导损失结合了扩散和参考重建目标以提升外观保真度，同时提出的Gap-RoPE位置嵌入技术将参考标记与视频标记分离，以稳定时序建模。实验表明，ContextAnyone在身份一致性和视觉质量上优于现有的参考到视频生成方法，能够在多样化的动作和场景中生成连贯且保持上下文信息的角色视频。项目页面：https://github.com/ziyang1106/ContextAnyone。",
    "url": "https://huggingface.co/papers/2512.07328",
    "arxiv_url": "https://arxiv.org/abs/2512.07328"
  }
]