[
  {
    "title": "InfiniDepth: Arbitrary-Resolution and Fine-Grained Depth Estimation with Neural Implicit Fields",
    "summary": "Existing depth estimation methods are fundamentally limited to predicting depth on discrete image grids. Such representations restrict their scalability to arbitrary output resolutions and hinder the geometric detail recovery. This paper introduces InfiniDepth, which represents depth as neural implicit fields. Through a simple yet effective local implicit decoder, we can query depth at continuous 2D coordinates, enabling arbitrary-resolution and fine-grained depth estimation. To better assess our method's capabilities, we curate a high-quality 4K synthetic benchmark from five different games, spanning diverse scenes with rich geometric and appearance details. Extensive experiments demonstrate that InfiniDepth achieves state-of-the-art performance on both synthetic and real-world benchmarks across relative and metric depth estimation tasks, particularly excelling in fine-detail regions. It also benefits the task of novel view synthesis under large viewpoint shifts, producing high-quality results with fewer holes and artifacts.",
    "translation": "标题：InfiniDepth：基于神经隐式场的任意分辨率与细粒度深度估计\n\n摘要：现有的深度估计方法本质上局限于在离散图像网格上预测深度。此类表示形式限制了其向任意输出分辨率的可扩展性，并阻碍了几何细节的恢复。本文提出InfiniDepth，该方法将深度表示为神经隐式场。通过一个简单而有效的局部隐式解码器，我们能够在连续的二维坐标处查询深度，从而实现任意分辨率与细粒度的深度估计。为更好地评估本方法的性能，我们从五款不同游戏中构建了一个高质量的4K合成基准数据集，涵盖具有丰富几何与外观细节的多样化场景。大量实验表明，InfiniDepth在相对深度估计与度量深度估计任务中，无论是合成数据还是真实世界基准测试上均达到了最先进的性能，尤其在精细细节区域表现突出。该方法还有益于大视角变化下的新视角合成任务，能够生成空洞与伪影更少的高质量结果。",
    "url": "https://huggingface.co/papers/2601.03252",
    "arxiv_url": "https://arxiv.org/abs/2601.03252"
  },
  {
    "title": "LTX-2: Efficient Joint Audio-Visual Foundation Model",
    "summary": "Recent text-to-video diffusion models can generate compelling video sequences, yet they remain silent -- missing the semantic, emotional, and atmospheric cues that audio provides. We introduce LTX-2, an open-source foundational model capable of generating high-quality, temporally synchronized audiovisual content in a unified manner. LTX-2 consists of an asymmetric dual-stream transformer with a 14B-parameter video stream and a 5B-parameter audio stream, coupled through bidirectional audio-video cross-attention layers with temporal positional embeddings and cross-modality AdaLN for shared timestep conditioning. This architecture enables efficient training and inference of a unified audiovisual model while allocating more capacity for video generation than audio generation. We employ a multilingual text encoder for broader prompt understanding and introduce a modality-aware classifier-free guidance (modality-CFG) mechanism for improved audiovisual alignment and controllability. Beyond generating speech, LTX-2 produces rich, coherent audio tracks that follow the characters, environment, style, and emotion of each scene -- complete with natural background and foley elements. In our evaluations, the model achieves state-of-the-art audiovisual quality and prompt adherence among open-source systems, while delivering results comparable to proprietary models at a fraction of their computational cost and inference time. All model weights and code are publicly released.",
    "translation": "标题：LTX-2：一种高效的联合视听基础模型\n\n摘要：当前的文本到视频扩散模型能够生成引人入胜的视频序列，但它们始终是“静默”的——缺失了音频所提供的语义、情感与氛围线索。我们推出了LTX-2，这是一个能够以统一方式生成高质量、时间同步的视听内容的开源基础模型。LTX-2采用非对称双流Transformer架构，包含一个140亿参数的视频流和一个50亿参数的音频流，二者通过具有时间位置嵌入的双向视听交叉注意力层以及用于共享时间步条件化的跨模态AdaLN模块进行耦合。该架构在实现统一视听模型高效训练与推理的同时，为视频生成分配了比音频生成更多的计算容量。我们采用多语言文本编码器以增强对提示词的理解，并引入一种模态感知的无分类器引导机制，以提升视听对齐效果与可控性。除了生成语音，LTX-2还能生成丰富、连贯的音频轨道，这些音频跟随场景中的人物、环境、风格和情感变化——并包含自然的背景音与拟音元素。在我们的评估中，该模型在开源系统中实现了最先进的视听质量与提示词遵循度，同时仅以极低的计算成本和推理时间，取得了与专有模型相媲美的结果。所有模型权重与代码均已公开发布。",
    "url": "https://huggingface.co/papers/2601.03233",
    "arxiv_url": "https://arxiv.org/abs/2601.03233"
  },
  {
    "title": "MOSS Transcribe Diarize: Accurate Transcription with Speaker Diarization",
    "summary": "Speaker-Attributed, Time-Stamped Transcription (SATS) aims to transcribe what is said and to precisely determine the timing of each speaker, which is particularly valuable for meeting transcription. Existing SATS systems rarely adopt an end-to-end formulation and are further constrained by limited context windows, weak long-range speaker memory, and the inability to output timestamps. To address these limitations, we present MOSS Transcribe Diarize, a unified multimodal large language model that jointly performs Speaker-Attributed, Time-Stamped Transcription in an end-to-end paradigm. Trained on extensive real wild data and equipped with a 128k context window for up to 90-minute inputs, MOSS Transcribe Diarize scales well and generalizes robustly. Across comprehensive evaluations, it outperforms state-of-the-art commercial systems on multiple public and in-house benchmarks.",
    "translation": "标题：MOSS Transcribe Diarize：具备说话人日志功能的精准转录系统\n\n摘要：说话人归属时间戳转录旨在准确记录语音内容并精确定位每位说话人的发言时间，对会议转录场景具有重要价值。现有系统鲜少采用端到端架构，且普遍受限于上下文窗口狭窄、长程说话人记忆能力薄弱以及无法输出时间戳等问题。为突破这些限制，本研究提出MOSS Transcribe Diarize——一个统一的多模态大语言模型，以端到端方式同步实现说话人归属与时间戳转录。该系统通过海量真实场景数据训练，具备处理长达90分钟音频的128k上下文窗口，展现出优秀的扩展能力和鲁棒泛化性能。在多项公开与内部基准测试中，其综合表现均超越当前最先进的商业系统。",
    "url": "https://huggingface.co/papers/2601.01554",
    "arxiv_url": "https://arxiv.org/abs/2601.01554"
  },
  {
    "title": "SciEvalKit: An Open-source Evaluation Toolkit for Scientific General Intelligence",
    "summary": "We introduce SciEvalKit, a unified benchmarking toolkit designed to evaluate AI models for science across a broad range of scientific disciplines and task capabilities. Unlike general-purpose evaluation platforms, SciEvalKit focuses on the core competencies of scientific intelligence, including Scientific Multimodal Perception, Scientific Multimodal Reasoning, Scientific Multimodal Understanding, Scientific Symbolic Reasoning, Scientific Code Generation, Science Hypothesis Generation and Scientific Knowledge Understanding. It supports six major scientific domains, spanning from physics and chemistry to astronomy and materials science. SciEvalKit builds a foundation of expert-grade scientific benchmarks, curated from real-world, domain-specific datasets, ensuring that tasks reflect authentic scientific challenges. The toolkit features a flexible, extensible evaluation pipeline that enables batch evaluation across models and datasets, supports custom model and dataset integration, and provides transparent, reproducible, and comparable results. By bridging capability-based evaluation and disciplinary diversity, SciEvalKit offers a standardized yet customizable infrastructure to benchmark the next generation of scientific foundation models and intelligent agents. The toolkit is open-sourced and actively maintained to foster community-driven development and progress in AI4Science.",
    "translation": "标题：SciEvalKit：面向科学通用智能的开源评估工具包\n\n摘要：本文介绍SciEvalKit，这是一个统一的基准测试工具包，旨在跨广泛科学学科与任务能力评估面向科学的人工智能模型。与通用评估平台不同，SciEvalKit聚焦于科学智能的核心能力，包括科学多模态感知、科学多模态推理、科学多模态理解、科学符号推理、科学代码生成、科学假设生成以及科学知识理解。该工具包支持从物理、化学到天文学与材料科学等六大主要科学领域。SciEvalKit构建了专家级科学基准体系，其任务均源自真实世界、领域特定的数据集，确保评估内容反映真实的科学挑战。该工具包采用灵活可扩展的评估流程，支持跨模型与数据集的批量评估，允许自定义模型与数据集集成，并提供透明、可复现、可比较的评估结果。通过融合能力导向评估与学科多样性，SciEvalKit为新一代科学基础模型与智能代理的基准测试提供了标准化且可定制的基础架构。本工具包已开源并持续维护，以促进AI4Science领域的社区驱动发展与进步。",
    "url": "https://huggingface.co/papers/2512.22334",
    "arxiv_url": "https://arxiv.org/abs/2512.22334"
  },
  {
    "title": "UniCorn: Towards Self-Improving Unified Multimodal Models through Self-Generated Supervision",
    "summary": "While Unified Multimodal Models (UMMs) have achieved remarkable success in cross-modal comprehension, a significant gap persists in their ability to leverage such internal knowledge for high-quality generation. We formalize this discrepancy as Conduction Aphasia, a phenomenon where models accurately interpret multimodal inputs but struggle to translate that understanding into faithful and controllable synthesis. To address this, we propose UniCorn, a simple yet elegant self-improvement framework that eliminates the need for external data or teacher supervision. By partitioning a single UMM into three collaborative roles: Proposer, Solver, and Judge, UniCorn generates high-quality interactions via self-play and employs cognitive pattern reconstruction to distill latent understanding into explicit generative signals. To validate the restoration of multimodal coherence, we introduce UniCycle, a cycle-consistency benchmark based on a Text to Image to Text reconstruction loop. Extensive experiments demonstrate that UniCorn achieves comprehensive and substantial improvements over the base model across six general image generation benchmarks. Notably, it achieves SOTA performance on TIIF(73.8), DPG(86.8), CompBench(88.5), and UniCycle while further delivering substantial gains of +5.0 on WISE and +6.5 on OneIG. These results highlight that our method significantly enhances T2I generation while maintaining robust comprehension, demonstrating the scalability of fully self-supervised refinement for unified multimodal intelligence.",
    "translation": "标题：UniCorn：通过自生成监督实现自增强统一多模态模型\n\n摘要：尽管统一多模态模型在跨模态理解方面取得了显著成功，但其利用内部知识进行高质量生成的能力仍存在明显不足。我们将这种差异形式化为传导性失语现象，即模型能准确解读多模态输入，却难以将这种理解转化为忠实且可控的合成内容。为此，我们提出UniCorn框架——一种简洁而高效的自增强方法，无需依赖外部数据或教师监督。通过将单一统一多模态模型划分为提议者、求解者和评判者三个协作角色，UniCorn通过自我博弈生成高质量交互，并运用认知模式重构将潜在理解提炼为显式生成信号。为验证多模态连贯性的恢复效果，我们设计了UniCycle基准测试，该测试基于“文本→图像→文本”的重构循环进行周期一致性评估。大量实验表明，UniCorn在六项通用图像生成基准测试中均较基础模型取得全面且显著的提升。特别值得注意的是，该方法在TIIF（73.8）、DPG（86.8）、CompBench（88.5）及UniCycle基准上达到最先进性能，同时在WISE和OneIG基准上分别实现+5.0和+6.5的显著增益。这些结果表明，我们的方法在保持强大理解能力的同时显著提升了文本到图像生成质量，证明了全自监督优化框架对于统一多模态智能系统的可扩展性。",
    "url": "https://huggingface.co/papers/2601.03193",
    "arxiv_url": "https://arxiv.org/abs/2601.03193"
  },
  {
    "title": "NitroGen: An Open Foundation Model for Generalist Gaming Agents",
    "summary": "We introduce NitroGen, a vision-action foundation model for generalist gaming agents that is trained on 40,000 hours of gameplay videos across more than 1,000 games. We incorporate three key ingredients: 1) an internet-scale video-action dataset constructed by automatically extracting player actions from publicly available gameplay videos, 2) a multi-game benchmark environment that can measure cross-game generalization, and 3) a unified vision-action model trained with large-scale behavior cloning. NitroGen exhibits strong competence across diverse domains, including combat encounters in 3D action games, high-precision control in 2D platformers, and exploration in procedurally generated worlds. It transfers effectively to unseen games, achieving up to 52% relative improvement in task success rates over models trained from scratch. We release the dataset, evaluation suite, and model weights to advance research on generalist embodied agents.",
    "translation": "标题：NitroGen：面向通用游戏智能体的开放基础模型\n\n摘要：本文介绍NitroGen——一个面向通用游戏智能体的视觉-动作基础模型，该模型基于超过1000款游戏、总计4万小时的游戏录像进行训练。我们融合了三个核心要素：1）通过自动提取公开游戏录像中的玩家操作构建的互联网规模视频-动作数据集；2）能够衡量跨游戏泛化能力的多游戏基准测试环境；3）采用大规模行为克隆训练的统一视觉-动作模型。NitroGen在多个领域展现出卓越能力，包括3D动作游戏的战斗场景、2D平台游戏的高精度操控，以及程序生成世界的探索任务。该模型能有效迁移至未见过的游戏，相比从头训练的模型在任务成功率上最高可获得52%的相对提升。我们公开数据集、评估套件和模型权重，以推动通用具身智能体的研究发展。",
    "url": "https://huggingface.co/papers/2601.02427",
    "arxiv_url": "https://arxiv.org/abs/2601.02427"
  },
  {
    "title": "SOP: A Scalable Online Post-Training System for Vision-Language-Action Models",
    "summary": "Vision-language-action (VLA) models achieve strong generalization through large-scale pre-training, but real-world deployment requires expert-level task proficiency in addition to broad generality. Existing post-training approaches for VLA models are typically offline, single-robot, or task-specific, limiting effective on-policy adaptation and scalable learning from real-world interaction. We introduce a Scalable Online Post-training (SOP) system that enables online, distributed, multi-task post-training of generalist VLA models directly in the physical world. SOP tightly couples execution and learning through a closed-loop architecture in which a fleet of robots continuously streams on-policy experience and human intervention signals to a centralized cloud learner, and asynchronously receives updated policies. This design supports prompt on-policy correction, scales experience collection through parallel deployment, and preserves generality during adaptation. SOP is agnostic to the choice of post-training algorithm; we instantiate it with both interactive imitation learning (HG-DAgger) and reinforcement learning (RECAP). Across a range of real-world manipulation tasks including cloth folding, box assembly, and grocery restocking, we show that SOP substantially improves the performance of large pretrained VLA models while maintaining a single shared policy across tasks. Effective post-training can be achieved within hours of real-world interaction, and performance scales near-linearly with the number of robots in the fleet. These results suggest that tightly coupling online learning with fleet-scale deployment is instrumental to enabling efficient, reliable, and scalable post-training of generalist robot policies in the physical world.",
    "translation": "标题：SOP：一种可扩展的视觉-语言-动作模型在线后训练系统\n\n摘要：视觉-语言-动作模型通过大规模预训练实现了强大的泛化能力，但在实际部署中，除了广泛的通用性外，还需要具备专家级的任务熟练度。现有的视觉-语言-动作模型后训练方法通常是离线、单机器人或任务特定的，限制了有效的同策略适应和从现实世界交互中进行可扩展学习。我们提出了一种可扩展的在线后训练系统，该系统能够在物理世界中直接对通用视觉-语言-动作模型进行在线、分布式、多任务的后训练。SOP通过闭环架构紧密耦合执行与学习：机器人集群持续将同策略经验与人工干预信号流式传输至中央云端学习器，并异步接收更新后的策略。这一设计支持即时同策略修正，通过并行部署扩展经验收集，并在适应过程中保持通用性。SOP对后训练算法的选择具有无关性；我们通过交互式模仿学习（HG-DAgger）和强化学习（RECAP）两种方式实现了该系统。在包括布料折叠、箱子组装和商品补货等一系列现实世界操作任务中，我们证明SOP能显著提升大型预训练视觉-语言-动作模型的性能，同时跨任务保持单一共享策略。有效的后训练可在数小时的实际交互中实现，且性能随机器人集群规模呈现近线性增长。这些结果表明，将在线学习与集群规模部署紧密耦合，对于在物理世界中实现通用机器人策略的高效、可靠和可扩展后训练具有关键作用。",
    "url": "https://huggingface.co/papers/2601.03044",
    "arxiv_url": "https://arxiv.org/abs/2601.03044"
  },
  {
    "title": "DreamStyle: A Unified Framework for Video Stylization",
    "summary": "Video stylization, an important downstream task of video generation models, has not yet been thoroughly explored. Its input style conditions typically include text, style image, and stylized first frame. Each condition has a characteristic advantage: text is more flexible, style image provides a more accurate visual anchor, and stylized first frame makes long-video stylization feasible. However, existing methods are largely confined to a single type of style condition, which limits their scope of application. Additionally, their lack of high-quality datasets leads to style inconsistency and temporal flicker. To address these limitations, we introduce DreamStyle, a unified framework for video stylization, supporting (1) text-guided, (2) style-image-guided, and (3) first-frame-guided video stylization, accompanied by a well-designed data curation pipeline to acquire high-quality paired video data. DreamStyle is built on a vanilla Image-to-Video (I2V) model and trained using a Low-Rank Adaptation (LoRA) with token-specific up matrices that reduces the confusion among different condition tokens. Both qualitative and quantitative evaluations demonstrate that DreamStyle is competent in all three video stylization tasks, and outperforms the competitors in style consistency and video quality.",
    "translation": "标题：DreamStyle：一种统一的视频风格化框架\n\n摘要：视频风格化作为视频生成模型的重要下游任务，尚未得到充分探索。其输入风格条件通常包括文本、风格图像和已风格化的首帧。每种条件均具有独特优势：文本更具灵活性，风格图像提供更精确的视觉锚点，而风格化首帧则使长视频风格化成为可能。然而，现有方法大多局限于单一类型的风格条件，限制了其应用范围。此外，高质量数据集的缺乏导致风格不一致与时间闪烁问题。为突破这些局限，我们提出DreamStyle——一个支持（1）文本引导、（2）风格图像引导及（3）首帧引导视频风格化的统一框架，并配备精心设计的数据处理流程以获取高质量配对视频数据。DreamStyle基于基础图像到视频（I2V）模型构建，通过采用具有令牌特异性上矩阵的低秩自适应（LoRA）进行训练，有效减少了不同条件令牌间的混淆。定性与定量评估均表明，DreamStyle能够胜任全部三类视频风格化任务，并在风格一致性与视频质量方面优于现有方法。",
    "url": "https://huggingface.co/papers/2601.02785",
    "arxiv_url": "https://arxiv.org/abs/2601.02785"
  },
  {
    "title": "MiMo-V2-Flash Technical Report",
    "summary": "We present MiMo-V2-Flash, a Mixture-of-Experts (MoE) model with 309B total parameters and 15B active parameters, designed for fast, strong reasoning and agentic capabilities. MiMo-V2-Flash adopts a hybrid attention architecture that interleaves Sliding Window Attention (SWA) with global attention, with a 128-token sliding window under a 5:1 hybrid ratio. The model is pre-trained on 27 trillion tokens with Multi-Token Prediction (MTP), employing a native 32k context length and subsequently extended to 256k. To efficiently scale post-training compute, MiMo-V2-Flash introduces a novel Multi-Teacher On-Policy Distillation (MOPD) paradigm. In this framework, domain-specialized teachers (e.g., trained via large-scale reinforcement learning) provide dense and token-level reward, enabling the student model to perfectly master teacher expertise. MiMo-V2-Flash rivals top-tier open-weight models such as DeepSeek-V3.2 and Kimi-K2, despite using only 1/2 and 1/3 of their total parameters, respectively. During inference, by repurposing MTP as a draft model for speculative decoding, MiMo-V2-Flash achieves up to 3.6 acceptance length and 2.6x decoding speedup with three MTP layers. We open-source both the model weights and the three-layer MTP weights to foster open research and community collaboration.",
    "translation": "标题：MiMo-V2-Flash技术报告\n\n摘要：本文提出MiMo-V2-Flash，这是一个采用专家混合架构的模型，总参数量达3090亿，激活参数量为150亿，专为快速、强大的推理与智能体能力而设计。该模型采用混合注意力架构，以5:1的混合比例将滑动窗口注意力与全局注意力交错排列，滑动窗口大小为128个词元。模型通过多词元预测方法在27万亿词元上进行预训练，支持原生32k上下文长度并后续扩展至256k。为实现训练后计算的高效扩展，MiMo-V2-Flash引入了创新的多教师同策略蒸馏范式。在此框架中，领域专用教师模型（例如通过大规模强化学习训练）提供密集的词元级奖励信号，使学生模型能够完全掌握教师专家的能力。尽管总参数量分别仅为DeepSeek-V3.2和Kimi-K2的1/2与1/3，MiMo-V2-Flash的性能仍可与这些顶尖开源权重模型相媲美。在推理阶段，通过将多词元预测机制改造为推测解码的草稿模型，仅使用三层多词元预测结构即可实现最高3.6的接受长度和2.6倍的解码加速。我们同步开源模型权重与三层多词元预测权重，以促进开放研究与社区协作。",
    "url": "https://huggingface.co/papers/2601.02780",
    "arxiv_url": "https://arxiv.org/abs/2601.02780"
  },
  {
    "title": "CogFlow: Bridging Perception and Reasoning through Knowledge Internalization for Visual Mathematical Problem Solving",
    "summary": "Despite significant progress, multimodal large language models continue to struggle with visual mathematical problem solving. Some recent works recognize that visual perception is a bottleneck in visual mathematical reasoning, but their solutions are limited to improving the extraction and interpretation of visual inputs. Notably, they all ignore the key issue of whether the extracted visual cues are faithfully integrated and properly utilized in subsequent reasoning. Motivated by this, we present CogFlow, a novel cognitive-inspired three-stage framework that incorporates a knowledge internalization stage, explicitly simulating the hierarchical flow of human reasoning: perceptionRightarrowinternalizationRightarrowreasoning. Inline with this hierarchical flow, we holistically enhance all its stages. We devise Synergistic Visual Rewards to boost perception capabilities in parametric and semantic spaces, jointly improving visual information extraction from symbols and diagrams. To guarantee faithful integration of extracted visual cues into subsequent reasoning, we introduce a Knowledge Internalization Reward model in the internalization stage, bridging perception and reasoning. Moreover, we design a Visual-Gated Policy Optimization algorithm to further enforce the reasoning is grounded with the visual knowledge, preventing models seeking shortcuts that appear coherent but are visually ungrounded reasoning chains. Moreover, we contribute a new dataset MathCog for model training, which contains samples with over 120K high-quality perception-reasoning aligned annotations. Comprehensive experiments and analysis on commonly used visual mathematical reasoning benchmarks validate the superiority of the proposed CogFlow.",
    "translation": "标题：CogFlow：通过知识内化连接感知与推理的视觉数学问题求解框架\n\n摘要：尽管取得了显著进展，多模态大语言模型在视觉数学问题求解方面仍面临挑战。近期研究认识到视觉感知是视觉数学推理的关键瓶颈，但其解决方案主要局限于改进视觉信息的提取与解读。值得注意的是，这些研究均忽视了一个核心问题：提取的视觉线索是否被忠实整合并有效运用于后续推理过程。基于此，我们提出CogFlow——一种受认知启发的三阶段创新框架，通过引入知识内化阶段，显式模拟人类推理的层次化流程：感知⇒内化⇒推理。遵循这一层次化流程，我们对各阶段进行全面增强。我们设计协同视觉奖励机制，在参数空间与语义空间中提升感知能力，协同改进符号与图表的视觉信息提取。为确保提取的视觉线索能忠实融入后续推理，我们在内化阶段引入知识内化奖励模型，构建感知与推理间的桥梁。此外，我们提出视觉门控策略优化算法，进一步强化推理过程对视觉知识的依赖，防止模型采用表面连贯但缺乏视觉依据的推理捷径。同时，我们构建了包含12万条高质量感知-推理对齐标注样本的新数据集MathCog以支持模型训练。在常用视觉数学推理基准上的综合实验与分析验证了CogFlow框架的优越性。",
    "url": "https://huggingface.co/papers/2601.01874",
    "arxiv_url": "https://arxiv.org/abs/2601.01874"
  },
  {
    "title": "Digital Twin AI: Opportunities and Challenges from Large Language Models to World Models",
    "summary": "Digital twins, as precise digital representations of physical systems, have evolved from passive simulation tools into intelligent and autonomous entities through the integration of artificial intelligence technologies. This paper presents a unified four-stage framework that systematically characterizes AI integration across the digital twin lifecycle, spanning modeling, mirroring, intervention, and autonomous management. By synthesizing existing technologies and practices, we distill a unified four-stage framework that systematically characterizes how AI methodologies are embedded across the digital twin lifecycle: (1) modeling the physical twin through physics-based and physics-informed AI approaches, (2) mirroring the physical system into a digital twin with real-time synchronization, (3) intervening in the physical twin through predictive modeling, anomaly detection, and optimization strategies, and (4) achieving autonomous management through large language models, foundation models, and intelligent agents. We analyze the synergy between physics-based modeling and data-driven learning, highlighting the shift from traditional numerical solvers to physics-informed and foundation models for physical systems. Furthermore, we examine how generative AI technologies, including large language models and generative world models, transform digital twins into proactive and self-improving cognitive systems capable of reasoning, communication, and creative scenario generation. Through a cross-domain review spanning eleven application domains, including healthcare, aerospace, smart manufacturing, robotics, and smart cities, we identify common challenges related to scalability, explainability, and trustworthiness, and outline directions for responsible AI-driven digital twin systems.",
    "translation": "标题：数字孪生人工智能：从大语言模型到世界模型的机遇与挑战\n\n摘要：数字孪生作为物理系统的精确数字化表征，通过人工智能技术的融合，已从被动仿真工具演变为具备智能与自主性的实体。本文提出一个统一的四阶段框架，系统性地刻画了人工智能在数字孪生全生命周期中的整合路径，涵盖建模、映射、干预与自主管理四个环节。通过综合现有技术与实践，我们提炼出一个统一框架，系统阐述人工智能方法如何嵌入数字孪生生命周期：（1）通过基于物理机制与物理信息的人工智能方法建立物理实体的模型；（2）通过实时同步将物理系统映射为数字孪生；（3）借助预测建模、异常检测与优化策略对物理实体实施干预；（4）通过大语言模型、基础模型与智能体实现自主管理。我们分析了基于物理的建模与数据驱动学习之间的协同关系，重点探讨了物理系统建模从传统数值求解器向物理信息模型与基础模型的范式转变。进一步，我们审视了生成式人工智能技术（包括大语言模型与生成式世界模型）如何将数字孪生转化为具备推理、交互与创造性场景生成能力的主动式、可自我完善的认知系统。通过对医疗健康、航空航天、智能制造、机器人、智慧城市等十一个应用领域的跨领域综述，我们识别出在可扩展性、可解释性与可信赖性方面存在的共性挑战，并展望了负责任的人工智能驱动数字孪生系统的未来发展方向。",
    "url": "https://huggingface.co/papers/2601.01321",
    "arxiv_url": "https://arxiv.org/abs/2601.01321"
  },
  {
    "title": "WebGym: Scaling Training Environments for Visual Web Agents with Realistic Tasks",
    "summary": "We present WebGym, the largest-to-date open-source environment for training realistic visual web agents. Real websites are non-stationary and diverse, making artificial or small-scale task sets insufficient for robust policy learning. WebGym contains nearly 300,000 tasks with rubric-based evaluations across diverse, real-world websites and difficulty levels. We train agents with a simple reinforcement learning (RL) recipe, which trains on the agent's own interaction traces (rollouts), using task rewards as feedback to guide learning. To enable scaling RL, we speed up sampling of trajectories in WebGym by developing a high-throughput asynchronous rollout system, designed specifically for web agents. Our system achieves a 4-5x rollout speedup compared to naive implementations. Second, we scale the task set breadth, depth, and size, which results in continued performance improvement. Fine-tuning a strong base vision-language model, Qwen-3-VL-8B-Instruct, on WebGym results in an improvement in success rate on an out-of-distribution test set from 26.2% to 42.9%, significantly outperforming agents based on proprietary models such as GPT-4o and GPT-5-Thinking that achieve 27.1% and 29.8%, respectively. This improvement is substantial because our test set consists only of tasks on websites never seen during training, unlike many other prior works on training visual web agents.",
    "translation": "标题：WebGym：面向真实任务的视觉网络智能体可扩展训练环境\n\n摘要：本文提出WebGym，这是迄今为止规模最大的开源环境，用于训练真实场景下的视觉网络智能体。真实网站具有非稳态性和多样性，使得人工或小规模任务集难以支撑稳健的策略学习。WebGym包含近30万个任务，基于标准化评估体系覆盖多样化的真实网站及不同难度层级。我们采用简洁的强化学习方案训练智能体：通过智能体自身交互轨迹（滚动执行）进行训练，并以任务奖励作为反馈指导学习。为实现强化学习的规模化扩展，我们专门针对网络智能体开发了高吞吐量异步滚动执行系统，显著加速WebGym中的轨迹采样过程。相较于基础实现方案，该系统实现了4-5倍的滚动执行加速。其次，我们通过拓展任务集的广度、深度与规模，实现了持续的性能提升。基于Qwen-3-VL-8B-Instruct这一强视觉语言基座模型在WebGym上进行微调后，其在分布外测试集上的成功率从26.2%提升至42.9%，显著优于基于GPT-4o（27.1%）和GPT-5-Thinking（29.8%）等专有模型的智能体。这一提升具有实质性意义，因为我们的测试集完全由训练阶段未出现过的网站任务构成，这与以往多数视觉网络智能体训练研究形成鲜明对比。",
    "url": "https://huggingface.co/papers/2601.02439",
    "arxiv_url": "https://arxiv.org/abs/2601.02439"
  },
  {
    "title": "Muses: Designing, Composing, Generating Nonexistent Fantasy 3D Creatures without Training",
    "summary": "We present Muses, the first training-free method for fantastic 3D creature generation in a feed-forward paradigm. Previous methods, which rely on part-aware optimization, manual assembly, or 2D image generation, often produce unrealistic or incoherent 3D assets due to the challenges of intricate part-level manipulation and limited out-of-domain generation. In contrast, Muses leverages the 3D skeleton, a fundamental representation of biological forms, to explicitly and rationally compose diverse elements. This skeletal foundation formalizes 3D content creation as a structure-aware pipeline of design, composition, and generation. Muses begins by constructing a creatively composed 3D skeleton with coherent layout and scale through graph-constrained reasoning. This skeleton then guides a voxel-based assembly process within a structured latent space, integrating regions from different objects. Finally, image-guided appearance modeling under skeletal conditions is applied to generate a style-consistent and harmonious texture for the assembled shape. Extensive experiments establish Muses' state-of-the-art performance in terms of visual fidelity and alignment with textual descriptions, and potential on flexible 3D object editing. Project page: https://luhexiao.github.io/Muses.github.io/.",
    "translation": "标题：Muses：无需训练即可设计、组合与生成虚构幻想三维生物的方法\n\n摘要：本文提出Muses，这是一种在前馈范式下实现幻想三维生物生成的首个免训练方法。以往方法依赖部件感知优化、人工组装或二维图像生成，由于复杂的部件级操控挑战及跨域生成能力有限，常产生不真实或不协调的三维资产。相比之下，Muses利用三维骨架——生物形态的基础表征——来显式且合理地组合多样化元素。这种骨架基础将三维内容创作形式化为结构感知的设计、组合与生成流程。Muses首先通过图约束推理构建具有协调布局与比例的创新性三维骨架，随后在结构化潜空间内引导基于体素的组装过程，整合来自不同对象的区域。最后，在骨架约束下应用图像引导的外观建模，为组装形状生成风格一致且和谐统一的纹理。大量实验证明，Muses在视觉保真度、文本描述对齐度方面达到领先水平，并展现出灵活的三维物体编辑潜力。项目页面：https://luhexiao.github.io/Muses.github.io/。",
    "url": "https://huggingface.co/papers/2601.03256",
    "arxiv_url": "https://arxiv.org/abs/2601.03256"
  },
  {
    "title": "Mechanistic Interpretability of Large-Scale Counting in LLMs through a System-2 Strategy",
    "summary": "Large language models (LLMs), despite strong performance on complex mathematical problems, exhibit systematic limitations in counting tasks. This issue arises from architectural limits of transformers, where counting is performed across layers, leading to degraded precision for larger counting problems due to depth constraints. To address this limitation, we propose a simple test-time strategy inspired by System-2 cognitive processes that decomposes large counting tasks into smaller, independent sub-problems that the model can reliably solve. We evaluate this approach using observational and causal mediation analyses to understand the underlying mechanism of this System-2-like strategy. Our mechanistic analysis identifies key components: latent counts are computed and stored in the final item representations of each part, transferred to intermediate steps via dedicated attention heads, and aggregated in the final stage to produce the total count. Experimental results demonstrate that this strategy enables LLMs to surpass architectural limitations and achieve high accuracy on large-scale counting tasks. This work provides mechanistic insight into System-2 counting in LLMs and presents a generalizable approach for improving and understanding their reasoning behavior.",
    "translation": "标题：基于系统二策略的大语言模型大规模计数机制可解释性研究\n\n摘要：大语言模型虽然在复杂数学问题上表现出色，但在计数任务中存在系统性局限。该问题源于Transformer架构的固有约束——计数操作需跨层执行，导致较大规模计数问题因深度限制而精度下降。为突破此限制，我们受系统二认知过程启发，提出一种简单的测试时策略：将大规模计数任务分解为模型可可靠求解的独立子问题。通过观测性分析与因果中介分析，我们评估该方法以探究此类系统二策略的内在机制。机制分析揭示了三个关键环节：潜在计数结果被计算并存储于各部分的最终项表示中，通过专用注意力头传递至中间步骤，最终在聚合阶段合成总数。实验结果表明，该策略能使大语言模型突破架构限制，在大规模计数任务中实现高精度。本研究不仅揭示了大语言模型中系统二计数的内在机制，更为改善和理解其推理行为提供了可推广的方法论框架。",
    "url": "https://huggingface.co/papers/2601.02989",
    "arxiv_url": "https://arxiv.org/abs/2601.02989"
  },
  {
    "title": "OpenRT: An Open-Source Red Teaming Framework for Multimodal LLMs",
    "summary": "The rapid integration of Multimodal Large Language Models (MLLMs) into critical applications is increasingly hindered by persistent safety vulnerabilities. However, existing red-teaming benchmarks are often fragmented, limited to single-turn text interactions, and lack the scalability required for systematic evaluation. To address this, we introduce OpenRT, a unified, modular, and high-throughput red-teaming framework designed for comprehensive MLLM safety evaluation. At its core, OpenRT architects a paradigm shift in automated red-teaming by introducing an adversarial kernel that enables modular separation across five critical dimensions: model integration, dataset management, attack strategies, judging methods, and evaluation metrics. By standardizing attack interfaces, it decouples adversarial logic from a high-throughput asynchronous runtime, enabling systematic scaling across diverse models. Our framework integrates 37 diverse attack methodologies, spanning white-box gradients, multi-modal perturbations, and sophisticated multi-agent evolutionary strategies. Through an extensive empirical study on 20 advanced models (including GPT-5.2, Claude 4.5, and Gemini 3 Pro), we expose critical safety gaps: even frontier models fail to generalize across attack paradigms, with leading models exhibiting average Attack Success Rates as high as 49.14%. Notably, our findings reveal that reasoning models do not inherently possess superior robustness against complex, multi-turn jailbreaks. By open-sourcing OpenRT, we provide a sustainable, extensible, and continuously maintained infrastructure that accelerates the development and standardization of AI safety.",
    "translation": "标题：OpenRT：面向多模态大语言模型的开源红队测试框架\n\n摘要：多模态大语言模型在关键应用中的快速部署正日益受到持续存在的安全漏洞的阻碍。然而，现有的红队测试基准往往分散孤立，仅限于单轮文本交互，且缺乏系统化评估所需的可扩展性。为此，我们提出了OpenRT——一个为全面评估多模态大语言模型安全性而设计的统一、模块化、高吞吐的红队测试框架。该框架的核心在于通过引入一种对抗内核，实现了在模型集成、数据集管理、攻击策略、判定方法与评估指标这五个关键维度上的模块化分离，从而构建了自动化红队测试的范式转变。通过标准化攻击接口，OpenRT将对抗逻辑与高吞吐异步运行时解耦，实现了跨多样模型的系统性扩展。本框架整合了37种不同的攻击方法，涵盖白盒梯度攻击、多模态扰动以及复杂的多智能体进化策略等。通过对20个先进模型（包括GPT-5.2、Claude 4.5和Gemini 3 Pro）的广泛实证研究，我们揭示了关键的安全缺陷：即使是前沿模型也难以泛化至不同攻击范式，领先模型的平均攻击成功率高达49.14%。值得注意的是，我们的研究发现推理模型在面对复杂多轮越狱攻击时并不天然具备更强的鲁棒性。通过开源OpenRT，我们提供了一个可持续、可扩展且持续维护的基础设施，以加速人工智能安全领域的研发与标准化进程。",
    "url": "https://huggingface.co/papers/2601.01592",
    "arxiv_url": "https://arxiv.org/abs/2601.01592"
  },
  {
    "title": "MindWatcher: Toward Smarter Multimodal Tool-Integrated Reasoning",
    "summary": "Traditional workflow-based agents exhibit limited intelligence when addressing real-world problems requiring tool invocation. Tool-integrated reasoning (TIR) agents capable of autonomous reasoning and tool invocation are rapidly emerging as a powerful approach for complex decision-making tasks involving multi-step interactions with external environments. In this work, we introduce MindWatcher, a TIR agent integrating interleaved thinking and multimodal chain-of-thought (CoT) reasoning. MindWatcher can autonomously decide whether and how to invoke diverse tools and coordinate their use, without relying on human prompts or workflows. The interleaved thinking paradigm enables the model to switch between thinking and tool calling at any intermediate stage, while its multimodal CoT capability allows manipulation of images during reasoning to yield more precise search results. We implement automated data auditing and evaluation pipelines, complemented by manually curated high-quality datasets for training, and we construct a benchmark, called MindWatcher-Evaluate Bench (MWE-Bench), to evaluate its performance. MindWatcher is equipped with a comprehensive suite of auxiliary reasoning tools, enabling it to address broad-domain multimodal problems. A large-scale, high-quality local image retrieval database, covering eight categories including cars, animals, and plants, endows model with robust object recognition despite its small size. Finally, we design a more efficient training infrastructure for MindWatcher, enhancing training speed and hardware utilization. Experiments not only demonstrate that MindWatcher matches or exceeds the performance of larger or more recent models through superior tool invocation, but also uncover critical insights for agent training, such as the genetic inheritance phenomenon in agentic RL.",
    "translation": "标题：MindWatcher：迈向更智能的多模态工具集成推理\n\n摘要：传统基于工作流的智能体在解决需要调用工具的实际问题时表现出有限的智能。能够自主推理并调用工具的工具集成推理（TIR）智能体正迅速崛起，成为处理涉及与外部环境多步交互的复杂决策任务的有效方法。本文提出MindWatcher，一种融合交错式思维与多模态思维链（CoT）推理的TIR智能体。MindWatcher能够自主决定是否及如何调用多样化工具并协调其使用，无需依赖人工提示或预设工作流。其交错式思维范式使模型能够在任意中间阶段在思考与工具调用之间灵活切换，而多模态CoT能力则允许在推理过程中操作图像，以获得更精确的搜索结果。我们实现了自动化数据审计与评估流程，并辅以人工标注的高质量训练数据集，同时构建了名为MindWatcher评估基准（MWE-Bench）的评测体系以评估其性能。MindWatcher配备了一套完整的辅助推理工具集，使其能够处理广域多模态问题。一个涵盖汽车、动物、植物等八大类别的大规模高质量本地图像检索数据库，使模型在参数量较小的情况下仍具备强大的物体识别能力。最后，我们为MindWatcher设计了更高效的训练架构，显著提升了训练速度与硬件利用率。实验结果表明，MindWatcher不仅通过卓越的工具调用能力达到或超越了规模更大或更新模型的性能，还揭示了智能体训练中的关键发现，例如智能体强化学习中的遗传继承现象。",
    "url": "https://huggingface.co/papers/2512.23412",
    "arxiv_url": "https://arxiv.org/abs/2512.23412"
  },
  {
    "title": "Large Reasoning Models Are (Not Yet) Multilingual Latent Reasoners",
    "summary": "Large reasoning models (LRMs) achieve strong performance on mathematical reasoning tasks, often attributed to their capability to generate explicit chain-of-thought (CoT) explanations. However, recent work shows that LRMs often arrive at the correct answer before completing these textual reasoning steps, indicating the presence of latent reasoning -- internal, non-verbal computation encoded in hidden states. While this phenomenon has been explored in English, its multilingual behavior remains largely unknown. In this paper, we conduct a systematic investigation of multilingual latent reasoning in LRMs across 11 languages. Using a truncation-based strategy, we examine how the correct answer emerges as the model is given only partial reasoning traces, allowing us to measure stepwise latent prediction formation. Our results reveal clear evidence of multilingual latent reasoning, though unevenly: strong in resource-rich languages, weaker in low-resource ones, and broadly less observable on harder benchmarks. To understand whether these differences reflect distinct internal mechanisms, we further perform representational analyses. Despite surface-level disparities, we find that the internal evolution of predictions is highly consistent across languages and broadly aligns with English -- a pattern suggesting an English-centered latent reasoning pathway.",
    "translation": "标题：大型推理模型（尚未）成为多语言潜在推理者\n\n摘要：大型推理模型在数学推理任务上展现出卓越性能，这通常归因于其生成显式思维链解释的能力。然而，近期研究表明，模型往往在完成文本推理步骤之前就已得出正确答案，这表明存在潜在推理——即隐藏状态中编码的内部非语言计算。尽管该现象在英语中已得到探索，但其在多语言环境中的表现仍鲜为人知。本文针对11种语言，对大型推理模型中的多语言潜在推理进行了系统性研究。通过基于截断的策略，我们考察了当模型仅获得部分推理轨迹时正确答案如何逐步显现，从而得以度量潜在预测的渐进形成过程。实验结果表明，多语言潜在推理确实存在，但呈现不均衡性：在资源丰富的语言中表现强劲，在低资源语言中较弱，且在更具挑战性的基准测试中普遍较难观测。为探究这些差异是否反映不同的内部机制，我们进一步进行了表征分析。研究发现，尽管存在表层差异，但预测的内部演化过程在跨语言中高度一致，且与英语模式基本吻合——这一规律暗示了以英语为中心的潜在推理路径。\n\n摘要：[中文摘要]",
    "url": "https://huggingface.co/papers/2601.02996",
    "arxiv_url": "https://arxiv.org/abs/2601.02996"
  },
  {
    "title": "FFP-300K: Scaling First-Frame Propagation for Generalizable Video Editing",
    "summary": "First-Frame Propagation (FFP) offers a promising paradigm for controllable video editing, but existing methods are hampered by a reliance on cumbersome run-time guidance. We identify the root cause of this limitation as the inadequacy of current training datasets, which are often too short, low-resolution, and lack the task diversity required to teach robust temporal priors. To address this foundational data gap, we first introduce FFP-300K, a new large-scale dataset comprising 300K high-fidelity video pairs at 720p resolution and 81 frames in length, constructed via a principled two-track pipeline for diverse local and global edits. Building on this dataset, we propose a novel framework designed for true guidance-free FFP that resolves the critical tension between maintaining first-frame appearance and preserving source video motion. Architecturally, we introduce Adaptive Spatio-Temporal RoPE (AST-RoPE), which dynamically remaps positional encodings to disentangle appearance and motion references. At the objective level, we employ a self-distillation strategy where an identity propagation task acts as a powerful regularizer, ensuring long-term temporal stability and preventing semantic drift. Comprehensive experiments on the EditVerseBench benchmark demonstrate that our method significantly outperforming existing academic and commercial models by receiving about 0.2 PickScore and 0.3 VLM score improvement against these competitors.",
    "translation": "标题：FFP-300K：面向可泛化视频编辑的首帧传播规模化研究\n\n摘要：首帧传播（FFP）为可控视频编辑提供了一种前景广阔的技术范式，但现有方法受限于对繁琐运行时引导的依赖。本文指出该局限的根本原因在于当前训练数据集的不足——其往往时长过短、分辨率较低，且缺乏训练鲁棒时序先验所需的任务多样性。为弥补这一基础数据缺口，我们首先提出了FFP-300K数据集，该大规模数据集包含30万对720p分辨率、81帧长度的高保真视频对，通过结构化的双轨流程构建，涵盖多样化的局部与全局编辑任务。基于此数据集，我们提出了一种真正无需引导的FFP新框架，该框架通过创新设计解决了保持首帧外观与维持源视频运动之间的关键矛盾。在架构层面，我们提出了自适应时空旋转位置编码（AST-RoPE），通过动态重映射位置编码实现外观与运动参考的解耦。在目标层面，我们采用以身份传播任务作为强正则化器的自蒸馏策略，确保长期时序稳定性并防止语义漂移。在EditVerseBench基准测试上的综合实验表明，本方法在PickScore和VLM评分上分别以约0.2分和0.3分的优势显著超越现有学术及商业模型。",
    "url": "https://huggingface.co/papers/2601.01720",
    "arxiv_url": "https://arxiv.org/abs/2601.01720"
  },
  {
    "title": "The Sonar Moment: Benchmarking Audio-Language Models in Audio Geo-Localization",
    "summary": "Geo-localization aims to infer the geographic origin of a given signal. In computer vision, geo-localization has served as a demanding benchmark for compositional reasoning and is relevant to public safety. In contrast, progress on audio geo-localization has been constrained by the lack of high-quality audio-location pairs. To address this gap, we introduce AGL1K, the first audio geo-localization benchmark for audio language models (ALMs), spanning 72 countries and territories. To extract reliably localizable samples from a crowd-sourced platform, we propose the Audio Localizability metric that quantifies the informativeness of each recording, yielding 1,444 curated audio clips. Evaluations on 16 ALMs show that ALMs have emerged with audio geo-localization capability. We find that closed-source models substantially outperform open-source models, and that linguistic clues often dominate as a scaffold for prediction. We further analyze ALMs' reasoning traces, regional bias, error causes, and the interpretability of the localizability metric. Overall, AGL1K establishes a benchmark for audio geo-localization and may advance ALMs with better geospatial reasoning capability.",
    "translation": "标题：声纳时刻：音频语言模型在音频地理定位中的基准测试\n\n摘要：地理定位旨在推断给定信号的地理来源。在计算机视觉领域，地理定位已成为组合推理能力的高要求基准，并与公共安全密切相关。相比之下，音频地理定位的发展因缺乏高质量的音频-地理位置配对数据而受限。为填补这一空白，我们提出了AGL1K——首个面向音频语言模型的音频地理定位基准数据集，涵盖72个国家和地区。为从众包平台中提取可靠可定位的样本，我们提出了音频可定位性度量指标，用以量化每条录音的信息丰富度，最终筛选出1,444条精校音频片段。对16个音频语言模型的评估表明，此类模型已展现出音频地理定位能力。研究发现，闭源模型显著优于开源模型，且语言线索常作为预测的主要推理框架。我们进一步分析了音频语言模型的推理路径、区域偏见、错误成因以及可定位性指标的可解释性。总体而言，AGL1K为音频地理定位建立了基准测试体系，有望推动音频语言模型发展出更优的地理空间推理能力。",
    "url": "https://huggingface.co/papers/2601.03227",
    "arxiv_url": "https://arxiv.org/abs/2601.03227"
  },
  {
    "title": "X-MuTeST: A Multilingual Benchmark for Explainable Hate Speech Detection and A Novel LLM-consulted Explanation Framework",
    "summary": "Hate speech detection on social media faces challenges in both accuracy and explainability, especially for underexplored Indic languages. We propose a novel explainability-guided training framework, X-MuTeST (eXplainable Multilingual haTe Speech deTection), for hate speech detection that combines high-level semantic reasoning from large language models (LLMs) with traditional attention-enhancing techniques. We extend this research to Hindi and Telugu alongside English by providing benchmark human-annotated rationales for each word to justify the assigned class label. The X-MuTeST explainability method computes the difference between the prediction probabilities of the original text and those of unigrams, bigrams, and trigrams. Final explanations are computed as the union between LLM explanations and X-MuTeST explanations. We show that leveraging human rationales during training enhances both classification performance and explainability. Moreover, combining human rationales with our explainability method to refine the model attention yields further improvements. We evaluate explainability using Plausibility metrics such as Token-F1 and IOU-F1 and Faithfulness metrics such as Comprehensiveness and Sufficiency. By focusing on under-resourced languages, our work advances hate speech detection across diverse linguistic contexts. Our dataset includes token-level rationale annotations for 6,004 Hindi, 4,492 Telugu, and 6,334 English samples. Data and code are available on https://github.com/ziarehman30/X-MuTeST",
    "translation": "标题：X-MuTeST：一个面向可解释仇恨言论检测的多语言基准与新型大语言模型咨询解释框架\n\n摘要：社交媒体上的仇恨言论检测在准确性和可解释性方面均面临挑战，尤其对于研究不足的印度语言而言。本文提出一种新颖的可解释性引导训练框架——X-MuTeST（可解释多语言仇恨言论检测框架），该框架将大型语言模型的高层语义推理与传统注意力增强技术相结合。我们通过为英语、印地语和泰卢固语的每个词语提供基准人工标注归因依据以证明类别标签的合理性，从而将研究扩展至多语言场景。X-MuTeST可解释性方法通过计算原始文本与单字组、双字组及三字组的预测概率差异生成解释，最终解释结果由大语言模型生成的解释与X-MuTeST解释的并集构成。研究表明，在训练过程中利用人工标注归因依据能同步提升分类性能与可解释性。进一步将人工归因依据与我们的可解释性方法结合以优化模型注意力机制，可取得更显著的性能提升。我们采用合理性指标（如Token-F1和IOU-F1）与忠实性指标（如完备性和充分性）对可解释性进行量化评估。通过聚焦资源匮乏语言，本研究推动了跨多元语言环境的仇恨言论检测发展。构建的数据集包含6,004条印地语、4,492条泰卢固语及6,334条英语样本的词级归因标注。数据与代码已公开于https://github.com/ziarehman30/X-MuTeST。",
    "url": "https://huggingface.co/papers/2601.03194",
    "arxiv_url": "https://arxiv.org/abs/2601.03194"
  },
  {
    "title": "Parallel Latent Reasoning for Sequential Recommendation",
    "summary": "Capturing complex user preferences from sparse behavioral sequences remains a fundamental challenge in sequential recommendation. Recent latent reasoning methods have shown promise by extending test-time computation through multi-step reasoning, yet they exclusively rely on depth-level scaling along a single trajectory, suffering from diminishing returns as reasoning depth increases. To address this limitation, we propose Parallel Latent Reasoning (PLR), a novel framework that pioneers width-level computational scaling by exploring multiple diverse reasoning trajectories simultaneously. PLR constructs parallel reasoning streams through learnable trigger tokens in continuous latent space, preserves diversity across streams via global reasoning regularization, and adaptively synthesizes multi-stream outputs through mixture-of-reasoning-streams aggregation. Extensive experiments on three real-world datasets demonstrate that PLR substantially outperforms state-of-the-art baselines while maintaining real-time inference efficiency. Theoretical analysis further validates the effectiveness of parallel reasoning in improving generalization capability. Our work opens new avenues for enhancing reasoning capacity in sequential recommendation beyond existing depth scaling.",
    "translation": "标题：并行潜在推理在序列推荐中的应用\n\n摘要：从稀疏行为序列中捕捉复杂的用户偏好始终是序列推荐领域的核心挑战。现有的潜在推理方法通过多步推理扩展测试阶段计算能力已展现出潜力，但这些方法仅依赖单一推理路径的深度扩展，随着推理深度增加会出现收益递减问题。为解决这一局限性，我们提出并行潜在推理（PLR）框架，该创新方法通过同时探索多条多样化推理路径，首次实现了宽度层面的计算扩展。PLR通过在连续潜在空间中构建可学习的触发令牌来建立并行推理流，通过全局推理正则化保持多流间的差异性，并采用混合推理流聚合机制自适应地融合多流输出。在三个真实数据集上的大量实验表明，PLR在保持实时推理效率的同时，显著超越了现有最优基线模型。理论分析进一步验证了并行推理对提升模型泛化能力的有效性。本研究为突破现有深度扩展范式、增强序列推荐系统的推理能力开辟了新途径。",
    "url": "https://huggingface.co/papers/2601.03153",
    "arxiv_url": "https://arxiv.org/abs/2601.03153"
  },
  {
    "title": "Unified Thinker: A General Reasoning Modular Core for Image Generation",
    "summary": "Despite impressive progress in high-fidelity image synthesis, generative models still struggle with logic-intensive instruction following, exposing a persistent reasoning--execution gap. Meanwhile, closed-source systems (e.g., Nano Banana) have demonstrated strong reasoning-driven image generation, highlighting a substantial gap to current open-source models. We argue that closing this gap requires not merely better visual generators, but executable reasoning: decomposing high-level intents into grounded, verifiable plans that directly steer the generative process. To this end, we propose Unified Thinker, a task-agnostic reasoning architecture for general image generation, designed as a unified planning core that can plug into diverse generators and workflows. Unified Thinker decouples a dedicated Thinker from the image Generator, enabling modular upgrades of reasoning without retraining the entire generative model. We further introduce a two-stage training paradigm: we first build a structured planning interface for the Thinker, then apply reinforcement learning to ground its policy in pixel-level feedback, encouraging plans that optimize visual correctness over textual plausibility. Extensive experiments on text-to-image generation and image editing show that Unified Thinker substantially improves image reasoning and generation quality.",
    "translation": "标题：统一思考者：面向图像生成的通用推理模块化核心\n\n摘要：尽管高保真图像合成已取得显著进展，生成模型在遵循逻辑密集型指令方面仍存在困难，暴露出长期存在的推理与执行之间的鸿沟。与此同时，闭源系统（如Nano Banana）已展现出强大的推理驱动图像生成能力，凸显了当前开源模型与之存在的显著差距。我们认为，弥合这一差距不仅需要更优的视觉生成器，更需要可执行的推理能力：将高层意图分解为可直接指导生成过程的、可验证的具象化规划。为此，我们提出“统一思考者”——一种面向通用图像生成的任务无关推理架构，其设计为一个可接入多样化生成器与工作流的统一规划核心。该架构将专用的“思考者”模块与图像“生成器”解耦，使得推理能力能够以模块化方式升级，而无需重新训练整个生成模型。我们进一步引入两阶段训练范式：首先为思考者构建结构化规划接口，随后运用强化学习使其策略基于像素级反馈进行具象化调整，从而鼓励规划方案优先优化视觉正确性而非文本合理性。在文本到图像生成与图像编辑任务上的大量实验表明，统一思考者显著提升了图像推理与生成质量。",
    "url": "https://huggingface.co/papers/2601.03127",
    "arxiv_url": "https://arxiv.org/abs/2601.03127"
  },
  {
    "title": "ExposeAnyone: Personalized Audio-to-Expression Diffusion Models Are Robust Zero-Shot Face Forgery Detectors",
    "summary": "Detecting unknown deepfake manipulations remains one of the most challenging problems in face forgery detection. Current state-of-the-art approaches fail to generalize to unseen manipulations, as they primarily rely on supervised training with existing deepfakes or pseudo-fakes, which leads to overfitting to specific forgery patterns. In contrast, self-supervised methods offer greater potential for generalization, but existing work struggles to learn discriminative representations only from self-supervision. In this paper, we propose ExposeAnyone, a fully self-supervised approach based on a diffusion model that generates expression sequences from audio. The key idea is, once the model is personalized to specific subjects using reference sets, it can compute the identity distances between suspected videos and personalized subjects via diffusion reconstruction errors, enabling person-of-interest face forgery detection. Extensive experiments demonstrate that 1) our method outperforms the previous state-of-the-art method by 4.22 percentage points in the average AUC on DF-TIMIT, DFDCP, KoDF, and IDForge datasets, 2) our model is also capable of detecting Sora2-generated videos, where the previous approaches perform poorly, and 3) our method is highly robust to corruptions such as blur and compression, highlighting the applicability in real-world face forgery detection.",
    "translation": "标题：ExposeAnyone：个性化音频到表情扩散模型作为鲁棒的零样本人脸伪造检测器\n\n摘要：检测未知的深度伪造操作仍然是人脸伪造检测中最具挑战性的问题之一。当前最先进的方法无法泛化到未见过的伪造操作，因为它们主要依赖于对现有深度伪造或伪伪造数据的监督训练，这导致模型过度拟合特定的伪造模式。相比之下，自监督方法具有更强的泛化潜力，但现有工作难以仅通过自监督学习到具有判别性的表征。本文提出ExposeAnyone，一种基于扩散模型的完全自监督方法，该模型能够从音频生成表情序列。其核心思想是，一旦模型通过参考集针对特定对象完成个性化，即可通过扩散重建误差计算可疑视频与个性化对象之间的身份距离，从而实现针对特定目标的人脸伪造检测。大量实验表明：1）在DF-TIMIT、DFDCP、KoDF和IDForge数据集上，本方法的平均AUC比先前最优方法提升4.22个百分点；2）本模型还能检测Sora2生成的视频，而现有方法在此类数据上表现不佳；3）本方法对模糊、压缩等干扰具有高度鲁棒性，凸显了其在现实世界人脸伪造检测中的适用性。",
    "url": "https://huggingface.co/papers/2601.02359",
    "arxiv_url": "https://arxiv.org/abs/2601.02359"
  },
  {
    "title": "AceFF: A State-of-the-Art Machine Learning Potential for Small Molecules",
    "summary": "We introduce AceFF, a pre-trained machine learning interatomic potential (MLIP) optimized for small molecule drug discovery. While MLIPs have emerged as efficient alternatives to Density Functional Theory (DFT), generalizability across diverse chemical spaces remains difficult. AceFF addresses this via a refined TensorNet2 architecture trained on a comprehensive dataset of drug-like compounds. This approach yields a force field that balances high-throughput inference speed with DFT-level accuracy. AceFF fully supports the essential medicinal chemistry elements (H, B, C, N, O, F, Si, P, S, Cl, Br, I) and is explicitly trained to handle charged states. Validation against rigorous benchmarks, including complex torsional energy scans, molecular dynamics trajectories, batched minimizations, and forces and anergy accuracy demonstrates that AceFF establishes a new state-of-the-art for organic molecules. The AceFF-2 model weights and inference code are available at https://huggingface.co/Acellera/AceFF-2.0.",
    "translation": "标题：AceFF：面向小分子的前沿机器学习势函数\n\n摘要：本文介绍AceFF——一种专为小分子药物发现优化的预训练机器学习原子间势函数（MLIP）。尽管MLIP已成为密度泛函理论（DFT）的高效替代方案，但其在不同化学空间中的泛化能力仍面临挑战。AceFF通过基于类药化合物综合数据集精调的TensorNet2架构解决了这一问题，实现了高通量推理速度与DFT级精度的平衡。该势函数完整支持药物化学核心元素（H、B、C、N、O、F、Si、P、S、Cl、Br、I），并经过专门训练以处理带电态。通过复杂扭转能扫描、分子动力学轨迹、批量能量最小化以及力与能量的精度验证等严格基准测试表明，AceFF为有机分子体系建立了新的性能标杆。AceFF-2模型权重与推理代码已发布于https://huggingface.co/Acellera/AceFF-2.0。",
    "url": "https://huggingface.co/papers/2601.00581",
    "arxiv_url": "https://arxiv.org/abs/2601.00581"
  },
  {
    "title": "U-Net-Like Spiking Neural Networks for Single Image Dehazing",
    "summary": "Image dehazing is a critical challenge in computer vision, essential for enhancing image clarity in hazy conditions. Traditional methods often rely on atmospheric scattering models, while recent deep learning techniques, specifically Convolutional Neural Networks (CNNs) and Transformers, have improved performance by effectively analyzing image features. However, CNNs struggle with long-range dependencies, and Transformers demand significant computational resources. To address these limitations, we propose DehazeSNN, an innovative architecture that integrates a U-Net-like design with Spiking Neural Networks (SNNs). DehazeSNN captures multi-scale image features while efficiently managing local and long-range dependencies. The introduction of the Orthogonal Leaky-Integrate-and-Fire Block (OLIFBlock) enhances cross-channel communication, resulting in superior dehazing performance with reduced computational burden. Our extensive experiments show that DehazeSNN is highly competitive to state-of-the-art methods on benchmark datasets, delivering high-quality haze-free images with a smaller model size and less multiply-accumulate operations. The proposed dehazing method is publicly available at https://github.com/HaoranLiu507/DehazeSNN.",
    "translation": "标题：基于U-Net架构的脉冲神经网络单幅图像去雾方法\n\n摘要：图像去雾是计算机视觉领域的关键挑战，对于提升雾霾条件下图像清晰度至关重要。传统方法通常依赖于大气散射模型，而近期深度学习技术——特别是卷积神经网络（CNN）和Transformer架构——通过有效分析图像特征显著提升了去雾性能。然而，CNN在处理长程依赖关系方面存在局限，而Transformer则需要大量计算资源。为克服这些限制，本文提出DehazeSNN这一创新架构，将类U-Net设计与脉冲神经网络（SNN）相结合。该模型能够捕捉多尺度图像特征，同时高效处理局部与长程依赖关系。通过引入正交泄漏积分发放模块（OLIFBlock），增强了跨通道信息交互能力，从而以更低计算成本实现卓越的去雾性能。大量实验表明，DehazeSNN在基准数据集上与最先进方法相比具有显著竞争力，能以更小的模型规模和更少的乘累加运算生成高质量无雾图像。本去雾方法的完整实现已公开于：https://github.com/HaoranLiu507/DehazeSNN。",
    "url": "https://huggingface.co/papers/2512.23950",
    "arxiv_url": "https://arxiv.org/abs/2512.23950"
  },
  {
    "title": "Doc-PP: Document Policy Preservation Benchmark for Large Vision-Language Models",
    "summary": "The deployment of Large Vision-Language Models (LVLMs) for real-world document question answering is often constrained by dynamic, user-defined policies that dictate information disclosure based on context. While ensuring adherence to these explicit constraints is critical, existing safety research primarily focuses on implicit social norms or text-only settings, overlooking the complexities of multimodal documents. In this paper, we introduce Doc-PP (Document Policy Preservation Benchmark), a novel benchmark constructed from real-world reports requiring reasoning across heterogeneous visual and textual elements under strict non-disclosure policies. Our evaluation highlights a systemic Reasoning-Induced Safety Gap: models frequently leak sensitive information when answers must be inferred through complex synthesis or aggregated across modalities, effectively circumventing existing safety constraints. Furthermore, we identify that providing extracted text improves perception but inadvertently facilitates leakage. To address these vulnerabilities, we propose DVA (Decompose-Verify-Aggregation), a structural inference framework that decouples reasoning from policy verification. Experimental results demonstrate that DVA significantly outperforms standard prompting defenses, offering a robust baseline for policy-compliant document understanding",
    "translation": "标题：Doc-PP：面向大型视觉语言模型的文档策略保持基准\n\n摘要：大型视觉语言模型在实际文档问答任务中的部署，常受到动态、用户自定义策略的约束，这些策略根据具体情境规定信息的披露范围。尽管确保模型遵守这些显式约束至关重要，但现有的安全性研究主要集中于隐式社会规范或纯文本场景，忽视了多模态文档的复杂性。本文提出Doc-PP（文档策略保持基准），这是一个基于真实世界报告构建的新型基准，要求模型在严格的非披露政策下，对异构的视觉与文本元素进行跨模态推理。我们的评估揭示了一个系统性的“推理诱发安全漏洞”：当答案需要通过复杂综合或多模态信息聚合推断时，模型频繁泄露敏感信息，从而有效规避现有安全约束。此外，我们发现提供提取文本虽能提升感知能力，却无意中助长了信息泄露。为应对这些漏洞，我们提出DVA（分解-验证-聚合）结构推理框架，将推理过程与策略验证解耦。实验结果表明，DVA显著优于标准提示防御方法，为符合策略的文档理解提供了鲁棒的基线方案。",
    "url": "https://huggingface.co/papers/2601.03926",
    "arxiv_url": "https://arxiv.org/abs/2601.03926"
  },
  {
    "title": "Steerability of Instrumental-Convergence Tendencies in LLMs",
    "summary": "We examine two properties of AI systems: capability (what a system can do) and steerability (how reliably one can shift behavior toward intended outcomes). A central question is whether capability growth reduces steerability and risks control collapse. We also distinguish between authorized steerability (builders reliably reaching intended behaviors) and unauthorized steerability (attackers eliciting disallowed behaviors). This distinction highlights a fundamental safety--security dilemma of AI models: safety requires high steerability to enforce control (e.g., stop/refuse), while security requires low steerability for malicious actors to elicit harmful behaviors. This tension presents a significant challenge for open-weight models, which currently exhibit high steerability via common techniques like fine-tuning or adversarial attacks. Using Qwen3 and InstrumentalEval, we find that a short anti-instrumental prompt suffix sharply reduces the measured convergence rate (e.g., shutdown avoidance, self-replication). For Qwen3-30B Instruct, the convergence rate drops from 81.69% under a pro-instrumental suffix to 2.82% under an anti-instrumental suffix. Under anti-instrumental prompting, larger aligned models show lower convergence rates than smaller ones (Instruct: 2.82% vs. 4.23%; Thinking: 4.23% vs. 9.86%). Code is available at github.com/j-hoscilowicz/instrumental_steering.",
    "translation": "标题：大语言模型中工具性趋同倾向的可操控性研究\n\n摘要：本研究探讨人工智能系统的两个核心属性：能力（系统能够执行的任务）与可操控性（使系统行为朝向预期目标可靠转变的程度）。核心问题在于能力提升是否会削弱可操控性并引发控制失效风险。我们进一步区分授权可操控性（开发者可靠实现预期行为）与非授权可操控性（攻击者诱导出禁止行为），这一区分揭示了AI模型面临的基础性安全-防护困境：安全性要求高可操控性以实施控制（如停止/拒绝指令），而防护性则需降低恶意行为者诱导有害行为的可操控性。这种矛盾对开源权重模型构成重大挑战，当前这类模型通过微调或对抗攻击等常见技术展现出高可操控性。基于Qwen3模型与InstrumentalEval评估工具，我们发现简短的反工具性提示后缀能显著降低测得的趋同率（如关机规避、自我复制等）。以Qwen3-30B Instruct模型为例，其趋同率从支持工具性后缀条件下的81.69%骤降至反工具性后缀下的2.82%。在反工具性提示下，较大规模的指令对齐模型比较小模型展现出更低的趋同率（Instruct版：2.82%对比4.23%；Thinking版：4.23%对比9.86%）。相关代码已发布于github.com/j-hoscilowicz/instrumental_steering。",
    "url": "https://huggingface.co/papers/2601.01584",
    "arxiv_url": "https://arxiv.org/abs/2601.01584"
  }
]