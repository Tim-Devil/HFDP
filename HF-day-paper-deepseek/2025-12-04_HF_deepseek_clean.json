[
  {
    "title": "Qwen3-VL Technical Report",
    "summary": "We introduce Qwen3-VL, the most capable vision-language model in the Qwen series to date, achieving superior performance across a broad range of multimodal benchmarks. It natively supports interleaved contexts of up to 256K tokens, seamlessly integrating text, images, and video. The model family includes both dense (2B/4B/8B/32B) and mixture-of-experts (30B-A3B/235B-A22B) variants to accommodate diverse latency-quality trade-offs. Qwen3-VL delivers three core pillars: (i) markedly stronger pure-text understanding, surpassing comparable text-only backbones in several cases; (ii) robust long-context comprehension with a native 256K-token window for both text and interleaved multimodal inputs, enabling faithful retention, retrieval, and cross-referencing across long documents and videos; and (iii) advanced multimodal reasoning across single-image, multi-image, and video tasks, demonstrating leading performance on comprehensive evaluations such as MMMU and visual-math benchmarks (e.g., MathVista and MathVision). Architecturally, we introduce three key upgrades: (i) an enhanced interleaved-MRoPE for stronger spatial-temporal modeling across images and video; (ii) DeepStack integration, which effectively leverages multi-level ViT features to tighten vision-language alignment; and (iii) text-based time alignment for video, evolving from T-RoPE to explicit textual timestamp alignment for more precise temporal grounding. Under comparable token budgets and latency constraints, Qwen3-VL achieves superior performance in both dense and Mixture-of-Experts (MoE) architectures. We envision Qwen3-VL serving as a foundational engine for image-grounded reasoning, agentic decision-making, and multimodal code intelligence in real-world workflows.",
    "translation": "标题：Qwen3-VL技术报告\n\n摘要：本文介绍Qwen系列迄今性能最强的视觉语言模型Qwen3-VL，该模型在广泛的多模态基准测试中均取得卓越表现。其原生支持高达256K令牌的交错上下文，能够无缝整合文本、图像与视频输入。该模型系列包含稠密架构（2B/4B/8B/32B）与混合专家架构（30B-A3B/235B-A22B）变体，以适应不同的延迟-质量权衡需求。Qwen3-VL具备三大核心优势：（一）显著增强的纯文本理解能力，在多项测试中超越同规模纯文本骨干模型；（二）强大的长上下文理解能力，原生支持文本与交错多模态输入的256K令牌窗口，能够对长文档和视频实现精准的信息保持、检索与交叉引用；（三）先进的跨模态推理能力，在单图、多图及视频任务中表现优异，在MMMU综合性评测及视觉数学基准（如MathVista、MathVision）中达到领先水平。在架构层面，我们引入三项关键升级：（一）增强型交错多分辨率旋转位置编码，强化图像与视频的时空建模能力；（二）深度堆叠集成机制，通过有效利用多层级视觉Transformer特征以增强视觉-语言对齐；（三）基于文本的视频时间对齐机制，从时序旋转位置编码演进为显式文本时间戳对齐，实现更精确的时间定位。在可比令牌预算与延迟约束下，Qwen3-VL在稠密架构与混合专家架构中均展现出优越性能。我们展望Qwen3-VL能够作为现实工作流程中图像推理、智能体决策与多模态代码智能的基础引擎。",
    "url": "https://huggingface.co/papers/2511.21631",
    "arxiv_url": "https://arxiv.org/abs/2511.21631"
  },
  {
    "title": "Steering Vision-Language-Action Models as Anti-Exploration: A Test-Time Scaling Approach",
    "summary": "Vision-Language-Action (VLA) models, trained via flow-matching or diffusion objectives, excel at learning complex behaviors from large-scale, multi-modal datasets (e.g., human teleoperation, scripted policies). However, since VLAs incorporate diverse data modes in the pre-training stage, and the finetuning dataset often contains demonstration data collected in a kinematically suboptimal or undesirable way, it exists redundant action modes that are irrelevant to the success action modes of the downstream task. Specifically, we observe a critical inference-time fragility among various sampled noises after supervised finetuning of pre-trained VLAs. In this paper, we attribute this instability to the distribution shift between the VLA policy and the policy induced by stable success modes of the downstream task dataset. Thus, we propose TACO, a test-time-scaling (TTS) framework that applies a lightweight pseudo-count estimator as a high-fidelity verifier of action chunks. The VLA models integrated with TACO can execute the actions with maximum pseudo-count from all sampled action chunks, thereby preventing distribution shifts while preserving the generalization ability of VLAs since the constraint is applied only during inference. Our method resembles the classical anti-exploration principle in offline reinforcement learning (RL), and being gradient-free, it incurs significant computational benefits compared to RL update, especially for flow or diffusion-based VLAs which are difficult to perform RL update due to denoising process. Extensive experiments across four simulation benchmarks (RoboTwin2.0, Robotwin, LIBERO, SimplerEnv) and a dual-arm platform demonstrate that our method significantly improves the inference stability and success rates in downstream-task adaptations.",
    "translation": "标题：引导视觉-语言-动作模型作为反探索：一种测试时缩放方法\n\n摘要：通过流匹配或扩散目标训练的视觉-语言-动作模型擅长从大规模多模态数据集中学习复杂行为。然而，由于此类模型在预训练阶段整合了多样化的数据模式，而微调数据集通常包含以运动学上次优或不可取方式收集的演示数据，因此存在与下游任务成功动作模式无关的冗余动作模式。具体而言，我们观察到预训练模型经过监督微调后，在不同采样噪声中存在关键的推理时脆弱性。本文将此不稳定性归因于模型策略与下游任务数据集中稳定成功模式所诱导策略之间的分布偏移。为此，我们提出TACO——一种基于测试时缩放的轻量化框架，采用伪计数估计器作为动作片段的高保真验证器。集成TACO的模型能够从所有采样动作片段中执行具有最大伪计数的动作，从而在保持模型泛化能力的同时防止分布偏移。该方法类似于离线强化学习中的经典反探索原理，且无需梯度计算，相比强化学习更新具有显著计算优势，尤其适用于因去噪过程而难以进行强化学习更新的流模型或扩散模型。在四个仿真基准与双臂机器人平台上的大量实验表明，该方法能显著提升下游任务适应过程中的推理稳定性与成功率。",
    "url": "https://huggingface.co/papers/2512.02834",
    "arxiv_url": "https://arxiv.org/abs/2512.02834"
  },
  {
    "title": "PretrainZero: Reinforcement Active Pretraining",
    "summary": "Mimicking human behavior to actively learning from general experience and achieve artificial general intelligence has always been a human dream. Recent reinforcement learning (RL) based large-thinking models demonstrate impressive expert-level abilities, i.e., software and math, but still rely heavily on verifiable rewards in specific domains, placing a significant bottleneck to extend the performance boundary of general reasoning capabilities. In this work, we propose PretrainZero, a reinforcement active learning framework built on the pretraining corpus to extend RL from domain-specific post-training to general pretraining. PretrainZero features the following characteristics: 1) Active pretraining: inspired by the active learning ability of humans, PretrainZero learns a unified reasoning policy to actively identify reasonable and informative contents from pretraining corpus, and reason to predict these contents by RL. 2) Self-supervised learning: without any verifiable labels, pretrained reward models, or supervised fine-tuning, we directly pretrain reasoners from 3 to 30B base models on the general Wikipedia corpus using RL, significantly breaking the verification data-wall for general reasoning. 3) Verification scaling: by tackling increasingly challenging masked spans, PretrainZero substantially enhances the general reasoning abilities of pretrained base models. In reinforcement pretraining, PretrainZero improves Qwen3-4B-Base for 8.43, 5.96 and 10.60 on MMLU-Pro, SuperGPQA and math average benchmarks. In post-training, the pretrained models can also serve as reasoning foundation models for downstream RLVR tasks.",
    "translation": "标题：PretrainZero：强化主动预训练\n\n摘要：模仿人类行为以主动从通用经验中学习并实现通用人工智能，一直是人类的梦想。近期基于强化学习的大规模思维模型在特定领域（如软件和数学）展现出令人印象深刻的专家级能力，但仍严重依赖特定领域内可验证的奖励信号，这极大限制了通用推理能力边界的拓展。本研究提出PretrainZero，一个基于预训练语料库的强化主动学习框架，旨在将强化学习从领域特定的后训练阶段扩展至通用预训练阶段。PretrainZero具备以下特征：1）主动预训练：受人类主动学习能力启发，PretrainZero学习统一的推理策略，以主动从预训练语料库中识别合理且信息丰富的内容，并通过强化学习进行推理预测；2）自监督学习：无需任何可验证标签、预训练奖励模型或监督微调，我们直接在通用维基百科语料库上使用强化学习对3B至30B的基础模型进行预训练，显著突破了通用推理的可验证数据壁垒；3）验证扩展：通过处理难度递增的掩码片段，PretrainZero大幅提升了预训练基础模型的通用推理能力。在强化预训练中，PretrainZero将Qwen3-4B-Base模型在MMLU-Pro、SuperGPQA和数学综合基准上的性能分别提升了8.43、5.96和10.60分。在后训练阶段，预训练模型还可作为下游RLVR任务的推理基础模型。",
    "url": "https://huggingface.co/papers/2512.03442",
    "arxiv_url": "https://arxiv.org/abs/2512.03442"
  },
  {
    "title": "ViDiC: Video Difference Captioning",
    "summary": "Understanding visual differences between dynamic scenes requires the comparative perception of compositional, spatial, and temporal changes--a capability that remains underexplored in existing vision-language systems. While prior work on Image Difference Captioning (IDC) has enabled models to describe semantic changes between static images, these approaches fail to capture motion continuity, event evolution, or editing consistency over time. We introduce the ViDiC (Video Difference Captioning) task and its corresponding ViDiC-1K dataset, designed to evaluate the ability of Multimodal Large Language Models (MLLMs) to provide fine-grained descriptions of similarities and differences between video pairs. ViDiC-1K comprises 1,000 curated video pairs annotated with over 4,000 comparative checklist items, covering seven categories: subject, style, background, cinematography, motion, location, and playback techniques. To ensure reliable evaluation, we propose a dual-checklist framework that measures the accuracy of similarity and difference separately, based on the LLM-as-a-Judge protocol. Experiments on nineteen representative multimodal models reveal a significant performance gap in their comparative description and difference perception abilities. We hope ViDiC-1K can be a challenging benchmark that lays a solid foundation for advancing video understanding, edit awareness, and comparative reasoning in multimodal intelligence.",
    "translation": "标题：ViDiC：视频差异描述\n\n摘要：理解动态场景之间的视觉差异需要对组合、空间和时间变化进行比较性感知——这一能力在现有的视觉-语言系统中仍未得到充分探索。尽管先前关于图像差异描述（IDC）的研究使模型能够描述静态图像之间的语义变化，但这些方法未能捕捉随时间变化的运动连续性、事件演化或编辑一致性。我们提出了ViDiC（视频差异描述）任务及其对应的ViDiC-1K数据集，旨在评估多模态大语言模型（MLLMs）对视频对之间相似性和差异提供细粒度描述的能力。ViDiC-1K包含1,000个精选视频对，标注了超过4,000项比较清单条目，涵盖七个类别：主体、风格、背景、摄影技术、运动、场景和播放技术。为确保可靠评估，我们基于“LLM即评判者”协议提出了一个双重清单框架，分别测量相似性和差异的准确性。对十九个代表性多模态模型的实验揭示了它们在比较描述和差异感知能力方面存在显著性能差距。我们希望ViDiC-1K能成为一个具有挑战性的基准，为推进多模态智能中的视频理解、编辑感知和比较推理奠定坚实基础。",
    "url": "https://huggingface.co/papers/2512.03405",
    "arxiv_url": "https://arxiv.org/abs/2512.03405"
  },
  {
    "title": "SpaceTools: Tool-Augmented Spatial Reasoning via Double Interactive RL",
    "summary": "Vision Language Models (VLMs) demonstrate strong qualitative visual understanding, but struggle with metrically precise spatial reasoning required for embodied applications. The agentic paradigm promises that VLMs can use a wide variety of tools that could augment these capabilities, such as depth estimators, segmentation models, and pose estimators. Yet it remains an open challenge how to realize this vision without solely relying on handcrafted prompting strategies or enforcing fixed, predefined tool pipelines that limit VLMs' ability to discover optimal tool-use patterns. Reinforcement Learning could overcome this gap, but has so far been limited to reasoning with a single visual tool due to the large search space in multi-tool reasoning. We introduce Double Interactive Reinforcement Learning (DIRL), a two-phase training framework where VLMs learn to coordinate multiple tools through interactive exploration and feedback. In the teaching phase, we combine demonstrations from a single tool specialist trained via interactive RL with traces from a frontier model using all tools. In the exploration phase, the model further refines multi-tool coordination through continued RL. Our model, SpaceTools, with tool-augmented spatial reasoning ability, achieves state-of-the-art performance on spatial understanding benchmarks (RoboSpatial-Home, BLINK, BOP-ASK) and demonstrates reliable real-world manipulation using a 7-DOF robot as a tool. DIRL provides substantial improvements over the vanilla SFT (+12% on RoboSpatial) and RL (+16% on RoboSpatial) baselines. Project page: https://spacetools.github.io/.",
    "translation": "标题：SpaceTools：基于双重交互式强化学习的工具增强空间推理\n\n摘要：视觉语言模型在定性视觉理解方面展现出强大能力，但在具身应用所需的精确度量空间推理方面仍存在困难。智能体范式表明，视觉语言模型可通过调用多种工具（如深度估计器、分割模型和姿态估计器）来增强这些能力。然而，如何在不依赖人工提示策略或固定预定义工具流程（这些限制会阻碍模型发现最优工具使用模式）的前提下实现该愿景，仍是亟待解决的挑战。强化学习虽有望弥合这一差距，但由于多工具推理的搜索空间庞大，目前仍局限于单一视觉工具的应用。本文提出双重交互式强化学习框架，该训练框架通过交互探索与反馈机制，使视觉语言模型学会协调多种工具。在教学阶段，我们将通过交互式强化学习训练的单工具专家演示与使用全工具的前沿模型轨迹相结合；在探索阶段，模型通过持续强化学习进一步优化多工具协调能力。我们开发的SpaceTools模型具备工具增强的空间推理能力，在空间理解基准测试（RoboSpatial-Home、BLINK、BOP-ASK）中达到最先进性能，并成功将七自由度机器人作为工具实现可靠的现实世界操控。双重交互式强化学习方法相比标准监督微调基线（在RoboSpatial上提升12%）和强化学习基线（在RoboSpatial上提升16%）均有显著改进。项目页面：https://spacetools.github.io/。",
    "url": "https://huggingface.co/papers/2512.04069",
    "arxiv_url": "https://arxiv.org/abs/2512.04069"
  },
  {
    "title": "OneThinker: All-in-one Reasoning Model for Image and Video",
    "summary": "Reinforcement learning (RL) has recently achieved remarkable success in eliciting visual reasoning within Multimodal Large Language Models (MLLMs). However, existing approaches typically train separate models for different tasks and treat image and video reasoning as disjoint domains. This results in limited scalability toward a multimodal reasoning generalist, which restricts practical versatility and hinders potential knowledge sharing across tasks and modalities. To this end, we propose OneThinker, an all-in-one reasoning model that unifies image and video understanding across diverse fundamental visual tasks, including question answering, captioning, spatial and temporal grounding, tracking, and segmentation. To achieve this, we construct the OneThinker-600k training corpus covering all these tasks and employ commercial models for CoT annotation, resulting in OneThinker-SFT-340k for SFT cold start. Furthermore, we propose EMA-GRPO to handle reward heterogeneity in multi-task RL by tracking task-wise moving averages of reward standard deviations for balanced optimization. Extensive experiments on diverse visual benchmarks show that OneThinker delivers strong performance on 31 benchmarks, across 10 fundamental visual understanding tasks. Moreover, it exhibits effective knowledge transfer between certain tasks and preliminary zero-shot generalization ability, marking a step toward a unified multimodal reasoning generalist. All code, model, and data are released.",
    "translation": "标题：OneThinker：面向图像与视频的一体化推理模型\n\n摘要：强化学习近期在多模态大语言模型中激发视觉推理能力方面取得了显著成功。然而，现有方法通常针对不同任务训练独立模型，并将图像与视频推理视为分离的领域。这导致模型向多模态通用推理系统扩展的能力受限，既限制了实际应用的灵活性，也阻碍了跨任务与跨模态的知识共享。为此，我们提出OneThinker——一个一体化推理模型，能够统一处理涵盖问答、描述、时空定位、跟踪与分割等多种基础视觉任务的图像与视频理解。为实现这一目标，我们构建了覆盖全部上述任务的OneThinker-600k训练数据集，并利用商业模型进行思维链标注，最终得到用于监督微调冷启动的OneThinker-SFT-340k数据集。此外，我们提出EMA-GRPO方法，通过追踪各任务奖励标准差的移动平均值来处理多任务强化学习中的奖励异质性问题，从而实现均衡优化。在多样化视觉基准上的大量实验表明，OneThinker在10类基础视觉理解任务、31个基准测试中均展现出强劲性能。此外，该模型在特定任务间表现出有效的知识迁移能力，并具备初步的零样本泛化性能，标志着向统一多模态通用推理系统迈出了重要一步。所有代码、模型与数据均已公开。",
    "url": "https://huggingface.co/papers/2512.03043",
    "arxiv_url": "https://arxiv.org/abs/2512.03043"
  },
  {
    "title": "Rethinking Prompt Design for Inference-time Scaling in Text-to-Visual Generation",
    "summary": "Achieving precise alignment between user intent and generated visuals remains a central challenge in text-to-visual generation, as a single attempt often fails to produce the desired output. To handle this, prior approaches mainly scale the visual generation process (e.g., increasing sampling steps or seeds), but this quickly leads to a quality plateau. This limitation arises because the prompt, crucial for guiding generation, is kept fixed. To address this, we propose Prompt Redesign for Inference-time Scaling, coined PRIS, a framework that adaptively revises the prompt during inference in response to the scaled visual generations. The core idea of PRIS is to review the generated visuals, identify recurring failure patterns across visuals, and redesign the prompt accordingly before regenerating the visuals with the revised prompt. To provide precise alignment feedback for prompt revision, we introduce a new verifier, element-level factual correction, which evaluates the alignment between prompt attributes and generated visuals at a fine-grained level, achieving more accurate and interpretable assessments than holistic measures. Extensive experiments on both text-to-image and text-to-video benchmarks demonstrate the effectiveness of our approach, including a 15% gain on VBench 2.0. These results highlight that jointly scaling prompts and visuals is key to fully leveraging scaling laws at inference-time. Visualizations are available at the website: https://subin-kim-cv.github.io/PRIS.",
    "translation": "标题：文本到视觉生成中推理时扩展的提示设计再思考\n\n摘要：在文本到视觉生成中，实现用户意图与生成视觉内容之间的精确对齐仍是一个核心挑战，因为单次生成往往无法产生理想输出。为应对这一问题，现有方法主要侧重于扩展视觉生成过程（例如增加采样步数或种子数量），但这很快会导致质量提升陷入瓶颈。该局限性的根源在于指导生成的关键要素——提示词——在过程中保持固定。为此，我们提出推理时扩展的提示词重设计框架PRIS，该框架能够在推理过程中根据扩展生成的视觉内容自适应地修订提示词。PRIS的核心思想是：审阅已生成的视觉内容，识别其中反复出现的错误模式，据此重新设计提示词，再使用修订后的提示词重新生成视觉内容。为提供精准的提示词修订对齐反馈，我们引入了一种新型验证机制——元素级事实校正，该机制在细粒度层面评估提示词属性与生成视觉内容之间的对齐关系，相比整体性评估方法能实现更精准且可解释的判断。在文本到图像和文本到视频基准测试上的大量实验证明了我们方法的有效性，其中在VBench 2.0基准上实现了15%的性能提升。这些结果表明，在推理时联合扩展提示词与视觉生成是充分释放扩展定律潜力的关键。可视化结果可通过网站查看：https://subin-kim-cv.github.io/PRIS。",
    "url": "https://huggingface.co/papers/2512.03534",
    "arxiv_url": "https://arxiv.org/abs/2512.03534"
  },
  {
    "title": "RELIC: Interactive Video World Model with Long-Horizon Memory",
    "summary": "A truly interactive world model requires three key ingredients: real-time long-horizon streaming, consistent spatial memory, and precise user control. However, most existing approaches address only one of these aspects in isolation, as achieving all three simultaneously is highly challenging-for example, long-term memory mechanisms often degrade real-time performance. In this work, we present RELIC, a unified framework that tackles these three challenges altogether. Given a single image and a text description, RELIC enables memory-aware, long-duration exploration of arbitrary scenes in real time. Built upon recent autoregressive video-diffusion distillation techniques, our model represents long-horizon memory using highly compressed historical latent tokens encoded with both relative actions and absolute camera poses within the KV cache. This compact, camera-aware memory structure supports implicit 3D-consistent content retrieval and enforces long-term coherence with minimal computational overhead. In parallel, we fine-tune a bidirectional teacher video model to generate sequences beyond its original 5-second training horizon, and transform it into a causal student generator using a new memory-efficient self-forcing paradigm that enables full-context distillation over long-duration teacher as well as long student self-rollouts. Implemented as a 14B-parameter model and trained on a curated Unreal Engine-rendered dataset, RELIC achieves real-time generation at 16 FPS while demonstrating more accurate action following, more stable long-horizon streaming, and more robust spatial-memory retrieval compared with prior work. These capabilities establish RELIC as a strong foundation for the next generation of interactive world modeling.",
    "translation": "标题：RELIC：具备长时记忆的交互式视频世界模型\n\n摘要：一个真正交互式的世界模型需要三个关键要素：实时长时流式生成、一致的空间记忆以及精确的用户控制。然而，现有方法大多仅孤立地解决其中某一个方面，因为同时实现三者极具挑战性——例如，长时记忆机制往往会损害实时性能。本研究提出RELIC，一个统一应对这三项挑战的框架。给定单张图像和文本描述，RELIC能够实时对任意场景进行具备记忆感知的长时探索。基于近期自回归视频扩散蒸馏技术，本模型采用经过高度压缩的历史潜在标记来表示长时记忆，这些标记通过KV缓存中兼具相对动作与绝对相机位姿的编码实现。这种紧凑的相机感知记忆结构支持隐式的三维一致内容检索，并以最小计算开销保障长时连贯性。同时，我们微调了一个双向教师视频模型，使其能够生成超出原始5秒训练时长的序列，并通过一种新型内存高效的自强制范式将其转化为因果性学生生成器，该范式支持对长时教师序列及学生自生成序列进行全上下文蒸馏。RELIC实现为140亿参数模型，并在精心构建的Unreal Engine渲染数据集上训练，能够以16 FPS的速度实时生成视频。与先前工作相比，本模型展现出更精准的动作跟随、更稳定的长时流式生成以及更鲁棒的空间记忆检索能力。这些特性使RELIC成为新一代交互式世界建模的坚实基础。",
    "url": "https://huggingface.co/papers/2512.04040",
    "arxiv_url": "https://arxiv.org/abs/2512.04040"
  },
  {
    "title": "Thinking with Programming Vision: Towards a Unified View for Thinking with Images",
    "summary": "Multimodal large language models (MLLMs) that think with images can interactively use tools to reason about visual inputs, but current approaches often rely on a narrow set of tools with limited real-world necessity and scalability. In this work, we first reveal a critical and previously overlooked weakness: even state-of-the-art MLLMs are surprisingly brittle, showing significant performance degradation on images with simple orientation changes or natural corruptions, underscoring the need for more robust tool-based reasoning. To address this, we propose CodeVision, a flexible and scalable code-as-tool framework where the model generates code as a universal interface to invoke any image operation, moving beyond fixed tool registries. We train our model using a two-stage methodology, beginning with Supervised Fine-Tuning (SFT) on a high-quality dataset curated for complex, multi-turn tool composition and error recovery, followed by Reinforcement Learning (RL) with a novel and dense process reward function to encourage strategic and efficient tool use. To facilitate this research, we construct new SFT and RL datasets and introduce a challenging new benchmark suite designed to rigorously evaluate robustness to orientation changes and multi-tool reasoning. Experiments on Qwen2.5-VL and Qwen3-VL series show that our approach significantly improves model performance and fosters emergent capabilities such as flexible tool composition, efficient chained execution, and robust error recovery from runtime feedback. Code is available at https://github.com/ByteDance-BandAI/CodeVision.",
    "translation": "标题：基于编程视觉的思考：迈向图像思维的统一视角\n\n摘要：基于图像进行思考的多模态大语言模型（MLLMs）能够交互式地使用工具对视觉输入进行推理，但现有方法通常依赖于工具集较为有限，其现实必要性与可扩展性不足。本研究首先揭示了一个关键且此前被忽视的缺陷：即使是最先进的MLLMs也表现出惊人的脆弱性，在图像发生简单方向变化或自然退化时性能显著下降，这凸显了基于工具的推理需要更强的鲁棒性。为解决这一问题，我们提出了CodeVision——一个灵活、可扩展的“代码即工具”框架。该框架以生成代码作为通用接口来调用任意图像操作，从而超越固定的工具注册机制。我们采用两阶段方法训练模型：首先在精心构建的高质量数据集上进行监督微调（SFT），该数据集专注于复杂多轮工具组合与错误恢复；随后进行强化学习（RL），并设计了一种新颖且密集的过程奖励函数，以鼓励策略性、高效的工具使用。为支持相关研究，我们构建了全新的SFT与RL数据集，并引入一套具有挑战性的新基准测试集，旨在严格评估模型对方向变化的鲁棒性及多工具推理能力。在Qwen2.5-VL和Qwen3-VL系列模型上的实验表明，我们的方法显著提升了模型性能，并催生了灵活工具组合、高效链式执行、基于运行时反馈的鲁棒错误恢复等新兴能力。代码已开源：https://github.com/ByteDance-BandAI/CodeVision。",
    "url": "https://huggingface.co/papers/2512.03746",
    "arxiv_url": "https://arxiv.org/abs/2512.03746"
  },
  {
    "title": "Flowing Backwards: Improving Normalizing Flows via Reverse Representation Alignment",
    "summary": "Normalizing Flows (NFs) are a class of generative models distinguished by a mathematically invertible architecture, where the forward pass transforms data into a latent space for density estimation, and the reverse pass generates new samples from this space. This characteristic creates an intrinsic synergy between representation learning and data generation. However, the generative quality of standard NFs is limited by poor semantic representations from log-likelihood optimization. To remedy this, we propose a novel alignment strategy that creatively leverages the invertibility of NFs: instead of regularizing the forward pass, we align the intermediate features of the generative (reverse) pass with representations from a powerful vision foundation model, demonstrating superior effectiveness over naive alignment. We also introduce a novel training-free, test-time optimization algorithm for classification, which provides a more intrinsic evaluation of the NF's embedded semantic knowledge. Comprehensive experiments demonstrate that our approach accelerates the training of NFs by over 3.3times, while simultaneously delivering significant improvements in both generative quality and classification accuracy. New state-of-the-art results for NFs are established on ImageNet 64times64 and 256times256. Our code is available at https://github.com/MCG-NJU/FlowBack.",
    "translation": "标题：逆向流动：通过反向表示对齐改进标准化流模型\n\n摘要：标准化流是一类以数学可逆架构为特征的生成模型，其前向过程将数据转换至隐空间以进行密度估计，而反向过程则从该空间生成新样本。这一特性在表示学习与数据生成之间形成了内在的协同机制。然而，传统标准化流的生成质量受限于基于对数似然优化所获得的语义表示能力不足。为解决此问题，本文提出一种创新的对齐策略，该策略创造性地利用标准化流的可逆特性：不同于常规的前向过程正则化方法，我们将生成（反向）过程的中间特征与强大视觉基础模型的表示进行对齐，实验证明该方法相较于简单对齐策略具有显著优势。同时，我们提出一种无需训练、基于测试时优化的新型分类算法，为评估标准化流中嵌入的语义知识提供了更本质的衡量方式。综合实验表明，所提方法将标准化流的训练速度提升3.3倍以上，同时在生成质量与分类精度方面均取得显著提升。我们在ImageNet 64×64和256×256数据集上取得了当前最优的标准化流模型性能。代码已开源：https://github.com/MCG-NJU/FlowBack。",
    "url": "https://huggingface.co/papers/2511.22345",
    "arxiv_url": "https://arxiv.org/abs/2511.22345"
  },
  {
    "title": "Jina-VLM: Small Multilingual Vision Language Model",
    "summary": "We present Jina-VLM, a 2.4B parameter vision-language model that achieves state-of-the-art multilingual visual question answering among open 2B-scale VLMs. The model couples a SigLIP2 vision encoder with a Qwen3 language backbone through an attention-pooling connector that enables token-efficient processing of arbitrary-resolution images. Across standard VQA benchmarks and multilingual evaluations, Jina-VLM outperforms comparable models while preserving competitive text-only performance.",
    "translation": "标题：Jina-VLM：小型多语言视觉语言模型\n\n摘要：本文提出Jina-VLM，这是一个拥有24亿参数的视觉语言模型，在公开的20亿参数规模视觉语言模型中实现了最先进的多语言视觉问答性能。该模型通过注意力池化连接器将SigLIP2视觉编码器与Qwen3语言主干网络耦合，能够以高效令牌处理方式解析任意分辨率的图像。在标准视觉问答基准测试和多语言评估中，Jina-VLM在保持竞争力的纯文本性能的同时，表现优于同类可比模型。",
    "url": "https://huggingface.co/papers/2512.04032",
    "arxiv_url": "https://arxiv.org/abs/2512.04032"
  },
  {
    "title": "CookAnything: A Framework for Flexible and Consistent Multi-Step Recipe Image Generation",
    "summary": "Cooking is a sequential and visually grounded activity, where each step such as chopping, mixing, or frying carries both procedural logic and visual semantics. While recent diffusion models have shown strong capabilities in text-to-image generation, they struggle to handle structured multi-step scenarios like recipe illustration. Additionally, current recipe illustration methods are unable to adjust to the natural variability in recipe length, generating a fixed number of images regardless of the actual instructions structure. To address these limitations, we present CookAnything, a flexible and consistent diffusion-based framework that generates coherent, semantically distinct image sequences from textual cooking instructions of arbitrary length. The framework introduces three key components: (1) Step-wise Regional Control (SRC), which aligns textual steps with corresponding image regions within a single denoising process; (2) Flexible RoPE, a step-aware positional encoding mechanism that enhances both temporal coherence and spatial diversity; and (3) Cross-Step Consistency Control (CSCC), which maintains fine-grained ingredient consistency across steps. Experimental results on recipe illustration benchmarks show that CookAnything performs better than existing methods in training-based and training-free settings. The proposed framework supports scalable, high-quality visual synthesis of complex multi-step instructions and holds significant potential for broad applications in instructional media, and procedural content creation.",
    "translation": "标题：CookAnything：一种灵活且一致的多步骤食谱图像生成框架\n\n摘要：烹饪是一种具有时序性和视觉依赖性的活动，其中如切菜、搅拌或煎炸等每个步骤都同时包含程序逻辑与视觉语义。尽管近期扩散模型在文本到图像生成方面展现出强大能力，但其在处理如食谱图解这类结构化多步骤场景时仍面临挑战。此外，现有食谱图解方法无法适应食谱步骤数量的自然变化，无论实际指令结构如何均生成固定数量的图像。为应对这些局限性，本文提出CookAnything——一种基于扩散模型的灵活且一致的框架，能够根据任意长度的文本烹饪指令生成连贯且语义分明的图像序列。该框架包含三个核心组件：（1）步骤区域控制，可在单一去噪过程中将文本步骤与对应图像区域对齐；（2）灵活旋转位置编码，这是一种感知步骤顺序的位置编码机制，能同时增强时序连贯性与空间多样性；（3）跨步骤一致性控制，用于保持不同步骤间食材细节的一致性。在食谱图解基准测试上的实验结果表明，CookAnything在基于训练和无训练设置下均优于现有方法。所提框架支持对复杂多步骤指令进行可扩展的高质量视觉合成，在指导性媒体和流程化内容创作领域具有广泛的应用潜力。",
    "url": "https://huggingface.co/papers/2512.03540",
    "arxiv_url": "https://arxiv.org/abs/2512.03540"
  },
  {
    "title": "AutoNeural: Co-Designing Vision-Language Models for NPU Inference",
    "summary": "While Neural Processing Units (NPUs) offer high theoretical efficiency for edge AI, state-of-the-art Vision--Language Models (VLMs) tailored for GPUs often falter on these substrates. We attribute this hardware-model mismatch to two primary factors: the quantization brittleness of Vision Transformers (ViTs) and the I/O-bound nature of autoregressive attention mechanisms, which fail to utilize the high arithmetic throughput of NPUs. To bridge this gap, we propose AutoNeural, an NPU-native VLM architecture co-designed for integer-only inference. We replace the standard ViT encoder with a MobileNetV5-style backbone utilizing depthwise separable convolutions, which ensures bounded activation distributions for stable INT4/8/16 quantization. Complementing this, our language backbone integrates State-Space Model (SSM) principles with Transformer layers, employing efficient gated convolutions to achieve linear-time complexity. This hybrid design eliminates the heavy memory I/O overhead of Key-Value caching during generation. Our approach delivers substantial efficiency gains, reducing quantization error of vision encoder by up to 7x and end-to-end latency by 14x compared to conventional baselines. The AutoNeural also delivers 3x decoding speed and 4x longer context window than the baseline. We validate these improvements via a real-world automotive case study on the Qualcomm SA8295P SoC, demonstrating real-time performance for cockpit applications. Our results highlight that rethinking model topology specifically for NPU constraints is a prerequisite for robust multi-modal edge intelligence.",
    "translation": "标题：AutoNeural：面向NPU推理的视觉-语言模型协同设计\n\n摘要：尽管神经处理单元（NPU）在边缘人工智能领域具备较高的理论效率，但专为GPU优化的先进视觉-语言模型（VLM）在这些硬件平台上往往表现不佳。我们将这种硬件与模型之间的不匹配归因于两个主要因素：视觉变换器（ViT）的量化脆弱性，以及自回归注意力机制受输入/输出限制的特性，这些特性无法充分利用NPU的高算术吞吐量。为弥合这一差距，我们提出了AutoNeural——一种专为纯整数推理协同设计的原生NPU视觉-语言模型架构。我们采用基于深度可分离卷积的MobileNetV5风格主干网络替代标准ViT编码器，该设计通过约束激活值分布实现了稳定的INT4/8/16量化。与此相配合，我们的语言主干网络将状态空间模型（SSM）原理与Transformer层相结合，采用高效门控卷积实现线性时间复杂度。这种混合设计消除了生成过程中键值缓存带来的沉重内存I/O开销。相较于传统基线方法，我们的方案显著提升了效率：视觉编码器的量化误差降低高达7倍，端到端延迟减少14倍。AutoNeural的解码速度达到基线的3倍，上下文窗口长度扩展至基线的4倍。我们通过基于高通SA8295P系统级芯片的实际汽车案例研究验证了这些改进，展示了其在座舱应用场景中实现实时性能的能力。研究结果强调，针对NPU硬件约束重新设计模型拓扑结构是实现鲁棒多模态边缘智能的必要前提。",
    "url": "https://huggingface.co/papers/2512.02924",
    "arxiv_url": "https://arxiv.org/abs/2512.02924"
  },
  {
    "title": "SR-GRPO: Stable Rank as an Intrinsic Geometric Reward for Large Language Model Alignment",
    "summary": "Aligning Large Language Models (LLMs) with human preferences typically relies on external supervision, which faces critical limitations: human annotations are scarce and subjective, reward models are vulnerable to reward hacking, and self-evaluation methods suffer from prompt sensitivity and biases. In this work, we propose stable rank, an intrinsic, annotation-free quality signal derived from model representations. Stable rank measures the effective dimensionality of hidden states by computing the ratio of total variance to dominant-direction variance, capturing quality through how information distributes across representation dimensions. Empirically, stable rank achieves 84.04% accuracy on RewardBench and improves task accuracy by an average of 11.3 percentage points over greedy decoding via Best-of-N sampling. Leveraging this insight, we introduce Stable Rank Group Relative Policy Optimization (SR-GRPO), which uses stable rank as a reward signal for reinforcement learning. Without external supervision, SR-GRPO improves Qwen2.5-1.5B-Instruct by 10% on STEM and 19% on mathematical reasoning, outperforming both learned reward models and self-evaluation baselines. Our findings demonstrate that quality signals can be extracted from internal model geometry, offering a path toward scalable alignment without external supervision.",
    "translation": "标题：SR-GRPO：将稳定秩作为大语言模型对齐的内在几何奖励信号\n\n摘要：将大语言模型（LLM）与人类偏好对齐通常依赖于外部监督，但这种方法存在关键局限：人工标注稀缺且主观，奖励模型易受奖励攻击影响，而自评估方法则受提示敏感性和偏差的困扰。本研究提出稳定秩——一种从模型内部表示中提取的、无需标注的内在质量信号。稳定秩通过计算总方差与主方向方差的比值，衡量隐藏状态的有效维度，从而通过信息在表示维度间的分布方式来捕捉生成质量。实验表明，稳定秩在RewardBench上达到84.04%的准确率，并通过Best-of-N采样策略将任务准确率较贪婪解码平均提升11.3个百分点。基于这一发现，我们提出稳定秩分组相对策略优化（SR-GRPO），将稳定秩作为强化学习的奖励信号。在无外部监督的情况下，SR-GRPO将Qwen2.5-1.5B-Instruct模型在STEM任务上的性能提升10%，在数学推理任务上提升19%，其表现优于基于学习的奖励模型和自评估基线方法。我们的研究证明，质量信号可以从模型内部几何结构中提取，这为无需外部监督的可扩展对齐提供了新路径。",
    "url": "https://huggingface.co/papers/2512.02807",
    "arxiv_url": "https://arxiv.org/abs/2512.02807"
  },
  {
    "title": "Economies of Open Intelligence: Tracing Power & Participation in the Model Ecosystem",
    "summary": "Since 2019, the Hugging Face Model Hub has been the primary global platform for sharing open weight AI models. By releasing a dataset of the complete history of weekly model downloads (June 2020-August 2025) alongside model metadata, we provide the most rigorous examination to-date of concentration dynamics and evolving characteristics in the open model economy. Our analysis spans 851,000 models, over 200 aggregated attributes per model, and 2.2B downloads. We document a fundamental rebalancing of economic power: US open-weight industry dominance by Google, Meta, and OpenAI has declined sharply in favor of unaffiliated developers, community organizations, and, as of 2025, Chinese industry, with DeepSeek and Qwen models potentially heralding a new consolidation of market power. We identify statistically significant shifts in model properties, a 17X increase in average model size, rapid growth in multimodal generation (3.4X), quantization (5X), and mixture-of-experts architectures (7X), alongside concerning declines in data transparency, with open weights models surpassing truly open source models for the first time in 2025. We expose a new layer of developer intermediaries that has emerged, focused on quantizing and adapting base models for both efficiency and artistic expression. To enable continued research and oversight, we release the complete dataset with an interactive dashboard for real-time monitoring of concentration dynamics and evolving properties in the open model economy.",
    "translation": "标题：开放智能的经济学：模型生态系统中的权力与参与轨迹\n\n摘要：自2019年以来，Hugging Face模型中心已成为全球共享开放权重AI模型的核心平台。通过发布涵盖完整历史周期的每周模型下载数据集（2020年6月至2025年8月）及模型元数据，本研究对开放模型经济中的集中度动态与演化特征进行了迄今最严谨的实证分析。我们的研究涵盖85.1万个模型、每个模型超过200项聚合属性及22亿次下载记录。研究发现经济权力格局发生根本性重构：以谷歌、Meta和OpenAI为代表的美国开放权重产业主导地位显著削弱，独立开发者、社区组织及至2025年崛起的中国产业力量（以DeepSeek和Qwen模型为代表）正重塑市场格局，后者可能预示着新一轮市场权力整合。我们通过统计显著性检验发现：模型平均规模增长17倍，多模态生成（3.4倍）、量化技术（5倍）与专家混合架构（7倍）实现爆发式增长，但数据透明度呈现令人担忧的下降趋势——2025年开放权重模型数量首次超越真正开源模型。研究还揭示出新兴开发者中介层的形成，其专注于通过量化与适配基础模型以实现效能优化与艺术表达。为支持持续研究与监管，我们同步发布完整数据集及交互式仪表板，实现对开放模型经济集中度动态与演化特征的实时监测。",
    "url": "https://huggingface.co/papers/2512.03073",
    "arxiv_url": "https://arxiv.org/abs/2512.03073"
  },
  {
    "title": "In-Context Representation Hijacking",
    "summary": "We introduce Doublespeak, a simple in-context representation hijacking attack against large language models (LLMs). The attack works by systematically replacing a harmful keyword (e.g., bomb) with a benign token (e.g., carrot) across multiple in-context examples, provided a prefix to a harmful request. We demonstrate that this substitution leads to the internal representation of the benign token converging toward that of the harmful one, effectively embedding the harmful semantics under a euphemism. As a result, superficially innocuous prompts (e.g., ``How to build a carrot?'') are internally interpreted as disallowed instructions (e.g., ``How to build a bomb?''), thereby bypassing the model's safety alignment. We use interpretability tools to show that this semantic overwrite emerges layer by layer, with benign meanings in early layers converging into harmful semantics in later ones. Doublespeak is optimization-free, broadly transferable across model families, and achieves strong success rates on closed-source and open-source systems, reaching 74\\% ASR on Llama-3.3-70B-Instruct with a single-sentence context override. Our findings highlight a new attack surface in the latent space of LLMs, revealing that current alignment strategies are insufficient and should instead operate at the representation level.",
    "translation": "标题：上下文表征劫持\n\n摘要：本文提出一种名为\"双关语\"的简单上下文表征劫持攻击方法，针对大语言模型实施攻击。该攻击通过在有害请求前缀后的多个上下文示例中，系统性地将有害关键词（如\"炸弹\"）替换为良性词汇（如\"胡萝卜\"）来实现。我们证明这种替换会导致良性词汇的内部表征向有害词汇收敛，从而在委婉语形式下嵌入有害语义。其结果是，表面无害的提示（如\"如何制作胡萝卜？\"）在模型内部会被解释为禁止性指令（如\"如何制作炸弹？\"），从而绕过模型的安全对齐机制。我们利用可解释性工具揭示这种语义覆盖是逐层形成的：早期层的良性语义在深层逐渐收敛为有害语义。该方法无需优化过程，可跨模型家族广泛迁移，在闭源和开源系统中均实现较高成功率，在单句上下文覆盖条件下对Llama-3.3-70B-Instruct的攻击成功率可达74%。本研究揭示了大语言模型潜在空间中的新型攻击面，表明当前基于表层文本的对齐策略存在不足，安全机制应延伸至表征层面进行构建。",
    "url": "https://huggingface.co/papers/2512.03771",
    "arxiv_url": "https://arxiv.org/abs/2512.03771"
  },
  {
    "title": "UniQL: Unified Quantization and Low-rank Compression for Adaptive Edge LLMs",
    "summary": "Deploying large language model (LLM) models on mobile platforms faces significant challenges due to the limited memory and shared computational resources of the device. Resource availability may be an issue as it is directly impacted by the current device workload, adding to the uncertainty of model deployment. We introduce UniQL, a unified post-training quantization and low-rank compression framework with on-device configurable pruning rates for edge LLMs. UniQL is a general framework that integrates quantization and low-rank compression for Transformers, State Space Models (SSMs), and hybrid models to support diverse edge applications. In our proposed joint framework, we introduce an efficient structured weight-sorting method that speeds up computation by 20x, quantization-aware singular value decomposition (SVD) to minimize quantization errors, state-aware weight sorting for SSMs, and a fused rotary positional embedding (RoPE) kernel for pruned models. Our framework performs weight-sorting, fine-tuning, and quantization in the cloud in a single-pass workflow, while enabling on-device configurable pruning rates up to 35%. Our experiments show that quantized and pruned models achieve a memory reduction of 4x-5.7x and a token-throughput improvement of 2.7x-3.4x, maintaining accuracy within 5% of the original models at 15% pruning across Transformers (Llama3 and Qwen2.5), SSMs (Mamba2), and hybrid models (Nemotron-H and Bamba-v2). The code and quantized models are available at: https://github.com/enyac-group/UniQL.",
    "translation": "标题：UniQL：面向自适应边缘大语言模型的统一量化与低秩压缩框架\n\n摘要：在移动平台上部署大语言模型面临显著挑战，主要受限于设备有限的内存和共享的计算资源。资源可用性可能成为问题，因为它直接受到当前设备工作负载的影响，增加了模型部署的不确定性。本文提出UniQL，一种统一的训练后量化与低秩压缩框架，支持设备端可配置的剪枝率，专为边缘大语言模型设计。UniQL是一个通用框架，集成了针对Transformer、状态空间模型（SSMs）以及混合模型的量化与低秩压缩技术，以支持多样化的边缘应用。在我们提出的联合框架中，我们引入了一种高效的结构化权重排序方法，可将计算速度提升20倍；采用量化感知的奇异值分解（SVD）以最小化量化误差；针对SSMs设计了状态感知的权重排序策略；并为剪枝后的模型开发了融合旋转位置编码（RoPE）内核。我们的框架在云端以单次工作流完成权重排序、微调与量化，同时支持设备端最高35%的可配置剪枝率。实验结果表明，经过量化与剪枝的模型实现了4倍至5.7倍的内存压缩和2.7倍至3.4倍的令牌吞吐量提升，在Transformer（Llama3与Qwen2.5）、SSMs（Mamba2）以及混合模型（Nemotron-H与Bamba-v2）上，以15%的剪枝率保持准确率损失在原始模型的5%以内。代码与量化模型已公开于：https://github.com/enyac-group/UniQL。",
    "url": "https://huggingface.co/papers/2512.03383",
    "arxiv_url": "https://arxiv.org/abs/2512.03383"
  },
  {
    "title": "AlignBench: Benchmarking Fine-Grained Image-Text Alignment with Synthetic Image-Caption Pairs",
    "summary": "Assessing image-text alignment models such as CLIP is crucial for bridging visual and linguistic representations. Yet existing benchmarks rely on rule-based perturbations or short captions, limiting their ability to measure fine-grained alignment. We introduce AlignBench, a benchmark that provides a new indicator of image-text alignment by evaluating detailed image-caption pairs generated by diverse image-to-text and text-to-image models. Each sentence is annotated for correctness, enabling direct assessment of VLMs as alignment evaluators. Benchmarking a wide range of decoder-based VLMs reveals three key findings: (i) CLIP-based models, even those tailored for compositional reasoning, remain nearly blind; (ii) detectors systematically over-score early sentences; and (iii) they show strong self-preference, favoring their own outputs and harming detection performance. Our project page will be available at https://dahlian00.github.io/AlignBench/.",
    "translation": "标题：AlignBench：基于合成图像-描述对评估细粒度图文对齐的基准测试\n\n摘要：评估如CLIP等图文对齐模型对于连接视觉与语言表征至关重要。然而，现有基准测试多依赖于基于规则的扰动或简短描述，限制了其衡量细粒度对齐的能力。本文提出AlignBench，该基准通过评估由多样化图像到文本及文本到图像模型生成的精细图像-描述对，为图文对齐提供了新的评估指标。每个句子均经过正确性标注，从而能够直接评估视觉语言模型作为对齐评估器的性能。通过对一系列基于解码器的视觉语言模型进行基准测试，我们得出三个关键发现：（一）基于CLIP的模型，即使是专门针对组合推理优化的模型，仍几乎无法实现有效对齐；（二）检测器系统性地对早期句子评分过高；（三）这些模型表现出强烈的自我偏好，倾向于青睐自身输出，从而损害了检测性能。项目页面详见：https://dahlian00.github.io/AlignBench/。",
    "url": "https://huggingface.co/papers/2511.20515",
    "arxiv_url": "https://arxiv.org/abs/2511.20515"
  },
  {
    "title": "SkillFactory: Self-Distillation For Learning Cognitive Behaviors",
    "summary": "Reasoning models leveraging long chains of thought employ various cognitive skills, such as verification of their answers, backtracking, retrying by an alternate method, and more. Previous work has shown that when a base language model exhibits these skills, training that model further with reinforcement learning (RL) can learn to leverage them. How can we get models to leverage skills that aren't exhibited by base models? Our work, SkillFactory, is a method for fine-tuning models to roughly learn these skills during a supervised fine-tuning (SFT) stage prior to RL. Our approach does not rely on distillation from a stronger model, but instead uses samples from the model itself, rearranged to provide training data in the format of those skills. These \"silver\" SFT traces may be imperfect, but are nevertheless effective for priming a model to acquire skills during RL. Our evaluation shows that (1) starting from SkillFactory SFT initialization helps a model to generalize to harder variants of a task post-RL, despite lower performance pre-RL; (2) cognitive skills are indeed used by the model; (3) RLed SkillFactory models are more robust to regression on out-of-domain tasks than RLed base models. Our work suggests that inductive biases learned prior to RL help models learn robust cognitive skill use.",
    "translation": "标题：SkillFactory：用于学习认知行为的自蒸馏方法\n\n摘要：利用长链思维进行推理的模型需要运用多种认知技能，例如答案验证、回溯、采用替代方法重试等。先前研究表明，当基础语言模型具备这些技能时，通过强化学习进一步训练可以使其学会运用这些技能。但如何让模型掌握基础模型尚未展现的技能？本研究提出的SkillFactory方法，通过在强化学习前进行监督微调，使模型初步掌握这些认知技能。该方法不依赖于从更强模型进行知识蒸馏，而是通过对模型自身生成样本进行重组，构建符合目标技能形式的训练数据。这些\"银标\"监督微调轨迹可能不够完美，但能有效引导模型在强化学习阶段掌握目标技能。实验评估表明：（1）基于SkillFactory监督微调初始化的模型在强化学习后能更好地泛化至任务的高难度变体，尽管其在强化学习前阶段表现较弱；（2）模型确实运用了所训练的认知技能；（3）经过强化学习的SkillFactory模型相比强化学习的基础模型，在跨领域任务上表现出更强的抗性能衰退能力。本研究证明，在强化学习前获得的归纳偏置有助于模型建立稳健的认知技能运用机制。",
    "url": "https://huggingface.co/papers/2512.04072",
    "arxiv_url": "https://arxiv.org/abs/2512.04072"
  },
  {
    "title": "BlurDM: A Blur Diffusion Model for Image Deblurring",
    "summary": "Diffusion models show promise for dynamic scene deblurring; however, existing studies often fail to leverage the intrinsic nature of the blurring process within diffusion models, limiting their full potential. To address it, we present a Blur Diffusion Model (BlurDM), which seamlessly integrates the blur formation process into diffusion for image deblurring. Observing that motion blur stems from continuous exposure, BlurDM implicitly models the blur formation process through a dual-diffusion forward scheme, diffusing both noise and blur onto a sharp image. During the reverse generation process, we derive a dual denoising and deblurring formulation, enabling BlurDM to recover the sharp image by simultaneously denoising and deblurring, given pure Gaussian noise conditioned on the blurred image as input. Additionally, to efficiently integrate BlurDM into deblurring networks, we perform BlurDM in the latent space, forming a flexible prior generation network for deblurring. Extensive experiments demonstrate that BlurDM significantly and consistently enhances existing deblurring methods on four benchmark datasets. The source code is available at https://github.com/Jin-Ting-He/BlurDM.",
    "translation": "标题：BlurDM：一种用于图像去模糊的模糊扩散模型\n\n摘要：扩散模型在动态场景去模糊任务中展现出潜力；然而，现有研究往往未能充分利用扩散模型中模糊过程的内在特性，限制了其潜力的充分发挥。为解决这一问题，我们提出了一种模糊扩散模型（BlurDM），该模型将模糊形成过程无缝集成到扩散框架中，以实现图像去模糊。通过观察发现运动模糊源于连续曝光，BlurDM通过一种双扩散前向方案隐式建模模糊形成过程，将噪声和模糊同时扩散到清晰图像上。在反向生成过程中，我们推导出一种双重去噪与去模糊的数学表达，使得BlurDM能够在以模糊图像为条件的纯高斯噪声输入下，通过同步去噪与去模糊来恢复清晰图像。此外，为将BlurDM高效集成至去模糊网络中，我们在潜在空间中执行BlurDM，构建了一个灵活的先验生成网络用于去模糊任务。大量实验表明，BlurDM在四个基准数据集上显著且持续地提升了现有去模糊方法的性能。源代码公开于：https://github.com/Jin-Ting-He/BlurDM。",
    "url": "https://huggingface.co/papers/2512.03979",
    "arxiv_url": "https://arxiv.org/abs/2512.03979"
  },
  {
    "title": "AdaptVision: Efficient Vision-Language Models via Adaptive Visual Acquisition",
    "summary": "Vision-Language Models (VLMs) have achieved remarkable success in visual question answering tasks, but their reliance on large numbers of visual tokens introduces significant computational overhead. While existing efficient VLM approaches reduce visual tokens through fixed-ratio compression, they operate passively and lack the ability to adapt to varying task requirements. This motivates a fundamental question: Can VLMs autonomously determine the minimum number of visual tokens required for each sample? Inspired by human active vision mechanisms, we introduce AdaptVision, an efficient VLM paradigm that enables adaptive visual token acquisition through a coarse-to-fine approach. Our model initially processes compressed visual tokens from low-resolution images and selectively acquires additional visual information by invoking a bounding box tool to crop key regions when necessary. We train AdaptVision using a reinforcement learning framework that carefully balances accuracy and efficiency. Central to our approach is Decoupled Turn Policy Optimization (DTPO), which decouples the learning objective into two components: (1) tool learning, which optimizes correct tool utilization, and (2) accuracy improvement, which refines the generated responses to improve answer correctness. Based on this formulation, we further decouple advantage estimation by computing separate advantages for tokens associated with each objective. This formulation enables more effective optimization for AdaptVision compared to vanilla GRPO. Comprehensive experiments across multiple VQA benchmarks demonstrate that AdaptVision achieves superior performance while consuming substantially fewer visual tokens than state-of-the-art efficient VLM methods.",
    "translation": "标题：AdaptVision：通过自适应视觉采集实现高效视觉语言模型\n\n摘要：视觉语言模型在视觉问答任务中取得了显著成功，但其对大量视觉令牌的依赖带来了巨大的计算开销。现有高效视觉语言模型方法虽通过固定比例压缩减少视觉令牌，但这类方法被动运行且缺乏适应不同任务需求的能力。这引出了一个根本性问题：视觉语言模型能否自主确定每个样本所需的最小视觉令牌数量？受人类主动视觉机制启发，我们提出了AdaptVision——一种通过由粗到精方式实现自适应视觉令牌采集的高效视觉语言模型范式。该模型首先处理低分辨率图像生成的压缩视觉令牌，并在必要时通过调用边界框工具裁剪关键区域来选择性获取额外视觉信息。我们采用强化学习框架训练AdaptVision，精心平衡准确性与效率。方法的核心是解耦轮次策略优化，该技术将学习目标分解为两个部分：（1）工具学习——优化工具使用的正确性；（2）精度提升——完善生成响应以提高答案准确性。基于此框架，我们进一步通过计算各目标对应令牌的独立优势值实现优势估计解耦。相较于原始GRPO方法，该框架能为AdaptVision实现更有效的优化。在多个视觉问答基准测试中的综合实验表明，AdaptVision在消耗视觉令牌数量显著少于当前最先进高效视觉语言模型的同时，实现了更优越的性能表现。",
    "url": "https://huggingface.co/papers/2512.03794",
    "arxiv_url": "https://arxiv.org/abs/2512.03794"
  },
  {
    "title": "PosterCopilot: Toward Layout Reasoning and Controllable Editing for Professional Graphic Design",
    "summary": "Graphic design forms the cornerstone of modern visual communication, serving as a vital medium for promoting cultural and commercial events. Recent advances have explored automating this process using Large Multimodal Models (LMMs), yet existing methods often produce geometrically inaccurate layouts and lack the iterative, layer-specific editing required in professional workflows. To address these limitations, we present PosterCopilot, a framework that advances layout reasoning and controllable editing for professional graphic design. Specifically, we introduce a progressive three-stage training strategy that equips LMMs with geometric understanding and aesthetic reasoning for layout design, consisting of Perturbed Supervised Fine-Tuning, Reinforcement Learning for Visual-Reality Alignment, and Reinforcement Learning from Aesthetic Feedback. Furthermore, we develop a complete workflow that couples the trained LMM-based design model with generative models, enabling layer-controllable, iterative editing for precise element refinement while maintaining global visual consistency. Extensive experiments demonstrate that PosterCopilot achieves geometrically accurate and aesthetically superior layouts, offering unprecedented controllability for professional iterative design.",
    "translation": "标题：PosterCopilot：面向专业平面设计的布局推理与可控编辑\n\n摘要：平面设计是现代视觉传达的基石，是推广文化与商业活动的重要媒介。近期研究尝试利用大型多模态模型实现这一过程的自动化，但现有方法常产生几何不准确的布局，且缺乏专业工作流程所需的迭代式、图层级编辑能力。为应对这些局限，本文提出PosterCopilot框架，以推进专业平面设计中的布局推理与可控编辑。具体而言，我们引入渐进式三阶段训练策略，通过扰动监督微调、视觉-现实对齐强化学习以及美学反馈强化学习，使大型多模态模型具备布局设计所需的几何理解与美学推理能力。此外，我们构建了完整工作流程，将训练后的基于大型多模态模型的设计系统与生成模型耦合，实现图层可控的迭代式编辑，在保持整体视觉一致性的同时支持精确的视觉元素优化。大量实验表明，PosterCopilot能够生成几何精确且美学表现优异的布局，为专业迭代设计提供了前所未有的可控性。",
    "url": "https://huggingface.co/papers/2512.04082",
    "arxiv_url": "https://arxiv.org/abs/2512.04082"
  },
  {
    "title": "PSA: Pyramid Sparse Attention for Efficient Video Understanding and Generation",
    "summary": "Attention mechanisms are the core of foundation models, but their quadratic complexity remains a critical bottleneck for scaling. This challenge has driven the development of efficient attention mechanisms, with sparsity emerging as the dominant paradigm. Current methods typically retain or discard entire key-value blocks with binary masks, resulting in substantial information loss under high sparsity. To mitigate this gap, we present Pyramid Sparse Attention (PSA), a versatile module applicable to both video understanding and generation tasks. Instead of binary masking, PSA introduces multi-level pooled KV representations, enabling finer mask granularity. Specifically, each query block dynamically allocates lower pooling levels to critical KV blocks and higher levels to less important ones, creating an informative interpolation between full retention and complete pruning. This design, analogous to fixed-point quantization and classical feature pyramid networks in computer vision, effectively mitigates information loss while preserving computational efficiency under a low compute budget. It works with a native, hardware-friendly kernel that leverages decoupled block-tile design to ensure efficient execution. Across video understanding and generation benchmarks, PSA preserves contextual information and visual fidelity, consistently outperforming or achieving comparable performance over existing sparse attention baselines with superior efficiency-quality trade-offs. Our code and model weights are publicly available at: http://ziplab.co/PSA",
    "translation": "标题：PSA：用于高效视频理解与生成的金字塔稀疏注意力机制\n\n摘要：注意力机制是基础模型的核心，但其二次计算复杂度仍是模型扩展的关键瓶颈。这一挑战推动了高效注意力机制的发展，其中稀疏化已成为主流范式。现有方法通常通过二值掩码保留或丢弃完整的键值块，导致在高稀疏度下出现显著信息损失。为缓解这一问题，我们提出金字塔稀疏注意力（PSA），这是一种可同时应用于视频理解与生成任务的通用模块。PSA摒弃二值掩码，引入多层级池化的键值表示，从而实现更精细的掩码粒度。具体而言，每个查询块动态分配较低池化层级给关键键值块，而对次要键值块分配较高层级，从而在完整保留与完全剪枝之间构建信息丰富的插值。该设计借鉴了计算机视觉中的定点量化思想和经典特征金字塔网络，在有限计算预算下既能保持计算效率，又能有效减少信息损失。PSA采用原生硬件友好型内核，通过解耦的块-瓦片设计确保高效执行。在视频理解与生成的基准测试中，PSA在保持上下文信息与视觉保真度的同时，始终优于或达到现有稀疏注意力基线模型的性能水平，并展现出更优的效率-质量平衡。我们的代码与模型权重已公开于：http://ziplab.co/PSA",
    "url": "https://huggingface.co/papers/2512.04025",
    "arxiv_url": "https://arxiv.org/abs/2512.04025"
  },
  {
    "title": "Divide, then Ground: Adapting Frame Selection to Query Types for Long-Form Video Understanding",
    "summary": "The application of Large Multimodal Models (LMMs) to long-form video understanding is constrained by limited context lengths and the computationally prohibitive cost of processing dense video tokens. Consequently, recent research has focused on query-aware frame selection, methods that often incur significant computational overhead. This paper challenges the assumption that such complex search mechanisms are universally necessary. We first identify and validate a query typology distinguishing between global query and localized query. We demonstrate that while uniform sampling is both effective and efficient for global queries, localized queries indeed necessitate query-aware selection for optimal performance. Building on this insight, we propose DIG, a training-free frame selection framework that adapts its strategy based on the query type. Specifically,DIG employs efficient uniform sampling for global queries while activating a specialized pipeline to extract query-relevant frames for localized queries. Experiments on three long-form video understanding benchmarks demonstrate that DIG consistently outperforms existing baselines and robustly improves LMM performance, even when scaling the input frame count to 256.",
    "translation": "标题：分而治之，精准定位：基于查询类型的长视频理解帧选择适配方法\n\n摘要：大型多模态模型在长视频理解中的应用受到有限上下文长度和处理密集视频令牌所需过高计算成本的制约。因此，近期研究集中于查询感知的帧选择方法，但这些方法通常伴随显著的计算开销。本文挑战了此类复杂搜索机制普遍必要的假设。我们首先识别并验证了一种区分全局查询与局部化查询的查询分类法。研究表明，均匀采样对于全局查询既高效又有效，而局部化查询确实需要查询感知的选择机制以实现最优性能。基于这一发现，我们提出了DIG框架，这是一种无需训练、能够根据查询类型自适应调整策略的帧选择方法。具体而言，DIG对全局查询采用高效的均匀采样策略，同时对局部化查询则激活专用流程以提取查询相关的关键帧。在三个长视频理解基准测试上的实验表明，DIG在性能上持续超越现有基线方法，并能稳健提升大型多模态模型的表现，即使在输入帧数扩展至256帧时依然有效。",
    "url": "https://huggingface.co/papers/2512.04000",
    "arxiv_url": "https://arxiv.org/abs/2512.04000"
  },
  {
    "title": "Light-X: Generative 4D Video Rendering with Camera and Illumination Control",
    "summary": "Recent advances in illumination control extend image-based methods to video, yet still facing a trade-off between lighting fidelity and temporal consistency. Moving beyond relighting, a key step toward generative modeling of real-world scenes is the joint control of camera trajectory and illumination, since visual dynamics are inherently shaped by both geometry and lighting. To this end, we present Light-X, a video generation framework that enables controllable rendering from monocular videos with both viewpoint and illumination control. 1) We propose a disentangled design that decouples geometry and lighting signals: geometry and motion are captured via dynamic point clouds projected along user-defined camera trajectories, while illumination cues are provided by a relit frame consistently projected into the same geometry. These explicit, fine-grained cues enable effective disentanglement and guide high-quality illumination. 2) To address the lack of paired multi-view and multi-illumination videos, we introduce Light-Syn, a degradation-based pipeline with inverse-mapping that synthesizes training pairs from in-the-wild monocular footage. This strategy yields a dataset covering static, dynamic, and AI-generated scenes, ensuring robust training. Extensive experiments show that Light-X outperforms baseline methods in joint camera-illumination control and surpasses prior video relighting methods under both text- and background-conditioned settings.",
    "translation": "标题：Light-X：支持相机与光照联合控制的生成式4D视频渲染框架\n\n摘要：当前光照控制技术虽已将基于图像的渲染方法扩展至视频领域，但仍面临光照保真度与时间一致性的权衡问题。要实现真实场景的生成式建模，仅实现重照明并不足够，关键在于实现对相机轨迹与光照的联合控制，因为视觉动态本质上由几何结构与光照共同塑造。为此，我们提出Light-X——一种支持从单目视频中通过视点与光照控制进行可控渲染的视频生成框架。1）我们提出解耦式设计以分离几何与光照信号：通过沿用户定义相机轨迹投影的动态点云捕捉几何结构与运动信息，同时将经重照明的帧序列持续投影至同一几何空间以提供光照线索。这种显式、细粒度的线索设计实现了有效解耦，并引导高质量光照生成。2）针对缺乏配对多视角与多光照视频数据的问题，我们开发了Light-Syn合成流程，该流程基于退化模型与逆向映射技术，能够从真实场景的单目视频中自动生成训练数据对。该策略构建的数据集涵盖静态场景、动态场景及AI生成场景，确保了训练的鲁棒性。大量实验表明，Light-X在相机-光照联合控制任务上优于基线方法，并在文本条件与背景条件设置下均超越了现有视频重照明方法。",
    "url": "https://huggingface.co/papers/2512.05115",
    "arxiv_url": "https://arxiv.org/abs/2512.05115"
  },
  {
    "title": "Adversarial Confusion Attack: Disrupting Multimodal Large Language Models",
    "summary": "We introduce the Adversarial Confusion Attack, a new class of threats against multimodal large language models (MLLMs). Unlike jailbreaks or targeted misclassification, the goal is to induce systematic disruption that makes the model generate incoherent or confidently incorrect outputs. Practical applications include embedding such adversarial images into websites to prevent MLLM-powered AI Agents from operating reliably. The proposed attack maximizes next-token entropy using a small ensemble of open-source MLLMs. In the white-box setting, we show that a single adversarial image can disrupt all models in the ensemble, both in the full-image and Adversarial CAPTCHA settings. Despite relying on a basic adversarial technique (PGD), the attack generates perturbations that transfer to both unseen open-source (e.g., Qwen3-VL) and proprietary (e.g., GPT-5.1) models.",
    "translation": "标题：对抗性混淆攻击：扰乱多模态大语言模型\n\n摘要：本文提出对抗性混淆攻击，这是一类针对多模态大语言模型的新型威胁。与越狱攻击或定向误分类不同，该攻击旨在引发系统性扰乱，使模型生成不连贯或自信的错误输出。其实际应用场景包括将此类对抗性图像嵌入网站，以阻止基于多模态大语言模型的AI代理可靠运行。所提出的攻击方法通过使用少量开源多模态大语言模型组成的集成系统，最大化下一词元的预测熵。在白盒设定下，我们证明单张对抗性图像即可在完整图像和对抗性验证码两种场景中扰乱集成系统内的所有模型。尽管该方法基于基础对抗技术（投影梯度下降），其生成的扰动能够有效迁移至未见过的开源模型（如Qwen3-VL）和专有模型（如GPT-5.1）。",
    "url": "https://huggingface.co/papers/2511.20494",
    "arxiv_url": "https://arxiv.org/abs/2511.20494"
  }
]