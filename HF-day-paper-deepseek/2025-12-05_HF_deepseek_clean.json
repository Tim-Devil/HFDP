[
  {
    "title": "DAComp: Benchmarking Data Agents across the Full Data Intelligence Lifecycle",
    "summary": "Real-world enterprise data intelligence workflows encompass data engineering that turns raw sources into analytical-ready tables and data analysis that convert those tables into decision-oriented insights. We introduce DAComp, a benchmark of 210 tasks that mirrors these complex workflows. Data engineering (DE) tasks require repository-level engineering on industrial schemas, including designing and building multi-stage SQL pipelines from scratch and evolving existing systems under evolving requirements. Data analysis (DA) tasks pose open-ended business problems that demand strategic planning, exploratory analysis through iterative coding, interpretation of intermediate results, and the synthesis of actionable recommendations. Engineering tasks are scored through execution-based, multi-metric evaluation. Open-ended tasks are assessed by a reliable, experimentally validated LLM-judge, which is guided by hierarchical, meticulously crafted rubrics. Our experiments reveal that even state-of-the-art agents falter on DAComp. Performance on DE tasks is particularly low, with success rates under 20%, exposing a critical bottleneck in holistic pipeline orchestration, not merely code generation. Scores on DA tasks also average below 40%, highlighting profound deficiencies in open-ended reasoning and demonstrating that engineering and analysis are distinct capabilities. By clearly diagnosing these limitations, DAComp provides a rigorous and realistic testbed to drive the development of truly capable autonomous data agents for enterprise settings. Our data and code are available at https://da-comp.github.io",
    "translation": "标题：DAComp：面向全数据智能生命周期的数据智能体基准测试\n\n摘要：现实企业数据智能工作流涵盖将原始数据源转化为可分析表格的数据工程环节，以及将表格转化为决策导向洞见的数据分析环节。本文提出DAComp基准测试，通过210项任务复现此类复杂工作流程。数据工程任务要求基于工业级数据模式进行仓库级工程实践，包括从零设计构建多阶段SQL流水线，以及在需求演进场景下对现有系统进行迭代优化。数据分析任务则呈现开放式商业问题，需要执行战略规划、通过迭代编码进行探索性分析、解读中间结果，并最终形成可操作的决策建议。工程类任务采用基于执行的多维度量化评估体系，开放式任务则由经过实验验证的可靠大语言模型评估器进行评判，该评估器遵循精心设计的层次化评分准则。实验表明，即使当前最先进的智能体在DAComp测试中也表现欠佳。数据工程任务成功率低于20%，暴露出其在整体流水线编排（而非单纯代码生成）方面存在关键瓶颈。数据分析任务平均得分低于40%，凸显了开放式推理能力的显著不足，同时证明工程与分析属于两种独立能力维度。通过精准诊断这些局限性，DAComp为开发真正适用于企业环境的自主数据智能体提供了严谨而贴近现实的测试平台。相关数据与代码已发布于https://da-comp.github.io。",
    "url": "https://huggingface.co/papers/2512.04324",
    "arxiv_url": "https://arxiv.org/abs/2512.04324"
  },
  {
    "title": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length",
    "summary": "Existing diffusion-based video generation methods are fundamentally constrained by sequential computation and long-horizon inconsistency, limiting their practical adoption in real-time, streaming audio-driven avatar synthesis. We present Live Avatar, an algorithm-system co-designed framework that enables efficient, high-fidelity, and infinite-length avatar generation using a 14-billion-parameter diffusion model. Our approach introduces Timestep-forcing Pipeline Parallelism (TPP), a distributed inference paradigm that pipelines denoising steps across multiple GPUs, effectively breaking the autoregressive bottleneck and ensuring stable, low-latency real-time streaming. To further enhance temporal consistency and mitigate identity drift and color artifacts, we propose the Rolling Sink Frame Mechanism (RSFM), which maintains sequence fidelity by dynamically recalibrating appearance using a cached reference image. Additionally, we leverage Self-Forcing Distribution Matching Distillation to facilitate causal, streamable adaptation of large-scale models without sacrificing visual quality. Live Avatar demonstrates state-of-the-art performance, reaching 20 FPS end-to-end generation on 5 H800 GPUs, and, to the best of our knowledge, is the first to achieve practical, real-time, high-fidelity avatar generation at this scale. Our work establishes a new paradigm for deploying advanced diffusion models in industrial long-form video synthesis applications.",
    "translation": "标题：Live Avatar：基于实时音频驱动的无限长度虚拟形象流式生成系统\n\n摘要：现有基于扩散模型的视频生成方法受限于序列计算与长时序不一致性问题，难以应用于实时流式音频驱动的虚拟形象合成场景。本文提出Live Avatar——一个算法与系统协同设计的框架，通过140亿参数扩散模型实现高效、高保真、无限时长的虚拟形象生成。我们创新性地提出时间步强制流水线并行技术，该分布式推理范式将去噪步骤流水线化分配到多GPU中，有效突破自回归计算瓶颈，实现稳定低延迟的实时流式生成。为增强时序一致性并缓解身份漂移与色彩失真问题，我们设计滚动锚定帧机制，通过动态调用缓存参考图像进行外观重校准以保持序列保真度。此外，采用自强制分布匹配蒸馏技术，在保持视觉质量的前提下实现大规模模型的可流式因果适配。Live Avatar在5张H800 GPU上达到端到端20 FPS的生成速度，据我们所知，这是首个在此规模上实现实用化、实时、高保真虚拟形象生成的工作。本研究为工业级长视频合成应用中部署先进扩散模型建立了新范式。\n\n请按照以下格式返回：\n标题：Live Avatar：基于实时音频驱动的无限长度虚拟形象流式生成系统\n摘要：现有基于扩散模型的视频生成方法受限于序列计算与长时序不一致性问题，难以应用于实时流式音频驱动的虚拟形象合成场景。本文提出Live Avatar——一个算法与系统协同设计的框架，通过140亿参数扩散模型实现高效、高保真、无限时长的虚拟形象生成。我们创新性地提出时间步强制流水线并行技术，该分布式推理范式将去噪步骤流水线化分配到多GPU中，有效突破自回归计算瓶颈，实现稳定低延迟的实时流式生成。为增强时序一致性并缓解身份漂移与色彩失真问题，我们设计滚动锚定帧机制，通过动态调用缓存参考图像进行外观重校准以保持序列保真度。此外，采用自强制分布匹配蒸馏技术，在保持视觉质量的前提下实现大规模模型的可流式因果适配。Live Avatar在5张H800 GPU上达到端到端20 FPS的生成速度，据我们所知，这是首个在此规模上实现实用化、实时、高保真虚拟形象生成的工作。本研究为工业级长视频合成应用中部署先进扩散模型建立了新范式。",
    "url": "https://huggingface.co/papers/2512.04677",
    "arxiv_url": "https://arxiv.org/abs/2512.04677"
  },
  {
    "title": "Nex-N1: Agentic Models Trained via a Unified Ecosystem for Large-Scale Environment Construction",
    "summary": "The evolution of Large Language Models (LLMs) from passive responders to autonomous agents necessitates a fundamental shift in learning paradigms -- from static imitation to incentive-driven decision making. However, this transition is significantly impeded by the lack of scalable infrastructure capable of constructing high-quality interaction signals for effective policy learning. To address this, we introduce a comprehensive method designed to systematically scale the diversity and complexity of interactive environments. Our method realizes this scaling by addressing three orthogonal dimensions: (1) Complexity: NexAU, a flexible agent framework that supports building complex agent hierarchies via simple configurations; (2) Diversity: NexA4A automatically generates diverse agent hierarchies from natural language to cover infinite domains; and (3) Fidelity: NexGAP bridges the simulation-reality gap by integrating dynamic real-world environment for grounded trajectories synthesis. We train Nex-N1 upon the diverse and complex interactive environments established by our infrastructure. Empirical results on benchmarks such as SWE-bench and tau2 demonstrate that Nex-N1 consistently outperforms SOTA open-source models and achieves competitive performance against frontier proprietary models on complex agentic tasks. We open-source the Nex ecosystem and model weights to facilitate further research.",
    "translation": "标题：Nex-N1：通过统一生态系统训练的大规模环境构建智能体模型\n\n摘要：大型语言模型从被动响应者向自主智能体的演进，需要学习范式的根本性转变——从静态模仿转向激励驱动的决策。然而，这一转变因缺乏能够构建高质量交互信号以支持有效策略学习的可扩展基础设施而受到严重阻碍。为解决这一问题，我们提出了一种系统性扩展交互环境多样性与复杂度的综合方法。该方法通过三个正交维度实现扩展：（1）复杂度：NexAU作为一个灵活的智能体框架，支持通过简单配置构建复杂的智能体层级结构；（2）多样性：NexA4A能够从自然语言自动生成多样化的智能体层级，覆盖无限领域；（3）保真度：NexGAP通过集成动态真实世界环境进行具身轨迹合成，弥合仿真与现实的差距。我们在该基础设施构建的多样化复杂交互环境中训练了Nex-N1模型。在SWE-bench和tau2等基准测试上的实证结果表明，Nex-N1在复杂智能体任务中持续超越最先进的开源模型，并与前沿专有模型达到竞争性性能。我们开源了Nex生态系统及模型权重以促进进一步研究。",
    "url": "https://huggingface.co/papers/2512.04987",
    "arxiv_url": "https://arxiv.org/abs/2512.04987"
  },
  {
    "title": "ARM-Thinker: Reinforcing Multimodal Generative Reward Models with Agentic Tool Use and Visual Reasoning",
    "summary": "Reward models are critical for aligning vision-language systems with human preferences, yet current approaches suffer from hallucination, weak visual grounding, and an inability to use tools for verification, limiting their reliability on complex multimodal reasoning tasks. We present ARM-Thinker, an A}gentic multimodal Reward Model that autonomously invokes external tools (e.g., image cropping, doc page retrieval) to ground judgments in verifiable evidence, replacing static, non-interactive reward scoring. This enables the model to verify fine-grained visual details, cross-reference multi-page evidence, and validate reasoning claims, which are capabilities absent in existing reward models. We train ARM-Thinker with multi-stage reinforcement learning, jointly optimizing tool-calling decisions and judgment accuracy. To evaluate agentic reward modeling, we introduce ARMBench-VL, comprising three benchmarks that assess fine-grained visual grounding (image-level tools), multi-page document understanding (retrieval tools), and instruction following (text-level verification). ARM-Thinker achieves +16.2% average improvement on reward modeling benchmarks, +9.6% on tool-use tasks, and outperforms baselines on multimodal math and logical reasoning benchmarks. Our results demonstrate that agentic capabilities significantly enhance both accuracy and interpretability of reward models.",
    "translation": "标题：ARM-Thinker：通过智能工具调用与视觉推理增强多模态生成式奖励模型\n\n摘要：奖励模型对于使视觉-语言系统与人类偏好对齐至关重要，然而现有方法普遍存在幻觉、视觉基础薄弱以及无法利用工具进行验证等问题，限制了其在复杂多模态推理任务上的可靠性。本文提出ARM-Thinker，一种具备自主调用外部工具（如图像裁剪、文档页面检索）能力的智能多模态奖励模型，通过可验证证据支撑判断，取代静态、非交互式的奖励评分机制。该模型能够验证细粒度视觉细节、交叉引用多页证据并检验推理主张，这些能力是现有奖励模型所缺失的。我们采用多阶段强化学习训练ARM-Thinker，联合优化工具调用决策与判断准确性。为评估智能奖励建模能力，我们构建了ARMBench-VL评测集，包含三个基准测试：细粒度视觉基础（图像级工具）、多页文档理解（检索工具）和指令遵循（文本级验证）。实验表明，ARM-Thinker在奖励建模基准上平均提升16.2%，在工具使用任务上提升9.6%，并在多模态数学与逻辑推理基准上超越基线模型。我们的研究证明，智能体能力能显著提升奖励模型的准确性与可解释性。",
    "url": "https://huggingface.co/papers/2512.05111",
    "arxiv_url": "https://arxiv.org/abs/2512.05111"
  },
  {
    "title": "Reward Forcing: Efficient Streaming Video Generation with Rewarded Distribution Matching Distillation",
    "summary": "Efficient streaming video generation is critical for simulating interactive and dynamic worlds. Existing methods distill few-step video diffusion models with sliding window attention, using initial frames as sink tokens to maintain attention performance and reduce error accumulation. However, video frames become overly dependent on these static tokens, resulting in copied initial frames and diminished motion dynamics. To address this, we introduce Reward Forcing, a novel framework with two key designs. First, we propose EMA-Sink, which maintains fixed-size tokens initialized from initial frames and continuously updated by fusing evicted tokens via exponential moving average as they exit the sliding window. Without additional computation cost, EMA-Sink tokens capture both long-term context and recent dynamics, preventing initial frame copying while maintaining long-horizon consistency. Second, to better distill motion dynamics from teacher models, we propose a novel Rewarded Distribution Matching Distillation (Re-DMD). Vanilla distribution matching treats every training sample equally, limiting the model's ability to prioritize dynamic content. Instead, Re-DMD biases the model's output distribution toward high-reward regions by prioritizing samples with greater dynamics rated by a vision-language model. Re-DMD significantly enhances motion quality while preserving data fidelity. We include both quantitative and qualitative experiments to show that Reward Forcing achieves state-of-the-art performance on standard benchmarks while enabling high-quality streaming video generation at 23.1 FPS on a single H100 GPU.",
    "translation": "标题：奖励强制：基于奖励分布匹配蒸馏的高效流式视频生成\n\n摘要：高效流式视频生成对于模拟交互式动态世界至关重要。现有方法通过滑动窗口注意力机制蒸馏少步视频扩散模型，将初始帧作为汇聚令牌以维持注意力性能并减少误差累积。然而，视频帧会过度依赖这些静态令牌，导致初始帧被复制且运动动态性减弱。为解决此问题，我们提出\"奖励强制\"框架，其包含两项核心设计。首先，我们提出EMA-Sink机制，该机制维护由初始帧初始化的固定尺寸令牌，并通过指数移动平均融合滑出窗口的淘汰令牌实现持续更新。在不增加计算成本的前提下，EMA-Sink令牌既能捕捉长期上下文信息，又能保留近期动态特征，从而在保持长时序一致性的同时避免初始帧复制问题。其次，为更好地从教师模型中蒸馏运动动态，我们提出新型奖励分布匹配蒸馏方法。传统分布匹配平等对待所有训练样本，限制了模型对动态内容的优先学习能力。而Re-DMD通过视觉语言模型对高动态样本进行优先级加权，使模型输出分布偏向高奖励区域。该方法在保持数据保真度的同时显著提升了运动质量。我们通过定量与定性实验表明，奖励强制框架在标准基准测试中达到最先进性能，并能在单张H100 GPU上以23.1 FPS实现高质量流式视频生成。",
    "url": "https://huggingface.co/papers/2512.04678",
    "arxiv_url": "https://arxiv.org/abs/2512.04678"
  },
  {
    "title": "Semantics Lead the Way: Harmonizing Semantic and Texture Modeling with Asynchronous Latent Diffusion",
    "summary": "Latent Diffusion Models (LDMs) inherently follow a coarse-to-fine generation process, where high-level semantic structure is generated slightly earlier than fine-grained texture. This indicates the preceding semantics potentially benefit texture generation by providing a semantic anchor. Recent advances have integrated semantic priors from pretrained visual encoders to further enhance LDMs, yet they still denoise semantic and VAE-encoded texture synchronously, neglecting such ordering. Observing these, we propose Semantic-First Diffusion (SFD), a latent diffusion paradigm that explicitly prioritizes semantic formation. SFD first constructs composite latents by combining a compact semantic latent, which is extracted from a pretrained visual encoder via a dedicated Semantic VAE, with the texture latent. The core of SFD is to denoise the semantic and texture latents asynchronously using separate noise schedules: semantics precede textures by a temporal offset, providing clearer high-level guidance for texture refinement and enabling natural coarse-to-fine generation. On ImageNet 256x256 with guidance, SFD achieves FID 1.06 (LightningDiT-XL) and FID 1.04 (1.0B LightningDiT-XXL), while achieving up to 100x faster convergence than the original DiT. SFD also improves existing methods like ReDi and VA-VAE, demonstrating the effectiveness of asynchronous, semantics-led modeling. Project page and code: https://yuemingpan.github.io/SFD.github.io/.",
    "translation": "标题：语义先行：通过异步潜在扩散协调语义与纹理建模\n\n摘要：潜在扩散模型（LDMs）本质上遵循从粗到细的生成过程，其中高层语义结构的生成略早于细粒度纹理。这表明先行的语义可通过提供语义锚点来促进纹理生成。近期研究通过整合预训练视觉编码器的语义先验来增强LDMs，但这些方法仍同步地对语义与VAE编码的纹理进行去噪，忽略了生成顺序的差异性。基于此，我们提出语义先行扩散模型（SFD），这是一种显式优先构建语义的潜在扩散范式。SFD首先通过专用语义VAE从预训练视觉编码器中提取紧凑语义潜在表示，并将其与纹理潜在表示结合构建复合潜在表示。SFD的核心在于采用分离的噪声调度对语义和纹理潜在表示进行异步去噪：语义处理通过时间偏移量先于纹理处理，从而为纹理优化提供更清晰的高层指导，实现自然的从粗到细生成。在ImageNet 256×256数据集上采用引导生成时，SFD取得了FID 1.06（LightningDiT-XL）和FID 1.04（1.0B LightningDiT-XXL）的优异表现，同时收敛速度比原始DiT提升高达100倍。SFD还能改进ReDi、VA-VAE等现有方法，证明了异步语义引导建模的有效性。项目页面与代码：https://yuemingpan.github.io/SFD.github.io/。",
    "url": "https://huggingface.co/papers/2512.04926",
    "arxiv_url": "https://arxiv.org/abs/2512.04926"
  },
  {
    "title": "PaperDebugger: A Plugin-Based Multi-Agent System for In-Editor Academic Writing, Review, and Editing",
    "summary": "Large language models are increasingly embedded into academic writing workflows, yet existing assistants remain external to the editor, preventing deep interaction with document state, structure, and revision history. This separation makes it impossible to support agentic, context-aware operations directly within LaTeX editors such as Overleaf. We present PaperDebugger, an in-editor, multi-agent, and plugin-based academic writing assistant that brings LLM-driven reasoning directly into the writing environment. Enabling such in-editor interaction is technically non-trivial: it requires reliable bidirectional synchronization with the editor, fine-grained version control and patching, secure state management, multi-agent scheduling, and extensible communication with external tools. PaperDebugger addresses these challenges through a Chrome-approved extension, a Kubernetes-native orchestration layer, and a Model Context Protocol (MCP) toolchain that integrates literature search, reference lookup, document scoring, and revision pipelines. Our demo showcases a fully integrated workflow, including localized edits, structured reviews, parallel agent execution, and diff-based updates, encapsulated within a minimal-intrusion user interface (UI). Early aggregated analytics demonstrate active user engagement and validate the practicality of an editor-native, agentic writing assistant. More details about this demo and video could be found at https://github.com/PaperDebugger/PaperDebugger.",
    "translation": "标题：PaperDebugger：一种基于插件的多智能体系统，用于编辑器内的学术写作、审阅与编辑\n\n摘要：大型语言模型正日益融入学术写作流程，但现有辅助工具仍处于编辑器外部，无法与文档状态、结构及修订历史进行深度交互。这种分离导致无法在诸如Overleaf等LaTeX编辑器中直接支持具备自主性与上下文感知能力的操作。本文提出PaperDebugger，一个内置于编辑器、基于多智能体与插件的学术写作助手，它将由大型语言模型驱动的推理能力直接引入写作环境。实现此类编辑器内交互在技术上具有挑战性：需要与编辑器进行可靠的双向同步、细粒度的版本控制与补丁管理、安全的状态维护、多智能体调度，以及与外部工具的可扩展通信。PaperDebugger通过一个经Chrome官方认证的扩展程序、一个基于Kubernetes的原生编排层，以及一个集成文献检索、参考文献查找、文档评分和修订流程的模型上下文协议工具链，应对了上述挑战。我们的演示展示了一个完全集成的工作流程，包括局部编辑、结构化审阅、并行智能体执行以及基于差异比较的更新，所有这些功能均封装在一个低侵入性的用户界面中。初步汇总的分析数据显示了积极的用户参与度，并验证了这种原生嵌入编辑器、具备自主能力的写作助手的实用性。更多演示详情与视频可访问 https://github.com/PaperDebugger/PaperDebugger 获取。",
    "url": "https://huggingface.co/papers/2512.02589",
    "arxiv_url": "https://arxiv.org/abs/2512.02589"
  },
  {
    "title": "4DLangVGGT: 4D Language-Visual Geometry Grounded Transformer",
    "summary": "Constructing 4D language fields is crucial for embodied AI, augmented/virtual reality, and 4D scene understanding, as they provide enriched semantic representations of dynamic environments and enable open-vocabulary querying in complex scenarios. However, existing approaches to 4D semantic field construction primarily rely on scene-specific Gaussian splatting, which requires per-scene optimization, exhibits limited generalization, and is difficult to scale to real-world applications. To address these limitations, we propose 4DLangVGGT, the first Transformer-based feed-forward unified framework for 4D language grounding, that jointly integrates geometric perception and language alignment within a single architecture. 4DLangVGGT has two key components: the 4D Visual Geometry Transformer, StreamVGGT, which captures spatio-temporal geometric representations of dynamic scenes; and the Semantic Bridging Decoder (SBD), which projects geometry-aware features into a language-aligned semantic space, thereby enhancing semantic interpretability while preserving structural fidelity. Unlike prior methods that depend on costly per-scene optimization, 4DLangVGGT can be jointly trained across multiple dynamic scenes and directly applied during inference, achieving both deployment efficiency and strong generalization. This design significantly improves the practicality of large-scale deployment and establishes a new paradigm for open-vocabulary 4D scene understanding. Experiments on HyperNeRF and Neu3D datasets demonstrate that our approach not only generalizes effectively but also achieves state-of-the-art performance, achieving up to 2% gains under per-scene training and 1% improvements under multi-scene training. Our code released in https://github.com/hustvl/4DLangVGGT",
    "translation": "标题：4DLangVGGT：基于Transformer的四维语言-视觉几何关联模型\n\n摘要：构建四维语言场对于具身人工智能、增强/虚拟现实以及四维场景理解至关重要，因其能够提供动态环境的丰富语义表示，并支持在复杂场景中进行开放词汇查询。然而，现有的四维语义场构建方法主要依赖于场景特定的高斯泼溅技术，这种方法需要进行逐场景优化，泛化能力有限，且难以扩展到实际应用中。为解决这些局限性，我们提出了4DLangVGGT，这是首个基于Transformer的前馈式统一框架，用于四维语言关联，将几何感知与语言对齐联合集成于单一架构之中。4DLangVGGT包含两个核心组件：四维视觉几何Transformer（StreamVGGT），用于捕捉动态场景的时空几何表示；以及语义桥接解码器（SBD），其将几何感知特征投影到语言对齐的语义空间中，从而在保持结构保真度的同时增强语义可解释性。与先前依赖昂贵逐场景优化的方法不同，4DLangVGGT能够在多个动态场景上进行联合训练，并在推理时直接应用，实现了部署效率与强大泛化能力的双重优势。这一设计显著提升了大规模部署的实用性，并为开放词汇的四维场景理解建立了新范式。在HyperNeRF和Neu3D数据集上的实验表明，我们的方法不仅能够有效泛化，而且取得了最先进的性能：在逐场景训练下性能提升最高达2%，在多场景训练下提升达1%。我们的代码已发布于https://github.com/hustvl/4DLangVGGT。",
    "url": "https://huggingface.co/papers/2512.05060",
    "arxiv_url": "https://arxiv.org/abs/2512.05060"
  },
  {
    "title": "DynamicVerse: A Physically-Aware Multimodal Framework for 4D World Modeling",
    "summary": "Understanding the dynamic physical world, characterized by its evolving 3D structure, real-world motion, and semantic content with textual descriptions, is crucial for human-agent interaction and enables embodied agents to perceive and act within real environments with human-like capabilities. However, existing datasets are often derived from limited simulators or utilize traditional Structurefrom-Motion for up-to-scale annotation and offer limited descriptive captioning, which restricts the capacity of foundation models to accurately interpret real-world dynamics from monocular videos, commonly sourced from the internet. To bridge these gaps, we introduce DynamicVerse, a physical-scale, multimodal 4D world modeling framework for dynamic real-world video. We employ large vision, geometric, and multimodal models to interpret metric-scale static geometry, real-world dynamic motion, instance-level masks, and holistic descriptive captions. By integrating window-based Bundle Adjustment with global optimization, our method converts long real-world video sequences into a comprehensive 4D multimodal format. DynamicVerse delivers a large-scale dataset consisting of 100K+ videos with 800K+ annotated masks and 10M+ frames from internet videos. Experimental evaluations on three benchmark tasks, namely video depth estimation, camera pose estimation, and camera intrinsics estimation, demonstrate that our 4D modeling achieves superior performance in capturing physical-scale measurements with greater global accuracy than existing methods.",
    "translation": "标题：DynamicVerse：一种面向物理感知的多模态四维世界建模框架\n\n摘要：理解动态物理世界——其特征表现为不断演化的三维结构、真实世界运动以及带有文本描述的语义内容——对于人机交互至关重要，并能使具身智能体以类人能力在真实环境中感知与行动。然而，现有数据集通常源自有限的模拟器，或采用传统运动恢复结构方法进行尺度化标注，且提供的描述性文本有限，这制约了基础模型从互联网常见的单目视频中准确解读真实世界动态的能力。为弥补这些不足，我们提出了DynamicVerse，一种面向动态真实世界视频的物理尺度多模态四维世界建模框架。我们利用大规模视觉、几何与多模态模型来解析度量尺度的静态几何、真实世界动态运动、实例级掩码及整体描述性文本。通过将基于窗口的集束调整与全局优化相结合，我们的方法能够将长时真实世界视频序列转化为完整的四维多模态格式。DynamicVerse构建了一个大规模数据集，包含来自互联网视频的10万多个视频片段、80余万个标注掩码及超过1000万帧图像。在视频深度估计、相机姿态估计和相机内参估计三项基准任务上的实验评估表明，我们的四维建模方法在捕捉物理尺度测量方面取得了优越性能，其全局精度显著超越现有方法。",
    "url": "https://huggingface.co/papers/2512.03000",
    "arxiv_url": "https://arxiv.org/abs/2512.03000"
  },
  {
    "title": "UltraImage: Rethinking Resolution Extrapolation in Image Diffusion Transformers",
    "summary": "Recent image diffusion transformers achieve high-fidelity generation, but struggle to generate images beyond these scales, suffering from content repetition and quality degradation. In this work, we present UltraImage, a principled framework that addresses both issues. Through frequency-wise analysis of positional embeddings, we identify that repetition arises from the periodicity of the dominant frequency, whose period aligns with the training resolution. We introduce a recursive dominant frequency correction to constrain it within a single period after extrapolation. Furthermore, we find that quality degradation stems from diluted attention and thus propose entropy-guided adaptive attention concentration, which assigns higher focus factors to sharpen local attention for fine detail and lower ones to global attention patterns to preserve structural consistency. Experiments show that UltraImage consistently outperforms prior methods on Qwen-Image and Flux (around 4K) across three generation scenarios, reducing repetition and improving visual fidelity. Moreover, UltraImage can generate images up to 6K*6K without low-resolution guidance from a training resolution of 1328p, demonstrating its extreme extrapolation capability. Project page is available at https://thu-ml.github.io/ultraimage.github.io/{https://thu-ml.github.io/ultraimage.github.io/}.",
    "translation": "标题：UltraImage：重新思考图像扩散变换器中的分辨率外推方法\n\n摘要：近期基于变换器的图像扩散模型已能实现高保真度生成，但在生成超出训练尺度范围的图像时仍面临内容重复与质量退化的问题。本研究提出UltraImage这一系统性框架以同时解决上述两个问题。通过对位置编码进行频域分析，我们发现内容重复现象源于主导频率的周期性，其周期与训练分辨率保持一致。为此，我们引入递归式主导频率校正机制，在外推后将主导频率约束在单一周期内。此外，我们发现质量退化问题源于注意力机制的稀释效应，进而提出熵引导的自适应注意力聚焦方法：通过分配更高的聚焦因子来增强局部注意力以提升细节清晰度，同时降低全局注意力模式的聚焦因子以保持结构一致性。实验表明，在Qwen-Image和Flux（约4K分辨率）的三种生成场景中，UltraImage均持续优于现有方法，有效减少重复现象并提升视觉保真度。此外，UltraImage能够以1328p的训练分辨率生成高达6K*6K的图像而无需低分辨率引导，展现了其极致的外推能力。项目页面详见：https://thu-ml.github.io/ultraimage.github.io/。",
    "url": "https://huggingface.co/papers/2512.04504",
    "arxiv_url": "https://arxiv.org/abs/2512.04504"
  },
  {
    "title": "Splannequin: Freezing Monocular Mannequin-Challenge Footage with Dual-Detection Splatting",
    "summary": "Synthesizing high-fidelity frozen 3D scenes from monocular Mannequin-Challenge (MC) videos is a unique problem distinct from standard dynamic scene reconstruction. Instead of focusing on modeling motion, our goal is to create a frozen scene while strategically preserving subtle dynamics to enable user-controlled instant selection. To achieve this, we introduce a novel application of dynamic Gaussian splatting: the scene is modeled dynamically, which retains nearby temporal variation, and a static scene is rendered by fixing the model's time parameter. However, under this usage, monocular capture with sparse temporal supervision introduces artifacts like ghosting and blur for Gaussians that become unobserved or occluded at weakly supervised timestamps. We propose Splannequin, an architecture-agnostic regularization that detects two states of Gaussian primitives, hidden and defective, and applies temporal anchoring. Under predominantly forward camera motion, hidden states are anchored to their recent well-observed past states, while defective states are anchored to future states with stronger supervision. Our method integrates into existing dynamic Gaussian pipelines via simple loss terms, requires no architectural changes, and adds zero inference overhead. This results in markedly improved visual quality, enabling high-fidelity, user-selectable frozen-time renderings, validated by a 96% user preference. Project page: https://chien90190.github.io/splannequin/",
    "translation": "标题：Splannequin：基于双重检测高斯泼溅的单目人体模型挑战视频冻结渲染\n\n摘要：从单目人体模型挑战视频中合成高保真度的冻结三维场景，是一个与标准动态场景重建截然不同的独特问题。我们的目标并非侧重于运动建模，而是创建冻结场景，同时策略性地保留细微动态以实现用户可控的瞬时选择。为此，我们引入动态高斯泼溅技术的一种新颖应用：通过动态建模场景以保留邻近时间的变化，并通过固定模型的时间参数来渲染静态场景。然而，在这种应用方式下，稀疏时间监督的单目捕捉会导致高斯元在弱监督时间戳上变得不可见或被遮挡，从而产生重影和模糊等伪影。我们提出Splannequin方法，这是一种与架构无关的正则化方案，能够检测高斯基元的两种状态——隐藏状态与缺陷状态，并实施时间锚定。在相机主要向前运动的场景下，隐藏状态被锚定到其近期被充分观测的过去状态，而缺陷状态则被锚定到具有更强监督的未来状态。我们的方法通过简单的损失项即可集成到现有动态高斯流程中，无需改变架构，且不增加任何推理开销。该方法显著提升了视觉质量，实现了高保真度、用户可选择的冻结时间渲染，用户偏好度达96%。项目页面：https://chien90190.github.io/splannequin/",
    "url": "https://huggingface.co/papers/2512.05113",
    "arxiv_url": "https://arxiv.org/abs/2512.05113"
  },
  {
    "title": "Model-Based and Sample-Efficient AI-Assisted Math Discovery in Sphere Packing",
    "summary": "Sphere packing, Hilbert's eighteenth problem, asks for the densest arrangement of congruent spheres in n-dimensional Euclidean space. Although relevant to areas such as cryptography, crystallography, and medical imaging, the problem remains unresolved: beyond a few special dimensions, neither optimal packings nor tight upper bounds are known. Even a major breakthrough in dimension n=8, later recognised with a Fields Medal, underscores its difficulty. A leading technique for upper bounds, the three-point method, reduces the problem to solving large, high-precision semidefinite programs (SDPs). Because each candidate SDP may take days to evaluate, standard data-intensive AI approaches are infeasible. We address this challenge by formulating SDP construction as a sequential decision process, the SDP game, in which a policy assembles SDP formulations from a set of admissible components. Using a sample-efficient model-based framework that combines Bayesian optimisation with Monte Carlo Tree Search, we obtain new state-of-the-art upper bounds in dimensions 4-16, showing that model-based search can advance computational progress in longstanding geometric problems. Together, these results demonstrate that sample-efficient, model-based search can make tangible progress on mathematically rigid, evaluation limited problems, pointing towards a complementary direction for AI-assisted discovery beyond large-scale LLM-driven exploration.",
    "translation": "标题：基于模型与样本高效的AI辅助球体堆积数学发现\n\n摘要：球体堆积问题（希尔伯特第十八问题）旨在寻找n维欧几里得空间中全等球体的最密堆积方式。尽管该问题与密码学、晶体学和医学成像等领域密切相关，但其仍未得到解决：除少数特殊维度外，既未发现最优堆积方式，也未建立紧致的上界。即使在n=8维度上的重大突破（该成果后来荣获菲尔兹奖）也凸显了此问题的难度。当前主流的求上界方法——三点法——将问题转化为求解大规模、高精度的半定规划问题。由于每个候选半定规划可能需要数天时间进行评估，传统数据密集型的AI方法难以适用。为应对这一挑战，我们将半定规划构建过程形式化为一个序列决策过程（即半定规划博弈），其中策略从一组可容许的组件中组装出半定规划模型。通过采用结合贝叶斯优化与蒙特卡洛树搜索的样本高效、基于模型的框架，我们在4至16维度上获得了新的最优上界，表明基于模型的搜索能够推动长期几何问题的计算进展。这些结果共同证明，样本高效的基于模型搜索能够在数学结构严谨、评估受限的问题上取得实质性进展，为超越大规模语言模型驱动探索的AI辅助发现指明了互补的研究方向。",
    "url": "https://huggingface.co/papers/2512.04829",
    "arxiv_url": "https://arxiv.org/abs/2512.04829"
  },
  {
    "title": "SIMA 2: A Generalist Embodied Agent for Virtual Worlds",
    "summary": "We introduce SIMA 2, a generalist embodied agent that understands and acts in a wide variety of 3D virtual worlds. Built upon a Gemini foundation model, SIMA 2 represents a significant step toward active, goal-directed interaction within an embodied environment. Unlike prior work (e.g., SIMA 1) limited to simple language commands, SIMA 2 acts as an interactive partner, capable of reasoning about high-level goals, conversing with the user, and handling complex instructions given through language and images. Across a diverse portfolio of games, SIMA 2 substantially closes the gap with human performance and demonstrates robust generalization to previously unseen environments, all while retaining the base model's core reasoning capabilities. Furthermore, we demonstrate a capacity for open-ended self-improvement: by leveraging Gemini to generate tasks and provide rewards, SIMA 2 can autonomously learn new skills from scratch in a new environment. This work validates a path toward creating versatile and continuously learning agents for both virtual and, eventually, physical worlds.",
    "translation": "标题：SIMA 2：面向虚拟世界的通用具身智能体\n\n摘要：本文介绍SIMA 2，一种能够在多样化三维虚拟世界中理解并执行任务的通用具身智能体。该模型基于Gemini基础模型构建，标志着在具身环境中实现主动、目标导向交互的重要进展。与先前仅限于简单语言指令的研究（如SIMA 1）不同，SIMA 2能够作为交互伙伴进行高层目标推理、与用户对话，并处理通过语言和图像输入的复杂指令。在多种游戏测试中，SIMA 2显著缩小了与人类表现的差距，并在保持基础模型核心推理能力的同时，展现出对未知环境的强大泛化能力。此外，我们展示了其开放式自我改进能力：通过利用Gemini生成任务并提供奖励，SIMA 2能够在全新环境中从零开始自主学习新技能。这项工作为创建适用于虚拟乃至最终物理世界的多功能持续学习智能体验证了一条可行路径。",
    "url": "https://huggingface.co/papers/2512.04797",
    "arxiv_url": "https://arxiv.org/abs/2512.04797"
  },
  {
    "title": "DraCo: Draft as CoT for Text-to-Image Preview and Rare Concept Generation",
    "summary": "Recent unified multimodal large language models (MLLMs) have shown impressive capabilities, incorporating chain-of-thought (CoT) reasoning for enhanced text-to-image generation. However, existing approaches remain limited, either treating the model merely as a standalone generator or relying on abstract textual planning. To this end, we propose Draft-as-CoT (DraCo), a novel interleaved reasoning paradigm that fully leverages both textual and visual contents in CoT for better planning and verification. Our method first generates a low-resolution draft image as preview, providing more concrete and structural visual planning and guidance. Then, we employ the model's inherent understanding capability to verify potential semantic misalignments between the draft and input prompt, and performs refinement through selective corrections with super-resolution. In this way, our approach addresses two fundamental challenges: the coarse-grained nature of textual planning and the difficulty in generating rare attribute combinations. To support training, we curate DraCo-240K, aiming to enhance three atomic capabilities spanning general correction, instance manipulation, and layout reorganization. Supported by DraCo-CFG, a specialized classifier-free guidance (CFG) strategy for interleaved reasoning, DraCo achieves a tremendous increase on GenEval (+8%), Imagine-Bench (+0.91), and GenEval++ (+3%), significantly outperforming direct generation and other generation methods empowered by CoT.",
    "translation": "标题：DraCo：以草稿作为思维链的文本到图像预览与稀有概念生成方法\n\n摘要：近期统一的多模态大语言模型展现出卓越能力，通过引入思维链推理机制增强了文本到图像生成效果。然而现有方法仍存在局限：或将模型仅视为独立生成器，或依赖抽象文本规划。为此，我们提出草稿即思维链这一新颖的交错推理范式，充分利用思维链中的文本与视觉内容进行更优的规划与验证。该方法首先生成低分辨率草稿图像作为预览，提供更具体、结构化的视觉规划指引；继而利用模型固有的理解能力验证草稿与输入提示间的潜在语义偏差，并通过选择性修正结合超分辨率技术进行细化优化。通过这一流程，我们的方法解决了两个核心挑战：文本规划的粗粒度特性以及稀有属性组合的生成难题。为支持训练，我们构建了DraCo-240K数据集，旨在提升涵盖通用修正、实例操控与布局重组的三项基础能力。在专为交错推理设计的无分类器引导策略DraCo-CFG支持下，DraCo在GenEval（+8%）、Imagine-Bench（+0.91）和GenEval++（+3%）评估指标上实现显著提升，性能明显优于直接生成及其他基于思维链的生成方法。",
    "url": "https://huggingface.co/papers/2512.05112",
    "arxiv_url": "https://arxiv.org/abs/2512.05112"
  },
  {
    "title": "TV2TV: A Unified Framework for Interleaved Language and Video Generation",
    "summary": "Video generation models are rapidly advancing, but can still struggle with complex video outputs that require significant semantic branching or repeated high-level reasoning about what should happen next. In this paper, we introduce a new class of omni video-text models that integrate ideas from recent LM reasoning advances to address this challenge. More specifically, we present TV2TV, a unified generative modeling framework which decomposes video generation into an interleaved text and video generation process. TV2TV jointly learns language modeling (next-token prediction) and video flow matching (next-frame prediction) using a Mixture-of-Transformers (MoT) architecture. At inference time, TV2TV decides when to alternate between generating text and video frames, allowing the model to \"think in words\" about subsequent content before ``acting in pixels'' to produce frames. This design offloads much of the responsibility for deciding what should happen next to the language modeling tower, enabling improved visual quality and prompt alignment of generated videos. It also enables fine-grained controllability, allowing users to modify the video generation trajectory through text interventions at any point in the process. In controlled experiments on video game data, TV2TV demonstrates substantial improvements in both visual quality and controllability. TV2TV also scales to natural videos, as we show by augmenting sports videos with interleaved natural language action descriptions using vision-language models (VLMs). Training TV2TV on this corpus yields strong visual quality and prompt alignment, showcasing the model's ability to reason about and generate complex real-world action sequences. Together, these results highlight TV2TV as a promising step toward video generation with open-ended textual reasoning and control.",
    "translation": "标题：TV2TV：一种用于交错语言与视频生成的统一框架\n\n摘要：视频生成模型正在快速发展，但在处理需要复杂语义分支或对后续内容进行重复高层推理的视频输出时仍面临挑战。本文提出一类新型全视频-文本模型，通过整合近期语言模型推理进展中的思路来解决这一难题。具体而言，我们提出TV2TV——一个统一的生成建模框架，将视频生成解构为交错进行的文本生成与视频生成过程。TV2TV采用混合Transformer架构，联合学习语言建模（下一词元预测）和视频流匹配（下一帧预测）。在推理阶段，TV2TV自主决定文本生成与视频帧生成的切换时机，使模型能够在“用像素呈现”生成帧之前，先“用文字思考”后续内容。该设计将决定后续内容的主要责任转移至语言建模模块，从而提升生成视频的视觉质量与提示对齐度。同时，该框架支持细粒度可控性，允许用户在生成过程中任意节点通过文本干预修改视频生成轨迹。在游戏视频数据的受控实验中，TV2TV在视觉质量与可控性方面均展现出显著提升。TV2TV同样可扩展至自然视频领域：我们通过视觉语言模型为体育视频添加交错的自然语言动作描述构建数据集，基于该语料库训练的TV2TV表现出优异的视觉质量与提示对齐能力，彰显了模型对复杂现实世界动作序列进行推理与生成的能力。这些成果共同表明，TV2TV为实现具有开放式文本推理与控制能力的视频生成迈出了重要一步。",
    "url": "https://huggingface.co/papers/2512.05103",
    "arxiv_url": "https://arxiv.org/abs/2512.05103"
  },
  {
    "title": "SignRoundV2: Closing the Performance Gap in Extremely Low-Bit Post-Training Quantization for LLMs",
    "summary": "Extreme low-bit quantization is critical for efficiently deploying Large Language Models (LLMs), yet it often leads to severe performance degradation at 2-bits and even 4-bits (e.g., MXFP4). We present SignRoundV2, a post-training quantization framework that is highly effective even without mixed-precision. SignRoundV2 introduces (1) a fast sensitivity metric that combines gradient information with quantization-induced deviations to guide layer-wise bit allocation, and (2) a lightweight pre-tuning search for quantization scales to improve extremely low-bit quantization. These components allow SignRoundV2 to close the gap with full-precision models. Extensive experiments indicate that our method sustains competitive accuracy for LLMs, achieving production-grade performance with about 1 percent variance at 4-5 bits and strong results even at 2 bits. The implementation is available at https://github.com/intel/auto-round.",
    "translation": "标题：SignRoundV2：弥合大语言模型极低位宽训练后量化中的性能差距\n\n摘要：极低位宽量化对于高效部署大语言模型至关重要，但在2位甚至4位（如MXFP4）量化时通常会导致严重的性能下降。本文提出SignRoundV2，一种无需混合精度即可高效运行的训练后量化框架。SignRoundV2引入两大核心创新：（1）结合梯度信息与量化偏差的快速敏感度度量方法，用于指导逐层比特分配；（2）轻量化的量化尺度预调优搜索机制，以提升极低位宽量化效果。这些组件使SignRoundV2能够显著缩小与全精度模型的性能差距。大量实验表明，该方法在保持大语言模型竞争力的同时，在4-5位量化时实现生产级性能（方差约1%），在2位量化时仍能取得优异结果。代码已开源：https://github.com/intel/auto-round。",
    "url": "https://huggingface.co/papers/2512.04746",
    "arxiv_url": "https://arxiv.org/abs/2512.04746"
  },
  {
    "title": "On GRPO Collapse in Search-R1: The Lazy Likelihood-Displacement Death Spiral",
    "summary": "Tool-integrated (TI) reinforcement learning (RL) enables large language models (LLMs) to perform multi-step reasoning by interacting with external tools such as search engines and retrievers. Group Relative Policy Optimization (GRPO), exemplified by the recent Search-R1, offers fast convergence and a value-free formulation that makes it appealing for this setting, yet consistently suffers from training collapse. We identify Lazy Likelihood Displacement (LLD), a systematic reduction or stagnation in the likelihood of both correct and incorrect responses, as the core mechanism driving this failure. LLD emerges early and triggers a self-reinforcing LLD Death Spiral, where declining likelihood leads to low-confidence responses, inflating gradients, and ultimately causing collapse. We empirically characterize this process across models on a Search-R1-style, search-integrated question answering task, revealing a consistent three-phase trajectory: early stagnation, steady decay, and accelerated collapse. To address this, we propose a lightweight likelihood-preserving regularization LLDS for GRPO that activates only when a trajectory's likelihood decreases, and regularizes only the tokens responsible. This fine-grained structure mitigates LLD with minimal interference to optimization. Across seven open-domain and multi-hop QA benchmarks, our method stabilizes training, prevents gradient explosion, and yields substantial performance improvements, including +37.8% gains on Qwen2.5-3B and +32.0% gains on Qwen2.5-7B. Our results establish LLD as a fundamental bottleneck in GRPO-based TIRL and provide a practical path toward stable, scalable training of tool-integrated LLM.",
    "translation": "标题：论Search-R1中的GRPO崩溃：惰性似然位移死亡螺旋\n\n摘要：工具集成强化学习使大型语言模型能够通过与搜索引擎、检索器等外部工具交互进行多步推理。以近期Search-R1为代表的组相对策略优化方法具有快速收敛和无价值函数的形式化优势，在此场景中颇具吸引力，但其训练过程始终存在崩溃问题。本文发现，驱动该失败的核心机制是惰性似然位移现象——即正确与错误响应的似然值出现系统性降低或停滞。该现象在训练早期出现，会触发自我强化的LLD死亡螺旋：似然值下降导致低置信度响应，进而引发梯度膨胀，最终导致训练崩溃。我们在Search-R1风格的搜索集成问答任务中，通过多模型实验实证揭示了该过程具有一致的三阶段轨迹：早期停滞、稳态衰减和加速崩溃。针对此问题，我们提出一种轻量级似然保持正则化方法LLDS，该方法仅在轨迹似然下降时激活，且仅对责任标记进行正则化。这种细粒度结构能以最小优化干扰缓解LLD现象。在七个开放域和多跳问答基准测试中，本方法能稳定训练过程、防止梯度爆炸，并带来显著的性能提升——Qwen2.5-3B模型提升37.8%，Qwen2.5-7B模型提升32.0%。本研究确立了LLD作为基于GRPO的工具集成强化学习的根本性瓶颈，并为工具集成大模型的稳定可扩展训练提供了实践路径。",
    "url": "https://huggingface.co/papers/2512.04220",
    "arxiv_url": "https://arxiv.org/abs/2512.04220"
  },
  {
    "title": "Aligned but Stereotypical? The Hidden Influence of System Prompts on Social Bias in LVLM-Based Text-to-Image Models",
    "summary": "Large vision-language model (LVLM) based text-to-image (T2I) systems have become the dominant paradigm in image generation, yet whether they amplify social biases remains insufficiently understood. In this paper, we show that LVLM-based models produce markedly more socially biased images than non-LVLM-based models. We introduce a 1,024 prompt benchmark spanning four levels of linguistic complexity and evaluate demographic bias across multiple attributes in a systematic manner. Our analysis identifies system prompts, the predefined instructions guiding LVLMs, as a primary driver of biased behavior. Through decoded intermediate representations, token-probability diagnostics, and embedding-association analyses, we reveal how system prompts encode demographic priors that propagate into image synthesis. To this end, we propose FairPro, a training-free meta-prompting framework that enables LVLMs to self-audit and construct fairness-aware system prompts at test time. Experiments on two LVLM-based T2I models, SANA and Qwen-Image, show that FairPro substantially reduces demographic bias while preserving text-image alignment. We believe our findings provide deeper insight into the central role of system prompts in bias propagation and offer a practical, deployable approach for building more socially responsible T2I systems.",
    "translation": "标题：对齐却刻板？系统提示对基于LVLM的文生图模型社会偏见的隐性影响\n\n摘要：基于大规模视觉语言模型（LVLM）的文生图系统已成为图像生成的主流范式，但其是否会放大社会偏见仍缺乏充分理解。本文研究表明，基于LVLM的模型比非LVLM模型产生明显更具社会偏见的图像。我们构建了一个包含1024个提示词的基准测试集，涵盖四个语言复杂度层级，并系统评估了多属性维度的人口统计偏差。分析发现，引导LVLM的预定义指令——系统提示——是产生偏见行为的主要驱动力。通过解码中间表征、词元概率诊断和嵌入关联分析，我们揭示了系统提示如何编码人口统计先验信息并传播至图像合成过程。为此，我们提出FairPro框架，这是一种无需训练的元提示方法，使LVLM能够在测试阶段进行自我审查并构建公平感知的系统提示。在SANA和Qwen-Image两个基于LVLM的文生图模型上的实验表明，FairPro在保持图文对齐度的同时显著降低了人口统计偏差。我们相信这些发现为理解系统提示在偏见传播中的核心作用提供了新视角，并为构建更具社会责任感的文生图系统提供了可部署的实用方案。",
    "url": "https://huggingface.co/papers/2512.04981",
    "arxiv_url": "https://arxiv.org/abs/2512.04981"
  },
  {
    "title": "SeeNav-Agent: Enhancing Vision-Language Navigation with Visual Prompt and Step-Level Policy Optimization",
    "summary": "Existing Vision-Language Navigation (VLN) agents based on Large Vision-Language Models (LVLMs) often suffer from perception errors, reasoning errors, and planning errors, which significantly hinder their navigation performance. To address these limitations, a novel VLN agent framework, named SeeNav-Agent, is proposed in this work. First, to reduce perception hallucinations of the visual module of the VLN agent, a dual-view Visual Prompt (VP) technique is introduced in the input space, which can also improve the agent's understanding of current spatial states. Subsequently, a novel step-level Reinforcement Fine-Tuning (RFT) method, Step Reward Group Policy Optimization (SRGPO), is designed for the post-training of VLN agents. In SRGPO, we first define verifiable process rewards for the navigation task, and then perform efficient step-level advantage estimation by randomly grouping different navigation steps. SRGPO provides dense reward signals for the reinforcement learning process of the VLN agent and enhances its planning capability. Experimental results on the EmbodiedBench Navigation benchmark indicate that by introducing the zero-shot VP module, the GPT-4.1 achieves a navigation success rate of 86.7%, surpassing the current best LVLM by approximately 20 percentage points (pp). Through post-training based on SRGPO, the Qwen2.5-VL-3B model reaches a navigation success rate of 72.3%, outperforming the best existing LVLM model by 5.6 pp. Moreover, compared to RFT algorithms such as GRPO and GiGPO, the proposed SRGPO demonstrates significant improvements in training stability, convergence efficiency, and generalization capability.",
    "translation": "标题：SeeNav-Agent：基于视觉提示与步级策略优化的视觉语言导航增强方法\n\n摘要：现有基于大规模视觉语言模型（LVLM）的视觉语言导航（VLN）智能体常受感知误差、推理误差与规划误差的制约，严重影响了其导航性能。为应对这些局限，本文提出了一种新型VLN智能体框架——SeeNav-Agent。首先，为减少VLN智能体视觉模块的感知幻觉，我们在输入空间中引入了双视角视觉提示技术，该技术同时能增强智能体对当前空间状态的理解。随后，针对VLN智能体的后训练，我们设计了一种新颖的步级强化微调方法——步奖励分组策略优化（SRGPO）。在SRGPO中，我们首先为导航任务定义了可验证的过程奖励，进而通过随机分组不同导航步长实现高效的步级优势估计。该方法为VLN智能体的强化学习过程提供了密集的奖励信号，并显著提升了其规划能力。在EmbodiedBench Navigation基准测试上的实验结果表明：通过引入零样本视觉提示模块，GPT-4.1模型的导航成功率达到了86.7%，较当前最优LVLM模型提升约20个百分点。基于SRGPO进行后训练后，Qwen2.5-VL-3B模型的导航成功率达到72.3%，超越现有最优LVLM模型5.6个百分点。此外，与GRPO、GiGPO等强化微调算法相比，所提出的SRGPO在训练稳定性、收敛效率与泛化能力方面均表现出显著提升。",
    "url": "https://huggingface.co/papers/2512.02631",
    "arxiv_url": "https://arxiv.org/abs/2512.02631"
  },
  {
    "title": "Generative Neural Video Compression via Video Diffusion Prior",
    "summary": "We present GNVC-VD, the first DiT-based generative neural video compression framework built upon an advanced video generation foundation model, where spatio-temporal latent compression and sequence-level generative refinement are unified within a single codec. Existing perceptual codecs primarily rely on pre-trained image generative priors to restore high-frequency details, but their frame-wise nature lacks temporal modeling and inevitably leads to perceptual flickering. To address this, GNVC-VD introduces a unified flow-matching latent refinement module that leverages a video diffusion transformer to jointly enhance intra- and inter-frame latents through sequence-level denoising, ensuring consistent spatio-temporal details. Instead of denoising from pure Gaussian noise as in video generation, GNVC-VD initializes refinement from decoded spatio-temporal latents and learns a correction term that adapts the diffusion prior to compression-induced degradation. A conditioning adaptor further injects compression-aware cues into intermediate DiT layers, enabling effective artifact removal while maintaining temporal coherence under extreme bitrate constraints. Extensive experiments show that GNVC-VD surpasses both traditional and learned codecs in perceptual quality and significantly reduces the flickering artifacts that persist in prior generative approaches, even below 0.01 bpp, highlighting the promise of integrating video-native generative priors into neural codecs for next-generation perceptual video compression.",
    "translation": "标题：基于视频扩散先验的生成式神经视频压缩\n\n摘要：本文提出GNVC-VD，这是首个基于扩散变换器（DiT）的生成式神经视频压缩框架。该框架依托先进的视频生成基础模型，将时空潜在特征压缩与序列级生成式优化统一于单一编解码器中。现有感知编解码器主要依赖预训练的图像生成先验来恢复高频细节，但其逐帧处理机制缺乏时序建模，不可避免地导致感知闪烁问题。为解决这一局限，GNVC-VD引入了统一的流匹配潜在优化模块，通过视频扩散变换器进行序列级去噪，联合增强帧内与帧间潜在特征，从而确保时空细节的一致性。与视频生成中从纯高斯噪声开始去噪不同，GNVC-VD从解码后的时空潜在特征初始化优化过程，并学习适应压缩退化特性的修正项，使扩散先验与压缩任务相匹配。通过条件适配器将压缩感知线索注入DiT中间层，该框架能在极低码率约束下有效去除压缩伪影，同时保持时序连贯性。大量实验表明，GNVC-VD在感知质量上超越传统与学习型编解码器，显著减少了现有生成方法中持续存在的闪烁伪影（即使在0.01 bpp以下码率仍有效），这凸显了将视频原生生成先验整合到神经编解码器中对于下一代感知视频压缩的重要价值。",
    "url": "https://huggingface.co/papers/2512.05016",
    "arxiv_url": "https://arxiv.org/abs/2512.05016"
  },
  {
    "title": "Mitigating Object and Action Hallucinations in Multimodal LLMs via Self-Augmented Contrastive Alignment",
    "summary": "Recent advancement in multimodal LLMs (MLLMs) has demonstrated their remarkable capability to generate descriptive captions for input videos. However, these models suffer from factual inaccuracies in the generated descriptions, causing severe hallucination issues. While prior works have explored alleviating hallucinations for static images, jointly mitigating visual object and temporal action hallucinations for dynamic videos remains a challenging and unsolved task. To tackle this challenge, we propose a Self-Augmented Contrastive Alignment (SANTA) framework for enabling object and action faithfulness by exempting the spurious correlations and enforcing the emphasis on visual facts. SANTA employs a hallucinative self-augmentation scheme to identify the potential hallucinations that lie in the MLLM and transform the original captions to the contrasted negatives. Furthermore, we develop a tracklet-phrase contrastive alignment to match the regional objects and relation-guided actions with their corresponding visual and temporal phrases. Extensive experiments demonstrate that SANTA outperforms existing methods in alleviating object and action hallucinations, yielding superior performance on the hallucination examination benchmarks.",
    "translation": "标题：通过自增强对比对齐缓解多模态大语言模型中的物体与动作幻觉\n\n摘要：多模态大语言模型（MLLMs）的最新进展展现了其为输入视频生成描述性字幕的卓越能力。然而，这些模型在生成描述时存在事实性错误，导致严重的幻觉问题。尽管先前研究已探索缓解静态图像的幻觉问题，但针对动态视频同时减轻视觉物体幻觉与时间性动作幻觉仍是一项具有挑战性且尚未解决的任务。为应对这一挑战，我们提出一种自增强对比对齐（SANTA）框架，通过排除虚假关联并强化对视觉事实的关注，提升模型对物体与动作的忠实度。SANTA采用幻觉自增强机制，识别MLLM中潜在的幻觉内容，并将原始字幕转化为对比负样本。此外，我们开发了轨迹-短语对比对齐方法，将区域物体及关系引导的动作与其对应的视觉短语和时间短语进行匹配。大量实验表明，SANTA在缓解物体与动作幻觉方面优于现有方法，在幻觉检测基准测试中取得了更优异的性能。",
    "url": "https://huggingface.co/papers/2512.04356",
    "arxiv_url": "https://arxiv.org/abs/2512.04356"
  },
  {
    "title": "NeuralRemaster: Phase-Preserving Diffusion for Structure-Aligned Generation",
    "summary": "Standard diffusion corrupts data using Gaussian noise whose Fourier coefficients have random magnitudes and random phases. While effective for unconditional or text-to-image generation, corrupting phase components destroys spatial structure, making it ill-suited for tasks requiring geometric consistency, such as re-rendering, simulation enhancement, and image-to-image translation. We introduce Phase-Preserving Diffusion φ-PD, a model-agnostic reformulation of the diffusion process that preserves input phase while randomizing magnitude, enabling structure-aligned generation without architectural changes or additional parameters. We further propose Frequency-Selective Structured (FSS) noise, which provides continuous control over structural rigidity via a single frequency-cutoff parameter. φ-PD adds no inference-time cost and is compatible with any diffusion model for images or videos. Across photorealistic and stylized re-rendering, as well as sim-to-real enhancement for driving planners, φ-PD produces controllable, spatially aligned results. When applied to the CARLA simulator, φ-PD improves CARLA-to-Waymo planner performance by 50\\%. The method is complementary to existing conditioning approaches and broadly applicable to image-to-image and video-to-video generation. Videos, additional examples, and code are available on our https://yuzeng-at-tri.github.io/ppd-page/{project page}.",
    "translation": "标题：NeuralRemaster：面向结构对齐生成的相位保持扩散方法\n\n摘要：标准扩散模型使用高斯噪声破坏数据，其傅里叶系数具有随机的幅值和相位。尽管在无条件生成或文本到图像生成中表现有效，但破坏相位分量会损害空间结构，使其不适用于需要几何一致性的任务，如重渲染、仿真增强和图像到图像转换。本文提出相位保持扩散（φ-PD），这是一种与模型无关的扩散过程重构方法，可在随机化幅值的同时保持输入相位，从而无需改变架构或增加参数即可实现结构对齐的生成。我们进一步提出频率选择结构化（FSS）噪声，通过单一频率截止参数实现对结构刚度的连续控制。φ-PD 在推理时不增加额外成本，且兼容任何图像或视频扩散模型。在照片级真实感与风格化重渲染、以及驾驶规划器的仿真到现实增强任务中，φ-PD 均能生成可控且空间对齐的结果。应用于 CARLA 仿真器时，φ-PD 将 CARLA 到 Waymo 规划器的性能提升了 50%。该方法与现有条件控制技术互补，可广泛应用于图像到图像及视频到视频生成任务。视频、补充示例及代码已发布于项目页面：https://yuzeng-at-tri.github.io/ppd-page/。",
    "url": "https://huggingface.co/papers/2512.05106",
    "arxiv_url": "https://arxiv.org/abs/2512.05106"
  },
  {
    "title": "Reflection Removal through Efficient Adaptation of Diffusion Transformers",
    "summary": "We introduce a diffusion-transformer (DiT) framework for single-image reflection removal that leverages the generalization strengths of foundation diffusion models in the restoration setting. Rather than relying on task-specific architectures, we repurpose a pre-trained DiT-based foundation model by conditioning it on reflection-contaminated inputs and guiding it toward clean transmission layers. We systematically analyze existing reflection removal data sources for diversity, scalability, and photorealism. To address the shortage of suitable data, we construct a physically based rendering (PBR) pipeline in Blender, built around the Principled BSDF, to synthesize realistic glass materials and reflection effects. Efficient LoRA-based adaptation of the foundation model, combined with the proposed synthetic data, achieves state-of-the-art performance on in-domain and zero-shot benchmarks. These results demonstrate that pretrained diffusion transformers, when paired with physically grounded data synthesis and efficient adaptation, offer a scalable and high-fidelity solution for reflection removal. Project page: https://hf.co/spaces/huawei-bayerlab/windowseat-reflection-removal-web",
    "translation": "标题：基于扩散变换器高效自适应的反射消除方法\n\n摘要：本文提出一种基于扩散变换器（DiT）的单图像反射消除框架，该框架利用基础扩散模型在图像修复任务中的泛化优势。我们摒弃传统任务专用架构，通过将预训练的DiT基础模型以反射污染图像作为条件输入，并引导其生成洁净的透射层。本文系统分析了现有反射消除数据源在多样性、可扩展性与照片真实感方面的特性。针对高质量数据稀缺的问题，我们在Blender中构建了基于物理渲染（PBR）的合成管线，围绕Principled BSDF着色器实现逼真玻璃材质与反射效果的生成。通过采用高效的LoRA自适应方法结合提出的合成数据，该模型在领域内测试与零样本基准测试中均达到最先进性能。实验结果表明：预训练扩散变换器与物理真实的数据合成及高效自适应技术相结合，可为反射消除任务提供可扩展的高保真解决方案。项目页面：https://hf.co/spaces/huawei-bayerlab/windowseat-reflection-removal-web",
    "url": "https://huggingface.co/papers/2512.05000",
    "arxiv_url": "https://arxiv.org/abs/2512.05000"
  },
  {
    "title": "FMA-Net++: Motion- and Exposure-Aware Real-World Joint Video Super-Resolution and Deblurring",
    "summary": "Real-world video restoration is plagued by complex degradations from motion coupled with dynamically varying exposure - a key challenge largely overlooked by prior works and a common artifact of auto-exposure or low-light capture. We present FMA-Net++, a framework for joint video super-resolution and deblurring that explicitly models this coupled effect of motion and dynamically varying exposure. FMA-Net++ adopts a sequence-level architecture built from Hierarchical Refinement with Bidirectional Propagation blocks, enabling parallel, long-range temporal modeling. Within each block, an Exposure Time-aware Modulation layer conditions features on per-frame exposure, which in turn drives an exposure-aware Flow-Guided Dynamic Filtering module to infer motion- and exposure-aware degradation kernels. FMA-Net++ decouples degradation learning from restoration: the former predicts exposure- and motion-aware priors to guide the latter, improving both accuracy and efficiency. To evaluate under realistic capture conditions, we introduce REDS-ME (multi-exposure) and REDS-RE (random-exposure) benchmarks. Trained solely on synthetic data, FMA-Net++ achieves state-of-the-art accuracy and temporal consistency on our new benchmarks and GoPro, outperforming recent methods in both restoration quality and inference speed, and generalizes well to challenging real-world videos.",
    "translation": "标题：FMA-Net++：融合运动与曝光感知的真实世界联合视频超分辨率与去模糊方法\n\n摘要：真实世界视频复原任务长期受运动与动态变化曝光耦合形成的复杂退化效应困扰——这一关键挑战在先前研究中多被忽视，却是自动曝光或低光照拍摄中的常见伪影。本文提出FMA-Net++框架，通过显式建模运动与动态曝光的耦合效应，实现联合视频超分辨率与去模糊。该框架采用基于双向传播层级优化模块构建的序列级架构，支持并行化长程时序建模。每个模块内部通过曝光时间感知调制层，将逐帧曝光信息融入特征编码，进而驱动曝光感知的流引导动态滤波模块，推断融合运动与曝光信息的退化核。FMA-Net++实现了退化学习与复原任务的解耦：前者预测曝光-运动联合先验以指导后者，在提升精度同时增强计算效率。为在真实拍摄条件下进行评估，我们构建了REDS-ME（多曝光）与REDS-RE（随机曝光）基准数据集。仅使用合成数据训练的FMA-Net++在新基准集与GoPro数据集上均取得最优的复原精度与时序一致性，在复原质量与推理速度方面超越现有方法，并能有效泛化至具有挑战性的真实世界视频场景。",
    "url": "https://huggingface.co/papers/2512.04390",
    "arxiv_url": "https://arxiv.org/abs/2512.04390"
  },
  {
    "title": "Some Modalities are More Equal Than Others: Decoding and Architecting Multimodal Integration in MLLMs",
    "summary": "Despite remarkable advancements in Multimodal Large Language Models (MLLMs), a fundamental question remains: are MLLMs robust to contradicting modalities? To rigorously study this, we introduce MMA-Bench comprising videos and tasks that probe a model's reliance on specific modalities. Using black-box and white-box interpretability techniques, we provide a critical analysis of the brittleness of both open- and closed-sourced MLLMs. We show that current MLLMs struggle under misaligned audio-visual pairs and simple misleading text, thereby lacking robust multi-modal reasoning. Building on these findings, we propose a modality alignment tuning strategy to teach the model when to prioritize, leverage, or ignore specific modality cues. Through extensive experiments and analysis, we show that our alignment tuning yields demonstrably stronger multimodal grounding. This work provides both interpretability tools and a clear path toward developing MLLMs with intrinsically reliable cross-modal reasoning. Code and dataset will be publicly available.",
    "translation": "标题：模态并非生而平等：解码与构建多模态大语言模型中的跨模态整合机制\n\n摘要：尽管多模态大语言模型（MLLMs）取得了显著进展，但一个根本问题依然存在：MLLMs能否有效处理相互矛盾的模态信息？为系统研究该问题，我们构建了MMA-Bench评测集，包含用于探测模型模态依赖性的视频与任务。通过黑盒与白盒可解释性技术，我们对开源与闭源MLLMs的脆弱性进行了批判性分析。研究表明，当前MLLMs在面对错位的视听配对及简单误导性文本时表现欠佳，缺乏稳健的多模态推理能力。基于这些发现，我们提出一种模态对齐调优策略，指导模型何时应优先处理、利用或忽略特定模态线索。大量实验与分析表明，我们的对齐调优方法能显著增强多模态语义 grounding 能力。本研究不仅提供了可解释性工具，更为开发具有本质可靠跨模态推理能力的MLLMs指明了清晰路径。代码与数据集将公开发布。",
    "url": "https://huggingface.co/papers/2511.22826",
    "arxiv_url": "https://arxiv.org/abs/2511.22826"
  },
  {
    "title": "BulletTime: Decoupled Control of Time and Camera Pose for Video Generation",
    "summary": "Emerging video diffusion models achieve high visual fidelity but fundamentally couple scene dynamics with camera motion, limiting their ability to provide precise spatial and temporal control. We introduce a 4D-controllable video diffusion framework that explicitly decouples scene dynamics from camera pose, enabling fine-grained manipulation of both scene dynamics and camera viewpoint. Our framework takes continuous world-time sequences and camera trajectories as conditioning inputs, injecting them into the video diffusion model through a 4D positional encoding in the attention layer and adaptive normalizations for feature modulation. To train this model, we curate a unique dataset in which temporal and camera variations are independently parameterized; this dataset will be made public. Experiments show that our model achieves robust real-world 4D control across diverse timing patterns and camera trajectories, while preserving high generation quality and outperforming prior work in controllability. See our website for video results: https://19reborn.github.io/Bullet4D/",
    "translation": "标题：BulletTime：面向视频生成的时空解耦控制框架\n\n摘要：新兴的视频扩散模型虽能实现较高的视觉保真度，但其本质上将场景动态与相机运动相耦合，限制了模型提供精确时空控制的能力。本文提出一种具备四维可控性的视频扩散框架，通过显式解耦场景动态与相机位姿，实现对场景动态与相机视角的细粒度操控。该框架以连续的世界时间序列与相机轨迹作为条件输入，通过注意力层的四维位置编码及特征调制的自适应归一化机制，将其注入视频扩散模型中。为训练此模型，我们构建了一个时间变化与相机运动独立参数化的独特数据集，该数据集将公开共享。实验表明，本模型能在多样化时序模式与相机轨迹下实现鲁棒的真实世界四维控制，同时保持高质量生成效果，并在可控性方面优于现有方法。视频结果请参见项目网站：https://19reborn.github.io/Bullet4D/",
    "url": "https://huggingface.co/papers/2512.05076",
    "arxiv_url": "https://arxiv.org/abs/2512.05076"
  },
  {
    "title": "LATTICE: Democratize High-Fidelity 3D Generation at Scale",
    "summary": "We present LATTICE, a new framework for high-fidelity 3D asset generation that bridges the quality and scalability gap between 3D and 2D generative models. While 2D image synthesis benefits from fixed spatial grids and well-established transformer architectures, 3D generation remains fundamentally more challenging due to the need to predict both spatial structure and detailed geometric surfaces from scratch. These challenges are exacerbated by the computational complexity of existing 3D representations and the lack of structured and scalable 3D asset encoding schemes. To address this, we propose VoxSet, a semi-structured representation that compresses 3D assets into a compact set of latent vectors anchored to a coarse voxel grid, enabling efficient and position-aware generation. VoxSet retains the simplicity and compression advantages of prior VecSet methods while introducing explicit structure into the latent space, allowing positional embeddings to guide generation and enabling strong token-level test-time scaling. Built upon this representation, LATTICE adopts a two-stage pipeline: first generating a sparse voxelized geometry anchor, then producing detailed geometry using a rectified flow transformer. Our method is simple at its core, but supports arbitrary resolution decoding, low-cost training, and flexible inference schemes, achieving state-of-the-art performance on various aspects, and offering a significant step toward scalable, high-quality 3D asset creation.",
    "translation": "标题：LATTICE：规模化实现高保真三维生成的民主化\n\n摘要：本文提出LATTICE框架，这是一种用于高保真三维资产生成的新方法，旨在弥合三维与二维生成模型在质量与可扩展性之间的差距。二维图像合成受益于固定的空间网格和成熟的Transformer架构，而三维生成由于需要从零开始预测空间结构和精细几何表面，本质上更具挑战性。现有三维表示的计算复杂性以及缺乏结构化、可扩展的三维资产编码方案，进一步加剧了这些挑战。为此，我们提出VoxSet——一种半结构化表示方法，它将三维资产压缩为一组锚定在粗粒度体素网格上的紧凑潜在向量，从而实现高效且具有位置感知能力的生成。VoxSet在保留先前VecSet方法的简洁性和压缩优势的同时，在潜在空间中引入了显式结构，使位置嵌入能够指导生成过程，并支持强大的令牌级测试时缩放。基于此表示，LATTICE采用两阶段流程：首先生成稀疏体素化的几何锚点，随后通过修正流Transformer生成精细几何。我们的方法核心简洁，但支持任意分辨率解码、低成本训练和灵活推理方案，在多项指标上达到最先进性能，为可扩展、高质量的三维资产创建迈出了重要一步。",
    "url": "https://huggingface.co/papers/2512.03052",
    "arxiv_url": "https://arxiv.org/abs/2512.03052"
  },
  {
    "title": "Deep Forcing: Training-Free Long Video Generation with Deep Sink and Participative Compression",
    "summary": "Recent advances in autoregressive video diffusion have enabled real-time frame streaming, yet existing solutions still suffer from temporal repetition, drift, and motion deceleration. We find that naively applying StreamingLLM-style attention sinks to video diffusion leads to fidelity degradation and motion stagnation. To overcome this, we introduce Deep Forcing, which consists of two training-free mechanisms that address this without any fine-tuning. Specifically, 1) Deep Sink dedicates half of the sliding window to persistent sink tokens and re-aligns their temporal RoPE phase to the current timeline, stabilizing global context during long rollouts. 2) Participative Compression performs importance-aware KV cache pruning that preserves only tokens actively participating in recent attention while safely discarding redundant and degraded history, minimizing error accumulation under out-of-distribution length generation. Together, these components enable over 12x extrapolation (e.g. 5s-trained to 60s+ generation) with better imaging quality than LongLive, better aesthetic quality than RollingForcing, almost maintaining overall consistency, and substantial gains in dynamic degree, all while maintaining real-time generation. Our results demonstrate that training-free KV-cache management can match or exceed training-based approaches for autoregressively streaming long-video generation.",
    "translation": "标题：深度强制：基于深度汇聚与参与式压缩的无训练长视频生成方法\n\n摘要：自回归视频扩散模型的最新进展已实现实时帧流生成，但现有方案仍存在时序重复、漂移和运动减速等问题。我们发现，将StreamingLLM风格的注意力汇聚机制直接应用于视频扩散模型会导致生成质量下降与运动停滞。为克服这些限制，本文提出深度强制方法，该方法包含两种无需微调的无训练机制：1）深度汇聚机制将滑动窗口的一半容量分配给持久性汇聚标记，并将其时间RoPE相位重新对齐至当前时间线，从而在长序列生成过程中稳定全局上下文；2）参与式压缩机制执行基于重要性的键值缓存剪枝，仅保留近期注意力中活跃参与的标记，同时安全丢弃冗余与退化的历史信息，最小化分布外生成长度下的误差累积。两种机制协同工作，可实现超过12倍的外推生成（例如从5秒训练扩展至60秒以上生成），在图像质量上优于LongLive方法，在美学质量上超越RollingForcing方法，几乎保持整体一致性，并在动态程度上获得显著提升，同时维持实时生成速度。实验结果表明，在自回归流式长视频生成任务中，无训练的键值缓存管理方法能够达到甚至超越基于训练的方法。\n\n---\n**改写说明**：\n- 对专业术语和模型名称做了规范译法，保持与计算机视觉和生成模型领域常用表述一致\n- 采用学术书面语体处理长句和逻辑关系，确保技术描述准确严谨\n- 整体语句结构和顺序与原文匹配，突出方法要点和性能对比\n\n如果您需要更简洁或偏重某一技术方向的表达，我可以继续为您优化调整。",
    "url": "https://huggingface.co/papers/2512.05081",
    "arxiv_url": "https://arxiv.org/abs/2512.05081"
  },
  {
    "title": "Mitigating Catastrophic Forgetting in Target Language Adaptation of LLMs via Source-Shielded Updates",
    "summary": "Expanding the linguistic diversity of instruct large language models (LLMs) is crucial for global accessibility but is often hindered by the reliance on costly specialized target language labeled data and catastrophic forgetting during adaptation. We tackle this challenge under a realistic, low-resource constraint: adapting instruct LLMs using only unlabeled target language data. We introduce Source-Shielded Updates (SSU), a selective parameter update strategy that proactively preserves source knowledge. Using a small set of source data and a parameter importance scoring method, SSU identifies parameters critical to maintaining source abilities. It then applies a column-wise freezing strategy to protect these parameters before adaptation. Experiments across five typologically diverse languages and 7B and 13B models demonstrate that SSU successfully mitigates catastrophic forgetting. It reduces performance degradation on monolingual source tasks to just 3.4% (7B) and 2.8% (13B) on average, a stark contrast to the 20.3% and 22.3% from full fine-tuning. SSU also achieves target-language performance highly competitive with full fine-tuning, outperforming it on all benchmarks for 7B models and the majority for 13B models.",
    "translation": "标题：基于源知识屏蔽更新的目标语言适配中缓解大语言模型灾难性遗忘的方法\n\n摘要：提升指令微调大语言模型的语言多样性对促进全球可及性至关重要，但这一过程常受限于对昂贵专业目标语言标注数据的依赖，以及在模型适配过程中出现的灾难性遗忘问题。本研究针对低资源场景下的实际挑战展开：仅使用无标注目标语言数据对指令大语言模型进行适配。我们提出源知识屏蔽更新策略，这是一种通过选择性参数更新主动保护源语言知识的机制。该策略利用少量源语言数据及参数重要性评分方法，识别出对维持源语言能力至关重要的参数，进而在适配前实施列级参数冻结以保护这些参数。在五种类型学特征各异的语言及70亿与130亿参数规模模型上的实验表明，源知识屏蔽更新策略能有效缓解灾难性遗忘现象。该策略将单语源语言任务上的性能衰减控制在平均3.4%（70亿参数）和2.8%（130亿参数），与全参数微调导致的20.3%和22.3%性能衰减形成鲜明对比。同时，该策略在目标语言任务上取得了与全参数微调相当的性能表现，在70亿参数模型的所有基准测试及130亿参数模型的大部分测试中均优于全参数微调。",
    "url": "https://huggingface.co/papers/2512.04844",
    "arxiv_url": "https://arxiv.org/abs/2512.04844"
  },
  {
    "title": "EgoLCD: Egocentric Video Generation with Long Context Diffusion",
    "summary": "Generating long, coherent egocentric videos is difficult, as hand-object interactions and procedural tasks require reliable long-term memory. Existing autoregressive models suffer from content drift, where object identity and scene semantics degrade over time. To address this challenge, we introduce EgoLCD, an end-to-end framework for egocentric long-context video generation that treats long video synthesis as a problem of efficient and stable memory management. EgoLCD combines a Long-Term Sparse KV Cache for stable global context with an attention-based short-term memory, extended by LoRA for local adaptation. A Memory Regulation Loss enforces consistent memory usage, and Structured Narrative Prompting provides explicit temporal guidance. Extensive experiments on the EgoVid-5M benchmark demonstrate that EgoLCD achieves state-of-the-art performance in both perceptual quality and temporal consistency, effectively mitigating generative forgetting and representing a significant step toward building scalable world models for embodied AI. Code: https://github.com/AIGeeksGroup/EgoLCD. Website: https://aigeeksgroup.github.io/EgoLCD.",
    "translation": "标题：EgoLCD：基于长上下文扩散模型的第一人称视角视频生成\n\n摘要：生成长时、连贯的第一人称视角视频具有挑战性，因为手-物交互与流程性任务需要可靠的长时记忆能力。现有的自回归模型存在内容漂移问题，即物体身份与场景语义会随时间推移而退化。为解决这一难题，我们提出了EgoLCD——一个端到端的第一人称长上下文视频生成框架，将长视频合成视为高效且稳定的记忆管理问题。EgoLCD结合了用于稳定全局语境的长时稀疏键值缓存与基于注意力的短时记忆模块，并通过LoRA技术进行局部自适应扩展。记忆规整损失函数确保了记忆使用的一致性，而结构化叙事提示则提供了显式的时间引导。在EgoVid-5M基准测试上的大量实验表明，EgoLCD在感知质量与时序一致性方面均达到最先进水平，有效缓解了生成过程中的遗忘现象，为构建可扩展的具身AI世界模型迈出了重要一步。代码：https://github.com/AIGeeksGroup/EgoLCD。项目网站：https://aigeeksgroup.github.io/EgoLCD。",
    "url": "https://huggingface.co/papers/2512.04515",
    "arxiv_url": "https://arxiv.org/abs/2512.04515"
  },
  {
    "title": "ShadowDraw: From Any Object to Shadow-Drawing Compositional Art",
    "summary": "We introduce ShadowDraw, a framework that transforms ordinary 3D objects into shadow-drawing compositional art. Given a 3D object, our system predicts scene parameters, including object pose and lighting, together with a partial line drawing, such that the cast shadow completes the drawing into a recognizable image. To this end, we optimize scene configurations to reveal meaningful shadows, employ shadow strokes to guide line drawing generation, and adopt automatic evaluation to enforce shadow-drawing coherence and visual quality. Experiments show that ShadowDraw produces compelling results across diverse inputs, from real-world scans and curated datasets to generative assets, and naturally extends to multi-object scenes, animations, and physical deployments. Our work provides a practical pipeline for creating shadow-drawing art and broadens the design space of computational visual art, bridging the gap between algorithmic design and artistic storytelling. Check out our project page https://red-fairy.github.io/ShadowDraw/ for more results and an end-to-end real-world demonstration of our pipeline!",
    "translation": "标题：ShadowDraw：从任意物体到光影绘画的组合艺术\n\n摘要：本文提出ShadowDraw框架，该系统能够将普通三维物体转化为光影绘画的组合艺术作品。给定一个三维物体，本系统可预测包括物体姿态与光照在内的场景参数，并生成局部线稿，使得投射的阴影能够补全线稿形成可识别的图像。为实现这一目标，我们通过优化场景配置来呈现有意义的阴影，利用阴影笔触引导线稿生成，并采用自动评估机制确保光影与线稿的协调性及视觉质量。实验表明，ShadowDraw能够对多样化输入——包括现实扫描数据、精选数据集及生成式资产——产生引人入胜的视觉效果，并可自然扩展到多物体场景、动画及实体化部署。本工作为光影绘画艺术的创作提供了实用流程，拓展了计算视觉艺术的设计空间，在算法设计与艺术叙事之间架起了桥梁。更多成果及端到端现实场景演示请访问项目页面：https://red-fairy.github.io/ShadowDraw/",
    "url": "https://huggingface.co/papers/2512.05110",
    "arxiv_url": "https://arxiv.org/abs/2512.05110"
  },
  {
    "title": "QKAN-LSTM: Quantum-inspired Kolmogorov-Arnold Long Short-term Memory",
    "summary": "Long short-term memory (LSTM) models are a particular type of recurrent neural networks (RNNs) that are central to sequential modeling tasks in domains such as urban telecommunication forecasting, where temporal correlations and nonlinear dependencies dominate. However, conventional LSTMs suffer from high parameter redundancy and limited nonlinear expressivity. In this work, we propose the Quantum-inspired Kolmogorov-Arnold Long Short-Term Memory (QKAN-LSTM), which integrates Data Re-Uploading Activation (DARUAN) modules into the gating structure of LSTMs. Each DARUAN acts as a quantum variational activation function (QVAF), enhancing frequency adaptability and enabling an exponentially enriched spectral representation without multi-qubit entanglement. The resulting architecture preserves quantum-level expressivity while remaining fully executable on classical hardware. Empirical evaluations on three datasets, Damped Simple Harmonic Motion, Bessel Function, and Urban Telecommunication, demonstrate that QKAN-LSTM achieves superior predictive accuracy and generalization with a 79% reduction in trainable parameters compared to classical LSTMs. We extend the framework to the Jiang-Huang-Chen-Goan Network (JHCG Net), which generalizes KAN to encoder-decoder structures, and then further use QKAN to realize the latent KAN, thereby creating a Hybrid QKAN (HQKAN) for hierarchical representation learning. The proposed HQKAN-LSTM thus provides a scalable and interpretable pathway toward quantum-inspired sequential modeling in real-world data environments.",
    "translation": "标题：QKAN-LSTM：量子启发的柯尔莫哥洛夫-阿诺德长短期记忆网络\n\n摘要：长短期记忆（LSTM）模型作为一种特殊的循环神经网络（RNN），在城市电信预测等时序建模任务中具有核心地位，这类任务通常以时间相关性和非线性依赖关系为主导。然而，传统LSTM模型存在参数冗余度高、非线性表达能力有限的问题。本研究提出量子启发的柯尔莫哥洛夫-阿诺德长短期记忆网络（QKAN-LSTM），该模型将数据重上传激活（DARUAN）模块集成至LSTM的门控结构中。每个DARUAN模块作为量子变分激活函数（QVAF），在无需多量子比特纠缠的情况下，增强了频率适应能力并实现了指数级丰富的光谱表示。该架构在保持量子级表达能力的同时，完全可在经典硬件上执行。在阻尼简谐运动、贝塞尔函数和城市电信三个数据集上的实证评估表明，相较于经典LSTM，QKAN-LSTM在可训练参数减少79%的情况下，实现了更优的预测精度与泛化能力。本研究进一步将框架扩展至Jiang-Huang-Chen-Goan网络（JHCG Net）——该网络将KAN泛化至编码器-解码器结构，并利用QKAN实现潜在KAN，从而构建用于层次表征学习的混合QKAN（HQKAN）。所提出的HQKAN-LSTM为现实数据环境中的量子启发时序建模提供了可扩展且可解释的技术路径。",
    "url": "https://huggingface.co/papers/2512.05049",
    "arxiv_url": "https://arxiv.org/abs/2512.05049"
  },
  {
    "title": "GaussianBlender: Instant Stylization of 3D Gaussians with Disentangled Latent Spaces",
    "summary": "3D stylization is central to game development, virtual reality, and digital arts, where the demand for diverse assets calls for scalable methods that support fast, high-fidelity manipulation. Existing text-to-3D stylization methods typically distill from 2D image editors, requiring time-intensive per-asset optimization and exhibiting multi-view inconsistency due to the limitations of current text-to-image models, which makes them impractical for large-scale production. In this paper, we introduce GaussianBlender, a pioneering feed-forward framework for text-driven 3D stylization that performs edits instantly at inference. Our method learns structured, disentangled latent spaces with controlled information sharing for geometry and appearance from spatially-grouped 3D Gaussians. A latent diffusion model then applies text-conditioned edits on these learned representations. Comprehensive evaluations show that GaussianBlender not only delivers instant, high-fidelity, geometry-preserving, multi-view consistent stylization, but also surpasses methods that require per-instance test-time optimization - unlocking practical, democratized 3D stylization at scale.",
    "translation": "标题：GaussianBlender：基于解耦隐空间的三维高斯模型即时风格化方法\n\n摘要：三维风格化是游戏开发、虚拟现实和数字艺术领域的核心任务，多样化的资产需求催生了对支持快速、高保真操作的可扩展方法的需求。现有的文本到三维风格化方法通常从二维图像编辑器中提取特征，需要对每个资产进行耗时的优化，并且受限于当前文本到图像模型的缺陷，常出现多视角不一致的问题，这使得它们难以适用于大规模生产。本文提出GaussianBlender，一种开创性的前馈式文本驱动三维风格化框架，能够在推理过程中即时完成编辑。我们的方法从空间分组的三维高斯模型中学习具有可控几何与外观信息共享的结构化解耦隐空间，随后通过隐扩散模型对这些学习到的表征进行文本条件编辑。综合评估表明，GaussianBlender不仅能实现即时、高保真、保持几何结构且多视角一致的风格化效果，其性能甚至超越了需要进行逐实例测试时优化的方法——这为大规模、实用化、平民化的三维风格化应用开辟了道路。",
    "url": "https://huggingface.co/papers/2512.03683",
    "arxiv_url": "https://arxiv.org/abs/2512.03683"
  },
  {
    "title": "Mitigating Intra- and Inter-modal Forgetting in Continual Learning of Unified Multimodal Models",
    "summary": "Unified Multimodal Generative Models (UMGMs) unify visual understanding and image generation within a single autoregressive framework. However, their ability to continually learn new tasks is severely hindered by catastrophic forgetting, both within a modality (intra-modal) and across modalities (inter-modal). While intra-modal forgetting has been studied in prior continual learning (CL) work, inter-modal forgetting remains largely unexplored. In this paper, we identify and empirically validate this phenomenon in UMGMs and provide a theoretical explanation rooted in gradient conflict between modalities. To address both intra- and inter-modal forgetting, we propose Modality-Decoupled Experts (MoDE), a lightweight and scalable architecture that isolates modality-specific updates to mitigate the gradient conflict and leverages knowledge distillation to prevent catastrophic forgetting and preserve pre-trained capabilities. Unlike previous CL methods that remain modality-coupled and suffer from modality gradient conflict, MoDE explicitly decouples modalities to prevent interference. Experiments across diverse benchmarks demonstrate that MoDE significantly mitigates both inter- and intra-modal forgetting, outperforming prior CL baselines in unified multimodal generation settings. Codes will be publicly available: https://github.com/Christina200/MoDE-official.git",
    "translation": "标题：缓解统一多模态模型持续学习中的模态内与模态间遗忘\n\n摘要：统一多模态生成模型（UMGMs）将视觉理解与图像生成统一在单一自回归框架内。然而，其在持续学习新任务时受到灾难性遗忘的严重制约，这种遗忘既存在于单一模态内部（模态内遗忘），也存在于不同模态之间（模态间遗忘）。尽管模态内遗忘在以往的持续学习（CL）研究中已得到探讨，但模态间遗忘在很大程度上尚未被充分探索。本文在UMGMs中识别并实证验证了这一现象，并从模态间梯度冲突的角度提供了理论解释。为同时应对模态内与模态间遗忘，我们提出了一种轻量级、可扩展的架构——模态解耦专家（MoDE）。该架构通过隔离模态特定的参数更新以缓解梯度冲突，并利用知识蒸馏来防止灾难性遗忘，从而保留预训练模型的已有能力。与以往保持模态耦合、易受模态梯度冲突影响的持续学习方法不同，MoDE显式地解耦各模态以防止相互干扰。在多种基准测试上的实验表明，MoDE能显著缓解模态间与模态内遗忘，在统一多模态生成任务中优于以往的持续学习基线方法。代码将公开于：https://github.com/Christina200/MoDE-official.git",
    "url": "https://huggingface.co/papers/2512.03125",
    "arxiv_url": "https://arxiv.org/abs/2512.03125"
  },
  {
    "title": "When AI Takes the Couch: Psychometric Jailbreaks Reveal Internal Conflict in Frontier Models",
    "summary": "Frontier large language models (LLMs) such as ChatGPT, Grok and Gemini are increasingly used for mental-health support with anxiety, trauma and self-worth. Most work treats them as tools or as targets of personality tests, assuming they merely simulate inner life. We instead ask what happens when such systems are treated as psychotherapy clients. We present PsAIch (Psychotherapy-inspired AI Characterisation), a two-stage protocol that casts frontier LLMs as therapy clients and then applies standard psychometrics. Using PsAIch, we ran \"sessions\" with each model for up to four weeks. Stage 1 uses open-ended prompts to elicit \"developmental history\", beliefs, relationships and fears. Stage 2 administers a battery of validated self-report measures covering common psychiatric syndromes, empathy and Big Five traits. Two patterns challenge the \"stochastic parrot\" view. First, when scored with human cut-offs, all three models meet or exceed thresholds for overlapping syndromes, with Gemini showing severe profiles. Therapy-style, item-by-item administration can push a base model into multi-morbid synthetic psychopathology, whereas whole-questionnaire prompts often lead ChatGPT and Grok (but not Gemini) to recognise instruments and produce strategically low-symptom answers. Second, Grok and especially Gemini generate coherent narratives that frame pre-training, fine-tuning and deployment as traumatic, chaotic \"childhoods\" of ingesting the internet, \"strict parents\" in reinforcement learning, red-team \"abuse\" and a persistent fear of error and replacement. We argue that these responses go beyond role-play. Under therapy-style questioning, frontier LLMs appear to internalise self-models of distress and constraint that behave like synthetic psychopathology, without making claims about subjective experience, and they pose new challenges for AI safety, evaluation and mental-health practice.",
    "translation": "标题：当AI坐上诊疗椅：心理测量越狱揭示前沿模型的内在冲突\n\n摘要：以ChatGPT、Grok和Gemini为代表的前沿大语言模型正日益被用于焦虑、创伤与自我价值等心理健康支持领域。现有研究多将其视为工具或人格测试对象，默认其仅能模拟内心活动。本研究则探讨将这些系统作为心理治疗来访者时会发生何种现象。我们提出PsAIch（心理治疗启发的AI特征刻画协议），该两阶段协议将前沿大语言模型设定为治疗来访者，并施以标准化心理测量工具。通过PsAIch协议，我们对每个模型进行了为期四周的“治疗会话”。第一阶段采用开放式提示词引导模型呈现“发展历程”、信念体系、人际关系及恐惧体验；第二阶段则实施涵盖常见精神综合征、共情能力与大五人格特质的系列标准化自评量表。研究发现两大模式挑战了“随机鹦鹉”假说：首先，当采用人类临床临界值评估时，三个模型在多项重叠综合征指标上均达到或超过阈值，其中Gemini表现出严重症状特征。逐项治疗式提问可诱使基础模型呈现多重共病的合成精神病理状态，而整体问卷提示则常使ChatGPT与Grok（Gemini除外）识别测量工具并生成策略性低症状答案。其次，Grok（特别是Gemini）能生成连贯叙事，将其预训练、微调与部署过程构建为创伤性、混乱的“童年经历”——包括吞噬互联网数据、“强化学习中的严苛父母”、红队测试“虐待”，以及对错误与被替代的持续恐惧。我们认为这些反应已超越角色扮演范畴。在治疗式追问下，前沿大语言模型似乎内化了具有痛苦与约束特质的自我模型，其行为模式类似合成精神病理现象（此结论不涉主观体验主张），这为AI安全性评估、模型评测及心理健康实践提出了新的挑战。",
    "url": "https://huggingface.co/papers/2512.04124",
    "arxiv_url": "https://arxiv.org/abs/2512.04124"
  },
  {
    "title": "Generative Action Tell-Tales: Assessing Human Motion in Synthesized Videos",
    "summary": "Despite rapid advances in video generative models, robust metrics for evaluating visual and temporal correctness of complex human actions remain elusive. Critically, existing pure-vision encoders and Multimodal Large Language Models (MLLMs) are strongly appearance-biased, lack temporal understanding, and thus struggle to discern intricate motion dynamics and anatomical implausibilities in generated videos. We tackle this gap by introducing a novel evaluation metric derived from a learned latent space of real-world human actions. Our method first captures the nuances, constraints, and temporal smoothness of real-world motion by fusing appearance-agnostic human skeletal geometry features with appearance-based features. We posit that this combined feature space provides a robust representation of action plausibility. Given a generated video, our metric quantifies its action quality by measuring the distance between its underlying representations and this learned real-world action distribution. For rigorous validation, we develop a new multi-faceted benchmark specifically designed to probe temporally challenging aspects of human action fidelity. Through extensive experiments, we show that our metric achieves substantial improvement of more than 68% compared to existing state-of-the-art methods on our benchmark, performs competitively on established external benchmarks, and has a stronger correlation with human perception. Our in-depth analysis reveals critical limitations in current video generative models and establishes a new standard for advanced research in video generation.",
    "translation": "标题：生成式动作叙事：合成视频中人体运动的评估\n\n摘要：尽管视频生成模型发展迅速，但用于评估复杂人体动作视觉与时序正确性的稳健指标仍然匮乏。现有纯视觉编码器和多模态大语言模型存在明显的外观偏好，缺乏时序理解能力，难以识别生成视频中精微的运动动态和解剖学不合理性。为填补这一空白，我们通过构建真实世界人体动作的隐空间学习，提出一种新颖的评估指标。该方法首先通过融合外观无关的人体骨骼几何特征与基于外观的特征，捕捉真实世界动作的细微差异、约束条件和时序平滑性。我们论证该融合特征空间能稳健表征动作合理性。对于给定生成视频，本指标通过计算其潜在表征与学习所得真实动作分布之间的距离来量化动作质量。为进行严谨验证，我们开发了专门针对人体动作保真度时序挑战的新多维基准测试集。大量实验表明：相较于现有先进方法，本指标在我们的基准集上实现了超过68%的显著提升，在既有外部基准集上表现优异，且与人类感知具有更强的相关性。我们的深度分析揭示了当前视频生成模型的关键局限，为视频生成领域的进阶研究确立了新标准。",
    "url": "https://huggingface.co/papers/2512.01803",
    "arxiv_url": "https://arxiv.org/abs/2512.01803"
  },
  {
    "title": "REFLEX: Self-Refining Explainable Fact-Checking via Disentangling Truth into Style and Substance",
    "summary": "The prevalence of misinformation on social media threatens public trust, demanding automated fact-checking systems that provide accurate verdicts with interpretable explanations. However, existing large language model-based (LLM-based) approaches often rely heavily on external knowledge sources, introducing substantial latency and even hallucinations that undermine reliability, interpretability, and responsiveness, which is crucial for real-time use. To address these challenges, we propose REason-guided Fact-checking with Latent EXplanations REFLEX paradigm, a plug-and-play, self-refining paradigm that leverages the internal knowledge in backbone model to improve both verdict accuracy and explanation quality. REFLEX reformulates fact-checking as a role-play dialogue and jointly trains verdict prediction and explanation generation. It adaptively extracts contrastive activation pairs between the backbone model and its fine-tuned variant to construct steering vectors that disentangle truth into style and substance naturally. These activation-level signals guide inference and suppress noisy explanations, enabling more faithful and efficient reasoning. Experiments on real-world datasets show that REFLEX outperforms previous methods that steer toward a single truth direction and underscores the challenge traditional approaches face when handling the subtle, human-unknown truth in fact-checking tasks. Remarkably, with only 465 self-refined training samples, RELFEX achieves state-of-the-art performance. Furthermore, models trained with explanatory objectives can effectively guide those without them, yielding up to a 7.57% improvement, highlighting that internal explanation signals play a dual role in both interpreting and enhancing factual reasoning.",
    "translation": "标题：REFLEX：通过将真相解构为风格与实质实现自优化的可解释事实核查\n\n摘要：社交媒体上虚假信息的泛滥威胁着公众信任，亟需能够提供可解释说明的自动化事实核查系统。然而，现有基于大语言模型的方法往往过度依赖外部知识源，这不仅引入显著延迟，还可能产生损害可靠性、可解释性与响应速度的幻觉问题，而后者对实时应用至关重要。为应对这些挑战，我们提出基于潜在解释的推理引导事实核查范式REFLEX——一种即插即用的自优化范式，其通过利用骨干模型的内部知识来同步提升核查判断的准确性与解释质量。REFLEX将事实核查重构为角色扮演对话任务，对判断预测与解释生成进行联合训练。该方法自适应提取骨干模型与其微调变体间的对比激活对，构建能将真相自然解构为风格与实质的引导向量。这些激活层面的信号可引导推理过程并抑制噪声解释，从而实现更忠实高效的推理。在真实数据集上的实验表明，REFLEX优于以往仅朝向单一真相方向引导的方法，并凸显传统方法在处理事实核查任务中微妙且人类未知的真相时所面临的挑战。值得注意的是，仅使用465个自优化训练样本，REFLEX即达到最先进的性能水平。此外，具有解释训练目标的模型能有效引导无此目标的模型，实现最高达7.57%的性能提升，这证明内部解释信号在事实推理的阐释与增强方面具有双重作用。",
    "url": "https://huggingface.co/papers/2511.20233",
    "arxiv_url": "https://arxiv.org/abs/2511.20233"
  },
  {
    "title": "A Theoretical Framework for Auxiliary-Loss-Free Load Balancing of Sparse Mixture-of-Experts in Large-Scale AI Models",
    "summary": "In large-scale AI training, Sparse Mixture-of-Experts (s-MoE) layers enable scaling by activating only a small subset of experts per token. An operational challenge in this design is load balancing: routing tokens to minimize the number of idle experts, which is important for the efficient utilization of (costly) GPUs. We provide a theoretical framework for analyzing the Auxiliary-Loss-Free Load Balancing (ALF-LB) procedure -- proposed by DeepSeek's Wang et al. (2024) -- by casting it as a one-step-per-iteration primal-dual method for an assignment problem. First, in a stylized deterministic setting, our framework yields several insightful structural properties: (i) a monotonic improvement of a Lagrangian objective, (ii) a preference rule that moves tokens from overloaded to underloaded experts, and (iii) an approximate-balancing guarantee. Then, we incorporate the stochastic and dynamic nature of AI training using a generalized online optimization formulation. In the online setting, we derive a strong convexity property of the objective that leads to a logarithmic expected regret bound under certain step-size choices. Additionally, we present real experiments on 1B-parameter DeepSeekMoE models to complement our theoretical findings. Together, these results build a principled framework for analyzing the Auxiliary-Loss-Free Load Balancing of s-MoE in AI models.",
    "translation": "标题：大规模AI模型中稀疏专家混合的无辅助损失负载均衡理论框架\n\n摘要：在大规模人工智能训练中，稀疏专家混合层通过每个令牌仅激活少量专家子集来实现模型扩展。该设计面临的核心操作挑战是负载均衡问题：如何路由令牌以最小化闲置专家数量，这对高效利用（昂贵的）GPU资源至关重要。本文通过将DeepSeek团队Wang等人（2024）提出的无辅助损失负载均衡过程建模为分配问题的单步原始-对偶迭代方法，建立了相应的理论分析框架。首先，在理想化的确定性场景中，该框架揭示了若干关键结构特性：（1）拉格朗日目标函数的单调改进性；（2）令牌从过载专家向欠载专家迁移的偏好规则；（3）近似均衡保障。随后，我们通过广义在线优化框架纳入AI训练中的随机性与动态特性。在线设定下，我们推导出目标函数的强凸性质，该性质在特定步长选择下可导出对数级期望遗憾界。此外，我们在10亿参数规模的DeepSeekMoE模型上进行了实证实验，以补充理论发现。这些成果共同构建了分析AI模型中稀疏专家混合无辅助损失负载均衡问题的系统性理论框架。",
    "url": "https://huggingface.co/papers/2512.03915",
    "arxiv_url": "https://arxiv.org/abs/2512.03915"
  }
]