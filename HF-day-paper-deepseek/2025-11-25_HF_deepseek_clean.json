[
  {
    "title": "General Agentic Memory Via Deep Research",
    "summary": "Memory is critical for AI agents, yet the widely-adopted static memory, aiming to create readily available memory in advance, is inevitably subject to severe information loss. To address this limitation, we propose a novel framework called general agentic memory (GAM). GAM follows the principle of \"just-in time (JIT) compilation\" where it focuses on creating optimized contexts for its client at runtime while keeping only simple but useful memory during the offline stage. To this end, GAM employs a duo-design with the following components. 1) Memorizer, which highlights key historical information using a lightweight memory, while maintaining complete historical information within a universal page-store. 2) Researcher, which retrieves and integrates useful information from the page-store for its online request guided by the pre-constructed memory. This design allows GAM to effectively leverage the agentic capabilities and test-time scalability of frontier large language models (LLMs), while also facilitating end-to-end performance optimization through reinforcement learning. In our experimental study, we demonstrate that GAM achieves substantial improvement on various memory-grounded task completion scenarios against existing memory systems.",
    "translation": "标题：基于深度研究的通用代理记忆框架\n\n摘要：记忆能力对智能体至关重要，然而当前广泛采用的静态记忆系统旨在预先创建即用记忆，不可避免地会导致严重的信息损失。为克服这一局限，我们提出名为通用代理记忆（GAM）的创新框架。GAM遵循\"即时编译\"原则，在离线阶段仅保留简洁有效的记忆，而在运行时专注于为客户端生成优化语境。该框架采用双重设计：1）记忆器通过轻量级记忆库突出关键历史信息，同时在通用页面存储中维护完整历史记录；2）研究器基于预构建记忆的指引，从页面存储中检索并整合有效信息以响应在线请求。该设计使GAM能有效利用前沿大语言模型的代理能力与测试时扩展性，同时通过强化学习实现端到端的性能优化。实验研究表明，相较于现有记忆系统，GAM在多种基于记忆的任务完成场景中均取得显著提升。",
    "url": "https://huggingface.co/papers/2511.18423",
    "arxiv_url": "https://arxiv.org/abs/2511.18423"
  },
  {
    "title": "AutoEnv: Automated Environments for Measuring Cross-Environment Agent Learning",
    "summary": "Humans naturally adapt to diverse environments by learning underlying rules across worlds with different dynamics, observations, and reward structures. In contrast, existing agents typically demonstrate improvements via self-evolving within a single domain, implicitly assuming a fixed environment distribution. Cross-environment learning has remained largely unmeasured: there is no standard collection of controllable, heterogeneous environments, nor a unified way to represent how agents learn. We address these gaps in two steps. First, we propose AutoEnv, an automated framework that treats environments as factorizable distributions over transitions, observations, and rewards, enabling low-cost (4.12 USD on average) generation of heterogeneous worlds. Using AutoEnv, we construct AutoEnv-36, a dataset of 36 environments with 358 validated levels, on which seven language models achieve 12-49% normalized reward, demonstrating the challenge of AutoEnv-36. Second, we formalize agent learning as a component-centric process driven by three stages of Selection, Optimization, and Evaluation applied to an improvable agent component. Using this formulation, we design eight learning methods and evaluate them on AutoEnv-36. Empirically, the gain of any single learning method quickly decrease as the number of environments increases, revealing that fixed learning methods do not scale across heterogeneous environments. Environment-adaptive selection of learning methods substantially improves performance but exhibits diminishing returns as the method space expands. These results highlight both the necessity and the current limitations of agent learning for scalable cross-environment generalization, and position AutoEnv and AutoEnv-36 as a testbed for studying cross-environment agent learning. The code is avaiable at https://github.com/FoundationAgents/AutoEnv.",
    "translation": "标题：AutoEnv：面向跨环境智能体学习的自动化环境测评框架\n\n摘要：人类能够通过在不同动态特性、观测空间与奖励结构的世界中学习底层规则，自然适应多样环境。相比之下，现有智能体通常通过在单一领域内自我演化实现改进，其隐含假设了固定环境分布。跨环境学习至今缺乏系统化度量：既缺乏可控异构环境的标准数据集，也没有统一表征智能体学习过程的范式。我们通过两个步骤解决这些问题。首先提出AutoEnv框架，将环境建模为状态转移、观测空间与奖励函数的可分解分布，实现低成本（平均4.12美元）生成异构世界。基于该框架构建包含36个环境、358个验证场景的AutoEnv-36数据集，在该数据集上七个语言模型仅获得12-49%的标准化奖励，证明了其挑战性。其次将智能体学习形式化为以组件为中心的演进过程，包含作用于可改进智能体组件的选择、优化与评估三阶段。基于此形式化框架设计八种学习方法并在AutoEnv-36上评估。实证研究表明：当环境数量增加时，任何单一学习方法的收益均快速衰减，表明固定方法难以适应异构环境扩展。环境自适应的学习方法选择能显著提升性能，但随着方法空间扩展示范收益递减。这些结果既揭示了实现可扩展跨环境泛化的必要性，也凸显出现有智能体学习的局限性，使AutoEnv与AutoEnv-36成为研究跨环境智能体学习的基准平台。代码已发布于https://github.com/FoundationAgents/AutoEnv。",
    "url": "https://huggingface.co/papers/2511.19304",
    "arxiv_url": "https://arxiv.org/abs/2511.19304"
  },
  {
    "title": "Computer-Use Agents as Judges for Generative User Interface",
    "summary": "Computer-Use Agents (CUA) are becoming increasingly capable of autonomously operating digital environments through Graphical User Interfaces (GUI). Yet, most GUI remain designed primarily for humans--prioritizing aesthetics and usability--forcing agents to adopt human-oriented behaviors that are unnecessary for efficient task execution. At the same time, rapid advances in coding-oriented language models (Coder) have transformed automatic GUI design. This raises a fundamental question: Can CUA as judges to assist Coder for automatic GUI design? To investigate, we introduce AUI-Gym, a benchmark for Automatic GUI development spanning 52 applications across diverse domains. Using language models, we synthesize 1560 tasks that simulate real-world scenarios. To ensure task reliability, we further develop a verifier that programmatically checks whether each task is executable within its environment. Building on this, we propose a Coder-CUA in Collaboration framework: the Coder acts as Designer, generating and revising websites, while the CUA serves as Judge, evaluating functionality and refining designs. Success is measured not by visual appearance, but by task solvability and CUA navigation success rate. To turn CUA feedback into usable guidance, we design a CUA Dashboard that compresses multi-step navigation histories into concise visual summaries, offering interpretable guidance for iterative redesign. By positioning agents as both designers and judges, our framework shifts interface design toward agent-native efficiency and reliability. Our work takes a step toward shifting agents from passive use toward active participation in digital environments. Our code and dataset are available at https://github.com/showlab/AUI.",
    "translation": "标题：计算机使用代理作为生成式用户界面的评估者\n\n摘要：计算机使用代理（CUA）正逐渐具备通过图形用户界面（GUI）自主操作数字环境的能力。然而，大多数GUI仍主要面向人类设计——优先考虑美学和可用性——迫使代理采用对高效任务执行非必要的人类导向行为。与此同时，面向编码的语言模型（Coder）的快速发展正在改变自动GUI设计范式。这引出一个根本性问题：能否以CUA作为评估者来辅助Coder进行自动GUI设计？为探究此问题，我们推出AUI-Gym基准测试平台，涵盖52个跨领域应用程序的自动GUI开发。通过语言模型合成模拟真实场景的1560项任务，并开发可编程验证器确保每个任务在对应环境中的可执行性。在此基础上，我们提出“编码者-使用代理协同”框架：Coder担任设计师角色，负责生成和修改网站；CUA担任评估者角色，负责功能测试与设计优化。成功标准不再局限于视觉呈现，而是以任务可解性与CUA导航成功率为核心指标。为将CUA反馈转化为可操作指导，我们设计CUA仪表盘，将多步导航历史压缩为简洁可视化摘要，为迭代重设计提供可解释指引。通过让代理同时担任设计者与评估者，本框架推动界面设计向代理原生效率与可靠性转变。我们的工作推动代理从被动使用转向数字环境的主动参与。代码与数据集详见https://github.com/showlab/AUI。",
    "url": "https://huggingface.co/papers/2511.15567",
    "arxiv_url": "https://arxiv.org/abs/2511.15567"
  },
  {
    "title": "DeCo: Frequency-Decoupled Pixel Diffusion for End-to-End Image Generation",
    "summary": "Pixel diffusion aims to generate images directly in pixel space in an end-to-end fashion. This approach avoids the limitations of VAE in the two-stage latent diffusion, offering higher model capacity. Existing pixel diffusion models suffer from slow training and inference, as they usually model both high-frequency signals and low-frequency semantics within a single diffusion transformer (DiT). To pursue a more efficient pixel diffusion paradigm, we propose the frequency-DeCoupled pixel diffusion framework. With the intuition to decouple the generation of high and low frequency components, we leverage a lightweight pixel decoder to generate high-frequency details conditioned on semantic guidance from the DiT. This thus frees the DiT to specialize in modeling low-frequency semantics. In addition, we introduce a frequency-aware flow-matching loss that emphasizes visually salient frequencies while suppressing insignificant ones. Extensive experiments show that DeCo achieves superior performance among pixel diffusion models, attaining FID of 1.62 (256x256) and 2.22 (512x512) on ImageNet, closing the gap with latent diffusion methods. Furthermore, our pretrained text-to-image model achieves a leading overall score of 0.86 on GenEval in system-level comparison. Codes are publicly available at https://github.com/Zehong-Ma/DeCo.",
    "translation": "标题：DeCo：面向端到端图像生成的频域解耦像素扩散方法\n\n摘要：像素扩散方法旨在以端到端方式直接在像素空间生成图像。该方法规避了两阶段潜在扩散中VAE的局限性，具有更高的模型容量。现有像素扩散模型存在训练与推理速度缓慢的问题，因其通常采用单一扩散变换器同时对高频信号与低频语义进行建模。为探索更高效的像素扩散范式，本文提出频域解耦的像素扩散框架。基于高低频分量解耦生成的直觉思路，我们采用轻量级像素解码器在扩散变换器的语义引导下生成高频细节，从而使扩散变换器专注于低频语义建模。此外，我们引入了频域感知流匹配损失函数，该函数能强化视觉显著频率同时抑制非显著频率。大量实验表明，DeCo在像素扩散模型中实现了卓越性能，在ImageNet数据集上分别获得1.62（256×256）和2.22（512×512）的FID指标，显著缩小了与潜在扩散方法的差距。此外，我们预训练的文本生成图像模型在GenEval系统级评估中取得了0.86的综合领先得分。代码已开源于https://github.com/Zehong-Ma/DeCo。",
    "url": "https://huggingface.co/papers/2511.19365",
    "arxiv_url": "https://arxiv.org/abs/2511.19365"
  },
  {
    "title": "DR Tulu: Reinforcement Learning with Evolving Rubrics for Deep Research",
    "summary": "Deep research models perform multi-step research to produce long-form, well-attributed answers. However, most open deep research models are trained on easily verifiable short-form QA tasks via reinforcement learning with verifiable rewards (RLVR), which does not extend to realistic long-form tasks. We address this with Reinforcement Learning with Evolving Rubrics (RLER), in which we construct and maintain rubrics that co-evolve with the policy model during training; this allows the rubrics to incorporate information that the model has newly explored and to provide discriminative, on-policy feedback. Using RLER, we develop Deep Research Tulu (DR Tulu-8B), the first open model that is directly trained for open-ended, long-form deep research. Across four long-form deep research benchmarks in science, healthcare and general domains, DR Tulu substantially outperforms existing open deep research models, and matches or exceeds proprietary deep research systems, while being significantly smaller and cheaper per query. To facilitate future research, we release all data, models, and code, including our new MCP-based agent infrastructure for deep research systems.",
    "translation": "标题：DR Tulu：基于演化评价标准的深度研究强化学习\n\n摘要：深度研究模型通过多步骤研究生成具有完善引证的长篇答案。然而多数开源深度研究模型通过可验证奖励的强化学习在易于验证的短问答任务上训练，这无法适用于现实长篇任务。我们提出基于演化评价标准的强化学习解决方案，通过构建与策略模型在训练过程中协同演进的评价标准体系，使评价标准能够整合模型新探索的信息并提供具有区分度的同策略反馈。基于该方法，我们开发了深度研究型Tulu模型（DR Tulu-8B），这是首个直接针对开放式长篇深度研究训练的开源模型。在涵盖科学、医疗和通用领域的四个长篇深度研究基准测试中，DR Tulu显著优于现有开源深度研究模型，达到或超越专业深度研究系统的性能，同时具有更小的模型规模与更低的单次查询成本。为促进后续研究，我们公开了全部数据、模型与代码，包括新开发的基于MCP的深度研究系统智能体架构。",
    "url": "https://huggingface.co/papers/2511.19399",
    "arxiv_url": "https://arxiv.org/abs/2511.19399"
  },
  {
    "title": "UltraFlux: Data-Model Co-Design for High-quality Native 4K Text-to-Image Generation across Diverse Aspect Ratios",
    "summary": "Diffusion transformers have recently delivered strong text-to-image generation around 1K resolution, but we show that extending them to native 4K across diverse aspect ratios exposes a tightly coupled failure mode spanning positional encoding, VAE compression, and optimization. Tackling any of these factors in isolation leaves substantial quality on the table. We therefore take a data-model co-design view and introduce UltraFlux, a Flux-based DiT trained natively at 4K on MultiAspect-4K-1M, a 1M-image 4K corpus with controlled multi-AR coverage, bilingual captions, and rich VLM/IQA metadata for resolution- and AR-aware sampling. On the model side, UltraFlux couples (i) Resonance 2D RoPE with YaRN for training-window-, frequency-, and AR-aware positional encoding at 4K; (ii) a simple, non-adversarial VAE post-training scheme that improves 4K reconstruction fidelity; (iii) an SNR-Aware Huber Wavelet objective that rebalances gradients across timesteps and frequency bands; and (iv) a Stage-wise Aesthetic Curriculum Learning strategy that concentrates high-aesthetic supervision on high-noise steps governed by the model prior. Together, these components yield a stable, detail-preserving 4K DiT that generalizes across wide, square, and tall ARs. On the Aesthetic-Eval at 4096 benchmark and multi-AR 4K settings, UltraFlux consistently outperforms strong open-source baselines across fidelity, aesthetic, and alignment metrics, and-with a LLM prompt refiner-matches or surpasses the proprietary Seedream 4.0.",
    "translation": "标题：UltraFlux：面向多比例原生4K文本到图像生成的数据-模型协同设计方法\n\n摘要：扩散变换器近期在1K分辨率文本到图像生成领域取得显著进展，但本研究表明，将其扩展至多比例原生4K生成时会暴露出位置编码、VAE压缩与优化间的强耦合失效模式。单独解决任一因素均会导致质量显著损失。为此，我们提出数据-模型协同设计框架，推出基于Flux架构的扩散变换器UltraFlux，该模型在MultiAspect-4K-1M数据集上实现原生4K训练——该百万量级4K图像库具备可控多比例覆盖、双语标注及丰富的视觉语言模型/图像质量评估元数据，支持分辨率与比例感知采样。模型层面，UltraFlux集成四大创新：(i) 共振二维RoPE与YaRN结合，实现训练窗口/频率/比例感知的4K位置编码；(ii) 采用简洁的非对抗式VAE后训练方案提升4K重建保真度；(iii) 设计信噪比感知的Huber小波目标函数，重构时间步与频带间的梯度平衡；(iv) 提出分阶段美学课程学习策略，将高美学监督集中于模型先验主导的高噪声阶段。这些组件共同构建出稳定保持细节的4K扩散变换器，可泛化至宽屏、方形与竖屏比例。在4096分辨率美学评估基准及多比例4K设定下，UltraFlux在保真度、美学质量与语义对齐指标上持续超越主流开源基线，配合大语言模型提示优化器时更达到或超越商用系统Seedream 4.0的性能水平。",
    "url": "https://huggingface.co/papers/2511.18050",
    "arxiv_url": "https://arxiv.org/abs/2511.18050"
  },
  {
    "title": "In-Video Instructions: Visual Signals as Generative Control",
    "summary": "Large-scale video generative models have recently demonstrated strong visual capabilities, enabling the prediction of future frames that adhere to the logical and physical cues in the current observation. In this work, we investigate whether such capabilities can be harnessed for controllable image-to-video generation by interpreting visual signals embedded within the frames as instructions, a paradigm we term In-Video Instruction. In contrast to prompt-based control, which provides textual descriptions that are inherently global and coarse, In-Video Instruction encodes user guidance directly into the visual domain through elements such as overlaid text, arrows, or trajectories. This enables explicit, spatial-aware, and unambiguous correspondences between visual subjects and their intended actions by assigning distinct instructions to different objects. Extensive experiments on three state-of-the-art generators, including Veo 3.1, Kling 2.5, and Wan 2.2, show that video models can reliably interpret and execute such visually embedded instructions, particularly in complex multi-object scenarios.",
    "translation": "标题：视频内指令：作为生成控制信号的视觉标识\n\n摘要：大规模视频生成模型近期展现出强大的视觉能力，能够基于当前观察中的逻辑与物理线索预测符合逻辑的后续帧。本研究探索如何通过解析嵌入视频帧的视觉信号作为指令，将此类能力应用于可控的图像-视频生成，这一范式被我们称为“视频内指令”。与基于文本提示（其描述 inherently 具有全局性和粗略性）的控制方式不同，视频内指令通过叠加文字、箭头或运动轨迹等视觉元素，将用户引导直接编码至视觉域。该方法通过为不同对象分配独立指令，实现了视觉主体与预期动作之间明确、空间感知且无歧义的对应关系。在包括Veo 3.1、Kling 2.5和Wan 2.2在内的三种前沿生成器上的大量实验表明，视频模型能够可靠解析并执行此类视觉嵌入指令，尤其在复杂多对象场景中表现突出。",
    "url": "https://huggingface.co/papers/2511.19401",
    "arxiv_url": "https://arxiv.org/abs/2511.19401"
  },
  {
    "title": "Budget-Aware Tool-Use Enables Effective Agent Scaling",
    "summary": "Scaling test-time computation improves performance across different tasks on large language models (LLMs), which has also been extended to tool-augmented agents. For these agents, scaling involves not only \"thinking\" in tokens but also \"acting\" via tool calls. The number of tool calls directly bounds the agent's interaction with the external environment. However, we find that simply granting agents a larger tool-call budget fails to improve performance, as they lack \"budget awareness\" and quickly hit a performance ceiling. To address this, we study how to scale such agents effectively under explicit tool-call budgets, focusing on web search agents. We first introduce the Budget Tracker, a lightweight plug-in that provides the agent with continuous budget awareness, enabling simple yet effective scaling. We further develop BATS (Budget Aware Test-time Scaling), an advanced framework that leverages this awareness to dynamically adapt its planning and verification strategy, deciding whether to \"dig deeper\" on a promising lead or \"pivot\" to new paths based on remaining resources. To analyze cost-performance scaling in a controlled manner, we formalize a unified cost metric that jointly accounts for token and tool consumption. We provide the first systematic study on budget-constrained agents, showing that budget-aware methods produce more favorable scaling curves and push the cost-performance Pareto frontier. Our work offers empirical insights toward a more transparent and principled understanding of scaling in tool-augmented agents.",
    "translation": "标题：预算感知的工具使用实现高效智能体扩展\n\n摘要：扩展测试时计算能够提升大语言模型在不同任务上的性能表现，这一机制已被延伸至工具增强型智能体领域。对于此类智能体而言，扩展不仅涉及基于标记的\"思考\"，还包括通过工具调用的\"行动\"。工具调用次数直接制约着智能体与外部环境的交互深度。然而我们发现，单纯增加工具调用预算并不能提升性能，因为智能体缺乏\"预算感知\"能力，会迅速触及性能瓶颈。为此，我们研究如何在明确工具调用预算约束下有效扩展智能体性能，重点关注网络搜索智能体。我们首先提出预算追踪器——一种轻量级插件，可为智能体提供持续预算感知，实现简单而有效的扩展。进一步开发出BATS（预算感知测试时扩展）框架，该高级框架利用预算感知动态调整其规划与验证策略，根据剩余资源决定是\"深入挖掘\"潜在线索还是\"转向\"新路径。为系统分析成本-性能扩展关系，我们构建了统一成本度量标准，统筹考量标记消耗与工具调用。我们首次对预算约束型智能体开展系统性研究，证明预算感知方法能产生更优的扩展曲线并推动成本-性能帕累托边界。本研究为工具增强型智能体的扩展机制提供了更透明、更原则性的实证依据。",
    "url": "https://huggingface.co/papers/2511.17006",
    "arxiv_url": "https://arxiv.org/abs/2511.17006"
  },
  {
    "title": "Chain-of-Visual-Thought: Teaching VLMs to See and Think Better with Continuous Visual Tokens",
    "summary": "Vision-Language Models (VLMs) excel at reasoning in linguistic space but struggle with perceptual understanding that requires dense visual perception, e.g., spatial reasoning and geometric awareness. This limitation stems from the fact that current VLMs have limited mechanisms to capture dense visual information across spatial dimensions. We introduce Chain-of-Visual-Thought (COVT), a framework that enables VLMs to reason not only in words but also through continuous visual tokens-compact latent representations that encode rich perceptual cues. Within a small budget of roughly 20 tokens, COVT distills knowledge from lightweight vision experts, capturing complementary properties such as 2D appearance, 3D geometry, spatial layout, and edge structure. During training, the VLM with COVT autoregressively predicts these visual tokens to reconstruct dense supervision signals (e.g., depth, segmentation, edges, and DINO features). At inference, the model reasons directly in the continuous visual token space, preserving efficiency while optionally decoding dense predictions for interpretability. Evaluated across more than ten diverse perception benchmarks, including CV-Bench, MMVP, RealWorldQA, MMStar, WorldMedQA, and HRBench, integrating COVT into strong VLMs such as Qwen2.5-VL and LLaVA consistently improves performance by 3% to 16% and demonstrates that compact continuous visual thinking enables more precise, grounded, and interpretable multimodal intelligence.",
    "translation": "标题：视觉思维链：通过连续视觉标记提升视觉语言模型的感知与推理能力\n\n摘要：视觉语言模型在语言空间的推理方面表现卓越，但在需要密集视觉感知的任务（如空间推理和几何认知）中存在明显局限。这一缺陷源于现有模型在捕捉空间维度密集视觉信息方面的机制不足。我们提出视觉思维链框架，使视觉语言模型不仅能够进行语言推理，还能通过连续视觉标记——编码丰富感知线索的紧凑潜在表示——进行视觉思考。该框架仅需约20个标记的预算，即可从轻量级视觉专家模型中蒸馏知识，捕捉二维外观、三维几何、空间布局和边缘结构等互补特性。在训练过程中，搭载视觉思维链的视觉语言模型通过自回归预测这些视觉标记来重建密集监督信号（如深度、分割、边缘和DINO特征）。在推理阶段，模型直接在连续视觉标记空间进行推理，在保持效率的同时可选择性解码密集预测以增强可解释性。在超过十项感知基准测试（包括CV-Bench、MMVP、RealWorldQA、MMStar、WorldMedQA和HRBench）上的评估表明，将视觉思维链集成至Qwen2.5-VL和LLaVA等强视觉语言模型后，性能持续提升3%至16%，证实紧凑的连续视觉思维能够实现更精准、可验证且可解释的多模态智能。",
    "url": "https://huggingface.co/papers/2511.19418",
    "arxiv_url": "https://arxiv.org/abs/2511.19418"
  },
  {
    "title": "HunyuanVideo 1.5 Technical Report",
    "summary": "We present HunyuanVideo 1.5, a lightweight yet powerful open-source video generation model that achieves state-of-the-art visual quality and motion coherence with only 8.3 billion parameters, enabling efficient inference on consumer-grade GPUs. This achievement is built upon several key components, including meticulous data curation, an advanced DiT architecture featuring selective and sliding tile attention (SSTA), enhanced bilingual understanding through glyph-aware text encoding, progressive pre-training and post-training, and an efficient video super-resolution network. Leveraging these designs, we developed a unified framework capable of high-quality text-to-video and image-to-video generation across multiple durations and resolutions.Extensive experiments demonstrate that this compact and proficient model establishes a new state-of-the-art among open-source video generation models. By releasing the code and model weights, we provide the community with a high-performance foundation that lowers the barrier to video creation and research, making advanced video generation accessible to a broader audience. All open-source assets are publicly available at https://github.com/Tencent-Hunyuan/HunyuanVideo-1.5.",
    "translation": "标题：混元视频1.5技术报告\n\n摘要：本文提出混元视频1.5——一个轻量级但功能强大的开源视频生成模型，仅需83亿参数即可实现业界领先的视觉质量与运动连贯性，并能在消费级GPU上实现高效推理。该成果基于多项核心技术构建：包括精细化的数据治理方案、采用选择性滑动分块注意力机制（SSTA）的先进DiT架构、通过字形感知文本编码强化的双语理解能力、渐进式预训练与后训练策略，以及高效视频超分网络。依托这些设计，我们开发出能够跨越多时长与多分辨率生成高质量文生视频和图生视频的统一框架。大量实验表明，这个紧凑而高效的模型在开源视频生成领域中确立了新的技术标杆。通过公开代码与模型权重，我们为社区提供了高性能基础架构，显著降低视频创作与研究门槛，使更广泛的用户群体能够接触先进的视频生成技术。所有开源资源已发布于https://github.com/Tencent-Hunyuan/HunyuanVideo-1.5。",
    "url": "https://huggingface.co/papers/2511.18870",
    "arxiv_url": "https://arxiv.org/abs/2511.18870"
  },
  {
    "title": "Pillar-0: A New Frontier for Radiology Foundation Models",
    "summary": "Radiology plays an integral role in modern medicine, yet rising imaging volumes have far outpaced workforce growth. Foundation models offer a path toward assisting with the full spectrum of radiology tasks, but existing medical models remain limited: they process volumetric CT and MRI as low-fidelity 2D slices, discard critical grayscale contrast information, and lack evaluation frameworks that reflect real clinical practice. We introduce Pillar-0, a radiology foundation model pretrained on 42,990 abdomen-pelvis CTs, 86,411 chest CTs, 14,348 head CTs, and 11,543 breast MRIs from a large academic center, together with RATE, a scalable framework that extracts structured labels for 366 radiologic findings with near-perfect accuracy using LLMs. Across internal test sets of 14,230 abdomen-pelvis CTs, 10,646 chest CTs, 4,906 head CTs, and 1,585 breast MRIs, Pillar-0 establishes a new performance frontier, achieving mean AUROCs of 86.4, 88.0, 90.1, and 82.9, outperforming MedGemma (Google), MedImageInsight (Microsoft), Lingshu (Alibaba), and Merlin (Stanford) by 7.8-15.8 AUROC points and ranking best in 87.2\\% (319/366) tasks. Pillar-0 similarly outperforms all baselines in an external validation on the Stanford Abdominal CT dataset, including Merlin (82.2 vs 80.6 AUROC). Pillar-0 extends to tasks beyond its pretraining, such as long-horizon lung cancer risk prediction, where it improves upon the state-of-the-art Sybil by 3.0 C-index points on NLST, and generalizes with gains of 5.9 (MGH) and 1.9 (CGMH). In brain hemorrhage detection, Pillar-0 obtained a >95 AUROC when using only 1/20th of the data of the next most sample efficient baseline. Pillar-0 and RATE together provide an open, clinically rigorous foundation for building high-performance radiology systems, enabling applications that were previously infeasible due to computational, data, and evaluation constraints.",
    "translation": "标题：Pillar-0：放射学基础模型的新前沿\n\n摘要：放射学在现代医学中扮演着不可或缺的角色，然而影像数据量的增长速度已远超放射科医师队伍的增长规模。基础模型为辅助完成全流程放射学任务提供了可行路径，但现有医学模型仍存在局限：将三维CT和MRI数据作为低精度二维切片处理、丢弃关键灰度对比信息、缺乏反映真实临床实践的评估框架。我们提出Pillar-0——基于某大型学术中心42,990例盆腹部CT、86,411例胸部CT、14,348例头部CT及11,543例乳腺MRI预训练的放射学基础模型，同时推出RATE框架，该框架利用大语言模型以近乎完美的准确率提取366种放射学征象的结构化标签。在包含14,230例盆腹部CT、10,646例胸部CT、4,906例头部CT和1,585例乳腺MRI的内部测试集中，Pillar-0创造了性能新标杆，平均AUROC分别达到86.4、88.0、90.1和82.9，以7.8-15.8个AUROC点的优势超越MedGemma（谷歌）、MedImageInsight（微软）、灵枢（阿里巴巴）和Merlin（斯坦福），在87.2%（319/366）的任务中位列第一。在斯坦福腹部CT数据集的外部验证中，Pillar-0同样优于所有基线模型（AUROC 82.2 vs 80.6）。该模型还可拓展至预训练范围外的任务，如在肺癌长期风险预测中，其于NLST数据集上将当前最优模型Sybil的C指数提升3.0点，并在MGH（提升5.9点）和CGMH（提升1.9点）数据集上展现卓越泛化能力。在脑出血检测任务中，Pillar-0仅需次优基线模型1/20的数据量即可获得>95的AUROC。Pillar-0与RATE共同为构建高性能放射学系统提供了开放且临床严谨的基础，突破了以往因计算资源、数据获取和评估限制而难以实现的应用场景。",
    "url": "https://huggingface.co/papers/2511.17803",
    "arxiv_url": "https://arxiv.org/abs/2511.17803"
  },
  {
    "title": "Plan-X: Instruct Video Generation via Semantic Planning",
    "summary": "Diffusion Transformers have demonstrated remarkable capabilities in visual synthesis, yet they often struggle with high-level semantic reasoning and long-horizon planning. This limitation frequently leads to visual hallucinations and mis-alignments with user instructions, especially in scenarios involving complex scene understanding, human-object interactions, multi-stage actions, and in-context motion reasoning. To address these challenges, we propose Plan-X, a framework that explicitly enforces high-level semantic planning to instruct video generation process. At its core lies a Semantic Planner, a learnable multimodal language model that reasons over the user's intent from both text prompts and visual context, and autoregressively generates a sequence of text-grounded spatio-temporal semantic tokens. These semantic tokens, complementary to high-level text prompt guidance, serve as structured \"semantic sketches\" over time for the video diffusion model, which has its strength at synthesizing high-fidelity visual details. Plan-X effectively integrates the strength of language models in multimodal in-context reasoning and planning, together with the strength of diffusion models in photorealistic video synthesis. Extensive experiments demonstrate that our framework substantially reduces visual hallucinations and enables fine-grained, instruction-aligned video generation consistent with multimodal context.",
    "translation": "标题：Plan-X：基于语义规划的教学视频生成框架\n\n摘要：扩散变换器在视觉合成领域展现出卓越能力，但其在高层语义推理与长程规划方面仍存在明显不足。这种局限性常导致视觉伪影及与用户指令的错位，尤其在涉及复杂场景理解、人物-物体交互、多阶段动作及情境运动推理的场景中更为突出。为解决这些挑战，我们提出Plan-X框架，通过显式强化高层语义规划来指导视频生成过程。其核心是语义规划器——一个可学习的多模态语言模型，能够基于文本提示和视觉上下文对用户意图进行推理，并自回归生成基于文本的时空语义标记序列。这些语义标记与高层文本提示指导形成互补，作为随时间演化的结构化\"语义草图\"输入视频扩散模型，后者擅长合成高保真度的视觉细节。Plan-X有效融合了语言模型在多模态情境推理与规划方面的优势，以及扩散模型在逼真视频合成方面的特长。大量实验表明，我们的框架显著减少了视觉伪影，能够生成与多模态语境一致、符合细粒度指令的视频内容。",
    "url": "https://huggingface.co/papers/2511.17986",
    "arxiv_url": "https://arxiv.org/abs/2511.17986"
  },
  {
    "title": "M3-Bench: Multi-Modal, Multi-Hop, Multi-Threaded Tool-Using MLLM Agent Benchmark",
    "summary": "We present M^3-Bench, the first benchmark for evaluating multimodal tool use under the Model Context Protocol. The benchmark targets realistic, multi-hop and multi-threaded workflows that require visual grounding and textual reasoning, cross-tool dependencies, and persistence of intermediate resources across steps. We introduce a similarity-driven alignment that serializes each tool call, embeds signatures with a sentence encoder, and performs similarity-bucketed Hungarian matching to obtain auditable one-to-one correspondences. On top of this alignment, we report interpretable metrics that decouple semantic fidelity from workflow consistency. The benchmark spans 28 servers with 231 tools, and provides standardized trajectories curated through an Executor & Judge pipeline with human verification; an auxiliary four large language models (LLMs) judge ensemble reports end-task Task Completion and information grounding. Evaluations of representative state-of-the-art Multimodal LLMs (MLLMs) reveal persistent gaps in multimodal MCP tool use, particularly in argument fidelity and structure consistency, underscoring the need for methods that jointly reason over images, text, and tool graphs. Our Benchmark's anonymous repository is at https://github.com/EtaYang10th/Open-M3-Bench",
    "translation": "标题：M3-Bench：面向多模态、多跳、多线程工具使用型MLLM智能体的基准测试框架\n\n摘要：本研究提出M³-Bench——首个基于模型上下文协议评估多模态工具使用能力的基准测试体系。该基准针对需要视觉定位与文本推理、跨工具依赖关系以及中间资源跨步骤持久化的现实场景，设计了多跳式与多线程工作流。我们引入基于相似度的对齐方法：通过序列化每个工具调用，使用语句编码器嵌入函数签名，并执行基于相似度分桶的匈牙利匹配算法，最终获得可审计的一对一对应关系。在此对齐机制基础上，我们提出可解释的评估指标，将语义保真度与工作流一致性进行解耦分析。该基准涵盖28个服务器共231种工具，通过配备人工验证的执行器-评判器流水线提供标准化轨迹；另设包含四个大语言模型的辅助评判组，专门评估终端任务完成度与信息锚定质量。对代表性前沿多模态大语言模型的评估结果表明，现有模型在多模态MCP工具使用方面存在显著差距，尤其在参数保真度与结构一致性方面，这凸显了需要开发能联合推理图像、文本与工具图结构的新方法。本基准测试的匿名代码库位于：https://github.com/EtaYang10th/Open-M3-Bench",
    "url": "https://huggingface.co/papers/2511.17729",
    "arxiv_url": "https://arxiv.org/abs/2511.17729"
  },
  {
    "title": "Multi-Agent Deep Research: Training Multi-Agent Systems with M-GRPO",
    "summary": "Multi-agent systems perform well on general reasoning tasks. However, the lack of training in specialized areas hinders their accuracy. Current training methods train a unified large language model (LLM) for all agents in the system. This may limit the performances due to different distributions underlying for different agents. Therefore, training multi-agent systems with distinct LLMs should be the next step to solve. However, this approach introduces optimization challenges. For example, agents operate at different frequencies, rollouts involve varying sub-agent invocations, and agents are often deployed across separate servers, disrupting end-to-end gradient flow. To address these issues, we propose M-GRPO, a hierarchical extension of Group Relative Policy Optimization designed for vertical Multi-agent systems with a main agent (planner) and multiple sub-agents (multi-turn tool executors). M-GRPO computes group-relative advantages for both main and sub-agents, maintaining hierarchical credit assignment. It also introduces a trajectory-alignment scheme that generates fixed-size batches despite variable sub-agent invocations. We deploy a decoupled training pipeline in which agents run on separate servers and exchange minimal statistics via a shared store. This enables scalable training without cross-server backpropagation. In experiments on real-world benchmarks (e.g., GAIA, XBench-DeepSearch, and WebWalkerQA), M-GRPO consistently outperforms both single-agent GRPO and multi-agent GRPO with frozen sub-agents, demonstrating improved stability and sample efficiency. These results show that aligning heterogeneous trajectories and decoupling optimization across specialized agents enhances tool-augmented reasoning tasks.",
    "translation": "标题：多智能体深度研究：基于M-GRPO的多智能体系统训练方法\n\n摘要：多智能体系统在通用推理任务中表现优异，但在专业领域因缺乏针对性训练而导致准确性不足。现有训练方法为系统中所有智能体构建统一的大语言模型，这受限于不同智能体底层数据分布的差异性而影响性能表现。因此，采用差异化大语言模型进行多智能体系统训练成为亟待解决的课题，但该方法会引发新的优化挑战：智能体运行频率存在差异、决策过程涉及可变子智能体调用、跨服务器部署模式破坏端到端梯度传递等。为解决这些问题，我们提出M-GRPO——面向具有主智能体（规划器）与多子智能体（多轮工具执行器）的垂直多智能体系统的群组相对策略优化分层扩展框架。该框架通过计算主次智能体的群组相对优势值保持分层信用分配，并设计轨迹对齐机制以应对可变子智能体调用生成固定尺寸训练批次。我们部署了去耦合训练管道，使智能体可在独立服务器运行并通过共享存储交换最小统计量，实现无需跨服务器反向传播的可扩展训练。在真实基准测试（GAIA、XBench-DeepSearch、WebWalkerQA）中，M-GRPO持续超越单智能体GRPO及冻结子智能体的多智能体GRPO，展现出更优的稳定性和样本效率。实验结果表明，对齐异构轨迹与解耦专业智能体优化能有效增强工具增强型推理任务性能。",
    "url": "https://huggingface.co/papers/2511.13288",
    "arxiv_url": "https://arxiv.org/abs/2511.13288"
  },
  {
    "title": "Beyond Multiple Choice: Verifiable OpenQA for Robust Vision-Language RFT",
    "summary": "Multiple-choice question answering (MCQA) has been a popular format for evaluating and reinforcement fine-tuning (RFT) of modern multimodal language models. Its constrained output format allows for simplified, deterministic automatic verification. However, we find that the options may leak exploitable signals, which makes the accuracy metrics unreliable for indicating real capabilities and encourages explicit or implicit answer guessing behaviors during RFT. We propose ReVeL (Rewrite and Verify by LLM), a framework that rewrites multiple-choice questions into open-form questions while keeping answers verifiable whenever possible. The framework categorizes questions according to different answer types, apply different rewriting and verification schemes, respectively. When applied for RFT, we converted 20k MCQA examples and use GRPO to finetune Qwen2.5-VL models. Models trained on ReVeL-OpenQA match MCQA accuracy on multiple-choice benchmarks and improve OpenQA accuracy by about six percentage points, indicating better data efficiency and more robust reward signals than MCQA-based training. When used for evaluation, ReVeL also reveals up to 20 percentage points of score inflation in MCQA benchmarks (relative to OpenQA), improves judging accuracy, and reduces both cost and latency. We will release code and data publicly.",
    "translation": "标题：超越选择题：面向鲁棒视觉语言强化微调的可验证开放式问答\n\n摘要：选择题问答（MCQA）长期以来作为评估现代多模态语言模型及其强化微-tuning（RFT）的主流范式。其受限的输出格式支持简化且确定性的自动验证。然而，我们发现选项可能泄露可被模型利用的线索，导致准确率指标无法真实反映模型能力，并助长RFT过程中显性或隐性的答案猜测行为。我们提出ReVeL（基于大语言模型的重写与验证）框架，将选择题转化为开放式问题，同时尽可能保持答案的可验证性。该框架根据不同答案类型对问题进行分类，并分别采用差异化的重写与验证方案。在RFT应用场景中，我们转化了2万个MCQA样本，并采用GRPO算法对Qwen2.5-VL模型进行微调。实验表明：基于ReVeL-OpenQA训练的模型在选择题基准测试中保持原有准确率，同时将开放式问答准确率提升约6个百分点，这证明其相较于基于MCQA的训练具有更优的数据效率和更鲁棒的奖励信号。在评估场景中，ReVeL还揭示了MCQA基准测试中最高达20个百分点的分数虚高（相对于开放式问答），同时提升了评判准确率，并降低了成本与延迟。我们将公开相关代码与数据。",
    "url": "https://huggingface.co/papers/2511.17405",
    "arxiv_url": "https://arxiv.org/abs/2511.17405"
  },
  {
    "title": "MIST: Mutual Information Via Supervised Training",
    "summary": "We propose a fully data-driven approach to designing mutual information (MI) estimators. Since any MI estimator is a function of the observed sample from two random variables, we parameterize this function with a neural network (MIST) and train it end-to-end to predict MI values. Training is performed on a large meta-dataset of 625,000 synthetic joint distributions with known ground-truth MI. To handle variable sample sizes and dimensions, we employ a two-dimensional attention scheme ensuring permutation invariance across input samples. To quantify uncertainty, we optimize a quantile regression loss, enabling the estimator to approximate the sampling distribution of MI rather than return a single point estimate. This research program departs from prior work by taking a fully empirical route, trading universal theoretical guarantees for flexibility and efficiency. Empirically, the learned estimators largely outperform classical baselines across sample sizes and dimensions, including on joint distributions unseen during training. The resulting quantile-based intervals are well-calibrated and more reliable than bootstrap-based confidence intervals, while inference is orders of magnitude faster than existing neural baselines. Beyond immediate empirical gains, this framework yields trainable, fully differentiable estimators that can be embedded into larger learning pipelines. Moreover, exploiting MI's invariance to invertible transformations, meta-datasets can be adapted to arbitrary data modalities via normalizing flows, enabling flexible training for diverse target meta-distributions.",
    "translation": "标题：MIST：基于监督训练的互信息估计方法\n\n摘要：本文提出一种完全数据驱动的互信息估计器设计方法。由于任何互信息估计器都是两个随机变量观测样本的函数，我们采用神经网络（MIST）对该函数进行参数化，并通过端到端训练来预测互信息值。训练过程基于包含625,000个具有真实互信息值的合成联合分布元数据集。为处理可变样本量与维度，我们采用二维注意力机制确保输入样本的排列不变性。为量化不确定性，我们优化分位数回归损失函数，使估计器能够逼近互信息的抽样分布而非返回单点估计值。本研究方案突破传统研究范式，通过完全经验化路径，以理论普适性换取灵活性与效率。实证研究表明，经过训练的估计器在不同样本量与维度条件下均显著优于经典基线方法，包括在训练阶段未出现的联合分布上亦表现良好。基于分位数的置信区间校准良好，且比基于自助法的置信区间更可靠，同时推理速度比现有神经基线方法快数个数量级。除直接实证优势外，该框架可生成可训练、完全可微的估计器，能够嵌入更大型的学习流程。此外，利用互信息对可逆变换的不变性特性，通过标准化流可将元数据集适配至任意数据模态，从而为不同目标元分布实现灵活训练。\n\n请按照以下格式返回：\n标题：MIST：基于监督训练的互信息估计方法\n摘要：本文提出一种完全数据驱动的互信息估计器设计方法。由于任何互信息估计器都是两个随机变量观测样本的函数，我们采用神经网络（MIST）对该函数进行参数化，并通过端到端训练来预测互信息值。训练过程基于包含625,000个具有真实互信息值的合成联合分布元数据集。为处理可变样本量与维度，我们采用二维注意力机制确保输入样本的排列不变性。为量化不确定性，我们优化分位数回归损失函数，使估计器能够逼近互信息的抽样分布而非返回单点估计值。本研究方案突破传统研究范式，通过完全经验化路径，以理论普适性换取灵活性与效率。实证研究表明，经过训练的估计器在不同样本量与维度条件下均显著优于经典基线方法，包括在训练阶段未出现的联合分布上亦表现良好。基于分位数的置信区间校准良好，且比基于自助法的置信区间更可靠，同时推理速度比现有神经基线方法快数个数量级。除直接实证优势外，该框架可生成可训练、完全可微的估计器，能够嵌入更大型的学习流程。此外，利用互信息对可逆变换的不变性特性，通过标准化流可将元数据集适配至任意数据模态，从而为不同目标元分布实现灵活训练。",
    "url": "https://huggingface.co/papers/2511.18945",
    "arxiv_url": "https://arxiv.org/abs/2511.18945"
  },
  {
    "title": "The Image as Its Own Reward: Reinforcement Learning with Adversarial Reward for Image Generation",
    "summary": "A reliable reward function is essential for reinforcement learning (RL) in image generation. Most current RL approaches depend on pre-trained preference models that output scalar rewards to approximate human preferences. However, these rewards often fail to capture human perception and are vulnerable to reward hacking, where higher scores do not correspond to better images. To address this, we introduce Adv-GRPO, an RL framework with an adversarial reward that iteratively updates both the reward model and the generator. The reward model is supervised using reference images as positive samples and can largely avoid being hacked. Unlike KL regularization that constrains parameter updates, our learned reward directly guides the generator through its visual outputs, leading to higher-quality images. Moreover, while optimizing existing reward functions can alleviate reward hacking, their inherent biases remain. For instance, PickScore may degrade image quality, whereas OCR-based rewards often reduce aesthetic fidelity. To address this, we take the image itself as a reward, using reference images and vision foundation models (e.g., DINO) to provide rich visual rewards. These dense visual signals, instead of a single scalar, lead to consistent gains across image quality, aesthetics, and task-specific metrics. Finally, we show that combining reference samples with foundation-model rewards enables distribution transfer and flexible style customization. In human evaluation, our method outperforms Flow-GRPO and SD3, achieving 70.0% and 72.4% win rates in image quality and aesthetics, respectively. Code and models have been released.",
    "translation": "标题：图像即自身奖赏：基于对抗性奖励的图像生成强化学习\n\n摘要：可靠的奖励函数对于图像生成领域的强化学习至关重要。当前大多数强化学习方法依赖于输出标量奖励的预训练偏好模型来近似人类偏好。然而，这些奖励往往难以准确捕捉人类感知，且容易遭受奖励破解——即更高的分数并不对应更好的图像质量。为此，我们提出Adv-GRPO框架，该框架通过对抗性奖励机制同步更新奖励模型与生成器。奖励模型以参考图像作为正样本进行监督训练，可有效规避奖励破解问题。与通过KL正则化约束参数更新的方式不同，我们学习的奖励函数直接通过视觉输出指导生成器，从而产生更高质量的图像。现有奖励函数优化虽能缓解奖励破解，但其固有偏差依然存在：例如PickScore可能降低图像质量，而基于OCR的奖励常损害美学保真度。针对此问题，我们将图像本身作为奖励，利用参考图像与视觉基础模型（如DINO）提供丰富的视觉奖励。这些密集的视觉信号（而非单一标量）在图像质量、美学评价和任务特定指标上均带来持续提升。最后，我们证明结合参考样本与基础模型奖励可实现分布迁移和灵活的风格定制。在人工评估中，本方法在图像质量和美学维度分别以70.0%和72.4%的胜率超越Flow-GRPO与SD3。代码与模型均已开源。",
    "url": "https://huggingface.co/papers/2511.20256",
    "arxiv_url": "https://arxiv.org/abs/2511.20256"
  },
  {
    "title": "Controllable Layer Decomposition for Reversible Multi-Layer Image Generation",
    "summary": "This work presents Controllable Layer Decomposition (CLD), a method for achieving fine-grained and controllable multi-layer separation of raster images. In practical workflows, designers typically generate and edit each RGBA layer independently before compositing them into a final raster image. However, this process is irreversible: once composited, layer-level editing is no longer possible. Existing methods commonly rely on image matting and inpainting, but remain limited in controllability and segmentation precision. To address these challenges, we propose two key modules: LayerDecompose-DiT (LD-DiT), which decouples image elements into distinct layers and enables fine-grained control; and Multi-Layer Conditional Adapter (MLCA), which injects target image information into multi-layer tokens to achieve precise conditional generation. To enable a comprehensive evaluation, we build a new benchmark and introduce tailored evaluation metrics. Experimental results show that CLD consistently outperforms existing methods in both decomposition quality and controllability. Furthermore, the separated layers produced by CLD can be directly manipulated in commonly used design tools such as PowerPoint, highlighting its practical value and applicability in real-world creative workflows.",
    "translation": "标题：可控图层分解在可逆多层图像生成中的应用\n\n摘要：本研究提出可控图层分解方法（CLD），实现栅格图像精细化可控的多层分离。在实际工作流程中，设计人员通常先独立生成并编辑各个RGBA图层，随后将其合成为最终栅格图像。然而这一过程具有不可逆性：图层一经合成，便无法再进行层级编辑。现有方法普遍依赖图像抠图与修复技术，但在可控性与分割精度方面仍存在局限。为解决这些挑战，我们提出两个核心模块：图层分解扩散变换器（LD-DiT）通过解耦图像元素至独立图层实现精细化控制；多层条件适配器（MLCA）通过将目标图像信息注入多层令牌实现精准条件生成。为进行全面评估，我们构建了新的测试基准并制定了专项评估指标。实验结果表明，CLD在分解质量与可控性方面均优于现有方法。此外，CLD生成的分离图层可直接在PowerPoint等常用设计工具中进行编辑，彰显了该方法在实际创作流程中的实用价值与应用潜力。",
    "url": "https://huggingface.co/papers/2511.16249",
    "arxiv_url": "https://arxiv.org/abs/2511.16249"
  },
  {
    "title": "MASS: Motion-Aware Spatial-Temporal Grounding for Physics Reasoning and Comprehension in Vision-Language Models",
    "summary": "Vision Language Models (VLMs) perform well on standard video tasks but struggle with physics-driven reasoning involving motion dynamics and spatial interactions. This limitation reduces their ability to interpret real or AI-generated content (AIGC) videos and to generate physically consistent content. We present an approach that addresses this gap by translating physical-world context cues into interpretable representations aligned with VLMs' perception, comprehension, and reasoning. We introduce MASS-Bench, a comprehensive benchmark consisting of 4,350 real-world and AIGC videos and 8,361 free-form video question-answering pairs focused on physics-related comprehension tasks, with detailed annotations including visual detections, sub-segment grounding, and full-sequence 3D motion tracking of entities. We further present MASS, a model-agnostic method that injects spatial-temporal signals into the VLM language space via depth-based 3D encoding and visual grounding, coupled with a motion tracker for object dynamics. To strengthen cross-modal alignment and reasoning, we apply reinforcement fine-tuning. Experiments and ablations show that our refined VLMs outperform comparable and larger baselines, as well as prior state-of-the-art models, by 8.7% and 6.0%, achieving performance comparable to close-source SoTA VLMs such as Gemini-2.5-Flash on physics reasoning and comprehension. These results validate the effectiveness of our approach.",
    "translation": "标题：MASS：面向视觉语言模型物理推理与理解的运动感知时空定位方法\n\n摘要：视觉语言模型在标准视频任务中表现良好，但在涉及运动动力学与空间交互的物理驱动推理方面存在明显局限，这削弱了其解析真实视频/AI生成内容以及生成物理一致性内容的能力。我们提出一种创新方法，通过将物理世界上下文线索转化为符合VLM感知、理解与推理机制的可解释表征来解决这一缺陷。本文构建了MASS-Bench综合基准数据集，包含4,350个真实世界与AIGC视频及8,361个自由形式视频问答对，聚焦物理相关理解任务，并提供包含视觉检测、子片段定位和全序列三维运动追踪的精细化标注。我们进一步提出MASS——一种模型无关的方法，通过基于深度的三维编码与视觉定位将时空信号注入VLM语言空间，并结合用于物体动态分析的运动追踪器。为增强跨模态对齐与推理能力，我们采用强化微调策略。实验与消融研究表明，优化后的VLM在物理推理与理解任务上分别以8.7%和6.0%的优势超越同类及更大规模基线模型，以及现有先进方法，达到与Gemini-2.5-Flash等闭源顶尖VLS相媲美的性能。这些结果充分验证了我们方法的有效性。",
    "url": "https://huggingface.co/papers/2511.18373",
    "arxiv_url": "https://arxiv.org/abs/2511.18373"
  },
  {
    "title": "Upsample Anything: A Simple and Hard to Beat Baseline for Feature Upsampling",
    "summary": "We present Upsample Anything, a lightweight test-time optimization (TTO) framework that restores low-resolution features to high-resolution, pixel-wise outputs without any training. Although Vision Foundation Models demonstrate strong generalization across diverse downstream tasks, their representations are typically downsampled by 14x/16x (e.g., ViT), which limits their direct use in pixel-level applications. Existing feature upsampling approaches depend on dataset-specific retraining or heavy implicit optimization, restricting scalability and generalization. Upsample Anything addresses these issues through a simple per-image optimization that learns an anisotropic Gaussian kernel combining spatial and range cues, effectively bridging Gaussian Splatting and Joint Bilateral Upsampling. The learned kernel acts as a universal, edge-aware operator that transfers seamlessly across architectures and modalities, enabling precise high-resolution reconstruction of features, depth, or probability maps. It runs in only approx0.419 s per 224x224 image and achieves state-of-the-art performance on semantic segmentation, depth estimation, and both depth and probability map upsampling. Project page: https://seominseok0429.github.io/Upsample-Anything/{https://seominseok0429.github.io/Upsample-Anything/}",
    "translation": "标题：上采样任意内容：一种简单且难以超越的特征上采样基线方法\n\n摘要：本文提出\"上采样任意内容\"——一种轻量级测试时优化框架，无需任何训练即可将低分辨率特征恢复为高分辨率像素级输出。尽管视觉基础模型在多样化下游任务中展现出强大的泛化能力，但其表征通常会被下采样14倍/16倍（如ViT），这限制了它们在像素级应用中的直接使用。现有特征上采样方法依赖于数据集特定的重新训练或繁重的隐式优化，制约了可扩展性和泛化能力。本方法通过简单的单图像优化学习结合空间与范围信息的各向异性高斯核，有效衔接高斯泼溅与联合双边上采样技术。学习得到的内核可作为通用、边缘感知的算子，在不同架构与模态间无缝迁移，实现特征图、深度图及概率图的精确高分辨率重建。该方法处理224×224图像仅需约0.419秒，在语义分割、深度估计以及深度图与概率图上采样任务中均达到最先进性能。项目页面：https://seominseok0429.github.io/Upsample-Anything/{https://seominseok0429.github.io/Upsample-Anything/}",
    "url": "https://huggingface.co/papers/2511.16301",
    "arxiv_url": "https://arxiv.org/abs/2511.16301"
  },
  {
    "title": "PRInTS: Reward Modeling for Long-Horizon Information Seeking",
    "summary": "Information-seeking is a core capability for AI agents, requiring them to gather and reason over tool-generated information across long trajectories. However, such multi-step information-seeking tasks remain challenging for agents backed by language models. While process reward models (PRMs) can guide agents by ranking candidate steps at test-time, existing PRMs, designed for short reasoning with binary judgment, cannot capture richer dimensions of information-seeking steps, such as tool interactions and reasoning over tool outputs, nor handle the rapidly growing context in long-horizon tasks. To address these limitations, we introduce PRInTS, a generative PRM trained with dual capabilities: (1) dense scoring based on the PRM's reasoning across multiple step quality dimensions (e.g., interpretation of tool outputs, tool call informativeness) and (2) trajectory summarization that compresses the growing context while preserving essential information for step evaluation. Extensive evaluations across FRAMES, GAIA (levels 1-3), and WebWalkerQA (easy-hard) benchmarks on multiple models, along with ablations, reveal that best-of-n sampling with PRInTS enhances information-seeking abilities of open-source models as well as specialized agents, matching or surpassing the performance of frontier models with a much smaller backbone agent and outperforming other strong reward modeling baselines.",
    "translation": "标题：PRInTS：面向长视野信息搜索的奖励建模方法\n\n摘要：信息搜索是智能代理的核心能力，要求其在长轨迹任务中持续收集工具生成的信息并进行推理。然而，这种多步骤信息搜索任务对基于语言模型的智能代理仍具挑战性。虽然过程奖励模型（PRM）可通过在测试时对候选步骤排序来指导代理，但现有PRM专为二元判断的短程推理设计，既无法捕捉信息搜索步骤中工具交互、输出解析等多维特征，也难以处理长视野任务中快速增长的上下文。为此，我们提出PRInTS——一种具备双重能力的生成式过程奖励模型：（1）基于多维度步骤质量评估（如工具输出解析、工具调用信息量）的密集评分；（2）轨迹摘要技术，在压缩增长上下文的同时保留步骤评估的关键信息。通过在FRAMES、GAIA（1-3级）和WebWalkerQA（易-难级）基准上对多个模型开展的广泛评估及消融实验表明：采用PRInTS的n选1采样策略能显著增强开源模型与专业代理的信息搜索能力，在使用更轻量主干代理的情况下达到或超越前沿模型性能，并优于其他强奖励建模基线方法。",
    "url": "https://huggingface.co/papers/2511.19314",
    "arxiv_url": "https://arxiv.org/abs/2511.19314"
  },
  {
    "title": "AICC: Parse HTML Finer, Make Models Better -- A 7.3T AI-Ready Corpus Built by a Model-Based HTML Parser",
    "summary": "While web data quality is crucial for large language models, most curation efforts focus on filtering and deduplication,treating HTML-to-text extraction as a fixed pre-processing step. Existing web corpora rely on heuristic-based extractors like Trafilatura, which struggle to preserve document structure and frequently corrupt structured elements such as formulas, codes, and tables. We hypothesize that improving extraction quality can be as impactful as aggressive filtering strategies for downstream performance. We introduce MinerU-HTML, a novel extraction pipeline that reformulates content extraction as a sequence labeling problem solved by a 0.6B-parameter language model. Unlike text-density heuristics, MinerU-HTML leverages semantic understanding and employs a two-stage formatting pipeline that explicitly categorizes semantic elements before converting to Markdown. Crucially, its model-based approach is inherently scalable, whereas heuristic methods offer limited improvement pathways. On MainWebBench, our benchmark of 7,887 annotated web pages, MinerU-HTML achieves 81.8\\% ROUGE-N F1 compared to Trafilatura's 63.6\\%, with exceptional structured element preservation (90.9\\% for code blocks, 94.0\\% for formulas). Using MinerU-HTML, we construct AICC (AI-ready Common Crawl), a 7.3-trillion token multilingual corpus from two Common Crawl snapshots. In controlled pretraining experiments where AICC and Trafilatura-extracted TfCC undergo identical filtering, models trained on AICC (62B tokens) achieve 50.8\\% average accuracy across 13 benchmarks, outperforming TfCC by 1.08pp-providing direct evidence that extraction quality significantly impacts model capabilities. AICC also surpasses RefinedWeb and FineWeb on key benchmarks. We publicly release MainWebBench, MinerU-HTML, and AICC, demonstrating that HTML extraction is a critical, often underestimated component of web corpus construction.",
    "translation": "标题：AICC：更精细解析HTML，打造更优模型——基于模型的HTML解析器构建的7.3T AI就绪语料库\n\n摘要：尽管网络数据质量对大型语言模型至关重要，但现有数据策展工作多聚焦于过滤与去重，将HTML到文本的提取视为固定预处理步骤。当前主流网络语料库采用基于启发式规则的提取器（如Trafilatura），这类方法难以保持文档结构完整性，且经常破坏公式、代码、表格等结构化元素。我们提出假设：提升提取质量对下游任务性能的影响不亚于激进过滤策略。本文介绍MinerU-HTML——一种将内容提取重构为序列标注问题的新型提取流程，通过60亿参数语言模型实现解析。与基于文本密度的启发式方法不同，MinerU-HTML利用语义理解能力，采用两阶段格式化流程，在转换为Markdown前显式分类语义元素。关键优势在于其基于模型的方法具备内在扩展性，而启发式方法的改进路径有限。在包含7,887个标注网页的基准测试集MainWebBench上，MinerU-HTML的ROUGE-N F1值达到81.8%，显著优于Trafilatura的63.6%，并在结构化元素保留方面表现卓越（代码块90.9%，公式94.0%）。基于该技术，我们构建了AICC（AI就绪通用爬虫语料库）——从两个Common Crawl快照中提取的多语言语料库，规模达7.3万亿词元。在严格控制预训练实验中，对AICC与Trafilatura提取的TfCC实施相同过滤后，基于AICC（620亿词元）训练的模型在13个基准测试中平均准确率达50.8%，较TfCC提升1.08个百分点，这为提取质量显著影响模型能力提供了直接证据。AICC在关键基准测试中也优于RefinedWeb和FineWeb。我们公开发布MainWebBench、MinerU-HTML和AICC，证明HTML提取是网络语料库构建中至关重要却常被低估的环节。",
    "url": "https://huggingface.co/papers/2511.16397",
    "arxiv_url": "https://arxiv.org/abs/2511.16397"
  },
  {
    "title": "EvoVLA: Self-Evolving Vision-Language-Action Model",
    "summary": "Long-horizon robotic manipulation remains challenging for Vision-Language-Action (VLA) models despite recent progress in zero-shot generalization and simulation-to-real-world transfer. Current VLA models suffer from stage hallucination, where agents exploit coarse evaluation signals to shortcut multi-step tasks, reporting high progress without truly completing them. We present EvoVLA, a self-supervised VLA framework that addresses this issue through three complementary components: Stage-Aligned Reward (SAR), which uses triplet contrastive learning with Gemini-generated hard negatives to prevent visual shortcuts; Pose-Based Object Exploration (POE), which grounds curiosity in relative object-gripper pose instead of raw pixels; and Long-Horizon Memory, which uses selective context retention and gated fusion to stabilize intrinsic shaping during extended rollouts. Extensive evaluations on Discoverse-L, a long-horizon manipulation benchmark with three multi-stage tasks, show that EvoVLA improves average task success by 10.2 percentage points over the strongest baseline (OpenVLA-OFT), reaching 69.2 percent. EvoVLA also achieves one-and-a-half times better sample efficiency and reduces stage hallucination from 38.5 percent to 14.8 percent. Real-world deployment on physical robots reaches an average success rate of 54.6 percent across four manipulation tasks, outperforming OpenVLA-OFT by 11 points, demonstrating effective sim-to-real transfer and strong generalization. Code: https://github.com/AIGeeksGroup/EvoVLA. Website: https://aigeeksgroup.github.io/EvoVLA.",
    "translation": "标题：EvoVLA：自演进视觉-语言-动作模型\n\n摘要：尽管视觉-语言-动作模型在零样本泛化和仿真到现实迁移方面取得进展，长周期机器人操作仍是其面临的重要挑战。现有VLA模型存在阶段幻觉问题——智能体利用粗糙评估信号取巧多步任务，虽报告高进度却未真正完成操作。我们提出EvoVLA，一种自监督VLA框架，通过三个互补组件解决该问题：采用三元组对比学习与Gemini生成难负样本的阶段对齐奖励机制，可防止视觉捷径；基于相对物体-夹具位姿而非原始像素的位姿引导探索策略，实现具象化好奇心；结合选择性上下文保持与门控融合的长周期记忆模块，稳定长程推演中的内在塑形。在包含三项多阶段任务的Discoverse-L长周期操作基准测试中，EvoVLA以69.2%的平均任务成功率超越最强基线（OpenVLA-OFT）10.2个百分点，同时实现1.5倍的样本效率提升，并将阶段幻觉率从38.5%降至14.8%。在实体机器人上的现实部署中，四项操作任务平均成功率达54.6%，较OpenVLA-OFT提升11个百分点，证明了有效的仿真到现实迁移能力与强大泛化性能。代码：https://github.com/AIGeeksGroup/EvoVLA 项目网站：https://aigeeksgroup.github.io/EvoVLA",
    "url": "https://huggingface.co/papers/2511.16166",
    "arxiv_url": "https://arxiv.org/abs/2511.16166"
  },
  {
    "title": "Flow Map Distillation Without Data",
    "summary": "State-of-the-art flow models achieve remarkable quality but require slow, iterative sampling. To accelerate this, flow maps can be distilled from pre-trained teachers, a procedure that conventionally requires sampling from an external dataset. We argue that this data-dependency introduces a fundamental risk of Teacher-Data Mismatch, as a static dataset may provide an incomplete or even misaligned representation of the teacher's full generative capabilities. This leads us to question whether this reliance on data is truly necessary for successful flow map distillation. In this work, we explore a data-free alternative that samples only from the prior distribution, a distribution the teacher is guaranteed to follow by construction, thereby circumventing the mismatch risk entirely. To demonstrate the practical viability of this philosophy, we introduce a principled framework that learns to predict the teacher's sampling path while actively correcting for its own compounding errors to ensure high fidelity. Our approach surpasses all data-based counterparts and establishes a new state-of-the-art by a significant margin. Specifically, distilling from SiT-XL/2+REPA, our method reaches an impressive FID of 1.45 on ImageNet 256x256, and 1.49 on ImageNet 512x512, both with only 1 sampling step. We hope our work establishes a more robust paradigm for accelerating generative models and motivates the broader adoption of flow map distillation without data.",
    "translation": "标题：无需数据的流映射蒸馏方法\n\n摘要：当前最先进的流模型虽能实现卓越生成质量，却依赖缓慢的迭代采样过程。为加速采样，现有方法通常从预训练教师模型中蒸馏流映射，这一过程传统上需要依赖外部数据集进行采样。我们认为这种数据依赖性会引发教师模型与数据失配的根本性风险——静态数据集可能无法完整呈现教师模型全部生成能力，甚至产生表征偏差。这促使我们反思数据依赖是否真是流映射蒸馏成功的必要条件。本研究探索了一种无需外部数据的替代方案：仅从先验分布中进行采样，该分布通过结构设计确保与教师模型完全兼容，从而彻底规避失配风险。为验证这一理念的可行性，我们提出了具有理论依据的框架，该框架既能预测教师模型的采样路径，又能主动修正自身误差累积，确保生成的高保真度。我们的方法超越了所有基于数据的对比方案，以显著优势确立了新的技术标杆。具体而言，基于SiT-XL/2+REPA模型的蒸馏实验显示，本方法在ImageNet 256×256数据集上获得1.45的FID指标，在ImageNet 512×512数据集上达到1.49的FID指标，且均仅需单步采样。我们期待本研究能为生成模型加速建立更稳健的范式，推动无需数据的流映射蒸馏技术获得更广泛采纳。",
    "url": "https://huggingface.co/papers/2511.19428",
    "arxiv_url": "https://arxiv.org/abs/2511.19428"
  },
  {
    "title": "One4D: Unified 4D Generation and Reconstruction via Decoupled LoRA Control",
    "summary": "We present One4D, a unified framework for 4D generation and reconstruction that produces dynamic 4D content as synchronized RGB frames and pointmaps. By consistently handling varying sparsities of conditioning frames through a Unified Masked Conditioning (UMC) mechanism, One4D can seamlessly transition between 4D generation from a single image, 4D reconstruction from a full video, and mixed generation and reconstruction from sparse frames. Our framework adapts a powerful video generation model for joint RGB and pointmap generation, with carefully designed network architectures. The commonly used diffusion finetuning strategies for depthmap or pointmap reconstruction often fail on joint RGB and pointmap generation, quickly degrading the base video model. To address this challenge, we introduce Decoupled LoRA Control (DLC), which employs two modality-specific LoRA adapters to form decoupled computation branches for RGB frames and pointmaps, connected by lightweight, zero-initialized control links that gradually learn mutual pixel-level consistency. Trained on a mixture of synthetic and real 4D datasets under modest computational budgets, One4D produces high-quality RGB frames and accurate pointmaps across both generation and reconstruction tasks. This work represents a step toward general, high-quality geometry-based 4D world modeling using video diffusion models. Project page: https://mizhenxing.github.io/One4D",
    "translation": "标题：One4D：基于解耦LoRA控制的统一四维生成与重建框架\n\n摘要：本文提出One4D——一个统一的四维生成与重建框架，能够生成以同步RGB帧和点云图形式呈现的动态四维内容。通过统一掩码条件（UMC）机制对输入帧的不同稀疏度进行一致性处理，该框架可实现从单图像的四维生成、完整视频的四维重建到稀疏帧混合生成与重建的无缝切换。我们基于强大的视频生成模型设计了专用网络架构，使其适配联合RGB与点云图的生成任务。传统基于扩散模型的深度图或点云图重建微调策略在联合RGB与点云图生成任务中往往失效，会导致基础视频模型性能迅速退化。为解决此问题，我们提出解耦LoRA控制（DLC）方法，通过两个模态特定的LoRA适配器构建RGB帧与点云图的解耦计算分支，并采用轻量级零初始化控制链接逐步学习像素级一致性。在适度计算预算下，通过合成与真实四维数据集的混合训练，One4D在生成与重建任务中均能产出高质量RGB帧与精确点云图。本工作标志着基于视频扩散模型实现通用高质量几何四维世界建模的重要进展。项目页面：https://mizhenxing.github.io/One4D",
    "url": "https://huggingface.co/papers/2511.18922",
    "arxiv_url": "https://arxiv.org/abs/2511.18922"
  },
  {
    "title": "Target-Bench: Can World Models Achieve Mapless Path Planning with Semantic Targets?",
    "summary": "While recent world models generate highly realistic videos, their ability to perform robot path planning remains unclear and unquantified. We introduce Target-Bench, the first benchmark specifically designed to evaluate world models on mapless path planning toward semantic targets in real-world environments. Target-Bench provides 450 robot-collected video sequences spanning 45 semantic categories with SLAM-based ground truth trajectories. Our evaluation pipeline recovers camera motion from generated videos and measures planning performance using five complementary metrics that quantify target-reaching capability, trajectory accuracy, and directional consistency. We evaluate state-of-the-art models including Sora 2, Veo 3.1, and the Wan series. The best off-the-shelf model (Wan2.2-Flash) achieves only 0.299 overall score, revealing significant limitations in current world models for robotic planning tasks. We show that fine-tuning an open-source 5B-parameter model on only 325 scenarios from our dataset achieves 0.345 overall score -- an improvement of more than 400% over its base version (0.066) and 15% higher than the best off-the-shelf model. We will open-source the code and dataset.",
    "translation": "标题：Target-Bench：世界模型能否实现基于语义目标的无地图路径规划？\n\n摘要：尽管当前世界模型能生成高度逼真的视频，但其执行机器人路径规划的能力仍不明确且缺乏量化评估。我们提出Target-Bench——首个专门用于评估世界模型在真实环境中基于语义目标进行无地图路径规划能力的基准测试平台。该平台提供450段机器人采集的视频序列，涵盖45个语义类别，并配备基于SLAM技术的真实轨迹数据。我们的评估流程通过生成视频还原相机运动，并采用五项互补指标量化规划性能，包括目标抵达能力、轨迹精度和方向一致性。我们对包括Sora 2、Veo 3.1及Wan系列在内的前沿模型进行评估，表现最佳的现成模型（Wan2.2-Flash）总体得分仅为0.299，揭示了当前世界模型在机器人规划任务中的显著局限性。实验表明，仅需使用本数据集中的325个场景对开源50亿参数模型进行微调，即可获得0.345的总体评分——较其基础版本（0.066）提升超400%，并优于最佳现成模型15%。我们将开源相关代码与数据集。",
    "url": "https://huggingface.co/papers/2511.17792",
    "arxiv_url": "https://arxiv.org/abs/2511.17792"
  },
  {
    "title": "Representational Stability of Truth in Large Language Models",
    "summary": "Large language models (LLMs) are widely used for factual tasks such as \"What treats asthma?\" or \"What is the capital of Latvia?\". However, it remains unclear how stably LLMs encode distinctions between true, false, and neither-true-nor-false content in their internal probabilistic representations. We introduce representational stability as the robustness of an LLM's veracity representations to perturbations in the operational definition of truth. We assess representational stability by (i) training a linear probe on an LLM's activations to separate true from not-true statements and (ii) measuring how its learned decision boundary shifts under controlled label changes. Using activations from sixteen open-source models and three factual domains, we compare two types of neither statements. The first are fact-like assertions about entities we believe to be absent from any training data. We call these unfamiliar neither statements. The second are nonfactual claims drawn from well-known fictional contexts. We call these familiar neither statements. The unfamiliar statements induce the largest boundary shifts, producing up to 40% flipped truth judgements in fragile domains (such as word definitions), while familiar fictional statements remain more coherently clustered and yield smaller changes (leq 8.2%). These results suggest that representational stability stems more from epistemic familiarity than from linguistic form. More broadly, our approach provides a diagnostic for auditing and training LLMs to preserve coherent truth assignments under semantic uncertainty, rather than optimizing for output accuracy alone.",
    "translation": "标题：大语言模型中真值表征的稳定性研究\n\n摘要：大语言模型已广泛应用于事实性任务，例如\"哮喘的治疗方法有哪些？\"或\"拉脱维亚首都是哪里？\"。然而，目前尚不清楚大语言模型如何在其内部概率表征中稳定地区分真实、虚假以及非真非假内容。本文提出表征稳定性的概念，用于衡量大语言模型对真值操作定义扰动的表征鲁棒性。我们通过以下方法评估表征稳定性：（1）基于大语言模型的激活值训练线性探测器以区分真实与非真实陈述；（2）在受控标签变化下测量其学习决策边界的偏移程度。通过分析十六个开源模型在三个事实领域的激活数据，我们比较了两种非真非假陈述：第一种是关于我们确信未出现在任何训练数据中的实体的事实型断言，称为陌生型中立陈述；第二种是来自知名虚构语境的非事实主张，称为熟悉型中立陈述。研究显示，陌生型陈述引发最大的边界偏移，在脆弱领域（如词汇定义）导致高达40%的真值判断反转，而熟悉的虚构陈述则保持更连贯的聚类特征，仅产生较小变化（≤8.2%）。这些结果表明，表征稳定性更多源于认知熟悉度而非语言形式。从更广义角度看，本研究方法为审计和训练大语言模型提供了诊断工具，使其在语义不确定性下保持连贯的真值分配，而非仅优化输出准确性。",
    "url": "https://huggingface.co/papers/2511.19166",
    "arxiv_url": "https://arxiv.org/abs/2511.19166"
  },
  {
    "title": "SyncMV4D: Synchronized Multi-view Joint Diffusion of Appearance and Motion for Hand-Object Interaction Synthesis",
    "summary": "Hand-Object Interaction (HOI) generation plays a critical role in advancing applications across animation and robotics. Current video-based methods are predominantly single-view, which impedes comprehensive 3D geometry perception and often results in geometric distortions or unrealistic motion patterns. While 3D HOI approaches can generate dynamically plausible motions, their dependence on high-quality 3D data captured in controlled laboratory settings severely limits their generalization to real-world scenarios. To overcome these limitations, we introduce SyncMV4D, the first model that jointly generates synchronized multi-view HOI videos and 4D motions by unifying visual prior, motion dynamics, and multi-view geometry. Our framework features two core innovations: (1) a Multi-view Joint Diffusion (MJD) model that co-generates HOI videos and intermediate motions, and (2) a Diffusion Points Aligner (DPA) that refines the coarse intermediate motion into globally aligned 4D metric point tracks. To tightly couple 2D appearance with 4D dynamics, we establish a closed-loop, mutually enhancing cycle. During the diffusion denoising process, the generated video conditions the refinement of the 4D motion, while the aligned 4D point tracks are reprojected to guide next-step joint generation. Experimentally, our method demonstrates superior performance to state-of-the-art alternatives in visual realism, motion plausibility, and multi-view consistency.",
    "translation": "标题：SyncMV4D：面向手物交互合成的外观与运动同步多视角联合扩散模型\n\n摘要：手物交互生成在推动动画与机器人应用发展中具有关键作用。当前基于视频的方法主要局限于单视角，这阻碍了全面的三维几何感知，并常导致几何失真或非真实运动模式。虽然三维手物交互方法能生成动力学合理的运动，但其对受控实验室环境下采集的高质量三维数据的依赖性，严重限制了其在真实场景中的泛化能力。为突破这些局限，我们提出SyncMV4D——首个通过统一视觉先验、运动动力学与多视角几何，联合生成同步多视角手物交互视频与四维运动的模型。我们的框架具有两大核心创新：(1) 多视角联合扩散模型，可协同生成手物交互视频与中间运动序列；(2) 扩散点云对齐器，能将粗粒度中间运动优化为全局对齐的四维度量点轨迹。为实现二维外观与四维动力学的紧密耦合，我们建立了闭环式相互增强循环：在扩散去噪过程中，生成视频为四维运动优化提供条件约束，而对齐后的四维点轨迹通过重投影指导下一步联合生成。实验表明，本方法在视觉真实感、运动合理性与多视角一致性方面均优于当前最先进方案。",
    "url": "https://huggingface.co/papers/2511.19319",
    "arxiv_url": "https://arxiv.org/abs/2511.19319"
  },
  {
    "title": "Fidelity-Aware Recommendation Explanations via Stochastic Path Integration",
    "summary": "Explanation fidelity, which measures how accurately an explanation reflects a model's true reasoning, remains critically underexplored in recommender systems. We introduce SPINRec (Stochastic Path Integration for Neural Recommender Explanations), a model-agnostic approach that adapts path-integration techniques to the sparse and implicit nature of recommendation data. To overcome the limitations of prior methods, SPINRec employs stochastic baseline sampling: instead of integrating from a fixed or unrealistic baseline, it samples multiple plausible user profiles from the empirical data distribution and selects the most faithful attribution path. This design captures the influence of both observed and unobserved interactions, yielding more stable and personalized explanations. We conduct the most comprehensive fidelity evaluation to date across three models (MF, VAE, NCF), three datasets (ML1M, Yahoo! Music, Pinterest), and a suite of counterfactual metrics, including AUC-based perturbation curves and fixed-length diagnostics. SPINRec consistently outperforms all baselines, establishing a new benchmark for faithful explainability in recommendation. Code and evaluation tools are publicly available at https://github.com/DeltaLabTLV/SPINRec.",
    "translation": "标题：基于随机路径积分的保真推荐解释方法\n\n摘要：解释保真度作为衡量解释准确反映模型真实推理过程的指标，在推荐系统领域仍存在严重的研究不足。我们提出SPINRec（基于随机路径积分的神经推荐解释方法），这是一种与模型无关的创新方案，将路径积分技术适配于推荐数据隐含稀疏的特性。为突破现有方法的局限，SPINRec采用随机基线采样策略：通过从经验数据分布中抽取多个可能的用户画像并选择最可信的归因路径，取代传统固定基线或不切实际的参照点。该设计能同时捕捉观测与非观测交互的影响，生成更稳定且个性化的解释。我们在三个模型（矩阵分解、变分自编码器、神经协同过滤）、三个数据集（MovieLens 1M、雅虎音乐、Pinterest）及包含基于AUC的扰动曲线和定长诊断的反事实评估体系上开展了迄今最全面的保真度评估。实验表明SPINRec持续超越所有基线方法，为推荐系统的可信解释建立了新基准。代码与评估工具已开源：https://github.com/DeltaLabTLV/SPINRec。",
    "url": "https://huggingface.co/papers/2511.18047",
    "arxiv_url": "https://arxiv.org/abs/2511.18047"
  },
  {
    "title": "Extracting Interaction-Aware Monosemantic Concepts in Recommender Systems",
    "summary": "We present a method for extracting monosemantic neurons, defined as latent dimensions that align with coherent and interpretable concepts, from user and item embeddings in recommender systems. Our approach employs a Sparse Autoencoder (SAE) to reveal semantic structure within pretrained representations. In contrast to work on language models, monosemanticity in recommendation must preserve the interactions between separate user and item embeddings. To achieve this, we introduce a prediction aware training objective that backpropagates through a frozen recommender and aligns the learned latent structure with the model's user-item affinity predictions. The resulting neurons capture properties such as genre, popularity, and temporal trends, and support post hoc control operations including targeted filtering and content promotion without modifying the base model. Our method generalizes across different recommendation models and datasets, providing a practical tool for interpretable and controllable personalization. Code and evaluation resources are available at https://github.com/DeltaLabTLV/Monosemanticity4Rec.",
    "translation": "标题：推荐系统中交互感知的单语义概念提取方法研究\n\n摘要：本文提出一种从推荐系统的用户与物品嵌入向量中提取单语义神经元的方法，该神经元被定义为与连贯可解释概念相对应的潜在维度。我们采用稀疏自编码器（SAE）来揭示预训练表征中的语义结构。与语言模型研究不同，推荐系统中的单语义性必须保持独立用户嵌入与物品嵌入间的交互关系。为此，我们引入了预测感知训练目标，该方法通过冻结推荐模型进行反向传播，并将学习到的潜在结构与模型的用户-物品亲和度预测相校准。最终获得的神经元能够捕捉类型特征、流行度指标及时间趋势等属性，支持包括定向过滤和内容推广在内的后置控制操作，且无需修改基础模型。本方法在不同推荐模型与数据集上均表现出良好泛化能力，为可解释与可控的个性化推荐提供了实用工具。代码与评估资源详见：https://github.com/DeltaLabTLV/Monosemanticity4Rec。",
    "url": "https://huggingface.co/papers/2511.18024",
    "arxiv_url": "https://arxiv.org/abs/2511.18024"
  },
  {
    "title": "MSRNet: A Multi-Scale Recursive Network for Camouflaged Object Detection",
    "summary": "Camouflaged object detection is an emerging and challenging computer vision task that requires identifying and segmenting objects that blend seamlessly into their environments due to high similarity in color, texture, and size. This task is further complicated by low-light conditions, partial occlusion, small object size, intricate background patterns, and multiple objects. While many sophisticated methods have been proposed for this task, current methods still struggle to precisely detect camouflaged objects in complex scenarios, especially with small and multiple objects, indicating room for improvement. We propose a Multi-Scale Recursive Network that extracts multi-scale features via a Pyramid Vision Transformer backbone and combines them via specialized Attention-Based Scale Integration Units, enabling selective feature merging. For more precise object detection, our decoder recursively refines features by incorporating Multi-Granularity Fusion Units. A novel recursive-feedback decoding strategy is developed to enhance global context understanding, helping the model overcome the challenges in this task. By jointly leveraging multi-scale learning and recursive feature optimization, our proposed method achieves performance gains, successfully detecting small and multiple camouflaged objects. Our model achieves state-of-the-art results on two benchmark datasets for camouflaged object detection and ranks second on the remaining two. Our codes, model weights, and results are available at https://github.com/linaagh98/MSRNet{https://github.com/linaagh98/MSRNet}.",
    "translation": "标题：MSRNet：用于伪装目标检测的多尺度递归网络\n\n摘要：伪装目标检测是一项新兴且具有挑战性的计算机视觉任务，其目标在于识别并分割那些因颜色、纹理及尺寸高度相似而与环境融为一体的目标。该任务在弱光条件、部分遮挡、目标尺寸微小、背景图案复杂及多目标共存等场景下面临更大挑战。尽管已有诸多精密方法被提出，现有技术仍难以在复杂场景中精准检测伪装目标，特别是在处理微小目标及多目标情况时表现不佳，这表明该领域仍有提升空间。我们提出了一种多尺度递归网络，通过金字塔视觉变换器主干网络提取多尺度特征，并借助基于注意力的尺度融合单元实现选择性特征融合。为提升检测精度，解码器通过多粒度融合单元递归优化特征。我们还开发了新型递归反馈解码策略以增强全局上下文理解，帮助模型克服任务中的各类挑战。通过联合利用多尺度学习与递归特征优化，所提方法实现了性能提升，成功检测微小及多重伪装目标。在两个伪装目标检测基准数据集上，我们的模型取得了最先进的性能，并在另外两个数据集中排名第二。代码、模型权重及实验结果已公开于https://github.com/linaagh98/MSRNet。",
    "url": "https://huggingface.co/papers/2511.12810",
    "arxiv_url": "https://arxiv.org/abs/2511.12810"
  }
]