[
  {
    "title": "Diffusion Language Models are Super Data Learners",
    "summary": "Under strictly controlled pre-training settings, we observe a Crossover: when\nunique data is limited, diffusion language models (DLMs) consistently surpass\nautoregressive (AR) models by training for more epochs. The crossover shifts\nlater with more or higher-quality data, earlier with larger models, and\npersists across dense and sparse architectures. We attribute the gains to three\ncompounding factors: (1) any-order modeling, (2) super-dense compute from\niterative bidirectional denoising, and (3) built-in Monte Carlo augmentation;\ninput or parameter noise improves AR under data constraint but cannot close the\ngap. At scale, a 1.7B DLM trained with a ~1.5T-token compute budget on 10B\nunique Python tokens overtakes an AR coder trained with strictly matched\nsettings. In addition, a 1B-parameter DLM achieves > 56% accuracy on HellaSwag\nand > 33% on MMLU using only 1B tokens, without any special tricks, just by\nrepeating standard pre-training data. We also show that rising validation\ncross-entropy does not imply degraded downstream performance in this regime.",
    "translation": "标题：扩散语言模型：卓越的数据学习器\n\n摘要：在严格受控的预训练环境下，我们观察到一种交叉现象：当唯一数据有限时，通过增加训练轮次，扩散语言模型（DLMs）持续超越自回归模型（AR）。这种交叉点会随数据量增加或质量提升而后移，随模型规模扩大而前移，并在稠密与稀疏架构中均稳定存在。我们认为这种优势源于三个复合因素：（1）任意顺序建模能力；（2）迭代式双向去噪带来的超密集计算；（3）内置蒙特卡洛增强机制。虽然输入噪声或参数噪声能在数据受限时提升AR模型表现，但无法弥合性能差距。大规模实验表明，在消耗约1.5T令牌计算预算、使用100亿个唯一Python令牌训练后，17亿参数的DLM在严格匹配的设置下超越了自回归编程模型。此外，仅使用10亿令牌进行标准预训练数据重复训练，10亿参数的DLM便在HellaSwag上取得超过56%的准确率，在MMLU上超过33%，且未采用任何特殊技巧。我们还发现，在此训练机制下，验证集交叉熵的上升并不代表下游任务性能的退化。",
    "url": "https://huggingface.co/papers/2511.03276",
    "arxiv_url": "https://arxiv.org/abs/2511.03276"
  },
  {
    "title": "UniAVGen: Unified Audio and Video Generation with Asymmetric Cross-Modal\n  Interactions",
    "summary": "Due to the lack of effective cross-modal modeling, existing open-source\naudio-video generation methods often exhibit compromised lip synchronization\nand insufficient semantic consistency. To mitigate these drawbacks, we propose\nUniAVGen, a unified framework for joint audio and video generation. UniAVGen is\nanchored in a dual-branch joint synthesis architecture, incorporating two\nparallel Diffusion Transformers (DiTs) to build a cohesive cross-modal latent\nspace. At its heart lies an Asymmetric Cross-Modal Interaction mechanism, which\nenables bidirectional, temporally aligned cross-attention, thus ensuring\nprecise spatiotemporal synchronization and semantic consistency. Furthermore,\nthis cross-modal interaction is augmented by a Face-Aware Modulation module,\nwhich dynamically prioritizes salient regions in the interaction process. To\nenhance generative fidelity during inference, we additionally introduce\nModality-Aware Classifier-Free Guidance, a novel strategy that explicitly\namplifies cross-modal correlation signals. Notably, UniAVGen's robust joint\nsynthesis design enables seamless unification of pivotal audio-video tasks\nwithin a single model, such as joint audio-video generation and continuation,\nvideo-to-audio dubbing, and audio-driven video synthesis. Comprehensive\nexperiments validate that, with far fewer training samples (1.3M vs. 30.1M),\nUniAVGen delivers overall advantages in audio-video synchronization, timbre\nconsistency, and emotion consistency.",
    "translation": "标题：UniAVGen：基于非对称跨模态交互的音频视频联合生成框架\n\n摘要：由于缺乏有效的跨模态建模机制，现有开源音视频生成方法常存在唇部同步偏差与语义一致性不足的问题。为解决这些缺陷，我们提出UniAVGen——一个统一的音视频联合生成框架。该框架采用双分支联合合成架构，通过两个并行的扩散变换器构建协同的跨模态潜在空间。其核心创新在于非对称跨模态交互机制，该机制通过双向时序对齐的跨注意力实现，确保精确的时空同步与语义一致性。此外，我们引入面部感知调制模块，在交互过程中动态增强显著区域的表征权重。为提升推理生成质量，我们进一步提出模态感知的无分类器引导策略，通过显式增强跨模态关联信号优化生成效果。值得注意的是，UniAVGen凭借其鲁棒的联合合成设计，可在单一模型中无缝整合关键音视频任务，包括音视频联合生成与续写、视频到音频的配音转换以及音频驱动视频合成。综合实验表明，在训练样本量显著减少的情况下（130万 vs 3010万），UniAVGen在音视频同步性、音色一致性与情感一致性方面均展现出综合优势。",
    "url": "https://huggingface.co/papers/2511.03334",
    "arxiv_url": "https://arxiv.org/abs/2511.03334"
  },
  {
    "title": "LEGO-Eval: Towards Fine-Grained Evaluation on Synthesizing 3D Embodied\n  Environments with Tool Augmentation",
    "summary": "Despite recent progress in using Large Language Models (LLMs) for\nautomatically generating 3D scenes, generated scenes often lack realistic\nspatial layouts and object attributes found in real-world environments. As this\nproblem stems from insufficiently detailed, coarse-grained instructions,\nadvancing 3D scene synthesis guided by more detailed, fine-grained instructions\nthat reflect real-world environments becomes crucial. Without such realistic\nscenes, training embodied agents in unrealistic environments can lead them to\nlearn priors that diverge significantly from real-world physics and semantics,\ndegrading their performance when deployed. Thus, verifying the alignment\nbetween the fine-grained instruction and the generated scene is essential for\neffective learning. However, current evaluation methods, such as CLIPScore and\nvision-language models (VLMs), often fail to reliably assess such alignment.\nThis shortcoming arises primarily from their shallow understanding of 3D\nscenes, which often leads to improperly grounded scene components. To address\nthis, we introduce LEGO-Eval, an evaluation framework equipped with diverse\ntools designed to explicitly ground scene components, enabling more accurate\nalignment assessments. We also present LEGO-Bench, a benchmark of detailed\ninstructions that specify complex layouts and attributes of real-world\nenvironments. Experiments demonstrate that LEGO-Eval outperforms VLM-as-a-judge\nby 0.41 F1 score in assessing scene-instruction alignment. Benchmarking with\nLEGO-Bench reveals significant limitations in current generation methods.\nAcross all evaluated approaches, success rates reached at most 10% in\ngenerating scenes that fully align with fine-grained instructions.",
    "translation": "标题：LEGO-Eval：基于工具增强的精细化三维具身环境合成评估框架\n\n摘要：尽管利用大语言模型自动生成三维场景已取得进展，但生成场景往往缺乏真实环境中的合理空间布局与物体属性。该问题的根源在于指令描述粒度不足，因此亟需发展基于更贴近真实环境的细粒度指令的三维场景合成方法。若缺乏真实场景支撑，在非真实环境中训练具身智能体将导致其学习到与现实物理规则和语义特征显著偏离的先验知识，从而影响实际部署性能。因此，验证细粒度指令与生成场景之间的对齐关系对有效学习至关重要。然而现有评估方法（如CLIPScore和视觉语言模型）往往难以可靠评估这种对齐关系，这主要源于其对三维场景的理解较为浅层，常导致场景要素的 grounding 结果失准。为此，我们提出LEGO-Eval评估框架，通过配备多样化工具显式实现场景要素的 grounding，从而提供更精准的对齐评估。同时我们构建LEGO-Bench基准数据集，包含描述真实环境复杂布局与属性的精细化指令集。实验表明，LEGO-Eval在场景-指令对齐评估中的F1分数较VLM-as-a-judge提升0.41。基于LEGO-Bench的测试结果显示，现有生成方法存在显著局限：在所有评估方案中，完全符合细粒度指令的场景生成成功率最高仅达10%。",
    "url": "https://huggingface.co/papers/2511.03001",
    "arxiv_url": "https://arxiv.org/abs/2511.03001"
  },
  {
    "title": "Orion-MSP: Multi-Scale Sparse Attention for Tabular In-Context Learning",
    "summary": "Tabular data remain the predominant format for real-world applications. Yet,\ndeveloping effective neural models for tabular data remains challenging due to\nheterogeneous feature types and complex interactions occurring at multiple\nscales. Recent advances in tabular in-context learning (ICL), such as TabPFN\nand TabICL, have achieved state-of-the-art performance comparable to\ngradient-boosted trees (GBTs) without task-specific fine-tuning. However,\ncurrent architectures exhibit key limitations: (1) single-scale feature\nprocessing that overlooks hierarchical dependencies, (2) dense attention with\nquadratic scaling in table width, and (3) strictly sequential component\nprocessing that prevents iterative representation refinement and\ncross-component communication. To address these challenges, we introduce\nOrion-MSP, a tabular ICL architecture featuring three key innovations: (1)\nmulti-scale processing to capture hierarchical feature interactions; (2)\nblock-sparse attention combining windowed, global, and random patterns for\nscalable efficiency and long-range connectivity; and (3) a Perceiver-style\nmemory enabling safe bidirectional information flow across components. Across\ndiverse benchmarks, Orion-MSP matches or surpasses state-of-the-art performance\nwhile scaling effectively to high-dimensional tables, establishing a new\nstandard for efficient tabular in-context learning. The model is publicly\navailable at https://github.com/Lexsi-Labs/Orion-MSP .",
    "translation": "标题：Orion-MSP：面向表格上下文学习的多尺度稀疏注意力机制\n\n摘要：表格数据仍是现实应用中最主要的数据形式。然而由于存在异构特征类型和多尺度下的复杂交互作用，开发有效的表格数据神经网络模型仍具挑战性。表格上下文学习的最新进展（如TabPFN和TabICL）已实现与梯度提升树相媲美的先进性能，且无需任务特定微调。但现有架构存在关键局限：（1）单尺度特征处理忽视层次化依赖关系；（2）注意力机制密度过高导致计算复杂度随表格宽度呈平方级增长；（3）严格的顺序组件处理阻碍迭代表示优化与跨组件通信。为解决这些问题，我们提出Orion-MSP表格上下文学习架构，其具备三大创新：（1）多尺度处理机制捕获层次化特征交互；（2）融合窗口化、全局化与随机模式的块稀疏注意力，实现可扩展效率与长程关联；（3）感知器式记忆模块确保组件间安全的双向信息流。在多样化基准测试中，Orion-MSP在有效扩展至高维表格的同时，达到或超越现有最优性能，为高效表格上下文学习确立了新标准。该模型已开源：https://github.com/Lexsi-Labs/Orion-MSP。",
    "url": "https://huggingface.co/papers/2511.02818",
    "arxiv_url": "https://arxiv.org/abs/2511.02818"
  },
  {
    "title": "TabTune: A Unified Library for Inference and Fine-Tuning Tabular\n  Foundation Models",
    "summary": "Tabular foundation models represent a growing paradigm in structured data\nlearning, extending the benefits of large-scale pretraining to tabular domains.\nHowever, their adoption remains limited due to heterogeneous preprocessing\npipelines, fragmented APIs, inconsistent fine-tuning procedures, and the\nabsence of standardized evaluation for deployment-oriented metrics such as\ncalibration and fairness. We present TabTune, a unified library that\nstandardizes the complete workflow for tabular foundation models through a\nsingle interface. TabTune provides consistent access to seven state-of-the-art\nmodels supporting multiple adaptation strategies, including zero-shot\ninference, meta-learning, supervised fine-tuning (SFT), and parameter-efficient\nfine-tuning (PEFT). The framework automates model-aware preprocessing, manages\narchitectural heterogeneity internally, and integrates evaluation modules for\nperformance, calibration, and fairness. Designed for extensibility and\nreproducibility, TabTune enables consistent benchmarking of adaptation\nstrategies of tabular foundation models. The library is open source and\navailable at https://github.com/Lexsi-Labs/TabTune .",
    "translation": "标题：TabTune：面向表格基础模型推理与微调的统一算法库\n\n摘要：表格基础模型正成为结构化数据学习的重要范式，将大规模预训练的优势扩展至表格数据领域。然而，由于异构的预处理流程、分散的应用程序接口、不一致的微调流程，以及缺乏面向部署的校准度与公平性等标准化评估指标，其应用推广仍面临挑战。本文提出TabTune统一算法库，通过单一接口实现了表格基础模型完整工作流的标准化。该库为七种前沿模型提供统一访问接口，支持零样本推理、元学习、监督微调和参数高效微调等多种适应策略。该框架实现了模型感知的自动化预处理，内部管理架构异构性，并集成了性能指标、校准度与公平性评估模块。TabTune以可扩展性和可复现性为设计原则，为表格基础模型的适应策略提供了标准化基准测试平台。本算法库已开源，访问地址为：https://github.com/Lexsi-Labs/TabTune。",
    "url": "https://huggingface.co/papers/2511.02802",
    "arxiv_url": "https://arxiv.org/abs/2511.02802"
  },
  {
    "title": "Kinematify: Open-Vocabulary Synthesis of High-DoF Articulated Objects",
    "summary": "A deep understanding of kinematic structures and movable components is\nessential for enabling robots to manipulate objects and model their own\narticulated forms. Such understanding is captured through articulated objects,\nwhich are essential for tasks such as physical simulation, motion planning, and\npolicy learning. However, creating these models, particularly for objects with\nhigh degrees of freedom (DoF), remains a significant challenge. Existing\nmethods typically rely on motion sequences or strong assumptions from\nhand-curated datasets, which hinders scalability. In this paper, we introduce\nKinematify, an automated framework that synthesizes articulated objects\ndirectly from arbitrary RGB images or textual descriptions. Our method\naddresses two core challenges: (i) inferring kinematic topologies for high-DoF\nobjects and (ii) estimating joint parameters from static geometry. To achieve\nthis, we combine MCTS search for structural inference with geometry-driven\noptimization for joint reasoning, producing physically consistent and\nfunctionally valid descriptions. We evaluate Kinematify on diverse inputs from\nboth synthetic and real-world environments, demonstrating improvements in\nregistration and kinematic topology accuracy over prior work.",
    "translation": "标题：Kinematify：高自由度铰接物体的开放词汇合成\n\n摘要：深入理解运动学结构与可动部件对于机器人操控物体及建模自身关节形态至关重要。这类认知通过铰接物体得以体现，其在物理仿真、运动规划与策略学习等任务中具有核心价值。然而针对高自由度物体的建模仍存在显著挑战。现有方法通常依赖手工标注数据集中的运动序列或强假设条件，这限制了方法的扩展性。本文提出Kinematify自动化框架，可直接基于任意RGB图像或文本描述合成铰接物体。我们的方法攻克了两个核心难题：(i) 高自由度物体的运动拓扑结构推断；(ii) 基于静态几何的关节参数估计。通过结合MCTS搜索的结构推理与几何驱动优化的关节解析，我们生成了物理一致且功能有效的描述。在合成与真实场景的多样化输入测试中，Kinematify在配准精度与运动拓扑准确性方面均优于现有方法。",
    "url": "https://huggingface.co/papers/2511.01294",
    "arxiv_url": "https://arxiv.org/abs/2511.01294"
  },
  {
    "title": "MME-CC: A Challenging Multi-Modal Evaluation Benchmark of Cognitive\n  Capacity",
    "summary": "As reasoning models scale rapidly, the essential role of multimodality in\nhuman cognition has come into sharp relief, driving a growing need to probe\nvision-centric cognitive behaviors. Yet, existing multimodal benchmarks either\noveremphasize textual reasoning or fall short of systematically capturing\nvision-centric cognitive behaviors, leaving the cognitive capacity of MLLMs\ninsufficiently assessed. To address this limitation, we introduce MME-CC\n(Multi-Modal Evaluation benchmark of Cognitive Capacity), a vision-grounded\nbenchmark that organizes 11 representative reasoning tasks into three\nfundamental categories of visual information: spatial, geometric, and\nknowledge-based reasoning, and provides fine-grained analyses of MLLMs'\ncognitive capacity across these dimensions. Based on MME-CC, we conduct\nextensive experiments over 16 representative MLLMs. Our study reveals that\nclosed-source models currently lead overall (e.g., 42.66 for Gemini-2.5-Pro vs.\n30.45 for GLM-4.5V), while spatial and geometric reasoning remain broadly weak\n(less than or equal to 30%). We further identify common error patterns,\nincluding orientation mistakes, fragile cross-view identity persistence, and\npoor adherence to counterfactual instructions, and observe that\nChain-of-Thought typically follows a three-stage process (extract -> reason ->\nverify) with heavy reliance on visual extraction. We hope this work catalyzes a\nshift toward treating the cognitive capacity of MLLMs as central to both\nevaluation and model design.",
    "translation": "标题：MME-CC：面向认知能力的挑战性多模态评估基准\n\n摘要：随着推理模型的快速发展，多模态在人类认知中的核心作用日益凸显，亟需系统探究以视觉为核心的认知行为。然而现有多模态基准或过度侧重文本推理，或未能系统捕捉视觉核心认知行为，导致对多模态大语言模型认知能力的评估存在不足。为此，我们提出MME-CC（多模态认知能力评估基准），这一基于视觉的基准将11类代表性推理任务划分为空间、几何和知识推理三大基础类别，从多维度对MLLMs的认知能力进行细粒度解析。基于该基准，我们对16个代表性MLLMs展开大规模实验。研究表明：闭源模型目前整体领先（如Gemini-2.5-Pro得分42.66 vs GLM-4.5V得分30.45），而空间与几何推理能力普遍薄弱（≤30%）。我们进一步识别出常见错误模式，包括方向判断失误、跨视角身份一致性维持脆弱、反事实指令遵循能力差等，并观察到思维链通常遵循“提取→推理→验证”三阶段模式且高度依赖视觉提取。本研究期望推动学界将认知能力作为MLLMs评估与模型设计的核心考量。",
    "url": "https://huggingface.co/papers/2511.03146",
    "arxiv_url": "https://arxiv.org/abs/2511.03146"
  },
  {
    "title": "LiveTradeBench: Seeking Real-World Alpha with Large Language Models",
    "summary": "Large language models (LLMs) achieve strong performance across\nbenchmarks--from knowledge quizzes and math reasoning to web-agent tasks--but\nthese tests occur in static settings, lacking real dynamics and uncertainty.\nConsequently, they evaluate isolated reasoning or problem-solving rather than\ndecision-making under uncertainty. To address this, we introduce\nLiveTradeBench, a live trading environment for evaluating LLM agents in\nrealistic and evolving markets. LiveTradeBench follows three design principles:\n(i) Live data streaming of market prices and news, eliminating dependence on\noffline backtesting and preventing information leakage while capturing\nreal-time uncertainty; (ii) a portfolio-management abstraction that extends\ncontrol from single-asset actions to multi-asset allocation, integrating risk\nmanagement and cross-asset reasoning; and (iii) multi-market evaluation across\nstructurally distinct environments--U.S. stocks and Polymarket prediction\nmarkets--differing in volatility, liquidity, and information flow. At each\nstep, an agent observes prices, news, and its portfolio, then outputs\npercentage allocations that balance risk and return. Using LiveTradeBench, we\nrun 50-day live evaluations of 21 LLMs across families. Results show that (1)\nhigh LMArena scores do not imply superior trading outcomes; (2) models display\ndistinct portfolio styles reflecting risk appetite and reasoning dynamics; and\n(3) some LLMs effectively leverage live signals to adapt decisions. These\nfindings expose a gap between static evaluation and real-world competence,\nmotivating benchmarks that test sequential decision making and consistency\nunder live uncertainty.",
    "translation": "标题：LiveTradeBench：基于大语言模型的现实市场阿尔法策略探索\n\n摘要：大语言模型在各类基准测试中表现优异——从知识问答、数学推理到网络智能体任务——但这些测试均处于静态环境，缺乏真实的动态性与不确定性。因此现有评估主要关注孤立推理或问题解决能力，而非不确定情境下的决策表现。为此我们推出LiveTradeBench，一个在真实演进市场环境中评估LLM智能体的实时交易平台。该平台遵循三大设计原则：（i）市场价格与新闻的实时数据流，摆脱对离线回测的依赖，在捕捉实时不确定性的同时防止信息泄露；（ii）组合管理抽象框架，将控制维度从单一资产操作拓展至多资产配置，整合风险管理与跨资产推理能力；（iii）跨市场评估机制，覆盖波动性、流动性和信息流特征迥异的两类市场——美国股票与Polymarket预测市场。在每个决策步，智能体观察价格波动、新闻动态及投资组合状态，随后输出平衡风险与收益的资产配置比例。通过LiveTradeBench，我们对21个不同架构的大语言模型进行了50天实时评估。结果表明：（1）较高的LMArena评分并不能保证卓越的交易表现；（2）不同模型展现出反映风险偏好与推理动态的差异化组合风格；（3）部分LLM能有效利用实时信号自适应调整决策。这些发现揭示了静态评估与现实能力之间的鸿沟，呼吁建立能够检验连续决策能力与实时不确定性下稳定表现的基准测试体系。",
    "url": "https://huggingface.co/papers/2511.03628",
    "arxiv_url": "https://arxiv.org/abs/2511.03628"
  },
  {
    "title": "CostBench: Evaluating Multi-Turn Cost-Optimal Planning and Adaptation in\n  Dynamic Environments for LLM Tool-Use Agents",
    "summary": "Current evaluations of Large Language Model (LLM) agents primarily emphasize\ntask completion, often overlooking resource efficiency and adaptability. This\nneglects a crucial capability: agents' ability to devise and adjust\ncost-optimal plans in response to changing environments. To bridge this gap, we\nintroduce CostBench, a scalable, cost-centric benchmark designed to evaluate\nagents' economic reasoning and replanning abilities. Situated in the\ntravel-planning domain, CostBench comprises tasks solvable via multiple\nsequences of atomic and composite tools with diverse, customizable costs. It\nalso supports four types of dynamic blocking events, such as tool failures and\ncost changes, to simulate real-world unpredictability and necessitate agents to\nadapt in real time. Evaluating leading open-sourced and proprietary models on\nCostBench reveals a substantial gap in cost-aware planning: agents frequently\nfail to identify cost-optimal solutions in static settings, with even GPT-5\nachieving less than 75% exact match rate on the hardest tasks, and performance\nfurther dropping by around 40% under dynamic conditions. By diagnosing these\nweaknesses, CostBench lays the groundwork for developing future agents that are\nboth economically rational and robust.",
    "translation": "标题：CostBench：评估动态环境中LLM工具使用代理的多轮成本最优规划与适应能力\n\n摘要：当前对大语言模型（LLM）智能体的评估主要关注任务完成度，往往忽视资源效率与适应能力。这种评估缺失忽略了一个关键能力：智能体在环境变化时制定并调整成本最优方案的能力。为弥补这一空白，我们推出CostBench——一个可扩展的以成本为中心的基准测试框架，旨在评估智能体的经济推理与重规划能力。该框架基于旅行规划领域构建，包含可通过具有多样化可定制成本的原子工具与复合工具组合求解的任务，同时支持工具失效、成本突变等四类动态阻断事件，以模拟现实世界的不确定性并促使智能体进行实时调整。通过对主流开源模型与专有模型在CostBench上的测试，我们发现其在成本感知规划方面存在显著不足：智能体在静态环境中经常无法识别成本最优方案，即使在最困难任务中GPT-5的精确匹配率也不足75%，而在动态环境下性能进一步下降约40%。通过系统诊断这些缺陷，CostBench为开发兼具经济合理性与鲁棒性的下一代智能体奠定了重要基础。",
    "url": "https://huggingface.co/papers/2511.02734",
    "arxiv_url": "https://arxiv.org/abs/2511.02734"
  },
  {
    "title": "The Sequential Edge: Inverse-Entropy Voting Beats Parallel\n  Self-Consistency at Matched Compute",
    "summary": "We revisit test-time scaling for language model reasoning and ask a\nfundamental question: at equal token budget and compute, is it better to run\nmultiple independent chains in parallel, or to run fewer chains that\niteratively refine through sequential steps? Through comprehensive evaluation\nacross 5 state-of-the-art open source models and 3 challenging reasoning\nbenchmarks, we find that sequential scaling where chains explicitly build upon\nprevious attempts consistently outperforms the dominant parallel\nself-consistency paradigm in 95.6% of configurations with gains in accuracy\nupto 46.7%. Further, we introduce inverse-entropy weighted voting, a novel\ntraining-free method to further boost the accuracy of sequential scaling. By\nweighing answers in proportion to the inverse entropy of their reasoning\nchains, we increase our success rate over parallel majority and establish it as\nthe optimal test-time scaling strategy. Our findings fundamentally challenge\nthe parallel reasoning orthodoxy that has dominated test-time scaling since\nWang et al.'s self-consistency decoding (Wang et al., 2022), positioning\nsequential refinement as the robust default for modern LLM reasoning and\nnecessitating a paradigm shift in how we approach inference-time optimization.",
    "translation": "标题：序列优势：在匹配计算量下逆熵投票机制优于并行自一致性方法\n\n摘要：本文重新审视语言模型推理的测试时扩展策略，并探讨一个核心问题：在相同令牌预算和计算量条件下，究竟是运行多个独立并行推理链更优，还是运行较少但通过序列化步骤迭代优化的推理链更有效？通过对5个顶尖开源模型和3个具有挑战性的推理基准进行全面评估，我们发现采用显式基于先前尝试的序列化扩展方法，在95.6%的配置中持续优于主流的并行自一致性范式，准确率提升最高达46.7%。此外，我们提出逆熵加权投票这一无需训练的新方法，进一步强化序列化扩展的准确率。通过按推理链逆熵比例加权答案，该方法相比并行多数投票机制取得了更高成功率，确立了其作为最优测试时扩展策略的地位。我们的研究结果从根本上挑战了自Wang等人提出自一致性解码（Wang et al., 2022）以来主导测试时扩展的并行推理范式，将序列优化定位为现代大语言模型推理的稳健默认方案，亟需推动推理时优化方法的范式转变。",
    "url": "https://huggingface.co/papers/2511.02309",
    "arxiv_url": "https://arxiv.org/abs/2511.02309"
  },
  {
    "title": "Jr. AI Scientist and Its Risk Report: Autonomous Scientific Exploration\n  from a Baseline Paper",
    "summary": "Understanding the current capabilities and risks of AI Scientist systems is\nessential for ensuring trustworthy and sustainable AI-driven scientific\nprogress while preserving the integrity of the academic ecosystem. To this end,\nwe develop Jr. AI Scientist, a state-of-the-art autonomous AI scientist system\nthat mimics the core research workflow of a novice student researcher: Given\nthe baseline paper from the human mentor, it analyzes its limitations,\nformulates novel hypotheses for improvement, validates them through rigorous\nexperimentation, and writes a paper with the results. Unlike previous\napproaches that assume full automation or operate on small-scale code, Jr. AI\nScientist follows a well-defined research workflow and leverages modern coding\nagents to handle complex, multi-file implementations, leading to scientifically\nvaluable contributions. For evaluation, we conducted automated assessments\nusing AI Reviewers, author-led evaluations, and submissions to Agents4Science,\na venue dedicated to AI-driven scientific contributions. The findings\ndemonstrate that Jr. AI Scientist generates papers receiving higher review\nscores than existing fully automated systems. Nevertheless, we identify\nimportant limitations from both the author evaluation and the Agents4Science\nreviews, indicating the potential risks of directly applying current AI\nScientist systems and key challenges for future research. Finally, we\ncomprehensively report various risks identified during development. We hope\nthese insights will deepen understanding of current progress and risks in AI\nScientist development.",
    "translation": "标题：Jr. AI科学家及其风险报告：基于基准论文的自主科学探索\n\n摘要：理解AI科学家系统的当前能力与风险，对于确保可信赖且可持续的AI驱动科学发展、同时维护学术生态系统的完整性至关重要。为此，我们开发了Jr. AI科学家——一个模拟初级学生研究者核心科研流程的先进自主AI科学家系统：在获得人类导师提供的基准论文后，系统会分析其局限性，提出改进的创新假设，通过严谨实验进行验证，并撰写成果论文。与以往假定全自动化或仅处理小规模代码的方法不同，Jr. AI科学家遵循明确的研究流程，利用现代代码代理处理复杂的多文件实现，最终产生具有科学价值的成果。在评估方面，我们采用AI评审员进行自动评估、作者主导评估，并向专注于AI驱动科学贡献的Agents4Science平台投稿。结果表明，Jr. AI科学家生成的论文评审分数优于现有全自动系统。然而，通过作者评估和Agents4Science评审，我们发现了当前系统的重要局限，这些局限揭示了直接应用现有AI科学家系统的潜在风险及未来研究的关键挑战。最后，我们全面报告了开发过程中识别的各类风险，期望这些发现能深化对AI科学家发展现状与风险的理解。",
    "url": "https://huggingface.co/papers/2511.04583",
    "arxiv_url": "https://arxiv.org/abs/2511.04583"
  },
  {
    "title": "Grounded Misunderstandings in Asymmetric Dialogue: A Perspectivist\n  Annotation Scheme for MapTask",
    "summary": "Collaborative dialogue relies on participants incrementally establishing\ncommon ground, yet in asymmetric settings they may believe they agree while\nreferring to different entities. We introduce a perspectivist annotation scheme\nfor the HCRC MapTask corpus (Anderson et al., 1991) that separately captures\nspeaker and addressee grounded interpretations for each reference expression,\nenabling us to trace how understanding emerges, diverges, and repairs over\ntime. Using a scheme-constrained LLM annotation pipeline, we obtain 13k\nannotated reference expressions with reliability estimates and analyze the\nresulting understanding states. The results show that full misunderstandings\nare rare once lexical variants are unified, but multiplicity discrepancies\nsystematically induce divergences, revealing how apparent grounding can mask\nreferential misalignment. Our framework provides both a resource and an\nanalytic lens for studying grounded misunderstanding and for evaluating\n(V)LLMs' capacity to model perspective-dependent grounding in collaborative\ndialogue.",
    "translation": "标题：非对称对话中的具身误解：MapTask的视角主义标注框架\n\n摘要：协作对话依赖于参与者逐步建立共同基础，但在非对称情境下，参与者可能自认为达成共识却实际指涉不同实体。本文针对HCRC MapTask语料库（Anderson等，1991）提出视角主义标注框架，分别捕捉说话者与受话者对每个指称表达的具身化解读，从而追踪理解形成、分歧产生与修复的动态过程。通过采用框架约束的大语言模型标注流程，我们获得1.3万个带可信度评估的标注指称表达，并解析其理解状态。研究表明：在统一词汇变体后，完全误解现象较为罕见，但多重性差异会系统引发理解分歧，揭示表面共识可能掩盖指称错位的本质。本框架既为研究具身误解提供资源与方法，也为评估（视觉）大语言模型在协作对话中建模视角依存性基础的能力提供分析工具。",
    "url": "https://huggingface.co/papers/2511.03718",
    "arxiv_url": "https://arxiv.org/abs/2511.03718"
  },
  {
    "title": "Let Multimodal Embedders Learn When to Augment Query via Adaptive Query\n  Augmentation",
    "summary": "Query augmentation makes queries more meaningful by appending further\ninformation to the queries to find relevant documents. Current studies have\nproposed Large Language Model (LLM)-based embedders, which learn representation\nfor embedding and generation for query augmentation in a multi-task manner by\nleveraging the generative capabilities of LLM. During inference, these jointly\ntrained embedders have conducted query augmentation followed by embedding,\nshowing effective results. However, augmenting every query leads to substantial\nembedding latency and query augmentation can be detrimental to performance for\nsome queries. Also, previous methods have not been explored in multimodal\nenvironments. To tackle these problems, we propose M-Solomon, a universal\nmultimodal embedder that can adaptively determine when to augment queries. Our\napproach first divides the queries of the training datasets into two groups at\nthe dataset level. One includes queries that require augmentation and the other\nincludes queries that do not. Then, we introduces a synthesis process that\ngenerates appropriate augmentations for queries that require them by leveraging\na powerful Multimodal LLM (MLLM). Next, we present adaptive query augmentation.\nThrough this step, M-Solomon can conduct query augmentation only when necessary\nby learning to generate synthetic augmentations with the prefix /augment for\nqueries that demand them and to generate the simple string /embed for others.\nExperimental results showed that M-Solomon not only surpassed the baseline\nwithout augmentation by a large margin but also outperformed the baseline that\nalways used augmentation, providing much faster embedding latency.",
    "translation": "标题：通过自适应查询增强让多模态嵌入器学习何时增强查询\n\n摘要：查询增强通过向查询附加额外信息使其更具语义完整性，从而提升相关文档检索效果。当前研究提出了基于大语言模型（LLM）的嵌入器，通过利用LLM的生成能力，以多任务方式同时学习嵌入表示和查询增强生成。在推理过程中，这些联合训练的嵌入器会先执行查询增强再进行嵌入，展现出显著效果。然而，对所有查询进行增强会导致显著的嵌入延迟，且某些查询的增强反而会损害检索性能。此外，现有方法尚未在多模态环境中进行探索。针对这些问题，我们提出M-Solomon——一种能自适应决定何时增强查询的通用多模态嵌入器。我们的方法首先在数据集层面将训练数据中的查询划分为两组：需要增强的查询和无需增强的查询。随后，我们引入合成生成流程，通过强大的多模态大语言模型（MLLM）为需要增强的查询生成合适的扩充内容。接着提出自适应查询增强机制，使M-Solomon仅需为需要增强的查询生成带\"/augment\"前缀的合成扩充内容，而为其他查询生成简单字符串\"/embed\"。实验结果表明，M-Solomon不仅大幅超越无增强基线模型，其性能也优于持续使用增强的基线模型，同时显著降低了嵌入延迟。",
    "url": "https://huggingface.co/papers/2511.02358",
    "arxiv_url": "https://arxiv.org/abs/2511.02358"
  }
]