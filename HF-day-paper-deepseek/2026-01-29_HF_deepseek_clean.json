[
  {
    "title": "Harder Is Better: Boosting Mathematical Reasoning via Difficulty-Aware GRPO and Multi-Aspect Question Reformulation",
    "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) offers a robust mechanism for enhancing mathematical reasoning in large models. However, we identify a systematic lack of emphasis on more challenging questions in existing methods from both algorithmic and data perspectives, despite their importance for refining underdeveloped capabilities. Algorithmically, widely used Group Relative Policy Optimization (GRPO) suffers from an implicit imbalance where the magnitude of policy updates is lower for harder questions. Data-wise, augmentation approaches primarily rephrase questions to enhance diversity without systematically increasing intrinsic difficulty. To address these issues, we propose a two-dual MathForge framework to improve mathematical reasoning by targeting harder questions from both perspectives, which comprises a Difficulty-Aware Group Policy Optimization (DGPO) algorithm and a Multi-Aspect Question Reformulation (MQR) strategy. Specifically, DGPO first rectifies the implicit imbalance in GRPO via difficulty-balanced group advantage estimation, and further prioritizes harder questions by difficulty-aware question-level weighting. Meanwhile, MQR reformulates questions across multiple aspects to increase difficulty while maintaining the original gold answer. Overall, MathForge forms a synergistic loop: MQR expands the data frontier, and DGPO effectively learns from the augmented data. Extensive experiments show that MathForge significantly outperforms existing methods on various mathematical reasoning tasks. The code and augmented data are all available at https://github.com/AMAP-ML/MathForge.",
    "translation": "标题：愈难愈优：基于难度感知GRPO与多维度问题重构的数学推理增强方法\n\n摘要：基于可验证奖励的强化学习（RLVR）为提升大模型数学推理能力提供了稳健机制。然而我们发现，现有方法在算法与数据层面均存在对高难度问题系统性关注不足的问题，而这类问题对于完善模型未充分发展的能力至关重要。算法层面，广泛使用的组相对策略优化（GRPO）存在隐性失衡：对更难问题的策略更新幅度较低。数据层面，现有增强方法主要通过语义改写提升多样性，未能系统性地增加问题内在难度。为解决这些问题，我们提出双轮驱动的MathForge框架，从算法与数据双重视角针对高难度问题进行优化，该框架包含难度感知组策略优化（DGPO）算法与多维度问题重构（MQR）策略。具体而言，DGPO首先通过难度均衡的组优势估计修正GRPO的隐性失衡，并进一步采用难度感知的问题级加权机制优先学习高难度问题。同时，MQR通过多维度重构在保持原答案不变的前提下系统性提升问题难度。整体上，MathForge形成协同闭环：MQR拓展数据边界，DGPO则有效学习增强数据。大量实验表明，MathForge在多项数学推理任务上显著优于现有方法。代码与增强数据已开源：https://github.com/AMAP-ML/MathForge。",
    "url": "https://huggingface.co/papers/2601.20614",
    "arxiv_url": "https://arxiv.org/abs/2601.20614"
  },
  {
    "title": "Advancing Open-source World Models",
    "summary": "We present LingBot-World, an open-sourced world simulator stemming from video generation. Positioned as a top-tier world model, LingBot-World offers the following features. (1) It maintains high fidelity and robust dynamics in a broad spectrum of environments, including realism, scientific contexts, cartoon styles, and beyond. (2) It enables a minute-level horizon while preserving contextual consistency over time, which is also known as \"long-term memory\". (3) It supports real-time interactivity, achieving a latency of under 1 second when producing 16 frames per second. We provide public access to the code and model in an effort to narrow the divide between open-source and closed-source technologies. We believe our release will empower the community with practical applications across areas like content creation, gaming, and robot learning.",
    "translation": "标题：推进开源世界模型的发展\n\n摘要：我们推出LingBot-World，这是一款基于视频生成技术开发的开源世界模拟器。作为顶级世界模型，LingBot-World具备以下核心特性：（1）在广泛环境中保持高保真度与强动态鲁棒性，涵盖写实场景、科学语境、卡通风格等多重维度；（2）实现分钟级时序预测能力，同时保持长期上下文一致性，即具备“长时记忆”特性；（3）支持实时交互，在以每秒16帧生成时延低于1秒。我们公开提供代码与模型资源，旨在缩小开源与闭源技术之间的差距。我们相信此次开源将推动内容创作、游戏开发、机器人学习等领域的实际应用创新，为研究社区注入新动能。",
    "url": "https://huggingface.co/papers/2601.20540",
    "arxiv_url": "https://arxiv.org/abs/2601.20540"
  },
  {
    "title": "Innovator-VL: A Multimodal Large Language Model for Scientific Discovery",
    "summary": "We present Innovator-VL, a scientific multimodal large language model designed to advance understanding and reasoning across diverse scientific domains while maintaining excellent performance on general vision tasks. Contrary to the trend of relying on massive domain-specific pretraining and opaque pipelines, our work demonstrates that principled training design and transparent methodology can yield strong scientific intelligence with substantially reduced data requirements. (i) First, we provide a fully transparent, end-to-end reproducible training pipeline, covering data collection, cleaning, preprocessing, supervised fine-tuning, reinforcement learning, and evaluation, along with detailed optimization recipes. This facilitates systematic extension by the community. (ii) Second, Innovator-VL exhibits remarkable data efficiency, achieving competitive performance on various scientific tasks using fewer than five million curated samples without large-scale pretraining. These results highlight that effective reasoning can be achieved through principled data selection rather than indiscriminate scaling. (iii) Third, Innovator-VL demonstrates strong generalization, achieving competitive performance on general vision, multimodal reasoning, and scientific benchmarks. This indicates that scientific alignment can be integrated into a unified model without compromising general-purpose capabilities. Our practices suggest that efficient, reproducible, and high-performing scientific multimodal models can be built even without large-scale data, providing a practical foundation for future research.",
    "translation": "标题：Innovator-VL：面向科学发现的多模态大语言模型\n\n摘要：本文提出Innovator-VL，一个面向科学领域的多模态大语言模型，旨在提升跨学科科学理解与推理能力，同时在通用视觉任务上保持优异性能。与当前依赖海量领域特定预训练和不透明技术路径的趋势不同，本研究表明，通过原则性的训练设计和透明的方法论，能够以显著减少的数据需求实现强大的科学智能。（i）首先，我们提供了一套完全透明、端到端可复现的训练流程，涵盖数据收集、清洗、预处理、监督微调、强化学习及评估环节，并附有详细的优化方案，便于学术界系统性地扩展研究。（ii）其次，Innovator-VL展现出卓越的数据效率，仅使用不足五百万条精选样本（无需大规模预训练）即在多项科学任务中取得具有竞争力的性能。这些结果证明，通过原则性数据筛选而非盲目扩大规模，同样可实现高效推理。（iii）再次，该模型表现出强大的泛化能力，在通用视觉、多模态推理及科学基准测试中均达到竞争性水平，表明科学对齐能力可融入统一模型而不损害其通用功能。我们的实践表明，即使不依赖大规模数据，也能构建高效、可复现、高性能的科学多模态模型，为未来研究提供了实用基础。",
    "url": "https://huggingface.co/papers/2601.19325",
    "arxiv_url": "https://arxiv.org/abs/2601.19325"
  },
  {
    "title": "DeepSeek-OCR 2: Visual Causal Flow",
    "summary": "We present DeepSeek-OCR 2 to investigate the feasibility of a novel encoder-DeepEncoder V2-capable of dynamically reordering visual tokens upon image semantics. Conventional vision-language models (VLMs) invariably process visual tokens in a rigid raster-scan order (top-left to bottom-right) with fixed positional encoding when fed into LLMs. However, this contradicts human visual perception, which follows flexible yet semantically coherent scanning patterns driven by inherent logical structures. Particularly for images with complex layouts, human vision exhibits causally-informed sequential processing. Inspired by this cognitive mechanism, DeepEncoder V2 is designed to endow the encoder with causal reasoning capabilities, enabling it to intelligently reorder visual tokens prior to LLM-based content interpretation. This work explores a novel paradigm: whether 2D image understanding can be effectively achieved through two-cascaded 1D causal reasoning structures, thereby offering a new architectural approach with the potential to achieve genuine 2D reasoning. Codes and model weights are publicly accessible at http://github.com/deepseek-ai/DeepSeek-OCR-2.",
    "translation": "标题：DeepSeek-OCR 2：视觉因果流\n\n摘要：本文提出DeepSeek-OCR 2模型，旨在探究一种新型编码器DeepEncoder V2的可行性——该编码器能够依据图像语义动态重排视觉标记。传统视觉语言模型在处理图像时，始终以固定的光栅扫描顺序（从左上方至右下方）配合静态位置编码将视觉标记输入大语言模型。然而，这种处理方式与人类视觉感知机制存在本质矛盾：人类的视觉扫描遵循灵活且语义连贯的模式，其过程受内在逻辑结构驱动。特别是在处理复杂版式图像时，人类视觉系统展现出基于因果关系的序列化处理特性。受此认知机制启发，DeepEncoder V2被设计为具备因果推理能力的编码器，使其在基于大语言模型的内容解析前，能够智能地对视觉标记进行重排序。本研究探索了一种新颖范式：是否可以通过两级级联的一维因果推理结构有效实现二维图像理解，从而为达成真正的二维推理提供具有潜力的新型架构方案。相关代码与模型权重已公开于http://github.com/deepseek-ai/DeepSeek-OCR-2。",
    "url": "https://huggingface.co/papers/2601.20552",
    "arxiv_url": "https://arxiv.org/abs/2601.20552"
  },
  {
    "title": "Spark: Strategic Policy-Aware Exploration via Dynamic Branching for Long-Horizon Agentic Learning",
    "summary": "Reinforcement learning has empowered large language models to act as intelligent agents, yet training them for long-horizon tasks remains challenging due to the scarcity of high-quality trajectories, especially under limited resources. Existing methods typically scale up rollout sizes and indiscriminately allocate computational resources among intermediate steps. Such attempts inherently waste substantial computation budget on trivial steps while failing to guarantee sample quality. To address this, we propose Spark (Strategic Policy-Aware exploRation via Key-state dynamic branching), a novel framework that selectively branches at critical decision states for resource-efficient exploration. Our key insight is to activate adaptive branching exploration at critical decision points to probe promising trajectories, thereby achieving precise resource allocation that prioritizes sampling quality over blind coverage. This design leverages the agent's intrinsic decision-making signals to reduce dependence on human priors, enabling the agent to autonomously expand exploration and achieve stronger generalization. Experiments across diverse tasks (e.g., embodied planning), demonstrate that Spark achieves superior success rates with significantly fewer training samples, exhibiting robust generalization even in unseen scenarios.",
    "translation": "标题：Spark：基于动态分支的长时程智能体学习策略感知探索框架\n\n摘要：强化学习赋能大语言模型成为智能体，但在资源受限条件下，由于高质量轨迹样本稀缺，长时程任务训练仍面临挑战。现有方法通常盲目扩大探索规模，并在中间步骤中无差别分配计算资源。此类尝试本质上将大量计算资源消耗在无关紧要的步骤上，且无法保证样本质量。为此，我们提出Spark（基于关键状态动态分支的策略感知探索框架），该创新框架通过在关键决策状态选择性分支以实现资源高效的探索。我们的核心思路是在关键决策点激活自适应分支探索机制，以此探测潜在优质轨迹，从而实现以采样质量优先、覆盖广度次之的精准资源分配。该设计利用智能体内在决策信号降低对人类先验知识的依赖，使智能体能够自主扩展探索范围并实现更强的泛化能力。在具身规划等多样化任务上的实验表明，Spark能以显著更少的训练样本获得更高的任务成功率，并在未见场景中展现出鲁棒的泛化性能。",
    "url": "https://huggingface.co/papers/2601.20209",
    "arxiv_url": "https://arxiv.org/abs/2601.20209"
  },
  {
    "title": "AACR-Bench: Evaluating Automatic Code Review with Holistic Repository-Level Context",
    "summary": "High-quality evaluation benchmarks are pivotal for deploying Large Language Models (LLMs) in Automated Code Review (ACR). However, existing benchmarks suffer from two critical limitations: first, the lack of multi-language support in repository-level contexts, which restricts the generalizability of evaluation results; second, the reliance on noisy, incomplete ground truth derived from raw Pull Request (PR) comments, which constrains the scope of issue detection. To address these challenges, we introduce AACR-Bench a comprehensive benchmark that provides full cross-file context across multiple programming languages. Unlike traditional datasets, AACR-Bench employs an \"AI-assisted, Expert-verified\" annotation pipeline to uncover latent defects often overlooked in original PRs, resulting in a 285% increase in defect coverage. Extensive evaluations of mainstream LLMs on AACR-Bench reveal that previous assessments may have either misjudged or only partially captured model capabilities due to data limitations. Our work establishes a more rigorous standard for ACR evaluation and offers new insights on LLM based ACR, i.e., the granularity/level of context and the choice of retrieval methods significantly impact ACR performance, and this influence varies depending on the LLM, programming language, and the LLM usage paradigm e.g., whether an Agent architecture is employed. The code, data, and other artifacts of our evaluation set are available at https://github.com/alibaba/aacr-bench .",
    "translation": "标题：AACR-Bench：基于全仓库级上下文的自动代码评审评估框架\n\n摘要：高质量的评估基准对于在自动代码评审（ACR）中部署大语言模型（LLMs）至关重要。然而，现有基准存在两个关键局限：其一，缺乏仓库级上下文的多语言支持，限制了评估结果的普适性；其二，依赖从原始拉取请求（PR）评论中提取的嘈杂且不完整的真实数据，制约了问题检测的范围。为应对这些挑战，我们提出了AACR-Bench——一个提供跨编程语言完整跨文件上下文的综合性基准。与传统数据集不同，AACR-Bench采用“AI辅助、专家验证”的标注流程，能够发现原始PR中常被忽略的潜在缺陷，使缺陷覆盖率提升285%。基于AACR-Bench对主流LLMs的广泛评估表明，由于数据局限性，既往评估可能误判或仅部分反映了模型能力。本研究为ACR评估建立了更严谨的标准，并为基于LLM的ACR提供了新见解：上下文粒度/层级与检索方法的选择显著影响ACR性能，且这种影响因LLM类型、编程语言及LLM使用范式（例如是否采用智能体架构）而异。本评估集的代码、数据及相关资源已公开于https://github.com/alibaba/aacr-bench。",
    "url": "https://huggingface.co/papers/2601.19494",
    "arxiv_url": "https://arxiv.org/abs/2601.19494"
  },
  {
    "title": "Linear representations in language models can change dramatically over a conversation",
    "summary": "Language model representations often contain linear directions that correspond to high-level concepts. Here, we study the dynamics of these representations: how representations evolve along these dimensions within the context of (simulated) conversations. We find that linear representations can change dramatically over a conversation; for example, information that is represented as factual at the beginning of a conversation can be represented as non-factual at the end and vice versa. These changes are content-dependent; while representations of conversation-relevant information may change, generic information is generally preserved. These changes are robust even for dimensions that disentangle factuality from more superficial response patterns, and occur across different model families and layers of the model. These representation changes do not require on-policy conversations; even replaying a conversation script written by an entirely different model can produce similar changes. However, adaptation is much weaker from simply having a sci-fi story in context that is framed more explicitly as such. We also show that steering along a representational direction can have dramatically different effects at different points in a conversation. These results are consistent with the idea that representations may evolve in response to the model playing a particular role that is cued by a conversation. Our findings may pose challenges for interpretability and steering -- in particular, they imply that it may be misleading to use static interpretations of features or directions, or probes that assume a particular range of features consistently corresponds to a particular ground-truth value. However, these types of representational dynamics also point to exciting new research directions for understanding how models adapt to context.",
    "translation": "标题：语言模型中的线性表征在对话过程中可能发生显著变化\n\n摘要：语言模型的表征通常包含与高层次概念相对应的线性方向。本文研究这些表征的动态特性：在（模拟）对话语境中，这些表征如何沿着特定维度演化。我们发现线性表征在对话过程中可能发生显著变化；例如，对话初期被表征为事实的信息可能在对话末期被表征为非事实，反之亦然。这些变化具有内容依赖性：与对话相关的信息表征可能发生变化，而通用信息通常保持稳定。即使对于能够将事实性与表层响应模式分离的维度，这些变化依然稳健存在，且在不同模型架构和模型层中均有体现。此类表征变化无需依赖策略性对话；即使重播由完全不同的模型编写的对话脚本也能产生类似变化。然而，若语境中仅存在明确标注为科幻小说的故事，模型的适应程度则显著减弱。我们还证明，沿着特定表征方向进行引导在对话的不同阶段可能产生截然不同的效果。这些结果与以下观点一致：表征可能因模型响应对话所提示的特定角色而演化。我们的研究发现可能对可解释性与引导技术构成挑战——具体而言，这意味着静态的特征/方向解释方法，或假定特定特征范围始终对应特定真实值的探测方法可能存在误导性。然而，这类表征动态特性也为理解模型如何适应语境指明了令人振奋的新研究方向。",
    "url": "https://huggingface.co/papers/2601.20834",
    "arxiv_url": "https://arxiv.org/abs/2601.20834"
  },
  {
    "title": "Reinforcement Learning via Self-Distillation",
    "summary": "Large language models are increasingly post-trained with reinforcement learning in verifiable domains such as code and math. Yet, current methods for reinforcement learning with verifiable rewards (RLVR) learn only from a scalar outcome reward per attempt, creating a severe credit-assignment bottleneck. Many verifiable environments actually provide rich textual feedback, such as runtime errors or judge evaluations, that explain why an attempt failed. We formalize this setting as reinforcement learning with rich feedback and introduce Self-Distillation Policy Optimization (SDPO), which converts tokenized feedback into a dense learning signal without any external teacher or explicit reward model. SDPO treats the current model conditioned on feedback as a self-teacher and distills its feedback-informed next-token predictions back into the policy. In this way, SDPO leverages the model's ability to retrospectively identify its own mistakes in-context. Across scientific reasoning, tool use, and competitive programming on LiveCodeBench v6, SDPO improves sample efficiency and final accuracy over strong RLVR baselines. Notably, SDPO also outperforms baselines in standard RLVR environments that only return scalar feedback by using successful rollouts as implicit feedback for failed attempts. Finally, applying SDPO to individual questions at test time accelerates discovery on difficult binary-reward tasks, achieving the same discovery probability as best-of-k sampling or multi-turn conversations with 3x fewer attempts.",
    "translation": "标题：基于自蒸馏的强化学习\n\n摘要：大型语言模型在代码和数学等可验证领域越来越多地通过强化学习进行后训练。然而，当前基于可验证奖励的强化学习方法仅从每次尝试的标量结果奖励中学习，造成了严重的信用分配瓶颈。许多可验证环境实际上提供了丰富的文本反馈（如运行时错误或裁判评估），这些反馈能够解释尝试失败的原因。我们将这一场景形式化为具有丰富反馈的强化学习，并提出自蒸馏策略优化方法。该方法能够将标记化的反馈转化为密集的学习信号，无需依赖外部教师模型或显式奖励模型。SDPO将基于反馈调节的当前模型视为自教师，并将其在反馈信息指导下生成的下一标记预测蒸馏回策略中。通过这种方式，SDPO利用了模型在上下文中回溯识别自身错误的能力。在科学推理、工具使用以及LiveCodeBench v6的竞技编程实验中，SDPO相较于现有强基准方法在样本效率和最终准确率上均展现出显著提升。值得注意的是，在仅返回标量反馈的标准RLVR环境中，SDPO通过将成功轨迹作为失败尝试的隐式反馈，其表现也优于基准方法。最后，在测试阶段对单个问题应用SDPO能够加速困难二元奖励任务的探索过程，以仅需三分之一尝试次数即可达到与k最佳采样或多轮对话相同的探索概率。",
    "url": "https://huggingface.co/papers/2601.20802",
    "arxiv_url": "https://arxiv.org/abs/2601.20802"
  },
  {
    "title": "SERA: Soft-Verified Efficient Repository Agents",
    "summary": "Open-weight coding agents should hold a fundamental advantage over closed-source systems: they can be specialized to private codebases, encoding repository-specific information directly in their weights. Yet the cost and complexity of training has kept this advantage theoretical. We show it is now practical. We present Soft-Verified Efficient Repository Agents (SERA), an efficient method for training coding agents that enables the rapid and cheap creation of agents specialized to private codebases. Using only supervised finetuning (SFT), SERA achieves state-of-the-art results among fully open-source (open data, method, code) models while matching the performance of frontier open-weight models like Devstral-Small-2. Creating SERA models is 26x cheaper than reinforcement learning and 57x cheaper than previous synthetic data methods to reach equivalent performance. Our method, Soft Verified Generation (SVG), generates thousands of trajectories from a single code repository. Combined with cost-efficiency, this enables specialization to private codebases. Beyond repository specialization, we apply SVG to a larger corpus of codebases, generating over 200,000 synthetic trajectories. We use this dataset to provide detailed analysis of scaling laws, ablations, and confounding factors for training coding agents. Overall, we believe our work will greatly accelerate research on open coding agents and showcase the advantage of open-source models that can specialize to private codebases. We release SERA as the first model in Ai2's Open Coding Agents series, along with all our code, data, and Claude Code integration to support the research community.",
    "translation": "标题：SERA：软验证高效代码库智能体\n\n摘要：开源权重编码智能体相较于闭源系统应具备根本性优势：它们能够针对私有代码库进行专业化训练，将特定代码库信息直接编码至权重中。然而训练成本与复杂性使得这一优势长期停留在理论层面。本研究证明该优势现已具备实践可行性。我们提出软验证高效代码库智能体（SERA）——一种高效的编码智能体训练方法，能够快速低成本创建针对私有代码库的专业化智能体。仅通过监督微调（SFT），SERA在完全开源（开放数据、方法、代码）模型中取得了最先进的成果，同时达到与Devstral-Small-2等前沿开源权重模型相当的性能。SERA模型的创建成本比强化学习方法降低26倍，比现有合成数据方法降低57倍即可达到同等性能。我们提出的软验证生成（SVG）方法能够从单一代码库生成数千条训练轨迹，结合成本效益优势，实现了对私有代码库的专业化适配。除代码库专业化应用外，我们将SVG应用于更大规模的代码库集合，生成超过20万条合成训练轨迹。基于该数据集，我们对编码智能体训练的扩展规律、消融实验及混杂因素进行了详细分析。总体而言，我们相信这项工作将极大加速开源编码智能体的研究进程，并彰显可适配私有代码库的开源模型优势。我们将SERA作为Ai2开源编码智能体系列的首个模型发布，同时开放全部代码、数据及Claude Code集成方案，以支持研究社区的发展。",
    "url": "https://huggingface.co/papers/2601.20789",
    "arxiv_url": "https://arxiv.org/abs/2601.20789"
  },
  {
    "title": "VERGE: Formal Refinement and Guidance Engine for Verifiable LLM Reasoning",
    "summary": "Despite the syntactic fluency of Large Language Models (LLMs), ensuring their logical correctness in high-stakes domains remains a fundamental challenge. We present a neurosymbolic framework that combines LLMs with SMT solvers to produce verification-guided answers through iterative refinement. Our approach decomposes LLM outputs into atomic claims, autoformalizes them into first-order logic, and verifies their logical consistency using automated theorem proving. We introduce three key innovations: (1) multi-model consensus via formal semantic equivalence checking to ensure logic-level alignment between candidates, eliminating the syntactic bias of surface-form metrics, (2) semantic routing that directs different claim types to appropriate verification strategies: symbolic solvers for logical claims and LLM ensembles for commonsense reasoning, and (3) precise logical error localization via Minimal Correction Subsets (MCS), which pinpoint the exact subset of claims to revise, transforming binary failure signals into actionable feedback. Our framework classifies claims by their logical status and aggregates multiple verification signals into a unified score with variance-based penalty. The system iteratively refines answers using structured feedback until acceptance criteria are met or convergence is achieved. This hybrid approach delivers formal guarantees where possible and consensus verification elsewhere, advancing trustworthy AI. With the GPT-OSS-120B model, VERGE demonstrates an average performance uplift of 18.7% at convergence across a set of reasoning benchmarks compared to single-pass approaches.",
    "translation": "标题：VERGE：可验证大语言模型推理的形式化精炼与引导引擎\n\n摘要：尽管大语言模型（LLM）具备流畅的语法生成能力，但在高风险领域中确保其逻辑正确性仍是一个根本性挑战。本文提出一种神经符号框架，将LLM与可满足性模理论（SMT）求解器相结合，通过迭代精炼生成可验证导向的答案。该框架将LLM输出分解为原子主张，将其自动形式化为一阶逻辑表示，并利用自动定理证明技术验证其逻辑一致性。我们引入三项关键创新：（1）通过形式语义等价性检查实现多模型共识机制，确保候选答案在逻辑层面的对齐，消除表面形式度量的语法偏差；（2）语义路由机制，将不同类型的主张导向适配的验证策略——逻辑主张使用符号求解器，常识推理则采用LLM集成方法；（3）基于最小修正子集（MCS）的精确逻辑错误定位技术，精确定位需要修正的具体主张子集，将二元失败信号转化为可操作的反馈。本框架根据主张的逻辑状态进行分类，并将多重验证信号聚合为包含方差惩罚的统一评分。系统通过结构化反馈迭代优化答案，直至满足接受标准或达到收敛状态。这种混合方法在可行处提供形式化保证，在其他场景采用共识验证机制，从而推动可信人工智能的发展。基于GPT-OSS-120B模型的实验表明，在一系列推理基准测试中，VERGE在收敛时的平均性能较单次推理方法提升18.7%。",
    "url": "https://huggingface.co/papers/2601.20055",
    "arxiv_url": "https://arxiv.org/abs/2601.20055"
  },
  {
    "title": "OmegaUse: Building a General-Purpose GUI Agent for Autonomous Task Execution",
    "summary": "Graphical User Interface (GUI) agents show great potential for enabling foundation models to complete real-world tasks, revolutionizing human-computer interaction and improving human productivity. In this report, we present OmegaUse, a general-purpose GUI agent model for autonomous task execution on both mobile and desktop platforms, supporting computer-use and phone-use scenarios. Building an effective GUI agent model relies on two factors: (1) high-quality data and (2) effective training methods. To address these, we introduce a carefully engineered data-construction pipeline and a decoupled training paradigm. For data construction, we leverage rigorously curated open-source datasets and introduce a novel automated synthesis framework that integrates bottom-up autonomous exploration with top-down taxonomy-guided generation to create high-fidelity synthetic data. For training, to better leverage these data, we adopt a two-stage strategy: Supervised Fine-Tuning (SFT) to establish fundamental interaction syntax, followed by Group Relative Policy Optimization (GRPO) to improve spatial grounding and sequential planning. To balance computational efficiency with agentic reasoning capacity, OmegaUse is built on a Mixture-of-Experts (MoE) backbone. To evaluate cross-terminal capabilities in an offline setting, we introduce OS-Nav, a benchmark suite spanning multiple operating systems: ChiM-Nav, targeting Chinese Android mobile environments, and Ubu-Nav, focusing on routine desktop interactions on Ubuntu. Extensive experiments show that OmegaUse is highly competitive across established GUI benchmarks, achieving a state-of-the-art (SOTA) score of 96.3% on ScreenSpot-V2 and a leading 79.1% step success rate on AndroidControl. OmegaUse also performs strongly on OS-Nav, reaching 74.24% step success on ChiM-Nav and 55.9% average success on Ubu-Nav.",
    "translation": "标题：OmegaUse：构建面向自主任务执行的通用图形用户界面智能体\n\n摘要：图形用户界面（GUI）智能体在赋能基础模型完成现实世界任务方面展现出巨大潜力，有望彻底改变人机交互模式并提升人类生产效率。本报告提出OmegaUse——一种支持跨移动端与桌面端平台自主任务执行的通用GUI智能体模型，涵盖计算机使用与手机操作两大场景。构建高效GUI智能体模型依赖于两大核心要素：（1）高质量数据与（2）有效训练方法。为此，我们设计了精密的数椐构建流程与解耦式训练范式。在数据构建方面，我们整合了严格筛选的开源数据集，并提出一种创新的自动化合成框架，该框架通过自底向上的自主探索与自顶向下的分类体系引导生成相结合的方式，构建高保真合成数据。在训练方法上，为充分发挥数据价值，我们采用两阶段策略：首先通过监督微调建立基础交互语法，继而采用分组相对策略优化增强空间定位与序列规划能力。为平衡计算效率与智能体推理能力，OmegaUse采用混合专家模型作为主干架构。为评估离线环境下的跨终端能力，我们构建了OS-Nav基准测试套件，涵盖多操作系统场景：针对中文安卓移动环境的ChiM-Nav，以及聚焦Ubuntu系统日常桌面交互的Ubu-Nav。大量实验表明，OmegaUse在现有GUI基准测试中表现卓越，在ScreenSpot-V2上取得96.3%的顶尖成绩，在AndroidControl上获得79.1%的领先步骤成功率。在OS-Nav测试中，OmegaUse同样表现优异，于ChiM-Nav达到74.24%的步骤成功率，在Ubu-Nav上实现55.9%的平均成功率。",
    "url": "https://huggingface.co/papers/2601.20380",
    "arxiv_url": "https://arxiv.org/abs/2601.20380"
  },
  {
    "title": "Group Distributionally Robust Optimization-Driven Reinforcement Learning for LLM Reasoning",
    "summary": "Recent progress in Large Language Model (LLM) reasoning is increasingly driven by the refinement of post-training loss functions and alignment strategies. However, standard Reinforcement Learning (RL) paradigms like Group Relative Policy Optimization (GRPO) remain constrained by static uniformity: uniform prompt sampling and a fixed number of rollouts per prompt. For heterogeneous, heavy-tailed reasoning data, this creates structural inefficiencies that waste compute on already-solved patterns while under-training the long tail of hard problems. To address this, we propose Multi-Adversary Group Distributionally Robust Optimization (GDRO), an optimization-first framework that moves beyond uniform reasoning models by dynamically adapting the training distribution.\n  We introduce an Online Difficulty Classifier that partitions prompts into dynamic pass@k difficulty groups. We then propose two independent GDRO games for post-training: (1) Prompt-GDRO, which employs an EMA-debiased multiplicative-weights bandit sampler to target the intensive difficulty margin and upweight persistently hard groups without frequency bias; and (2) Rollout-GDRO, which uses a shadow-price controller to reallocate rollouts across groups, maximizing gradient variance reduction on hard tasks under a fixed mean budget (compute-neutral). We provide no-regret guarantees for both controllers and additionally a variance-proxy analysis motivating a square-root optimal rollout allocation for Rollout-GDRO. We validate our framework on the DAPO 14.1k dataset using Qwen3-Base models. Prompt-GDRO and Rollout-GDRO achieve average relative gains of +10.6% and +10.1%, respectively, in pass@8 accuracy across 1.7B, 4B, and 8B scales compared to the GRPO baseline. Qualitative analysis shows an emergent curriculum: the adversaries shift resources to the evolving reasoning frontier, enhancing the reasoning model's performance.",
    "translation": "标题：基于群体分布鲁棒优化驱动的大语言模型推理强化学习\n\n摘要：大语言模型推理能力的近期进展日益依赖于训练后损失函数与对齐策略的优化。然而，诸如群体相对策略优化等标准强化学习范式仍受限于静态均匀性约束：即均匀的提示采样和每个提示固定次数的轨迹展开。对于异构、重尾的推理数据，这种机制会导致结构性低效——在已解决的模式上浪费计算资源，同时对困难问题的长尾部分训练不足。为解决这一问题，我们提出多对抗群体分布鲁棒优化框架，这是一种以优化为先导的范式，通过动态调整训练分布，突破了均匀推理模型的限制。\n\n我们引入在线难度分类器，将提示动态划分为基于pass@k指标的难度组。进而提出两个独立的训练后GDRO博弈机制：（1）提示-GDRO：采用指数移动平均去偏的乘权赌博机采样器，聚焦于高强度难度边界，对持续困难组进行无频率偏见的加权提升；（2）展开-GDRO：通过影子价格控制器在组间重新分配轨迹展开次数，在固定平均计算预算（计算中性）条件下，最大化困难任务的梯度方差缩减。我们为两个控制器提供了无悔保证，并针对展开-GDRO提出方差代理分析，推导出平方根最优的展开分配方案。\n\n我们在DAPO 14.1k数据集上使用Qwen3-Base系列模型验证本框架。与GRPO基线相比，提示-GDRO与展开-GDRO在17亿、40亿和80亿参数规模上，pass@8准确率平均相对提升分别达到+10.6%和+10.1%。定性分析揭示了 emergent curriculum 现象：对抗机制将资源动态调配至持续演进的推理前沿，从而显著增强推理模型的性能表现。",
    "url": "https://huggingface.co/papers/2601.19280",
    "arxiv_url": "https://arxiv.org/abs/2601.19280"
  },
  {
    "title": "RIR-Mega-Speech: A Reverberant Speech Corpus with Comprehensive Acoustic Metadata and Reproducible Evaluation",
    "summary": "Despite decades of research on reverberant speech, comparing methods remains difficult because most corpora lack per-file acoustic annotations or provide limited documentation for reproduction. We present RIR-Mega-Speech, a corpus of approximately 117.5 hours created by convolving LibriSpeech utterances with roughly 5,000 simulated room impulse responses from the RIR-Mega collection. Every file includes RT60, direct-to-reverberant ratio (DRR), and clarity index (C_{50}) computed from the source RIR using clearly defined, reproducible procedures. We also provide scripts to rebuild the dataset and reproduce all evaluation results.\n  Using Whisper small on 1,500 paired utterances, we measure 5.20% WER (95% CI: 4.69--5.78) on clean speech and 7.70% (7.04--8.35) on reverberant versions, corresponding to a paired increase of 2.50 percentage points (2.06--2.98). This represents a 48% relative degradation. WER increases monotonically with RT60 and decreases with DRR, consistent with prior perceptual studies. While the core finding that reverberation harms recognition is well established, we aim to provide the community with a standardized resource where acoustic conditions are transparent and results can be verified independently. The repository includes one-command rebuild instructions for both Windows and Linux environments.",
    "translation": "标题：RIR-Mega-Speech：一个包含全面声学元数据与可复现评估的混响语音语料库\n\n摘要：尽管针对混响语音的研究已持续数十年，但由于大多数语料库缺乏逐文件的声学标注或仅提供有限的复现文档，不同方法之间的比较仍存在困难。本文提出RIR-Mega-Speech语料库，该语料库通过将LibriSpeech语音片段与来自RIR-Mega集合的大约5,000条模拟房间脉冲响应进行卷积生成，总时长约117.5小时。每个文件均包含依据明确定义且可复现的计算流程，从原始房间脉冲响应中提取的混响时间（RT60）、直达声与混响声能量比（DRR）以及清晰度指数（C_{50}）。我们还提供了用于重建数据集和复现所有评估结果的脚本。\n\n基于Whisper small模型对1,500组配对语音的测试，我们在纯净语音上测得5.20%的词错误率（95%置信区间：4.69–5.78），在混响版本上测得7.70%（7.04–8.35），对应配对增量达2.50个百分点（2.06–2.98），相当于相对性能下降48%。词错误率随RT60增加而单调上升，随DRR增加而下降，这与先前的感知研究结论一致。尽管混响损害识别性能这一核心结论已广为人知，我们旨在为学界提供一个声学条件透明、结果可独立验证的标准化资源。该资源库同时包含适用于Windows和Linux环境的一键重建指令。",
    "url": "https://huggingface.co/papers/2601.19949",
    "arxiv_url": "https://arxiv.org/abs/2601.19949"
  },
  {
    "title": "UPLiFT: Efficient Pixel-Dense Feature Upsampling with Local Attenders",
    "summary": "The space of task-agnostic feature upsampling has emerged as a promising area of research to efficiently create denser features from pre-trained visual backbones. These methods act as a shortcut to achieve dense features for a fraction of the cost by learning to map low-resolution features to high-resolution versions. While early works in this space used iterative upsampling approaches, more recent works have switched to cross-attention-based methods, which risk falling into the same efficiency scaling problems of the backbones they are upsampling. In this work, we demonstrate that iterative upsampling methods can still compete with cross-attention-based methods; moreover, they can achieve state-of-the-art performance with lower inference costs. We propose UPLiFT, an architecture for Universal Pixel-dense Lightweight Feature Transforms. We also propose an efficient Local Attender operator to overcome the limitations of prior iterative feature upsampling methods. This operator uses an alternative attentional pooling formulation defined fully locally. We show that our Local Attender allows UPLiFT to maintain stable features throughout upsampling, enabling state-of-the-art performance with lower inference costs than existing pixel-dense feature upsamplers. In addition, we apply UPLiFT to generative downstream tasks and show that it achieves competitive performance with state-of-the-art Coupled Flow Matching models for VAE feature upsampling. Altogether, UPLiFT offers a versatile and efficient approach to creating denser features.",
    "translation": "标题：UPLiFT：基于局部注意力机制的高效像素密集特征上采样方法\n\n摘要：任务无关的特征上采样领域已成为一个前景广阔的研究方向，旨在从预训练的视觉骨干网络中高效生成更密集的特征。这类方法通过学习将低分辨率特征映射至高分辨率版本，以较低成本实现密集特征提取，成为一种有效的捷径。早期研究多采用迭代上采样策略，而近期工作则转向基于交叉注意力的方法，但这些方法可能面临与其上采样骨干网络相似的效率扩展问题。本研究证明，迭代上采样方法仍可与基于交叉注意力的方法竞争，且能以更低推理成本实现最先进的性能。我们提出UPLiFT（通用像素密集轻量级特征变换架构），并设计了一种高效的局部注意力算子以克服现有迭代特征上采样方法的局限性。该算子采用完全局部化的注意力池化公式，使UPLiFT能在整个上采样过程中保持特征稳定性，从而以低于现有像素密集特征上采样器的推理成本达到最优性能。此外，我们将UPLiFT应用于生成式下游任务，结果表明其在VAE特征上采样任务中与最先进的耦合流匹配模型具有相当的性能。总体而言，UPLiFT为生成密集特征提供了一种通用且高效的解决方案。",
    "url": "https://huggingface.co/papers/2601.17950",
    "arxiv_url": "https://arxiv.org/abs/2601.17950"
  },
  {
    "title": "Training Reasoning Models on Saturated Problems via Failure-Prefix Conditioning",
    "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has substantially improved the reasoning abilities of large language models (LLMs), yet training often stalls as problems become saturated. We identify the core challenge as the poor accessibility of informative failures: learning signals exist but are rarely encountered during standard rollouts. To address this, we propose failure-prefix conditioning, a simple and effective method for learning from saturated problems. Rather than starting from the original question, our approach reallocates exploration by conditioning training on prefixes derived from rare incorrect reasoning trajectories, thereby exposing the model to failure-prone states. We observe that failure-prefix conditioning yields performance gains matching those of training on medium-difficulty problems, while preserving token efficiency. Furthermore, we analyze the model's robustness, finding that our method reduces performance degradation under misleading failure prefixes, albeit with a mild trade-off in adherence to correct early reasoning. Finally, we demonstrate that an iterative approach, which refreshes failure prefixes during training, unlocks additional gains after performance plateaus. Overall, our results suggest that failure-prefix conditioning offers an effective pathway to extend RLVR training on saturated problems.",
    "translation": "标题：基于失败前缀条件化的饱和问题推理模型训练\n\n摘要：基于可验证奖励的强化学习（RLVR）显著提升了大型语言模型（LLM）的推理能力，但随着问题趋于饱和，训练进程常陷入停滞。我们发现核心挑战在于信息性失败样本的可及性不足：学习信号虽然存在，但在标准训练过程中却极少被遇到。为解决此问题，我们提出失败前缀条件化方法——一种从饱和问题中学习的简单而有效的策略。该方法并非从原始问题开始训练，而是通过将训练过程条件化于从罕见错误推理轨迹中提取的前缀，从而重新分配探索资源，使模型暴露于易失败的状态。实验表明，失败前缀条件化带来的性能提升与在中等难度问题上训练的效果相当，同时保持了较高的标记效率。此外，我们分析了模型的鲁棒性，发现该方法能降低模型在误导性失败前缀下的性能衰减，尽管对早期正确推理的遵循程度略有折衷。最后，我们证明了一种在训练过程中动态更新失败前缀的迭代方法，能够在性能平台期后实现额外提升。总体而言，我们的研究结果表明，失败前缀条件化为在饱和问题上延续RLVR训练提供了一条有效路径。",
    "url": "https://huggingface.co/papers/2601.20829",
    "arxiv_url": "https://arxiv.org/abs/2601.20829"
  },
  {
    "title": "GDCNet: Generative Discrepancy Comparison Network for Multimodal Sarcasm Detection",
    "summary": "Multimodal sarcasm detection (MSD) aims to identify sarcasm within image-text pairs by modeling semantic incongruities across modalities. Existing methods often exploit cross-modal embedding misalignment to detect inconsistency but struggle when visual and textual content are loosely related or semantically indirect. While recent approaches leverage large language models (LLMs) to generate sarcastic cues, the inherent diversity and subjectivity of these generations often introduce noise. To address these limitations, we propose the Generative Discrepancy Comparison Network (GDCNet). This framework captures cross-modal conflicts by utilizing descriptive, factually grounded image captions generated by Multimodal LLMs (MLLMs) as stable semantic anchors. Specifically, GDCNet computes semantic and sentiment discrepancies between the generated objective description and the original text, alongside measuring visual-textual fidelity. These discrepancy features are then fused with visual and textual representations via a gated module to adaptively balance modality contributions. Extensive experiments on MSD benchmarks demonstrate GDCNet's superior accuracy and robustness, establishing a new state-of-the-art on the MMSD2.0 benchmark.",
    "translation": "标题：GDCNet：面向多模态讽刺检测的生成式差异对比网络\n\n摘要：多模态讽刺检测旨在通过建模跨模态语义不一致性来识别图文对中的讽刺内容。现有方法通常利用跨模态嵌入失配来检测不一致性，但当视觉与文本内容关联松散或语义间接时效果受限。尽管近期研究采用大语言模型生成讽刺线索，但这些生成结果固有的多样性和主观性常引入噪声。为应对这些局限性，本文提出生成式差异对比网络。该框架通过使用多模态大语言模型生成的描述性、事实依据型图像标题作为稳定语义锚点，以捕捉跨模态冲突。具体而言，GDCNet计算生成的目标描述与原始文本之间的语义及情感差异，同时测量视觉-文本保真度。这些差异特征通过门控模块与视觉、文本表征融合，以自适应平衡模态贡献。在多模态讽刺检测基准上的大量实验表明，GDCNet在准确性与鲁棒性方面均表现优异，在MMSD2.0基准上实现了最先进的性能。",
    "url": "https://huggingface.co/papers/2601.20618",
    "arxiv_url": "https://arxiv.org/abs/2601.20618"
  },
  {
    "title": "How AI Impacts Skill Formation",
    "summary": "AI assistance produces significant productivity gains across professional domains, particularly for novice workers. Yet how this assistance affects the development of skills required to effectively supervise AI remains unclear. Novice workers who rely heavily on AI to complete unfamiliar tasks may compromise their own skill acquisition in the process. We conduct randomized experiments to study how developers gained mastery of a new asynchronous programming library with and without the assistance of AI. We find that AI use impairs conceptual understanding, code reading, and debugging abilities, without delivering significant efficiency gains on average. Participants who fully delegated coding tasks showed some productivity improvements, but at the cost of learning the library. We identify six distinct AI interaction patterns, three of which involve cognitive engagement and preserve learning outcomes even when participants receive AI assistance. Our findings suggest that AI-enhanced productivity is not a shortcut to competence and AI assistance should be carefully adopted into workflows to preserve skill formation -- particularly in safety-critical domains.",
    "translation": "标题：人工智能如何影响技能形成\n\n摘要：人工智能辅助在各专业领域显著提升了生产效率，尤其对新手工作者而言。然而，这种辅助如何影响有效监督人工智能所需技能的发展仍不明确。新手工作者若过度依赖人工智能完成不熟悉的任务，可能会在此过程中削弱自身技能的习得。我们通过随机实验，研究开发者在有无人工智能辅助的情况下掌握新型异步编程库的差异。研究发现，使用人工智能会损害概念理解、代码阅读与调试能力，且平均未能带来显著的效率提升。完全委托编码任务的参与者虽表现出一定的生产效率改进，但这是以牺牲对编程库的学习为代价的。我们识别出六种不同的人工智能交互模式，其中三种涉及认知参与，即使在参与者接受人工智能辅助时仍能保持学习效果。我们的研究结果表明，人工智能提升的生产效率并非通往专业能力的捷径，在引入工作流程时应审慎采用人工智能辅助以保障技能形成——尤其是在安全关键领域。",
    "url": "https://huggingface.co/papers/2601.20245",
    "arxiv_url": "https://arxiv.org/abs/2601.20245"
  },
  {
    "title": "SE-DiCoW: Self-Enrolled Diarization-Conditioned Whisper",
    "summary": "Speaker-attributed automatic speech recognition (ASR) in multi-speaker environments remains a major challenge. While some approaches achieve strong performance when fine-tuned on specific domains, few systems generalize well across out-of-domain datasets. Our prior work, Diarization-Conditioned Whisper (DiCoW), leverages speaker diarization outputs as conditioning information and, with minimal fine-tuning, demonstrated strong multilingual and multi-domain performance. In this paper, we address a key limitation of DiCoW: ambiguity in Silence-Target-Non-target-Overlap (STNO) masks, where two or more fully overlapping speakers may have nearly identical conditioning despite differing transcriptions. We introduce SE-DiCoW (Self-Enrolled Diarization-Conditioned Whisper), which uses diarization output to locate an enrollment segment anywhere in the conversation where the target speaker is most active. This enrollment segment is used as fixed conditioning via cross-attention at each encoder layer. We further refine DiCoW with improved data segmentation, model initialization, and augmentation. Together, these advances yield substantial gains: SE-DiCoW reduces macro-averaged tcpWER by 52.4% relative to the original DiCoW on the EMMA MT-ASR benchmark.",
    "translation": "标题：SE-DiCoW：自注册的说话人日志条件化Whisper模型\n\n摘要：在多说话人环境中实现说话人归属的自动语音识别（ASR）仍是一项重大挑战。尽管部分方法在特定领域微调后能取得优异性能，但鲜有系统能在跨领域数据集上表现出良好的泛化能力。我们先前提出的说话人日志条件化Whisper模型（DiCoW）利用说话人日志输出作为条件信息，通过极少量微调即展现出强大的多语言与多领域性能。本文针对DiCoW的一个关键局限进行改进：静默-目标-非目标-重叠（STNO）掩码的模糊性问题——当两个或多个说话人完全重叠时，即使其转写内容不同，模型接收的条件信息也可能近乎相同。我们提出SE-DiCoW（自注册的说话人日志条件化Whisper模型），该模型通过说话人日志定位目标说话人最活跃的对话片段作为注册段，并在编码器各层通过交叉注意力机制将其作为固定条件信息注入。我们进一步通过改进数据分割策略、模型初始化方法和数据增强技术优化DiCoW框架。这些改进共同带来显著性能提升：在EMMA MT-ASR基准测试中，SE-DiCoW相较于原始DiCoW模型将宏观平均tcpWER指标相对降低了52.4%。",
    "url": "https://huggingface.co/papers/2601.19194",
    "arxiv_url": "https://arxiv.org/abs/2601.19194"
  },
  {
    "title": "FP8-RL: A Practical and Stable Low-Precision Stack for LLM Reinforcement Learning",
    "summary": "Reinforcement learning (RL) for large language models (LLMs) is increasingly bottlenecked by rollout (generation), where long output sequence lengths make attention and KV-cache memory dominate end-to-end step time. FP8 offers an attractive lever for accelerating RL by reducing compute cost and memory traffic during rollout, but applying FP8 in RL introduces unique engineering and algorithmic challenges: policy weights change every step (requiring repeated quantization and weight synchronization into the inference engine) and low-precision rollouts can deviate from the higher-precision policy assumed by the trainer, causing train-inference mismatch and potential instability. This report presents a practical FP8 rollout stack for LLM RL, implemented in the veRL ecosystem with support for common training backends (e.g., FSDP/Megatron-LM) and inference engines (e.g., vLLM/SGLang). We (i) enable FP8 W8A8 linear-layer rollout using blockwise FP8 quantization, (ii) extend FP8 to KV-cache to remove long-context memory bottlenecks via per-step QKV scale recalibration, and (iii) mitigate mismatch using importance-sampling-based rollout correction (token-level TIS/MIS variants). Across dense and MoE models, these techniques deliver up to 44% rollout throughput gains while preserving learning behavior comparable to BF16 baselines.",
    "translation": "标题：FP8-RL：一种实用且稳定的大语言模型强化学习低精度技术栈\n\n摘要：大语言模型（LLM）的强化学习（RL）正日益受限于生成阶段（rollout），其中长输出序列导致注意力机制与KV缓存内存成为端到端步骤时间的主要瓶颈。FP8（8位浮点数）通过降低生成阶段的计算成本和内存流量，为加速强化学习提供了极具吸引力的技术手段。然而，在强化学习中应用FP8引入了独特的工程与算法挑战：策略权重每一步都会更新（需要重复量化和向推理引擎同步权重），且低精度生成过程可能偏离训练器所假设的高精度策略，导致训练-推理不匹配及潜在的不稳定性。本报告提出了一种用于LLM强化学习的实用FP8生成技术栈，该技术栈在veRL生态系统中实现，并支持常见的训练后端（如FSDP/Megatron-LM）与推理引擎（如vLLM/SGLang）。我们实现了以下关键创新：（i）采用分块FP8量化技术，实现FP8 W8A8线性层的生成；（ii）将FP8扩展至KV缓存，通过每步QKV缩放因子重校准，消除长上下文内存瓶颈；（iii）引入基于重要性采样的生成校正方法（包括词元级TIS/MIS变体）以缓解不匹配问题。在稠密模型与混合专家模型上的实验表明，这些技术能够实现高达44%的生成吞吐量提升，同时保持与BF16基线相当的学习性能。",
    "url": "https://huggingface.co/papers/2601.18150",
    "arxiv_url": "https://arxiv.org/abs/2601.18150"
  },
  {
    "title": "Persona Prompting as a Lens on LLM Social Reasoning",
    "summary": "For socially sensitive tasks like hate speech detection, the quality of explanations from Large Language Models (LLMs) is crucial for factors like user trust and model alignment. While Persona prompting (PP) is increasingly used as a way to steer model towards user-specific generation, its effect on model rationales remains underexplored. We investigate how LLM-generated rationales vary when conditioned on different simulated demographic personas. Using datasets annotated with word-level rationales, we measure agreement with human annotations from different demographic groups, and assess the impact of PP on model bias and human alignment. Our evaluation across three LLMs results reveals three key findings: (1) PP improving classification on the most subjective task (hate speech) but degrading rationale quality. (2) Simulated personas fail to align with their real-world demographic counterparts, and high inter-persona agreement shows models are resistant to significant steering. (3) Models exhibit consistent demographic biases and a strong tendency to over-flag content as harmful, regardless of PP. Our findings reveal a critical trade-off: while PP can improve classification in socially-sensitive tasks, it often comes at the cost of rationale quality and fails to mitigate underlying biases, urging caution in its application.",
    "translation": "标题：角色提示作为大语言模型社会推理的观察视角\n\n摘要：在仇恨言论检测等社会敏感性任务中，大语言模型（LLMs）生成解释的质量对于用户信任和模型对齐等因素至关重要。虽然角色提示（PP）作为一种引导模型生成用户定制内容的方法正被广泛使用，但其对模型推理过程的影响尚未得到充分研究。本文通过模拟不同人口统计学角色，探究大语言模型生成推理依据的差异性。基于包含词语级标注依据的数据集，我们测量了模型与不同人口统计群体人工标注的一致性，并评估了角色提示对模型偏见和人类对齐的影响。通过对三种大语言模型的评估，我们得出三个关键发现：（1）角色提示在最具主观性的任务（仇恨言论检测）中提升了分类性能，但降低了推理依据的质量；（2）模拟角色未能与现实世界对应人口统计群体对齐，且高跨角色一致性表明模型对显著引导具有抵抗性；（3）无论是否使用角色提示，模型均表现出稳定的人口统计偏见，并存在过度标记内容为有害的强烈倾向。本研究揭示了一个关键权衡：虽然角色提示能提升社会敏感性任务的分类性能，但往往以牺牲推理质量为代价，且无法缓解深层偏见，这提示我们在应用该方法时需要保持审慎态度。",
    "url": "https://huggingface.co/papers/2601.20757",
    "arxiv_url": "https://arxiv.org/abs/2601.20757"
  },
  {
    "title": "SketchDynamics: Exploring Free-Form Sketches for Dynamic Intent Expression in Animation Generation",
    "summary": "Sketching provides an intuitive way to convey dynamic intent in animation authoring (i.e., how elements change over time and space), making it a natural medium for automatic content creation. Yet existing approaches often constrain sketches to fixed command tokens or predefined visual forms, overlooking their freeform nature and the central role of humans in shaping intention. To address this, we introduce an interaction paradigm where users convey dynamic intent to a vision-language model via free-form sketching, instantiated here in a sketch storyboard to motion graphics workflow. We implement an interface and improve it through a three-stage study with 24 participants. The study shows how sketches convey motion with minimal input, how their inherent ambiguity requires users to be involved for clarification, and how sketches can visually guide video refinement. Our findings reveal the potential of sketch and AI interaction to bridge the gap between intention and outcome, and demonstrate its applicability to 3D animation and video generation.",
    "translation": "标题：SketchDynamics：探索自由手绘草图在动画生成中的动态意图表达\n\n摘要：草图绘制为动画创作中传达动态意图（即元素如何随时间与空间变化）提供了一种直观方式，使其成为自动化内容创作的天然媒介。然而，现有方法通常将草图限制为固定的指令标记或预定义的视觉形式，忽视了其自由形式的本质以及人在塑造意图中的核心作用。为此，我们提出一种交互范式：用户通过自由手绘草图向视觉-语言模型传达动态意图，并在此以草图故事板到动态图形的工作流程进行实例化。我们实现了一个交互界面，并通过一项包含24名参与者的三阶段研究对其进行了改进。研究展示了草图如何以最简输入传达运动信息，其固有的模糊性如何需要用户参与澄清，以及草图如何能在视觉上引导视频优化。我们的研究结果揭示了草图与人工智能交互在弥合意图与结果之间差距的潜力，并论证了其在三维动画与视频生成领域的适用性。",
    "url": "https://huggingface.co/papers/2601.20622",
    "arxiv_url": "https://arxiv.org/abs/2601.20622"
  },
  {
    "title": "Shallow-π: Knowledge Distillation for Flow-based VLAs",
    "summary": "The growing demand for real-time robotic deployment necessitates fast and on-device inference for vision-language-action (VLA) models. Within the VLA literature, efficiency has been extensively studied at the token level, such as visual token pruning. In contrast, systematic transformer layer reduction has received limited attention and, to the best of our knowledge, has not been explored for flow-based VLA models under knowledge distillation. In this work, we propose Shallow-pi, a principled knowledge distillation framework that aggressively reduces the transformer depth of both the VLM backbone and the flow-based action head, compressing the model from 18 to 6 layers. Shallow-pi achieves over two times faster inference with less than one percent absolute drop in success rate on standard manipulation benchmarks, establishing state-of-the-art performance among reduced VLA models. Crucially, we validate our approach through industrial-scale real-world experiments on Jetson Orin and Jetson Thor across multiple robot platforms, including humanoid systems, in complex and dynamic manipulation scenarios.",
    "translation": "标题：Shallow-π：基于流式视觉语言动作模型的知识蒸馏方法\n\n摘要：机器人实时部署需求的日益增长，对视觉-语言-动作（VLA）模型提出了快速设备端推理的要求。现有VLA研究主要在令牌层面（如视觉令牌剪枝）对效率优化进行了广泛探索，而系统性的Transformer层数缩减研究则相对有限。据我们所知，在知识蒸馏框架下对基于流式预测的VLA模型进行深度压缩的研究尚未开展。本研究提出Shallow-π——一种基于知识蒸馏原理的模型压缩框架，通过大幅削减VLM主干网络与流式动作预测头的Transformer层数，将模型总层数从18层压缩至6层。该方法在标准操作基准测试中实现了超过两倍的推理加速，成功率绝对下降幅度不足1%，在压缩型VLA模型中达到了最优性能。关键的是，我们在Jetson Orin和Jetson Thor边缘计算平台上，通过多机器人平台（包括人形机器人系统）在复杂动态操作场景中的工业级真实环境实验，验证了该方法的有效性。",
    "url": "https://huggingface.co/papers/2601.20262",
    "arxiv_url": "https://arxiv.org/abs/2601.20262"
  }
]