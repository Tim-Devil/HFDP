[
  {
    "title": "Latent Implicit Visual Reasoning",
    "summary": "While Large Multimodal Models (LMMs) have made significant progress, they remain largely text-centric, relying on language as their core reasoning modality. As a result, they are limited in their ability to handle reasoning tasks that are predominantly visual. Recent approaches have sought to address this by supervising intermediate visual steps with helper images, depth maps, or image crops. However, these strategies impose restrictive priors on what \"useful\" visual abstractions look like, add heavy annotation costs, and struggle to generalize across tasks. To address this critical limitation, we propose a task-agnostic mechanism that trains LMMs to discover and use visual reasoning tokens without explicit supervision. These tokens attend globally and re-encode the image in a task-adaptive way, enabling the model to extract relevant visual information without hand-crafted supervision. Our approach outperforms direct fine-tuning and achieves state-of-the-art results on a diverse range of vision-centric tasks -- including those where intermediate abstractions are hard to specify -- while also generalizing to multi-task instruction tuning.",
    "translation": "标题：潜在隐式视觉推理\n\n摘要：尽管大型多模态模型已取得显著进展，但其本质上仍以文本为中心，依赖语言作为核心推理模态。这导致其在处理以视觉为主的推理任务时存在明显局限。近期研究尝试通过辅助图像、深度图或图像裁剪来监督中间视觉步骤以解决该问题，但这些策略对“有效”视觉抽象形式施加了限制性先验，增加了大量标注成本，且难以实现跨任务泛化。为突破这一关键局限，本文提出一种任务无关机制，通过无显式监督的方式训练大型多模态模型自主发现并利用视觉推理标记。这些标记通过全局注意力机制以任务自适应方式对图像进行重编码，使模型无需人工标注即可提取相关视觉信息。实验表明，该方法在多种以视觉为中心的任务上——包括难以定义中间抽象层的任务——均优于直接微调方法，取得最先进的性能表现，同时展现出多任务指令调优的泛化能力。",
    "url": "https://huggingface.co/papers/2512.21218",
    "arxiv_url": "https://arxiv.org/abs/2512.21218"
  },
  {
    "title": "Emergent temporal abstractions in autoregressive models enable hierarchical reinforcement learning",
    "summary": "Large-scale autoregressive models pretrained on next-token prediction and finetuned with reinforcement learning (RL) have achieved unprecedented success on many problem domains. During RL, these models explore by generating new outputs, one token at a time. However, sampling actions token-by-token can result in highly inefficient learning, particularly when rewards are sparse. Here, we show that it is possible to overcome this problem by acting and exploring within the internal representations of an autoregressive model. Specifically, to discover temporally-abstract actions, we introduce a higher-order, non-causal sequence model whose outputs control the residual stream activations of a base autoregressive model. On grid world and MuJoCo-based tasks with hierarchical structure, we find that the higher-order model learns to compress long activation sequence chunks onto internal controllers. Critically, each controller executes a sequence of behaviorally meaningful actions that unfold over long timescales and are accompanied with a learned termination condition, such that composing multiple controllers over time leads to efficient exploration on novel tasks. We show that direct internal controller reinforcement, a process we term \"internal RL\", enables learning from sparse rewards in cases where standard RL finetuning fails. Our results demonstrate the benefits of latent action generation and reinforcement in autoregressive models, suggesting internal RL as a promising avenue for realizing hierarchical RL within foundation models.",
    "translation": "标题：自回归模型中的涌现时间抽象实现分层强化学习\n\n摘要：基于下一标记预测进行预训练、并通过强化学习微调的大规模自回归模型已在众多问题领域取得前所未有的成功。在强化学习过程中，这些模型通过逐标记生成新输出来进行探索。然而，这种逐标记采样的行动方式可能导致学习效率低下，尤其在奖励稀疏的场景中。本文研究表明，通过在自回归模型的内部表征空间中进行行动与探索，可以克服这一问题。具体而言，为发现时间抽象动作，我们引入了一个高阶非因果序列模型，其输出可控制基础自回归模型的残差流激活状态。在具有层次结构的网格世界与MuJoCo基准任务中，高阶模型成功将长激活序列块压缩至内部控制器。关键的是，每个控制器能够执行在长时间尺度上展开、具有行为意义的行为序列，并附带学习得到的终止条件，使得多个控制器在时间维度上的组合能够在新任务中实现高效探索。我们证明，通过直接对内部控制器进行强化（这一过程我们称之为“内部强化学习”），能够在标准强化学习微调失效的稀疏奖励场景中实现有效学习。本研究结果揭示了自回归模型中潜在动作生成与强化的优势，表明内部强化学习为实现基础模型中的分层强化学习提供了可行路径。",
    "url": "https://huggingface.co/papers/2512.20605",
    "arxiv_url": "https://arxiv.org/abs/2512.20605"
  },
  {
    "title": "Spatia: Video Generation with Updatable Spatial Memory",
    "summary": "Existing video generation models struggle to maintain long-term spatial and temporal consistency due to the dense, high-dimensional nature of video signals. To overcome this limitation, we propose Spatia, a spatial memory-aware video generation framework that explicitly preserves a 3D scene point cloud as persistent spatial memory. Spatia iteratively generates video clips conditioned on this spatial memory and continuously updates it through visual SLAM. This dynamic-static disentanglement design enhances spatial consistency throughout the generation process while preserving the model's ability to produce realistic dynamic entities. Furthermore, Spatia enables applications such as explicit camera control and 3D-aware interactive editing, providing a geometrically grounded framework for scalable, memory-driven video generation.",
    "translation": "标题：Spatia：基于可更新空间记忆的视频生成方法\n\n摘要：现有视频生成模型因视频信号具有密集、高维的特性，难以维持长期的空间与时间一致性。为突破此限制，本文提出Spatia——一种空间记忆感知的视频生成框架，其显式地将三维场景点云作为持久性空间记忆进行维护。Spatia基于该空间记忆迭代生成视频片段，并通过视觉SLAM技术持续更新记忆内容。这种动态-静态解耦的设计增强了生成过程中的空间一致性，同时保持了模型生成逼真动态实体的能力。此外，Spatia支持显式相机控制与三维感知交互编辑等应用，为可扩展、记忆驱动的视频生成提供了几何基础扎实的框架。",
    "url": "https://huggingface.co/papers/2512.15716",
    "arxiv_url": "https://arxiv.org/abs/2512.15716"
  },
  {
    "title": "Schoenfeld's Anatomy of Mathematical Reasoning by Language Models",
    "summary": "Large language models increasingly expose reasoning traces, yet their underlying cognitive structure and steps remain difficult to identify and analyze beyond surface-level statistics. We adopt Schoenfeld's Episode Theory as an inductive, intermediate-scale lens and introduce ThinkARM (Anatomy of Reasoning in Models), a scalable framework that explicitly abstracts reasoning traces into functional reasoning steps such as Analysis, Explore, Implement, Verify, etc. When applied to mathematical problem solving by diverse models, this abstraction reveals reproducible thinking dynamics and structural differences between reasoning and non-reasoning models, which are not apparent from token-level views. We further present two diagnostic case studies showing that exploration functions as a critical branching step associated with correctness, and that efficiency-oriented methods selectively suppress evaluative feedback steps rather than uniformly shortening responses. Together, our results demonstrate that episode-level representations make reasoning steps explicit, enabling systematic analysis of how reasoning is structured, stabilized, and altered in modern language models.",
    "translation": "标题：语言模型数学推理的舍恩菲尔德解剖学\n\n摘要：大型语言模型日益展现出推理轨迹，然而超越表层统计数据，其底层的认知结构与步骤仍难以识别与分析。本研究采用舍恩菲尔德的片段理论作为归纳性、中观尺度的分析视角，并引入ThinkARM（模型推理解剖学）这一可扩展框架，该框架能够将推理轨迹显式抽象为功能性推理步骤，如分析、探索、实施、验证等。当将此框架应用于不同模型的数学问题求解时，这种抽象揭示了可复现的思维动态，以及推理模型与非推理模型之间的结构性差异，这些差异在词元层面的视角下并不明显。我们进一步呈现了两个诊断性案例研究，表明探索功能是影响正确性的关键分支步骤，而面向效率的方法会有选择性地抑制评估反馈步骤，而非均匀地缩短响应长度。综上所述，我们的研究结果表明，片段层面的表征使推理步骤显性化，从而能够系统分析现代语言模型中推理的结构化、稳定化与改变方式。",
    "url": "https://huggingface.co/papers/2512.19995",
    "arxiv_url": "https://arxiv.org/abs/2512.19995"
  },
  {
    "title": "How Much 3D Do Video Foundation Models Encode?",
    "summary": "Videos are continuous 2D projections of 3D worlds. After training on large video data, will global 3D understanding naturally emerge? We study this by quantifying the 3D understanding of existing Video Foundation Models (VidFMs) pretrained on vast video data. We propose the first model-agnostic framework that measures the 3D awareness of various VidFMs by estimating multiple 3D properties from their features via shallow read-outs. Our study presents meaningful findings regarding the 3D awareness of VidFMs on multiple axes. In particular, we show that state-of-the-art video generation models exhibit a strong understanding of 3D objects and scenes, despite not being trained on any 3D data. Such understanding can even surpass that of large expert models specifically trained for 3D tasks. Our findings, together with the 3D benchmarking of major VidFMs, provide valuable observations for building scalable 3D models.",
    "translation": "标题：视频基础模型编码了多少三维信息？\n\n摘要：视频是三维世界在二维平面上的连续投影。在大规模视频数据上训练后，全局三维理解能力是否会自然涌现？我们通过量化现有视频基础模型在大量视频数据预训练后所获得的三维理解能力来研究这一问题。我们提出了首个模型无关的评估框架，该框架通过浅层读出器从模型特征中估计多种三维属性，从而系统衡量各类视频基础模型的三维感知能力。我们的研究在多个维度上揭示了视频基础模型三维感知能力的重要发现。特别值得注意的是，研究显示当前最先进的视频生成模型虽未经过任何三维数据训练，却展现出对三维物体与场景的深刻理解能力，其表现甚至可能超越专门针对三维任务训练的大型专家模型。本研究所得结论，结合对主流视频基础模型的三维基准测试结果，为构建可扩展的三维模型提供了重要参考。\n\n请按照以下格式返回：\n标题：视频基础模型编码了多少三维信息？\n摘要：视频是三维世界在二维平面上的连续投影。在大规模视频数据上训练后，全局三维理解能力是否会自然涌现？我们通过量化现有视频基础模型在大量视频数据预训练后所获得的三维理解能力来研究这一问题。我们提出了首个模型无关的评估框架，该框架通过浅层读出器从模型特征中估计多种三维属性，从而系统衡量各类视频基础模型的三维感知能力。我们的研究在多个维度上揭示了视频基础模型三维感知能力的重要发现。特别值得注意的是，研究显示当前最先进的视频生成模型虽未经过任何三维数据训练，却展现出对三维物体与场景的深刻理解能力，其表现甚至可能超越专门针对三维任务训练的大型专家模型。本研究所得结论，结合对主流视频基础模型的三维基准测试结果，为构建可扩展的三维模型提供了重要参考。",
    "url": "https://huggingface.co/papers/2512.19949",
    "arxiv_url": "https://arxiv.org/abs/2512.19949"
  },
  {
    "title": "VA-π: Variational Policy Alignment for Pixel-Aware Autoregressive Generation",
    "summary": "Autoregressive (AR) visual generation relies on tokenizers to map images to and from discrete sequences. However, tokenizers are trained to reconstruct clean images from ground-truth tokens, while AR generators are optimized only for token likelihood. This misalignment leads to generated token sequences that may decode into low-quality images, without direct supervision from the pixel space. We propose VA-π, a lightweight post-training framework that directly optimizes AR models with a principled pixel-space objective. VA-π formulates the generator-tokenizer alignment as a variational optimization, deriving an evidence lower bound (ELBO) that unifies pixel reconstruction and autoregressive modeling. To optimize under the discrete token space, VA-π introduces a reinforcement-based alignment strategy that treats the AR generator as a policy, uses pixel-space reconstruction quality as its intrinsic reward. The reward is measured by how well the predicted token sequences can reconstruct the original image under teacher forcing, giving the model direct pixel-level guidance without expensive free-running sampling. The regularization term of the ELBO serves as a natural regularizer, maintaining distributional consistency of tokens. VA-π enables rapid adaptation of existing AR generators, without neither tokenizer retraining nor external reward models. With only 1% ImageNet-1K data and 25 minutes of tuning, it reduces FID from 14.36 to 7.65 and improves IS from 86.55 to 116.70 on LlamaGen-XXL, while also yielding notable gains in the text-to-image task on GenEval for both visual generation model (LlamaGen: from 0.306 to 0.339) and unified multi-modal model (Janus-Pro: from 0.725 to 0.744). Code is available at https://github.com/Lil-Shake/VA-Pi.",
    "translation": "标题：VA-π：面向像素感知自回归生成的可变分策略对齐方法\n\n摘要：自回归视觉生成模型依赖于分词器将图像与离散序列进行相互映射。然而，分词器的训练目标是从真实标记重建清晰图像，而自回归生成器仅针对标记似然性进行优化。这种目标错位导致生成的标记序列可能解码为低质量图像，且缺乏来自像素空间的直接监督。本文提出VA-π——一种轻量级的训练后优化框架，通过基于原理的像素空间目标直接优化自回归模型。VA-π将生成器与分词器的对齐问题构建为变分优化，推导出统一像素重建与自回归建模的证据下界。为在离散标记空间中进行优化，VA-π引入基于强化学习的对齐策略：将自回归生成器视为策略网络，以像素空间重建质量作为内在奖励。该奖励通过预测标记序列在教师强制条件下重建原始图像的能力进行度量，使模型获得无需昂贵自由运行采样的像素级直接指导。证据下界中的正则化项作为天然正则器，保持标记的分布一致性。VA-π能够快速适配现有自回归生成器，既无需重新训练分词器，也不依赖外部奖励模型。仅使用1%的ImageNet-1K数据及25分钟微调，即在LlamaGen-XXL模型上将FID从14.36降至7.65，IS从86.55提升至116.70；同时在GenEval文本到图像任务中，视觉生成模型（LlamaGen：从0.306至0.339）与统一多模态模型（Janus-Pro：从0.725至0.744）均取得显著提升。代码发布于https://github.com/Lil-Shake/VA-Pi。",
    "url": "https://huggingface.co/papers/2512.19680",
    "arxiv_url": "https://arxiv.org/abs/2512.19680"
  },
  {
    "title": "GTR-Turbo: Merged Checkpoint is Secretly a Free Teacher for Agentic VLM Training",
    "summary": "Multi-turn reinforcement learning (RL) for multi-modal agents built upon vision-language models (VLMs) is hampered by sparse rewards and long-horizon credit assignment. Recent methods densify the reward by querying a teacher that provides step-level feedback, e.g., Guided Thought Reinforcement (GTR) and On-Policy Distillation, but rely on costly, often privileged models as the teacher, limiting practicality and reproducibility. We introduce GTR-Turbo, a highly efficient upgrade to GTR, which matches the performance without training or querying an expensive teacher model. Specifically, GTR-Turbo merges the weights of checkpoints produced during the ongoing RL training, and then uses this merged model as a \"free\" teacher to guide the subsequent RL via supervised fine-tuning or soft logit distillation. This design removes dependence on privileged VLMs (e.g., GPT or Gemini), mitigates the \"entropy collapse\" observed in prior work, and keeps training stable. Across diverse visual agentic tasks, GTR-Turbo improves the accuracy of the baseline model by 10-30% while reducing wall-clock training time by 50% and compute cost by 60% relative to GTR.",
    "translation": "标题：GTR-Turbo：融合检查点在智能视觉语言模型训练中悄然成为免费教师\n\n摘要：基于视觉语言模型构建的多模态智能体在进行多轮强化学习时，常受稀疏奖励与长程信用分配问题的制约。近期方法通过引入教师模型提供步骤级反馈以稠密化奖励信号，例如引导思维强化与同策略蒸馏，但这些方法依赖昂贵且通常具有特权访问权限的教师模型，限制了其实用性与可复现性。本文提出GTR-Turbo，作为GTR的高效升级版本，在无需训练或调用昂贵教师模型的情况下实现了同等性能。具体而言，GTR-Turbo将在持续强化学习训练过程中产生的检查点权重进行融合，随后将该融合模型作为“免费”教师，通过监督微调或软对数蒸馏引导后续强化学习。该设计消除了对特权视觉语言模型的依赖，缓解了先前工作中观察到的“熵崩塌”现象，并保持了训练稳定性。在多种视觉智能任务中，相较于GTR方法，GTR-Turbo将基线模型准确率提升10-30%，同时减少50%的实际训练时间与60%的计算成本。",
    "url": "https://huggingface.co/papers/2512.13043",
    "arxiv_url": "https://arxiv.org/abs/2512.13043"
  }
]