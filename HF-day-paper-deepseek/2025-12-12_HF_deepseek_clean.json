[
  {
    "title": "T-pro 2.0: An Efficient Russian Hybrid-Reasoning Model and Playground",
    "summary": "We introduce T-pro 2.0, an open-weight Russian LLM for hybrid reasoning and efficient inference. The model supports direct answering and reasoning-trace generation, using a Cyrillic-dense tokenizer and an adapted EAGLE speculative-decoding pipeline to reduce latency. To enable reproducible and extensible research, we release the model weights, the T-Wix 500k instruction corpus, the T-Math reasoning benchmark, and the EAGLE weights on Hugging Face. These resources allow users to study Russian-language reasoning and to extend or adapt both the model and the inference pipeline. A public web demo exposes reasoning and non-reasoning modes and illustrates the speedups achieved by our inference stack across domains. T-pro 2.0 thus serves as an accessible open system for building and evaluating efficient, practical Russian LLM applications.",
    "translation": "标题：T-pro 2.0：一种高效的俄语混合推理模型与实验平台\n\n摘要：本文介绍T-pro 2.0，一个用于混合推理与高效推理的开源权重俄语大语言模型。该模型支持直接回答与推理轨迹生成，通过采用西里尔字母密集型分词器及适配的EAGLE推测解码流水线以降低推理延迟。为促进可复现与可扩展的研究，我们在Hugging Face平台公开了模型权重、T-Wix 50万条指令数据集、T-Math推理基准测试集及EAGLE权重。这些资源使用户能够深入研究俄语推理任务，并可对模型及推理流水线进行扩展与适配。公开的网页演示展示了推理与非推理两种模式，并呈现了我们的推理架构在多领域实现的加速效果。T-pro 2.0由此成为一个易于使用的开放系统，可用于构建和评估高效、实用的俄语大语言模型应用。",
    "url": "https://huggingface.co/papers/2512.10430",
    "arxiv_url": "https://arxiv.org/abs/2512.10430"
  },
  {
    "title": "Long-horizon Reasoning Agent for Olympiad-Level Mathematical Problem Solving",
    "summary": "Large language models (LLMs) have achieved significant progress in solving complex reasoning tasks by Reinforcement Learning with Verifiable Rewards (RLVR). This advancement is also inseparable from the oversight automated by reliable verifiers. However, current outcome-based verifiers (OVs) are unable to inspect the unreliable intermediate steps in the long reasoning chains of thought (CoTs). Meanwhile, current process-based verifiers (PVs) have difficulties in reliably detecting errors in the complex long CoTs, limited by the scarcity of high-quality annotations due to the prohibitive costs of human annotations. Therefore, we propose the Outcome-based Process Verifier (OPV), which verifies the rationale process of summarized outcomes from long CoTs to achieve both accurate and efficient verification and enable large-scale annotation. To empower the proposed verifier, we adopt an iterative active learning framework with expert annotations to progressively improve the verification capability of OPV with fewer annotation costs. Specifically, in each iteration, the most uncertain cases of the current best OPV are annotated and then subsequently used to train a new OPV through Rejection Fine-Tuning (RFT) and RLVR for the next round. Extensive experiments demonstrate OPV's superior performance and broad applicability. It achieves new state-of-the-art results on our held-out \\thisbench, outperforming much larger open-source models such as Qwen3-Max-Preview with an F1 score of 83.1 compared to 76.3. Furthermore, OPV effectively detects false positives within synthetic dataset, closely align with expert assessment. When collaborating with policy models, OPV consistently yields performance gains, e.g., raising the accuracy of DeepSeek-R1-Distill-Qwen-32B from 55.2\\% to 73.3\\% on AIME2025 as the compute budget scales.",
    "translation": "标题：面向奥赛级数学问题求解的长程推理智能体\n\n摘要：大型语言模型（LLM）通过基于可验证奖励的强化学习（RLVR）在解决复杂推理任务方面取得了显著进展。这一进步同样离不开可靠验证器提供的自动化监督。然而，当前基于结果的验证器（OV）无法有效检测长链思维推理（CoT）中不可靠的中间步骤。同时，现有基于过程的验证器（PV）受限于人工标注成本高昂导致的高质量标注数据稀缺，难以可靠地识别复杂长链CoT中的错误。为此，我们提出基于结果的过程验证器（OPV），该验证器通过检验长链CoT中总结性结果的推理过程，实现精准高效的验证并支持大规模标注。为增强该验证器的能力，我们采用结合专家标注的迭代式主动学习框架，以较低标注成本逐步提升OPV的验证性能。具体而言，在每轮迭代中，当前最优OPV预测最不确定的案例将由专家标注，随后通过拒绝微调（RFT）和RLVR训练新一代OPV用于后续轮次。大量实验证明OPV具有卓越性能与广泛适用性：在内部基准测试\\thisbench 中取得最新最优结果，以83.1的F1分数超越Qwen3-Max-Preview等更大规模开源模型（其F1分数为76.3）；同时，OPV能有效检测合成数据集中的误判案例，其判断与专家评估高度一致。在与策略模型协同工作时，OPV持续带来性能提升，例如在计算资源扩展条件下，将DeepSeek-R1-Distill-Qwen-32B在AIME2025上的准确率从55.2%提升至73.3%。",
    "url": "https://huggingface.co/papers/2512.10739",
    "arxiv_url": "https://arxiv.org/abs/2512.10739"
  },
  {
    "title": "Are We Ready for RL in Text-to-3D Generation? A Progressive Investigation",
    "summary": "Reinforcement learning (RL), earlier proven to be effective in large language and multi-modal models, has been successfully extended to enhance 2D image generation recently. However, applying RL to 3D generation remains largely unexplored due to the higher spatial complexity of 3D objects, which require globally consistent geometry and fine-grained local textures. This makes 3D generation significantly sensitive to reward designs and RL algorithms. To address these challenges, we conduct the first systematic study of RL for text-to-3D autoregressive generation across several dimensions. (1) Reward designs: We evaluate reward dimensions and model choices, showing that alignment with human preference is crucial, and that general multi-modal models provide robust signal for 3D attributes. (2) RL algorithms: We study GRPO variants, highlighting the effectiveness of token-level optimization, and further investigate the scaling of training data and iterations. (3) Text-to-3D Benchmarks: Since existing benchmarks fail to measure implicit reasoning abilities in 3D generation models, we introduce MME-3DR. (4) Advanced RL paradigms: Motivated by the natural hierarchy of 3D generation, we propose Hi-GRPO, which optimizes the global-to-local hierarchical 3D generation through dedicated reward ensembles. Based on these insights, we develop AR3D-R1, the first RL-enhanced text-to-3D model, expert from coarse shape to texture refinement. We hope this study provides insights into RL-driven reasoning for 3D generation. Code is released at https://github.com/Ivan-Tang-3D/3DGen-R1.",
    "translation": "标题：文本到三维生成是否已为强化学习做好准备？一项渐进式探究\n\n摘要：强化学习（RL）先前已被证明在大语言模型和多模态模型中具有显著效果，近期已成功扩展至增强二维图像生成领域。然而，由于三维对象具有更高的空间复杂性——需要全局一致的几何结构与细粒度局部纹理，将强化学习应用于三维生成的研究仍处于探索阶段。这使得三维生成对奖励设计与强化学习算法尤为敏感。为应对这些挑战，我们首次从多个维度对文本到三维自回归生成中的强化学习进行了系统性研究：（1）奖励设计：我们评估了奖励维度与模型选择，证明与人类偏好对齐至关重要，且通用多模态模型能为三维属性提供稳健的信号反馈；（2）强化学习算法：我们研究了GRPO算法的变体，突显了词元级优化的有效性，并进一步探究了训练数据与迭代次数的规模化影响；（3）文本到三维基准测试：针对现有基准无法有效衡量三维生成模型隐含推理能力的问题，我们提出了MME-3DR基准；（4）先进强化学习范式：基于三维生成天然的层次性结构，我们提出Hi-GRPO方法，通过定制化奖励组合优化从全局到局部的层次化三维生成。基于以上发现，我们开发了AR3D-R1模型——首个通过强化学习增强的文本到三维生成系统，实现了从粗糙形状到纹理细节的专家级优化。本研究旨在为强化学习驱动的三维生成推理提供理论洞见与实践参考。代码已发布于 https://github.com/Ivan-Tang-3D/3DGen-R1。",
    "url": "https://huggingface.co/papers/2512.10949",
    "arxiv_url": "https://arxiv.org/abs/2512.10949"
  },
  {
    "title": "OPV: Outcome-based Process Verifier for Efficient Long Chain-of-Thought Verification",
    "summary": "Large language models (LLMs) have achieved significant progress in solving complex reasoning tasks by Reinforcement Learning with Verifiable Rewards (RLVR). This advancement is also inseparable from the oversight automated by reliable verifiers. However, current outcome-based verifiers (OVs) are unable to inspect the unreliable intermediate steps in the long reasoning chains of thought (CoTs). Meanwhile, current process-based verifiers (PVs) have difficulties in reliably detecting errors in the complex long CoTs, limited by the scarcity of high-quality annotations due to the prohibitive costs of human annotations. Therefore, we propose the Outcome-based Process Verifier (OPV), which verifies the rationale process of summarized outcomes from long CoTs to achieve both accurate and efficient verification and enable large-scale annotation. To empower the proposed verifier, we adopt an iterative active learning framework with expert annotations to progressively improve the verification capability of OPV with fewer annotation costs. Specifically, in each iteration, the most uncertain cases of the current best OPV are annotated and then subsequently used to train a new OPV through Rejection Fine-Tuning (RFT) and RLVR for the next round. Extensive experiments demonstrate OPV's superior performance and broad applicability. It achieves new state-of-the-art results on our held-out OPV-Bench, outperforming much larger open-source models such as Qwen3-Max-Preview with an F1 score of 83.1 compared to 76.3. Furthermore, OPV effectively detects false positives within synthetic dataset, closely align with expert assessment. When collaborating with policy models, OPV consistently yields performance gains, e.g., raising the accuracy of DeepSeek-R1-Distill-Qwen-32B from 55.2% to 73.3% on AIME2025 as the compute budget scales.",
    "translation": "标题：OPV：基于结果的过程验证器——实现高效长链思维验证的新方法\n\n摘要：大型语言模型（LLM）通过可验证奖励的强化学习（RLVR）在解决复杂推理任务方面取得了显著进展，这一进步同样离不开可靠验证器的自动化监督。然而，当前基于结果的验证器（OV）难以审查长链思维（CoT）推理中不可靠的中间步骤；而基于过程的验证器（PV）由于人工标注成本高昂导致高质量标注稀缺，难以可靠地检测复杂长链思维中的错误。为此，我们提出基于结果的过程验证器（OPV），该方法通过验证长链思维中总结性结果的推理过程，实现精准高效的验证并支持大规模标注。为增强该验证器的能力，我们采用结合专家标注的迭代式主动学习框架，以较低标注成本逐步提升OPV的验证性能。具体而言，在每轮迭代中，当前最优OPV预测最不确定的案例由专家标注，随后通过拒绝微调（RFT）和RLVR训练新一代OPV用于下一轮验证。大量实验证明了OPV的卓越性能与广泛适用性：在自建的OPV-Bench测试集上取得最新最优结果，F1分数达83.1，显著优于Qwen3-Max-Preview等更大规模开源模型（76.3分）；同时，OPV能有效检测合成数据集中的误报案例，其判断与专家评估高度一致。在与策略模型协同工作时，OPV持续带来性能提升，例如在AIME2025任务中，随着计算预算增加，成功将DeepSeek-R1-Distill-Qwen-32B的准确率从55.2%提升至73.3%。",
    "url": "https://huggingface.co/papers/2512.10756",
    "arxiv_url": "https://arxiv.org/abs/2512.10756"
  },
  {
    "title": "Achieving Olympia-Level Geometry Large Language Model Agent via Complexity Boosting Reinforcement Learning",
    "summary": "Large language model (LLM) agents exhibit strong mathematical problem-solving abilities and can even solve International Mathematical Olympiad (IMO) level problems with the assistance of formal proof systems. However, due to weak heuristics for auxiliary constructions, AI for geometry problem solving remains dominated by expert models such as AlphaGeometry 2, which rely heavily on large-scale data synthesis and search for both training and evaluation. In this work, we make the first attempt to build a medalist-level LLM agent for geometry and present InternGeometry. InternGeometry overcomes the heuristic limitations in geometry by iteratively proposing propositions and auxiliary constructions, verifying them with a symbolic engine, and reflecting on the engine's feedback to guide subsequent proposals. A dynamic memory mechanism enables InternGeometry to conduct more than two hundred interactions with the symbolic engine per problem. To further accelerate learning, we introduce Complexity-Boosting Reinforcement Learning (CBRL), which gradually increases the complexity of synthesized problems across training stages. Built on InternThinker-32B, InternGeometry solves 44 of 50 IMO geometry problems (2000-2024), exceeding the average gold medalist score (40.9), using only 13K training examples, just 0.004% of the data used by AlphaGeometry 2, demonstrating the potential of LLM agents on expert-level geometry tasks. InternGeometry can also propose novel auxiliary constructions for IMO problems that do not appear in human solutions. We will release the model, data, and symbolic engine to support future research.",
    "translation": "标题：通过复杂度提升强化学习实现奥林匹克水平的几何大语言模型智能体\n\n摘要：大语言模型智能体展现出强大的数学问题解决能力，甚至能够在形式化证明系统的辅助下解决国际数学奥林匹克竞赛级别的几何问题。然而，由于在辅助构造方面的启发式能力较弱，几何问题求解领域仍由专家模型主导，例如AlphaGeometry 2，其训练和评估严重依赖大规模数据合成与搜索。本研究首次尝试构建具有奖牌获得者水平的几何大语言模型智能体，提出了InternGeometry。该模型通过迭代提出命题与辅助构造、使用符号引擎进行验证，并基于引擎反馈指导后续提议，从而克服了几何问题中的启发式局限。动态记忆机制使InternGeometry能够针对每个问题与符号引擎进行超过两百次交互。为进一步加速学习，我们提出了复杂度提升强化学习方法，通过在训练阶段逐步增加合成问题的复杂度来优化模型性能。基于InternThinker-32B构建的InternGeometry，在仅使用13K训练样本（仅为AlphaGeometry 2数据量的0.004%）的情况下，成功解决了2000年至2024年50道国际数学奥林匹克几何问题中的44道，超过了金牌获得者的平均得分（40.9分），这证明了大语言模型智能体在专家级几何任务上的潜力。此外，InternGeometry能够针对人类解答中未出现的国际数学奥林匹克问题提出新颖的辅助构造方案。我们将公开模型、数据及符号引擎以支持后续研究。",
    "url": "https://huggingface.co/papers/2512.10534",
    "arxiv_url": "https://arxiv.org/abs/2512.10534"
  },
  {
    "title": "MoCapAnything: Unified 3D Motion Capture for Arbitrary Skeletons from Monocular Videos",
    "summary": "Motion capture now underpins content creation far beyond digital humans, yet most existing pipelines remain species- or template-specific. We formalize this gap as Category-Agnostic Motion Capture (CAMoCap): given a monocular video and an arbitrary rigged 3D asset as a prompt, the goal is to reconstruct a rotation-based animation such as BVH that directly drives the specific asset. We present MoCapAnything, a reference-guided, factorized framework that first predicts 3D joint trajectories and then recovers asset-specific rotations via constraint-aware inverse kinematics. The system contains three learnable modules and a lightweight IK stage: (1) a Reference Prompt Encoder that extracts per-joint queries from the asset's skeleton, mesh, and rendered images; (2) a Video Feature Extractor that computes dense visual descriptors and reconstructs a coarse 4D deforming mesh to bridge the gap between video and joint space; and (3) a Unified Motion Decoder that fuses these cues to produce temporally coherent trajectories. We also curate Truebones Zoo with 1038 motion clips, each providing a standardized skeleton-mesh-render triad. Experiments on both in-domain benchmarks and in-the-wild videos show that MoCapAnything delivers high-quality skeletal animations and exhibits meaningful cross-species retargeting across heterogeneous rigs, enabling scalable, prompt-driven 3D motion capture for arbitrary assets. Project page: https://animotionlab.github.io/MoCapAnything/",
    "translation": "标题：MoCapAnything：基于单目视频的任意骨架统一三维运动捕捉\n\n摘要：运动捕捉技术如今已支撑起远超数字人范畴的内容创作，但现有流程大多仍局限于特定物种或固定模板。我们将这一局限形式化为类别无关运动捕捉：给定单目视频与任意绑定三维资产作为提示，目标在于重建可直接驱动该特定资产的基于旋转的动画（如BVH格式）。本文提出MoCapAnything——一个参考引导的因子化框架，首先生成三维关节轨迹，再通过约束感知逆向运动学恢复资产专属旋转。该系统包含三个可学习模块与轻量级逆向运动学阶段：（1）参考提示编码器：从资产骨架、网格及渲染图像中提取逐关节查询向量；（2）视频特征提取器：计算稠密视觉描述符并重建粗粒度四维变形网格，以弥合视频空间与关节空间的鸿沟；（3）统一运动解码器：融合多模态线索生成时序连贯的轨迹。我们还构建了包含1038个动作片段的Truebones Zoo数据集，每个片段均提供标准化的骨架-网格-渲染三元组。在领域内基准测试与真实场景视频上的实验表明，MoCapAnything能生成高质量骨骼动画，在异构绑定系统间实现有效的跨物种动作重定向，为任意资产提供了可扩展的提示驱动三维运动捕捉方案。项目页面：https://animotionlab.github.io/MoCapAnything/",
    "url": "https://huggingface.co/papers/2512.10881",
    "arxiv_url": "https://arxiv.org/abs/2512.10881"
  },
  {
    "title": "BEAVER: An Efficient Deterministic LLM Verifier",
    "summary": "As large language models (LLMs) transition from research prototypes to production systems, practitioners often need reliable methods to verify that model outputs satisfy required constraints. While sampling-based estimates provide an intuition of model behavior, they offer no sound guarantees. We present BEAVER, the first practical framework for computing deterministic, sound probability bounds on LLM constraint satisfaction. Given any prefix-closed semantic constraint, BEAVER systematically explores the generation space using novel token trie and frontier data structures, maintaining provably sound bounds at every iteration. We formalize the verification problem, prove soundness of our approach, and evaluate BEAVER on correctness verification, privacy verification and secure code generation tasks across multiple state of the art LLMs. BEAVER achieves 6 to 8 times tighter probability bounds and identifies 3 to 4 times more high risk instances compared to baseline methods under identical computational budgets, enabling precise characterization and risk assessment that loose bounds or empirical evaluation cannot provide.",
    "translation": "标题：BEAVER：一种高效的大语言模型确定性验证器\n\n摘要：随着大语言模型从研究原型转向生产系统，从业者通常需要可靠的方法来验证模型输出是否满足特定约束。虽然基于采样的评估方法能够提供模型行为的直观认知，但无法提供严格的理论保证。本文提出BEAVER框架——首个能够计算大语言模型约束满足确定性概率边界的实用系统。针对任意前缀封闭的语义约束，BEAVER通过创新的词汇树和边界数据结构系统性地探索生成空间，在每次迭代中始终保持可证明的严格边界。我们形式化定义了验证问题，证明了方法的可靠性，并在多个前沿大语言模型上对BEAVER进行了正确性验证、隐私验证和安全代码生成任务的评估。在相同计算资源下，BEAVER获得的概率边界比基线方法精确6至8倍，识别出的高风险实例数量增加3至4倍，实现了宽松边界或经验评估无法达成的精确特性刻画与风险评估。",
    "url": "https://huggingface.co/papers/2512.05439",
    "arxiv_url": "https://arxiv.org/abs/2512.05439"
  },
  {
    "title": "From Macro to Micro: Benchmarking Microscopic Spatial Intelligence on Molecules via Vision-Language Models",
    "summary": "This paper introduces the concept of Microscopic Spatial Intelligence (MiSI), the capability to perceive and reason about the spatial relationships of invisible microscopic entities, which is fundamental to scientific discovery. To assess the potential of Vision-Language Models (VLMs) in this domain, we propose a systematic benchmark framework MiSI-Bench. This framework features over 163,000 question-answer pairs and 587,000 images derived from approximately 4,000 molecular structures, covering nine complementary tasks that evaluate abilities ranging from elementary spatial transformations to complex relational identifications. Experimental results reveal that current state-of-the-art VLMs perform significantly below human level on this benchmark. However, a fine-tuned 7B model demonstrates substantial potential, even surpassing humans in spatial transformation tasks, while its poor performance in scientifically-grounded tasks like hydrogen bond recognition underscores the necessity of integrating explicit domain knowledge for progress toward scientific AGI. The datasets are available at https://huggingface.co/datasets/zongzhao/MiSI-bench.",
    "translation": "标题：从宏观到微观：基于视觉语言模型的分子微观空间智能基准测试\n\n摘要：本文提出微观空间智能（MiSI）的概念，即感知与推断不可见微观实体空间关系的能力，这是科学发现的基础。为评估视觉语言模型（VLMs）在该领域的潜力，我们构建了系统化基准测试框架MiSI-Bench。该框架包含超过16.3万个问答对和58.7万张图像，数据源自约4000个分子结构，涵盖九项互补任务，评估能力范围涵盖基础空间变换到复杂关系识别。实验结果表明，当前最先进的VLMs在该基准测试中的表现显著低于人类水平。然而，经过微调的70亿参数模型展现出巨大潜力，甚至在空间变换任务中超越人类表现，而其在氢键识别等科学基础任务中的薄弱表现，凸显了整合显式领域知识对于实现科学通用人工智能的必要性。数据集发布于https://huggingface.co/datasets/zongzhao/MiSI-bench。",
    "url": "https://huggingface.co/papers/2512.10867",
    "arxiv_url": "https://arxiv.org/abs/2512.10867"
  },
  {
    "title": "VQRAE: Representation Quantization Autoencoders for Multimodal Understanding, Generation and Reconstruction",
    "summary": "Unifying multimodal understanding, generation and reconstruction representation in a single tokenizer remains a key challenge in building unified models. Previous research predominantly attempts to address this in a dual encoder paradigm, e.g., utilizing the separate encoders for understanding and generation respectively or balancing semantic representations and low-level features with contrastive loss. In this paper, we propose VQRAE, a Vector Quantization version of Representation AutoEncoders, which pioneers the first exploration in unified representation to produce Continuous semantic features for image understanding and Discrete tokens for visual generation within a unified tokenizer. Specifically, we build upon pretrained vision foundation models with a symmetric ViT decoder and adopt a two-stage training strategy: first, it freezes the encoder and learns a high-dimensional semantic VQ codebook with pixel reconstruction objective; then jointly optimizes the encoder with self-distillation constraints. This design enables negligible semantic information for maintaining the ability of multimodal understanding, discrete tokens that are compatible for generation and fine-grained reconstruction. Besides, we identify the intriguing property in quantizing semantic encoders that rely on high-dimensional codebook in contrast to the previous common practice of low-dimensional codebook in image reconstruction. The semantic VQ codebook can achieve a 100% utilization ratio at a dimension of 1536. VQRAE presents competitive performance on several benchmarks of visual understanding, generation and reconstruction with promising scaling property in the autoregressive paradigm for its discrete merits.",
    "translation": "标题：VQRAE：面向多模态理解、生成与重建的表征量化自编码器\n\n摘要：在单一标记器中统一多模态理解、生成与重建表征，仍是构建统一模型的核心挑战。现有研究主要尝试在双编码器范式下解决此问题，例如分别使用独立编码器进行理解与生成，或通过对比损失平衡语义表征与低级特征。本文提出VQRAE（表征自编码器的向量量化版本），首次在统一标记器框架内探索统一表征，以生成用于图像理解的连续语义特征和用于视觉生成的离散标记。具体而言，我们基于预训练视觉基础模型构建对称ViT解码器，并采用两阶段训练策略：首先冻结编码器，以像素重建为目标学习高维语义向量量化码本；随后通过自蒸馏约束联合优化编码器。该设计能够在保持多模态理解能力的同时，使语义信息损失可忽略不计，并生成兼容生成任务与细粒度重建的离散标记。此外，我们发现语义编码器量化依赖于高维码本，这与图像重建中常见的低维码本实践形成对比。语义向量量化码本在1536维度下可实现100%的利用率。VQRAE在视觉理解、生成与重建的多个基准测试中展现出竞争力，其离散特性在自回归范式下具有优异的扩展潜力。",
    "url": "https://huggingface.co/papers/2511.23386",
    "arxiv_url": "https://arxiv.org/abs/2511.23386"
  },
  {
    "title": "Evaluating Gemini Robotics Policies in a Veo World Simulator",
    "summary": "Generative world models hold significant potential for simulating interactions with visuomotor policies in varied environments. Frontier video models can enable generation of realistic observations and environment interactions in a scalable and general manner. However, the use of video models in robotics has been limited primarily to in-distribution evaluations, i.e., scenarios that are similar to ones used to train the policy or fine-tune the base video model. In this report, we demonstrate that video models can be used for the entire spectrum of policy evaluation use cases in robotics: from assessing nominal performance to out-of-distribution (OOD) generalization, and probing physical and semantic safety. We introduce a generative evaluation system built upon a frontier video foundation model (Veo). The system is optimized to support robot action conditioning and multi-view consistency, while integrating generative image-editing and multi-view completion to synthesize realistic variations of real-world scenes along multiple axes of generalization. We demonstrate that the system preserves the base capabilities of the video model to enable accurate simulation of scenes that have been edited to include novel interaction objects, novel visual backgrounds, and novel distractor objects. This fidelity enables accurately predicting the relative performance of different policies in both nominal and OOD conditions, determining the relative impact of different axes of generalization on policy performance, and performing red teaming of policies to expose behaviors that violate physical or semantic safety constraints. We validate these capabilities through 1600+ real-world evaluations of eight Gemini Robotics policy checkpoints and five tasks for a bimanual manipulator.",
    "translation": "标题：在Veo世界模拟器中评估Gemini机器人策略\n\n摘要：生成式世界模型在模拟不同环境中视觉运动策略的交互方面具有显著潜力。前沿视频模型能够以可扩展且通用的方式生成逼真的观测结果与环境交互。然而，视频模型在机器人领域的应用主要局限于分布内评估，即评估与训练策略或微调基础视频模型所用场景相似的场景。本研究报告证明，视频模型可适用于机器人策略评估的全场景应用：从评估标称性能到分布外泛化能力，再到探究物理安全与语义安全性。我们提出了一种基于前沿视频基础模型（Veo）构建的生成式评估系统。该系统通过优化设计支持机器人动作条件约束与多视角一致性，同时集成生成式图像编辑与多视角补全技术，能够沿多个泛化维度合成真实场景的逼真变体。实验表明，该系统保留了视频模型的基础能力，可精确模拟经过编辑的场景——包括添加新型交互物体、新颖视觉背景及干扰物体。这种高保真特性使得系统能够准确预测不同策略在标称条件与分布外条件下的相对性能，确定不同泛化维度对策略性能的相对影响，并对策略进行红队测试以暴露违反物理或语义安全约束的行为。我们通过对八种Gemini机器人策略检查点及双手机械臂的五项任务进行1600余次真实世界评估，验证了该系统的各项能力。",
    "url": "https://huggingface.co/papers/2512.10675",
    "arxiv_url": "https://arxiv.org/abs/2512.10675"
  },
  {
    "title": "Thinking with Images via Self-Calling Agent",
    "summary": "Thinking-with-images paradigms have showcased remarkable visual reasoning capability by integrating visual information as dynamic elements into the Chain-of-Thought (CoT). However, optimizing interleaved multimodal CoT (iMCoT) through reinforcement learning remains challenging, as it relies on scarce high-quality reasoning data. In this study, we propose Self-Calling Chain-of-Thought (sCoT), a novel visual reasoning paradigm that reformulates iMCoT as a language-only CoT with self-calling. Specifically, a main agent decomposes the complex visual reasoning task to atomic subtasks and invokes its virtual replicas, i.e. parameter-sharing subagents, to solve them in isolated context. sCoT enjoys substantial training effectiveness and efficiency, as it requires no explicit interleaving between modalities. sCoT employs group-relative policy optimization to reinforce effective reasoning behavior to enhance optimization. Experiments on HR-Bench 4K show that sCoT improves the overall reasoning performance by up to 1.9% with sim 75% fewer GPU hours compared to strong baseline approaches. Code is available at https://github.com/YWenxi/think-with-images-through-self-calling.",
    "translation": "标题：基于自调用智能体的图像思维推理\n\n摘要：图像思维推理范式通过将视觉信息作为动态元素整合至思维链中，已展现出卓越的视觉推理能力。然而，由于依赖稀缺的高质量推理数据，通过强化学习优化交错式多模态思维链仍面临挑战。本研究提出自调用思维链——一种创新的视觉推理范式，将交错式多模态思维链重构为具有自调用机制的纯语言思维链。具体而言，主智能体将复杂视觉推理任务分解为原子子任务，并调用其虚拟副本（即参数共享的子智能体）在隔离上下文中解决问题。该范式无需显式的模态交错处理，从而显著提升了训练效能与效率。通过采用组间相对策略优化方法，系统能够强化有效推理行为以增强优化效果。在HR-Bench 4K数据集上的实验表明，相较于强基线方法，自调用思维链在减少约75%GPU计算时的同时，将整体推理性能提升最高达1.9%。代码已开源：https://github.com/YWenxi/think-with-images-through-self-calling。",
    "url": "https://huggingface.co/papers/2512.08511",
    "arxiv_url": "https://arxiv.org/abs/2512.08511"
  },
  {
    "title": "StereoSpace: Depth-Free Synthesis of Stereo Geometry via End-to-End Diffusion in a Canonical Space",
    "summary": "We introduce StereoSpace, a diffusion-based framework for monocular-to-stereo synthesis that models geometry purely through viewpoint conditioning, without explicit depth or warping. A canonical rectified space and the conditioning guide the generator to infer correspondences and fill disocclusions end-to-end. To ensure fair and leakage-free evaluation, we introduce an end-to-end protocol that excludes any ground truth or proxy geometry estimates at test time. The protocol emphasizes metrics reflecting downstream relevance: iSQoE for perceptual comfort and MEt3R for geometric consistency. StereoSpace surpasses other methods from the warp & inpaint, latent-warping, and warped-conditioning categories, achieving sharp parallax and strong robustness on layered and non-Lambertian scenes. This establishes viewpoint-conditioned diffusion as a scalable, depth-free solution for stereo generation.",
    "translation": "标题：StereoSpace：基于规范空间端到端扩散的无深度立体几何合成方法\n\n摘要：本文提出StereoSpace，一种基于扩散模型的单目到立体合成框架，该框架仅通过视点条件建模几何信息，无需显式深度估计或图像扭曲操作。通过构建规范矫正空间并结合条件引导，生成器能够端到端地推断对应关系并填补遮挡区域。为确保评估的公平性与无信息泄露，我们设计了一套端到端评估协议，在测试阶段完全排除真实几何数据或代理几何估计的介入。该协议重点关注反映下游应用价值的指标：以iSQoE衡量感知舒适度，以MEt3R评估几何一致性。实验表明，StereoSpace在扭曲修复、潜在空间扭曲及扭曲条件生成等类别方法中均取得优势，能够在层叠场景与非朗伯场景中生成清晰视差并保持强鲁棒性。这证实了视点条件扩散模型可作为无需深度信息的可扩展立体生成解决方案。",
    "url": "https://huggingface.co/papers/2512.10959",
    "arxiv_url": "https://arxiv.org/abs/2512.10959"
  },
  {
    "title": "Stronger Normalization-Free Transformers",
    "summary": "Although normalization layers have long been viewed as indispensable components of deep learning architectures, the recent introduction of Dynamic Tanh (DyT) has demonstrated that alternatives are possible. The point-wise function DyT constrains extreme values for stable convergence and reaches normalization-level performance; this work seeks further for function designs that can surpass it. We first study how the intrinsic properties of point-wise functions influence training and performance. Building on these findings, we conduct a large-scale search for a more effective function design. Through this exploration, we introduce Derf(x) = erf(αx + s), where erf(x) is the rescaled Gaussian cumulative distribution function, and identify it as the most performant design. Derf outperforms LayerNorm, RMSNorm, and DyT across a wide range of domains, including vision (image recognition and generation), speech representation, and DNA sequence modeling. Our findings suggest that the performance gains of Derf largely stem from its improved generalization rather than stronger fitting capacity. Its simplicity and stronger performance make Derf a practical choice for normalization-free Transformer architectures.",
    "translation": "标题：无需归一化的更强Transformer模型\n\n摘要：尽管归一化层长期以来被视为深度学习架构中不可或缺的组成部分，但动态双曲正切函数（DyT）的提出表明替代方案是存在的。该点态函数通过约束极端值实现稳定收敛，并达到归一化级别的性能；本研究旨在探索能够超越其性能的函数设计。我们首先研究点态函数的内在特性如何影响训练与性能表现。基于这些发现，我们进行了大规模搜索以寻找更有效的函数设计。通过系统探索，我们提出Derf(x) = erf(αx + s)函数，其中erf(x)为缩放后的高斯累积分布函数，并验证其为当前最优设计。在视觉（图像识别与生成）、语音表征及DNA序列建模等多个领域，Derf在性能上均超越层归一化、RMSNorm及DyT方法。研究结果表明，Derf的性能提升主要源于其增强的泛化能力而非拟合能力。其简洁性与卓越性能使Derf成为无需归一化的Transformer架构的理想选择。",
    "url": "https://huggingface.co/papers/2512.10938",
    "arxiv_url": "https://arxiv.org/abs/2512.10938"
  },
  {
    "title": "The FACTS Leaderboard: A Comprehensive Benchmark for Large Language Model Factuality",
    "summary": "We introduce The FACTS Leaderboard, an online leaderboard suite and associated set of benchmarks that comprehensively evaluates the ability of language models to generate factually accurate text across diverse scenarios. The suite provides a holistic measure of factuality by aggregating the performance of models on four distinct sub-leaderboards: (1) FACTS Multimodal, which measures the factuality of responses to image-based questions; (2) FACTS Parametric, which assesses models' world knowledge by answering closed-book factoid questions from internal parameters; (3) FACTS Search, which evaluates factuality in information-seeking scenarios, where the model must use a search API; and (4) FACTS Grounding (v2), which evaluates whether long-form responses are grounded in provided documents, featuring significantly improved judge models. Each sub-leaderboard employs automated judge models to score model responses, and the final suite score is an average of the four components, designed to provide a robust and balanced assessment of a model's overall factuality. The FACTS Leaderboard Suite will be actively maintained, containing both public and private splits to allow for external participation while guarding its integrity. It can be found at https://www.kaggle.com/benchmarks/google/facts .",
    "translation": "标题：FACTS排行榜：大型语言模型事实性综合评测基准\n\n摘要：本文推出FACTS排行榜——一套在线评测体系及相关基准测试，旨在全面评估语言模型在不同场景下生成事实准确文本的能力。该体系通过整合模型在四个独立子榜单的表现提供整体事实性度量：（1）FACTS多模态榜单，通过图像问答任务评估响应的事实性；（2）FACTS参数化榜单，通过闭卷事实型问题测试模型基于内部参数的世界知识；（3）FACTS搜索榜单，在信息检索场景中评估模型调用搜索API时的事实准确性；（4）FACTS文本锚定榜单（v2版），评估长文本响应是否基于给定文档，其判定模型性能显著提升。各子榜单均采用自动化判定模型对输出进行评分，最终体系得分为四项得分的平均值，从而实现对模型整体事实性的稳健均衡评估。FACTS排行榜体系将持续维护，包含公开与私有数据分割以兼顾开放参与和评测完整性。评测平台地址：https://www.kaggle.com/benchmarks/google/facts。",
    "url": "https://huggingface.co/papers/2512.10791",
    "arxiv_url": "https://arxiv.org/abs/2512.10791"
  },
  {
    "title": "Tool-Augmented Spatiotemporal Reasoning for Streamlining Video Question Answering Task",
    "summary": "Video Question Answering (VideoQA) task serves as a critical playground for evaluating whether foundation models can effectively perceive, understand, and reason about dynamic real-world scenarios. However, existing Multimodal Large Language Models (MLLMs) struggle with simultaneously modeling spatial relationships within video frames and understanding the causal dynamics of temporal evolution on complex and reasoning-intensive VideoQA task. In this work, we equip MLLM with a comprehensive and extensible Video Toolkit, to enhance MLLM's spatiotemporal reasoning capabilities and ensure the harmony between the quantity and diversity of tools. To better control the tool invocation sequence and avoid toolchain shortcut issues, we propose a Spatiotemporal Reasoning Framework (STAR) that strategically schedules temporal and spatial tools, thereby progressively localizing the key area in the video. Our STAR framework enhances GPT-4o using lightweight tools, achieving an 8.2% gain on VideoMME and 4.6% on LongVideoBench. We believe that our proposed Video Toolkit and STAR framework make an important step towards building autonomous and intelligent video analysis assistants. The code is publicly available at https://github.com/fansunqi/VideoTool.",
    "translation": "标题：工具增强时空推理：面向视频问答任务的高效化处理\n\n摘要：视频问答任务作为评估基础模型能否有效感知、理解并推理动态现实场景的关键测试平台。然而，现有的多模态大语言模型在复杂且需要深度推理的视频问答任务中，难以同时建模视频帧内的空间关系并理解时序演化的因果动态。本研究为多模态大语言模型配备了一个全面且可扩展的视频工具包，以增强其时空推理能力，并确保工具数量与多样性之间的协调。为了更好地控制工具调用顺序并避免工具链捷径问题，我们提出了一种时空推理框架，该框架策略性地调度时序与空间工具，从而逐步定位视频中的关键区域。我们的时空推理框架通过轻量级工具增强了GPT-4o的性能，在VideoMME基准上实现了8.2%的性能提升，在LongVideoBench基准上提升了4.6%。我们相信，所提出的视频工具包与时空推理框架为构建自主智能的视频分析助手迈出了重要一步。代码已公开于https://github.com/fansunqi/VideoTool。",
    "url": "https://huggingface.co/papers/2512.10359",
    "arxiv_url": "https://arxiv.org/abs/2512.10359"
  },
  {
    "title": "H2R-Grounder: A Paired-Data-Free Paradigm for Translating Human Interaction Videos into Physically Grounded Robot Videos",
    "summary": "Robots that learn manipulation skills from everyday human videos could acquire broad capabilities without tedious robot data collection. We propose a video-to-video translation framework that converts ordinary human-object interaction videos into motion-consistent robot manipulation videos with realistic, physically grounded interactions. Our approach does not require any paired human-robot videos for training only a set of unpaired robot videos, making the system easy to scale. We introduce a transferable representation that bridges the embodiment gap: by inpainting the robot arm in training videos to obtain a clean background and overlaying a simple visual cue (a marker and arrow indicating the gripper's position and orientation), we can condition a generative model to insert the robot arm back into the scene. At test time, we apply the same process to human videos (inpainting the person and overlaying human pose cues) and generate high-quality robot videos that mimic the human's actions. We fine-tune a SOTA video diffusion model (Wan 2.2) in an in-context learning manner to ensure temporal coherence and leveraging of its rich prior knowledge. Empirical results demonstrate that our approach achieves significantly more realistic and grounded robot motions compared to baselines, pointing to a promising direction for scaling up robot learning from unlabeled human videos. Project page: https://showlab.github.io/H2R-Grounder/",
    "translation": "标题：H2R-Grounder：一种无需配对数据的范式，用于将人类交互视频转化为物理接地的机器人视频\n\n摘要：能够从日常人类视频中学习操作技能的机器人，有望在不依赖繁琐的机器人数据收集的情况下获得广泛的能力。我们提出了一种视频到视频的翻译框架，该框架能将普通的人-物交互视频转换为具有真实、物理接地交互效果且运动一致的机器人操作视频。我们的方法在训练时无需任何配对的人类-机器人视频，仅需一组非配对的机器人视频，这使得系统易于扩展。我们引入了一种可迁移的表征来弥合具身鸿沟：通过在训练视频中对机器人手臂进行修复以获得干净的背景，并叠加一个简单的视觉提示（指示夹爪位置和方向的标记与箭头），我们可以引导生成模型将机器人手臂重新插入场景中。在测试时，我们对人类视频应用相同的过程（修复人物并叠加人体姿态提示），从而生成模仿人类动作的高质量机器人视频。我们以情境学习的方式对最先进的视频扩散模型（Wan 2.2）进行微调，以确保时间连贯性并充分利用其丰富的先验知识。实验结果表明，与基线方法相比，我们的方法能生成显著更真实、更接地的机器人运动，这为从无标注的人类视频中规模化学习机器人技能指出了一个有前景的方向。项目页面：https://showlab.github.io/H2R-Grounder/",
    "url": "https://huggingface.co/papers/2512.09406",
    "arxiv_url": "https://arxiv.org/abs/2512.09406"
  },
  {
    "title": "MoRel: Long-Range Flicker-Free 4D Motion Modeling via Anchor Relay-based Bidirectional Blending with Hierarchical Densification",
    "summary": "Recent advances in 4D Gaussian Splatting (4DGS) have extended the high-speed rendering capability of 3D Gaussian Splatting (3DGS) into the temporal domain, enabling real-time rendering of dynamic scenes. However, one of the major remaining challenges lies in modeling long-range motion-contained dynamic videos, where a naive extension of existing methods leads to severe memory explosion, temporal flickering, and failure to handle appearing or disappearing occlusions over time. To address these challenges, we propose a novel 4DGS framework characterized by an Anchor Relay-based Bidirectional Blending (ARBB) mechanism, named MoRel, which enables temporally consistent and memory-efficient modeling of long-range dynamic scenes. Our method progressively constructs locally canonical anchor spaces at key-frame time index and models inter-frame deformations at the anchor level, enhancing temporal coherence. By learning bidirectional deformations between KfA and adaptively blending them through learnable opacity control, our approach mitigates temporal discontinuities and flickering artifacts. We further introduce a Feature-variance-guided Hierarchical Densification (FHD) scheme that effectively densifies KfA's while keeping rendering quality, based on an assigned level of feature-variance. To effectively evaluate our model's capability to handle real-world long-range 4D motion, we newly compose long-range 4D motion-contained dataset, called SelfCap_{LR}. It has larger average dynamic motion magnitude, captured at spatially wider spaces, compared to previous dynamic video datasets. Overall, our MoRel achieves temporally coherent and flicker-free long-range 4D reconstruction while maintaining bounded memory usage, demonstrating both scalability and efficiency in dynamic Gaussian-based representations.",
    "translation": "标题：MoRel：基于锚点中继双向混合与层次化致密化的长程无闪烁四维运动建模\n\n摘要：四维高斯泼溅（4DGS）技术的最新进展将三维高斯泼溅（3DGS）的高速渲染能力扩展至时间域，实现了动态场景的实时渲染。然而，当前仍存在的主要挑战之一在于对包含长程运动的动态视频进行建模——现有方法的简单扩展会导致严重的内存爆炸、时间闪烁，以及无法处理随时间出现或消失的遮挡。为应对这些挑战，我们提出了一种新颖的4DGS框架，其核心为基于锚点中继的双向混合（ARBB）机制，命名为MoRel。该框架能够以内存高效的方式对长程动态场景进行时间一致性建模。我们的方法在关键帧时间索引处逐步构建局部规范锚点空间，并在锚点层级建模帧间形变，从而增强时间连贯性。通过学习关键帧锚点之间的双向形变，并通过可学习的透明度控制进行自适应混合，我们的方法有效缓解了时间不连续性与闪烁伪影。我们进一步提出了一种特征方差引导的层次化致密化（FHD）方案，该方案基于指定的特征方差水平，在保持渲染质量的同时有效致密化关键帧锚点。为有效评估模型处理真实世界长程四维运动的能力，我们新构建了一个包含长程四维运动的数据集，命名为SelfCap_{LR}。与先前的动态视频数据集相比，该数据集具有更大的平均动态运动幅度，并在空间更广的范围内采集。总体而言，我们的MoRel方法在保持内存使用可控的同时，实现了时间连贯且无闪烁的长程四维重建，展现了基于高斯表示的动态场景建模的可扩展性与高效性。",
    "url": "https://huggingface.co/papers/2512.09270",
    "arxiv_url": "https://arxiv.org/abs/2512.09270"
  },
  {
    "title": "Omni-Attribute: Open-vocabulary Attribute Encoder for Visual Concept Personalization",
    "summary": "Visual concept personalization aims to transfer only specific image attributes, such as identity, expression, lighting, and style, into unseen contexts. However, existing methods rely on holistic embeddings from general-purpose image encoders, which entangle multiple visual factors and make it difficult to isolate a single attribute. This often leads to information leakage and incoherent synthesis. To address this limitation, we introduce Omni-Attribute, the first open-vocabulary image attribute encoder designed to learn high-fidelity, attribute-specific representations. Our approach jointly designs the data and model: (i) we curate semantically linked image pairs annotated with positive and negative attributes to explicitly teach the encoder what to preserve or suppress; and (ii) we adopt a dual-objective training paradigm that balances generative fidelity with contrastive disentanglement. The resulting embeddings prove effective for open-vocabulary attribute retrieval, personalization, and compositional generation, achieving state-of-the-art performance across multiple benchmarks.",
    "translation": "标题：Omni-Attribute：面向视觉概念个性化的开放词汇属性编码器\n\n摘要：视觉概念个性化旨在仅将特定图像属性（如身份、表情、光照和风格）迁移至未见过的场景中。然而，现有方法依赖于通用图像编码器提取的整体嵌入表示，这些表示往往纠缠了多种视觉因素，难以分离单一属性，常导致信息泄露与合成结果不一致。为克服这一局限，本文提出Omni-Attribute——首个开放词汇图像属性编码器，旨在学习高保真、属性特定的表征。我们的方法协同设计了数据与模型：（1）通过构建带有正负属性标注的语义关联图像对，显式指导编码器学习应保留或抑制的特征；（2）采用双目标训练范式，在生成保真度与对比解耦之间取得平衡。实验表明，所得嵌入表示能有效支持开放词汇属性检索、个性化及组合生成任务，在多个基准测试中达到了最先进的性能水平。",
    "url": "https://huggingface.co/papers/2512.10955",
    "arxiv_url": "https://arxiv.org/abs/2512.10955"
  },
  {
    "title": "Confucius Code Agent: An Open-sourced AI Software Engineer at Industrial Scale",
    "summary": "Real-world AI software engineering demands coding agents that can reason over massive repositories, maintain durable memory across and within long sessions, and robustly coordinate complex toolchains at test time. Existing open-source coding agents provide transparency but frequently fall short when pushed to these industrial-scale workloads, while proprietary coding agents offer strong practical performance but limited extensibility, interpretability, and controllability. We present the Confucius Code Agent (CCA), an open-sourced AI software engineer that can operate at an industrial scale. CCA is built atop the Confucius SDK, an open-sourced agent development platform designed around three complementary perspectives: Agent Experience (AX), User Experience (UX), and Developer Experience (DX). The SDK introduces a unified orchestrator with hierarchical working memory for long-context reasoning, a persistent note-taking system for cross-session continual learning, and a modular extension module for robust tool use. Moreover, a meta-agent automates the synthesis, evaluation, and refinement of agent configurations through a build-test-improve loop, enabling rapid agent development on new tasks, environments, and tool stacks. Instantiated on Confucius SDK with these mechanisms, CCA delivers strong performance on real-world software engineering tasks. On SWE-Bench-Pro, CCA achieves a state-of-the-art Resolve@1 performance of 54.3%, substantially improving over prior coding agents. Together, the Confucius SDK and CCA provide a transparent, extensible, and reproducible foundation for AI agents, bridge gaps between research prototypes and production-grade systems, and support agent development and deployment at industrial scale.",
    "translation": "标题：孔子代码智能体：工业级开源人工智能软件工程师\n\n摘要：现实世界的人工智能软件工程需要能够对海量代码库进行推理、在长会话中保持持久记忆、并在测试阶段稳健协调复杂工具链的编码智能体。现有开源编码智能体虽具透明度，但在应对工业级工作负载时常显不足；而专有编码智能体虽实践性能强劲，却在可扩展性、可解释性与可控性方面存在局限。本文提出孔子代码智能体——一个可在工业级规模运行的开源人工智能软件工程师。该智能体构建于孔子软件开发工具包之上，该开源智能体开发平台围绕三个互补维度设计：智能体体验、用户体验与开发者体验。该工具包引入具备分层工作记忆的统一编排器以支持长上下文推理，配备跨会话持续学习的持久笔记系统，并采用模块化扩展机制保障工具使用的稳健性。此外，元智能体通过“构建-测试-优化”循环自动完成智能体配置的合成、评估与精调，使其能快速适应新任务、新环境与新工具栈。基于孔子软件开发工具包实例化的孔子代码智能体在真实软件工程任务中展现出卓越性能：在SWE-Bench-Pro基准测试中，其Resolve@1指标达到54.3%的领先水平，较现有编码智能体实现显著提升。孔子软件开发工具包与孔子代码智能体共同为人工智能智能体提供了透明、可扩展、可复现的基础框架，弥合了研究原型与生产级系统之间的鸿沟，为工业级智能体的开发与部署提供支撑。",
    "url": "https://huggingface.co/papers/2512.10398",
    "arxiv_url": "https://arxiv.org/abs/2512.10398"
  },
  {
    "title": "ReViSE: Towards Reason-Informed Video Editing in Unified Models with Self-Reflective Learning",
    "summary": "Video unified models exhibit strong capabilities in understanding and generation, yet they struggle with reason-informed visual editing even when equipped with powerful internal vision-language models (VLMs). We attribute this gap to two factors: 1) existing datasets are inadequate for training and evaluating reasoning-aware video editing, and 2) an inherent disconnect between the models' reasoning and editing capabilities, which prevents the rich understanding from effectively instructing the editing process. Bridging this gap requires an integrated framework that connects reasoning with visual transformation. To address this gap, we introduce the Reason-Informed Video Editing (RVE) task, which requires reasoning about physical plausibility and causal dynamics during editing. To support systematic evaluation, we construct RVE-Bench, a comprehensive benchmark with two complementary subsets: Reasoning-Informed Video Editing and In-Context Video Generation. These subsets cover diverse reasoning dimensions and real-world editing scenarios. Building upon this foundation, we propose the ReViSE, a Self-Reflective Reasoning (SRF) framework that unifies generation and evaluation within a single architecture. The model's internal VLM provides intrinsic feedback by assessing whether the edited video logically satisfies the given instruction. The differential feedback that refines the generator's reasoning behavior during training. Extensive experiments on RVE-Bench demonstrate that ReViSE significantly enhances editing accuracy and visual fidelity, achieving a 32% improvement of the Overall score in the reasoning-informed video editing subset over state-of-the-art methods.",
    "translation": "标题：ReViSE：基于自反思学习的统一模型推理感知视频编辑研究\n\n摘要：视频统一模型在内容理解与生成方面展现出强大能力，但即使配备先进的内部视觉语言模型，其在推理感知的视频编辑任务中仍存在明显局限。我们认为这一差距源于两方面因素：1）现有数据集难以支撑推理感知视频编辑的训练与评估；2）模型推理能力与编辑能力之间存在固有割裂，导致丰富的语义理解无法有效指导编辑过程。弥合这一差距需要构建连接推理与视觉转换的集成框架。为此，我们提出推理感知视频编辑任务，该任务要求编辑过程中综合考虑物理合理性与因果动态关系。为建立系统化评估体系，我们构建了RVE-Bench综合基准数据集，包含两个互补子集：推理感知视频编辑与上下文视频生成，覆盖多维推理场景与现实编辑需求。基于此，我们提出ReViSE自反思推理框架，该框架将生成与评估功能整合于统一架构中。模型通过内部视觉语言模型对编辑后视频是否符合指令逻辑进行内在评估，其产生的差分反馈在训练过程中持续优化生成器的推理行为。在RVE-Bench上的大量实验表明，ReViSE显著提升了编辑准确度与视觉保真度，在推理感知视频编辑子集上的综合得分较现有最优方法提升32%。",
    "url": "https://huggingface.co/papers/2512.09924",
    "arxiv_url": "https://arxiv.org/abs/2512.09924"
  },
  {
    "title": "Fed-SE: Federated Self-Evolution for Privacy-Constrained Multi-Environment LLM Agents",
    "summary": "LLM agents are widely deployed in complex interactive tasks, yet privacy constraints often preclude centralized optimization and co-evolution across dynamic environments. While Federated Learning (FL) has proven effective on static datasets, its extension to the open-ended self-evolution of agents remains underexplored. Directly applying standard FL is challenging: heterogeneous tasks and sparse, trajectory-level rewards introduce severe gradient conflicts, destabilizing the global optimization process. To bridge this gap, we propose Fed-SE, a Federated Self-Evolution framework for LLM agents. Fed-SE establishes a local evolution-global aggregation paradigm. Locally, agents employ parameter-efficient fine-tuning on filtered, high-return trajectories to achieve stable gradient updates. Globally, Fed-SE aggregates updates within a low-rank subspace that disentangles environment-specific dynamics, effectively reducing negative transfer across clients. Experiments across five heterogeneous environments demonstrate that Fed-SE improves average task success rates by approximately 18% over federated baselines, validating its effectiveness in robust cross-environment knowledge transfer in privacy-constrained deployments.",
    "translation": "标题：Fed-SE：面向隐私受限多环境大语言模型智能体的联邦自进化框架\n\n摘要：大语言模型智能体已广泛应用于复杂的交互式任务，然而隐私约束通常阻碍了其在动态环境中的集中式优化与协同进化。尽管联邦学习在静态数据集上已被证明有效，但其在智能体开放式自进化方面的扩展仍研究不足。直接应用标准联邦学习面临挑战：异构任务以及稀疏的轨迹级奖励会引发严重的梯度冲突，从而破坏全局优化过程的稳定性。为弥合这一差距，本文提出Fed-SE，一个面向大语言模型智能体的联邦自进化框架。Fed-SE构建了“局部进化-全局聚合”范式。在局部，智能体通过对筛选出的高回报轨迹进行参数高效微调，实现稳定的梯度更新。在全局层面，Fed-SE在一个低秩子空间内聚合更新，该子空间能解耦环境特定的动态特性，从而有效减少客户端间的负迁移。在五个异构环境中的实验表明，相较于联邦学习基线方法，Fed-SE将平均任务成功率提升了约18%，验证了其在隐私受限部署场景下实现鲁棒跨环境知识迁移的有效性。",
    "url": "https://huggingface.co/papers/2512.08870",
    "arxiv_url": "https://arxiv.org/abs/2512.08870"
  },
  {
    "title": "MOA: Multi-Objective Alignment for Role-Playing Agents",
    "summary": "Role-playing agents (RPAs) must simultaneously master many conflicting skills -- following multi-turn instructions, exhibiting domain knowledge, and adopting a consistent linguistic style. Existing work either relies on supervised fine-tuning (SFT) that over-fits surface cues and yields low diversity, or applies reinforcement learning (RL) that fails to learn multiple dimensions for comprehensive RPA optimization. We present MOA (Multi-Objective Alignment), a reinforcement-learning framework that enables multi-dimensional, fine-grained rubric optimization for general RPAs. MOA introduces a novel multi-objective optimization strategy that trains simultaneously on multiple fine-grained rubrics to boost optimization performance. Besides, to address the issues of model output diversity and quality, we have also employed thought-augmented rollout with off-policy guidance. Extensive experiments on challenging benchmarks such as PersonaGym and RoleMRC show that MOA enables an 8B model to match or even outperform strong baselines such as GPT-4o and Claude across numerous dimensions. This demonstrates the great potential of MOA in building RPAs that can simultaneously meet the demands of role knowledge, persona style, diverse scenarios, and complex multi-turn conversations.",
    "translation": "标题：MOA：面向角色扮演智能体的多目标对齐框架\n\n摘要：角色扮演智能体（RPAs）需同时掌握多项相互冲突的技能——遵循多轮次指令、展现领域知识并保持连贯的语言风格。现有研究要么依赖监督微调（SFT）方法（易过度拟合表面特征且生成多样性不足），要么采用强化学习（RL）方法（难以学习多维度指标以实现全面的RPA优化）。本文提出MOA（多目标对齐），一种基于强化学习的框架，能够为通用角色扮演智能体实现多维度、细粒度评估标准的优化。MOA引入创新的多目标优化策略，通过同步训练多个细粒度评估标准以提升优化性能。此外，针对模型输出多样性与质量的挑战，我们采用思维增强推演与离轨策略引导相结合的方法。在PersonaGym和RoleMRC等复杂基准测试上的大量实验表明，MOA能使80亿参数模型在多个维度上达到甚至超越GPT-4o和Claude等强基线模型的性能。这证明了MOA在构建角色扮演智能体方面的巨大潜力，使其能够同时满足角色知识、人物风格、多样化场景及复杂多轮对话的综合需求。",
    "url": "https://huggingface.co/papers/2512.09756",
    "arxiv_url": "https://arxiv.org/abs/2512.09756"
  },
  {
    "title": "X-Humanoid: Robotize Human Videos to Generate Humanoid Videos at Scale",
    "summary": "The advancement of embodied AI has unlocked significant potential for intelligent humanoid robots. However, progress in both Vision-Language-Action (VLA) models and world models is severely hampered by the scarcity of large-scale, diverse training data. A promising solution is to \"robotize\" web-scale human videos, which has been proven effective for policy training. However, these solutions mainly \"overlay\" robot arms to egocentric videos, which cannot handle complex full-body motions and scene occlusions in third-person videos, making them unsuitable for robotizing humans. To bridge this gap, we introduce X-Humanoid, a generative video editing approach that adapts the powerful Wan 2.2 model into a video-to-video structure and finetunes it for the human-to-humanoid translation task. This finetuning requires paired human-humanoid videos, so we designed a scalable data creation pipeline, turning community assets into 17+ hours of paired synthetic videos using Unreal Engine. We then apply our trained model to 60 hours of the Ego-Exo4D videos, generating and releasing a new large-scale dataset of over 3.6 million \"robotized\" humanoid video frames. Quantitative analysis and user studies confirm our method's superiority over existing baselines: 69% of users rated it best for motion consistency, and 62.1% for embodiment correctness.",
    "translation": "标题：X-Humanoid：大规模人形视频生成——基于人类视频的机器人化转换\n\n摘要：具身人工智能的发展为智能人形机器人开辟了巨大潜力。然而，视觉-语言-动作模型与世界模型的进展均受限于大规模多样化训练数据的稀缺。一种可行的解决方案是对网络规模的人类视频进行“机器人化”处理，该方法已在策略训练中被证明有效。但现有方案主要将机械臂“叠加”于第一人称视角视频，无法处理第三人称视频中复杂的全身运动与场景遮挡，因而难以实现人类动作的机器人化转换。为弥补这一空白，我们提出X-Humanoid——一种生成式视频编辑方法，将强大的Wan 2.2模型适配为视频到视频架构，并针对人类到人形机器人的转换任务进行微调。该微调需要成对的人类-人形机器人视频数据，为此我们设计了可扩展的数据生成流程，利用虚幻引擎将社区资源转化为超过17小时的配对合成视频。随后，我们将训练模型应用于60小时的Ego-Exo4D视频数据集，生成并发布了包含超过360万帧“机器人化”人形视频的大规模新数据集。定量分析与用户研究证实了本方法相较于现有基线的优越性：69%的用户认为其在运动连贯性方面表现最佳，62.1%的用户认可其具身形态准确性。",
    "url": "https://huggingface.co/papers/2512.04537",
    "arxiv_url": "https://arxiv.org/abs/2512.04537"
  },
  {
    "title": "DuetSVG: Unified Multimodal SVG Generation with Internal Visual Guidance",
    "summary": "Recent vision-language model (VLM)-based approaches have achieved impressive results on SVG generation. However, because they generate only text and lack visual signals during decoding, they often struggle with complex semantics and fail to produce visually appealing or geometrically coherent SVGs. We introduce DuetSVG, a unified multimodal model that jointly generates image tokens and corresponding SVG tokens in an end-to-end manner. DuetSVG is trained on both image and SVG datasets. At inference, we apply a novel test-time scaling strategy that leverages the model's native visual predictions as guidance to improve SVG decoding quality. Extensive experiments show that our method outperforms existing methods, producing visually faithful, semantically aligned, and syntactically clean SVGs across a wide range of applications.",
    "translation": "标题：DuetSVG：基于内部视觉引导的统一多模态SVG生成方法\n\n摘要：近期基于视觉语言模型（VLM）的方法在SVG生成领域取得了显著成果。然而，由于这些方法在解码过程中仅生成文本而缺乏视觉信号，它们往往难以处理复杂语义，且生成的SVG在视觉吸引力与几何一致性方面存在不足。本文提出DuetSVG，一种统一的多模态模型，能够以端到端方式联合生成图像标记与对应的SVG标记。该模型在图像与SVG数据集上进行训练。在推理阶段，我们采用一种新颖的测试时缩放策略，利用模型自身的视觉预测结果作为引导，以提升SVG解码质量。大量实验表明，本方法在多种应用场景中均优于现有方法，能够生成视觉逼真、语义对齐且语法简洁的SVG图形。",
    "url": "https://huggingface.co/papers/2512.10894",
    "arxiv_url": "https://arxiv.org/abs/2512.10894"
  },
  {
    "title": "DragMesh: Interactive 3D Generation Made Easy",
    "summary": "While generative models have excelled at creating static 3D content, the pursuit of systems that understand how objects move and respond to interactions remains a fundamental challenge. Current methods for articulated motion lie at a crossroads: they are either physically consistent but too slow for real-time use, or generative but violate basic kinematic constraints. We present DragMesh, a robust framework for real-time interactive 3D articulation built around a lightweight motion generation core. Our core contribution is a novel decoupled kinematic reasoning and motion generation framework. First, we infer the latent joint parameters by decoupling semantic intent reasoning (which determines the joint type) from geometric regression (which determines the axis and origin using our Kinematics Prediction Network (KPP-Net)). Second, to leverage the compact, continuous, and singularity-free properties of dual quaternions for representing rigid body motion, we develop a novel Dual Quaternion VAE (DQ-VAE). This DQ-VAE receives these predicted priors, along with the original user drag, to generate a complete, plausible motion trajectory. To ensure strict adherence to kinematics, we inject the joint priors at every layer of the DQ-VAE's non-autoregressive Transformer decoder using FiLM (Feature-wise Linear Modulation) conditioning. This persistent, multi-scale guidance is complemented by a numerically-stable cross-product loss to guarantee axis alignment. This decoupled design allows DragMesh to achieve real-time performance and enables plausible, generative articulation on novel objects without retraining, offering a practical step toward generative 3D intelligence. Code: https://github.com/AIGeeksGroup/DragMesh. Website: https://aigeeksgroup.github.io/DragMesh.",
    "translation": "标题：DragMesh：轻松实现交互式三维生成\n\n摘要：尽管生成模型在创建静态三维内容方面表现出色，但如何构建能够理解物体运动方式及交互响应的系统仍是根本性挑战。当前关节运动生成方法面临两难困境：要么符合物理规律但速度过慢无法实时应用，要么具备生成能力却违背基本运动学约束。本文提出DragMesh——一个围绕轻量级运动生成核心构建的实时交互式三维关节运动鲁棒框架。我们的核心贡献在于新颖的解耦式运动学推理与运动生成框架：首先通过分离语义意图推理（确定关节类型）与几何回归（使用运动学预测网络KPP-Net确定轴线和原点）来推断潜在关节参数；其次，为利用对偶四元数表示刚体运动的紧凑性、连续性和无奇异性优势，我们开发了新型对偶四元数变分自编码器（DQ-VAE）。该DQ-VAE接收预测先验参数与原始用户拖拽指令，生成完整合理的运动轨迹。为确保严格遵循运动学规律，我们通过特征线性调制（FiLM）条件化方法，在DQ-VAE非自回归Transformer解码器的每一层注入关节先验信息。这种持续多尺度引导机制辅以数值稳定的叉积损失函数，共同保障轴线对齐精度。解耦式设计使DragMesh既能实现实时性能，又能在未经重新训练的情况下对新物体生成合理的关节运动，为生成式三维智能迈出实用化一步。代码：https://github.com/AIGeeksGroup/DragMesh。项目网站：https://aigeeksgroup.github.io/DragMesh。",
    "url": "https://huggingface.co/papers/2512.06424",
    "arxiv_url": "https://arxiv.org/abs/2512.06424"
  }
]