[
  {
    "title": "T-pro 2.0: An Efficient Russian Hybrid-Reasoning Model and Playground",
    "summary": "We introduce T-pro 2.0, an open-weight Russian LLM for hybrid reasoning and efficient inference. The model supports direct answering and reasoning-trace generation, using a Cyrillic-dense tokenizer and an adapted EAGLE speculative-decoding pipeline to reduce latency. To enable reproducible and extensible research, we release the model weights, the T-Wix 500k instruction corpus, the T-Math reasoning benchmark, and the EAGLE weights on Hugging Face. These resources allow users to study Russian-language reasoning and to extend or adapt both the model and the inference pipeline. A public web demo exposes reasoning and non-reasoning modes and illustrates the speedups achieved by our inference stack across domains. T-pro 2.0 thus serves as an accessible open system for building and evaluating efficient, practical Russian LLM applications.",
    "translation": "标题：T-pro 2.0：一种高效的俄语混合推理模型与实验平台\n\n摘要：本文介绍T-pro 2.0，一个用于混合推理与高效推理的开源权重俄语大语言模型。该模型支持直接回答与推理轨迹生成，通过采用西里尔字母密集型分词器及适配的EAGLE推测解码流水线以降低推理延迟。为促进可复现与可扩展的研究，我们在Hugging Face平台公开了模型权重、T-Wix 50万条指令数据集、T-Math推理基准测试集及EAGLE权重。这些资源使用户能够深入研究俄语推理任务，并对模型及推理流水线进行扩展或适配。公开的网页演示展示了推理与非推理两种模式，并说明了我们的推理框架在多领域实现的加速效果。T-pro 2.0由此构建了一个易于使用的开放系统，可用于开发和评估高效、实用的俄语大语言模型应用。",
    "url": "https://huggingface.co/papers/2512.10430",
    "arxiv_url": "https://arxiv.org/abs/2512.10430"
  },
  {
    "title": "Long-horizon Reasoning Agent for Olympiad-Level Mathematical Problem Solving",
    "summary": "Large language models (LLMs) have achieved significant progress in solving complex reasoning tasks by Reinforcement Learning with Verifiable Rewards (RLVR). This advancement is also inseparable from the oversight automated by reliable verifiers. However, current outcome-based verifiers (OVs) are unable to inspect the unreliable intermediate steps in the long reasoning chains of thought (CoTs). Meanwhile, current process-based verifiers (PVs) have difficulties in reliably detecting errors in the complex long CoTs, limited by the scarcity of high-quality annotations due to the prohibitive costs of human annotations. Therefore, we propose the Outcome-based Process Verifier (OPV), which verifies the rationale process of summarized outcomes from long CoTs to achieve both accurate and efficient verification and enable large-scale annotation. To empower the proposed verifier, we adopt an iterative active learning framework with expert annotations to progressively improve the verification capability of OPV with fewer annotation costs. Specifically, in each iteration, the most uncertain cases of the current best OPV are annotated and then subsequently used to train a new OPV through Rejection Fine-Tuning (RFT) and RLVR for the next round. Extensive experiments demonstrate OPV's superior performance and broad applicability. It achieves new state-of-the-art results on our held-out \\thisbench, outperforming much larger open-source models such as Qwen3-Max-Preview with an F1 score of 83.1 compared to 76.3. Furthermore, OPV effectively detects false positives within synthetic dataset, closely align with expert assessment. When collaborating with policy models, OPV consistently yields performance gains, e.g., raising the accuracy of DeepSeek-R1-Distill-Qwen-32B from 55.2\\% to 73.3\\% on AIME2025 as the compute budget scales.",
    "translation": "标题：面向奥赛级数学解题的长程推理智能体\n\n摘要：大型语言模型（LLM）通过可验证奖励的强化学习（RLVR）在解决复杂推理任务方面取得了显著进展，这一进步同样离不开可靠验证器实现的自动化监督。然而，当前基于结果的验证器（OV）无法有效检测长链思维推理（CoT）中不可靠的中间步骤；而现有基于过程的验证器（PV）受限于高昂人工标注成本导致的高质量标注稀缺，难以可靠地识别复杂长链CoT中的错误。为此，我们提出基于结果的过程验证器（OPV），该验证器通过检验长链CoT中总结性结果的推理过程，实现精准高效的验证并支持大规模标注。为增强该验证器的能力，我们采用结合专家标注的迭代式主动学习框架，以较低标注成本逐步提升OPV的验证性能。具体而言，在每轮迭代中，当前最优OPV预测最不确定的案例将由专家标注，随后通过拒绝微调（RFT）和RLVR训练新一代OPV用于后续轮次。大量实验证明了OPV的卓越性能与广泛适用性：其在保留测试集\\thisbench上取得了最新最优结果，以83.1的F1分数超越Qwen3-Max-Preview等更大规模开源模型（其分数为76.3）；同时，OPV能有效检测合成数据集中的误判案例，其结果与专家评估高度吻合。在与策略模型协同工作时，OPV持续带来性能提升，例如在计算资源扩展时，将DeepSeek-R1-Distill-Qwen-32B在AIME2025上的准确率从55.2%提升至73.3%。",
    "url": "https://huggingface.co/papers/2512.10739",
    "arxiv_url": "https://arxiv.org/abs/2512.10739"
  },
  {
    "title": "Are We Ready for RL in Text-to-3D Generation? A Progressive Investigation",
    "summary": "Reinforcement learning (RL), earlier proven to be effective in large language and multi-modal models, has been successfully extended to enhance 2D image generation recently. However, applying RL to 3D generation remains largely unexplored due to the higher spatial complexity of 3D objects, which require globally consistent geometry and fine-grained local textures. This makes 3D generation significantly sensitive to reward designs and RL algorithms. To address these challenges, we conduct the first systematic study of RL for text-to-3D autoregressive generation across several dimensions. (1) Reward designs: We evaluate reward dimensions and model choices, showing that alignment with human preference is crucial, and that general multi-modal models provide robust signal for 3D attributes. (2) RL algorithms: We study GRPO variants, highlighting the effectiveness of token-level optimization, and further investigate the scaling of training data and iterations. (3) Text-to-3D Benchmarks: Since existing benchmarks fail to measure implicit reasoning abilities in 3D generation models, we introduce MME-3DR. (4) Advanced RL paradigms: Motivated by the natural hierarchy of 3D generation, we propose Hi-GRPO, which optimizes the global-to-local hierarchical 3D generation through dedicated reward ensembles. Based on these insights, we develop AR3D-R1, the first RL-enhanced text-to-3D model, expert from coarse shape to texture refinement. We hope this study provides insights into RL-driven reasoning for 3D generation. Code is released at https://github.com/Ivan-Tang-3D/3DGen-R1.",
    "translation": "标题：文本到三维生成是否已为强化学习做好准备？一项渐进式探究\n\n摘要：强化学习（RL）先前已被证明在大语言模型和多模态模型中具有显著效果，近期已成功扩展至增强二维图像生成领域。然而，由于三维对象具有更高的空间复杂性——需要全局一致的几何结构与细粒度局部纹理——将强化学习应用于三维生成的研究仍处于探索阶段。这使得三维生成对奖励设计与强化学习算法尤为敏感。为应对这些挑战，我们首次从多个维度对文本到三维自回归生成中的强化学习进行了系统性研究：（1）奖励设计：通过评估奖励维度与模型选择，我们发现与人类偏好对齐至关重要，且通用多模态模型能为三维属性提供稳健的信号反馈；（2）强化学习算法：我们研究了GRPO算法的变体，证明了词元级优化的有效性，并进一步探索了训练数据与迭代次数的规模化影响；（3）文本到三维基准测试：针对现有基准无法有效衡量三维生成模型隐含推理能力的问题，我们提出了MME-3DR基准；（4）先进强化学习范式：基于三维生成固有的层次化特性，我们提出Hi-GRPO方法，通过定制化奖励组合优化从全局到局部的层次化三维生成。基于以上发现，我们开发了首个强化学习增强的文本到三维模型AR3D-R1，实现了从粗糙形状到纹理细化的全流程生成。本研究旨在为强化学习驱动的三维生成推理提供新的见解。代码已发布于https://github.com/Ivan-Tang-3D/3DGen-R1。",
    "url": "https://huggingface.co/papers/2512.10949",
    "arxiv_url": "https://arxiv.org/abs/2512.10949"
  },
  {
    "title": "OPV: Outcome-based Process Verifier for Efficient Long Chain-of-Thought Verification",
    "summary": "Large language models (LLMs) have achieved significant progress in solving complex reasoning tasks by Reinforcement Learning with Verifiable Rewards (RLVR). This advancement is also inseparable from the oversight automated by reliable verifiers. However, current outcome-based verifiers (OVs) are unable to inspect the unreliable intermediate steps in the long reasoning chains of thought (CoTs). Meanwhile, current process-based verifiers (PVs) have difficulties in reliably detecting errors in the complex long CoTs, limited by the scarcity of high-quality annotations due to the prohibitive costs of human annotations. Therefore, we propose the Outcome-based Process Verifier (OPV), which verifies the rationale process of summarized outcomes from long CoTs to achieve both accurate and efficient verification and enable large-scale annotation. To empower the proposed verifier, we adopt an iterative active learning framework with expert annotations to progressively improve the verification capability of OPV with fewer annotation costs. Specifically, in each iteration, the most uncertain cases of the current best OPV are annotated and then subsequently used to train a new OPV through Rejection Fine-Tuning (RFT) and RLVR for the next round. Extensive experiments demonstrate OPV's superior performance and broad applicability. It achieves new state-of-the-art results on our held-out OPV-Bench, outperforming much larger open-source models such as Qwen3-Max-Preview with an F1 score of 83.1 compared to 76.3. Furthermore, OPV effectively detects false positives within synthetic dataset, closely align with expert assessment. When collaborating with policy models, OPV consistently yields performance gains, e.g., raising the accuracy of DeepSeek-R1-Distill-Qwen-32B from 55.2% to 73.3% on AIME2025 as the compute budget scales.",
    "translation": "标题：OPV：基于结果的过程验证器——面向高效长链思维验证的新方法\n\n摘要：大型语言模型（LLM）通过基于可验证奖励的强化学习（RLVR）在解决复杂推理任务方面取得了显著进展。这一进步同样离不开可靠验证器的自动化监督。然而，当前基于结果的验证器（OV）无法有效检测长链思维（CoT）推理过程中不可靠的中间步骤；而基于过程的验证器（PV）由于人工标注成本高昂导致高质量标注稀缺，难以可靠识别复杂长链思维中的错误。为此，我们提出基于结果的过程验证器（OPV），该验证器通过对长链思维生成的总结性结果进行推理过程验证，实现精准高效的验证并支持大规模标注。为增强验证器性能，我们采用结合专家标注的迭代式主动学习框架，以较低标注成本逐步提升OPV的验证能力。具体而言，在每轮迭代中，当前最优OPV预测置信度最低的样本由专家标注，随后通过拒绝微调（RFT）和RLVR训练新一代OPV用于下一轮迭代。大量实验证明OPV具有卓越性能和广泛适用性：在我们构建的OPV-Bench测试集上取得最新最优结果，F1分数达83.1，显著优于Qwen3-Max-Preview等更大规模开源模型（76.3分）；同时OPV能有效识别合成数据集中的误判案例，其评估结果与专家判断高度一致。在与策略模型协同工作时，OPV持续带来性能提升，例如在计算资源扩展条件下，将DeepSeek-R1-Distill-Qwen-32B模型在AIME2025数据集上的准确率从55.2%提升至73.3%。",
    "url": "https://huggingface.co/papers/2512.10756",
    "arxiv_url": "https://arxiv.org/abs/2512.10756"
  },
  {
    "title": "Achieving Olympia-Level Geometry Large Language Model Agent via Complexity Boosting Reinforcement Learning",
    "summary": "Large language model (LLM) agents exhibit strong mathematical problem-solving abilities and can even solve International Mathematical Olympiad (IMO) level problems with the assistance of formal proof systems. However, due to weak heuristics for auxiliary constructions, AI for geometry problem solving remains dominated by expert models such as AlphaGeometry 2, which rely heavily on large-scale data synthesis and search for both training and evaluation. In this work, we make the first attempt to build a medalist-level LLM agent for geometry and present InternGeometry. InternGeometry overcomes the heuristic limitations in geometry by iteratively proposing propositions and auxiliary constructions, verifying them with a symbolic engine, and reflecting on the engine's feedback to guide subsequent proposals. A dynamic memory mechanism enables InternGeometry to conduct more than two hundred interactions with the symbolic engine per problem. To further accelerate learning, we introduce Complexity-Boosting Reinforcement Learning (CBRL), which gradually increases the complexity of synthesized problems across training stages. Built on InternThinker-32B, InternGeometry solves 44 of 50 IMO geometry problems (2000-2024), exceeding the average gold medalist score (40.9), using only 13K training examples, just 0.004% of the data used by AlphaGeometry 2, demonstrating the potential of LLM agents on expert-level geometry tasks. InternGeometry can also propose novel auxiliary constructions for IMO problems that do not appear in human solutions. We will release the model, data, and symbolic engine to support future research.",
    "translation": "标题：通过复杂度提升强化学习实现奥林匹克级别的几何大语言模型智能体\n\n摘要：大语言模型智能体展现出强大的数学问题解决能力，甚至能够在形式化证明系统的辅助下解决国际数学奥林匹克竞赛级别的几何问题。然而，由于在辅助构造方面的启发式能力较弱，几何问题求解领域目前仍由专家模型主导，例如AlphaGeometry 2，这类模型在训练和评估过程中严重依赖大规模数据合成与搜索。本研究首次尝试构建一个达到奖牌获得者水平的几何大语言模型智能体，并提出了InternGeometry。该模型通过迭代提出命题与辅助构造、利用符号引擎进行验证，并基于引擎反馈进行反思以指导后续提议，从而克服了几何问题中的启发式局限。动态记忆机制使得InternGeometry能够在每个问题上与符号引擎进行超过两百次交互。为进一步加速学习过程，我们引入了复杂度提升强化学习方法，该方法在训练阶段逐步增加合成问题的复杂度。基于InternThinker-32B构建的InternGeometry，仅使用1.3万个训练样本（仅为AlphaGeometry 2所用数据量的0.004%），便成功解决了2000年至2024年间50道国际数学奥林匹克几何问题中的44道，超过了金牌获得者的平均得分（40.9分），这证明了大语言模型智能体在专家级几何任务上的潜力。此外，InternGeometry能够为国际数学奥林匹克问题提出人类解答中未曾出现的新颖辅助构造。我们将公开模型、数据及符号引擎，以支持后续研究。",
    "url": "https://huggingface.co/papers/2512.10534",
    "arxiv_url": "https://arxiv.org/abs/2512.10534"
  },
  {
    "title": "MoCapAnything: Unified 3D Motion Capture for Arbitrary Skeletons from Monocular Videos",
    "summary": "Motion capture now underpins content creation far beyond digital humans, yet most existing pipelines remain species- or template-specific. We formalize this gap as Category-Agnostic Motion Capture (CAMoCap): given a monocular video and an arbitrary rigged 3D asset as a prompt, the goal is to reconstruct a rotation-based animation such as BVH that directly drives the specific asset. We present MoCapAnything, a reference-guided, factorized framework that first predicts 3D joint trajectories and then recovers asset-specific rotations via constraint-aware inverse kinematics. The system contains three learnable modules and a lightweight IK stage: (1) a Reference Prompt Encoder that extracts per-joint queries from the asset's skeleton, mesh, and rendered images; (2) a Video Feature Extractor that computes dense visual descriptors and reconstructs a coarse 4D deforming mesh to bridge the gap between video and joint space; and (3) a Unified Motion Decoder that fuses these cues to produce temporally coherent trajectories. We also curate Truebones Zoo with 1038 motion clips, each providing a standardized skeleton-mesh-render triad. Experiments on both in-domain benchmarks and in-the-wild videos show that MoCapAnything delivers high-quality skeletal animations and exhibits meaningful cross-species retargeting across heterogeneous rigs, enabling scalable, prompt-driven 3D motion capture for arbitrary assets. Project page: https://animotionlab.github.io/MoCapAnything/",
    "translation": "标题：MoCapAnything：基于单目视频的任意骨架统一三维运动捕捉\n\n摘要：运动捕捉技术如今已支撑起远超数字人范畴的内容创作，但现有流程大多仍局限于特定物种或预设模板。本文将这一局限形式化为类别无关运动捕捉（CAMoCap）：给定单目视频与任意绑定三维资产作为提示，其目标是重建可直接驱动该特定资产的基于旋转的动画（如BVH格式）。我们提出MoCapAnything——一个参考引导的因子化框架，首先生成三维关节轨迹，再通过约束感知逆向运动学恢复资产专属旋转。该系统包含三个可学习模块与一个轻量级逆向运动学阶段：（1）参考提示编码器：从资产骨架、网格及渲染图像中提取逐关节查询；（2）视频特征提取器：计算稠密视觉描述符并重建粗糙四维变形网格，以弥合视频与关节空间之间的鸿沟；（3）统一运动解码器：融合多模态线索以生成时序连贯的轨迹。我们还构建了包含1038个动作片段的Truebones Zoo数据集，每个片段均提供标准化的骨架-网格-渲染三元组。在领域内基准测试与真实场景视频上的实验表明，MoCapAnything能生成高质量骨骼动画，并在异构绑定资产间实现有效的跨物种动作重定向，为任意资产提供了可扩展的提示驱动三维运动捕捉方案。项目页面：https://animotionlab.github.io/MoCapAnything/",
    "url": "https://huggingface.co/papers/2512.10881",
    "arxiv_url": "https://arxiv.org/abs/2512.10881"
  },
  {
    "title": "BEAVER: An Efficient Deterministic LLM Verifier",
    "summary": "As large language models (LLMs) transition from research prototypes to production systems, practitioners often need reliable methods to verify that model outputs satisfy required constraints. While sampling-based estimates provide an intuition of model behavior, they offer no sound guarantees. We present BEAVER, the first practical framework for computing deterministic, sound probability bounds on LLM constraint satisfaction. Given any prefix-closed semantic constraint, BEAVER systematically explores the generation space using novel token trie and frontier data structures, maintaining provably sound bounds at every iteration. We formalize the verification problem, prove soundness of our approach, and evaluate BEAVER on correctness verification, privacy verification and secure code generation tasks across multiple state of the art LLMs. BEAVER achieves 6 to 8 times tighter probability bounds and identifies 3 to 4 times more high risk instances compared to baseline methods under identical computational budgets, enabling precise characterization and risk assessment that loose bounds or empirical evaluation cannot provide.",
    "translation": "标题：BEAVER：一种高效的大语言模型确定性验证器\n\n摘要：随着大语言模型从研究原型转向生产系统，实践者通常需要可靠的方法来验证模型输出是否满足特定约束。虽然基于采样的估计方法能够提供模型行为的直观认知，但其无法提供严格的理论保证。本文提出BEAVER，这是首个用于计算大语言模型约束满足的确定性、严格概率边界的实用框架。针对任意前缀封闭的语义约束，BEAVER通过创新的词汇树和边界数据结构系统性地探索生成空间，并在每次迭代中保持可证明的严格边界。我们形式化了验证问题，证明了方法的可靠性，并在多个前沿大语言模型上对BEAVER进行了正确性验证、隐私验证和安全代码生成任务的评估。在相同计算资源下，BEAVER获得的概率边界比基线方法紧缩6至8倍，识别出的高风险实例数量增加3至4倍，从而实现了宽松边界或经验评估无法提供的精确特性刻画与风险评估能力。",
    "url": "https://huggingface.co/papers/2512.05439",
    "arxiv_url": "https://arxiv.org/abs/2512.05439"
  },
  {
    "title": "From Macro to Micro: Benchmarking Microscopic Spatial Intelligence on Molecules via Vision-Language Models",
    "summary": "This paper introduces the concept of Microscopic Spatial Intelligence (MiSI), the capability to perceive and reason about the spatial relationships of invisible microscopic entities, which is fundamental to scientific discovery. To assess the potential of Vision-Language Models (VLMs) in this domain, we propose a systematic benchmark framework MiSI-Bench. This framework features over 163,000 question-answer pairs and 587,000 images derived from approximately 4,000 molecular structures, covering nine complementary tasks that evaluate abilities ranging from elementary spatial transformations to complex relational identifications. Experimental results reveal that current state-of-the-art VLMs perform significantly below human level on this benchmark. However, a fine-tuned 7B model demonstrates substantial potential, even surpassing humans in spatial transformation tasks, while its poor performance in scientifically-grounded tasks like hydrogen bond recognition underscores the necessity of integrating explicit domain knowledge for progress toward scientific AGI. The datasets are available at https://huggingface.co/datasets/zongzhao/MiSI-bench.",
    "translation": "标题：从宏观到微观：基于视觉语言模型的分子微观空间智能基准测试\n\n摘要：本文提出微观空间智能（MiSI）的概念，即感知和推理不可见微观实体空间关系的能力，这是科学发现的基础。为评估视觉语言模型（VLMs）在该领域的潜力，我们提出系统性基准框架MiSI-Bench。该框架包含超过16.3万个问答对和58.7万张图像，数据源自约4000个分子结构，涵盖九项互补任务，评估能力范围从基础空间变换到复杂关系识别。实验结果表明，当前最先进的VLMs在此基准测试中的表现显著低于人类水平。然而，经过微调的70亿参数模型展现出巨大潜力，甚至在空间变换任务中超越人类表现，而其在氢键识别等科学基础任务中的薄弱表现，凸显了整合显式领域知识对于实现科学通用人工智能的必要性。数据集发布于https://huggingface.co/datasets/zongzhao/MiSI-bench。",
    "url": "https://huggingface.co/papers/2512.10867",
    "arxiv_url": "https://arxiv.org/abs/2512.10867"
  },
  {
    "title": "VQRAE: Representation Quantization Autoencoders for Multimodal Understanding, Generation and Reconstruction",
    "summary": "Unifying multimodal understanding, generation and reconstruction representation in a single tokenizer remains a key challenge in building unified models. Previous research predominantly attempts to address this in a dual encoder paradigm, e.g., utilizing the separate encoders for understanding and generation respectively or balancing semantic representations and low-level features with contrastive loss. In this paper, we propose VQRAE, a Vector Quantization version of Representation AutoEncoders, which pioneers the first exploration in unified representation to produce Continuous semantic features for image understanding and Discrete tokens for visual generation within a unified tokenizer. Specifically, we build upon pretrained vision foundation models with a symmetric ViT decoder and adopt a two-stage training strategy: first, it freezes the encoder and learns a high-dimensional semantic VQ codebook with pixel reconstruction objective; then jointly optimizes the encoder with self-distillation constraints. This design enables negligible semantic information for maintaining the ability of multimodal understanding, discrete tokens that are compatible for generation and fine-grained reconstruction. Besides, we identify the intriguing property in quantizing semantic encoders that rely on high-dimensional codebook in contrast to the previous common practice of low-dimensional codebook in image reconstruction. The semantic VQ codebook can achieve a 100% utilization ratio at a dimension of 1536. VQRAE presents competitive performance on several benchmarks of visual understanding, generation and reconstruction with promising scaling property in the autoregressive paradigm for its discrete merits.",
    "translation": "标题：VQRAE：面向多模态理解、生成与重建的表征量化自编码器\n\n摘要：在单一标记器中统一多模态理解、生成与重建表征，仍是构建统一模型的核心挑战。现有研究主要尝试在双编码器范式下解决此问题，例如分别使用独立编码器进行理解与生成，或通过对比损失平衡语义表征与低级特征。本文提出VQRAE（表征自编码器的向量量化版本），首次在统一标记器框架内探索联合表征，以生成用于图像理解的连续语义特征和用于视觉生成的离散标记。具体而言，我们基于预训练视觉基础模型构建对称ViT解码器，并采用两阶段训练策略：首先冻结编码器，以像素重建为目标学习高维语义向量量化码本；随后通过自蒸馏约束联合优化编码器。该设计在保持多模态理解能力的同时，仅引入可忽略的语义信息损失，并产生兼容生成任务与细粒度重建的离散标记。此外，我们发现语义编码器量化需依赖高维码本，这与图像重建中普遍采用低维码本的常见实践形成对比。实验表明，语义向量量化码本在1536维度下可实现100%的利用率。凭借其离散化优势，VQRAE在视觉理解、生成与重建的多个基准测试中展现出竞争力，并在自回归范式中表现出良好的扩展特性。",
    "url": "https://huggingface.co/papers/2511.23386",
    "arxiv_url": "https://arxiv.org/abs/2511.23386"
  },
  {
    "title": "Evaluating Gemini Robotics Policies in a Veo World Simulator",
    "summary": "Generative world models hold significant potential for simulating interactions with visuomotor policies in varied environments. Frontier video models can enable generation of realistic observations and environment interactions in a scalable and general manner. However, the use of video models in robotics has been limited primarily to in-distribution evaluations, i.e., scenarios that are similar to ones used to train the policy or fine-tune the base video model. In this report, we demonstrate that video models can be used for the entire spectrum of policy evaluation use cases in robotics: from assessing nominal performance to out-of-distribution (OOD) generalization, and probing physical and semantic safety. We introduce a generative evaluation system built upon a frontier video foundation model (Veo). The system is optimized to support robot action conditioning and multi-view consistency, while integrating generative image-editing and multi-view completion to synthesize realistic variations of real-world scenes along multiple axes of generalization. We demonstrate that the system preserves the base capabilities of the video model to enable accurate simulation of scenes that have been edited to include novel interaction objects, novel visual backgrounds, and novel distractor objects. This fidelity enables accurately predicting the relative performance of different policies in both nominal and OOD conditions, determining the relative impact of different axes of generalization on policy performance, and performing red teaming of policies to expose behaviors that violate physical or semantic safety constraints. We validate these capabilities through 1600+ real-world evaluations of eight Gemini Robotics policy checkpoints and five tasks for a bimanual manipulator.",
    "translation": "标题：在Veo世界模拟器中评估Gemini机器人策略\n\n摘要：生成式世界模型在模拟不同环境中视觉运动策略的交互方面具有巨大潜力。前沿视频模型能够以可扩展且通用的方式生成逼真的观测结果与环境交互。然而，视频模型在机器人领域的应用主要局限于分布内评估，即评估与训练策略或微调基础视频模型时所用场景相似的场景。本报告证明，视频模型可应用于机器人策略评估的全场景：从评估基准性能到分布外泛化能力，再到探究物理安全与语义安全约束。我们提出了一种基于前沿视频基础模型（Veo）构建的生成式评估系统。该系统经优化可支持机器人动作条件控制与多视角一致性，同时集成生成式图像编辑与多视角补全技术，能够沿多个泛化维度合成真实场景的逼真变体。实验表明，该系统保留了基础视频模型的核心能力，能够精确模拟经过编辑的场景——包括添加新型交互物体、新颖视觉背景及干扰物体。这种高保真特性使得系统能够准确预测不同策略在基准条件与分布外条件下的相对性能，确定不同泛化维度对策略性能的影响程度，并对策略进行红队测试以暴露违反物理或语义安全约束的行为。我们通过对八种Gemini机器人策略检查点及双臂操作器的五项任务进行1600余次真实世界评估，验证了该系统的各项能力。",
    "url": "https://huggingface.co/papers/2512.10675",
    "arxiv_url": "https://arxiv.org/abs/2512.10675"
  },
  {
    "title": "Thinking with Images via Self-Calling Agent",
    "summary": "Thinking-with-images paradigms have showcased remarkable visual reasoning capability by integrating visual information as dynamic elements into the Chain-of-Thought (CoT). However, optimizing interleaved multimodal CoT (iMCoT) through reinforcement learning remains challenging, as it relies on scarce high-quality reasoning data. In this study, we propose Self-Calling Chain-of-Thought (sCoT), a novel visual reasoning paradigm that reformulates iMCoT as a language-only CoT with self-calling. Specifically, a main agent decomposes the complex visual reasoning task to atomic subtasks and invokes its virtual replicas, i.e. parameter-sharing subagents, to solve them in isolated context. sCoT enjoys substantial training effectiveness and efficiency, as it requires no explicit interleaving between modalities. sCoT employs group-relative policy optimization to reinforce effective reasoning behavior to enhance optimization. Experiments on HR-Bench 4K show that sCoT improves the overall reasoning performance by up to 1.9% with sim 75% fewer GPU hours compared to strong baseline approaches. Code is available at https://github.com/YWenxi/think-with-images-through-self-calling.",
    "translation": "标题：基于自调用智能体的图像思维推理\n\n摘要：图像思维推理范式通过将视觉信息作为动态元素整合至思维链中，已展现出卓越的视觉推理能力。然而，由于依赖稀缺的高质量推理数据，通过强化学习优化交错式多模态思维链仍面临挑战。本研究提出自调用思维链——一种创新的视觉推理范式，将交错式多模态思维链重构为具有自调用机制的单语言思维链。具体而言，主智能体将复杂视觉推理任务分解为原子子任务，并调用其虚拟副本（即参数共享子智能体）在隔离上下文中解决问题。该范式无需显式的模态交错处理，从而显著提升训练效能与效率。通过采用组相对策略优化方法强化有效推理行为，进一步提升了优化效果。在HR-Bench 4K数据集上的实验表明：相较于强基线方法，自调用思维链在减少约75%GPU计算时的同时，将整体推理性能提升最高达1.9%。代码已开源：https://github.com/YWenxi/think-with-images-through-self-calling。",
    "url": "https://huggingface.co/papers/2512.08511",
    "arxiv_url": "https://arxiv.org/abs/2512.08511"
  },
  {
    "title": "StereoSpace: Depth-Free Synthesis of Stereo Geometry via End-to-End Diffusion in a Canonical Space",
    "summary": "We introduce StereoSpace, a diffusion-based framework for monocular-to-stereo synthesis that models geometry purely through viewpoint conditioning, without explicit depth or warping. A canonical rectified space and the conditioning guide the generator to infer correspondences and fill disocclusions end-to-end. To ensure fair and leakage-free evaluation, we introduce an end-to-end protocol that excludes any ground truth or proxy geometry estimates at test time. The protocol emphasizes metrics reflecting downstream relevance: iSQoE for perceptual comfort and MEt3R for geometric consistency. StereoSpace surpasses other methods from the warp & inpaint, latent-warping, and warped-conditioning categories, achieving sharp parallax and strong robustness on layered and non-Lambertian scenes. This establishes viewpoint-conditioned diffusion as a scalable, depth-free solution for stereo generation.",
    "translation": "标题：StereoSpace：基于规范空间端到端扩散的无深度立体几何合成方法\n\n摘要：本文提出StereoSpace，一种基于扩散模型的单目到立体合成框架，该框架仅通过视角条件建模几何关系，无需显式深度估计或图像变形操作。通过构建规范矫正空间并结合条件引导，生成器能够端到端地推断对应关系并补全遮挡区域。为确保评估的公平性与无信息泄漏，我们设计了一种端到端评估协议，在测试阶段完全排除真实几何数据或代理几何估计的干扰。该协议重点关注反映下游应用价值的指标：感知舒适度指标iSQoE与几何一致性指标MEt3R。实验表明，StereoSpace在图像变形修复、潜在空间变形及变形条件引导等各类方法中均取得最优性能，在分层场景与非朗伯场景中实现了清晰的视差效果与强鲁棒性。这确立了视角条件扩散模型作为一种可扩展、无需深度信息的立体生成解决方案的有效性。",
    "url": "https://huggingface.co/papers/2512.10959",
    "arxiv_url": "https://arxiv.org/abs/2512.10959"
  },
  {
    "title": "Stronger Normalization-Free Transformers",
    "summary": "Although normalization layers have long been viewed as indispensable components of deep learning architectures, the recent introduction of Dynamic Tanh (DyT) has demonstrated that alternatives are possible. The point-wise function DyT constrains extreme values for stable convergence and reaches normalization-level performance; this work seeks further for function designs that can surpass it. We first study how the intrinsic properties of point-wise functions influence training and performance. Building on these findings, we conduct a large-scale search for a more effective function design. Through this exploration, we introduce Derf(x) = erf(αx + s), where erf(x) is the rescaled Gaussian cumulative distribution function, and identify it as the most performant design. Derf outperforms LayerNorm, RMSNorm, and DyT across a wide range of domains, including vision (image recognition and generation), speech representation, and DNA sequence modeling. Our findings suggest that the performance gains of Derf largely stem from its improved generalization rather than stronger fitting capacity. Its simplicity and stronger performance make Derf a practical choice for normalization-free Transformer architectures.",
    "translation": "标题：更强无需归一化的Transformer模型\n\n摘要：尽管归一化层长期以来被视为深度学习架构中不可或缺的组成部分，但近期提出的动态双曲正切（DyT）函数表明替代方案是存在的。该点态函数DyT通过约束极端值以实现稳定收敛，并达到了归一化级别的性能；本研究旨在进一步探索能够超越其性能的函数设计。我们首先研究了点态函数的内在特性如何影响训练与模型表现。基于这些发现，我们进行了大规模搜索以寻求更有效的函数设计。通过系统探索，我们提出了Derf(x) = erf(αx + s)函数（其中erf(x)为缩放后的高斯累积分布函数），并确认其为当前最优设计。Derf在视觉（图像识别与生成）、语音表征及DNA序列建模等多个领域均优于层归一化（LayerNorm）、均方根归一化（RMSNorm）及DyT。研究结果表明，Derf的性能提升主要源于其更强的泛化能力而非拟合能力。其简洁性与卓越性能使得Derf成为无需归一化的Transformer架构的理想选择。",
    "url": "https://huggingface.co/papers/2512.10938",
    "arxiv_url": "https://arxiv.org/abs/2512.10938"
  },
  {
    "title": "The FACTS Leaderboard: A Comprehensive Benchmark for Large Language Model Factuality",
    "summary": "We introduce The FACTS Leaderboard, an online leaderboard suite and associated set of benchmarks that comprehensively evaluates the ability of language models to generate factually accurate text across diverse scenarios. The suite provides a holistic measure of factuality by aggregating the performance of models on four distinct sub-leaderboards: (1) FACTS Multimodal, which measures the factuality of responses to image-based questions; (2) FACTS Parametric, which assesses models' world knowledge by answering closed-book factoid questions from internal parameters; (3) FACTS Search, which evaluates factuality in information-seeking scenarios, where the model must use a search API; and (4) FACTS Grounding (v2), which evaluates whether long-form responses are grounded in provided documents, featuring significantly improved judge models. Each sub-leaderboard employs automated judge models to score model responses, and the final suite score is an average of the four components, designed to provide a robust and balanced assessment of a model's overall factuality. The FACTS Leaderboard Suite will be actively maintained, containing both public and private splits to allow for external participation while guarding its integrity. It can be found at https://www.kaggle.com/benchmarks/google/facts .",
    "translation": "标题：FACTS排行榜：大型语言模型事实性综合评估基准\n\n摘要：本文推出FACTS排行榜——一个在线综合评测体系及其关联基准，旨在全面评估语言模型在不同场景下生成事实准确文本的能力。该体系通过整合模型在四个独立子榜单的表现，提供对事实性的整体度量：(1) FACTS多模态榜单，通过图像问答任务评估响应的事实性；(2) FACTS参数化榜单，通过闭卷事实类问题测试模型基于内部参数的世界知识；(3) FACTS搜索榜单，在信息检索场景中评估模型使用搜索API时的事实准确性；(4) FACTS文本锚定榜单（v2版），评估长文本回答是否基于给定文档，其判定模型性能显著提升。各子榜单均采用自动化判定模型对输出进行评分，最终体系得分为四项得分的平均值，从而实现对模型整体事实性的稳健均衡评估。FACTS排行榜体系将动态维护，包含公开与私有数据分区，在保障体系完整性的同时支持外部参与。该平台可通过 https://www.kaggle.com/benchmarks/google/facts 访问。",
    "url": "https://huggingface.co/papers/2512.10791",
    "arxiv_url": "https://arxiv.org/abs/2512.10791"
  },
  {
    "title": "Tool-Augmented Spatiotemporal Reasoning for Streamlining Video Question Answering Task",
    "summary": "Video Question Answering (VideoQA) task serves as a critical playground for evaluating whether foundation models can effectively perceive, understand, and reason about dynamic real-world scenarios. However, existing Multimodal Large Language Models (MLLMs) struggle with simultaneously modeling spatial relationships within video frames and understanding the causal dynamics of temporal evolution on complex and reasoning-intensive VideoQA task. In this work, we equip MLLM with a comprehensive and extensible Video Toolkit, to enhance MLLM's spatiotemporal reasoning capabilities and ensure the harmony between the quantity and diversity of tools. To better control the tool invocation sequence and avoid toolchain shortcut issues, we propose a Spatiotemporal Reasoning Framework (STAR) that strategically schedules temporal and spatial tools, thereby progressively localizing the key area in the video. Our STAR framework enhances GPT-4o using lightweight tools, achieving an 8.2% gain on VideoMME and 4.6% on LongVideoBench. We believe that our proposed Video Toolkit and STAR framework make an important step towards building autonomous and intelligent video analysis assistants. The code is publicly available at https://github.com/fansunqi/VideoTool.",
    "translation": "标题：工具增强的时空推理：简化视频问答任务的框架\n\n摘要：视频问答任务作为评估基础模型能否有效感知、理解并推理动态现实场景的关键测试平台。然而，现有的多模态大语言模型在复杂且需要深度推理的视频问答任务中，难以同时建模视频帧内的空间关系并理解时间演化的因果动态。为此，本研究为多模态大语言模型配备了一套全面且可扩展的视频工具包，以增强其时空推理能力，并确保工具数量与多样性的协调。为更好地控制工具调用顺序并避免工具链捷径问题，我们提出了一种时空推理框架，该框架策略性地调度时间与空间工具，从而逐步定位视频中的关键区域。我们的时空推理框架通过轻量级工具增强了GPT-4o的性能，在VideoMME基准上实现了8.2%的性能提升，在LongVideoBench上提升了4.6%。我们相信，所提出的视频工具包与时空推理框架为构建自主智能的视频分析助手迈出了重要一步。代码已公开于https://github.com/fansunqi/VideoTool。",
    "url": "https://huggingface.co/papers/2512.10359",
    "arxiv_url": "https://arxiv.org/abs/2512.10359"
  },
  {
    "title": "H2R-Grounder: A Paired-Data-Free Paradigm for Translating Human Interaction Videos into Physically Grounded Robot Videos",
    "summary": "Robots that learn manipulation skills from everyday human videos could acquire broad capabilities without tedious robot data collection. We propose a video-to-video translation framework that converts ordinary human-object interaction videos into motion-consistent robot manipulation videos with realistic, physically grounded interactions. Our approach does not require any paired human-robot videos for training only a set of unpaired robot videos, making the system easy to scale. We introduce a transferable representation that bridges the embodiment gap: by inpainting the robot arm in training videos to obtain a clean background and overlaying a simple visual cue (a marker and arrow indicating the gripper's position and orientation), we can condition a generative model to insert the robot arm back into the scene. At test time, we apply the same process to human videos (inpainting the person and overlaying human pose cues) and generate high-quality robot videos that mimic the human's actions. We fine-tune a SOTA video diffusion model (Wan 2.2) in an in-context learning manner to ensure temporal coherence and leveraging of its rich prior knowledge. Empirical results demonstrate that our approach achieves significantly more realistic and grounded robot motions compared to baselines, pointing to a promising direction for scaling up robot learning from unlabeled human videos. Project page: https://showlab.github.io/H2R-Grounder/",
    "translation": "标题：H2R-Grounder：一种无需配对数据的范式，用于将人类交互视频转化为物理接地的机器人视频\n\n摘要：通过从日常人类视频中学习操作技能，机器人无需繁琐的机器人数据收集即可获得广泛的能力。我们提出了一种视频到视频的翻译框架，该框架将普通的人-物交互视频转换为具有真实、物理接地交互且运动一致的机器人操作视频。我们的方法在训练过程中不需要任何配对的人类-机器人视频，仅需一组未配对的机器人视频，这使得系统易于扩展。我们引入了一种可迁移的表征来弥合“具身鸿沟”：通过在训练视频中对机器人手臂进行修复以获取干净的背景，并叠加一个简单的视觉提示（指示夹爪位置和方向的标记与箭头），我们可以条件化一个生成模型，将机器人手臂重新插入场景中。在测试时，我们对人类视频应用相同的过程（修复人体并叠加人体姿态提示），从而生成模仿人类动作的高质量机器人视频。我们以情境学习的方式对最先进的视频扩散模型（Wan 2.2）进行微调，以确保时间一致性并利用其丰富的先验知识。实验结果表明，与基线方法相比，我们的方法能生成显著更真实、更接地的机器人运动，这为从无标签人类视频中扩展机器人学习指明了一个有前景的方向。项目页面：https://showlab.github.io/H2R-Grounder/",
    "url": "https://huggingface.co/papers/2512.09406",
    "arxiv_url": "https://arxiv.org/abs/2512.09406"
  },
  {
    "title": "MoRel: Long-Range Flicker-Free 4D Motion Modeling via Anchor Relay-based Bidirectional Blending with Hierarchical Densification",
    "summary": "Recent advances in 4D Gaussian Splatting (4DGS) have extended the high-speed rendering capability of 3D Gaussian Splatting (3DGS) into the temporal domain, enabling real-time rendering of dynamic scenes. However, one of the major remaining challenges lies in modeling long-range motion-contained dynamic videos, where a naive extension of existing methods leads to severe memory explosion, temporal flickering, and failure to handle appearing or disappearing occlusions over time. To address these challenges, we propose a novel 4DGS framework characterized by an Anchor Relay-based Bidirectional Blending (ARBB) mechanism, named MoRel, which enables temporally consistent and memory-efficient modeling of long-range dynamic scenes. Our method progressively constructs locally canonical anchor spaces at key-frame time index and models inter-frame deformations at the anchor level, enhancing temporal coherence. By learning bidirectional deformations between KfA and adaptively blending them through learnable opacity control, our approach mitigates temporal discontinuities and flickering artifacts. We further introduce a Feature-variance-guided Hierarchical Densification (FHD) scheme that effectively densifies KfA's while keeping rendering quality, based on an assigned level of feature-variance. To effectively evaluate our model's capability to handle real-world long-range 4D motion, we newly compose long-range 4D motion-contained dataset, called SelfCap_{LR}. It has larger average dynamic motion magnitude, captured at spatially wider spaces, compared to previous dynamic video datasets. Overall, our MoRel achieves temporally coherent and flicker-free long-range 4D reconstruction while maintaining bounded memory usage, demonstrating both scalability and efficiency in dynamic Gaussian-based representations.",
    "translation": "标题：MoRel：基于锚点中继双向混合与分层致密化的长时程无闪烁四维运动建模\n\n摘要：四维高斯泼溅（4DGS）技术的最新进展将三维高斯泼溅（3DGS）的高速渲染能力扩展至时间域，实现了动态场景的实时渲染。然而，当前仍面临的主要挑战之一在于对包含长时程运动的动态视频进行建模，现有方法的简单扩展会导致严重的内存爆炸、时间闪烁，并无法处理随时间出现或消失的遮挡。为解决这些挑战，我们提出了一种新颖的4DGS框架，其核心为基于锚点中继的双向混合（ARBB）机制，命名为MoRel。该框架能够以内存高效的方式对长时程动态场景进行时间一致性的建模。我们的方法在关键帧时间索引处逐步构建局部规范锚点空间，并在锚点层级建模帧间形变，从而增强时间连贯性。通过学习关键帧锚点之间的双向形变，并通过可学习的透明度控制进行自适应混合，我们的方法有效缓解了时间不连续性和闪烁伪影。我们进一步提出了一种基于特征方差指导的分层致密化（FHD）方案，该方案根据指定的特征方差级别，在保持渲染质量的同时有效致密化关键帧锚点。为有效评估模型处理真实世界长时程四维运动的能力，我们新构建了一个包含长时程四维运动的数据集，命名为SelfCap_{LR}。与先前的动态视频数据集相比，该数据集在空间更广的范围内捕获，具有更大的平均动态运动幅度。总体而言，我们的MoRel方法在保持有限内存使用的同时，实现了时间连贯且无闪烁的长时程四维重建，证明了基于高斯的动态表征兼具可扩展性与高效性。",
    "url": "https://huggingface.co/papers/2512.09270",
    "arxiv_url": "https://arxiv.org/abs/2512.09270"
  },
  {
    "title": "Omni-Attribute: Open-vocabulary Attribute Encoder for Visual Concept Personalization",
    "summary": "Visual concept personalization aims to transfer only specific image attributes, such as identity, expression, lighting, and style, into unseen contexts. However, existing methods rely on holistic embeddings from general-purpose image encoders, which entangle multiple visual factors and make it difficult to isolate a single attribute. This often leads to information leakage and incoherent synthesis. To address this limitation, we introduce Omni-Attribute, the first open-vocabulary image attribute encoder designed to learn high-fidelity, attribute-specific representations. Our approach jointly designs the data and model: (i) we curate semantically linked image pairs annotated with positive and negative attributes to explicitly teach the encoder what to preserve or suppress; and (ii) we adopt a dual-objective training paradigm that balances generative fidelity with contrastive disentanglement. The resulting embeddings prove effective for open-vocabulary attribute retrieval, personalization, and compositional generation, achieving state-of-the-art performance across multiple benchmarks.",
    "translation": "标题：Omni-Attribute：面向视觉概念个性化的开放词汇属性编码器\n\n摘要：视觉概念个性化的目标在于仅将特定图像属性（如身份、表情、光照和风格）迁移至未见过的场景中。然而，现有方法依赖于通用图像编码器提取的整体嵌入表示，这些表示往往纠缠了多种视觉因素，导致难以分离单一属性，从而引发信息泄露与合成结果不连贯的问题。为克服这一局限，本文提出了Omni-Attribute——首个专为学习高保真、属性特异性表示而设计的开放词汇图像属性编码器。我们的方法从数据与模型两方面进行协同设计：（1）我们构建了带有正负属性标注的语义关联图像对数据集，以显式指导编码器学习应保留或抑制的特征；（2）采用双目标训练范式，在生成保真度与对比解耦能力之间取得平衡。实验表明，所得到的嵌入表示在开放词汇属性检索、个性化迁移及组合生成任务中均表现出色，在多个基准测试中达到了最先进的性能水平。",
    "url": "https://huggingface.co/papers/2512.10955",
    "arxiv_url": "https://arxiv.org/abs/2512.10955"
  },
  {
    "title": "Confucius Code Agent: An Open-sourced AI Software Engineer at Industrial Scale",
    "summary": "Real-world AI software engineering demands coding agents that can reason over massive repositories, maintain durable memory across and within long sessions, and robustly coordinate complex toolchains at test time. Existing open-source coding agents provide transparency but frequently fall short when pushed to these industrial-scale workloads, while proprietary coding agents offer strong practical performance but limited extensibility, interpretability, and controllability. We present the Confucius Code Agent (CCA), an open-sourced AI software engineer that can operate at an industrial scale. CCA is built atop the Confucius SDK, an open-sourced agent development platform designed around three complementary perspectives: Agent Experience (AX), User Experience (UX), and Developer Experience (DX). The SDK introduces a unified orchestrator with hierarchical working memory for long-context reasoning, a persistent note-taking system for cross-session continual learning, and a modular extension module for robust tool use. Moreover, a meta-agent automates the synthesis, evaluation, and refinement of agent configurations through a build-test-improve loop, enabling rapid agent development on new tasks, environments, and tool stacks. Instantiated on Confucius SDK with these mechanisms, CCA delivers strong performance on real-world software engineering tasks. On SWE-Bench-Pro, CCA achieves a state-of-the-art Resolve@1 performance of 54.3%, substantially improving over prior coding agents. Together, the Confucius SDK and CCA provide a transparent, extensible, and reproducible foundation for AI agents, bridge gaps between research prototypes and production-grade systems, and support agent development and deployment at industrial scale.",
    "translation": "标题：孔子代码智能体：工业级开源人工智能软件工程师\n\n摘要：现实世界的人工智能软件工程需要具备以下能力的编码智能体：能够对海量代码库进行推理、在长会话中保持持久记忆、并在测试阶段稳健协调复杂工具链。现有开源编码智能体虽具透明度，但在应对工业级工作负载时常显不足；而专有编码智能体虽实践性能较强，却在可扩展性、可解释性与可控性方面存在局限。本文提出孔子代码智能体——一个可在工业级规模运行的开源人工智能软件工程师。该智能体基于孔子软件开发工具包构建，该开源智能体开发平台围绕三个互补维度设计：智能体体验、用户体验与开发者体验。该工具包引入具备分层工作记忆的统一编排器以实现长上下文推理，通过持久化笔记系统支持跨会话持续学习，并采用模块化扩展机制保障工具调用的鲁棒性。此外，元智能体通过“构建-测试-优化”循环自动完成智能体配置的合成、评估与改进，使其能快速适应新任务、新环境与新工具栈。基于孔子软件开发工具包实例化的孔子代码智能体在真实软件工程任务中展现出卓越性能：在SWE-Bench-Pro基准测试中取得54.3%的最优Resolve@1指标，较现有编码智能体实现显著提升。孔子软件开发工具包与孔子代码智能体共同为人工智能智能体提供了透明、可扩展、可复现的基础框架，弥合了研究原型与生产级系统之间的鸿沟，为工业级智能体的开发与部署提供支撑。",
    "url": "https://huggingface.co/papers/2512.10398",
    "arxiv_url": "https://arxiv.org/abs/2512.10398"
  },
  {
    "title": "ReViSE: Towards Reason-Informed Video Editing in Unified Models with Self-Reflective Learning",
    "summary": "Video unified models exhibit strong capabilities in understanding and generation, yet they struggle with reason-informed visual editing even when equipped with powerful internal vision-language models (VLMs). We attribute this gap to two factors: 1) existing datasets are inadequate for training and evaluating reasoning-aware video editing, and 2) an inherent disconnect between the models' reasoning and editing capabilities, which prevents the rich understanding from effectively instructing the editing process. Bridging this gap requires an integrated framework that connects reasoning with visual transformation. To address this gap, we introduce the Reason-Informed Video Editing (RVE) task, which requires reasoning about physical plausibility and causal dynamics during editing. To support systematic evaluation, we construct RVE-Bench, a comprehensive benchmark with two complementary subsets: Reasoning-Informed Video Editing and In-Context Video Generation. These subsets cover diverse reasoning dimensions and real-world editing scenarios. Building upon this foundation, we propose the ReViSE, a Self-Reflective Reasoning (SRF) framework that unifies generation and evaluation within a single architecture. The model's internal VLM provides intrinsic feedback by assessing whether the edited video logically satisfies the given instruction. The differential feedback that refines the generator's reasoning behavior during training. Extensive experiments on RVE-Bench demonstrate that ReViSE significantly enhances editing accuracy and visual fidelity, achieving a 32% improvement of the Overall score in the reasoning-informed video editing subset over state-of-the-art methods.",
    "translation": "标题：ReViSE：基于自反思学习的统一模型推理感知视频编辑研究\n\n摘要：视频统一模型在内容理解与生成方面展现出强大能力，但即使配备高性能内部视觉语言模型，其在推理感知的视频编辑任务中仍存在明显局限。我们认为这一差距源于两个关键因素：1）现有数据集难以支撑推理感知视频编辑任务的训练与评估；2）模型推理能力与编辑能力之间存在固有割裂，导致丰富的语义理解无法有效指导编辑过程。弥合这一差距需要建立连接推理与视觉转换的集成化框架。为此，我们提出推理感知视频编辑任务，该任务要求编辑过程中兼顾物理合理性与因果动态推理。为建立系统化评估体系，我们构建了RVE-Bench综合基准数据集，包含两个互补子集：推理感知视频编辑与上下文视频生成。这些子集覆盖多维推理场景与现实编辑需求。基于此，我们提出ReViSE框架——一种融合生成与评估的自反思推理架构。该模型通过内部视觉语言模型对编辑后视频是否符合指令逻辑进行内在评估，其产生的差异化反馈在训练过程中持续优化生成器的推理行为。在RVE-Bench上的大量实验表明，ReViSE显著提升了编辑准确度与视觉保真度，在推理感知视频编辑子集上的综合得分较现有最优方法提升32%。",
    "url": "https://huggingface.co/papers/2512.09924",
    "arxiv_url": "https://arxiv.org/abs/2512.09924"
  },
  {
    "title": "Fed-SE: Federated Self-Evolution for Privacy-Constrained Multi-Environment LLM Agents",
    "summary": "LLM agents are widely deployed in complex interactive tasks, yet privacy constraints often preclude centralized optimization and co-evolution across dynamic environments. While Federated Learning (FL) has proven effective on static datasets, its extension to the open-ended self-evolution of agents remains underexplored. Directly applying standard FL is challenging: heterogeneous tasks and sparse, trajectory-level rewards introduce severe gradient conflicts, destabilizing the global optimization process. To bridge this gap, we propose Fed-SE, a Federated Self-Evolution framework for LLM agents. Fed-SE establishes a local evolution-global aggregation paradigm. Locally, agents employ parameter-efficient fine-tuning on filtered, high-return trajectories to achieve stable gradient updates. Globally, Fed-SE aggregates updates within a low-rank subspace that disentangles environment-specific dynamics, effectively reducing negative transfer across clients. Experiments across five heterogeneous environments demonstrate that Fed-SE improves average task success rates by approximately 18% over federated baselines, validating its effectiveness in robust cross-environment knowledge transfer in privacy-constrained deployments.",
    "translation": "标题：Fed-SE：面向隐私受限多环境大语言模型智能体的联邦自进化框架\n\n摘要：大语言模型智能体已广泛应用于复杂的交互式任务，然而隐私限制往往阻碍了跨动态环境的集中式优化与协同进化。尽管联邦学习在静态数据集上已被证明有效，但其在智能体开放式自进化场景中的扩展仍待深入探索。直接应用标准联邦学习面临挑战：异构任务以及稀疏的轨迹级奖励会引发严重的梯度冲突，从而破坏全局优化过程的稳定性。为弥合这一差距，本文提出Fed-SE——一种面向大语言模型智能体的联邦自进化框架。Fed-SE建立了局部进化-全局聚合的范式：在局部层面，智能体通过对筛选出的高回报轨迹进行参数高效微调，实现稳定的梯度更新；在全局层面，Fed-SE将更新聚合于解耦环境特定动态的低秩子空间内，有效减少了客户端间的负迁移效应。在五个异构环境中的实验表明，Fed-SE相较于联邦基线方法平均任务成功率提升约18%，验证了其在隐私受限部署场景下实现鲁棒跨环境知识迁移的有效性。",
    "url": "https://huggingface.co/papers/2512.08870",
    "arxiv_url": "https://arxiv.org/abs/2512.08870"
  },
  {
    "title": "MOA: Multi-Objective Alignment for Role-Playing Agents",
    "summary": "Role-playing agents (RPAs) must simultaneously master many conflicting skills -- following multi-turn instructions, exhibiting domain knowledge, and adopting a consistent linguistic style. Existing work either relies on supervised fine-tuning (SFT) that over-fits surface cues and yields low diversity, or applies reinforcement learning (RL) that fails to learn multiple dimensions for comprehensive RPA optimization. We present MOA (Multi-Objective Alignment), a reinforcement-learning framework that enables multi-dimensional, fine-grained rubric optimization for general RPAs. MOA introduces a novel multi-objective optimization strategy that trains simultaneously on multiple fine-grained rubrics to boost optimization performance. Besides, to address the issues of model output diversity and quality, we have also employed thought-augmented rollout with off-policy guidance. Extensive experiments on challenging benchmarks such as PersonaGym and RoleMRC show that MOA enables an 8B model to match or even outperform strong baselines such as GPT-4o and Claude across numerous dimensions. This demonstrates the great potential of MOA in building RPAs that can simultaneously meet the demands of role knowledge, persona style, diverse scenarios, and complex multi-turn conversations.",
    "translation": "标题：MOA：面向角色扮演智能体的多目标对齐框架\n\n摘要：角色扮演智能体（RPAs）需同时掌握多项相互冲突的技能——遵循多轮次指令、展现领域知识并保持连贯的语言风格。现有方法要么依赖监督微调（SFT）导致对表面线索过拟合且生成多样性不足，要么采用强化学习（RL）难以实现多维度综合优化。本文提出MOA（多目标对齐），一种支持通用角色扮演智能体进行多维度细粒度指标优化的强化学习框架。MOA引入创新的多目标优化策略，可基于多个细粒度评估指标进行同步训练以提升优化性能。此外，为解决模型输出多样性与质量问题，我们采用思维增强推演与离轨策略引导机制。在PersonaGym和RoleMRC等挑战性基准测试中的大量实验表明，MOA能使80亿参数模型在多个维度上匹配甚至超越GPT-4o和Claude等强基线模型。这证明了MOA在构建同时满足角色知识、人物风格、多样化场景及复杂多轮对话需求的角色扮演智能体方面具有巨大潜力。",
    "url": "https://huggingface.co/papers/2512.09756",
    "arxiv_url": "https://arxiv.org/abs/2512.09756"
  },
  {
    "title": "X-Humanoid: Robotize Human Videos to Generate Humanoid Videos at Scale",
    "summary": "The advancement of embodied AI has unlocked significant potential for intelligent humanoid robots. However, progress in both Vision-Language-Action (VLA) models and world models is severely hampered by the scarcity of large-scale, diverse training data. A promising solution is to \"robotize\" web-scale human videos, which has been proven effective for policy training. However, these solutions mainly \"overlay\" robot arms to egocentric videos, which cannot handle complex full-body motions and scene occlusions in third-person videos, making them unsuitable for robotizing humans. To bridge this gap, we introduce X-Humanoid, a generative video editing approach that adapts the powerful Wan 2.2 model into a video-to-video structure and finetunes it for the human-to-humanoid translation task. This finetuning requires paired human-humanoid videos, so we designed a scalable data creation pipeline, turning community assets into 17+ hours of paired synthetic videos using Unreal Engine. We then apply our trained model to 60 hours of the Ego-Exo4D videos, generating and releasing a new large-scale dataset of over 3.6 million \"robotized\" humanoid video frames. Quantitative analysis and user studies confirm our method's superiority over existing baselines: 69% of users rated it best for motion consistency, and 62.1% for embodiment correctness.",
    "translation": "标题：X-Humanoid：规模化机器人化人类视频以生成仿人机器人视频\n\n摘要：具身人工智能的发展为智能仿人机器人开辟了巨大潜力。然而，视觉-语言-动作模型与世界模型的进展均因缺乏大规模、多样化的训练数据而受到严重制约。一种可行的解决方案是对网络规模的人类视频进行“机器人化”处理，该方法已被证明对策略训练有效。但现有方案主要将机械臂“叠加”于第一人称视角视频中，无法处理第三人称视频中复杂的全身运动与场景遮挡问题，因而难以适用于人类动作的机器人化转换。为填补这一空白，我们提出X-Humanoid——一种生成式视频编辑方法，该方法将强大的Wan 2.2模型适配为视频到视频结构，并针对人类到仿人机器人的转换任务进行微调。微调过程需要成对的人类-仿人机器人视频数据，为此我们设计了一套可扩展的数据生成流程，利用虚幻引擎将社区资源转化为超过17小时的配对合成视频。随后，我们将训练好的模型应用于60小时的Ego-Exo4D视频数据集，生成并发布了包含超过360万帧“机器人化”仿人视频的大规模新数据集。定量分析与用户研究证实了本方法相较于现有基线的优越性：69%的用户认为其在运动连贯性方面表现最佳，62.1%的用户认可其具身形态的正确性。",
    "url": "https://huggingface.co/papers/2512.04537",
    "arxiv_url": "https://arxiv.org/abs/2512.04537"
  },
  {
    "title": "DuetSVG: Unified Multimodal SVG Generation with Internal Visual Guidance",
    "summary": "Recent vision-language model (VLM)-based approaches have achieved impressive results on SVG generation. However, because they generate only text and lack visual signals during decoding, they often struggle with complex semantics and fail to produce visually appealing or geometrically coherent SVGs. We introduce DuetSVG, a unified multimodal model that jointly generates image tokens and corresponding SVG tokens in an end-to-end manner. DuetSVG is trained on both image and SVG datasets. At inference, we apply a novel test-time scaling strategy that leverages the model's native visual predictions as guidance to improve SVG decoding quality. Extensive experiments show that our method outperforms existing methods, producing visually faithful, semantically aligned, and syntactically clean SVGs across a wide range of applications.",
    "translation": "标题：DuetSVG：基于内部视觉引导的统一多模态SVG生成方法\n\n摘要：近期基于视觉语言模型的方法在SVG生成任务中取得了显著成果。然而，由于这些方法仅生成文本且在解码过程中缺乏视觉信号引导，它们往往难以处理复杂语义，且生成的SVG在视觉吸引力与几何一致性方面存在不足。本文提出DuetSVG，一种统一的多模态模型，能够以端到端方式联合生成图像标记与对应的SVG标记。该模型在图像与SVG数据集上进行训练。在推理阶段，我们采用一种新颖的测试时缩放策略，利用模型自身生成的视觉预测作为引导，以提升SVG解码质量。大量实验表明，本方法在多种应用场景中均优于现有方法，能够生成视觉逼真、语义对齐且语法简洁的SVG图形。",
    "url": "https://huggingface.co/papers/2512.10894",
    "arxiv_url": "https://arxiv.org/abs/2512.10894"
  },
  {
    "title": "DragMesh: Interactive 3D Generation Made Easy",
    "summary": "While generative models have excelled at creating static 3D content, the pursuit of systems that understand how objects move and respond to interactions remains a fundamental challenge. Current methods for articulated motion lie at a crossroads: they are either physically consistent but too slow for real-time use, or generative but violate basic kinematic constraints. We present DragMesh, a robust framework for real-time interactive 3D articulation built around a lightweight motion generation core. Our core contribution is a novel decoupled kinematic reasoning and motion generation framework. First, we infer the latent joint parameters by decoupling semantic intent reasoning (which determines the joint type) from geometric regression (which determines the axis and origin using our Kinematics Prediction Network (KPP-Net)). Second, to leverage the compact, continuous, and singularity-free properties of dual quaternions for representing rigid body motion, we develop a novel Dual Quaternion VAE (DQ-VAE). This DQ-VAE receives these predicted priors, along with the original user drag, to generate a complete, plausible motion trajectory. To ensure strict adherence to kinematics, we inject the joint priors at every layer of the DQ-VAE's non-autoregressive Transformer decoder using FiLM (Feature-wise Linear Modulation) conditioning. This persistent, multi-scale guidance is complemented by a numerically-stable cross-product loss to guarantee axis alignment. This decoupled design allows DragMesh to achieve real-time performance and enables plausible, generative articulation on novel objects without retraining, offering a practical step toward generative 3D intelligence. Code: https://github.com/AIGeeksGroup/DragMesh. Website: https://aigeeksgroup.github.io/DragMesh.",
    "translation": "标题：DragMesh：简易交互式三维生成\n\n摘要：尽管生成模型在创建静态三维内容方面表现出色，但开发能够理解物体如何运动并响应交互的系统仍然是一个根本性挑战。当前关于关节运动的方法正处于十字路口：它们要么物理一致但速度过慢无法实时使用，要么具有生成能力却违背基本运动学约束。我们提出DragMesh，这是一个围绕轻量级运动生成核心构建的、用于实时交互式三维关节运动的鲁棒框架。我们的核心贡献是一种新颖的解耦式运动学推理与运动生成框架。首先，我们通过将语义意图推理（用于确定关节类型）与几何回归（使用我们的运动学预测网络（KPP-Net）确定轴线和原点）解耦来推断潜在关节参数。其次，为了利用对偶四元数表示刚体运动时紧凑、连续且无奇点的特性，我们开发了一种新颖的对偶四元数变分自编码器（DQ-VAE）。该DQ-VAE接收这些预测的先验信息以及原始用户拖拽输入，以生成完整、合理的运动轨迹。为确保严格遵循运动学约束，我们使用FiLM（特征级线性调制）条件化技术，在DQ-VAE非自回归Transformer解码器的每一层注入关节先验。这种持续、多尺度的指导辅以数值稳定的叉积损失，以保证轴线对齐。这种解耦设计使DragMesh能够实现实时性能，并在无需重新训练的情况下对新物体进行合理的生成式关节运动，为生成式三维智能迈出了实用的一步。代码：https://github.com/AIGeeksGroup/DragMesh。项目网站：https://aigeeksgroup.github.io/DragMesh。",
    "url": "https://huggingface.co/papers/2512.06424",
    "arxiv_url": "https://arxiv.org/abs/2512.06424"
  }
]