[
  {
    "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization",
    "summary": "As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.",
    "translation": "标题：GDPO：面向多奖励强化学习优化的组奖励解耦归一化策略优化\n\n摘要：随着语言模型能力日益增强，用户不仅期望其提供准确响应，还要求其在多样场景中表现出符合不同人类偏好的行为。为实现这一目标，强化学习（RL）流程开始引入多个奖励信号，每个奖励捕获一种特定偏好，以引导模型朝向期望行为发展。然而，近期研究默认在多奖励设置下直接应用组相对策略优化（GRPO），而未检验其适用性。本文证明，直接应用GRPO对不同 rollout 奖励组合进行归一化会导致其坍缩为相同的优势值，从而降低训练信号的分辨率，导致收敛效果欠佳，甚至在某些情况下引发早期训练失败。为此，我们提出组奖励解耦归一化策略优化（GDPO），该方法通过解耦各奖励的归一化过程，更真实地保留其相对差异，实现更精确的多奖励优化，并显著提升训练稳定性。我们在工具调用、数学推理和代码推理三项任务中对比GDPO与GRPO，同时评估正确性指标（准确率、错误率）与约束遵循指标（格式、长度）。在所有实验设置下，GDPO均持续优于GRPO，证明了其在多奖励强化学习优化中的有效性和泛化能力。",
    "url": "https://huggingface.co/papers/2601.05242",
    "arxiv_url": "https://arxiv.org/abs/2601.05242"
  },
  {
    "title": "Learnable Multipliers: Freeing the Scale of Language Model Matrix Layers",
    "summary": "Applying weight decay (WD) to matrix layers is standard practice in large-language-model pretraining. Prior work suggests that stochastic gradient noise induces a Brownian-like expansion of the weight matrices W, whose growth is counteracted by WD, leading to a WD-noise equilibrium with a certain weight norm ||W||. In this work, we view the equilibrium norm as a harmful artifact of the training procedure, and address it by introducing learnable multipliers to learn the optimal scale. First, we attach a learnable scalar multiplier to W and confirm that the WD-noise equilibrium norm is suboptimal: the learned scale adapts to data and improves performance. We then argue that individual row and column norms are similarly constrained, and free their scale by introducing learnable per-row and per-column multipliers. Our method can be viewed as a learnable, more expressive generalization of muP multipliers. It outperforms a well-tuned muP baseline, reduces the computational overhead of multiplier tuning, and surfaces practical questions such as forward-pass symmetries and the width-scaling of the learned multipliers. Finally, we validate learnable multipliers with both Adam and Muon optimizers, where it shows improvement in downstream evaluations matching the improvement of the switching from Adam to Muon.",
    "translation": "标题：可学习乘子：释放语言模型矩阵层的尺度约束\n\n摘要：在大语言模型预训练中，对矩阵层施加权重衰减是标准实践。已有研究表明，随机梯度噪声会引发权重矩阵W的类布朗运动式扩张，而权重衰减则抑制这种扩张，最终形成具有特定权重范数||W||的权重衰减-噪声平衡态。本研究将该平衡态范数视为训练过程中的有害伪影，并通过引入可学习乘子来学习最优尺度以解决此问题。首先，我们在W上附加可学习的标量乘子，并证实权重衰减-噪声平衡范数具有次优性：学习得到的尺度能够自适应数据并提升模型性能。进而，我们论证了矩阵各行与各列的范数同样受此约束，通过引入可学习的行乘子与列乘子释放了其尺度限制。该方法可视为对μP乘子的一种可学习、更具表达力的泛化形式。实验表明，本方法优于经充分调优的μP基线，降低了乘子调优的计算开销，并引发出前向传播对称性、学习乘子的宽度缩放等实践性问题。最后，我们在Adam与Muon优化器上均验证了可学习乘子的有效性，其在下游任务评估中的提升效果与从Adam切换至Muon所带来的改进相当。\n\n摘要：[中文摘要]",
    "url": "https://huggingface.co/papers/2601.04890",
    "arxiv_url": "https://arxiv.org/abs/2601.04890"
  },
  {
    "title": "RL-AWB: Deep Reinforcement Learning for Auto White Balance Correction in Low-Light Night-time Scenes",
    "summary": "Nighttime color constancy remains a challenging problem in computational photography due to low-light noise and complex illumination conditions. We present RL-AWB, a novel framework combining statistical methods with deep reinforcement learning for nighttime white balance. Our method begins with a statistical algorithm tailored for nighttime scenes, integrating salient gray pixel detection with novel illumination estimation. Building on this foundation, we develop the first deep reinforcement learning approach for color constancy that leverages the statistical algorithm as its core, mimicking professional AWB tuning experts by dynamically optimizing parameters for each image. To facilitate cross-sensor evaluation, we introduce the first multi-sensor nighttime dataset. Experiment results demonstrate that our method achieves superior generalization capability across low-light and well-illuminated images. Project page: https://ntuneillee.github.io/research/rl-awb/",
    "translation": "标题：RL-AWB：基于深度强化学习的低光照夜间场景自动白平衡校正\n\n摘要：在计算摄影中，由于低光照噪声和复杂的照明条件，夜间色彩恒常性仍是一个具有挑战性的问题。本文提出RL-AWB，一种将统计方法与深度强化学习相结合的新型夜间白平衡校正框架。该方法首先采用一种专为夜间场景设计的统计算法，该算法融合了显著灰度像素检测与新颖的照明估计技术。在此基础上，我们开发了首个以该统计算法为核心的色彩恒常性深度强化学习方法，通过动态优化每幅图像的参数，模拟专业AWB调校专家的决策过程。为促进跨传感器评估，我们构建了首个多传感器夜间数据集。实验结果表明，该方法在低光照与良好光照图像上均展现出卓越的泛化能力。项目页面：https://ntuneillee.github.io/research/rl-awb/",
    "url": "https://huggingface.co/papers/2601.05249",
    "arxiv_url": "https://arxiv.org/abs/2601.05249"
  },
  {
    "title": "Token-Level LLM Collaboration via FusionRoute",
    "summary": "Large language models (LLMs) exhibit strengths across diverse domains. However, achieving strong performance across these domains with a single general-purpose model typically requires scaling to sizes that are prohibitively expensive to train and deploy. On the other hand, while smaller domain-specialized models are much more efficient, they struggle to generalize beyond their training distributions. To address this dilemma, we propose FusionRoute, a robust and effective token-level multi-LLM collaboration framework in which a lightweight router simultaneously (i) selects the most suitable expert at each decoding step and (ii) contributes a complementary logit that refines or corrects the selected expert's next-token distribution via logit addition. Unlike existing token-level collaboration methods that rely solely on fixed expert outputs, we provide a theoretical analysis showing that pure expert-only routing is fundamentally limited: unless strong global coverage assumptions hold, it cannot in general realize the optimal decoding policy. By augmenting expert selection with a trainable complementary generator, FusionRoute expands the effective policy class and enables recovery of optimal value functions under mild conditions. Empirically, across both Llama-3 and Gemma-2 families and diverse benchmarks spanning mathematical reasoning, code generation, and instruction following, FusionRoute outperforms both sequence- and token-level collaboration, model merging, and direct fine-tuning, while remaining competitive with domain experts on their respective tasks.",
    "translation": "标题：基于FusionRoute的令牌级大语言模型协同机制\n\n摘要：大语言模型在不同领域展现出显著优势。然而，要构建在多个领域均表现优异的通用模型，通常需要将模型规模扩展至训练和部署成本极高的程度。另一方面，尽管小型领域专用模型效率更高，但其在训练分布之外的泛化能力有限。为解决这一困境，本文提出FusionRoute——一种鲁棒且高效的令牌级多模型协同框架。该框架通过轻量级路由机制同步实现：（1）在每个解码步骤选择最合适的专家模型；（2）生成互补对数概率，通过对数加法优化或校正所选专家的下一令牌概率分布。与现有仅依赖固定专家输出的令牌级协同方法不同，本文理论分析表明纯专家路由机制存在本质局限：除非满足强全局覆盖假设，否则通常无法实现最优解码策略。FusionRoute通过可训练的互补生成器增强专家选择机制，扩展了有效策略类别，并在温和条件下实现了最优价值函数恢复。实证研究表明，在Llama-3与Gemma-2系列模型上，跨越数学推理、代码生成和指令遵循等多类基准测试，FusionRoute在序列级/令牌级协同、模型融合及直接微调等方法中均表现更优，同时在各自任务上与领域专家模型保持竞争力。",
    "url": "https://huggingface.co/papers/2601.05106",
    "arxiv_url": "https://arxiv.org/abs/2601.05106"
  },
  {
    "title": "RelayLLM: Efficient Reasoning via Collaborative Decoding",
    "summary": "Large Language Models (LLMs) for complex reasoning is often hindered by high computational costs and latency, while resource-efficient Small Language Models (SLMs) typically lack the necessary reasoning capacity. Existing collaborative approaches, such as cascading or routing, operate at a coarse granularity by offloading entire queries to LLMs, resulting in significant computational waste when the SLM is capable of handling the majority of reasoning steps. To address this, we propose RelayLLM, a novel framework for efficient reasoning via token-level collaborative decoding. Unlike routers, RelayLLM empowers the SLM to act as an active controller that dynamically invokes the LLM only for critical tokens via a special command, effectively \"relaying\" the generation process. We introduce a two-stage training framework, including warm-up and Group Relative Policy Optimization (GRPO) to teach the model to balance independence with strategic help-seeking. Empirical results across six benchmarks demonstrate that RelayLLM achieves an average accuracy of 49.52%, effectively bridging the performance gap between the two models. Notably, this is achieved by invoking the LLM for only 1.07% of the total generated tokens, offering a 98.2% cost reduction compared to performance-matched random routers.",
    "translation": "标题：RelayLLM：基于协同解码的高效推理框架\n\n摘要：大型语言模型（LLMs）在复杂推理任务中常受限于高计算成本与延迟问题，而资源高效的小型语言模型（SLMs）通常缺乏必要的推理能力。现有的协同方法（如级联或路由机制）以粗粒度方式将完整查询卸载至LLMs处理，当SLM能够承担多数推理步骤时会导致显著的计算资源浪费。为解决这一问题，我们提出RelayLLM——一种基于词元级协同解码的高效推理新框架。与路由机制不同，RelayLLM使SLM成为主动控制器，仅通过特殊指令动态调用LLM处理关键词元，实现生成过程的“接力式”协作。我们设计了两阶段训练框架，包括预热阶段和组相对策略优化（GRPO）阶段，以指导模型在自主推理与策略性求助之间取得平衡。在六个基准测试上的实验结果表明，RelayLLM平均准确率达到49.52%，有效弥合了两类模型的性能差距。值得注意的是，该框架仅需对总生成词元的1.07%调用LLM，相比性能匹配的随机路由方法实现了98.2%的成本降低。",
    "url": "https://huggingface.co/papers/2601.05167",
    "arxiv_url": "https://arxiv.org/abs/2601.05167"
  },
  {
    "title": "RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation",
    "summary": "The diversity, quantity, and quality of manipulation data are critical for training effective robot policies. However, due to hardware and physical setup constraints, collecting large-scale real-world manipulation data remains difficult to scale across diverse environments. Recent work uses text-prompt conditioned image diffusion models to augment manipulation data by altering the backgrounds and tabletop objects in the visual observations. However, these approaches often overlook the practical need for multi-view and temporally coherent observations required by state-of-the-art policy models. Further, text prompts alone cannot reliably specify the scene setup. To provide the diffusion model with explicit visual guidance, we introduce visual identity prompting, which supplies exemplar images as conditioning inputs to guide the generation of the desired scene setup. To this end, we also build a scalable pipeline to curate a visual identity pool from large robotics datasets. Using our augmented manipulation data to train downstream vision-language-action and visuomotor policy models yields consistent performance gains in both simulation and real-robot settings.",
    "translation": "标题：RoboVIP：基于视觉身份提示增强的多视角视频生成提升机器人操作性能\n\n摘要：操作数据的多样性、数量与质量对于训练有效的机器人策略至关重要。然而，受硬件与物理环境配置的限制，在多样化场景中收集大规模真实世界操作数据仍难以实现规模化。近期研究通过文本提示条件化的图像扩散模型，改变视觉观测中的背景与桌面物体以增强操作数据。然而，这些方法往往忽略了先进策略模型所需的多视角与时序一致观测的实际需求。此外，仅依靠文本提示难以可靠地指定场景配置。为向扩散模型提供明确的视觉引导，本文提出视觉身份提示方法，通过提供示例图像作为条件输入来引导生成目标场景配置。为此，我们构建了可扩展的流程框架，从大规模机器人数据集中筛选构建视觉身份池。使用本方法增强的操作数据训练下游视觉-语言-动作策略模型与视觉运动策略模型，在仿真与真实机器人场景中均实现了持续的性能提升。",
    "url": "https://huggingface.co/papers/2601.05241",
    "arxiv_url": "https://arxiv.org/abs/2601.05241"
  },
  {
    "title": "AT^2PO: Agentic Turn-based Policy Optimization via Tree Search",
    "summary": "LLM agents have emerged as powerful systems for tackling multi-turn tasks by interleaving internal reasoning and external tool interactions. Agentic Reinforcement Learning has recently drawn significant research attention as a critical post-training paradigm to further refine these capabilities. In this paper, we present AT^2PO (Agentic Turn-based Policy Optimization via Tree Search), a unified framework for multi-turn agentic RL that addresses three core challenges: limited exploration diversity, sparse credit assignment, and misaligned policy optimization. AT^2PO introduces a turn-level tree structure that jointly enables Entropy-Guided Tree Expansion for strategic exploration and Turn-wise Credit Assignment for fine-grained reward propagation from sparse outcomes. Complementing this, we propose Agentic Turn-based Policy Optimization, a turn-level learning objective that aligns policy updates with the natural decision granularity of agentic interactions. ATPO is orthogonal to tree search and can be readily integrated into any multi-turn RL pipeline. Experiments across seven benchmarks demonstrate consistent improvements over the state-of-the-art baseline by up to 1.84 percentage points in average, with ablation studies validating the effectiveness of each component. Our code is available at https://github.com/zzfoutofspace/ATPO.",
    "translation": "标题：AT^2PO：基于树搜索的智能体回合制策略优化\n\n摘要：大型语言模型智能体已发展成为通过交替进行内部推理与外部工具交互来处理多回合任务的强大系统。智能体强化学习作为一种关键的训练后优化范式，近年来受到广泛研究关注，旨在进一步提升此类系统的能力。本文提出AT^2PO（基于树搜索的智能体回合制策略优化），这是一个面向多回合智能体强化学习的统一框架，解决了三个核心挑战：探索多样性有限、稀疏信用分配以及策略优化失准。AT^2PO引入了一种回合级树结构，该结构同时支持两种机制：通过熵引导树扩展实现策略性探索，以及通过回合级信用分配实现稀疏结果下的细粒度奖励传播。在此基础上，我们提出智能体回合制策略优化方法——一种回合级学习目标，使策略更新与智能体交互的自然决策粒度保持一致。该方法与树搜索正交，可轻松集成到任何多回合强化学习流程中。在七个基准测试上的实验表明，该框架相较最先进基线模型平均提升达1.84个百分点，消融研究验证了各模块的有效性。代码已开源：https://github.com/zzfoutofspace/ATPO。",
    "url": "https://huggingface.co/papers/2601.04767",
    "arxiv_url": "https://arxiv.org/abs/2601.04767"
  },
  {
    "title": "Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models",
    "summary": "Vision-language models (VLMs) achieve remarkable performance but remain vulnerable to adversarial attacks. Entropy, a measure of model uncertainty, is strongly correlated with the reliability of VLM. Prior entropy-based attacks maximize uncertainty at all decoding steps, implicitly assuming that every token contributes equally to generation instability. We show instead that a small fraction (about 20%) of high-entropy tokens, i.e., critical decision points in autoregressive generation, disproportionately governs output trajectories. By concentrating adversarial perturbations on these positions, we achieve semantic degradation comparable to global methods while using substantially smaller budgets. More importantly, across multiple representative VLMs, such selective attacks convert 35-49% of benign outputs into harmful ones, exposing a more critical safety risk. Remarkably, these vulnerable high-entropy forks recur across architecturally diverse VLMs, enabling feasible transferability (17-26% harmful rates on unseen targets). Motivated by these findings, we propose Entropy-bank Guided Adversarial attacks (EGA), which achieves competitive attack success rates (93-95%) alongside high harmful conversion, thereby revealing new weaknesses in current VLM safety mechanisms.",
    "translation": "标题：少数关键标记决定成败：基于熵的视觉语言模型攻击方法\n\n摘要：视觉语言模型（VLMs）虽展现出卓越性能，但仍易受对抗性攻击影响。熵作为模型不确定性的度量指标，与VLM的可靠性存在显著关联。现有基于熵的攻击方法默认所有解码步骤中的标记对生成不稳定性具有同等贡献，因而在每一步均最大化模型不确定性。本研究发现，实际上仅需针对自回归生成过程中约20%的高熵标记（即关键决策点）进行攻击，即可对输出轨迹产生不成比例的巨大影响。通过将对抗扰动集中于这些关键位置，本方法在显著降低攻击成本的同时，实现了与全局攻击相当的语义破坏效果。更重要的是，在多个代表性VLM上的实验表明，此类选择性攻击可将35-49%的良性输出转化为有害内容，暴露出更为严峻的安全风险。值得注意的是，这些脆弱的高熵决策分支在不同架构的VLM中反复出现，使得跨模型攻击具备可行性（对未见目标模型达到17-26%的有害转化率）。基于这些发现，我们提出熵库引导对抗攻击（EGA）方法，在实现93-95%攻击成功率的同时保持高有害转化率，从而揭示了当前VLM安全机制中尚未被充分认知的脆弱性。",
    "url": "https://huggingface.co/papers/2512.21815",
    "arxiv_url": "https://arxiv.org/abs/2512.21815"
  },
  {
    "title": "VideoAuto-R1: Video Auto Reasoning via Thinking Once, Answering Twice",
    "summary": "Chain-of-thought (CoT) reasoning has emerged as a powerful tool for multimodal large language models on video understanding tasks. However, its necessity and advantages over direct answering remain underexplored. In this paper, we first demonstrate that for RL-trained video models, direct answering often matches or even surpasses CoT performance, despite CoT producing step-by-step analyses at a higher computational cost. Motivated by this, we propose VideoAuto-R1, a video understanding framework that adopts a reason-when-necessary strategy. During training, our approach follows a Thinking Once, Answering Twice paradigm: the model first generates an initial answer, then performs reasoning, and finally outputs a reviewed answer. Both answers are supervised via verifiable rewards. During inference, the model uses the confidence score of the initial answer to determine whether to proceed with reasoning. Across video QA and grounding benchmarks, VideoAuto-R1 achieves state-of-the-art accuracy with significantly improved efficiency, reducing the average response length by ~3.3x, e.g., from 149 to just 44 tokens. Moreover, we observe a low rate of thinking-mode activation on perception-oriented tasks, but a higher rate on reasoning-intensive tasks. This suggests that explicit language-based reasoning is generally beneficial but not always necessary.",
    "translation": "标题：VideoAuto-R1：基于“思考一次，回答两次”的视频自动推理框架\n\n摘要：思维链推理已成为多模态大语言模型在视频理解任务中的强大工具，但其相对于直接回答的必要性与优势尚未得到充分探索。本文首先证明，对于基于强化学习训练的视频模型，直接回答的性能往往与思维链相当甚至更优，尽管思维链能以分步分析的方式生成结果，但计算成本更高。基于此，我们提出VideoAuto-R1，一种采用“按需推理”策略的视频理解框架。在训练阶段，该方法遵循“思考一次，回答两次”范式：模型首先生成初始答案，随后进行推理，最终输出经过复核的答案。两种答案均通过可验证的奖励机制进行监督。在推理阶段，模型依据初始答案的置信度决定是否启动推理流程。在视频问答与定位基准测试中，VideoAuto-R1在显著提升效率的同时实现了最优准确率，平均响应长度缩短约3.3倍（例如从149个标记减少至44个标记）。此外，我们观察到在感知导向任务中思维模式的激活率较低，而在推理密集型任务中激活率较高。这表明基于语言的显式推理通常有益，但并非总是必要。",
    "url": "https://huggingface.co/papers/2601.05175",
    "arxiv_url": "https://arxiv.org/abs/2601.05175"
  },
  {
    "title": "VerseCrafter: Dynamic Realistic Video World Model with 4D Geometric Control",
    "summary": "Video world models aim to simulate dynamic, real-world environments, yet existing methods struggle to provide unified and precise control over camera and multi-object motion, as videos inherently operate dynamics in the projected 2D image plane. To bridge this gap, we introduce VerseCrafter, a 4D-aware video world model that enables explicit and coherent control over both camera and object dynamics within a unified 4D geometric world state. Our approach is centered on a novel 4D Geometric Control representation, which encodes the world state through a static background point cloud and per-object 3D Gaussian trajectories. This representation captures not only an object's path but also its probabilistic 3D occupancy over time, offering a flexible, category-agnostic alternative to rigid bounding boxes or parametric models. These 4D controls are rendered into conditioning signals for a pretrained video diffusion model, enabling the generation of high-fidelity, view-consistent videos that precisely adhere to the specified dynamics. Unfortunately, another major challenge lies in the scarcity of large-scale training data with explicit 4D annotations. We address this by developing an automatic data engine that extracts the required 4D controls from in-the-wild videos, allowing us to train our model on a massive and diverse dataset.",
    "translation": "标题：VerseCrafter：具备四维几何控制的动态逼真视频世界模型\n\n摘要：视频世界模型旨在模拟动态的真实世界环境，然而现有方法难以对相机与多物体运动提供统一且精确的控制，因为视频本质上是在投影的二维图像平面上运作动态。为弥合这一差距，我们提出了VerseCrafter——一种具备四维感知能力的视频世界模型，能够在统一的四维几何世界状态中对相机和物体动态进行显式且连贯的控制。我们的方法核心在于一种新颖的四维几何控制表示，该表示通过静态背景点云与逐物体三维高斯轨迹来编码世界状态。此表示不仅捕捉物体的运动路径，还记录其随时间变化的概率性三维占据情况，为刚性边界框或参数化模型提供了一种灵活且与类别无关的替代方案。这些四维控制被渲染为预训练视频扩散模型的条件信号，从而能够生成高保真、视角一致且严格遵循指定动态的视频。然而，另一主要挑战在于缺乏具有显式四维标注的大规模训练数据。为此，我们开发了一种自动数据引擎，能够从真实场景视频中提取所需的四维控制，从而使模型得以在海量多样化数据集上进行训练。",
    "url": "https://huggingface.co/papers/2601.05138",
    "arxiv_url": "https://arxiv.org/abs/2601.05138"
  },
  {
    "title": "The Illusion of Specialization: Unveiling the Domain-Invariant \"Standing Committee\" in Mixture-of-Experts Models",
    "summary": "Mixture of Experts models are widely assumed to achieve domain specialization through sparse routing. In this work, we question this assumption by introducing COMMITTEEAUDIT, a post hoc framework that analyzes routing behavior at the level of expert groups rather than individual experts. Across three representative models and the MMLU benchmark, we uncover a domain-invariant Standing Committee. This is a compact coalition of routed experts that consistently captures the majority of routing mass across domains, layers, and routing budgets, even when architectures already include shared experts. Qualitative analysis further shows that Standing Committees anchor reasoning structure and syntax, while peripheral experts handle domain-specific knowledge. These findings reveal a strong structural bias toward centralized computation, suggesting that specialization in Mixture of Experts models is far less pervasive than commonly believed. This inherent bias also indicates that current training objectives, such as load-balancing losses that enforce uniform expert utilization, may be working against the model's natural optimization path, thereby limiting training efficiency and performance.",
    "translation": "标题：专业化的幻象：揭示混合专家模型中领域不变的“常务委员会”\n\n摘要：混合专家模型被广泛认为通过稀疏路由机制实现领域专业化。本研究通过引入COMMITTEEAUDIT后验分析框架，对上述假设提出质疑。该框架在专家组层面（而非单个专家层面）分析路由行为。通过对三个代表性模型及MMLU基准测试的实证分析，我们发现了一个领域不变的“常务委员会”——这是一个由被路由专家组成的紧凑联盟，在不同领域、网络层和路由预算条件下始终占据路由质量的主导地位，即使在已包含共享专家的架构中亦然。定性分析进一步表明，常务委员会负责锚定推理结构与句法框架，而边缘专家则处理领域特定知识。这些发现揭示了模型存在强烈的中心化计算结构偏好，表明混合专家模型的专业化程度远低于普遍认知。这种固有偏好同时暗示，当前训练目标（如强制均衡专家使用率的负载平衡损失函数）可能违背模型的自然优化路径，从而限制训练效率与性能表现。",
    "url": "https://huggingface.co/papers/2601.03425",
    "arxiv_url": "https://arxiv.org/abs/2601.03425"
  },
  {
    "title": "Agent-as-a-Judge",
    "summary": "LLM-as-a-Judge has revolutionized AI evaluation by leveraging large language models for scalable assessments. However, as evaluands become increasingly complex, specialized, and multi-step, the reliability of LLM-as-a-Judge has become constrained by inherent biases, shallow single-pass reasoning, and the inability to verify assessments against real-world observations. This has catalyzed the transition to Agent-as-a-Judge, where agentic judges employ planning, tool-augmented verification, multi-agent collaboration, and persistent memory to enable more robust, verifiable, and nuanced evaluations. Despite the rapid proliferation of agentic evaluation systems, the field lacks a unified framework to navigate this shifting landscape. To bridge this gap, we present the first comprehensive survey tracing this evolution. Specifically, we identify key dimensions that characterize this paradigm shift and establish a developmental taxonomy. We organize core methodologies and survey applications across general and professional domains. Furthermore, we analyze frontier challenges and identify promising research directions, ultimately providing a clear roadmap for the next generation of agentic evaluation.",
    "translation": "标题：智能体即评委\n\n摘要：“大语言模型即评委”通过利用大语言模型进行规模化评估，彻底改变了人工智能评价范式。然而，随着评估对象日益复杂化、专业化且涉及多步骤任务，该模式的可靠性逐渐受限于其固有偏见、浅层的单次推理能力，以及无法通过现实观察验证评估结果的缺陷。这推动了向“智能体即评委”范式的转变——智能体评委通过任务规划、工具增强验证、多智能体协作与持久记忆机制，实现更鲁棒、可验证且精细化的评估。尽管智能体评估系统正快速涌现，该领域仍缺乏统一框架以应对这一变革趋势。为填补这一空白，本文首次系统梳理该演进脉络，具体通过界定范式转型的关键特征维度，建立发展分类体系；系统归纳核心方法学，并综述其在通用领域与专业领域的应用实践；进而剖析前沿挑战，指明具有潜力的研究方向，最终为下一代智能体评估系统的发展提供清晰路线图。",
    "url": "https://huggingface.co/papers/2601.05111",
    "arxiv_url": "https://arxiv.org/abs/2601.05111"
  },
  {
    "title": "Plenoptic Video Generation",
    "summary": "Camera-controlled generative video re-rendering methods, such as ReCamMaster, have achieved remarkable progress. However, despite their success in single-view setting, these works often struggle to maintain consistency across multi-view scenarios. Ensuring spatio-temporal coherence in hallucinated regions remains challenging due to the inherent stochasticity of generative models. To address it, we introduce PlenopticDreamer, a framework that synchronizes generative hallucinations to maintain spatio-temporal memory. The core idea is to train a multi-in-single-out video-conditioned model in an autoregressive manner, aided by a camera-guided video retrieval strategy that adaptively selects salient videos from previous generations as conditional inputs. In addition, Our training incorporates progressive context-scaling to improve convergence, self-conditioning to enhance robustness against long-range visual degradation caused by error accumulation, and a long-video conditioning mechanism to support extended video generation. Extensive experiments on the Basic and Agibot benchmarks demonstrate that PlenopticDreamer achieves state-of-the-art video re-rendering, delivering superior view synchronization, high-fidelity visuals, accurate camera control, and diverse view transformations (e.g., third-person to third-person, and head-view to gripper-view in robotic manipulation). Project page: https://research.nvidia.com/labs/dir/plenopticdreamer/",
    "translation": "标题：全光视频生成\n\n摘要：以ReCamMaster为代表的相机控制式生成视频重渲染方法已取得显著进展。然而，尽管在单视角场景中表现优异，这些方法在多视角场景下往往难以保持一致性。由于生成模型固有的随机性，确保生成区域的空间-时间连贯性仍具挑战。为此，我们提出PlenopticDreamer框架，通过同步生成幻觉来维持时空记忆。其核心思想是以自回归方式训练多输入单输出的视频条件生成模型，并辅以相机引导的视频检索策略——该策略自适应地从历史生成片段中选取显著视频作为条件输入。此外，我们的训练方案融合了渐进式上下文扩展以提升收敛效率，引入自条件机制以增强对误差累积导致的长程视觉退化问题的鲁棒性，并采用长视频条件机制以支持扩展视频生成。在Basic与Agibot基准测试上的大量实验表明，PlenopticDreamer实现了最先进的视频重渲染效果，在视角同步性、视觉保真度、相机控制精确度及多样化视角转换（如第三人称到第三人称、机器人操作中的头部视角到抓取器视角）方面均表现卓越。项目页面：https://research.nvidia.com/labs/dir/plenopticdreamer/",
    "url": "https://huggingface.co/papers/2601.05239",
    "arxiv_url": "https://arxiv.org/abs/2601.05239"
  },
  {
    "title": "CoV: Chain-of-View Prompting for Spatial Reasoning",
    "summary": "Embodied question answering (EQA) in 3D environments often requires collecting context that is distributed across multiple viewpoints and partially occluded. However, most recent vision--language models (VLMs) are constrained to a fixed and finite set of input views, which limits their ability to acquire question-relevant context at inference time and hinders complex spatial reasoning. We propose Chain-of-View (CoV) prompting, a training-free, test-time reasoning framework that transforms a VLM into an active viewpoint reasoner through a coarse-to-fine exploration process. CoV first employs a View Selection agent to filter redundant frames and identify question-aligned anchor views. It then performs fine-grained view adjustment by interleaving iterative reasoning with discrete camera actions, obtaining new observations from the underlying 3D scene representation until sufficient context is gathered or a step budget is reached.\n  We evaluate CoV on OpenEQA across four mainstream VLMs and obtain an average +11.56\\% improvement in LLM-Match, with a maximum gain of +13.62\\% on Qwen3-VL-Flash. CoV further exhibits test-time scaling: increasing the minimum action budget yields an additional +2.51\\% average improvement, peaking at +3.73\\% on Gemini-2.5-Flash. On ScanQA and SQA3D, CoV delivers strong performance (e.g., 116 CIDEr / 31.9 EM@1 on ScanQA and 51.1 EM@1 on SQA3D). Overall, these results suggest that question-aligned view selection coupled with open-view search is an effective, model-agnostic strategy for improving spatial reasoning in 3D EQA without additional training.",
    "translation": "标题：CoV：面向空间推理的视点链提示方法\n\n摘要：三维环境中的具身问答任务通常需要整合分布在多个视角且部分被遮挡的上下文信息。然而，当前大多数视觉-语言模型受限于固定且有限的输入视角集合，这限制了其在推理时获取问题相关上下文的能力，并阻碍了复杂空间推理的进行。本文提出视点链提示方法，这是一种无需训练、在测试时进行推理的框架，通过从粗到细的探索过程将视觉-语言模型转化为主动的视点推理器。CoV首先利用视点选择代理过滤冗余帧并识别与问题对齐的锚定视点，随后通过离散相机动作与迭代推理交替进行细粒度视点调整，从底层三维场景表示中持续获取新的观测信息，直至收集到足够上下文或达到步骤预算上限。\n\n我们在OpenEQA基准上对四种主流视觉-语言模型进行CoV评估，在LLM-Match指标上平均提升11.56%，其中Qwen3-VL-Flash模型最高提升达13.62%。CoV进一步展现出测试时扩展性：增加最小动作预算可带来额外2.51%的平均提升，在Gemini-2.5-Flash模型上峰值提升达3.73%。在ScanQA和SQA3D数据集上，CoV同样表现出强劲性能（例如ScanQA达到116 CIDEr/31.9 EM@1，SQA3D达到51.1 EM@1）。总体而言，这些结果表明：结合问题对齐的视点选择与开放视点搜索的策略，能够在不增加训练成本的前提下，成为提升三维具身问答空间推理能力的有效且模型无关的方法。",
    "url": "https://huggingface.co/papers/2601.05172",
    "arxiv_url": "https://arxiv.org/abs/2601.05172"
  },
  {
    "title": "DocDancer: Towards Agentic Document-Grounded Information Seeking",
    "summary": "Document Question Answering (DocQA) focuses on answering questions grounded in given documents, yet existing DocQA agents lack effective tool utilization and largely rely on closed-source models. In this work, we introduce DocDancer, an end-to-end trained open-source Doc agent. We formulate DocQA as an information-seeking problem and propose a tool-driven agent framework that explicitly models document exploration and comprehension. To enable end-to-end training of such agents, we introduce an Exploration-then-Synthesis data synthesis pipeline that addresses the scarcity of high-quality training data for DocQA. Training on the synthesized data, the trained models on two long-context document understanding benchmarks, MMLongBench-Doc and DocBench, show their effectiveness. Further analysis provides valuable insights for the agentic tool design and synthetic data.",
    "translation": "标题：DocDancer：迈向基于文档的自主信息检索智能体\n\n摘要：文档问答任务专注于依据给定文档回答问题，然而现有的文档问答智能体缺乏有效的工具利用能力，且主要依赖闭源模型。本研究提出了DocDancer，一种端到端训练的开源文档智能体。我们将文档问答任务构建为信息检索问题，并提出一种工具驱动的智能体框架，该框架显式建模文档探索与理解过程。为实现此类智能体的端到端训练，我们设计了“先探索后合成”的数据合成流程，以解决文档问答领域高质量训练数据稀缺的问题。通过在合成数据上进行训练，模型在两个长上下文文档理解基准测试（MMLongBench-Doc与DocBench）上展现出显著有效性。进一步的分析为智能体工具设计与合成数据生成提供了有价值的见解。",
    "url": "https://huggingface.co/papers/2601.05163",
    "arxiv_url": "https://arxiv.org/abs/2601.05163"
  },
  {
    "title": "Re-Align: Structured Reasoning-guided Alignment for In-Context Image Generation and Editing",
    "summary": "In-context image generation and editing (ICGE) enables users to specify visual concepts through interleaved image-text prompts, demanding precise understanding and faithful execution of user intent. Although recent unified multimodal models exhibit promising understanding capabilities, these strengths often fail to transfer effectively to image generation. We introduce Re-Align, a unified framework that bridges the gap between understanding and generation through structured reasoning-guided alignment. At its core lies the In-Context Chain-of-Thought (IC-CoT), a structured reasoning paradigm that decouples semantic guidance and reference association, providing clear textual target and mitigating confusion among reference images. Furthermore, Re-Align introduces an effective RL training scheme that leverages a surrogate reward to measure the alignment between structured reasoning text and the generated image, thereby improving the model's overall performance on ICGE tasks. Extensive experiments verify that Re-Align outperforms competitive methods of comparable model scale and resources on both in-context image generation and editing tasks.",
    "translation": "标题：Re-Align：基于结构化推理引导对齐的上下文图像生成与编辑\n\n摘要：上下文图像生成与编辑（ICGE）允许用户通过交错的图像-文本提示来指定视觉概念，这要求模型能够精确理解并忠实执行用户意图。尽管近期出现的统一多模态模型展现出令人期待的理解能力，但这些优势往往未能有效迁移至图像生成任务中。本文提出Re-Align，一个通过结构化推理引导对齐来弥合理解与生成之间差距的统一框架。其核心是上下文思维链（IC-CoT），这是一种结构化推理范式，能够解耦语义引导与参考关联，提供清晰的文本目标并减轻参考图像间的混淆。此外，Re-Align引入了一种有效的强化学习训练方案，利用代理奖励来衡量结构化推理文本与生成图像之间的对齐程度，从而提升模型在ICGE任务上的整体性能。大量实验验证表明，在模型规模和资源相当的情况下，Re-Align在上下文图像生成与编辑任务上均优于现有竞争方法。",
    "url": "https://huggingface.co/papers/2601.05124",
    "arxiv_url": "https://arxiv.org/abs/2601.05124"
  },
  {
    "title": "DiffCoT: Diffusion-styled Chain-of-Thought Reasoning in LLMs",
    "summary": "Chain-of-Thought (CoT) reasoning improves multi-step mathematical problem solving in large language models but remains vulnerable to exposure bias and error accumulation, as early mistakes propagate irreversibly through autoregressive decoding. In this work, we propose DiffCoT, a diffusion-styled CoT framework that reformulates CoT reasoning as an iterative denoising process. DiffCoT integrates diffusion principles at the reasoning-step level via a sliding-window mechanism, enabling unified generation and retrospective correction of intermediate steps while preserving token-level autoregression. To maintain causal consistency, we further introduce a causal diffusion noise schedule that respects the temporal structure of reasoning chains. Extensive experiments on three multi-step CoT reasoning benchmarks across diverse model backbones demonstrate that DiffCoT consistently outperforms existing CoT preference optimization methods, yielding improved robustness and error-correction capability in CoT reasoning.",
    "translation": "标题：DiffCoT：大语言模型中扩散式思维链推理\n\n摘要：思维链推理提升了大型语言模型在多步数学问题求解中的表现，但仍易受曝光偏差和错误累积的影响——早期错误会通过自回归解码过程不可逆地传播。本研究提出DiffCoT，一种扩散式思维链推理框架，将思维链推理重新构建为迭代去噪过程。DiffCoT通过滑动窗口机制在推理步骤层面融合扩散原理，在保持词元级自回归的同时，实现了中间步骤的统一生成与回溯修正。为保持因果一致性，我们进一步提出一种遵循推理链时序结构的因果扩散噪声调度方法。在多种模型架构上对三个多步思维链推理基准的广泛实验表明，DiffCoT始终优于现有的思维链偏好优化方法，显著提升了思维链推理的鲁棒性与错误修正能力。",
    "url": "https://huggingface.co/papers/2601.03559",
    "arxiv_url": "https://arxiv.org/abs/2601.03559"
  },
  {
    "title": "ProFuse: Efficient Cross-View Context Fusion for Open-Vocabulary 3D Gaussian Splatting",
    "summary": "We present ProFuse, an efficient context-aware framework for open-vocabulary 3D scene understanding with 3D Gaussian Splatting (3DGS). The pipeline enhances cross-view consistency and intra-mask cohesion within a direct registration setup, adding minimal overhead and requiring no render-supervised fine-tuning. Instead of relying on a pretrained 3DGS scene, we introduce a dense correspondence-guided pre-registration phase that initializes Gaussians with accurate geometry while jointly constructing 3D Context Proposals via cross-view clustering. Each proposal carries a global feature obtained through weighted aggregation of member embeddings, and this feature is fused onto Gaussians during direct registration to maintain per-primitive language coherence across views. With associations established in advance, semantic fusion requires no additional optimization beyond standard reconstruction, and the model retains geometric refinement without densification. ProFuse achieves strong open-vocabulary 3DGS understanding while completing semantic attachment in about five minutes per scene, which is two times faster than SOTA.",
    "translation": "标题：ProFuse：一种面向开放词汇3D高斯溅射的高效跨视角上下文融合方法\n\n摘要：本文提出ProFuse，这是一个面向开放词汇3D场景理解的高效上下文感知框架，基于3D高斯溅射（3DGS）技术。该流程在直接配准框架内增强了跨视角一致性与掩码内部凝聚力，仅增加极少的计算开销，且无需基于渲染的微调。与依赖预训练3DGS场景的传统方法不同，我们引入了稠密对应引导的预配准阶段，通过跨视角聚类在初始化具有精确几何结构的高斯模型的同时，联合构建3D上下文提议。每个提议通过成员嵌入的加权聚合获得全局特征，该特征在直接配准过程中融合到高斯模型上，以保持跨视角中每个图元的语言连贯性。由于关联关系已预先建立，语义融合除标准重建外无需额外优化，且模型在保持几何细化的同时避免了稠密化处理。ProFuse在实现强大开放词汇3DGS理解能力的同时，能以每场景约五分钟的速度完成语义附着，其效率较当前最优技术提升两倍。",
    "url": "https://huggingface.co/papers/2601.04754",
    "arxiv_url": "https://arxiv.org/abs/2601.04754"
  },
  {
    "title": "Guardians of the Hair: Rescuing Soft Boundaries in Depth, Stereo, and Novel Views",
    "summary": "Soft boundaries, like thin hairs, are commonly observed in natural and computer-generated imagery, but they remain challenging for 3D vision due to the ambiguous mixing of foreground and background cues. This paper introduces Guardians of the Hair (HairGuard), a framework designed to recover fine-grained soft boundary details in 3D vision tasks. Specifically, we first propose a novel data curation pipeline that leverages image matting datasets for training and design a depth fixer network to automatically identify soft boundary regions. With a gated residual module, the depth fixer refines depth precisely around soft boundaries while maintaining global depth quality, allowing plug-and-play integration with state-of-the-art depth models. For view synthesis, we perform depth-based forward warping to retain high-fidelity textures, followed by a generative scene painter that fills disoccluded regions and eliminates redundant background artifacts within soft boundaries. Finally, a color fuser adaptively combines warped and inpainted results to produce novel views with consistent geometry and fine-grained details. Extensive experiments demonstrate that HairGuard achieves state-of-the-art performance across monocular depth estimation, stereo image/video conversion, and novel view synthesis, with significant improvements in soft boundary regions.",
    "translation": "标题：发丝守护者：在深度、立体与新视角中修复软边界\n\n摘要：软边界（如细发丝）在自然图像与计算机生成图像中普遍存在，但由于前景与背景线索的模糊混合，其在三维视觉任务中仍具挑战性。本文提出“发丝守护者”（HairGuard）框架，旨在恢复三维视觉任务中细粒度的软边界细节。具体而言，我们首先设计了一种新颖的数据处理流程，利用图像抠图数据集进行训练，并构建深度修正网络以自动识别软边界区域。该网络通过门控残差模块，在保持全局深度质量的同时精准优化软边界区域的深度，实现与前沿深度模型的即插即用式集成。在视角合成任务中，我们采用基于深度的前向扭曲以保留高保真纹理，随后通过生成式场景绘制器填充因遮挡暴露的区域，并消除软边界内的冗余背景伪影。最后，色彩融合模块自适应地结合扭曲与修复结果，生成具有几何一致性与细粒度细节的新视角图像。大量实验表明，HairGuard在单目深度估计、立体图像/视频转换及新视角合成任务中均达到领先性能，尤其在软边界区域取得显著改进。",
    "url": "https://huggingface.co/papers/2601.03362",
    "arxiv_url": "https://arxiv.org/abs/2601.03362"
  },
  {
    "title": "One Sample to Rule Them All: Extreme Data Efficiency in RL Scaling",
    "summary": "The reasoning ability of large language models (LLMs) can be unleashed with reinforcement learning (RL) (OpenAI, 2024; DeepSeek-AI et al., 2025a; Zeng et al., 2025). The success of existing RL attempts in LLMs usually relies on high-quality samples of thousands or beyond. In this paper, we challenge fundamental assumptions about data requirements in RL for LLMs by demonstrating the remarkable effectiveness of one-shot learning. Specifically, we introduce polymath learning, a framework for designing one training sample that elicits multidisciplinary impact. We present three key findings: (1) A single, strategically selected math reasoning sample can produce significant performance improvements across multiple domains, including physics, chemistry, and biology with RL; (2) The math skills salient to reasoning suggest the characteristics of the optimal polymath sample; and (3) An engineered synthetic sample that integrates multidiscipline elements outperforms training with individual samples that naturally occur. Our approach achieves superior performance to training with larger datasets across various reasoning benchmarks, demonstrating that sample quality and design, rather than quantity, may be the key to unlock enhanced reasoning capabilities in language models. Our results suggest a shift, dubbed as sample engineering, toward precision engineering of training samples rather than simply increasing data volume.",
    "translation": "标题：一例统御全局：强化学习规模化中的极致数据效率\n\n摘要：大规模语言模型（LLM）的推理能力可通过强化学习（RL）得以释放（OpenAI，2024；深度求索等，2025a；Zeng等，2025）。现有针对LLM的强化学习尝试通常依赖于数千乃至更多的高质量训练样本。本文通过展示单样本学习的显著有效性，对LLM强化学习中数据需求的基本假设提出了挑战。具体而言，我们提出了“博学学习”框架，该框架旨在设计能够引发多学科影响的单一训练样本。我们展示了三项关键发现：（1）单个经过策略性选择的数学推理样本，结合强化学习，可在物理、化学、生物等多个领域带来显著的性能提升；（2）对推理至关重要的数学技能揭示了最优博学样本应具备的特征；（3）一个融合多学科要素的工程化合成样本，其训练效果优于使用自然产生的单个样本。我们的方法在多种推理基准测试中均取得了优于使用更大规模数据集进行训练的性能，这表明样本质量与设计（而非数量）可能是解锁语言模型增强推理能力的关键。我们的研究结果预示着一个被称为“样本工程”的范式转变，即从单纯增加数据量转向对训练样本进行精准设计。",
    "url": "https://huggingface.co/papers/2601.03111",
    "arxiv_url": "https://arxiv.org/abs/2601.03111"
  },
  {
    "title": "Memorization in 3D Shape Generation: An Empirical Study",
    "summary": "Generative models are increasingly used in 3D vision to synthesize novel shapes, yet it remains unclear whether their generation relies on memorizing training shapes. Understanding their memorization could help prevent training data leakage and improve the diversity of generated results. In this paper, we design an evaluation framework to quantify memorization in 3D generative models and study the influence of different data and modeling designs on memorization. We first apply our framework to quantify memorization in existing methods. Next, through controlled experiments with a latent vector-set (Vecset) diffusion model, we find that, on the data side, memorization depends on data modality, and increases with data diversity and finer-grained conditioning; on the modeling side, it peaks at a moderate guidance scale and can be mitigated by longer Vecsets and simple rotation augmentation. Together, our framework and analysis provide an empirical understanding of memorization in 3D generative models and suggest simple yet effective strategies to reduce it without degrading generation quality. Our code is available at https://github.com/zlab-princeton/3d_mem.",
    "translation": "标题：三维形状生成中的记忆化现象：一项实证研究\n\n摘要：生成模型在三维视觉领域被日益广泛地应用于合成新形状，但其生成过程是否依赖于对训练数据的记忆仍不明确。理解这种记忆化现象有助于防止训练数据泄露并提升生成结果的多样性。本文设计了一个评估框架来量化三维生成模型中的记忆化程度，并探究不同数据与模型设计对记忆化的影响。我们首先应用该框架量化现有方法的记忆化水平。随后，通过对潜在向量集扩散模型进行对照实验发现：在数据层面，记忆化程度取决于数据模态，并随数据多样性和更细粒度条件控制的增强而增加；在模型层面，记忆化在中等引导强度时达到峰值，但可通过延长向量集长度和简单的旋转数据增强来缓解。本研究提出的框架与分析为理解三维生成模型的记忆化机制提供了实证依据，并提出了一系列在保持生成质量的同时有效降低记忆化的简洁策略。代码已开源：https://github.com/zlab-princeton/3d_mem。",
    "url": "https://huggingface.co/papers/2512.23628",
    "arxiv_url": "https://arxiv.org/abs/2512.23628"
  },
  {
    "title": "Multi-Scale Local Speculative Decoding for Image Generation",
    "summary": "Autoregressive (AR) models have achieved remarkable success in image synthesis, yet their sequential nature imposes significant latency constraints. Speculative Decoding offers a promising avenue for acceleration, but existing approaches are limited by token-level ambiguity and lack of spatial awareness. In this work, we introduce Multi-Scale Local Speculative Decoding (MuLo-SD), a novel framework that combines multi-resolution drafting with spatially informed verification to accelerate AR image generation. Our method leverages a low-resolution drafter paired with learned up-samplers to propose candidate image tokens, which are then verified in parallel by a high-resolution target model. Crucially, we incorporate a local rejection and resampling mechanism, enabling efficient correction of draft errors by focusing on spatial neighborhoods rather than raster-scan resampling after the first rejection. We demonstrate that MuLo-SD achieves substantial speedups - up to 1.7times - outperforming strong speculative decoding baselines such as EAGLE-2 and LANTERN in terms of acceleration, while maintaining comparable semantic alignment and perceptual quality. These results are validated using GenEval, DPG-Bench, and FID/HPSv2 on the MS-COCO 5k validation split. Extensive ablations highlight the impact of up-sampling design, probability pooling, and local rejection and resampling with neighborhood expansion. Our approach sets a new state-of-the-art in speculative decoding for image synthesis, bridging the gap between efficiency and fidelity.",
    "translation": "标题：多尺度局部推测解码在图像生成中的应用\n\n摘要：自回归模型在图像合成领域取得了显著成就，但其序列化特性导致严重的延迟问题。推测解码技术为加速生成提供了可行路径，然而现有方法受限于令牌级歧义且缺乏空间感知能力。本研究提出多尺度局部推测解码框架，通过结合多分辨率草案生成与空间感知验证机制，实现自回归图像生成的高效加速。该方法采用低分辨率草案生成器与可学习上采样器协同工作，生成候选图像令牌序列，随后由高分辨率目标模型进行并行验证。创新性地引入局部拒绝与重采样机制，通过聚焦空间邻域而非首次拒绝后的光栅扫描式重采样，实现对草案错误的高效修正。实验表明，该框架在MS-COCO 5k验证集上取得最高1.7倍的加速效果，在加速性能上超越EAGLE-2和LANTERN等强基线模型，同时保持相当的语义对齐度与感知质量。通过GenEval、DPG-Bench及FID/HPSv2评估体系的验证，系统消融实验进一步揭示了上采样设计、概率池化以及邻域扩展的局部拒绝重采样机制的关键作用。本方法在图像合成推测解码领域建立了新的性能标杆，有效弥合了生成效率与保真度之间的鸿沟。",
    "url": "https://huggingface.co/papers/2601.05149",
    "arxiv_url": "https://arxiv.org/abs/2601.05149"
  },
  {
    "title": "PyramidalWan: On Making Pretrained Video Model Pyramidal for Efficient Inference",
    "summary": "Recently proposed pyramidal models decompose the conventional forward and backward diffusion processes into multiple stages operating at varying resolutions. These models handle inputs with higher noise levels at lower resolutions, while less noisy inputs are processed at higher resolutions. This hierarchical approach significantly reduces the computational cost of inference in multi-step denoising models. However, existing open-source pyramidal video models have been trained from scratch and tend to underperform compared to state-of-the-art systems in terms of visual plausibility. In this work, we present a pipeline that converts a pretrained diffusion model into a pyramidal one through low-cost finetuning, achieving this transformation without degradation in quality of output videos. Furthermore, we investigate and compare various strategies for step distillation within pyramidal models, aiming to further enhance the inference efficiency. Our results are available at https://qualcomm-ai-research.github.io/PyramidalWan.",
    "translation": "标题：PyramidalWan：构建金字塔式预训练视频模型以实现高效推理\n\n摘要：近期提出的金字塔模型将传统的正向与反向扩散过程分解为多个在不同分辨率下运行的阶段。这些模型在较低分辨率下处理噪声水平较高的输入，而在较高分辨率下处理噪声较少的输入。这种分层方法显著降低了多步去噪模型推理时的计算成本。然而，现有的开源金字塔视频模型均从头开始训练，在视觉合理性方面往往落后于当前最优系统。本研究提出一种通过低成本微调将预训练扩散模型转换为金字塔模型的流程，实现这一转换的同时不降低输出视频的质量。此外，我们探索并比较了金字塔模型内部步数蒸馏的不同策略，旨在进一步提升推理效率。我们的研究成果发布于 https://qualcomm-ai-research.github.io/PyramidalWan。",
    "url": "https://huggingface.co/papers/2601.04792",
    "arxiv_url": "https://arxiv.org/abs/2601.04792"
  },
  {
    "title": "AgentDevel: Reframing Self-Evolving LLM Agents as Release Engineering",
    "summary": "Recent progress in large language model (LLM) agents has largely focused on embedding self-improvement mechanisms inside the agent or searching over many concurrent variants. While these approaches can raise aggregate scores, they often yield unstable and hard-to-audit improvement trajectories, making it difficult to guarantee non-regression or to reason about failures across versions. We reframe agent improvement as release engineering: agents are treated as shippable artifacts, and improvement is externalized into a regression-aware release pipeline. We introduce AgentDevel, a release engineering pipeline that iteratively runs the current agent, produces implementation-blind, symptom-level quality signals from execution traces, synthesizes a single release candidate (RC) via executable diagnosis, and promotes it under flip-centered gating. AgentDevel features three core designs: (i) an implementation-blind LLM critic that characterizes failure appearances without accessing agent internals, (ii) script-based executable diagnosis that aggregates dominant symptom patterns and produces auditable engineering specifications, and (iii) flip-centered gating that prioritizes pass to fail regressions and fail to pass fixes as first-class evidence. Unlike population-based search or in-agent self-refinement, AgentDevel maintains a single canonical version line and emphasizes non-regression as a primary objective. Experiments on execution-heavy benchmarks demonstrate that AgentDevel yields stable improvements with significantly fewer regressions while producing reproducible, auditable artifacts. Overall, AgentDevel provides a practical development discipline for building, debugging, and releasing LLM agents as software development.",
    "translation": "标题：AgentDevel：将自演进大语言模型智能体重构为发布工程\n\n摘要：近期大语言模型（LLM）智能体的研究进展主要集中于在智能体内部嵌入自我改进机制或对多个并发变体进行搜索。尽管这些方法能够提升综合评分，但其改进轨迹往往不稳定且难以审计，导致难以保证版本间的非退化性或在故障发生时进行跨版本归因分析。本文将智能体改进重构为发布工程问题：将智能体视为可交付产物，并将改进过程外化为具备回归感知能力的发布流水线。我们提出AgentDevel——一种迭代运行当前智能体、从执行轨迹中生成与实现无关的症状级质量信号、通过可执行诊断合成单一发布候选版本，并基于翻转中心化门控机制进行版本升级的发布工程流水线。AgentDevel包含三项核心设计：（1）与实现无关的LLM批评器，在不访问智能体内部实现的情况下表征故障表象；（2）基于脚本的可执行诊断，聚合主导症状模式并生成可审计的工程规范；（3）以翻转为中心的门控机制，将“通过→失败”的回归案例与“失败→通过”的修复案例作为首要判定依据。与基于群体搜索或智能体内自优化方法不同，AgentDevel维持单一标准版本线，并将非退化性作为核心目标。在侧重执行能力的基准测试实验中，AgentDevel以显著更少的回归次数实现稳定改进，同时生成可复现、可审计的交付产物。总体而言，AgentDevel为将LLM智能体作为软件开发对象进行构建、调试与发布提供了实用的工程规范。",
    "url": "https://huggingface.co/papers/2601.04620",
    "arxiv_url": "https://arxiv.org/abs/2601.04620"
  },
  {
    "title": "Scaling Behavior Cloning Improves Causal Reasoning: An Open Model for Real-Time Video Game Playing",
    "summary": "Behavior cloning is enjoying a resurgence in popularity as scaling both model and data sizes proves to provide a strong starting point for many tasks of interest. In this work, we introduce an open recipe for training a video game playing foundation model designed for inference in realtime on a consumer GPU. We release all data (8300+ hours of high quality human gameplay), training and inference code, and pretrained checkpoints under an open license. We show that our best model is capable of playing a variety of 3D video games at a level competitive with human play. We use this recipe to systematically examine the scaling laws of behavior cloning to understand how the model's performance and causal reasoning varies with model and data scale. We first show in a simple toy problem that, for some types of causal reasoning, increasing both the amount of training data and the depth of the network results in the model learning a more causal policy. We then systematically study how causality varies with the number of parameters (and depth) and training steps in scaled models of up to 1.2 billion parameters, and we find similar scaling results to what we observe in the toy problem.",
    "translation": "标题：扩展行为克隆提升因果推理能力：一种用于实时视频游戏游玩的开源模型\n\n摘要：随着模型与数据规模的扩展被证明能为众多重要任务提供有力起点，行为克隆技术正重新获得广泛关注。本研究提出一种开源训练方案，用于构建专为消费级GPU实时推理设计的视频游戏游玩基础模型。我们以开放许可协议发布了全部数据（8300+小时高质量人类游玩记录）、训练与推理代码及预训练模型检查点。实验表明，我们的最优模型能在多种3D视频游戏中达到与人类玩家相当的水平。基于此训练方案，我们系统研究了行为克隆的缩放规律，以探究模型性能与因果推理能力如何随模型及数据规模变化。我们首先通过简单示例问题证明：对于特定类型的因果推理任务，增加训练数据量与网络深度可使模型习得更具因果性的策略。随后我们系统研究了在参数规模达12亿的扩展模型中，因果性如何随参数数量（及深度）与训练步数变化，并发现其缩放规律与示例问题中的观察结果具有一致性。",
    "url": "https://huggingface.co/papers/2601.04575",
    "arxiv_url": "https://arxiv.org/abs/2601.04575"
  },
  {
    "title": "ReHyAt: Recurrent Hybrid Attention for Video Diffusion Transformers",
    "summary": "Recent advances in video diffusion models have shifted towards transformer-based architectures, achieving state-of-the-art video generation but at the cost of quadratic attention complexity, which severely limits scalability for longer sequences. We introduce ReHyAt, a Recurrent Hybrid Attention mechanism that combines the fidelity of softmax attention with the efficiency of linear attention, enabling chunk-wise recurrent reformulation and constant memory usage. Unlike the concurrent linear-only SANA Video, ReHyAt's hybrid design allows efficient distillation from existing softmax-based models, reducing the training cost by two orders of magnitude to ~160 GPU hours, while being competitive in the quality. Our light-weight distillation and finetuning pipeline provides a recipe that can be applied to future state-of-the-art bidirectional softmax-based models. Experiments on VBench and VBench-2.0, as well as a human preference study, demonstrate that ReHyAt achieves state-of-the-art video quality while reducing attention cost from quadratic to linear, unlocking practical scalability for long-duration and on-device video generation. Project page is available at https://qualcomm-ai-research.github.io/rehyat.",
    "translation": "标题：ReHyAt：面向视频扩散变换器的循环混合注意力机制\n\n摘要：近期视频扩散模型的研究进展已转向基于变换器的架构，虽实现了最先进的视频生成效果，却以二次注意力复杂度为代价，严重限制了长序列的可扩展性。本文提出ReHyAt——一种循环混合注意力机制，通过融合Softmax注意力的精确性与线性注意力的高效性，实现了分块循环重构与恒定内存占用。与同期仅采用线性注意力的SANA Video不同，ReHyAt的混合设计能够高效地从现有基于Softmax的模型中提取知识，将训练成本降低两个数量级至约160 GPU小时，同时保持质量竞争力。我们提出的轻量化知识提取与微调流程为未来基于双向Softmax的先进模型提供了可复用的技术方案。在VBench与VBench-2.0基准测试及人工偏好评估中的实验表明，ReHyAt在将注意力成本从二次降至线性的同时，实现了业界领先的视频生成质量，为长时视频生成与端侧部署提供了切实可行的扩展路径。项目页面详见：https://qualcomm-ai-research.github.io/rehyat。",
    "url": "https://huggingface.co/papers/2601.04342",
    "arxiv_url": "https://arxiv.org/abs/2601.04342"
  },
  {
    "title": "Beyond Binary Preference: Aligning Diffusion Models to Fine-grained Criteria by Decoupling Attributes",
    "summary": "Post-training alignment of diffusion models relies on simplified signals, such as scalar rewards or binary preferences. This limits alignment with complex human expertise, which is hierarchical and fine-grained. To address this, we first construct a hierarchical, fine-grained evaluation criteria with domain experts, which decomposes image quality into multiple positive and negative attributes organized in a tree structure. Building on this, we propose a two-stage alignment framework. First, we inject domain knowledge to an auxiliary diffusion model via Supervised Fine-Tuning. Second, we introduce Complex Preference Optimization (CPO) that extends DPO to align the target diffusion to our non-binary, hierarchical criteria. Specifically, we reformulate the alignment problem to simultaneously maximize the probability of positive attributes while minimizing the probability of negative attributes with the auxiliary diffusion. We instantiate our approach in the domain of painting generation and conduct CPO training with an annotated dataset of painting with fine-grained attributes based on our criteria. Extensive experiments demonstrate that CPO significantly enhances generation quality and alignment with expertise, opening new avenues for fine-grained criteria alignment.",
    "translation": "标题：超越二元偏好：通过属性解耦实现扩散模型与细粒度评价标准的对齐\n\n摘要：扩散模型的后训练对齐通常依赖于简化的信号，如标量奖励或二元偏好。这限制了模型与复杂人类专业知识的对齐，而人类知识具有层次化和细粒度的特点。为解决这一问题，我们首先与领域专家共同构建了一套层次化、细粒度的评估标准，将图像质量分解为多个正负属性，并以树状结构组织。在此基础上，我们提出了一个两阶段对齐框架。第一阶段，我们通过监督微调将领域知识注入辅助扩散模型。第二阶段，我们提出复杂偏好优化方法，该方法将DPO扩展至非二元、层次化的标准对齐，使目标扩散模型能够与我们的标准对齐。具体而言，我们重新构建了对齐问题，旨在同时最大化正属性概率并最小化负属性概率，其中以辅助扩散模型作为参照。我们在绘画生成领域实例化了该方法，并基于标注的细粒度属性绘画数据集进行了CPO训练。大量实验表明，CPO显著提升了生成质量以及与专业知识的对齐程度，为细粒度标准对齐开辟了新途径。",
    "url": "https://huggingface.co/papers/2601.04300",
    "arxiv_url": "https://arxiv.org/abs/2601.04300"
  },
  {
    "title": "Enhancing Object Detection with Privileged Information: A Model-Agnostic Teacher-Student Approach",
    "summary": "This paper investigates the integration of the Learning Using Privileged Information (LUPI) paradigm in object detection to exploit fine-grained, descriptive information available during training but not at inference. We introduce a general, model-agnostic methodology for injecting privileged information-such as bounding box masks, saliency maps, and depth cues-into deep learning-based object detectors through a teacher-student architecture. Experiments are conducted across five state-of-the-art object detection models and multiple public benchmarks, including UAV-based litter detection datasets and Pascal VOC 2012, to assess the impact on accuracy, generalization, and computational efficiency. Our results demonstrate that LUPI-trained students consistently outperform their baseline counterparts, achieving significant boosts in detection accuracy with no increase in inference complexity or model size. Performance improvements are especially marked for medium and large objects, while ablation studies reveal that intermediate weighting of teacher guidance optimally balances learning from privileged and standard inputs. The findings affirm that the LUPI framework provides an effective and practical strategy for advancing object detection systems in both resource-constrained and real-world settings.",
    "translation": "标题：利用特权信息增强目标检测：一种模型无关的师生方法\n\n摘要：本文研究将特权信息学习范式融入目标检测中，以利用训练阶段可用但推理阶段缺失的细粒度描述性信息。我们提出了一种通用的、模型无关的方法论，通过师生架构将特权信息（如边界框掩码、显著性图和深度线索）注入基于深度学习的目标检测器中。实验在五种先进目标检测模型及多个公共基准数据集上进行，包括基于无人机的垃圾检测数据集和Pascal VOC 2012，以评估其对检测精度、泛化能力和计算效率的影响。结果表明，经特权信息学习训练的学生模型始终优于基线模型，在未增加推理复杂度或模型规模的情况下显著提升了检测精度。该改进对中型和大型目标尤为明显，而消融实验表明教师指导的中间加权策略能最优平衡从特权信息与标准输入中的学习。研究证实，特权信息学习框架为在资源受限和实际应用场景中推进目标检测系统提供了有效且实用的策略。",
    "url": "https://huggingface.co/papers/2601.02016",
    "arxiv_url": "https://arxiv.org/abs/2601.02016"
  },
  {
    "title": "VERSE: Visual Embedding Reduction and Space Exploration. Clustering-Guided Insights for Training Data Enhancement in Visually-Rich Document Understanding",
    "summary": "This work introduces VERSE, a methodology for analyzing and improving Vision-Language Models applied to Visually-rich Document Understanding by exploring their visual embedding space. VERSE enables the visualization of latent representations, supporting the assessment of model feasibility. It also facilitates the identification of problematic regions and guides the generation of synthetic data to enhance performance in those clusters. We validate the methodology by training on the synthetic MERIT Dataset and evaluating on its real-world counterpart, MERIT Secret. Results show that VERSE helps uncover the visual features associated with error-prone clusters, and that retraining with samples containing these features substantially boosts F1 performance without degrading generalization. Furthermore, we demonstrate that on-premise models such as Donut and Idefics2, when optimized with VERSE, match or even surpass the performance of SaaS solutions like GPT-4 and Pixtral.",
    "translation": "标题：VERSE：视觉嵌入降维与空间探索——面向视觉丰富文档理解的聚类引导训练数据增强方法\n\n摘要：本研究提出VERSE方法，通过探索视觉语言模型在视觉丰富文档理解任务中的视觉嵌入空间，实现模型分析与性能优化。该方法能够可视化潜在表征空间，辅助评估模型可行性，并支持识别问题区域以指导合成数据生成，从而针对性提升特定聚类区域的性能。我们在合成数据集MERIT上进行训练，并在其实验版本MERIT Secret上评估验证。结果表明，VERSE能有效揭示易出错聚类区域关联的视觉特征，通过加入包含这些特征的样本进行再训练，可在保持泛化能力的前提下显著提升F1性能。此外，研究证明经VERSE优化的本地化模型（如Donut和Idefics2）能够达到甚至超越GPT-4、Pixtral等SaaS解决方案的性能表现。",
    "url": "https://huggingface.co/papers/2601.05125",
    "arxiv_url": "https://arxiv.org/abs/2601.05125"
  },
  {
    "title": "Learning User Preferences Through Interaction for Long-Term Collaboration",
    "summary": "As conversational agents accumulate experience collaborating with users, adapting to user preferences is essential for fostering long-term relationships and improving collaboration quality over time. We introduce MultiSessionCollab, a benchmark that evaluates how well agents can learn user preferences and leverage them to improve collaboration quality throughout multiple sessions. To develop agents that succeed in this setting, we present long-term collaborative agents equipped with a memory that persists and refines user preference as interaction experience accumulates. Moreover, we demonstrate that learning signals can be derived from user simulator behavior in MultiSessionCollab to train agents to generate more comprehensive reflections and update their memory more effectively. Extensive experiments show that equipping agents with memory improves long-term collaboration, yielding higher task success rates, more efficient interactions, and reduced user effort. Finally, we conduct a human user study that demonstrates that memory helps improve user experience in real-world settings.",
    "translation": "标题：通过交互学习用户偏好以实现长期协作\n\n摘要：随着对话智能体在与用户协作过程中积累经验，适应用户偏好对于建立长期合作关系并持续提升协作质量至关重要。本文提出MultiSessionCollab基准测试框架，用于评估智能体在多轮会话中学习用户偏好并利用这些偏好提升协作质量的能力。为构建适应此场景的智能体，我们设计了配备记忆模块的长期协作智能体，该模块能够随着交互经验的积累持续存储并优化用户偏好模型。研究进一步表明，通过MultiSessionCollab中的用户模拟器行为可提取学习信号，用以训练智能体生成更全面的反思并实现更高效的内存更新机制。大量实验证明，配备记忆模块的智能体显著提升了长期协作效能，具体表现为任务成功率提高、交互效率优化以及用户操作负担降低。最后，我们通过真实用户实验验证了记忆机制在实际应用场景中对用户体验的改善作用。",
    "url": "https://huggingface.co/papers/2601.02702",
    "arxiv_url": "https://arxiv.org/abs/2601.02702"
  },
  {
    "title": "Safety at One Shot: Patching Fine-Tuned LLMs with A Single Instance",
    "summary": "Fine-tuning safety-aligned large language models (LLMs) can substantially compromise their safety. Previous approaches require many safety samples or calibration sets, which not only incur significant computational overhead during realignment but also lead to noticeable degradation in model utility. Contrary to this belief, we show that safety alignment can be fully recovered with only a single safety example, without sacrificing utility and at minimal cost. Remarkably, this recovery is effective regardless of the number of harmful examples used in fine-tuning or the size of the underlying model, and convergence is achieved within just a few epochs. Furthermore, we uncover the low-rank structure of the safety gradient, which explains why such efficient correction is possible. We validate our findings across five safety-aligned LLMs and multiple datasets, demonstrating the generality of our approach.",
    "translation": "标题：一例安全：基于单个实例修复微调后的大型语言模型\n\n摘要：对安全对齐的大型语言模型进行微调可能会严重损害其安全性。现有方法通常需要大量安全样本或校准集，这不仅在重新对齐过程中带来显著的计算开销，还会导致模型实用性能明显下降。与此观点相反，本研究表明，仅需单个安全示例即可完全恢复模型的安全对齐能力，且无需牺牲实用性能，成本极低。值得注意的是，无论微调过程中使用了多少有害示例，或基础模型的规模如何，这种恢复方法均能有效发挥作用，并且仅需几个训练周期即可实现收敛。此外，我们揭示了安全梯度的低秩结构，从而解释了这种高效修正何以成为可能。我们在五种安全对齐的大型语言模型及多个数据集上验证了研究结果，证明了本方法的普适性。",
    "url": "https://huggingface.co/papers/2601.01887",
    "arxiv_url": "https://arxiv.org/abs/2601.01887"
  },
  {
    "title": "LEMAS: Large A 150K-Hour Large-scale Extensible Multilingual Audio Suite with Generative Speech Models",
    "summary": "We present the LEMAS-Dataset, which, to our knowledge, is currently the largest open-source multilingual speech corpus with word-level timestamps. Covering over 150,000 hours across 10 major languages, LEMAS-Dataset is constructed via a efficient data processing pipeline that ensures high-quality data and annotations. To validate the effectiveness of LEMAS-Dataset across diverse generative paradigms, we train two benchmark models with distinct architectures and task specializations on this dataset. LEMAS-TTS, built upon a non-autoregressive flow-matching framework, leverages the dataset's massive scale and linguistic diversity to achieve robust zero-shot multilingual synthesis. Our proposed accent-adversarial training and CTC loss mitigate cross-lingual accent issues, enhancing synthesis stability. Complementarily, LEMAS-Edit employs an autoregressive decoder-only architecture that formulates speech editing as a masked token infilling task. By exploiting precise word-level alignments to construct training masks and adopting adaptive decoding strategies, it achieves seamless, smooth-boundary speech editing with natural transitions. Experimental results demonstrate that models trained on LEMAS-Dataset deliver high-quality synthesis and editing performance, confirming the dataset's quality. We envision that this richly timestamp-annotated, fine-grained multilingual corpus will drive future advances in prompt-based speech generation systems.",
    "translation": "标题：LEMAS：基于生成式语音模型的150千小时大规模可扩展多语言音频套件\n\n摘要：本文提出LEMAS数据集，据我们所知，这是目前规模最大的开源多语言语音语料库，具备词级时间戳标注。该数据集涵盖10种主要语言，总时长超过15万小时，通过高效的数据处理流程构建，确保了高质量的数据与标注。为验证LEMAS数据集在不同生成范式下的有效性，我们基于该数据集训练了两种不同架构与任务专精的基准模型。LEMAS-TTS基于非自回归流匹配框架，利用数据集的大规模与语言多样性实现了鲁棒的零样本多语言合成。我们提出的口音对抗训练与CTC损失缓解了跨语言口音问题，提升了合成稳定性。与之互补的LEMAS-Edit采用仅解码器的自回归架构，将语音编辑建模为掩码标记填充任务。通过利用精确的词级对齐构建训练掩码，并采用自适应解码策略，该模型实现了边界平滑、过渡自然的无缝语音编辑。实验结果表明，基于LEMAS数据集训练的模型能够提供高质量的合成与编辑性能，验证了数据集的优越性。我们预期这个具备丰富时间戳标注的细粒度多语言语料库将推动基于提示的语音生成系统的未来发展。",
    "url": "https://huggingface.co/papers/2601.04233",
    "arxiv_url": "https://arxiv.org/abs/2601.04233"
  },
  {
    "title": "Towards Open-Vocabulary Industrial Defect Understanding with a Large-Scale Multimodal Dataset",
    "summary": "We present IMDD-1M, the first large-scale Industrial Multimodal Defect Dataset comprising 1,000,000 aligned image-text pairs, designed to advance multimodal learning for manufacturing and quality inspection. IMDD-1M contains high-resolution real-world defects spanning over 60 material categories and more than 400 defect types, each accompanied by expert-verified annotations and fine-grained textual descriptions detailing defect location, severity, and contextual attributes. This dataset enables a wide spectrum of applications, including classification, segmentation, retrieval, captioning, and generative modeling. Building upon IMDD-1M, we train a diffusion-based vision-language foundation model from scratch, specifically tailored for industrial scenarios. The model serves as a generalizable foundation that can be efficiently adapted to specialized domains through lightweight fine-tuning. With less than 5% of the task-specific data required by dedicated expert models, it achieves comparable performance, highlighting the potential of data-efficient foundation model adaptation for industrial inspection and generation, paving the way for scalable, domain-adaptive, and knowledge-grounded manufacturing intelligence.",
    "translation": "标题：迈向开放词汇工业缺陷理解：基于大规模多模态数据集的研究\n\n摘要：本文提出首个大规模工业多模态缺陷数据集IMDD-1M，包含100万组对齐的图像-文本对，旨在推动制造业与质量检测领域的多模态学习。该数据集涵盖60余种材料类别和400多种缺陷类型的高分辨率真实缺陷样本，每个样本均配备经专家验证的标注信息及细粒度文本描述，详细说明缺陷位置、严重程度和上下文属性。本数据集支持分类、分割、检索、描述生成和生成式建模等多种应用场景。基于IMDD-1M，我们从头训练了一个专为工业场景设计的扩散式视觉-语言基础模型。该模型作为通用化基础架构，可通过轻量级微调高效适配特定领域：仅需专用专家模型不足5%的任务数据量即可达到相当性能，凸显了数据高效的基础模型适配在工业检测与生成任务中的潜力，为构建可扩展、领域自适应、知识驱动的智能制造系统开辟了新路径。",
    "url": "https://huggingface.co/papers/2512.24160",
    "arxiv_url": "https://arxiv.org/abs/2512.24160"
  }
]