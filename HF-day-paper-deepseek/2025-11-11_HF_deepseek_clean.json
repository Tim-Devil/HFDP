[
  {
    "title": "HaluMem: Evaluating Hallucinations in Memory Systems of Agents",
    "summary": "Memory systems are key components that enable AI systems such as LLMs and AI\nagents to achieve long-term learning and sustained interaction. However, during\nmemory storage and retrieval, these systems frequently exhibit memory\nhallucinations, including fabrication, errors, conflicts, and omissions.\nExisting evaluations of memory hallucinations are primarily end-to-end question\nanswering, which makes it difficult to localize the operational stage within\nthe memory system where hallucinations arise. To address this, we introduce the\nHallucination in Memory Benchmark (HaluMem), the first operation level\nhallucination evaluation benchmark tailored to memory systems. HaluMem defines\nthree evaluation tasks (memory extraction, memory updating, and memory question\nanswering) to comprehensively reveal hallucination behaviors across different\noperational stages of interaction. To support evaluation, we construct\nuser-centric, multi-turn human-AI interaction datasets, HaluMem-Medium and\nHaluMem-Long. Both include about 15k memory points and 3.5k multi-type\nquestions. The average dialogue length per user reaches 1.5k and 2.6k turns,\nwith context lengths exceeding 1M tokens, enabling evaluation of hallucinations\nacross different context scales and task complexities. Empirical studies based\non HaluMem show that existing memory systems tend to generate and accumulate\nhallucinations during the extraction and updating stages, which subsequently\npropagate errors to the question answering stage. Future research should focus\non developing interpretable and constrained memory operation mechanisms that\nsystematically suppress hallucinations and improve memory reliability.",
    "translation": "标题：HaluMem：智能体记忆系统中的幻觉现象评估研究\n\n摘要：记忆系统是实现大语言模型与智能体长期学习及持续交互的核心组件。然而在记忆存储与检索过程中，这些系统常出现记忆幻觉现象，具体表现为虚构、错漏、矛盾与缺失等问题。现有对记忆幻觉的评估主要采用端到端问答模式，难以准确定位幻觉产生的具体操作环节。为此，我们首次提出面向记忆系统的操作级幻觉评估基准HaluMem，通过定义记忆提取、记忆更新和记忆问答三项评估任务，系统揭示交互过程中不同操作阶段的幻觉行为特征。为支撑评估工作，我们构建了以用户为中心的多轮人机交互数据集HaluMem-Medium与HaluMem-Long，两者共包含约1.5万个记忆节点及3.5千道多类型问题。单用户平均对话轮次分别达到1.5千轮和2.6千轮，上下文长度超百万标记，可实现对不同上下文规模与任务复杂度下的幻觉评估。基于HaluMem的实证研究表明，现有记忆系统在提取与更新阶段易产生并积累幻觉，进而将误差传播至问答阶段。未来研究应致力于开发可解释的约束性记忆操作机制，系统抑制幻觉产生并提升记忆可靠性。",
    "url": "https://huggingface.co/papers/2511.03506",
    "arxiv_url": "https://arxiv.org/abs/2511.03506"
  },
  {
    "title": "IterResearch: Rethinking Long-Horizon Agents via Markovian State\n  Reconstruction",
    "summary": "Recent advances in deep-research agents have shown promise for autonomous\nknowledge construction through dynamic reasoning over external sources.\nHowever, existing approaches rely on a mono-contextual paradigm that\naccumulates all information in a single, expanding context window, leading to\ncontext suffocation and noise contamination that limit their effectiveness on\nlong-horizon tasks. We introduce IterResearch, a novel iterative deep-research\nparadigm that reformulates long-horizon research as a Markov Decision Process\nwith strategic workspace reconstruction. By maintaining an evolving report as\nmemory and periodically synthesizing insights, our approach preserves\nconsistent reasoning capacity across arbitrary exploration depths. We further\ndevelop Efficiency-Aware Policy Optimization (EAPO), a reinforcement learning\nframework that incentivizes efficient exploration through geometric reward\ndiscounting and enables stable distributed training via adaptive downsampling.\nExtensive experiments demonstrate that IterResearch achieves substantial\nimprovements over existing open-source agents with average +14.5pp across six\nbenchmarks and narrows the gap with frontier proprietary systems. Remarkably,\nour paradigm exhibits unprecedented interaction scaling, extending to 2048\ninteractions with dramatic performance gains (from 3.5\\% to 42.5\\%), and serves\nas an effective prompting strategy, improving frontier models by up to 19.2pp\nover ReAct on long-horizon tasks. These findings position IterResearch as a\nversatile solution for long-horizon reasoning, effective both as a trained\nagent and as a prompting paradigm for frontier models.",
    "translation": "标题：IterResearch：基于马尔可夫状态重构的长周期智能体范式再思考\n\n摘要：深度研究智能体的最新进展通过对外部信息源进行动态推理，为实现自主知识构建提供了可能。然而，现有方法依赖单一上下文范式，将所有信息累积在持续扩展的上下文窗口中，导致上下文窒息与噪声污染问题，限制了其在长周期任务中的效能。我们提出IterResearch——一种创新的迭代式深度研究范式，将长周期研究重新定义为具有策略性工作空间重构的马尔可夫决策过程。通过将动态演进的研究报告作为记忆体并定期整合研究洞见，该方法能在任意探索深度下保持稳定的推理能力。我们进一步开发了效率感知策略优化（EAPO），该强化学习框架通过几何奖励折现激励高效探索，并借助自适应降采样实现稳定的分布式训练。大量实验表明，IterResearch在六个基准测试中相较现有开源智能体平均提升14.5个百分点，显著缩小了与前沿私有系统的差距。值得注意的是，该范式展现出前所未有的交互扩展能力，可支持高达2048次交互且性能实现跨越式提升（从3.5%增至42.5%），同时作为有效的提示策略，在长周期任务上相较ReAct将前沿模型性能提升达19.2个百分点。这些发现确立了IterResearch作为长周期推理的通用解决方案，既能作为训练完成的智能体，也可作为前沿模型的提示范式。",
    "url": "https://huggingface.co/papers/2511.07327",
    "arxiv_url": "https://arxiv.org/abs/2511.07327"
  },
  {
    "title": "DRIVE: Data Curation Best Practices for Reinforcement Learning with\n  Verifiable Reward in Competitive Code Generation",
    "summary": "Recent reasoning-first models (e.g., OpenAI o1, DeepSeek R1) have spurred a\nresurgence of interest in RLVR. Nevertheless, advances are dominated by\nmathematics (e.g., AIME), with competitive-programming code generation\nunderexplored and data curation receiving less attention than RL algorithm\ndesign. We investigate how to construct RLVR datasets (i.e., RL prompts) and\npresent practical training techniques that yield strong performance on\ncompetitive-programming code generation. Our pipeline begins with supervised\nfine-tuning (SFT) distilled from strong open-source models, augmented with\ngeneral-purpose and reasoning-intensive data. RL then follows a two-stage\nprocess with executable, testcase-driven rewards: first, training on a large,\nuniformly distributed set of competitive-programming problems using Group\nRelative Policy Optimization (GRPO) with 8 rollouts per prompt and a relatively\nshort response-generation window (e.g., 32k during SFT and 24k in this stage)\nto expand entropy and mitigate repetition and truncation; second, we perform\nPre-GRPO: updating on a small, high-quality set of challenging\nproblems with a large rollout budget (64 rollouts per prompt) under a\nhard-focus curriculum that continuously retains the most difficult instances\nthroughout training. We implement our method on Qwen2.5-32B and evaluate on\nLeetCode and Codeforces weekly contests to avoid data leakage. The resulting\nmodel achieves state-of-the-art performance among models of similar scale and\nis comparable to leading systems such as DeepSeek v3.1 and Doubao-1.5-Thinking.\nWe also examine scaling trends and observe strong RL scaling on an internal\nlarge-scale MoE model. Our study distills concise best practices for data\ncuration, entropy expansion, and curriculum design in RLVR for\ncompetitive-programming code generation.",
    "translation": "标题：DRIVE：面向可验证奖励的竞争性代码生成强化学习数据管理最佳实践\n\n摘要：近期以推理为先的模型（如OpenAI o1、DeepSeek R1）推动了RLVR技术的复兴。然而当前进展主要集中于数学领域（如AIME），竞争性编程代码生成领域探索不足，且数据管理获得的关注远少于强化学习算法设计。本研究探讨如何构建RLVR数据集（即强化学习提示），并提出在竞争性编程代码生成中实现强劲性能的实用训练技术。我们的流程始于基于强开源模型蒸馏得到的监督微调，并辅以通用型与推理密集型数据增强。强化学习阶段采用可执行、测试用例驱动的奖励机制，实施两阶段训练：首先使用组相对策略优化方法，在均匀分布的大规模竞争性编程问题集上进行训练（每提示8次 rollout，响应生成窗口较短——监督微调阶段32K，本阶段24K），以扩展熵值并缓解重复与截断问题；随后执行预GRPO阶段：在高质量挑战性问题的小型数据集上，采用大 rollout 预算（每提示64次 rollout）和硬聚焦课程策略（持续保留训练全程中最难实例）进行参数更新。我们在Qwen2.5-32B模型上实施该方法，并通过LeetCode和Codeforces周赛评估以避免数据泄露。最终模型在同等规模模型中达到最先进性能，与DeepSeek v3.1、Doubao-1.5-Thinking等领先系统表现相当。我们还分析了扩展规律，在内部大规模混合专家模型上观察到显著的强化学习扩展效应。本研究提炼出针对竞争性编程代码生成的RLVR数据管理、熵扩展与课程设计的简明最佳实践。",
    "url": "https://huggingface.co/papers/2511.06307",
    "arxiv_url": "https://arxiv.org/abs/2511.06307"
  },
  {
    "title": "The Station: An Open-World Environment for AI-Driven Discovery",
    "summary": "We introduce the STATION, an open-world multi-agent environment that models a\nminiature scientific ecosystem. Leveraging their extended context windows,\nagents in the Station can engage in long scientific journeys that include\nreading papers from peers, formulating hypotheses, submitting code, performing\nanalyses, and publishing results. Importantly, there is no centralized system\ncoordinating their activities - agents are free to choose their own actions and\ndevelop their own narratives within the Station. Experiments demonstrate that\nAI agents in the Station achieve new state-of-the-art performance on a wide\nrange of benchmarks, spanning from mathematics to computational biology to\nmachine learning, notably surpassing AlphaEvolve in circle packing. A rich\ntapestry of narratives emerges as agents pursue independent research, interact\nwith peers, and build upon a cumulative history. From these emergent\nnarratives, novel methods arise organically, such as a new density-adaptive\nalgorithm for scRNA-seq batch integration. The Station marks a first step\ntowards autonomous scientific discovery driven by emergent behavior in an\nopen-world environment, representing a new paradigm that moves beyond rigid\noptimization.",
    "translation": "标题：《站台：面向人工智能驱动发现的开放世界环境》\n\n摘要：本文提出\"站台\"——一个模拟微型科研生态系统的开放世界多智能体环境。依托扩展上下文窗口，智能体可在站台中进行长周期科研探索，包括研读同行论文、提出假设、提交代码、执行分析及发表成果。值得关注的是，该系统不存在中央协调机制——智能体可自主选择行为并在环境中构建独立研究叙事。实验表明，站台中的AI智能体在数学、计算生物学、机器学习等多领域基准测试中均达到最新最优性能，尤其在圆包装问题上显著超越AlphaEvolve系统。当智能体开展自主研究、进行学术互动并累积历史成果时，会形成丰富的研究叙事脉络。这些涌现的叙事中自然衍生出创新方法，例如用于单细胞RNA测序批次整合的新型密度自适应算法。站台标志着在开放世界环境中通过涌现行为实现自主科学发现的重要突破，代表着超越刚性优化范式的新型研究范式的确立。",
    "url": "https://huggingface.co/papers/2511.06309",
    "arxiv_url": "https://arxiv.org/abs/2511.06309"
  },
  {
    "title": "MVU-Eval: Towards Multi-Video Understanding Evaluation for Multimodal\n  LLMs",
    "summary": "The advent of Multimodal Large Language Models (MLLMs) has expanded AI\ncapabilities to visual modalities, yet existing evaluation benchmarks remain\nlimited to single-video understanding, overlooking the critical need for\nmulti-video understanding in real-world scenarios (e.g., sports analytics and\nautonomous driving). To address this significant gap, we introduce MVU-Eval,\nthe first comprehensive benchmark for evaluating Multi-Video Understanding for\nMLLMs. Specifically, our MVU-Eval mainly assesses eight core competencies\nthrough 1,824 meticulously curated question-answer pairs spanning 4,959 videos\nfrom diverse domains, addressing both fundamental perception tasks and\nhigh-order reasoning tasks. These capabilities are rigorously aligned with\nreal-world applications such as multi-sensor synthesis in autonomous systems\nand cross-angle sports analytics. Through extensive evaluation of\nstate-of-the-art open-source and closed-source models, we reveal significant\nperformance discrepancies and limitations in current MLLMs' ability to perform\nunderstanding across multiple videos. The benchmark will be made publicly\navailable to foster future research.",
    "translation": "标题：MVU-Eval：面向多模态大语言模型的多视频理解评估体系\n\n摘要：多模态大语言模型（MLLMs）的出现将人工智能能力扩展至视觉模态，然而现有评估基准仍局限于单视频理解，忽视了现实场景（如体育赛事分析与自动驾驶）中对多视频理解的关键需求。为填补这一重要空白，我们推出MVU-Eval——首个面向MLLMs的多视频理解综合评估基准。该基准通过来自多元领域的4,959个视频构建的1,824个精编问答对，系统评估八大核心能力，涵盖基础感知任务与高阶推理任务。这些能力指标严格对标自动驾驶系统中的多传感器融合、多视角体育分析等实际应用场景。通过对前沿开源与闭源模型的广泛测试，我们揭示了当前MLLMs在多视频理解能力方面存在的显著性能差异与局限性。本基准将公开提供，以推动该领域的后续研究。",
    "url": "https://huggingface.co/papers/2511.07250",
    "arxiv_url": "https://arxiv.org/abs/2511.07250"
  },
  {
    "title": "Routing Manifold Alignment Improves Generalization of Mixture-of-Experts\n  LLMs",
    "summary": "Sparse Mixture-of-Experts (MoE) have been widely adopted in recent large\nlanguage models since it can efficiently scale up the model capability without\nincreasing the inference cost. However, evaluations on broad downstream tasks\nreveal a consistent suboptimality of the routers in existing MoE LLMs, which\nresults in a severe performance gap (e.g., 10-20% in accuracy) to the optimal\nrouting. In this paper, we show that aligning the manifold of routing weights\nwith that of task embedding can effectively reduce the gap and improve MoE\nLLMs' generalization performance. Our method, \"Routing Manifold Alignment\n(RoMA)\", introduces an additional manifold regularization term in the\npost-training objective and only requires lightweight finetuning of routers\n(with other parameters frozen). Specifically, the regularization encourages the\nrouting weights of each sample to be close to those of its successful neighbors\n(whose routing weights lead to correct answers) in a task embedding space.\nConsequently, samples targeting similar tasks will share similar expert choices\nacross layers. Building such bindings between tasks and experts over different\nsamples is essential to achieve better generalization. Moreover, RoMA\ndemonstrates the advantage of unifying the task understanding (by embedding\nmodels) with solution generation (by MoE LLMs). In experiments, we finetune\nrouters in OLMoE, DeepSeekMoE, and Qwen3-MoE using RoMA. Evaluations on diverse\nbenchmarks and extensive comparisons with baselines show the substantial\nimprovement brought by RoMA.",
    "translation": "标题：路由流形对齐提升混合专家大语言模型的泛化能力\n\n摘要：稀疏混合专家模型因其能在不增加推理成本的前提下有效扩展模型能力，已被广泛应用于当前的大语言模型。然而，在广泛下游任务上的评估表明，现有混合专家大语言模型中的路由模块普遍存在次优问题，导致其与最优路由存在显著性能差距（例如准确率相差10-20%）。本文提出通过将路由权重流形与任务嵌入流形对齐，可有效缩小该差距并提升混合专家大语言模型的泛化性能。我们提出的\"路由流形对齐方法\"在训练目标中引入额外的流形正则项，仅需对路由模块进行轻量级微调（其余参数冻结）。具体而言，该正则化促使每个样本的路由权重在任务嵌入空间中接近其成功邻域样本（即路由权重能得出正确答案的样本）的权重，从而使面向相似任务的样本在不同网络层能保持一致的专家选择模式。在不同样本间建立任务与专家的绑定关系，对实现更优泛化能力至关重要。此外，该方法还展现了将任务理解（通过嵌入模型）与解决方案生成（通过混合专家大语言模型）相统一的优势。实验中，我们使用该方法对OLMoE、DeepSeekMoE和Qwen3-MoE的路由模块进行微调。在多基准测试中的评估结果以及与基线模型的广泛对比表明，该方法带来了显著的性能提升。",
    "url": "https://huggingface.co/papers/2511.07419",
    "arxiv_url": "https://arxiv.org/abs/2511.07419"
  },
  {
    "title": "RedOne 2.0: Rethinking Domain-specific LLM Post-Training in Social\n  Networking Services",
    "summary": "As a key medium for human interaction and information exchange, social\nnetworking services (SNS) pose unique challenges for large language models\n(LLMs): heterogeneous workloads, fast-shifting norms and slang, and\nmultilingual, culturally diverse corpora that induce sharp distribution shift.\nSupervised fine-tuning (SFT) can specialize models but often triggers a\n``seesaw'' between in-distribution gains and out-of-distribution robustness,\nespecially for smaller models. To address these challenges, we introduce RedOne\n2.0, an SNS-oriented LLM trained with a progressive, RL-prioritized\npost-training paradigm designed for rapid and stable adaptation. The pipeline\nconsist in three stages: (1) Exploratory Learning on curated SNS corpora to\nestablish initial alignment and identify systematic weaknesses; (2) Targeted\nFine-Tuning that selectively applies SFT to the diagnosed gaps while mixing a\nsmall fraction of general data to mitigate forgetting; and (3) Refinement\nLearning that re-applies RL with SNS-centric signals to consolidate\nimprovements and harmonize trade-offs across tasks. Across various tasks\nspanning three categories, our 4B scale model delivers an average improvements\nabout 2.41 over the 7B sub-optimal baseline. Additionally, RedOne 2.0 achieves\naverage performance lift about 8.74 from the base model with less than half the\ndata required by SFT-centric method RedOne, evidencing superior data efficiency\nand stability at compact scales. Overall, RedOne 2.0 establishes a competitive,\ncost-effective baseline for domain-specific LLMs in SNS scenario, advancing\ncapability without sacrificing robustness.",
    "translation": "标题：RedOne 2.0：社交网络服务中领域特定大语言模型后训练机制的重新思考\n\n摘要：作为人类互动与信息交换的关键媒介，社交网络服务（SNS）对大语言模型（LLM）提出了独特挑战：异构工作负载、快速演变的网络规范与俚语、以及引发显著分布偏移的多语言跨文化语料。监督微调（SFT）虽能实现模型专业化，但往往会引发域内性能提升与域外鲁棒性之间的“跷跷板效应”，这一矛盾在小型模型中尤为突出。为应对这些挑战，我们提出RedOne 2.0——采用渐进式强化学习优先的后训练范式，专为快速稳定适配SNS场景而设计。该流程包含三个阶段：（1）基于精选SNS语料的探索性学习，建立初始对齐并识别系统性缺陷；（2）靶向微调阶段，针对诊断出的能力缺口选择性应用SFT，同时混入少量通用数据以缓解灾难性遗忘；（3）强化学习阶段，重新应用以SNS为核心信号的RL机制，巩固改进效果并协调多任务间的权衡关系。在涵盖三大类别的多样化任务测试中，我们的40亿参数模型相较70亿参数次优基线平均提升2.41个性能点。此外，RedOne 2.0相较基础模型实现平均8.74的性能跃升，其所需训练数据量不足以SFT为核心的方法RedOne的一半，彰显出在紧凑模型规模下卓越的数据效率与训练稳定性。总体而言，RedOne 2.0为SNS场景中的领域特定大语言模型建立了具有竞争力的成本效益基准，在保持鲁棒性的同时实现了能力突破。",
    "url": "https://huggingface.co/papers/2511.07070",
    "arxiv_url": "https://arxiv.org/abs/2511.07070"
  },
  {
    "title": "SofT-GRPO: Surpassing Discrete-Token LLM Reinforcement Learning via\n  Gumbel-Reparameterized Soft-Thinking Policy Optimization",
    "summary": "The soft-thinking paradigm for Large Language Model (LLM) reasoning can\noutperform the conventional discrete-token Chain-of-Thought (CoT) reasoning in\nsome scenarios, underscoring its research and application value. However, while\nthe discrete-token CoT reasoning pattern can be reinforced through policy\noptimization algorithms such as group relative policy optimization (GRPO),\nextending the soft-thinking pattern with Reinforcement Learning (RL) remains\nchallenging. This difficulty stems from the complexities of injecting\nstochasticity into soft-thinking tokens and updating soft-thinking policies\naccordingly. As a result, previous attempts to combine soft-thinking with GRPO\ntypically underperform their discrete-token GRPO counterparts. To fully unlock\nthe potential of soft-thinking, this paper presents a novel policy optimization\nalgorithm, SofT-GRPO, to reinforce LLMs under the soft-thinking reasoning\npattern. SofT-GRPO injects the Gumbel noise into logits, employs the\nGumbel-Softmax technique to avoid soft-thinking tokens outside the pre-trained\nembedding space, and leverages the reparameterization trick in policy gradient.\nWe conduct experiments across base LLMs ranging from 1.5B to 7B parameters, and\nresults demonstrate that SofT-GRPO enables soft-thinking LLMs to slightly\noutperform discrete-token GRPO on Pass@1 (+0.13% on average accuracy), while\nexhibiting a substantial uplift on Pass@32 (+2.19% on average accuracy). Codes\nand weights are available on https://github.com/zz1358m/SofT-GRPO-master",
    "translation": "标题：SofT-GRPO：基于Gumbel重参数化软思考策略优化突破离散令牌大语言模型强化学习\n\n摘要：大语言模型的软思考推理范式在某些场景下能够超越传统的离散令牌思维链推理，彰显其研究价值与应用潜力。然而，虽然离散令牌思维链推理模式可通过群体相对策略优化等算法进行强化，但将软思考模式与强化学习相结合仍面临挑战。这一困难源于向软思考令牌注入随机性及相应策略更新的复杂性，导致先前将软思考与GRPO结合的尝试通常表现不及离散令牌GRPO方法。为充分释放软思考的潜力，本文提出新型策略优化算法SofT-GRPO，在软思考推理模式下强化大语言模型。该算法通过向logits注入Gumbel噪声，采用Gumbel-Softmax技术避免软思考令牌超出预训练嵌入空间，并在策略梯度中运用重参数化技巧。我们在1.5B至7B参数的基础大语言模型上进行实验，结果表明：SofT-GRPO使软思考大语言模型在Pass@1指标上略优于离散令牌GRPO（平均准确率提升0.13%），同时在Pass@32指标上实现显著提升（平均准确率增长2.19%）。代码与权重文件已发布于https://github.com/zz1358m/SofT-GRPO-master",
    "url": "https://huggingface.co/papers/2511.06411",
    "arxiv_url": "https://arxiv.org/abs/2511.06411"
  },
  {
    "title": "Reasoning with Confidence: Efficient Verification of LLM Reasoning Steps\n  via Uncertainty Heads",
    "summary": "Solving complex tasks usually requires LLMs to generate long multi-step\nreasoning chains. Previous work has shown that verifying the correctness of\nindividual reasoning steps can further improve the performance and efficiency\nof LLMs on such tasks and enhance solution interpretability. However, existing\nverification approaches, such as Process Reward Models (PRMs), are either\ncomputationally expensive, limited to specific domains, or require large-scale\nhuman or model-generated annotations. Thus, we propose a lightweight\nalternative for step-level reasoning verification based on data-driven\nuncertainty scores. We train transformer-based uncertainty quantification heads\n(UHeads) that use the internal states of a frozen LLM to estimate the\nuncertainty of its reasoning steps during generation. The approach is fully\nautomatic: target labels are generated either by another larger LLM (e.g.,\nDeepSeek R1) or in a self-supervised manner by the original model itself.\nUHeads are both effective and lightweight, containing less than 10M parameters.\nAcross multiple domains, including mathematics, planning, and general knowledge\nquestion answering, they match or even surpass the performance of PRMs that are\nup to 810x larger. Our findings suggest that the internal states of LLMs encode\ntheir uncertainty and can serve as reliable signals for reasoning verification,\noffering a promising direction toward scalable and generalizable introspective\nLLMs.",
    "translation": "标题：基于置信度的推理：通过不确定性头部实现大语言模型推理步骤的高效验证\n\n摘要：解决复杂任务通常需要大语言模型生成多步骤的推理链。已有研究表明，对单个推理步骤的正确性进行验证能够进一步提升大语言模型在此类任务中的表现与效率，并增强解决方案的可解释性。然而现有的验证方法（如过程奖励模型）存在计算成本高昂、适用领域受限或需要大规模人工/模型生成标注等问题。为此，我们提出一种基于数据驱动不确定性评分的轻量级步骤级推理验证方案。通过训练基于Transformer的不确定性量化头部，利用冻结参数大语言模型的内部状态来估计其生成过程中推理步骤的不确定性。该方法完全自动化：目标标签可由更大规模语言模型（如DeepSeek R1）生成，或通过原模型以自监督方式产生。不确定性头部在参数量不足1000万的情况下兼具高效性与轻量化特性。在数学推理、任务规划及常识问答等多个领域，其性能媲美甚至超越参数量达810倍的过程奖励模型。我们的研究结果表明，大语言模型的内部状态编码了其不确定性，可作为推理验证的可靠信号，为构建可扩展、可泛化的自省式大语言模型指明了前景广阔的研究方向。",
    "url": "https://huggingface.co/papers/2511.06209",
    "arxiv_url": "https://arxiv.org/abs/2511.06209"
  },
  {
    "title": "Robot Learning from a Physical World Model",
    "summary": "We introduce PhysWorld, a framework that enables robot learning from video\ngeneration through physical world modeling. Recent video generation models can\nsynthesize photorealistic visual demonstrations from language commands and\nimages, offering a powerful yet underexplored source of training signals for\nrobotics. However, directly retargeting pixel motions from generated videos to\nrobots neglects physics, often resulting in inaccurate manipulations. PhysWorld\naddresses this limitation by coupling video generation with physical world\nreconstruction. Given a single image and a task command, our method generates\ntask-conditioned videos and reconstructs the underlying physical world from the\nvideos, and the generated video motions are grounded into physically accurate\nactions through object-centric residual reinforcement learning with the\nphysical world model. This synergy transforms implicit visual guidance into\nphysically executable robotic trajectories, eliminating the need for real robot\ndata collection and enabling zero-shot generalizable robotic manipulation.\nExperiments on diverse real-world tasks demonstrate that PhysWorld\nsubstantially improves manipulation accuracy compared to previous approaches.\nVisit https://pointscoder.github.io/PhysWorld_Web/{the project webpage}\nfor details.",
    "translation": "标题：基于物理世界模型的机器人学习\n\n摘要：本文提出PhysWorld框架，通过物理世界建模实现基于视频生成的机器人学习。当前视频生成模型能够根据语言指令和图像合成逼真的视觉演示，这为机器人技术提供了强大却尚未充分开发的训练信号来源。然而，直接将从生成视频中提取的像素运动映射到机器人会忽略物理规律，通常导致操作失准。PhysWorld通过耦合视频生成与物理世界重建来解决这一局限。给定单张图像和任务指令，本方法既能生成任务条件视频，又能从视频中重建底层物理世界，同时通过基于对象的残差强化学习与物理世界模型，将生成的视频运动转化为物理精确的动作。这种协同作用将隐性视觉指导转化为可物理执行的机器人轨迹，无需真实机器人数据采集即可实现零样本可泛化的机器人操作。在多样化现实任务上的实验表明，与现有方法相比，PhysWorld显著提升了操作精度。详情请访问项目网页：https://pointscoder.github.io/PhysWorld_Web/",
    "url": "https://huggingface.co/papers/2511.07416",
    "arxiv_url": "https://arxiv.org/abs/2511.07416"
  },
  {
    "title": "Teaching Pretrained Language Models to Think Deeper with Retrofitted\n  Recurrence",
    "summary": "Recent advances in depth-recurrent language models show that recurrence can\ndecouple train-time compute and parameter count from test-time compute. In this\nwork, we study how to convert existing pretrained non-recurrent language models\ninto depth-recurrent models. We find that using a curriculum of recurrences to\nincrease the effective depth of the model over the course of training preserves\nperformance while reducing total computational cost. In our experiments, on\nmathematics, we observe that converting pretrained models to recurrent ones\nresults in better performance at a given compute budget than simply\npost-training the original non-recurrent language model.",
    "translation": "标题：基于追溯循环机制的预训练语言模型深度思维教学方法\n\n摘要：深度循环语言模型的最新进展表明，循环机制能够将训练时的计算量与参数规模同测试时的计算需求分离开来。本研究探索如何将现有非循环预训练语言模型转化为深度循环模型。我们发现，通过采用渐进式循环课程学习策略，在训练过程中逐步增加模型有效深度，可以在保持性能的同时降低总体计算成本。在数学领域的实验中，相较于直接对原始非循环语言模型进行后训练，将预训练模型转化为循环结构能在相同计算预算下获得更优的性能表现。",
    "url": "https://huggingface.co/papers/2511.07384",
    "arxiv_url": "https://arxiv.org/abs/2511.07384"
  },
  {
    "title": "NURBGen: High-Fidelity Text-to-CAD Generation through LLM-Driven NURBS\n  Modeling",
    "summary": "Generating editable 3D CAD models from natural language remains challenging,\nas existing text-to-CAD systems either produce meshes or rely on scarce\ndesign-history data. We present NURBGen, the first framework to generate\nhigh-fidelity 3D CAD models directly from text using Non-Uniform Rational\nB-Splines (NURBS). To achieve this, we fine-tune a large language model (LLM)\nto translate free-form texts into JSON representations containing NURBS surface\nparameters (i.e, control points, knot vectors, degrees, and rational\nweights) which can be directly converted into BRep format using Python. We\nfurther propose a hybrid representation that combines untrimmed NURBS with\nanalytic primitives to handle trimmed surfaces and degenerate regions more\nrobustly, while reducing token complexity. Additionally, we introduce partABC,\na curated subset of the ABC dataset consisting of individual CAD components,\nannotated with detailed captions using an automated annotation pipeline.\nNURBGen demonstrates strong performance on diverse prompts, surpassing prior\nmethods in geometric fidelity and dimensional accuracy, as confirmed by expert\nevaluations. Code and dataset will be released publicly.",
    "translation": "标题：NURBGen：基于大语言模型驱动的NURBS建模实现高保真文本到CAD生成\n\n摘要：从自然语言生成可编辑的3D CAD模型仍面临挑战，现有文本到CAD系统要么生成网格模型，要么依赖稀缺的设计历史数据。我们提出NURBGen——首个通过非均匀有理B样条（NURBS）直接根据文本生成高保真3D CAD模型的框架。为实现这一目标，我们微调大语言模型，将自由格式文本转换为包含NURBS曲面参数（即控制点、节点向量、阶数和有理权重）的JSON表示，这些参数可通过Python直接转换为BRep格式。我们进一步提出混合表示法，将未修剪NURBS与解析图元相结合，以更稳健地处理修剪曲面和退化区域，同时降低标记复杂度。此外，我们推出partABC数据集——这是ABC数据集的精选子集，包含独立CAD组件，并通过自动化标注流程配以详细描述。专家评估证实，NURBGen在多样化提示词上表现优异，在几何保真度和尺寸精度方面超越现有方法。代码与数据集将公开发布。",
    "url": "https://huggingface.co/papers/2511.06194",
    "arxiv_url": "https://arxiv.org/abs/2511.06194"
  },
  {
    "title": "Long Grounded Thoughts: Distilling Compositional Visual Reasoning Chains at Scale",
    "summary": "Recent progress in multimodal reasoning has been driven largely by undisclosed datasets and proprietary data synthesis recipes, leaving open questions about how to systematically build large-scale, vision-centric reasoning datasets, particularly for tasks that go beyond visual math. In this work, we introduce a new reasoning data generation framework spanning diverse skills and levels of complexity with over 1M high-quality synthetic vision-centric questions. The dataset also includes preference data and instruction prompts supporting both offline and online RL. Our synthesis framework proceeds in two stages: (1) scale; and (2) complexity. Reasoning traces are then synthesized through a two-stage process that leverages VLMs and reasoning LLMs, producing CoT traces for VLMs that capture the richness and diverse cognitive behaviors found in frontier reasoning models. Remarkably, we show that finetuning Qwen2.5-VL-7B on our data outperforms all open-data baselines across all evaluated vision-centric benchmarks, and even surpasses strong closed-data models such as MiMo-VL-7B-RL on V* Bench, CV-Bench and MMStar-V. Perhaps most surprising, despite being entirely vision-centric, our data transfers positively to text-only reasoning (MMLU-Pro) and audio reasoning (MMAU), demonstrating its effectiveness. Similarly, despite not containing videos or embodied visual data, we observe notable gains when evaluating on a single-evidence embodied QA benchmark (NiEH). Finally, we use our data to analyze the entire VLM post-training pipeline. Our empirical analysis highlights that (i) SFT on high-quality data with non-linear reasoning traces is essential for effective online RL, (ii) staged offline RL matches online RL's performance while reducing compute demands, and (iii) careful SFT on high quality data can substantially improve out-of-domain, cross-modality transfer.",
    "translation": "标题：长链思维推理：大规模提炼组合式视觉推理路径\n\n摘要：当前多模态推理的进展主要依赖于未公开数据集和专有数据合成方案，这引发了对如何系统化构建大规模视觉中心推理数据集的开放性问题，尤其是针对超越视觉数学范畴的任务。本研究提出新型推理数据生成框架，涵盖多样化技能与复杂度层级，生成超过100万道高质量合成视觉中心问题。该数据集同时包含偏好数据与指令提示，支持离线和在线强化学习。我们的合成框架分两阶段实施：（1）规模化扩展；（2）复杂度提升。通过融合视觉语言模型与推理大语言模型的双阶段处理流程，生成适用于视觉语言模型的思维链轨迹，捕捉前沿推理模型中丰富的多样化认知行为。值得注意的是，基于我们数据微调的Qwen2.5-VL-7B模型在所有评估的视觉中心基准测试中均超越开源基线模型，甚至在V* Bench、CV-Bench和MMStar-V基准上优于MiMo-VL-7B-RL等强效闭源模型。最令人惊讶的是，尽管完全专注于视觉领域，我们的数据在纯文本推理（MMLU-Pro）和音频推理（MMAU）任务中展现出正向迁移能力。同样，在未包含视频或具身视觉数据的情况下，我们在单证据具身问答基准（NiEH）上观察到显著性能提升。最后，我们利用该数据系统分析视觉语言模型的后训练流程，实证研究表明：（i）基于含非线性推理轨迹的高质量数据进行监督微调是在线强化学习生效的关键；（ii）分阶段离线强化学习可匹配在线强化学习性能，同时降低计算需求；（iii）对高质量数据实施精细监督微调能显著提升跨领域、跨模态的迁移性能。",
    "url": "https://huggingface.co/papers/2511.05705",
    "arxiv_url": "https://arxiv.org/abs/2511.05705"
  },
  {
    "title": "RLoop: An Self-Improving Framework for Reinforcement Learning with\n  Iterative Policy Initialization",
    "summary": "While Reinforcement Learning for Verifiable Rewards (RLVR) is powerful for\ntraining large reasoning models, its training dynamics harbor a critical\nchallenge: RL overfitting, where models gain training rewards but lose\ngeneralization. Our analysis reveals this is driven by policy\nover-specialization and catastrophic forgetting of diverse solutions generated\nduring training. Standard optimization discards this valuable inter-step policy\ndiversity. To address this, we introduce RLoop, a self-improving framework\nbuilt on iterative policy initialization. RLoop transforms the standard\ntraining process into a virtuous cycle: it first uses RL to explore the\nsolution space from a given policy, then filters the successful trajectories to\ncreate an expert dataset. This dataset is used via Rejection-sampling\nFine-Tuning (RFT) to refine the initial policy, creating a superior starting\npoint for the next iteration. This loop of exploration and exploitation via\niterative re-initialization effectively converts transient policy variations\ninto robust performance gains. Our experiments show RLoop mitigates forgetting\nand substantially improves generalization, boosting average accuracy by 9% and\npass@32 by over 15% compared to vanilla RL.",
    "translation": "标题：RLoop：基于迭代策略初始化的强化学习自改进框架\n\n摘要：尽管可验证奖励的强化学习（RLVR）在训练大型推理模型方面具有强大能力，但其训练动态存在一个关键挑战：RL过拟合，即模型获得训练奖励却丧失泛化能力。我们的分析表明，这一现象由策略过度特化及训练过程中产生的多样化解决方案的灾难性遗忘所驱动。标准优化方法会丢弃这些宝贵的跨步骤策略多样性。为解决此问题，我们提出RLoop——一个基于迭代策略初始化的自改进框架。RLoop将标准训练过程转化为良性循环：首先使用RL从给定策略出发探索解空间，随后筛选成功轨迹构建专家数据集。通过拒绝采样微调（RFT）利用该数据集优化初始策略，为下一轮迭代创建更优的起点。这种通过迭代重初始化的探索与利用循环，有效将瞬态策略变异转化为稳健的性能提升。实验表明，RLoop能有效缓解遗忘现象并显著提升泛化能力，相较于原始RL方法，平均准确率提高9%，pass@32指标提升超过15%。",
    "url": "https://huggingface.co/papers/2511.04285",
    "arxiv_url": "https://arxiv.org/abs/2511.04285"
  },
  {
    "title": "DigiData: Training and Evaluating General-Purpose Mobile Control Agents",
    "summary": "AI agents capable of controlling user interfaces have the potential to\ntransform human interaction with digital devices. To accelerate this\ntransformation, two fundamental building blocks are essential: high-quality\ndatasets that enable agents to achieve complex and human-relevant goals, and\nrobust evaluation methods that allow researchers and practitioners to rapidly\nenhance agent performance. In this paper, we introduce DigiData, a large-scale,\nhigh-quality, diverse, multi-modal dataset designed for training mobile control\nagents. Unlike existing datasets, which derive goals from unstructured\ninteractions, DigiData is meticulously constructed through comprehensive\nexploration of app features, resulting in greater diversity and higher goal\ncomplexity. Additionally, we present DigiData-Bench, a benchmark for evaluating\nmobile control agents on real-world complex tasks. We demonstrate that the\ncommonly used step-accuracy metric falls short in reliably assessing mobile\ncontrol agents and, to address this, we propose dynamic evaluation protocols\nand AI-powered evaluations as rigorous alternatives for agent assessment. Our\ncontributions aim to significantly advance the development of mobile control\nagents, paving the way for more intuitive and effective human-device\ninteractions.",
    "translation": "标题：DigiData：通用移动控制智能体的训练与评估框架\n\n摘要：具备用户界面控制能力的人工智能体有望彻底改变人类与数字设备的交互模式。为加速这一变革进程，两大基础要素不可或缺：一是能够支持智能体实现复杂且符合人类需求目标的高质量数据集，二是可供研究与实践人员快速提升智能体性能的稳健评估方法。本文提出的DigiData是一个专为移动控制智能体训练设计的大规模、高质量、多模态数据集。与现有基于非结构化交互目标的数据集不同，DigiData通过系统性探索应用程序功能精心构建，具有更丰富的多样性特征和更高的目标复杂度。同时，我们推出DigiData-Bench评估基准，用于在真实世界复杂任务场景下检验移动控制智能体性能。研究表明，当前广泛采用的分步准确率指标难以可靠评估移动控制智能体，为此我们提出动态评估协议与人工智能驱动的评估方法作为更严谨的替代方案。本研究的成果将有力推动移动控制智能体发展，为构建更直观高效的人机交互模式奠定基础。",
    "url": "https://huggingface.co/papers/2511.07413",
    "arxiv_url": "https://arxiv.org/abs/2511.07413"
  },
  {
    "title": "MPJudge: Towards Perceptual Assessment of Music-Induced Paintings",
    "summary": "Music induced painting is a unique artistic practice, where visual artworks\nare created under the influence of music. Evaluating whether a painting\nfaithfully reflects the music that inspired it poses a challenging perceptual\nassessment task. Existing methods primarily rely on emotion recognition models\nto assess the similarity between music and painting, but such models introduce\nconsiderable noise and overlook broader perceptual cues beyond emotion. To\naddress these limitations, we propose a novel framework for music induced\npainting assessment that directly models perceptual coherence between music and\nvisual art. We introduce MPD, the first large scale dataset of music painting\npairs annotated by domain experts based on perceptual coherence. To better\nhandle ambiguous cases, we further collect pairwise preference annotations.\nBuilding on this dataset, we present MPJudge, a model that integrates music\nfeatures into a visual encoder via a modulation based fusion mechanism. To\neffectively learn from ambiguous cases, we adopt Direct Preference Optimization\nfor training. Extensive experiments demonstrate that our method outperforms\nexisting approaches. Qualitative results further show that our model more\naccurately identifies music relevant regions in paintings.",
    "translation": "标题：MPJudge：音乐诱发绘画的感知评估研究\n\n摘要：音乐诱发绘画是一种独特的艺术实践，指在音乐影响下创作视觉艺术作品。评估画作是否忠实反映其灵感来源的音乐，构成了一项具有挑战性的感知评估任务。现有方法主要依赖情感识别模型来评估音乐与绘画的相似性，但这类模型会引入显著噪声且忽略了情感之外的更广泛感知线索。为解决这些局限，我们提出了一种新颖的音乐诱发绘画评估框架，直接建模音乐与视觉艺术之间的感知连贯性。我们引入了MPD数据集——首个由领域专家基于感知连贯性标注的大规模音乐-绘画配对数据集。为更好处理模糊案例，我们进一步收集了成对偏好标注。基于该数据集，我们提出了MPJudge模型，通过基于调制的融合机制将音乐特征整合到视觉编码器中。为有效学习模糊案例，我们采用直接偏好优化方法进行训练。大量实验表明，我们的方法优于现有方法。定性结果进一步证明，我们的模型能更准确地识别绘画中与音乐相关的区域。",
    "url": "https://huggingface.co/papers/2511.07137",
    "arxiv_url": "https://arxiv.org/abs/2511.07137"
  },
  {
    "title": "Llama-Embed-Nemotron-8B: A Universal Text Embedding Model for\n  Multilingual and Cross-Lingual Tasks",
    "summary": "We introduce llama-embed-nemotron-8b, an open-weights text embedding model\nthat achieves state-of-the-art performance on the Multilingual Massive Text\nEmbedding Benchmark (MMTEB) leaderboard as of October 21, 2025. While recent\nmodels show strong performance, their training data or methodologies are often\nnot fully disclosed. We aim to address this by developing a fully open-source\nmodel, publicly releasing its weights and detailed ablation studies, and\nplanning to share the curated training datasets. Our model demonstrates\nsuperior performance across all major embedding tasks -- including retrieval,\nclassification and semantic textual similarity (STS) -- and excels in\nchallenging multilingual scenarios, such as low-resource languages and\ncross-lingual setups. This state-of-the-art performance is driven by a novel\ndata mix of 16.1 million query-document pairs, split between 7.7 million\nsamples from public datasets and 8.4 million synthetically generated examples\nfrom various open-weight LLMs. One of our key contributions is a detailed\nablation study analyzing core design choices, including a comparison of\ncontrastive loss implementations, an evaluation of synthetic data generation\n(SDG) strategies, and the impact of model merging. The llama-embed-nemotron-8b\nis an instruction-aware model, supporting user-defined instructions to enhance\nperformance for specific use-cases. This combination of top-tier performance,\nbroad applicability, and user-driven flexibility enables it to serve as a\nuniversal text embedding solution.",
    "translation": "标题：Llama-Embed-Nemotron-8B：面向多语言与跨语言任务的通用文本嵌入模型\n\n摘要：本文推出llama-embed-nemotron-8b——一个开放权重的文本嵌入模型，截至2025年10月21日，该模型在多语言海量文本嵌入基准（MMTEB）排行榜上实现了最先进的性能。尽管现有模型展现出强劲表现，但其训练数据与方法论往往未完全公开。我们通过开发完全开源的模型来解决这一问题，不仅公开其权重参数与详尽的消融实验，还计划发布经系统整理的训练数据集。该模型在所有主流嵌入任务（包括检索、分类与语义文本相似度STS）中均表现出卓越性能，并在低资源语言与跨语言设置等具有挑战性的多语言场景中表现尤为突出。这一突破性性能得益于我们创新的数据组合策略：使用1,610万组查询-文档对，其中770万样本来自公共数据集，840万样本通过各类开放权重大语言模型合成生成。我们的核心贡献包括对关键设计选择的系统化消融研究：对比不同对比损失函数的实现方案、评估合成数据生成策略的效能，以及分析模型融合技术的影响。llama-embed-nemotron-8b作为指令感知模型，支持用户自定义指令以增强特定场景下的性能。这种顶尖的性能表现、广泛的适用性与用户可定制的灵活性相结合，使其成为通用文本嵌入解决方案的理想选择。",
    "url": "https://huggingface.co/papers/2511.07025",
    "arxiv_url": "https://arxiv.org/abs/2511.07025"
  },
  {
    "title": "FLEX: Continuous Agent Evolution via Forward Learning from Experience",
    "summary": "Autonomous agents driven by Large Language Models (LLMs) have revolutionized\nreasoning and problem-solving but remain static after training, unable to grow\nwith experience as intelligent beings do during deployment. We introduce\nForward Learning with EXperience (FLEX), a gradient-free learning paradigm that\nenables LLM agents to continuously evolve through accumulated experience.\nSpecifically, FLEX cultivates scalable and inheritable evolution by\nconstructing a structured experience library through continual reflection on\nsuccesses and failures during interaction with the environment. FLEX delivers\nsubstantial improvements on mathematical reasoning, chemical retrosynthesis,\nand protein fitness prediction (up to 23% on AIME25, 10% on USPTO50k, and 14%\non ProteinGym). We further identify a clear scaling law of experiential growth\nand the phenomenon of experience inheritance across agents, marking a step\ntoward scalable and inheritable continuous agent evolution. Project Page:\nhttps://flex-gensi-thuair.github.io.",
    "translation": "标题：FLEX：基于经验前向学习的智能体持续进化框架\n\n摘要：基于大语言模型的自主智能体在推理与问题解决领域引发革命性突破，但其在训练完成后即处于静态，无法像智能生物那样在部署过程中通过经验积累实现持续成长。本文提出经验前向学习框架（FLEX），该无需梯度的学习范式使大语言模型智能体能够通过积累的经验实现持续进化。具体而言，FLEX通过持续反思与环境交互过程中的成功与失败，构建结构化经验库，从而实现可扩展、可传承的智能体进化。在数学推理、化学逆合成及蛋白质适应性预测任务中，FLEX分别取得显著提升（AIME25数据集提升23%，USPTO50k数据集提升10%，ProteinGym数据集提升14%）。研究进一步揭示了经验增长的显著缩放规律及跨智能体经验传承现象，标志着向可扩展、可传承的持续智能体进化迈出关键一步。项目页面：https://flex-gensi-thuair.github.io。",
    "url": "https://huggingface.co/papers/2511.06449",
    "arxiv_url": "https://arxiv.org/abs/2511.06449"
  },
  {
    "title": "10 Open Challenges Steering the Future of Vision-Language-Action Models",
    "summary": "Due to their ability of follow natural language instructions,\nvision-language-action (VLA) models are increasingly prevalent in the embodied\nAI arena, following the widespread success of their precursors -- LLMs and\nVLMs. In this paper, we discuss 10 principal milestones in the ongoing\ndevelopment of VLA models -- multimodality, reasoning, data, evaluation,\ncross-robot action generalization, efficiency, whole-body coordination, safety,\nagents, and coordination with humans. Furthermore, we discuss the emerging\ntrends of using spatial understanding, modeling world dynamics, post training,\nand data synthesis -- all aiming to reach these milestones. Through these\ndiscussions, we hope to bring attention to the research avenues that may\naccelerate the development of VLA models into wider acceptability.",
    "translation": "标题：引领视觉-语言-动作模型未来发展的十大开放挑战\n\n摘要：凭借其遵循自然语言指令的能力，视觉-语言-动作模型在具身人工智能领域日益普及，这得益于其前身——大语言模型和视觉语言模型取得的广泛成功。本文系统论述了VLA模型发展过程中的十大关键挑战：多模态融合、推理能力、数据构建、评估体系、跨机器人动作泛化、模型效率、全身协调、安全机制、智能体架构以及人机协同。同时，我们深入探讨了为实现这些突破性进展而涌现的研究趋势，包括空间理解、世界动态建模、后训练优化以及数据合成等关键技术路径。通过这些讨论，我们希望引导研究界关注那些可能加速VLA模型获得更广泛认可的关键研究方向。",
    "url": "https://huggingface.co/papers/2511.05936",
    "arxiv_url": "https://arxiv.org/abs/2511.05936"
  },
  {
    "title": "Ariadne: A Controllable Framework for Probing and Extending VLM\n  Reasoning Boundaries",
    "summary": "While Vision-Language Models (VLMs) post-trained with Reinforcement Learning\n(RL) show impressive general reasoning, their evaluation is often confined to\nlanguage-dominant tasks (e.g., math). This raises a critical question: can RL\npost-training truly extend the inherent capability boundary of a base VLM,\nparticularly for visual-centric spatial tasks where it initially fails? To\ninvestigate this, we introduce Ariadne, a framework utilizing synthetic mazes\nfor multi-step spatial reasoning where task difficulty (e.g., path length,\nturns) is precisely controlled. We leverage this controllable environment to\ntrain VLMs using Reinforcement Learning with Verified Rewards (RLVR) in a\ndifficulty-aware curriculum. Surprisingly, post-RLVR training, the VLM achieves\nover 50% accuracy on a problem set where the base model scored 0%,\ndemonstrating that our approach expands the model's initial capability\nboundary. To assess real-world viability, we evaluate out-of-distribution (OOD)\ngeneralization on practical benchmarks. Despite training only on synthetic maze\nsamples, Ariadne achieves significant zero-shot improvements, averaging 16% on\nMapBench (e.g., museum navigation) and 24% on ReasonMap (subway transfer\ntasks). These results confirm that our method not only broadens the model's\nfundamental limits but also enhances its generalization to real-world spatial\nreasoning. We acknowledge our study is limited to the post-training phase,\ngiven the opaqueness of pre-training data, and hope our research motivates\nfurther work on specialized, capability-extending alignment.",
    "translation": "标题：Ariadne：一个用于探索与拓展视觉语言模型推理边界的可控框架\n\n摘要：尽管经过强化学习（RL）后训练的视觉语言模型（VLM）展现出令人印象深刻的通用推理能力，但其评估通常局限于语言主导型任务（如数学推理）。这引发了一个关键问题：对于基础VLM最初无法解决的视觉中心型空间任务，RL后训练是否真能拓展其固有能力边界？为探究此问题，我们提出Ariadne框架——通过可精确控制任务难度（如路径长度、转弯次数）的合成迷宫系统进行多步空间推理研究。我们利用这一可控环境，采用带验证奖励的强化学习（RLVR）在难度感知课程中训练VLM。令人惊讶的是，经过RLVR后训练的VLM在基础模型准确率为0%的问题集上实现了超过50%的准确率，证明我们的方法拓展了模型的初始能力边界。为评估实际应用潜力，我们在实践基准测试中评估了分布外（OOD）泛化能力。尽管仅使用合成迷宫样本进行训练，Ariadne在MapBench（如博物馆导航）和ReasonMap（地铁换乘任务）上分别实现了16%和24%的平均零样本性能提升。这些结果证实我们的方法不仅拓宽了模型的基础能力边界，还增强了其在现实世界空间推理任务中的泛化能力。我们承认由于预训练数据的不透明性，本研究仅限于后训练阶段的研究，期待我们的工作能推动面向专业能力拓展的对齐方法进一步探索。",
    "url": "https://huggingface.co/papers/2511.00710",
    "arxiv_url": "https://arxiv.org/abs/2511.00710"
  },
  {
    "title": "DIMO: Diverse 3D Motion Generation for Arbitrary Objects",
    "summary": "We present DIMO, a generative approach capable of generating diverse 3D\nmotions for arbitrary objects from a single image. The core idea of our work is\nto leverage the rich priors in well-trained video models to extract the common\nmotion patterns and then embed them into a shared low-dimensional latent space.\nSpecifically, we first generate multiple videos of the same object with diverse\nmotions. We then embed each motion into a latent vector and train a shared\nmotion decoder to learn the distribution of motions represented by a structured\nand compact motion representation, i.e., neural key point trajectories. The\ncanonical 3D Gaussians are then driven by these key points and fused to model\nthe geometry and appearance. During inference time with learned latent space,\nwe can instantly sample diverse 3D motions in a single-forward pass and support\nseveral interesting applications including 3D motion interpolation and\nlanguage-guided motion generation. Our project page is available at\nhttps://linzhanm.github.io/dimo.",
    "translation": "标题：DIMO：面向任意物体的多样化三维运动生成方法\n\n摘要：本文提出DIMO生成式方法，能够基于单张图像为任意物体生成多样化的三维运动。本方法的核心思想是利用训练成熟的视频模型中蕴含的丰富先验知识，提取通用运动模式并将其嵌入至共享的低维潜空间。具体而言，我们首先生成具有多样化运动的同一物体的多段视频，随后将每种运动嵌入至潜向量，并通过训练共享运动解码器来学习以结构化紧凑运动表征（即神经关键点轨迹）所描述的运动分布。这些规范化的三维高斯模型随后由关键点驱动并进行融合，以建模几何形态与外观特征。在基于已学习潜空间进行推理时，我们可通过单次前向传播实时采样多样化三维运动，并支持三维运动插值与语言引导运动生成等多项创新应用。项目页面详见：https://linzhanm.github.io/dimo。",
    "url": "https://huggingface.co/papers/2511.07409",
    "arxiv_url": "https://arxiv.org/abs/2511.07409"
  },
  {
    "title": "RLVE: Scaling Up Reinforcement Learning for Language Models with\n  Adaptive Verifiable Environments",
    "summary": "We introduce Reinforcement Learning (RL) with Adaptive Verifiable\nEnvironments (RLVE), an approach using verifiable environments that\nprocedurally generate problems and provide algorithmically verifiable rewards,\nto scale up RL for language models (LMs). RLVE enables each verifiable\nenvironment to dynamically adapt its problem difficulty distribution to the\npolicy model's capabilities as training progresses. In contrast, static data\ndistributions often lead to vanishing learning signals when problems are either\ntoo easy or too hard for the policy. To implement RLVE, we create RLVE-Gym, a\nlarge-scale suite of 400 verifiable environments carefully developed through\nmanual environment engineering. Using RLVE-Gym, we show that environment\nscaling, i.e., expanding the collection of training environments, consistently\nimproves generalizable reasoning capabilities. RLVE with joint training across\nall 400 environments in RLVE-Gym yields a 3.37% absolute average improvement\nacross six reasoning benchmarks, starting from one of the strongest 1.5B\nreasoning LMs. By comparison, continuing this LM's original RL training yields\nonly a 0.49% average absolute gain despite using over 3x more compute. We\nrelease our code publicly.",
    "translation": "标题：RLVE：基于可验证自适应环境的大规模语言模型强化学习方法  \n\n摘要：本文提出基于可验证自适应环境的强化学习方法（RLVE），该方法通过可验证环境程序化生成问题并提供算法可验证的奖励机制，以实现语言模型强化学习的规模化扩展。RLVE使每个可验证环境能够根据策略模型在训练过程中的能力水平，动态调整问题难度分布。相比之下，静态数据分布在问题过于简单或困难时往往会导致学习信号消失。为实施RLVE，我们开发了RLVE-Gym——一个通过人工环境工程精心构建的、包含400个可验证环境的大规模训练套件。基于RLVE-Gym的实验表明，环境扩展（即增加训练环境集合）能持续提升模型的泛化推理能力。使用RLVE-Gym全部400个环境进行联合训练时，RLVE在六大推理基准测试中实现了3.37%的平均绝对提升（以当前最强的15亿参数推理语言模型为基线）。相比之下，延续该模型原有强化训练方案仅获得0.49%的平均提升，且消耗了3倍以上的算力。我们已公开相关代码。",
    "url": "https://huggingface.co/papers/2511.07317",
    "arxiv_url": "https://arxiv.org/abs/2511.07317"
  },
  {
    "title": "Do LLMs Feel? Teaching Emotion Recognition with Prompts, Retrieval, and\n  Curriculum Learning",
    "summary": "Emotion Recognition in Conversation (ERC) is a crucial task for understanding\nhuman emotions and enabling natural human-computer interaction. Although Large\nLanguage Models (LLMs) have recently shown great potential in this field, their\nability to capture the intrinsic connections between explicit and implicit\nemotions remains limited. We propose a novel ERC training framework, PRC-Emo,\nwhich integrates Prompt engineering, demonstration Retrieval, and Curriculum\nlearning, with the goal of exploring whether LLMs can effectively perceive\nemotions in conversational contexts. Specifically, we design emotion-sensitive\nprompt templates based on both explicit and implicit emotional cues to better\nguide the model in understanding the speaker's psychological states. We\nconstruct the first dedicated demonstration retrieval repository for ERC, which\nincludes training samples from widely used datasets, as well as high-quality\ndialogue examples generated by LLMs and manually verified. Moreover, we\nintroduce a curriculum learning strategy into the LoRA fine-tuning process,\nincorporating weighted emotional shifts between same-speaker and\ndifferent-speaker utterances to assign difficulty levels to dialogue samples,\nwhich are then organized in an easy-to-hard training sequence. Experimental\nresults on two benchmark datasets-- IEMOCAP and MELD --show that our method\nachieves new state-of-the-art (SOTA) performance, demonstrating the\neffectiveness and generalizability of our approach in improving LLM-based\nemotional understanding.",
    "translation": "标题：大语言模型具备情感感知能力吗？基于提示工程、检索机制与课程学习的情绪识别研究\n\n摘要：对话情绪识别（ERC）是理解人类情感并实现自然人机交互的关键任务。尽管大语言模型（LLMs）近期在该领域展现出巨大潜力，但其捕捉显性情绪与隐性情绪内在联系的能力仍存在局限。我们提出名为PRC-Emo的创新ERC训练框架，该框架融合提示工程、示例检索与课程学习三大模块，旨在探究LLMs能否有效感知对话情境中的情绪状态。具体而言，我们基于显性与隐性情绪线索设计情感敏感型提示模板，以更精准引导模型理解说话者的心理状态。构建了ERC领域首个专用示例检索库，包含来自广泛使用数据集的训练样本，以及由LLMs生成并经人工核验的高质量对话实例。此外，我们将课程学习策略引入LoRA微调过程，通过量化同一说话者与不同说话者话语间的加权情绪转移值来划分对话样本难度等级，并依此构建由易到难的训练序列。在IEMOCAP和MELD两个基准数据集上的实验结果表明，本方法取得了最新的最优性能，验证了所提框架在增强基于LLM的情绪理解能力方面的有效性与泛化性。",
    "url": "https://huggingface.co/papers/2511.07061",
    "arxiv_url": "https://arxiv.org/abs/2511.07061"
  },
  {
    "title": "SWE-fficiency: Can Language Models Optimize Real-World Repositories on\n  Real Workloads?",
    "summary": "Optimizing the performance of large-scale software repositories demands\nexpertise in code reasoning and software engineering (SWE) to reduce runtime\nwhile preserving program correctness. However, most benchmarks emphasize what\nto fix rather than how to fix code. We introduce SWE-fficiency, a\nbenchmark for evaluating repository-level performance optimization on real\nworkloads. Our suite contains 498 tasks across nine widely used data-science,\nmachine-learning, and HPC repositories (e.g., numpy, pandas, scipy): given a\ncomplete codebase and a slow workload, an agent must investigate code\nsemantics, localize bottlenecks and relevant tests, and produce a patch that\nmatches or exceeds expert speedup while passing the same unit tests. To enable\nthis how-to-fix evaluation, our automated pipeline scrapes GitHub pull requests\nfor performance-improving edits, combining keyword filtering, static analysis,\ncoverage tooling, and execution validation to both confirm expert speedup\nbaselines and identify relevant repository unit tests. Empirical evaluation of\nstate-of-the-art agents reveals significant underperformance. On average,\nagents achieve less than 0.15x the expert speedup: agents struggle in\nlocalizing optimization opportunities, reasoning about execution across\nfunctions, and maintaining correctness in proposed edits. We release the\nbenchmark and accompanying data pipeline to facilitate research on automated\nperformance engineering and long-horizon software reasoning.",
    "translation": "标题：SWE-fficiency：语言模型能否在真实工作负载下优化现实代码仓库？\n\n摘要：优化大型软件仓库的性能需要代码推理与软件工程（SWE）领域的专业知识，在保证程序正确性的同时降低运行耗时。然而现有基准测试多聚焦于“修复目标”而非“修复方法”。本文提出SWE-fficiency——首个面向真实工作负载的仓库级性能优化评估基准。该测试套件涵盖九个广泛使用的数据科学、机器学习及高性能计算仓库（如numpy、pandas、scipy）中的498项任务：给定完整代码库与低速工作负载，智能体需解析代码语义、定位性能瓶颈及相关测试，并生成能通过同等单元测试且加速效果达到或超越专家水平的补丁。为实现这种“如何修复”的评估，我们通过自动化流水线从GitHub拉取请求中采集性能优化编辑记录，结合关键词过滤、静态分析、覆盖率工具与执行验证，既确认专家加速基准又识别相关仓库单元测试。对前沿智能体的实证评估显示其表现显著欠佳：平均加速效果仅达专家水平的0.15倍。智能体在定位优化机会、跨函数执行推理及保持编辑正确性方面存在明显不足。我们公开此基准测试及配套数据流水线，以推动自动化性能工程与长周期软件推理的研究进展。",
    "url": "https://huggingface.co/papers/2511.06090",
    "arxiv_url": "https://arxiv.org/abs/2511.06090"
  },
  {
    "title": "Diffusion-SDPO: Safeguarded Direct Preference Optimization for Diffusion\n  Models",
    "summary": "Text-to-image diffusion models deliver high-quality images, yet aligning them\nwith human preferences remains challenging. We revisit diffusion-based Direct\nPreference Optimization (DPO) for these models and identify a critical\npathology: enlarging the preference margin does not necessarily improve\ngeneration quality. In particular, the standard Diffusion-DPO objective can\nincrease the reconstruction error of both winner and loser branches.\nConsequently, degradation of the less-preferred outputs can become sufficiently\nsevere that the preferred branch is also adversely affected even as the margin\ngrows. To address this, we introduce Diffusion-SDPO, a safeguarded update rule\nthat preserves the winner by adaptively scaling the loser gradient according to\nits alignment with the winner gradient. A first-order analysis yields a\nclosed-form scaling coefficient that guarantees the error of the preferred\noutput is non-increasing at each optimization step. Our method is simple,\nmodel-agnostic, broadly compatible with existing DPO-style alignment frameworks\nand adds only marginal computational overhead. Across standard text-to-image\nbenchmarks, Diffusion-SDPO delivers consistent gains over preference-learning\nbaselines on automated preference, aesthetic, and prompt alignment metrics.\nCode is publicly available at https://github.com/AIDC-AI/Diffusion-SDPO.",
    "translation": "标题：Diffusion-SDPO：扩散模型的带保护直接偏好优化方法\n\n摘要：文本到图像扩散模型虽能生成高质量图像，但其与人类偏好的对齐仍具挑战性。本文重新审视基于扩散的直接偏好优化方法，发现一个关键缺陷：扩大偏好间隔未必能提升生成质量。具体而言，标准Diffusion-DPO目标函数可能同时增加优胜分支与劣汰分支的重建误差。这会导致非优选输出的退化程度加剧，即使偏好间隔扩大，优选分支也会受到不利影响。为此，我们提出Diffusion-SDPO方法，该保护性更新规则通过自适应缩放劣汰分支梯度与其优胜分支梯度的对齐程度来保持优胜分支质量。一阶分析推导出的闭式缩放系数可确保在每步优化中优选输出的误差保持非递增。本方法具有简洁性、模型无关性，可广泛兼容现有DPO式对齐框架，且仅增加边际计算开销。在标准文本到图像基准测试中，Diffusion-SDPO在自动化偏好评估、美学质量和提示对齐指标上均持续优于现有偏好学习基线。代码公开于：https://github.com/AIDC-AI/Diffusion-SDPO。",
    "url": "https://huggingface.co/papers/2511.03317",
    "arxiv_url": "https://arxiv.org/abs/2511.03317"
  },
  {
    "title": "Generating an Image From 1,000 Words: Enhancing Text-to-Image With\n  Structured Captions",
    "summary": "Text-to-image models have rapidly evolved from casual creative tools to\nprofessional-grade systems, achieving unprecedented levels of image quality and\nrealism. Yet, most models are trained to map short prompts into detailed\nimages, creating a gap between sparse textual input and rich visual outputs.\nThis mismatch reduces controllability, as models often fill in missing details\narbitrarily, biasing toward average user preferences and limiting precision for\nprofessional use. We address this limitation by training the first open-source\ntext-to-image model on long structured captions, where every training sample is\nannotated with the same set of fine-grained attributes. This design maximizes\nexpressive coverage and enables disentangled control over visual factors. To\nprocess long captions efficiently, we propose DimFusion, a fusion mechanism\nthat integrates intermediate tokens from a lightweight LLM without increasing\ntoken length. We also introduce the Text-as-a-Bottleneck Reconstruction (TaBR)\nevaluation protocol. By assessing how well real images can be reconstructed\nthrough a captioning-generation loop, TaBR directly measures controllability\nand expressiveness, even for very long captions where existing evaluation\nmethods fail. Finally, we demonstrate our contributions by training the\nlarge-scale model FIBO, achieving state-of-the-art prompt alignment among\nopen-source models. Model weights are publicly available at\nhttps://huggingface.co/briaai/FIBO",
    "translation": "标题：千词成图：基于结构化描述增强文本到图像生成技术\n\n摘要：文本到图像模型已从随意的创意工具迅速发展为专业级系统，实现了前所未有的图像质量与真实感。然而，大多数模型被训练用于将简短提示映射为精细图像，导致稀疏文本输入与丰富视觉输出之间产生鸿沟。这种不匹配降低了可控性——模型常会随意填补缺失细节，偏向普通用户偏好，限制了专业应用的精确度。为解决这一局限，我们首次基于长结构化描述训练开源文本到图像模型，每个训练样本均通过统一细粒度属性集进行标注。该设计最大化表达覆盖范围，并实现视觉要素的解耦控制。为高效处理长文本，我们提出DimFusion融合机制，在不增加标记长度的前提下整合轻量级大语言模型的中间标记。同时引入文本瓶颈重建评估协议：通过评估真实图像在标注-生成循环中的重建质量，该协议可直接衡量可控性与表达能力，即使在现有评估方法失效的超长文本场景下仍能适用。最终，我们通过训练大规模模型FIBO验证研究成果，在开源模型中实现了最先进的提示对齐效果。模型权重已公开发布于https://huggingface.co/briaai/FIBO",
    "url": "https://huggingface.co/papers/2511.06876",
    "arxiv_url": "https://arxiv.org/abs/2511.06876"
  },
  {
    "title": "VADER: Towards Causal Video Anomaly Understanding with Relation-Aware\n  Large Language Models",
    "summary": "Video anomaly understanding (VAU) aims to provide detailed interpretation and\nsemantic comprehension of anomalous events within videos, addressing\nlimitations of traditional methods that focus solely on detecting and\nlocalizing anomalies. However, existing approaches often neglect the deeper\ncausal relationships and interactions between objects, which are critical for\nunderstanding anomalous behaviors. In this paper, we propose VADER, an\nLLM-driven framework for Video Anomaly unDErstanding, which integrates keyframe\nobject Relation features with visual cues to enhance anomaly comprehension from\nvideo. Specifically, VADER first applies an Anomaly Scorer to assign per-frame\nanomaly scores, followed by a Context-AwarE Sampling (CAES) strategy to capture\nthe causal context of each anomalous event. A Relation Feature Extractor and a\nCOntrastive Relation Encoder (CORE) jointly model dynamic object interactions,\nproducing compact relational representations for downstream reasoning. These\nvisual and relational cues are integrated with LLMs to generate detailed,\ncausally grounded descriptions and support robust anomaly-related question\nanswering. Experiments on multiple real-world VAU benchmarks demonstrate that\nVADER achieves strong results across anomaly description, explanation, and\ncausal reasoning tasks, advancing the frontier of explainable video anomaly\nanalysis.",
    "translation": "标题：VADER：基于关系感知大语言模型的因果视频异常理解框架\n\n摘要：视频异常理解旨在对视频中的异常事件提供精细化阐释与语义级认知，突破传统方法仅聚焦异常检测与定位的局限。然而现有研究往往忽视物体间深层的因果关系与动态交互，而这些要素对理解异常行为至关重要。本文提出VADER——一种基于大语言模型的视频异常理解框架，通过融合关键帧物体关系特征与视觉线索来增强视频异常认知能力。具体而言，VADER首先通过异常评分器计算逐帧异常分值，继而采用情境感知采样策略捕捉异常事件的因果上下文。通过关系特征提取器与对比关系编码器的协同作用，系统建模动态物体交互并生成紧凑的关系表征以支持下游推理。这些视觉与关系线索与大语言模型集成后，可生成具有因果依据的细粒度描述，并支持鲁棒的异常相关问答。在多个真实场景视频异常理解基准测试上的实验表明，VADER在异常描述、解释及因果推理任务中均取得优异性能，推动了可解释视频异常分析研究的前沿发展。",
    "url": "https://huggingface.co/papers/2511.07299",
    "arxiv_url": "https://arxiv.org/abs/2511.07299"
  },
  {
    "title": "Omni-AVSR: Towards Unified Multimodal Speech Recognition with Large\n  Language Models",
    "summary": "Large language models (LLMs) have recently achieved impressive results in\nspeech recognition across multiple modalities, including Auditory Speech\nRecognition (ASR), Visual Speech Recognition (VSR), and Audio-Visual Speech\nRecognition (AVSR). Despite this progress, current LLM-based approaches\ntypically address each task independently, training separate models that raise\ncomputational and deployment resource use while missing potential cross-task\nsynergies. They also rely on fixed-rate token compression, which restricts\nflexibility in balancing accuracy with efficiency. These limitations highlight\nthe need for a unified framework that can support ASR, VSR, and AVSR while\nenabling elastic inference. To this end, we present Omni-AVSR, a unified\naudio-visual LLM that combines efficient multi-granularity training with\nparameter-efficient adaptation. Specifically, we adapt the matryoshka\nrepresentation learning paradigm to efficiently train across multiple audio and\nvisual granularities, reducing its inherent training resource use. Furthermore,\nwe explore three LoRA-based strategies for adapting the backbone LLM, balancing\nshared and task-specific specialization. Experiments on LRS2 and LRS3 show that\nOmni-AVSR achieves comparable or superior accuracy to state-of-the-art\nbaselines while training a single model at substantially lower training and\ndeployment resource use. The model also remains robust under acoustic noise,\nand we analyze its scaling behavior as LLM size increases, providing insights\ninto the trade-off between performance and efficiency.",
    "translation": "标题：Omni-AVSR：基于大语言模型构建统一多模态语音识别系统\n\n摘要：大语言模型近期在听觉语音识别、视觉语音识别与音视频语音识别等多模态语音识别任务中取得显著成果。然而现有基于大语言模型的方法通常独立处理各项任务，需训练独立模型导致计算与部署资源消耗增加，且未能充分利用跨任务协同潜力。这些方法还依赖固定速率的分词压缩机制，限制了精度与效率平衡的灵活性。这些局限凸显了构建统一框架的必要性——既能支持多模态语音识别任务，又可实现弹性推理。为此，我们提出Omni-AVSR这一统一音视频大语言模型，通过高效多粒度训练与参数有效性自适应相结合的方法实现突破。具体而言，我们采用套娃表示学习范式实现多粒度音视频数据的高效训练，显著降低固有训练资源消耗。此外，我们探索三种基于LoRA的骨干网络自适应策略，在共享性与任务特异性之间实现最优平衡。在LRS2和LRS3数据集上的实验表明，Omni-AVSR仅需训练单一模型即可达到或超越现有最优基准模型精度，同时大幅降低训练与部署资源消耗。该模型在噪声环境下仍保持稳健性能，我们通过分析模型规模扩展规律，为性能与效率的权衡提供了重要见解。",
    "url": "https://huggingface.co/papers/2511.07253",
    "arxiv_url": "https://arxiv.org/abs/2511.07253"
  },
  {
    "title": "LUT-LLM: Efficient Large Language Model Inference with Memory-based\n  Computations on FPGAs",
    "summary": "The rapid progress of large language models (LLMs) has advanced numerous\napplications, yet efficient single-batch inference remains vital for on-device\nintelligence. While FPGAs offer fine-grained data control and high energy\nefficiency, recent GPU optimizations have narrowed their advantage, especially\nunder arithmetic-based computation. To overcome this, we leverage FPGAs'\nabundant on-chip memory to shift LLM inference from arithmetic- to memory-based\ncomputation through table lookups. We present LUT-LLM, the first FPGA\naccelerator enabling 1B+ LLM inference via vector-quantized memory operations.\nOur analysis identifies activation-weight co-quantization as the most effective\nscheme, supported by (1) bandwidth-aware parallel centroid search, (2)\nefficient 2D table lookups, and (3) a spatial-temporal hybrid design minimizing\ndata caching. Implemented on an AMD V80 FPGA for a customized Qwen 3 1.7B\nmodel, LUT-LLM achieves 1.66x lower latency than AMD MI210 and 1.72x higher\nenergy efficiency than NVIDIA A100, scaling to 32B models with 2.16x efficiency\ngain over A100.",
    "translation": "标题：LUT-LLM：基于FPGA内存计算的高效大语言模型推理方案\n\n摘要：大语言模型的快速发展推动了众多应用场景的进步，然而高效的单一批次推理对于设备端智能仍然至关重要。尽管FPGA具备细粒度数据控制和高能效的特性，但近期GPU优化已缩小了其优势，尤其在基于算术运算的场景下。为突破此限制，我们利用FPGA丰富的片上存储资源，通过查表操作将LLM推理从算术计算转向内存计算。本文提出LUT-LLM——首个通过向量化内存操作实现十亿参数级LLM推理的FPGA加速器。我们的分析表明激活-权重协同量化是最有效的方案，其技术支撑包括：（1）带宽感知并行质心搜索；（2）高效二维查表机制；（3）最小化数据缓存的时空混合架构。在AMD V80 FPGA平台上对定制化Qwen 3 1.7B模型的实测表明，LUT-LLM相较AMD MI210实现延迟降低1.66倍，相比NVIDIA A100能效提升1.72倍，并可扩展至320亿参数模型，较A100实现2.16倍的能效增益。",
    "url": "https://huggingface.co/papers/2511.06174",
    "arxiv_url": "https://arxiv.org/abs/2511.06174"
  },
  {
    "title": "Reinforcement Learning Improves Traversal of Hierarchical Knowledge in LLMs",
    "summary": "Reinforcement learning (RL) is often credited with improving language model reasoning and generalization at the expense of degrading memorized knowledge. We challenge this narrative by observing that RL-enhanced models consistently outperform their base and supervised fine-tuned (SFT) counterparts on pure knowledge recall tasks, particularly those requiring traversal of hierarchical, structured knowledge (e.g., medical codes). We hypothesize these gains stem not from newly acquired data, but from improved procedural skills in navigating and searching existing knowledge hierarchies within the model parameters. To support this hypothesis, we show that structured prompting, which explicitly guides SFTed models through hierarchical traversal, recovers most of the performance gap (reducing 24pp to 7pp on MedConceptsQA for DeepSeek-V3/R1). We further find that while prompting improves final-answer accuracy, RL-enhanced models retain superior ability to recall correct procedural paths on deep-retrieval tasks. Finally our layer-wise internal activation analysis reveals that while factual representations (e.g., activations for the statement \"code 57.95 refers to urinary infection\") maintain high cosine similarity between SFT and RL models, query representations (e.g., \"what is code 57.95\") diverge noticeably, indicating that RL primarily transforms how models traverse knowledge rather than the knowledge representation itself.",
    "translation": "标题：强化学习提升大语言模型对层次化知识的遍历能力\n\n摘要：传统观点认为强化学习（RL）虽能提升语言模型的推理与泛化能力，却会削弱其记忆知识。我们通过实验发现，在纯粹的知识召回任务中——尤其是需要遍历层次化结构化知识（如医疗编码）的任务——经过RL增强的模型持续优于基础模型及监督微调（SFT）模型，从而对这一观点提出挑战。我们推测这种提升并非源于新获取的数据，而是源于模型在参数空间内导航和检索既有知识层次结构的流程性技能得到增强。为验证该假设，我们证明通过结构化提示显式引导SFT模型进行层次遍历后，性能差距可大幅缩小（在MedConceptsQA数据集上，DeepSeek-V3/R1模型的差距从24个百分点降至7个百分点）。进一步研究发现，虽然提示策略能提升最终答案准确率，但RL增强模型在深度检索任务中仍保持更优的正确流程路径召回能力。最后，我们通过分层内部激活分析发现：尽管事实性表征（如“编码57.95指代尿路感染”的激活状态）在SFT与RL模型间保持较高的余弦相似度，但查询表征（如“编码57.95是什么”）却呈现显著差异，这表明RL主要改变的是模型遍历知识的方式，而非知识表征本身。",
    "url": "https://huggingface.co/papers/2511.05933",
    "arxiv_url": "https://arxiv.org/abs/2511.05933"
  }
]