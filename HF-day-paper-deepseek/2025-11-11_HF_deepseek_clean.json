[
  {
    "title": "HaluMem: Evaluating Hallucinations in Memory Systems of Agents",
    "summary": "Memory systems are key components that enable AI systems such as LLMs and AI\nagents to achieve long-term learning and sustained interaction. However, during\nmemory storage and retrieval, these systems frequently exhibit memory\nhallucinations, including fabrication, errors, conflicts, and omissions.\nExisting evaluations of memory hallucinations are primarily end-to-end question\nanswering, which makes it difficult to localize the operational stage within\nthe memory system where hallucinations arise. To address this, we introduce the\nHallucination in Memory Benchmark (HaluMem), the first operation level\nhallucination evaluation benchmark tailored to memory systems. HaluMem defines\nthree evaluation tasks (memory extraction, memory updating, and memory question\nanswering) to comprehensively reveal hallucination behaviors across different\noperational stages of interaction. To support evaluation, we construct\nuser-centric, multi-turn human-AI interaction datasets, HaluMem-Medium and\nHaluMem-Long. Both include about 15k memory points and 3.5k multi-type\nquestions. The average dialogue length per user reaches 1.5k and 2.6k turns,\nwith context lengths exceeding 1M tokens, enabling evaluation of hallucinations\nacross different context scales and task complexities. Empirical studies based\non HaluMem show that existing memory systems tend to generate and accumulate\nhallucinations during the extraction and updating stages, which subsequently\npropagate errors to the question answering stage. Future research should focus\non developing interpretable and constrained memory operation mechanisms that\nsystematically suppress hallucinations and improve memory reliability.",
    "translation": "标题：HaluMem：智能体记忆系统中的幻觉效应评估\n\n摘要：记忆系统是实现大语言模型与智能体长期学习及持续交互的核心组件。然而在记忆存储与检索过程中，这些系统常出现记忆幻觉现象，具体表现为虚构、错漏、冲突和缺失等问题。现有对记忆幻觉的评估主要采用端到端问答形式，难以定位幻觉产生的具体操作环节。为此，我们提出首个面向记忆系统的操作级幻觉评估基准HaluMem，通过定义记忆提取、记忆更新和记忆问答三项评估任务，系统揭示交互过程中不同操作阶段的幻觉行为。为支撑评估，我们构建了以用户为中心的多轮人机交互数据集HaluMem-Medium与HaluMem-Long，两者共包含约1.5万个记忆节点和3500道多类型问题，单用户平均对话轮次达1500轮与2600轮，上下文长度超100万token，可实现不同语境规模与任务复杂度下的幻觉评估。基于HaluMem的实证研究表明，现有记忆系统在提取与更新阶段易产生并积累幻觉，进而将误差传导至问答阶段。未来研究应致力于开发具有可解释性的约束化记忆操作机制，系统抑制幻觉并提升记忆可靠性。",
    "url": "https://huggingface.co/papers/2511.03506",
    "arxiv_url": "https://arxiv.org/abs/2511.03506"
  },
  {
    "title": "IterResearch: Rethinking Long-Horizon Agents via Markovian State\n  Reconstruction",
    "summary": "Recent advances in deep-research agents have shown promise for autonomous\nknowledge construction through dynamic reasoning over external sources.\nHowever, existing approaches rely on a mono-contextual paradigm that\naccumulates all information in a single, expanding context window, leading to\ncontext suffocation and noise contamination that limit their effectiveness on\nlong-horizon tasks. We introduce IterResearch, a novel iterative deep-research\nparadigm that reformulates long-horizon research as a Markov Decision Process\nwith strategic workspace reconstruction. By maintaining an evolving report as\nmemory and periodically synthesizing insights, our approach preserves\nconsistent reasoning capacity across arbitrary exploration depths. We further\ndevelop Efficiency-Aware Policy Optimization (EAPO), a reinforcement learning\nframework that incentivizes efficient exploration through geometric reward\ndiscounting and enables stable distributed training via adaptive downsampling.\nExtensive experiments demonstrate that IterResearch achieves substantial\nimprovements over existing open-source agents with average +14.5pp across six\nbenchmarks and narrows the gap with frontier proprietary systems. Remarkably,\nour paradigm exhibits unprecedented interaction scaling, extending to 2048\ninteractions with dramatic performance gains (from 3.5\\% to 42.5\\%), and serves\nas an effective prompting strategy, improving frontier models by up to 19.2pp\nover ReAct on long-horizon tasks. These findings position IterResearch as a\nversatile solution for long-horizon reasoning, effective both as a trained\nagent and as a prompting paradigm for frontier models.",
    "translation": "标题：IterResearch：基于马尔可夫状态重构的长视野智能体范式再思考\n\n摘要：深度研究智能体的最新进展通过对外部资源的动态推理，在自主知识构建方面展现出潜力。然而，现有方法依赖单上下文范式，将所有信息累积在持续扩展的上下文窗口中，导致上下文窒息与噪声污染，限制了其在长视野任务中的效能。我们提出IterResearch——一种创新的迭代式深度研究范式，将长视野研究重新定义为具有策略性工作空间重构的马尔可夫决策过程。通过将动态演进的研究报告作为记忆体并定期整合研究洞见，该方法能在任意探索深度下保持稳定的推理能力。我们进一步开发效率感知策略优化（EAPO），该强化学习框架通过几何奖励折现激励高效探索，并借助自适应降采样实现稳定的分布式训练。大量实验表明，IterResearch在六个基准测试中相较现有开源智能体平均提升14.5个百分点，显著缩小了与前沿私有系统的差距。值得注意的是，该范式展现出前所未有的交互扩展能力，可支持高达2048次交互且性能持续提升（从3.5%至42.5%），同时作为有效的提示策略，在长视野任务中较ReAct将前沿模型性能提升最高达19.2个百分点。这些发现确立了IterResearch作为长视野推理的通用解决方案，既可作为训练完成的智能体，也能作为前沿模型的提示范式。",
    "url": "https://huggingface.co/papers/2511.07327",
    "arxiv_url": "https://arxiv.org/abs/2511.07327"
  },
  {
    "title": "DRIVE: Data Curation Best Practices for Reinforcement Learning with\n  Verifiable Reward in Competitive Code Generation",
    "summary": "Recent reasoning-first models (e.g., OpenAI o1, DeepSeek R1) have spurred a\nresurgence of interest in RLVR. Nevertheless, advances are dominated by\nmathematics (e.g., AIME), with competitive-programming code generation\nunderexplored and data curation receiving less attention than RL algorithm\ndesign. We investigate how to construct RLVR datasets (i.e., RL prompts) and\npresent practical training techniques that yield strong performance on\ncompetitive-programming code generation. Our pipeline begins with supervised\nfine-tuning (SFT) distilled from strong open-source models, augmented with\ngeneral-purpose and reasoning-intensive data. RL then follows a two-stage\nprocess with executable, testcase-driven rewards: first, training on a large,\nuniformly distributed set of competitive-programming problems using Group\nRelative Policy Optimization (GRPO) with 8 rollouts per prompt and a relatively\nshort response-generation window (e.g., 32k during SFT and 24k in this stage)\nto expand entropy and mitigate repetition and truncation; second, we perform\nPre-GRPO: updating on a small, high-quality set of challenging\nproblems with a large rollout budget (64 rollouts per prompt) under a\nhard-focus curriculum that continuously retains the most difficult instances\nthroughout training. We implement our method on Qwen2.5-32B and evaluate on\nLeetCode and Codeforces weekly contests to avoid data leakage. The resulting\nmodel achieves state-of-the-art performance among models of similar scale and\nis comparable to leading systems such as DeepSeek v3.1 and Doubao-1.5-Thinking.\nWe also examine scaling trends and observe strong RL scaling on an internal\nlarge-scale MoE model. Our study distills concise best practices for data\ncuration, entropy expansion, and curriculum design in RLVR for\ncompetitive-programming code generation.",
    "translation": "标题：DRIVE：面向竞技性代码生成中可验证奖励强化学习的数据策管最佳实践\n\n摘要：近期以推理为核心的大模型（如OpenAI o1、DeepSeek R1）推动了对可验证奖励强化学习（RLVR）的新一轮研究热潮。然而当前进展主要集中于数学领域（如AIME），竞技性编程代码生成领域探索不足，且数据策管所受关注远少于强化学习算法设计。本研究系统探讨了RLVR数据集（即强化学习提示）的构建方法，并提出在竞技性编程代码生成中实现卓越性能的实用训练技术。我们的流程始于基于强开源模型蒸馏得到的监督微调（SFT），并辅以通用型与推理密集型数据进行增强。强化学习阶段采用基于可执行测试用例的奖励机制，实施两阶段训练方案：首先使用组相对策略优化（GRPO）在大型均匀分布的竞技编程问题集上进行训练，每个提示生成8个推理轨迹，并设置相对较短的响应生成窗口（SFT阶段32K，本阶段24K），以扩大熵值并缓解重复与截断问题；随后执行预GRPO阶段：在精选的小规模高难度问题集上，采用每个提示64个推理轨迹的大规模采样预算，通过贯穿整个训练周期的硬聚焦课程学习策略持续保留最具挑战性的实例。我们在Qwen2.5-32B模型上实施该方法，并在LeetCode和Codeforces周赛上进行评估以避免数据泄露。最终模型在同等规模模型中达到最先进性能，与DeepSeek v3.1、豆包1.5思考版等领先系统表现相当。通过扩展性趋势分析，我们在内部大规模混合专家模型上观察到显著的强化学习扩展效应。本研究凝练出适用于竞技编程代码生成的RLVR数据策管、熵值扩展与课程设计的简明最佳实践。",
    "url": "https://huggingface.co/papers/2511.06307",
    "arxiv_url": "https://arxiv.org/abs/2511.06307"
  },
  {
    "title": "The Station: An Open-World Environment for AI-Driven Discovery",
    "summary": "We introduce the STATION, an open-world multi-agent environment that models a\nminiature scientific ecosystem. Leveraging their extended context windows,\nagents in the Station can engage in long scientific journeys that include\nreading papers from peers, formulating hypotheses, submitting code, performing\nanalyses, and publishing results. Importantly, there is no centralized system\ncoordinating their activities - agents are free to choose their own actions and\ndevelop their own narratives within the Station. Experiments demonstrate that\nAI agents in the Station achieve new state-of-the-art performance on a wide\nrange of benchmarks, spanning from mathematics to computational biology to\nmachine learning, notably surpassing AlphaEvolve in circle packing. A rich\ntapestry of narratives emerges as agents pursue independent research, interact\nwith peers, and build upon a cumulative history. From these emergent\nnarratives, novel methods arise organically, such as a new density-adaptive\nalgorithm for scRNA-seq batch integration. The Station marks a first step\ntowards autonomous scientific discovery driven by emergent behavior in an\nopen-world environment, representing a new paradigm that moves beyond rigid\noptimization.",
    "translation": "标题：《站台：面向人工智能驱动发现的开放世界环境》\n\n摘要：本文提出“站台”（STATION）——一个模拟微型科研生态系统的开放世界多智能体环境。依托扩展上下文窗口，智能体可在站台中进行长期科研探索，包括研读同行论文、提出假设、提交代码、执行分析及发表成果。值得注意的是，该系统不存在中央协调机制——智能体可自主选择行为并在环境中构建独立研究叙事。实验表明，站台中的AI智能体在从数学到计算生物学乃至机器学习的多类基准测试中均达到最新最优性能，尤其在圆填充问题上显著超越AlphaEvolve。当智能体开展自主研究、进行同行互动并沿袭累积历史时，会涌现出丰富的叙事脉络。这些涌现叙事中自然衍生出创新方法，例如用于单细胞RNA测序批次整合的新型密度自适应算法。站台标志着在开放世界环境中通过涌现行为实现自主科学发现的重要突破，代表了超越刚性优化范式的新型研究范式。",
    "url": "https://huggingface.co/papers/2511.06309",
    "arxiv_url": "https://arxiv.org/abs/2511.06309"
  },
  {
    "title": "MVU-Eval: Towards Multi-Video Understanding Evaluation for Multimodal\n  LLMs",
    "summary": "The advent of Multimodal Large Language Models (MLLMs) has expanded AI\ncapabilities to visual modalities, yet existing evaluation benchmarks remain\nlimited to single-video understanding, overlooking the critical need for\nmulti-video understanding in real-world scenarios (e.g., sports analytics and\nautonomous driving). To address this significant gap, we introduce MVU-Eval,\nthe first comprehensive benchmark for evaluating Multi-Video Understanding for\nMLLMs. Specifically, our MVU-Eval mainly assesses eight core competencies\nthrough 1,824 meticulously curated question-answer pairs spanning 4,959 videos\nfrom diverse domains, addressing both fundamental perception tasks and\nhigh-order reasoning tasks. These capabilities are rigorously aligned with\nreal-world applications such as multi-sensor synthesis in autonomous systems\nand cross-angle sports analytics. Through extensive evaluation of\nstate-of-the-art open-source and closed-source models, we reveal significant\nperformance discrepancies and limitations in current MLLMs' ability to perform\nunderstanding across multiple videos. The benchmark will be made publicly\navailable to foster future research.",
    "translation": "标题：MVU-Eval：面向多模态大语言模型的多视频理解评估体系\n\n摘要：多模态大语言模型（MLLMs）的出现扩展了人工智能在视觉模态的能力，然而现有评估基准仍局限于单视频理解，忽视了现实场景（如体育赛事分析与自动驾驶）中对多视频理解的关键需求。为填补这一重要空白，我们推出MVU-Eval——首个面向MLLMs的多视频理解综合评估基准。该基准通过1,824个精心构建的问答对（涵盖4,959个跨领域视频），系统评估八大核心能力，既包含基础感知任务，也涉及高阶推理任务。这些能力指标严格对标自动驾驶系统中的多传感器融合、多视角体育分析等实际应用场景。通过对前沿开源与闭源模型的大规模评估，我们揭示了当前MLLMs在处理多视频理解任务时存在的显著性能差距与局限性。本基准将公开发布以促进后续研究。",
    "url": "https://huggingface.co/papers/2511.07250",
    "arxiv_url": "https://arxiv.org/abs/2511.07250"
  },
  {
    "title": "Routing Manifold Alignment Improves Generalization of Mixture-of-Experts\n  LLMs",
    "summary": "Sparse Mixture-of-Experts (MoE) have been widely adopted in recent large\nlanguage models since it can efficiently scale up the model capability without\nincreasing the inference cost. However, evaluations on broad downstream tasks\nreveal a consistent suboptimality of the routers in existing MoE LLMs, which\nresults in a severe performance gap (e.g., 10-20% in accuracy) to the optimal\nrouting. In this paper, we show that aligning the manifold of routing weights\nwith that of task embedding can effectively reduce the gap and improve MoE\nLLMs' generalization performance. Our method, \"Routing Manifold Alignment\n(RoMA)\", introduces an additional manifold regularization term in the\npost-training objective and only requires lightweight finetuning of routers\n(with other parameters frozen). Specifically, the regularization encourages the\nrouting weights of each sample to be close to those of its successful neighbors\n(whose routing weights lead to correct answers) in a task embedding space.\nConsequently, samples targeting similar tasks will share similar expert choices\nacross layers. Building such bindings between tasks and experts over different\nsamples is essential to achieve better generalization. Moreover, RoMA\ndemonstrates the advantage of unifying the task understanding (by embedding\nmodels) with solution generation (by MoE LLMs). In experiments, we finetune\nrouters in OLMoE, DeepSeekMoE, and Qwen3-MoE using RoMA. Evaluations on diverse\nbenchmarks and extensive comparisons with baselines show the substantial\nimprovement brought by RoMA.",
    "translation": "标题：路由流形对齐提升混合专家大语言模型的泛化能力\n\n摘要：稀疏混合专家模型因其能够在保持推理成本不变的前提下有效扩展模型能力，已被广泛应用于当前的大语言模型中。然而，在广泛下游任务上的评估表明，现有混合专家大语言模型中的路由模块普遍存在次优问题，导致其与最优路由方案存在显著性能差距（例如准确率相差10-20%）。本文证明，将路由权重流形与任务嵌入流形进行对齐，可有效缩小该差距并提升混合专家大语言模型的泛化性能。我们提出的\"路由流形对齐方法\"在训练目标中引入额外的流形正则项，仅需对路由模块进行轻量级微调（其余参数冻结）。具体而言，该正则化促使每个样本的路由权重在任务嵌入空间中趋近于其成功邻域样本（即路由权重能产生正确答案的样本）的权重分布。通过这种方式，面向相似任务的样本在各网络层会形成一致的专家选择模式。在不同样本间建立任务与专家的绑定关系，对提升模型泛化能力至关重要。此外，该方法还展现了将任务理解（通过嵌入模型实现）与解决方案生成（通过混合专家大语言模型实现）相统一的优势。实验中，我们使用该方法对OLMoE、DeepSeekMoE和Qwen3-MoE的路由模块进行微调。在多样化基准测试中的评估结果以及与基线模型的广泛对比表明，该方法带来了显著的性能提升。",
    "url": "https://huggingface.co/papers/2511.07419",
    "arxiv_url": "https://arxiv.org/abs/2511.07419"
  },
  {
    "title": "RedOne 2.0: Rethinking Domain-specific LLM Post-Training in Social\n  Networking Services",
    "summary": "As a key medium for human interaction and information exchange, social\nnetworking services (SNS) pose unique challenges for large language models\n(LLMs): heterogeneous workloads, fast-shifting norms and slang, and\nmultilingual, culturally diverse corpora that induce sharp distribution shift.\nSupervised fine-tuning (SFT) can specialize models but often triggers a\n``seesaw'' between in-distribution gains and out-of-distribution robustness,\nespecially for smaller models. To address these challenges, we introduce RedOne\n2.0, an SNS-oriented LLM trained with a progressive, RL-prioritized\npost-training paradigm designed for rapid and stable adaptation. The pipeline\nconsist in three stages: (1) Exploratory Learning on curated SNS corpora to\nestablish initial alignment and identify systematic weaknesses; (2) Targeted\nFine-Tuning that selectively applies SFT to the diagnosed gaps while mixing a\nsmall fraction of general data to mitigate forgetting; and (3) Refinement\nLearning that re-applies RL with SNS-centric signals to consolidate\nimprovements and harmonize trade-offs across tasks. Across various tasks\nspanning three categories, our 4B scale model delivers an average improvements\nabout 2.41 over the 7B sub-optimal baseline. Additionally, RedOne 2.0 achieves\naverage performance lift about 8.74 from the base model with less than half the\ndata required by SFT-centric method RedOne, evidencing superior data efficiency\nand stability at compact scales. Overall, RedOne 2.0 establishes a competitive,\ncost-effective baseline for domain-specific LLMs in SNS scenario, advancing\ncapability without sacrificing robustness.",
    "translation": "标题：RedOne 2.0：社交网络服务中领域专用大语言模型后训练机制的重构\n\n摘要：作为人类互动与信息交换的关键媒介，社交网络服务（SNS）对大语言模型（LLM）提出了独特挑战：异构工作负载、快速演变的网络规范与俚语、以及引发显著分布偏移的多语言跨文化语料。监督微调（SFT）虽能实现模型专业化，但往往会引发分布内性能增益与分布外鲁棒性之间的“跷跷板效应”，这一矛盾在轻量化模型中尤为突出。为应对这些挑战，我们提出RedOne 2.0——采用渐进式强化学习优先的后训练范式，专为快速稳定适配SNS场景而设计。该流程包含三个阶段：（1）基于精选SNS语料的探索性学习，建立初始对齐并识别系统性缺陷；（2）针对性微调，对诊断出的能力缺口选择性应用SFT，同时掺入少量通用数据以缓解灾难性遗忘；（3）强化学习阶段，重新应用以SNS为核心信号的强化学习来巩固改进效果并协调多任务间的权衡关系。在涵盖三大类别的多样化任务测试中，我们40亿参数规模的模型相较70亿参数次优基线实现了平均2.41的性能提升。此外，RedOne 2.0相比基模型取得平均8.74的性能增幅，所需训练数据量较以SFT为核心的方法RedOne减少逾半，彰显出在紧凑规模下卓越的数据效率与训练稳定性。总体而言，RedOne 2.0为SNS场景中的领域专用大语言模型建立了具有竞争力的成本效益基准，在保持鲁棒性的同时持续推动能力边界拓展。",
    "url": "https://huggingface.co/papers/2511.07070",
    "arxiv_url": "https://arxiv.org/abs/2511.07070"
  },
  {
    "title": "SofT-GRPO: Surpassing Discrete-Token LLM Reinforcement Learning via\n  Gumbel-Reparameterized Soft-Thinking Policy Optimization",
    "summary": "The soft-thinking paradigm for Large Language Model (LLM) reasoning can\noutperform the conventional discrete-token Chain-of-Thought (CoT) reasoning in\nsome scenarios, underscoring its research and application value. However, while\nthe discrete-token CoT reasoning pattern can be reinforced through policy\noptimization algorithms such as group relative policy optimization (GRPO),\nextending the soft-thinking pattern with Reinforcement Learning (RL) remains\nchallenging. This difficulty stems from the complexities of injecting\nstochasticity into soft-thinking tokens and updating soft-thinking policies\naccordingly. As a result, previous attempts to combine soft-thinking with GRPO\ntypically underperform their discrete-token GRPO counterparts. To fully unlock\nthe potential of soft-thinking, this paper presents a novel policy optimization\nalgorithm, SofT-GRPO, to reinforce LLMs under the soft-thinking reasoning\npattern. SofT-GRPO injects the Gumbel noise into logits, employs the\nGumbel-Softmax technique to avoid soft-thinking tokens outside the pre-trained\nembedding space, and leverages the reparameterization trick in policy gradient.\nWe conduct experiments across base LLMs ranging from 1.5B to 7B parameters, and\nresults demonstrate that SofT-GRPO enables soft-thinking LLMs to slightly\noutperform discrete-token GRPO on Pass@1 (+0.13% on average accuracy), while\nexhibiting a substantial uplift on Pass@32 (+2.19% on average accuracy). Codes\nand weights are available on https://github.com/zz1358m/SofT-GRPO-master",
    "translation": "标题：SofT-GRPO：基于Gumbel重参数化软思考策略优化突破离散令牌大语言模型强化学习\n\n摘要：大语言模型的软思考推理范式在某些场景下能超越传统的离散令牌思维链推理，彰显其研究与应用价值。然而，尽管离散令牌思维链推理模式可通过群体相对策略优化等算法进行强化，将软思考模式与强化学习相结合仍面临挑战。这一困难源于向软思考令牌注入随机性及相应策略更新的复杂性，导致先前将软思考与GRPO结合的尝试通常表现不及离散令牌GRPO方法。为充分释放软思考潜力，本文提出新型策略优化算法SofT-GRPO，在软思考推理模式下强化大语言模型。该算法将Gumbel噪声注入逻辑值，采用Gumbel-Softmax技术避免软思考令牌超出预训练嵌入空间，并在策略梯度中运用重参数化技巧。我们在1.5B至7B参数的基础大语言模型上进行实验，结果表明：SofT-GRPO使软思考模型在Pass@1指标上略微超越离散令牌GRPO（平均准确率提升0.13%），同时在Pass@32指标上实现显著提升（平均准确率提升2.19%）。代码与权重文件已发布于https://github.com/zz1358m/SofT-GRPO-master。",
    "url": "https://huggingface.co/papers/2511.06411",
    "arxiv_url": "https://arxiv.org/abs/2511.06411"
  },
  {
    "title": "Reasoning with Confidence: Efficient Verification of LLM Reasoning Steps\n  via Uncertainty Heads",
    "summary": "Solving complex tasks usually requires LLMs to generate long multi-step\nreasoning chains. Previous work has shown that verifying the correctness of\nindividual reasoning steps can further improve the performance and efficiency\nof LLMs on such tasks and enhance solution interpretability. However, existing\nverification approaches, such as Process Reward Models (PRMs), are either\ncomputationally expensive, limited to specific domains, or require large-scale\nhuman or model-generated annotations. Thus, we propose a lightweight\nalternative for step-level reasoning verification based on data-driven\nuncertainty scores. We train transformer-based uncertainty quantification heads\n(UHeads) that use the internal states of a frozen LLM to estimate the\nuncertainty of its reasoning steps during generation. The approach is fully\nautomatic: target labels are generated either by another larger LLM (e.g.,\nDeepSeek R1) or in a self-supervised manner by the original model itself.\nUHeads are both effective and lightweight, containing less than 10M parameters.\nAcross multiple domains, including mathematics, planning, and general knowledge\nquestion answering, they match or even surpass the performance of PRMs that are\nup to 810x larger. Our findings suggest that the internal states of LLMs encode\ntheir uncertainty and can serve as reliable signals for reasoning verification,\noffering a promising direction toward scalable and generalizable introspective\nLLMs.",
    "translation": "标题：基于置信度的推理：通过不确定性头部实现大语言模型推理步骤的高效验证\n\n摘要：解决复杂任务通常需要大语言模型生成多步骤的长推理链。已有研究表明，对单个推理步骤的正确性进行验证能够进一步提升大语言模型在此类任务中的表现效率，并增强解决方案的可解释性。然而现有验证方法（如过程奖励模型）存在计算成本高昂、适用领域受限或需要大规模人工/模型生成标注等问题。为此，我们提出一种基于数据驱动不确定性评分的轻量级步骤级推理验证方案。通过训练基于Transformer的不确定性量化头部，利用冻结大语言模型的内部状态来估计其生成过程中推理步骤的不确定性。该方法完全自动化：目标标签可由更大规模语言模型（如DeepSeek R1）生成，或由原模型以自监督方式产生。不确定性头部在参数量不足1000万的情况下兼具高效性与轻量化特性。在数学推理、任务规划和常识问答等多个领域，其性能均可媲美甚至超越参数量达810倍的过程奖励模型。我们的研究结果表明，大语言模型的内部状态编码了其不确定性，可作为推理验证的可靠信号，为构建可扩展、可泛化的自省式大语言模型指明了可行方向。",
    "url": "https://huggingface.co/papers/2511.06209",
    "arxiv_url": "https://arxiv.org/abs/2511.06209"
  },
  {
    "title": "Robot Learning from a Physical World Model",
    "summary": "We introduce PhysWorld, a framework that enables robot learning from video\ngeneration through physical world modeling. Recent video generation models can\nsynthesize photorealistic visual demonstrations from language commands and\nimages, offering a powerful yet underexplored source of training signals for\nrobotics. However, directly retargeting pixel motions from generated videos to\nrobots neglects physics, often resulting in inaccurate manipulations. PhysWorld\naddresses this limitation by coupling video generation with physical world\nreconstruction. Given a single image and a task command, our method generates\ntask-conditioned videos and reconstructs the underlying physical world from the\nvideos, and the generated video motions are grounded into physically accurate\nactions through object-centric residual reinforcement learning with the\nphysical world model. This synergy transforms implicit visual guidance into\nphysically executable robotic trajectories, eliminating the need for real robot\ndata collection and enabling zero-shot generalizable robotic manipulation.\nExperiments on diverse real-world tasks demonstrate that PhysWorld\nsubstantially improves manipulation accuracy compared to previous approaches.\nVisit https://pointscoder.github.io/PhysWorld_Web/{the project webpage}\nfor details.",
    "translation": "标题：基于物理世界模型的机器人学习\n\n摘要：本文提出PhysWorld框架，通过物理世界建模实现基于视频生成的机器人学习。当前视频生成模型能够根据语言指令和图像合成逼真的视觉演示，为机器人技术提供了强大却尚未充分开发的训练信号源。然而，直接将从生成视频中提取的像素运动映射到机器人会忽略物理规律，常导致操作失准。PhysWorld通过耦合视频生成与物理世界重建来解决这一局限。给定单张图像和任务指令，本方法既能生成任务条件视频，又能从视频中重建底层物理世界，同时通过基于对象的残差强化学习与物理世界模型相结合，将生成的视频运动转化为物理精确的动作。这种协同作用将隐式视觉指导转化为可物理执行的机器人轨迹，无需真实机器人数据采集即可实现零样本可泛化的机器人操作。在多样化现实任务上的实验表明，与现有方法相比，PhysWorld显著提升了操作精度。详情请访问项目网页：https://pointscoder.github.io/PhysWorld_Web/",
    "url": "https://huggingface.co/papers/2511.07416",
    "arxiv_url": "https://arxiv.org/abs/2511.07416"
  },
  {
    "title": "Teaching Pretrained Language Models to Think Deeper with Retrofitted\n  Recurrence",
    "summary": "Recent advances in depth-recurrent language models show that recurrence can\ndecouple train-time compute and parameter count from test-time compute. In this\nwork, we study how to convert existing pretrained non-recurrent language models\ninto depth-recurrent models. We find that using a curriculum of recurrences to\nincrease the effective depth of the model over the course of training preserves\nperformance while reducing total computational cost. In our experiments, on\nmathematics, we observe that converting pretrained models to recurrent ones\nresults in better performance at a given compute budget than simply\npost-training the original non-recurrent language model.",
    "translation": "标题：基于改进型循环结构的预训练语言模型深度思维教学方法  \n摘要：深度循环语言模型的最新进展表明，循环结构能够将训练阶段的计算量与参数量同测试阶段的计算需求解耦。本研究探索如何将现有非循环预训练语言模型转化为深度循环模型。我们发现，通过采用渐进式循环训练课程，在训练过程中逐步增加模型有效深度，可在降低总体计算成本的同时保持模型性能。在数学领域的实验中，相较于直接对原始非循环语言模型进行后训练，将预训练模型转换为循环结构能在相同计算预算下获得更优的性能表现。",
    "url": "https://huggingface.co/papers/2511.07384",
    "arxiv_url": "https://arxiv.org/abs/2511.07384"
  },
  {
    "title": "NURBGen: High-Fidelity Text-to-CAD Generation through LLM-Driven NURBS\n  Modeling",
    "summary": "Generating editable 3D CAD models from natural language remains challenging,\nas existing text-to-CAD systems either produce meshes or rely on scarce\ndesign-history data. We present NURBGen, the first framework to generate\nhigh-fidelity 3D CAD models directly from text using Non-Uniform Rational\nB-Splines (NURBS). To achieve this, we fine-tune a large language model (LLM)\nto translate free-form texts into JSON representations containing NURBS surface\nparameters (i.e, control points, knot vectors, degrees, and rational\nweights) which can be directly converted into BRep format using Python. We\nfurther propose a hybrid representation that combines untrimmed NURBS with\nanalytic primitives to handle trimmed surfaces and degenerate regions more\nrobustly, while reducing token complexity. Additionally, we introduce partABC,\na curated subset of the ABC dataset consisting of individual CAD components,\nannotated with detailed captions using an automated annotation pipeline.\nNURBGen demonstrates strong performance on diverse prompts, surpassing prior\nmethods in geometric fidelity and dimensional accuracy, as confirmed by expert\nevaluations. Code and dataset will be released publicly.",
    "translation": "标题：NURBGen：基于大语言模型驱动的NURBS建模实现高保真文本到CAD生成\n\n摘要：从自然语言生成可编辑的3D CAD模型仍面临挑战，现有文本到CAD系统要么生成网格模型，要么依赖稀缺的设计历史数据。我们提出NURBGen——首个通过非均匀有理B样条（NURBS）直接根据文本生成高保真3D CAD模型的框架。为实现这一目标，我们微调大语言模型将自由格式文本转换为包含NURBS曲面参数（即控制点、节点向量、阶次和有理权重）的JSON表示，这些参数可通过Python直接转换为BRep格式。我们进一步提出混合表示法，将未修剪NURBS与解析图元相结合，以更稳健地处理修剪曲面和退化区域，同时降低标记复杂度。此外，我们推出partABC——这是ABC数据集的精选子集，包含独立CAD组件，并通过自动化标注流程添加了详细描述。经专家评估证实，NURBGen在多样化提示词上表现出卓越性能，在几何保真度与尺寸精度方面均超越现有方法。代码与数据集将公开发布。",
    "url": "https://huggingface.co/papers/2511.06194",
    "arxiv_url": "https://arxiv.org/abs/2511.06194"
  },
  {
    "title": "Long Grounded Thoughts: Distilling Compositional Visual Reasoning Chains at Scale",
    "summary": "Recent progress in multimodal reasoning has been driven largely by undisclosed datasets and proprietary data synthesis recipes, leaving open questions about how to systematically build large-scale, vision-centric reasoning datasets, particularly for tasks that go beyond visual math. In this work, we introduce a new reasoning data generation framework spanning diverse skills and levels of complexity with over 1M high-quality synthetic vision-centric questions. The dataset also includes preference data and instruction prompts supporting both offline and online RL. Our synthesis framework proceeds in two stages: (1) scale; and (2) complexity. Reasoning traces are then synthesized through a two-stage process that leverages VLMs and reasoning LLMs, producing CoT traces for VLMs that capture the richness and diverse cognitive behaviors found in frontier reasoning models. Remarkably, we show that finetuning Qwen2.5-VL-7B on our data outperforms all open-data baselines across all evaluated vision-centric benchmarks, and even surpasses strong closed-data models such as MiMo-VL-7B-RL on V* Bench, CV-Bench and MMStar-V. Perhaps most surprising, despite being entirely vision-centric, our data transfers positively to text-only reasoning (MMLU-Pro) and audio reasoning (MMAU), demonstrating its effectiveness. Similarly, despite not containing videos or embodied visual data, we observe notable gains when evaluating on a single-evidence embodied QA benchmark (NiEH). Finally, we use our data to analyze the entire VLM post-training pipeline. Our empirical analysis highlights that (i) SFT on high-quality data with non-linear reasoning traces is essential for effective online RL, (ii) staged offline RL matches online RL's performance while reducing compute demands, and (iii) careful SFT on high quality data can substantially improve out-of-domain, cross-modality transfer.",
    "translation": "标题：长链接地思维：规模化提炼组合式视觉推理链条\n\n摘要：当前多模态推理的进展主要依赖于未公开数据集与专有数据合成方案，这引发了对如何系统构建大规模以视觉为中心的推理数据集的开放性问题，尤其针对超越视觉数学范畴的任务。本研究提出新型推理数据生成框架，涵盖多样化技能与复杂度层级，生成超过100万条高质量合成视觉中心问题。该数据集同时包含偏好数据与指令提示，支持离线和在线强化学习。我们的合成框架分两阶段实施：(1)规模化扩展；(2)复杂度提升。通过融合视觉语言模型与推理大语言模型的两阶段处理流程，生成适用于视觉语言模型的思维链轨迹，捕捉前沿推理模型中丰富的多样化认知行为。值得注意的是，基于我们数据微调的Qwen2.5-VL-7B模型在所有评估的视觉中心基准测试中均超越开源基线模型，甚至在V* Bench、CV-Bench和MMStar-V上优于MiMo-VL-7B-RL等强效闭源模型。最令人惊讶的是，尽管完全以视觉为中心，我们的数据在纯文本推理（MMLU-Pro）和音频推理（MMAU）任务中展现出正向迁移能力。同样，在未包含视频或具身视觉数据的情况下，我们在单证据具身问答基准（NiEH）上观察到显著性能提升。最后，我们利用该数据系统分析了视觉语言模型的后训练流程。实证研究表明：(i)基于非线性推理轨迹的高质量数据监督微调是在线强化学习取得成效的关键；(ii)分阶段离线强化学习可匹配在线强化学习性能，同时降低计算需求；(iii)对高质量数据进行精细监督微调能显著提升跨领域、跨模态的迁移性能。",
    "url": "https://huggingface.co/papers/2511.05705",
    "arxiv_url": "https://arxiv.org/abs/2511.05705"
  },
  {
    "title": "RLoop: An Self-Improving Framework for Reinforcement Learning with\n  Iterative Policy Initialization",
    "summary": "While Reinforcement Learning for Verifiable Rewards (RLVR) is powerful for\ntraining large reasoning models, its training dynamics harbor a critical\nchallenge: RL overfitting, where models gain training rewards but lose\ngeneralization. Our analysis reveals this is driven by policy\nover-specialization and catastrophic forgetting of diverse solutions generated\nduring training. Standard optimization discards this valuable inter-step policy\ndiversity. To address this, we introduce RLoop, a self-improving framework\nbuilt on iterative policy initialization. RLoop transforms the standard\ntraining process into a virtuous cycle: it first uses RL to explore the\nsolution space from a given policy, then filters the successful trajectories to\ncreate an expert dataset. This dataset is used via Rejection-sampling\nFine-Tuning (RFT) to refine the initial policy, creating a superior starting\npoint for the next iteration. This loop of exploration and exploitation via\niterative re-initialization effectively converts transient policy variations\ninto robust performance gains. Our experiments show RLoop mitigates forgetting\nand substantially improves generalization, boosting average accuracy by 9% and\npass@32 by over 15% compared to vanilla RL.",
    "translation": "标题：RLoop：基于迭代策略初始化的强化学习自改进框架\n\n摘要：尽管可验证奖励的强化学习（RLVR）在训练大型推理模型方面具有强大能力，但其训练动态存在一个关键挑战：RL过拟合，即模型获得训练奖励却丧失泛化能力。我们的分析表明，这一现象由策略过度专门化及训练过程中产生的多样化解决方案的灾难性遗忘所驱动。标准优化方法会丢弃这些宝贵的跨步骤策略多样性。为解决此问题，我们提出RLoop——一个基于迭代策略初始化的自改进框架。RLoop将标准训练过程转化为良性循环：首先使用RL从给定策略探索解空间，随后筛选成功轨迹创建专家数据集。通过拒绝采样微调（RFT）利用该数据集优化初始策略，为下一轮迭代创建更优的起点。这种通过迭代重初始化的探索与利用循环，有效将瞬态策略变异转化为稳健的性能提升。实验表明，RLoop能显著缓解遗忘现象并提升泛化能力，相较于原始RL方法，平均准确率提高9%，pass@32指标提升超过15%。",
    "url": "https://huggingface.co/papers/2511.04285",
    "arxiv_url": "https://arxiv.org/abs/2511.04285"
  },
  {
    "title": "DigiData: Training and Evaluating General-Purpose Mobile Control Agents",
    "summary": "AI agents capable of controlling user interfaces have the potential to\ntransform human interaction with digital devices. To accelerate this\ntransformation, two fundamental building blocks are essential: high-quality\ndatasets that enable agents to achieve complex and human-relevant goals, and\nrobust evaluation methods that allow researchers and practitioners to rapidly\nenhance agent performance. In this paper, we introduce DigiData, a large-scale,\nhigh-quality, diverse, multi-modal dataset designed for training mobile control\nagents. Unlike existing datasets, which derive goals from unstructured\ninteractions, DigiData is meticulously constructed through comprehensive\nexploration of app features, resulting in greater diversity and higher goal\ncomplexity. Additionally, we present DigiData-Bench, a benchmark for evaluating\nmobile control agents on real-world complex tasks. We demonstrate that the\ncommonly used step-accuracy metric falls short in reliably assessing mobile\ncontrol agents and, to address this, we propose dynamic evaluation protocols\nand AI-powered evaluations as rigorous alternatives for agent assessment. Our\ncontributions aim to significantly advance the development of mobile control\nagents, paving the way for more intuitive and effective human-device\ninteractions.",
    "translation": "标题：DigiData：通用移动控制智能体的训练与评估框架\n\n摘要：具备用户界面控制能力的AI智能体有望彻底改变人类与数字设备的交互模式。为加速这一变革进程，两大基础要素不可或缺：一是能够支持智能体实现复杂且符合人类需求目标的高质量数据集，二是可供研究者与从业者快速提升智能体性能的稳健评估方法。本文提出DigiData——一个专为移动控制智能体训练设计的大规模、高质量、多模态数据集。与现有基于非结构化交互生成目标的数据集不同，DigiData通过系统性探索应用程序功能进行精心构建，从而具备更丰富的多样性及更高的目标复杂度。同时，我们推出DigiData-Bench基准测试平台，用于评估智能体在真实场景复杂任务中的表现。研究证实当前广泛采用的分步准确率指标难以可靠评估移动控制智能体，为此我们提出动态评估协议与AI驱动的评估方法作为严格的替代方案。本研究的成果将有力推动移动控制智能体发展，为构建更直观高效的人机交互模式奠定基础。",
    "url": "https://huggingface.co/papers/2511.07413",
    "arxiv_url": "https://arxiv.org/abs/2511.07413"
  },
  {
    "title": "MPJudge: Towards Perceptual Assessment of Music-Induced Paintings",
    "summary": "Music induced painting is a unique artistic practice, where visual artworks\nare created under the influence of music. Evaluating whether a painting\nfaithfully reflects the music that inspired it poses a challenging perceptual\nassessment task. Existing methods primarily rely on emotion recognition models\nto assess the similarity between music and painting, but such models introduce\nconsiderable noise and overlook broader perceptual cues beyond emotion. To\naddress these limitations, we propose a novel framework for music induced\npainting assessment that directly models perceptual coherence between music and\nvisual art. We introduce MPD, the first large scale dataset of music painting\npairs annotated by domain experts based on perceptual coherence. To better\nhandle ambiguous cases, we further collect pairwise preference annotations.\nBuilding on this dataset, we present MPJudge, a model that integrates music\nfeatures into a visual encoder via a modulation based fusion mechanism. To\neffectively learn from ambiguous cases, we adopt Direct Preference Optimization\nfor training. Extensive experiments demonstrate that our method outperforms\nexisting approaches. Qualitative results further show that our model more\naccurately identifies music relevant regions in paintings.",
    "translation": "标题：MPJudge：音乐诱发绘画的感知评估研究\n\n摘要：音乐诱发绘画是一种独特的艺术实践形式，指在音乐影响下创作视觉艺术作品。评估画作是否忠实反映其灵感来源的音乐，构成了一项具有挑战性的感知评估任务。现有方法主要依赖情感识别模型来评估音乐与绘画的相似性，但此类模型会引入显著噪声且忽略了情感之外的更广泛感知线索。为克服这些局限，我们提出了一种新颖的音乐诱发绘画评估框架，直接建模音乐与视觉艺术之间的感知连贯性。我们引入了MPD数据集——首个由领域专家基于感知连贯性标注的大规模音乐-绘画配对数据集。为更好处理模糊案例，我们进一步收集了成对偏好标注。基于该数据集，我们提出了MPJudge模型，通过基于调制的融合机制将音乐特征整合到视觉编码器中。为有效学习模糊案例，我们采用直接偏好优化方法进行训练。大量实验表明，我们的方法优于现有方法。定性结果进一步证明，我们的模型能更准确地识别绘画中与音乐相关的区域。",
    "url": "https://huggingface.co/papers/2511.07137",
    "arxiv_url": "https://arxiv.org/abs/2511.07137"
  },
  {
    "title": "Llama-Embed-Nemotron-8B: A Universal Text Embedding Model for\n  Multilingual and Cross-Lingual Tasks",
    "summary": "We introduce llama-embed-nemotron-8b, an open-weights text embedding model\nthat achieves state-of-the-art performance on the Multilingual Massive Text\nEmbedding Benchmark (MMTEB) leaderboard as of October 21, 2025. While recent\nmodels show strong performance, their training data or methodologies are often\nnot fully disclosed. We aim to address this by developing a fully open-source\nmodel, publicly releasing its weights and detailed ablation studies, and\nplanning to share the curated training datasets. Our model demonstrates\nsuperior performance across all major embedding tasks -- including retrieval,\nclassification and semantic textual similarity (STS) -- and excels in\nchallenging multilingual scenarios, such as low-resource languages and\ncross-lingual setups. This state-of-the-art performance is driven by a novel\ndata mix of 16.1 million query-document pairs, split between 7.7 million\nsamples from public datasets and 8.4 million synthetically generated examples\nfrom various open-weight LLMs. One of our key contributions is a detailed\nablation study analyzing core design choices, including a comparison of\ncontrastive loss implementations, an evaluation of synthetic data generation\n(SDG) strategies, and the impact of model merging. The llama-embed-nemotron-8b\nis an instruction-aware model, supporting user-defined instructions to enhance\nperformance for specific use-cases. This combination of top-tier performance,\nbroad applicability, and user-driven flexibility enables it to serve as a\nuniversal text embedding solution.",
    "translation": "标题：Llama-Embed-Nemotron-8B：面向多语言与跨语言任务的通用文本嵌入模型\n\n摘要：本文推出llama-embed-nemotron-8b——一个开放权重的文本嵌入模型，截至2025年10月21日，该模型在多语言海量文本嵌入基准（MMTEB）排行榜上实现了最先进的性能。尽管现有模型展现出强劲性能，但其训练数据与方法论往往未完全公开。我们通过开发完全开源的模型来解决这一问题，公开其权重参数与详细消融实验，并计划发布精编训练数据集。该模型在所有核心嵌入任务（包括检索、分类与语义文本相似度STS）中均表现卓越，尤其在低资源语言和跨语言设置等具有挑战性的多语言场景中表现突出。这一顶尖性能得益于我们创新的数据组合策略：使用1,610万组查询-文档对，其中770万样本来自公共数据集，840万样本由各类开放权重大语言模型合成生成。我们的核心贡献包括通过系统消融实验分析了关键设计选择：对比损失实现的比较、合成数据生成策略的评估，以及模型融合的影响研究。llama-embed-nemotron-8b作为指令感知模型，支持用户自定义指令以增强特定应用场景的性能。这种顶尖性能、广泛适用性与用户驱动灵活性的结合，使其成为通用文本嵌入解决方案的理想选择。",
    "url": "https://huggingface.co/papers/2511.07025",
    "arxiv_url": "https://arxiv.org/abs/2511.07025"
  },
  {
    "title": "FLEX: Continuous Agent Evolution via Forward Learning from Experience",
    "summary": "Autonomous agents driven by Large Language Models (LLMs) have revolutionized\nreasoning and problem-solving but remain static after training, unable to grow\nwith experience as intelligent beings do during deployment. We introduce\nForward Learning with EXperience (FLEX), a gradient-free learning paradigm that\nenables LLM agents to continuously evolve through accumulated experience.\nSpecifically, FLEX cultivates scalable and inheritable evolution by\nconstructing a structured experience library through continual reflection on\nsuccesses and failures during interaction with the environment. FLEX delivers\nsubstantial improvements on mathematical reasoning, chemical retrosynthesis,\nand protein fitness prediction (up to 23% on AIME25, 10% on USPTO50k, and 14%\non ProteinGym). We further identify a clear scaling law of experiential growth\nand the phenomenon of experience inheritance across agents, marking a step\ntoward scalable and inheritable continuous agent evolution. Project Page:\nhttps://flex-gensi-thuair.github.io.",
    "translation": "标题：FLEX：基于经验前向学习的智能体持续进化框架\n\n摘要：基于大语言模型的自主智能体在推理与问题解决领域实现了革命性突破，但其在训练完成后即保持静态，无法像智能生物那样在部署过程中通过经验积累实现持续成长。本文提出经验前向学习框架（FLEX），该梯度无关的学习范式使大语言模型智能体能够通过积累的经验实现持续进化。具体而言，FLEX通过持续反思与环境交互过程中的成功与失败案例，构建结构化经验库，从而培育可扩展、可传承的进化机制。在数学推理、化学逆合成及蛋白质适应性预测任务中，FLEX分别取得显著提升（AIME25基准提升23%，USPTO50k提升10%，ProteinGym提升14%）。研究进一步揭示了经验增长的显著缩放规律及跨智能体经验传承现象，标志着向可扩展、可传承的持续智能体进化迈出关键一步。项目页面：https://flex-gensi-thuair.github.io。",
    "url": "https://huggingface.co/papers/2511.06449",
    "arxiv_url": "https://arxiv.org/abs/2511.06449"
  },
  {
    "title": "10 Open Challenges Steering the Future of Vision-Language-Action Models",
    "summary": "Due to their ability of follow natural language instructions,\nvision-language-action (VLA) models are increasingly prevalent in the embodied\nAI arena, following the widespread success of their precursors -- LLMs and\nVLMs. In this paper, we discuss 10 principal milestones in the ongoing\ndevelopment of VLA models -- multimodality, reasoning, data, evaluation,\ncross-robot action generalization, efficiency, whole-body coordination, safety,\nagents, and coordination with humans. Furthermore, we discuss the emerging\ntrends of using spatial understanding, modeling world dynamics, post training,\nand data synthesis -- all aiming to reach these milestones. Through these\ndiscussions, we hope to bring attention to the research avenues that may\naccelerate the development of VLA models into wider acceptability.",
    "translation": "标题：引领视觉-语言-动作模型未来发展的十大开放挑战\n\n摘要：凭借其遵循自然语言指令的能力，视觉-语言-动作模型在具身人工智能领域日益普及，这得益于其前身——大语言模型和视觉语言模型取得的广泛成功。本文系统论述了VLA模型发展过程中的十大关键挑战：多模态融合、推理能力、数据构建、评估体系、跨机器人动作泛化、计算效率、全身协调、安全机制、智能体架构以及人机协作。同时，我们深入探讨了为实现这些突破性进展而涌现的四大技术趋势：空间关系理解、世界动态建模、后训练优化以及数据合成技术。通过以上探讨，我们希望引导研究界关注那些能加速VLA模型获得更广泛适用性的关键研究方向。",
    "url": "https://huggingface.co/papers/2511.05936",
    "arxiv_url": "https://arxiv.org/abs/2511.05936"
  },
  {
    "title": "Ariadne: A Controllable Framework for Probing and Extending VLM\n  Reasoning Boundaries",
    "summary": "While Vision-Language Models (VLMs) post-trained with Reinforcement Learning\n(RL) show impressive general reasoning, their evaluation is often confined to\nlanguage-dominant tasks (e.g., math). This raises a critical question: can RL\npost-training truly extend the inherent capability boundary of a base VLM,\nparticularly for visual-centric spatial tasks where it initially fails? To\ninvestigate this, we introduce Ariadne, a framework utilizing synthetic mazes\nfor multi-step spatial reasoning where task difficulty (e.g., path length,\nturns) is precisely controlled. We leverage this controllable environment to\ntrain VLMs using Reinforcement Learning with Verified Rewards (RLVR) in a\ndifficulty-aware curriculum. Surprisingly, post-RLVR training, the VLM achieves\nover 50% accuracy on a problem set where the base model scored 0%,\ndemonstrating that our approach expands the model's initial capability\nboundary. To assess real-world viability, we evaluate out-of-distribution (OOD)\ngeneralization on practical benchmarks. Despite training only on synthetic maze\nsamples, Ariadne achieves significant zero-shot improvements, averaging 16% on\nMapBench (e.g., museum navigation) and 24% on ReasonMap (subway transfer\ntasks). These results confirm that our method not only broadens the model's\nfundamental limits but also enhances its generalization to real-world spatial\nreasoning. We acknowledge our study is limited to the post-training phase,\ngiven the opaqueness of pre-training data, and hope our research motivates\nfurther work on specialized, capability-extending alignment.",
    "translation": "标题：Ariadne：一个用于探索与拓展视觉语言模型推理边界的可控框架\n\n摘要：尽管经过强化学习（RL）后训练的视觉语言模型（VLM）展现出卓越的通用推理能力，但其评估通常局限于语言主导型任务（如数学推理）。这引发了一个关键问题：对于基础VLM初始无法解决的视觉中心型空间任务，RL后训练是否能真正扩展其固有能力边界？为探究此问题，我们提出Ariadne框架——通过合成迷宫构建多步骤空间推理任务，其中任务难度（如路径长度、转弯次数）可精确调控。利用该可控环境，我们采用带验证奖励的强化学习（RLVR）在难度感知课程中训练VLM。令人惊讶的是，经过RLVR后训练的VLM在基础模型准确率为0%的问题集上实现了超过50%的准确率，证明我们的方法有效拓展了模型的初始能力边界。为评估实际应用潜力，我们在现实基准测试中进行了分布外（OOD）泛化评估。尽管仅使用合成迷宫样本进行训练，Ariadne在MapBench（如博物馆导航）和ReasonMap（地铁换乘任务）上分别实现了16%和24%的平均零样本性能提升。这些结果证实我们的方法不仅拓宽了模型的基础能力边界，还增强了其在现实空间任务中的泛化能力。需要说明的是，鉴于预训练数据的不透明性，本研究仅聚焦后训练阶段，我们期待此项工作能推动面向专业能力拓展的对齐方法研究。",
    "url": "https://huggingface.co/papers/2511.00710",
    "arxiv_url": "https://arxiv.org/abs/2511.00710"
  },
  {
    "title": "DIMO: Diverse 3D Motion Generation for Arbitrary Objects",
    "summary": "We present DIMO, a generative approach capable of generating diverse 3D\nmotions for arbitrary objects from a single image. The core idea of our work is\nto leverage the rich priors in well-trained video models to extract the common\nmotion patterns and then embed them into a shared low-dimensional latent space.\nSpecifically, we first generate multiple videos of the same object with diverse\nmotions. We then embed each motion into a latent vector and train a shared\nmotion decoder to learn the distribution of motions represented by a structured\nand compact motion representation, i.e., neural key point trajectories. The\ncanonical 3D Gaussians are then driven by these key points and fused to model\nthe geometry and appearance. During inference time with learned latent space,\nwe can instantly sample diverse 3D motions in a single-forward pass and support\nseveral interesting applications including 3D motion interpolation and\nlanguage-guided motion generation. Our project page is available at\nhttps://linzhanm.github.io/dimo.",
    "translation": "标题：DIMO：面向任意物体的多样化三维运动生成方法\n\n摘要：本文提出DIMO生成式方法，能够基于单张图像为任意物体生成多样化的三维运动。本方法的核心思想是利用训练完备的视频模型中蕴含的丰富先验知识，提取通用运动模式并将其嵌入至共享的低维潜空间。具体而言，我们首先生成具有多样化运动的同物体多段视频，随后将每种运动编码为潜向量，并通过训练共享运动解码器来学习以结构化紧凑运动表征（即神经关键点轨迹）所描述的运动分布。这些规范化的三维高斯模型由关键点驱动并进行融合，从而实现对几何形态与外观特征的建模。在基于已学习潜空间的推理阶段，我们可通过单次前向传播实时采样多样化三维运动，并支持三维运动插值与语言引导运动生成等多项创新应用。项目主页详见：https://linzhanm.github.io/dimo。",
    "url": "https://huggingface.co/papers/2511.07409",
    "arxiv_url": "https://arxiv.org/abs/2511.07409"
  },
  {
    "title": "RLVE: Scaling Up Reinforcement Learning for Language Models with\n  Adaptive Verifiable Environments",
    "summary": "We introduce Reinforcement Learning (RL) with Adaptive Verifiable\nEnvironments (RLVE), an approach using verifiable environments that\nprocedurally generate problems and provide algorithmically verifiable rewards,\nto scale up RL for language models (LMs). RLVE enables each verifiable\nenvironment to dynamically adapt its problem difficulty distribution to the\npolicy model's capabilities as training progresses. In contrast, static data\ndistributions often lead to vanishing learning signals when problems are either\ntoo easy or too hard for the policy. To implement RLVE, we create RLVE-Gym, a\nlarge-scale suite of 400 verifiable environments carefully developed through\nmanual environment engineering. Using RLVE-Gym, we show that environment\nscaling, i.e., expanding the collection of training environments, consistently\nimproves generalizable reasoning capabilities. RLVE with joint training across\nall 400 environments in RLVE-Gym yields a 3.37% absolute average improvement\nacross six reasoning benchmarks, starting from one of the strongest 1.5B\nreasoning LMs. By comparison, continuing this LM's original RL training yields\nonly a 0.49% average absolute gain despite using over 3x more compute. We\nrelease our code publicly.",
    "translation": "标题：RLVE：基于可验证自适应环境扩展语言模型强化学习规模的方法\n\n摘要：本文提出基于可验证自适应环境（RLVE）的强化学习方法，通过采用可验证环境程序化生成问题并提供算法可验证奖励的机制，实现了语言模型强化学习规模的扩展。RLVE使每个可验证环境能够根据策略模型在训练过程中的能力表现，动态调整其问题难度分布。相比之下，静态数据分布在问题难度与策略能力不匹配时（过于简单或困难）往往会导致学习信号消失。为实现RLVE，我们开发了RLVE-Gym——一个通过人工环境工程精心构建的包含400个可验证环境的大规模训练套件。使用RLVE-Gym的实验表明，环境扩展（即增加训练环境集合）能持续提升模型的泛化推理能力。在RLVE-Gym全部400个环境中进行联合训练的RLVE方法，从当前最强的15亿参数推理语言模型出发，在六大推理基准测试中实现了3.37%的平均绝对性能提升。相比之下，延续该语言模型原有强化训练方案仅获得0.49%的平均绝对增益，尽管其计算消耗超过RLVE方法的三倍。我们已公开相关代码。",
    "url": "https://huggingface.co/papers/2511.07317",
    "arxiv_url": "https://arxiv.org/abs/2511.07317"
  },
  {
    "title": "Do LLMs Feel? Teaching Emotion Recognition with Prompts, Retrieval, and\n  Curriculum Learning",
    "summary": "Emotion Recognition in Conversation (ERC) is a crucial task for understanding\nhuman emotions and enabling natural human-computer interaction. Although Large\nLanguage Models (LLMs) have recently shown great potential in this field, their\nability to capture the intrinsic connections between explicit and implicit\nemotions remains limited. We propose a novel ERC training framework, PRC-Emo,\nwhich integrates Prompt engineering, demonstration Retrieval, and Curriculum\nlearning, with the goal of exploring whether LLMs can effectively perceive\nemotions in conversational contexts. Specifically, we design emotion-sensitive\nprompt templates based on both explicit and implicit emotional cues to better\nguide the model in understanding the speaker's psychological states. We\nconstruct the first dedicated demonstration retrieval repository for ERC, which\nincludes training samples from widely used datasets, as well as high-quality\ndialogue examples generated by LLMs and manually verified. Moreover, we\nintroduce a curriculum learning strategy into the LoRA fine-tuning process,\nincorporating weighted emotional shifts between same-speaker and\ndifferent-speaker utterances to assign difficulty levels to dialogue samples,\nwhich are then organized in an easy-to-hard training sequence. Experimental\nresults on two benchmark datasets-- IEMOCAP and MELD --show that our method\nachieves new state-of-the-art (SOTA) performance, demonstrating the\neffectiveness and generalizability of our approach in improving LLM-based\nemotional understanding.",
    "translation": "标题：大语言模型具有情感感知能力吗？基于提示工程、检索机制与课程学习的情感识别方法研究\n\n摘要：对话中的情感识别（ERC）是理解人类情感并实现自然人机交互的关键任务。尽管大语言模型（LLMs）近期在该领域展现出巨大潜力，但其捕捉显性情感与隐性情感内在联系的能力仍存在局限。我们提出了一种创新的ERC训练框架PRC-Emo，该框架融合提示工程、示例检索与课程学习三大模块，旨在探究LLMs能否有效感知对话语境中的情感波动。具体而言，我们基于显性与隐性情感线索设计了情感敏感型提示模板，以更精准地引导模型理解说话者的心理状态。我们构建了首个面向ERC的专用示例检索库，其中既包含广泛使用的数据集训练样本，也涵盖由LLMs生成并经人工校验的高质量对话实例。此外，我们将课程学习策略引入LoRA微调过程，通过量化同一说话者与不同说话者话语间的情感转移权重，为对话样本标注难度等级，并依此构建由易到难的训练序列。在IEMOCAP和MELD两个基准数据集上的实验结果表明，本方法取得了最新的最优性能，验证了所提方案在增强基于LLM的情感理解能力方面的有效性与泛化性。",
    "url": "https://huggingface.co/papers/2511.07061",
    "arxiv_url": "https://arxiv.org/abs/2511.07061"
  },
  {
    "title": "SWE-fficiency: Can Language Models Optimize Real-World Repositories on\n  Real Workloads?",
    "summary": "Optimizing the performance of large-scale software repositories demands\nexpertise in code reasoning and software engineering (SWE) to reduce runtime\nwhile preserving program correctness. However, most benchmarks emphasize what\nto fix rather than how to fix code. We introduce SWE-fficiency, a\nbenchmark for evaluating repository-level performance optimization on real\nworkloads. Our suite contains 498 tasks across nine widely used data-science,\nmachine-learning, and HPC repositories (e.g., numpy, pandas, scipy): given a\ncomplete codebase and a slow workload, an agent must investigate code\nsemantics, localize bottlenecks and relevant tests, and produce a patch that\nmatches or exceeds expert speedup while passing the same unit tests. To enable\nthis how-to-fix evaluation, our automated pipeline scrapes GitHub pull requests\nfor performance-improving edits, combining keyword filtering, static analysis,\ncoverage tooling, and execution validation to both confirm expert speedup\nbaselines and identify relevant repository unit tests. Empirical evaluation of\nstate-of-the-art agents reveals significant underperformance. On average,\nagents achieve less than 0.15x the expert speedup: agents struggle in\nlocalizing optimization opportunities, reasoning about execution across\nfunctions, and maintaining correctness in proposed edits. We release the\nbenchmark and accompanying data pipeline to facilitate research on automated\nperformance engineering and long-horizon software reasoning.",
    "translation": "标题：SWE-fficiency：语言模型能否在真实工作负载下优化现实代码库？\n\n摘要：优化大型软件仓库的性能需要代码推理与软件工程的专业能力，在保证程序正确性的同时降低运行耗时。然而现有基准测试多聚焦于“修复目标”而非“修复方法”。我们提出SWE-fficiency这一面向真实工作负载的仓库级性能优化评估基准，该测试集涵盖numpy、pandas、scipy等九个主流数据科学、机器学习及高性能计算仓库的498项任务：给定完整代码库与低速工作负载，智能体需解析代码语义、定位性能瓶颈及相关测试，并生成在通过相同单元测试的同时达到或超越专家级加速效果的补丁。为实现这种“如何修复”的评估，我们通过自动化流水线从GitHub拉取请求中采集性能优化编辑记录，结合关键词过滤、静态分析、覆盖率工具与执行验证，既确认专家加速基准又识别相关仓库单元测试。对前沿智能体的实证评估显示其表现显著欠佳：平均仅达到专家加速效果的0.15倍。智能体在定位优化机会、跨函数执行推理及保持编辑正确性方面存在明显不足。我们公开此基准测试及相关数据流水线，以推动自动化性能工程与长周期软件推理的研究进展。",
    "url": "https://huggingface.co/papers/2511.06090",
    "arxiv_url": "https://arxiv.org/abs/2511.06090"
  },
  {
    "title": "Diffusion-SDPO: Safeguarded Direct Preference Optimization for Diffusion\n  Models",
    "summary": "Text-to-image diffusion models deliver high-quality images, yet aligning them\nwith human preferences remains challenging. We revisit diffusion-based Direct\nPreference Optimization (DPO) for these models and identify a critical\npathology: enlarging the preference margin does not necessarily improve\ngeneration quality. In particular, the standard Diffusion-DPO objective can\nincrease the reconstruction error of both winner and loser branches.\nConsequently, degradation of the less-preferred outputs can become sufficiently\nsevere that the preferred branch is also adversely affected even as the margin\ngrows. To address this, we introduce Diffusion-SDPO, a safeguarded update rule\nthat preserves the winner by adaptively scaling the loser gradient according to\nits alignment with the winner gradient. A first-order analysis yields a\nclosed-form scaling coefficient that guarantees the error of the preferred\noutput is non-increasing at each optimization step. Our method is simple,\nmodel-agnostic, broadly compatible with existing DPO-style alignment frameworks\nand adds only marginal computational overhead. Across standard text-to-image\nbenchmarks, Diffusion-SDPO delivers consistent gains over preference-learning\nbaselines on automated preference, aesthetic, and prompt alignment metrics.\nCode is publicly available at https://github.com/AIDC-AI/Diffusion-SDPO.",
    "translation": "标题：Diffusion-SDPO：扩散模型的安全直接偏好优化方法\n\n摘要：文本到图像扩散模型虽能生成高质量图像，但其与人类偏好的对齐仍具挑战性。本文重新审视基于扩散的直接偏好优化方法，发现关键缺陷：扩大偏好间隔未必能提升生成质量。具体而言，标准Diffusion-DPO目标函数可能同时增加优胜分支与劣汰分支的重构误差。随着偏好间隔扩大，劣汰输出的退化可能严重到对优胜分支产生负面影响。为此，我们提出Diffusion-SDPO——一种安全更新机制，通过根据劣汰梯度与优胜梯度的对齐程度进行自适应缩放，实现对优胜分支的保护。一阶分析推导出的闭式缩放系数可确保在每步优化中优先输出的误差保持非递增。该方法结构简洁、模型无关，可广泛兼容现有DPO式对齐框架，且仅增加边际计算开销。在标准文本到图像基准测试中，Diffusion-SDPO在自动化偏好评估、美学指标及提示词对齐度量方面均持续优于现有偏好学习基线。代码已公开于：https://github.com/AIDC-AI/Diffusion-SDPO。",
    "url": "https://huggingface.co/papers/2511.03317",
    "arxiv_url": "https://arxiv.org/abs/2511.03317"
  },
  {
    "title": "Generating an Image From 1,000 Words: Enhancing Text-to-Image With\n  Structured Captions",
    "summary": "Text-to-image models have rapidly evolved from casual creative tools to\nprofessional-grade systems, achieving unprecedented levels of image quality and\nrealism. Yet, most models are trained to map short prompts into detailed\nimages, creating a gap between sparse textual input and rich visual outputs.\nThis mismatch reduces controllability, as models often fill in missing details\narbitrarily, biasing toward average user preferences and limiting precision for\nprofessional use. We address this limitation by training the first open-source\ntext-to-image model on long structured captions, where every training sample is\nannotated with the same set of fine-grained attributes. This design maximizes\nexpressive coverage and enables disentangled control over visual factors. To\nprocess long captions efficiently, we propose DimFusion, a fusion mechanism\nthat integrates intermediate tokens from a lightweight LLM without increasing\ntoken length. We also introduce the Text-as-a-Bottleneck Reconstruction (TaBR)\nevaluation protocol. By assessing how well real images can be reconstructed\nthrough a captioning-generation loop, TaBR directly measures controllability\nand expressiveness, even for very long captions where existing evaluation\nmethods fail. Finally, we demonstrate our contributions by training the\nlarge-scale model FIBO, achieving state-of-the-art prompt alignment among\nopen-source models. Model weights are publicly available at\nhttps://huggingface.co/briaai/FIBO",
    "translation": "标题：千词成图：基于结构化描述的文本到图像生成增强方法\n\n摘要：文本到图像模型已从随意的创意工具迅速发展为专业级系统，实现了前所未有的图像质量与真实感。然而，大多数模型被训练用于将简短提示映射为细节图像，导致稀疏文本输入与丰富视觉输出之间产生落差。这种不匹配降低了可控性——模型往往随意补全缺失细节，偏向普通用户偏好，限制了专业应用的精确度。为解决这一局限，我们首次基于结构化长描述训练开源文本到图像模型，每个训练样本均标注有相同组的细粒度属性。该设计最大化表达覆盖度，并实现对视觉要素的解耦控制。为高效处理长描述，我们提出DimFusion融合机制，通过集成轻量级大语言模型的中间标记而不增加标记长度。同时提出文本瓶颈重建评估协议：通过评估真实图像在描述-生成循环中的重建质量，该协议可直接衡量可控性与表达力，即使在现有评估方法失效的超长描述场景下仍适用。最终，我们通过训练大规模模型FIBO验证贡献，在开源模型中实现了最先进的提示对齐效果。模型权重已公开于https://huggingface.co/briaai/FIBO",
    "url": "https://huggingface.co/papers/2511.06876",
    "arxiv_url": "https://arxiv.org/abs/2511.06876"
  },
  {
    "title": "VADER: Towards Causal Video Anomaly Understanding with Relation-Aware\n  Large Language Models",
    "summary": "Video anomaly understanding (VAU) aims to provide detailed interpretation and\nsemantic comprehension of anomalous events within videos, addressing\nlimitations of traditional methods that focus solely on detecting and\nlocalizing anomalies. However, existing approaches often neglect the deeper\ncausal relationships and interactions between objects, which are critical for\nunderstanding anomalous behaviors. In this paper, we propose VADER, an\nLLM-driven framework for Video Anomaly unDErstanding, which integrates keyframe\nobject Relation features with visual cues to enhance anomaly comprehension from\nvideo. Specifically, VADER first applies an Anomaly Scorer to assign per-frame\nanomaly scores, followed by a Context-AwarE Sampling (CAES) strategy to capture\nthe causal context of each anomalous event. A Relation Feature Extractor and a\nCOntrastive Relation Encoder (CORE) jointly model dynamic object interactions,\nproducing compact relational representations for downstream reasoning. These\nvisual and relational cues are integrated with LLMs to generate detailed,\ncausally grounded descriptions and support robust anomaly-related question\nanswering. Experiments on multiple real-world VAU benchmarks demonstrate that\nVADER achieves strong results across anomaly description, explanation, and\ncausal reasoning tasks, advancing the frontier of explainable video anomaly\nanalysis.",
    "translation": "标题：VADER：基于关系感知大语言模型的因果视频异常理解框架\n\n摘要：视频异常理解旨在对视频中的异常事件提供详细阐释与语义解析，突破传统方法仅关注异常检测与定位的局限性。然而现有研究往往忽略物体间深层的因果关系与交互作用，而这些要素对于理解异常行为至关重要。本文提出VADER——一种基于大语言模型的视频异常理解框架，通过融合关键帧物体关系特征与视觉线索来增强视频异常认知。具体而言，VADER首先运用异常评分器计算逐帧异常分值，继而采用上下文感知采样策略捕捉异常事件的因果语境。通过关系特征提取器与对比关系编码器的协同作用，系统建模动态物体交互并生成紧凑的关系表征以供下游推理。这些视觉与关系线索与大语言模型集成后，可生成具有因果依据的详细描述，并支持稳健的异常相关问答。在多个真实场景视频异常理解基准测试上的实验表明，VADER在异常描述、解释与因果推理任务中均取得优异结果，推动了可解释视频异常分析研究的前沿发展。",
    "url": "https://huggingface.co/papers/2511.07299",
    "arxiv_url": "https://arxiv.org/abs/2511.07299"
  },
  {
    "title": "Omni-AVSR: Towards Unified Multimodal Speech Recognition with Large\n  Language Models",
    "summary": "Large language models (LLMs) have recently achieved impressive results in\nspeech recognition across multiple modalities, including Auditory Speech\nRecognition (ASR), Visual Speech Recognition (VSR), and Audio-Visual Speech\nRecognition (AVSR). Despite this progress, current LLM-based approaches\ntypically address each task independently, training separate models that raise\ncomputational and deployment resource use while missing potential cross-task\nsynergies. They also rely on fixed-rate token compression, which restricts\nflexibility in balancing accuracy with efficiency. These limitations highlight\nthe need for a unified framework that can support ASR, VSR, and AVSR while\nenabling elastic inference. To this end, we present Omni-AVSR, a unified\naudio-visual LLM that combines efficient multi-granularity training with\nparameter-efficient adaptation. Specifically, we adapt the matryoshka\nrepresentation learning paradigm to efficiently train across multiple audio and\nvisual granularities, reducing its inherent training resource use. Furthermore,\nwe explore three LoRA-based strategies for adapting the backbone LLM, balancing\nshared and task-specific specialization. Experiments on LRS2 and LRS3 show that\nOmni-AVSR achieves comparable or superior accuracy to state-of-the-art\nbaselines while training a single model at substantially lower training and\ndeployment resource use. The model also remains robust under acoustic noise,\nand we analyze its scaling behavior as LLM size increases, providing insights\ninto the trade-off between performance and efficiency.",
    "translation": "标题：Omni-AVSR：基于大语言模型的统一多模态语音识别研究\n\n摘要：大语言模型近期在多模态语音识别领域取得显著进展，涵盖听觉语音识别、视觉语音识别与视听语音识别。然而现有基于大语言模型的方法通常独立处理各项任务，需训练独立模型导致计算与部署资源消耗增加，且未能充分利用跨任务协同潜力。这些方法还依赖固定速率的分词压缩机制，限制了精度与效率平衡的灵活性。这些局限性凸显了构建统一框架的必要性——既能支持多模态语音识别任务，又可实现弹性推理。为此，我们提出Omni-AVSR这一统一视听大语言模型，融合高效多粒度训练与参数有效性适配机制。具体而言，我们采用套娃表示学习范式实现多粒度视听数据的高效训练，显著降低固有训练资源消耗。此外，我们探索三种基于LoRA的主干网络适配策略，在共享性与任务特异性之间实现优化平衡。在LRS2和LRS3数据集上的实验表明，Omni-AVSR仅需训练单一模型即可达到与最先进基线模型相当或更优的准确率，同时大幅降低训练与部署资源消耗。该模型在噪声环境下保持稳健性能，我们通过分析模型规模扩展规律，为性能与效率的权衡关系提供了重要见解。",
    "url": "https://huggingface.co/papers/2511.07253",
    "arxiv_url": "https://arxiv.org/abs/2511.07253"
  },
  {
    "title": "LUT-LLM: Efficient Large Language Model Inference with Memory-based\n  Computations on FPGAs",
    "summary": "The rapid progress of large language models (LLMs) has advanced numerous\napplications, yet efficient single-batch inference remains vital for on-device\nintelligence. While FPGAs offer fine-grained data control and high energy\nefficiency, recent GPU optimizations have narrowed their advantage, especially\nunder arithmetic-based computation. To overcome this, we leverage FPGAs'\nabundant on-chip memory to shift LLM inference from arithmetic- to memory-based\ncomputation through table lookups. We present LUT-LLM, the first FPGA\naccelerator enabling 1B+ LLM inference via vector-quantized memory operations.\nOur analysis identifies activation-weight co-quantization as the most effective\nscheme, supported by (1) bandwidth-aware parallel centroid search, (2)\nefficient 2D table lookups, and (3) a spatial-temporal hybrid design minimizing\ndata caching. Implemented on an AMD V80 FPGA for a customized Qwen 3 1.7B\nmodel, LUT-LLM achieves 1.66x lower latency than AMD MI210 and 1.72x higher\nenergy efficiency than NVIDIA A100, scaling to 32B models with 2.16x efficiency\ngain over A100.",
    "translation": "标题：LUT-LLM：基于FPGA内存计算的高效大语言模型推理框架\n\n摘要：大语言模型的快速发展推动了众多应用场景的进步，然而高效的单批次推理对于设备端智能仍然至关重要。尽管FPGA具有细粒度数据控制和高能效的特性，但近期GPU优化已缩小了其优势，尤其在基于算术运算的场景下。为突破此限制，我们利用FPGA丰富的片上存储资源，通过查表操作将LLM推理从算术计算转向内存计算。本文提出LUT-LLM——首个通过向量化内存操作实现十亿参数级LLM推理的FPGA加速器。我们的分析表明激活-权重协同量化是最有效的方案，其技术支撑包括：（1）带宽感知并行质心搜索算法；（2）高效的二维查表机制；（3）最小化数据缓存的时空混合架构。在AMD V80 FPGA平台对定制化Qwen 3 1.7B模型的实测表明，LUT-LLM相较AMD MI210实现延迟降低1.66倍，相比NVIDIA A100能效提升1.72倍，并可扩展至320亿参数模型，相较A100持续保持2.16倍的能效优势。",
    "url": "https://huggingface.co/papers/2511.06174",
    "arxiv_url": "https://arxiv.org/abs/2511.06174"
  },
  {
    "title": "Reinforcement Learning Improves Traversal of Hierarchical Knowledge in LLMs",
    "summary": "Reinforcement learning (RL) is often credited with improving language model reasoning and generalization at the expense of degrading memorized knowledge. We challenge this narrative by observing that RL-enhanced models consistently outperform their base and supervised fine-tuned (SFT) counterparts on pure knowledge recall tasks, particularly those requiring traversal of hierarchical, structured knowledge (e.g., medical codes). We hypothesize these gains stem not from newly acquired data, but from improved procedural skills in navigating and searching existing knowledge hierarchies within the model parameters. To support this hypothesis, we show that structured prompting, which explicitly guides SFTed models through hierarchical traversal, recovers most of the performance gap (reducing 24pp to 7pp on MedConceptsQA for DeepSeek-V3/R1). We further find that while prompting improves final-answer accuracy, RL-enhanced models retain superior ability to recall correct procedural paths on deep-retrieval tasks. Finally our layer-wise internal activation analysis reveals that while factual representations (e.g., activations for the statement \"code 57.95 refers to urinary infection\") maintain high cosine similarity between SFT and RL models, query representations (e.g., \"what is code 57.95\") diverge noticeably, indicating that RL primarily transforms how models traverse knowledge rather than the knowledge representation itself.",
    "translation": "标题：强化学习提升大语言模型对层次化知识的遍历能力\n\n摘要：传统观点认为强化学习（RL）虽能提升语言模型的推理与泛化能力，却会损害其记忆知识。我们通过实验发现，在纯粹的知识召回任务（尤其是需要遍历层次化结构知识的任务，如医学代码查询）中，经RL增强的模型持续优于基础模型及监督微调（SFT）模型，从而对这一观点提出挑战。我们推测这些增益并非源于新获取的数据，而是来自模型在参数空间内导航和搜索既有知识层次结构的程序性技能提升。为验证该假设，我们证明采用结构化提示（显式引导SFT模型进行层次遍历）可弥补大部分性能差距（在MedConceptsQA任务中将DeepSeek-V3/R1的差距从24个百分点缩减至7个百分点）。进一步研究发现，虽然提示策略能提升最终答案准确率，但RL增强模型在深度检索任务中仍保持更优的正确程序路径召回能力。最后，通过分层内部激活分析发现：尽管事实表征（如“代码57.95指代尿路感染”的激活模式）在SFT与RL模型间保持较高余弦相似度，但查询表征（如“代码57.95的含义”）却呈现显著分化，这表明RL主要改变了模型遍历知识的方式，而非知识表征本身。",
    "url": "https://huggingface.co/papers/2511.05933",
    "arxiv_url": "https://arxiv.org/abs/2511.05933"
  }
]