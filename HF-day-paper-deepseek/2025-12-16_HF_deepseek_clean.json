[
  {
    "title": "ReFusion: A Diffusion Large Language Model with Parallel Autoregressive Decoding",
    "summary": "Autoregressive models (ARMs) are hindered by slow sequential inference. While masked diffusion models (MDMs) offer a parallel alternative, they suffer from critical drawbacks: high computational overhead from precluding Key-Value (KV) caching, and incoherent generation arising from learning dependencies over an intractable space of token combinations. To address these limitations, we introduce ReFusion, a novel masked diffusion model that achieves superior performance and efficiency by elevating parallel decoding from the token level to a higher slot level, where each slot is a fixed-length, contiguous sub-sequence. This is achieved through an iterative ``plan-and-infill'' decoding process: a diffusion-based planning step first identifies a set of weakly dependent slots, and an autoregressive infilling step then decodes these selected slots in parallel. The slot-based design simultaneously unlocks full KV cache reuse with a unified causal framework and reduces the learning complexity from the token combination space to a manageable slot-level permutation space. Extensive experiments on seven diverse benchmarks show that ReFusion not only overwhelmingly surpasses prior MDMs with 34% performance gains and an over 18times speedup on average, but also bridges the performance gap to strong ARMs while maintaining a 2.33times average speedup.",
    "translation": "标题：ReFusion：一种具备并行自回归解码能力的扩散大语言模型\n\n摘要：自回归模型因顺序推理速度缓慢而受到限制。虽然掩码扩散模型提供了一种并行替代方案，但其存在关键缺陷：因无法使用键值缓存而导致计算开销高昂，以及在难以处理的词元组合空间上学习依赖关系时产生不连贯的生成结果。为应对这些局限，本文提出ReFusion——一种新颖的掩码扩散模型，通过将并行解码从词元层级提升至更高维的槽位层级（每个槽位为固定长度的连续子序列），实现了卓越的性能与效率。该模型通过迭代式的“规划-填充”解码流程实现：基于扩散机制的规划步骤首先识别出一组弱依赖槽位，随后自回归填充步骤并行解码这些选定槽位。这种基于槽位的设计在统一因果框架下实现了完整的键值缓存复用，同时将学习复杂度从词元组合空间降至可管理的槽位排列空间。在七个多样化基准测试上的大量实验表明，ReFusion不仅以34%的性能提升和平均超18倍的加速比显著超越现有掩码扩散模型，更在保持平均2.33倍加速优势的同时，弥合了与强效自回归模型之间的性能差距。",
    "url": "https://huggingface.co/papers/2512.13586",
    "arxiv_url": "https://arxiv.org/abs/2512.13586"
  },
  {
    "title": "Towards Scalable Pre-training of Visual Tokenizers for Generation",
    "summary": "The quality of the latent space in visual tokenizers (e.g., VAEs) is crucial for modern generative models. However, the standard reconstruction-based training paradigm produces a latent space that is biased towards low-level information, leading to a foundation flaw: better pixel-level accuracy does not lead to higher-quality generation. This implies that pouring extensive compute into visual tokenizer pre-training translates poorly to improved performance in generation. We identify this as the ``pre-training scaling problem`` and suggest a necessary shift: to be effective for generation, a latent space must concisely represent high-level semantics. We present VTP, a unified visual tokenizer pre-training framework, pioneering the joint optimization of image-text contrastive, self-supervised, and reconstruction losses. Our large-scale study reveals two principal findings: (1) understanding is a key driver of generation, and (2) much better scaling properties, where generative performance scales effectively with compute, parameters, and data allocated to the pretraining of the visual tokenizer. After large-scale pre-training, our tokenizer delivers a competitive profile (78.2 zero-shot accuracy and 0.36 rFID on ImageNet) and 4.1 times faster convergence on generation compared to advanced distillation methods. More importantly, it scales effectively: without modifying standard DiT training specs, solely investing more FLOPS in pretraining VTP achieves 65.8\\% FID improvement in downstream generation, while conventional autoencoder stagnates very early at 1/10 FLOPS. Our pre-trained models are available at https://github.com/MiniMax-AI/VTP.",
    "translation": "标题：面向生成任务的可扩展视觉分词器预训练研究\n\n摘要：视觉分词器（如变分自编码器）的潜在空间质量对现代生成模型至关重要。然而，基于标准重建的训练范式产生的潜在空间偏向于低层次信息，这导致了一个根本性缺陷：像素级精度的提升并不能带来更高质量的生成结果。这意味着将大量计算资源投入视觉分词器预训练对生成性能的提升效果有限。我们将此定义为\"预训练扩展问题\"，并提出必要的研究转向：为有效支持生成任务，潜在空间必须简洁地表征高层次语义。本文提出VTP——一个统一的视觉分词器预训练框架，率先实现了图像-文本对比损失、自监督损失与重建损失的联合优化。我们的大规模实验揭示了两项核心发现：（1）理解能力是驱动生成性能的关键因素；（2）模型展现出更优越的扩展特性，生成性能随预训练投入的计算量、参数量和数据量实现有效提升。经过大规模预训练后，我们的分词器取得了具有竞争力的性能指标（ImageNet数据集上零样本准确率78.2%，rFID得分0.36），且相比先进蒸馏方法在生成任务上实现4.1倍收敛加速。更重要的是，该框架展现出卓越的扩展能力：在不改变标准DiT训练配置的情况下，仅通过增加VTP预训练的计算量即可在下游生成任务中获得65.8%的FID提升，而传统自编码器在仅使用十分之一计算量时性能便过早停滞。预训练模型已发布于https://github.com/MiniMax-AI/VTP。",
    "url": "https://huggingface.co/papers/2512.13687",
    "arxiv_url": "https://arxiv.org/abs/2512.13687"
  },
  {
    "title": "Memory in the Age of AI Agents",
    "summary": "Memory has emerged, and will continue to remain, a core capability of foundation model-based agents. As research on agent memory rapidly expands and attracts unprecedented attention, the field has also become increasingly fragmented. Existing works that fall under the umbrella of agent memory often differ substantially in their motivations, implementations, and evaluation protocols, while the proliferation of loosely defined memory terminologies has further obscured conceptual clarity. Traditional taxonomies such as long/short-term memory have proven insufficient to capture the diversity of contemporary agent memory systems. This work aims to provide an up-to-date landscape of current agent memory research. We begin by clearly delineating the scope of agent memory and distinguishing it from related concepts such as LLM memory, retrieval augmented generation (RAG), and context engineering. We then examine agent memory through the unified lenses of forms, functions, and dynamics. From the perspective of forms, we identify three dominant realizations of agent memory, namely token-level, parametric, and latent memory. From the perspective of functions, we propose a finer-grained taxonomy that distinguishes factual, experiential, and working memory. From the perspective of dynamics, we analyze how memory is formed, evolved, and retrieved over time. To support practical development, we compile a comprehensive summary of memory benchmarks and open-source frameworks. Beyond consolidation, we articulate a forward-looking perspective on emerging research frontiers, including memory automation, reinforcement learning integration, multimodal memory, multi-agent memory, and trustworthiness issues. We hope this survey serves not only as a reference for existing work, but also as a conceptual foundation for rethinking memory as a first-class primitive in the design of future agentic intelligence.",
    "translation": "标题：人工智能代理时代的记忆系统\n\n摘要：记忆已成为基于基础模型的智能代理的核心能力，并将持续保持这一地位。随着代理记忆研究迅速扩展并吸引前所未有的关注，该领域也日益呈现碎片化态势。现有关于代理记忆的研究在动机、实现方式和评估标准上存在显著差异，而定义松散的记忆术语激增进一步模糊了概念清晰度。传统分类法（如长/短期记忆）已不足以涵盖当代代理记忆系统的多样性。本文旨在系统梳理当前代理记忆研究的最新进展。首先明确界定代理记忆的范畴，并将其与大型语言模型记忆、检索增强生成（RAG）及上下文工程等相关概念进行区分。随后通过形式、功能与动态演化三个维度对代理记忆进行统一分析：在形式维度，我们识别出符号级记忆、参数化记忆和潜空间记忆三种主流实现方式；在功能维度，提出包含事实记忆、经验记忆与工作记忆的细粒度分类体系；在动态维度，深入探讨记忆的形成、演化与检索机制。为支持实际开发，本文汇编了完整的记忆基准测试集与开源框架综述。在整合现有成果的基础上，我们前瞻性地提出新兴研究方向，包括记忆自动化、强化学习融合、多模态记忆、多代理记忆及可信性问题。本研究不仅可作为现有工作的参考指南，更期望为将记忆重新定位为未来智能代理设计的核心要素提供概念基础。",
    "url": "https://huggingface.co/papers/2512.13564",
    "arxiv_url": "https://arxiv.org/abs/2512.13564"
  },
  {
    "title": "QwenLong-L1.5: Post-Training Recipe for Long-Context Reasoning and Memory Management",
    "summary": "We introduce QwenLong-L1.5, a model that achieves superior long-context reasoning capabilities through systematic post-training innovations. The key technical breakthroughs of QwenLong-L1.5 are as follows: (1) Long-Context Data Synthesis Pipeline: We develop a systematic synthesis framework that generates challenging reasoning tasks requiring multi-hop grounding over globally distributed evidence. By deconstructing documents into atomic facts and their underlying relationships, and then programmatically composing verifiable reasoning questions, our approach creates high-quality training data at scale, moving substantially beyond simple retrieval tasks to enable genuine long-range reasoning capabilities. (2) Stabilized Reinforcement Learning for Long-Context Training: To overcome the critical instability in long-context RL, we introduce task-balanced sampling with task-specific advantage estimation to mitigate reward bias, and propose Adaptive Entropy-Controlled Policy Optimization (AEPO) that dynamically regulates exploration-exploitation trade-offs. (3) Memory-Augmented Architecture for Ultra-Long Contexts: Recognizing that even extended context windows cannot accommodate arbitrarily long sequences, we develop a memory management framework with multi-stage fusion RL training that seamlessly integrates single-pass reasoning with iterative memory-based processing for tasks exceeding 4M tokens. Based on Qwen3-30B-A3B-Thinking, QwenLong-L1.5 achieves performance comparable to GPT-5 and Gemini-2.5-Pro on long-context reasoning benchmarks, surpassing its baseline by 9.90 points on average. On ultra-long tasks (1M~4M tokens), QwenLong-L1.5's memory-agent framework yields a 9.48-point gain over the agent baseline. Additionally, the acquired long-context reasoning ability translates to enhanced performance in general domains like scientific reasoning, memory tool using, and extended dialogue.",
    "translation": "标题：QwenLong-L1.5：面向长上下文推理与记忆管理的后训练方案\n\n摘要：本文介绍QwenLong-L1.5模型，该模型通过系统性的后训练创新实现了卓越的长上下文推理能力。QwenLong-L1.5的核心技术突破包括：（1）长上下文数据合成流程：我们开发了系统化的合成框架，可生成需要基于全局分布证据进行多跳溯因的复杂推理任务。通过将文档解构为原子事实及其内在关联，并以可编程方式组合可验证的推理问题，该方法实现了高质量训练数据的大规模生成，显著超越了简单检索任务，真正实现了长程推理能力。（2）面向长上下文训练的稳定强化学习：为克服长上下文强化学习中关键的不稳定性问题，我们引入基于任务平衡采样与任务特定优势估计的奖励偏差缓解机制，并提出自适应熵控策略优化方法，动态调节探索与利用的平衡。（3）超长上下文的记忆增强架构：针对扩展上下文窗口仍无法容纳无限长序列的局限，我们开发了具有多阶段融合强化训练的记忆管理框架，通过单次推理与基于记忆的迭代处理无缝结合，可处理超过400万标记的超长任务。基于Qwen3-30B-A3B-Thinking架构，QwenLong-L1.5在长上下文推理基准测试中达到与GPT-5和Gemini-2.5-Pro相当的性能，较基线模型平均提升9.90分。在超长任务（100万至400万标记）中，其记忆智能体框架较智能体基线提升9.48分。此外，所获得的长上下文推理能力可迁移至科学推理、记忆工具使用及长对话等通用领域，实现性能的全面提升。",
    "url": "https://huggingface.co/papers/2512.12967",
    "arxiv_url": "https://arxiv.org/abs/2512.12967"
  },
  {
    "title": "LongVie 2: Multimodal Controllable Ultra-Long Video World Model",
    "summary": "Building video world models upon pretrained video generation systems represents an important yet challenging step toward general spatiotemporal intelligence. A world model should possess three essential properties: controllability, long-term visual quality, and temporal consistency. To this end, we take a progressive approach-first enhancing controllability and then extending toward long-term, high-quality generation. We present LongVie 2, an end-to-end autoregressive framework trained in three stages: (1) Multi-modal guidance, which integrates dense and sparse control signals to provide implicit world-level supervision and improve controllability; (2) Degradation-aware training on the input frame, bridging the gap between training and long-term inference to maintain high visual quality; and (3) History-context guidance, which aligns contextual information across adjacent clips to ensure temporal consistency. We further introduce LongVGenBench, a comprehensive benchmark comprising 100 high-resolution one-minute videos covering diverse real-world and synthetic environments. Extensive experiments demonstrate that LongVie 2 achieves state-of-the-art performance in long-range controllability, temporal coherence, and visual fidelity, and supports continuous video generation lasting up to five minutes, marking a significant step toward unified video world modeling.",
    "translation": "标题：LongVie 2：多模态可控超长视频世界模型\n\n摘要：在预训练视频生成系统基础上构建视频世界模型是实现通用时空智能的重要且具有挑战性的步骤。一个理想的世界模型应具备三个核心特性：可控性、长期视觉质量与时间一致性。为此，我们采用渐进式研究路径——先提升可控性，再向长期高质量生成拓展。本文提出LongVie 2，这是一个通过三阶段训练构建的端到端自回归框架：（1）多模态引导机制，通过融合稠密与稀疏控制信号提供隐式世界级监督，显著增强可控性；（2）输入帧的退化感知训练，弥合训练与长期推理间的差距以维持高视觉质量；（3）历史上下文引导，通过对齐相邻片段间的语境信息确保时间连贯性。我们进一步构建了LongVGenBench综合评测基准，包含100段涵盖真实与合成场景的高清一分钟视频。大量实验表明，LongVie 2在长程可控性、时间连贯性与视觉保真度方面均达到最先进水平，并支持持续生成长达五分钟的视频，这标志着向统一视频世界建模迈出了重要一步。",
    "url": "https://huggingface.co/papers/2512.13604",
    "arxiv_url": "https://arxiv.org/abs/2512.13604"
  },
  {
    "title": "Finch: Benchmarking Finance & Accounting across Spreadsheet-Centric Enterprise Workflows",
    "summary": "We introduce a finance & accounting benchmark (Finch) for evaluating AI agents on real-world, enterprise-grade professional workflows -- interleaving data entry, structuring, formatting, web search, cross-file retrieval, calculation, modeling, validation, translation, visualization, and reporting. Finch is sourced from authentic enterprise workspaces at Enron (15,000 spreadsheets and 500,000 emails from 150 employees) and other financial institutions, preserving in-the-wild messiness across multimodal artifacts (text, tables, formulas, charts, code, and images) and spanning diverse domains such as budgeting, trading, and asset management.\n  We propose a workflow construction process that combines LLM-assisted discovery with expert annotation: (1) LLM-assisted, expert-verified derivation of workflows from real-world email threads and version histories of spreadsheet files, and (2) meticulous expert annotation for workflows, requiring over 700 hours of domain-expert effort. This yields 172 composite workflows with 384 tasks, involving 1,710 spreadsheets with 27 million cells, along with PDFs and other artifacts, capturing the intrinsically messy, long-horizon, knowledge-intensive, and collaborative nature of real-world enterprise work.\n  We conduct both human and automated evaluations of frontier AI systems including GPT 5.1, Claude Sonnet 4.5, Gemini 3 Pro, Grok 4, and Qwen 3 Max, and GPT 5.1 Pro spends 48 hours in total yet passes only 38.4% of workflows, while Claude Sonnet 4.5 passes just 25.0%. Comprehensive case studies further surface the challenges that real-world enterprise workflows pose for AI agents.",
    "translation": "标题：Finch：以电子表格为核心的企业工作流财务与会计基准测试\n\n摘要：我们提出了一个财务与会计基准测试（Finch），用于评估人工智能代理在真实企业级专业工作流中的表现——这些工作流融合了数据录入、结构化、格式化、网络搜索、跨文件检索、计算、建模、验证、翻译、可视化与报告生成等多种任务。Finch的数据来源于安然公司（包含150名员工的15,000份电子表格和50万封电子邮件）及其他金融机构的真实企业工作环境，完整保留了多模态材料（文本、表格、公式、图表、代码和图像）中实际存在的杂乱性，并涵盖预算编制、交易与资产管理等多个领域。\n\n我们提出了一种结合大语言模型辅助发现与专家标注的工作流构建流程：（1）通过大语言模型辅助、专家验证的方式，从真实电子邮件线程与电子表格文件版本历史中推导工作流；（2）由领域专家对工作流进行精细标注，累计投入超过700小时的专业工作量。该流程最终构建出包含384项任务的172个复合工作流，涉及1,710份总计2,700万个单元格的电子表格，以及PDF等其他材料，真实体现了企业工作中固有的杂乱性、长期性、知识密集性与协作性特征。\n\n我们对包括GPT 5.1、Claude Sonnet 4.5、Gemini 3 Pro、Grok 4和Qwen 3 Max在内的前沿人工智能系统进行了人工与自动化评估。结果显示，GPT 5.1 Pro耗时48小时仅能通过38.4%的工作流，而Claude Sonnet 4.5仅通过25.0%。深入的案例研究进一步揭示了真实企业工作流为人工智能代理带来的核心挑战。",
    "url": "https://huggingface.co/papers/2512.13168",
    "arxiv_url": "https://arxiv.org/abs/2512.13168"
  },
  {
    "title": "NL2Repo-Bench: Towards Long-Horizon Repository Generation Evaluation of Coding Agents",
    "summary": "Recent advances in coding agents suggest rapid progress toward autonomous software development, yet existing benchmarks fail to rigorously evaluate the long-horizon capabilities required to build complete software systems. Most prior evaluations focus on localized code generation, scaffolded completion, or short-term repair tasks, leaving open the question of whether agents can sustain coherent reasoning, planning, and execution over the extended horizons demanded by real-world repository construction. To address this gap, we present NL2Repo Bench, a benchmark explicitly designed to evaluate the long-horizon repository generation ability of coding agents. Given only a single natural-language requirements document and an empty workspace, agents must autonomously design the architecture, manage dependencies, implement multi-module logic, and produce a fully installable Python library. Our experiments across state-of-the-art open- and closed-source models reveal that long-horizon repository generation remains largely unsolved: even the strongest agents achieve below 40% average test pass rates and rarely complete an entire repository correctly. Detailed analysis uncovers fundamental long-horizon failure modes, including premature termination, loss of global coherence, fragile cross-file dependencies, and inadequate planning over hundreds of interaction steps. NL2Repo Bench establishes a rigorous, verifiable testbed for measuring sustained agentic competence and highlights long-horizon reasoning as a central bottleneck for the next generation of autonomous coding agents.",
    "translation": "标题：NL2Repo-Bench：面向代码智能体长周期仓库生成能力的评估基准\n\n摘要：代码智能体的最新进展表明自主软件开发正快速推进，但现有基准测试未能严格评估构建完整软件系统所需的长周期能力。以往评估多集中于局部代码生成、框架补全或短期修复任务，导致智能体能否在现实仓库构建所要求的扩展周期内保持连贯推理、规划与执行的问题尚未得到解答。为填补这一空白，我们提出NL2Repo Bench——一个专门用于评估代码智能体长周期仓库生成能力的基准测试。该测试仅提供一个自然语言需求文档与空白工作空间，要求智能体自主设计架构、管理依赖项、实现多模块逻辑，并最终生成可完整安装的Python库。通过对前沿开源与闭源模型的实验发现，长周期仓库生成任务总体上仍未得到解决：即使性能最强的智能体平均测试通过率也低于40%，且极少能完整正确地生成整个仓库。深入分析揭示了根本性的长周期失效模式，包括过早终止、全局一致性丧失、脆弱的跨文件依赖关系，以及在数百个交互步骤中规划能力不足等问题。NL2Repo Bench为衡量智能体持续自主能力建立了严谨可验证的测试平台，并凸显出长周期推理能力是下一代自主代码智能体发展的核心瓶颈。",
    "url": "https://huggingface.co/papers/2512.12730",
    "arxiv_url": "https://arxiv.org/abs/2512.12730"
  },
  {
    "title": "Error-Free Linear Attention is a Free Lunch: Exact Solution from Continuous-Time Dynamics",
    "summary": "Linear-time attention and State Space Models (SSMs) promise to solve the quadratic cost bottleneck in long-context language models employing softmax attention. We introduce Error-Free Linear Attention (EFLA), a numerically stable, fully parallelism and generalized formulation of the delta rule. Specifically, we formulate the online learning update as a continuous-time dynamical system and prove that its exact solution is not only attainable but also computable in linear time with full parallelism. By leveraging the rank-1 structure of the dynamics matrix, we directly derive the exact closed-form solution effectively corresponding to the infinite-order Runge-Kutta method. This attention mechanism is theoretically free from error accumulation, perfectly capturing the continuous dynamics while preserving the linear-time complexity. Through an extensive suite of experiments, we show that EFLA enables robust performance in noisy environments, achieving lower language modeling perplexity and superior downstream benchmark performance than DeltaNet without introducing additional parameters. Our work provides a new theoretical foundation for building high-fidelity, scalable linear-time attention models.",
    "translation": "标题：无误差线性注意力是免费午餐：基于连续时间动力学的精确解\n\n摘要：线性时间注意力与状态空间模型有望解决采用softmax注意力的长上下文语言模型中的二次计算成本瓶颈。本文提出无误差线性注意力——一种数值稳定、完全可并行化且广义化的增量规则形式化方法。具体而言，我们将在线学习更新构建为连续时间动力系统，并证明其精确解不仅可获取，还能以线性时间复杂度和完全并行化方式计算。通过利用动力学矩阵的秩-1结构，我们直接推导出有效对应无限阶龙格-库塔方法的精确闭式解。该注意力机制理论上不存在误差累积，在保持线性时间复杂度的同时完美捕捉连续动力学特性。通过大量实验验证，EFLA在噪声环境中表现出鲁棒性能，在不引入额外参数的情况下，相比DeltaNet实现了更低的语言建模困惑度和更优的下游基准性能。本研究为构建高保真、可扩展的线性时间注意力模型提供了新的理论基础。",
    "url": "https://huggingface.co/papers/2512.12602",
    "arxiv_url": "https://arxiv.org/abs/2512.12602"
  },
  {
    "title": "KlingAvatar 2.0 Technical Report",
    "summary": "Avatar video generation models have achieved remarkable progress in recent years. However, prior work exhibits limited efficiency in generating long-duration high-resolution videos, suffering from temporal drifting, quality degradation, and weak prompt following as video length increases. To address these challenges, we propose KlingAvatar 2.0, a spatio-temporal cascade framework that performs upscaling in both spatial resolution and temporal dimension. The framework first generates low-resolution blueprint video keyframes that capture global semantics and motion, and then refines them into high-resolution, temporally coherent sub-clips using a first-last frame strategy, while retaining smooth temporal transitions in long-form videos. To enhance cross-modal instruction fusion and alignment in extended videos, we introduce a Co-Reasoning Director composed of three modality-specific large language model (LLM) experts. These experts reason about modality priorities and infer underlying user intent, converting inputs into detailed storylines through multi-turn dialogue. A Negative Director further refines negative prompts to improve instruction alignment. Building on these components, we extend the framework to support ID-specific multi-character control. Extensive experiments demonstrate that our model effectively addresses the challenges of efficient, multimodally aligned long-form high-resolution video generation, delivering enhanced visual clarity, realistic lip-teeth rendering with accurate lip synchronization, strong identity preservation, and coherent multimodal instruction following.",
    "translation": "标题：KlingAvatar 2.0技术报告\n\n摘要：近年来，虚拟形象视频生成模型取得了显著进展。然而，现有方法在生成长时长高分辨率视频时效率有限，随着视频长度增加，容易出现时序漂移、质量下降以及提示跟随能力弱等问题。为应对这些挑战，我们提出了KlingAvatar 2.0——一个在空间分辨率和时间维度上进行级联放大的时空级联框架。该框架首先生成捕捉全局语义与运动的低分辨率蓝图视频关键帧，随后通过首尾帧策略将其细化为高分辨率、时序连贯的视频片段，同时保持长视频中平滑的时间过渡。为增强长视频中的跨模态指令融合与对齐，我们引入了由三个模态专用大语言模型专家组成的协同推理导演模块。这些专家通过多轮对话推理模态优先级并推断潜在用户意图，将输入转化为详细的故事线。负面指令导演模块进一步优化负面提示以提升指令对齐效果。基于这些组件，我们将框架扩展至支持特定身份的多角色控制。大量实验表明，我们的模型能有效解决高效、多模态对齐的长时长高分辨率视频生成难题，在视觉清晰度、具有精确唇形同步的逼真唇齿渲染、强身份保持以及连贯的多模态指令跟随方面均表现出显著提升。",
    "url": "https://huggingface.co/papers/2512.13313",
    "arxiv_url": "https://arxiv.org/abs/2512.13313"
  },
  {
    "title": "MentraSuite: Post-Training Large Language Models for Mental Health Reasoning and Assessment",
    "summary": "Mental health disorders affect hundreds of millions globally, and the Web now serves as a primary medium for accessing support, information, and assessment. Large language models (LLMs) offer scalable and accessible assistance, yet their deployment in mental-health settings remains risky when their reasoning is incomplete, inconsistent, or ungrounded. Existing psychological LLMs emphasize emotional understanding or knowledge recall but overlook the step-wise, clinically aligned reasoning required for appraisal, diagnosis, intervention planning, abstraction, and verification. To address these issues, we introduce MentraSuite, a unified framework for advancing reliable mental-health reasoning. We propose MentraBench, a comprehensive benchmark spanning five core reasoning aspects, six tasks, and 13 datasets, evaluating both task performance and reasoning quality across five dimensions: conciseness, coherence, hallucination avoidance, task understanding, and internal consistency. We further present Mindora, a post-trained model optimized through a hybrid SFT-RL framework with an inconsistency-detection reward to enforce faithful and coherent reasoning. To support training, we construct high-quality trajectories using a novel reasoning trajectory generation strategy, that strategically filters difficult samples and applies a structured, consistency-oriented rewriting process to produce concise, readable, and well-balanced trajectories. Across 20 evaluated LLMs, Mindora achieves the highest average performance on MentraBench and shows remarkable performances in reasoning reliability, demonstrating its effectiveness for complex mental-health scenarios.",
    "translation": "标题：MentraSuite：面向心理健康推理与评估的大语言模型后训练框架\n\n摘要：心理健康障碍影响着全球数亿人口，而网络已成为获取支持、信息与评估的主要渠道。大语言模型能够提供可扩展且易获取的辅助支持，但其在心理健康领域的应用仍存在风险，尤其是当模型推理过程存在不完整、不一致或缺乏依据时。现有心理学大语言模型多侧重于情感理解或知识复现，却忽视了评估、诊断、干预规划、抽象归纳及验证等环节所需的渐进式、临床导向的推理能力。为解决这些问题，本文提出MentraSuite——一个用于推进可靠心理健康推理的统一框架。我们构建了MentraBench综合评估基准，涵盖五个核心推理维度、六类任务及13个数据集，从简洁性、连贯性、幻觉规避、任务理解与内部一致性五个层面系统评估任务表现与推理质量。进一步，我们提出通过混合SFT-RL框架进行后训练的Mindora模型，该框架采用不一致性检测奖励机制以强化忠实且连贯的推理能力。为支持训练过程，我们设计了一种创新的推理轨迹生成策略，通过策略性筛选困难样本并实施结构化、一致性导向的重写流程，构建出简洁可读且均衡的高质量推理轨迹。在评估的20个大语言模型中，Mindora在MentraBench上取得了最优平均性能，并在推理可靠性方面表现突出，证明了其在复杂心理健康场景中的有效性。",
    "url": "https://huggingface.co/papers/2512.09636",
    "arxiv_url": "https://arxiv.org/abs/2512.09636"
  },
  {
    "title": "Openpi Comet: Competition Solution For 2025 BEHAVIOR Challenge",
    "summary": "The 2025 BEHAVIOR Challenge is designed to rigorously track progress toward solving long-horizon tasks by physical agents in simulated environments. BEHAVIOR-1K focuses on everyday household tasks that people most want robots to assist with and these tasks introduce long-horizon mobile manipulation challenges in realistic settings, bridging the gap between current research and real-world, human-centric applications. This report presents our solution to the 2025 BEHAVIOR Challenge in a very close 2nd place and substantially outperforms the rest of the submissions. Building on π_{0.5}, we focus on systematically building our solution by studying the effects of training techniques and data. Through careful ablations, we show the scaling power in pre-training and post-training phases for competitive performance. We summarize our practical lessons and design recommendations that we hope will provide actionable insights for the broader embodied AI community when adapting powerful foundation models to complex embodied scenarios.",
    "translation": "标题：Openpi Comet：2025年BEHAVIOR挑战赛竞赛方案\n\n摘要：2025年BEHAVIOR挑战赛旨在系统追踪物理智能体在仿真环境中解决长周期任务的研究进展。BEHAVIOR-1K聚焦于人们最期望机器人协助完成的日常家庭任务，这些任务在真实场景中引入了长周期移动操作挑战，从而弥合了当前研究与现实世界人本应用之间的差距。本报告介绍了我们在2025年BEHAVIOR挑战赛中以微弱差距获得亚军并显著优于其他参赛方案的解决方案。基于π_{0.5}框架，我们通过系统研究训练技术与数据的影响来构建解决方案。经过严谨的消融实验，我们证明了预训练与后训练阶段的扩展能力对提升竞赛性能的关键作用。我们总结了实践经验和设计建议，旨在为更广泛的具身智能社区在将强大基础模型适配复杂具身场景时提供可操作的参考。",
    "url": "https://huggingface.co/papers/2512.10071",
    "arxiv_url": "https://arxiv.org/abs/2512.10071"
  },
  {
    "title": "Spatial-Aware VLA Pretraining through Visual-Physical Alignment from Human Videos",
    "summary": "Vision-Language-Action (VLA) models provide a promising paradigm for robot learning by integrating visual perception with language-guided policy learning. However, most existing approaches rely on 2D visual inputs to perform actions in 3D physical environments, creating a significant gap between perception and action grounding. To bridge this gap, we propose a Spatial-Aware VLA Pretraining paradigm that performs explicit alignment between visual space and physical space during pretraining, enabling models to acquire 3D spatial understanding before robot policy learning. Starting from pretrained vision-language models, we leverage large-scale human demonstration videos to extract 3D visual and 3D action annotations, forming a new source of supervision that aligns 2D visual observations with 3D spatial reasoning. We instantiate this paradigm with VIPA-VLA, a dual-encoder architecture that incorporates a 3D visual encoder to augment semantic visual representations with 3D-aware features. When adapted to downstream robot tasks, VIPA-VLA achieves significantly improved grounding between 2D vision and 3D action, resulting in more robust and generalizable robotic policies.",
    "translation": "标题：基于人类视频视觉-物理空间对齐的空间感知视觉-语言-动作预训练\n\n摘要：视觉-语言-动作模型通过将视觉感知与语言引导的策略学习相结合，为机器人学习提供了前景广阔的范式。然而，现有方法大多依赖二维视觉输入在三维物理环境中执行动作，导致感知与动作落地之间存在显著鸿沟。为弥合这一差距，本文提出一种空间感知的视觉-语言-动作预训练范式，通过在预训练阶段显式对齐视觉空间与物理空间，使模型在机器人策略学习前即可获得三维空间理解能力。基于预训练的视觉-语言模型，我们利用大规模人类示范视频提取三维视觉标注与三维动作标注，构建出能够将二维视觉观测与三维空间推理相校准的新型监督信号。基于该范式，我们构建了VIPA-VLA模型——采用双编码器架构，通过引入三维视觉编码器将空间感知特征融入语义视觉表征。在适应下游机器人任务时，VIPA-VLA显著提升了二维视觉与三维动作的关联性，从而产生更具鲁棒性与泛化能力的机器人策略。",
    "url": "https://huggingface.co/papers/2512.13080",
    "arxiv_url": "https://arxiv.org/abs/2512.13080"
  },
  {
    "title": "WebOperator: Action-Aware Tree Search for Autonomous Agents in Web Environment",
    "summary": "LLM-based agents often operate in a greedy, step-by-step manner, selecting actions solely based on the current observation without considering long-term consequences or alternative paths. This lack of foresight is particularly problematic in web environments, which are only partially observable-limited to browser-visible content (e.g., DOM and UI elements)-where a single misstep often requires complex and brittle navigation to undo. Without an explicit backtracking mechanism, agents struggle to correct errors or systematically explore alternative paths. Tree-search methods provide a principled framework for such structured exploration, but existing approaches lack mechanisms for safe backtracking, making them prone to unintended side effects. They also assume that all actions are reversible, ignoring the presence of irreversible actions-limitations that reduce their effectiveness in realistic web tasks. To address these challenges, we introduce WebOperator, a tree-search framework that enables reliable backtracking and strategic exploration. Our method incorporates a best-first search strategy that ranks actions by both reward estimates and safety considerations, along with a robust backtracking mechanism that verifies the feasibility of previously visited paths before replaying them, preventing unintended side effects. To further guide exploration, WebOperator generates action candidates from multiple, varied reasoning contexts to ensure diverse and robust exploration, and subsequently curates a high-quality action set by filtering out invalid actions pre-execution and merging semantically equivalent ones. Experimental results on WebArena and WebVoyager demonstrate the effectiveness of WebOperator. On WebArena, WebOperator achieves a state-of-the-art 54.6% success rate with gpt-4o, underscoring the critical advantage of integrating strategic foresight with safe execution.",
    "translation": "标题：WebOperator：面向网页环境中自主智能体的动作感知树搜索方法\n\n摘要：基于大语言模型的智能体通常以贪婪的逐步方式运行，仅根据当前观察选择动作，而忽略长期后果或替代路径。这种前瞻性的缺失在网页环境中尤为突出——由于环境仅部分可观测（仅限于浏览器可见内容，如DOM和UI元素），单个错误步骤往往需要复杂且脆弱的导航操作才能撤销。若缺乏显式的回溯机制，智能体难以纠正错误或系统性地探索替代路径。树搜索方法为此类结构化探索提供了理论框架，但现有方法缺乏安全回溯机制，容易引发非预期的副作用。这些方法通常假设所有动作皆可逆，却忽略了实际网页任务中不可逆动作的存在，从而降低了其实用性。为应对这些挑战，我们提出了WebOperator——一种支持可靠回溯与策略性探索的树搜索框架。该方法融合了最佳优先搜索策略，通过奖励估计与安全性考量对动作进行排序，并配备鲁棒的回溯机制，在重放历史路径前验证其可行性，从而避免非预期副作用。为进一步引导探索过程，WebOperator从多样化推理语境中生成候选动作集，以确保探索的多样性与鲁棒性，随后通过预执行过滤无效动作及合并语义等价动作，构建高质量动作集合。在WebArena和WebVoyager平台上的实验结果表明了WebOperator的有效性。在WebArena测试中，WebOperator结合gpt-4o模型实现了54.6%的最优成功率，凸显了策略性前瞻与安全执行相结合的关键优势。",
    "url": "https://huggingface.co/papers/2512.12692",
    "arxiv_url": "https://arxiv.org/abs/2512.12692"
  },
  {
    "title": "DrivePI: Spatial-aware 4D MLLM for Unified Autonomous Driving Understanding, Perception, Prediction and Planning",
    "summary": "Although multi-modal large language models (MLLMs) have shown strong capabilities across diverse domains, their application in generating fine-grained 3D perception and prediction outputs in autonomous driving remains underexplored. In this paper, we propose DrivePI, a novel spatial-aware 4D MLLM that serves as a unified Vision-Language-Action (VLA) framework that is also compatible with vision-action (VA) models. Our method jointly performs spatial understanding, 3D perception (i.e., 3D occupancy), prediction (i.e., occupancy flow), and planning (i.e., action outputs) in parallel through end-to-end optimization. To obtain both precise geometric information and rich visual appearance, our approach integrates point clouds, multi-view images, and language instructions within a unified MLLM architecture. We further develop a data engine to generate text-occupancy and text-flow QA pairs for 4D spatial understanding. Remarkably, with only a 0.5B Qwen2.5 model as MLLM backbone, DrivePI as a single unified model matches or exceeds both existing VLA models and specialized VA models. Specifically, compared to VLA models, DrivePI outperforms OpenDriveVLA-7B by 2.5% mean accuracy on nuScenes-QA and reduces collision rate by 70% over ORION (from 0.37% to 0.11%) on nuScenes. Against specialized VA models, DrivePI surpasses FB-OCC by 10.3 RayIoU for 3D occupancy on OpenOcc, reduces the mAVE from 0.591 to 0.509 for occupancy flow on OpenOcc, and achieves 32% lower L2 error than VAD (from 0.72m to 0.49m) for planning on nuScenes. Code will be available at https://github.com/happinesslz/DrivePI",
    "translation": "标题：DrivePI：面向统一自动驾驶理解、感知、预测与规划的空间感知四维多模态大语言模型\n\n摘要：尽管多模态大语言模型（MLLMs）已在多个领域展现出强大能力，但其在自动驾驶中生成细粒度三维感知与预测输出的应用仍待深入探索。本文提出DrivePI，一种新颖的空间感知四维MLLM，作为一个统一的视觉-语言-动作（VLA）框架，同时兼容视觉-动作（VA）模型。我们的方法通过端到端优化并行实现空间理解、三维感知（即三维占据）、预测（即占据流）与规划（即动作输出）。为同时获取精确的几何信息与丰富的视觉外观，本方法将点云、多视角图像及语言指令整合至统一的MLLM架构中。我们进一步开发了数据引擎，用于生成面向四维空间理解的文本-占据与文本-流问答对。值得注意的是，仅以0.5B参数的Qwen2.5模型作为MLLM骨干，DrivePI作为单一统一模型，其性能达到或超越了现有VLA模型与专用VA模型。具体而言，相较于VLA模型，DrivePI在nuScenes-QA数据集上的平均准确率比OpenDriveVLA-7B高出2.5%，并在nuScenes数据集上将碰撞率较ORION降低了70%（从0.37%降至0.11%）。相比专用VA模型，DrivePI在OpenOcc数据集上的三维占据任务中，RayIoU指标超过FB-OCC模型10.3个点；在OpenOcc的占据流任务中，将mAVE从0.591降至0.509；在nuScenes的规划任务中，其L2误差比VAD模型降低32%（从0.72米降至0.49米）。代码将在https://github.com/happinesslz/DrivePI 公开。",
    "url": "https://huggingface.co/papers/2512.12799",
    "arxiv_url": "https://arxiv.org/abs/2512.12799"
  },
  {
    "title": "V-REX: Benchmarking Exploratory Visual Reasoning via Chain-of-Questions",
    "summary": "While many vision-language models (VLMs) are developed to answer well-defined, straightforward questions with highly specified targets, as in most benchmarks, they often struggle in practice with complex open-ended tasks, which usually require multiple rounds of exploration and reasoning in the visual space. Such visual thinking paths not only provide step-by-step exploration and verification as an AI detective but also produce better interpretations of the final answers. However, these paths are challenging to evaluate due to the large exploration space of intermediate steps. To bridge the gap, we develop an evaluation suite, ``Visual Reasoning with multi-step EXploration (V-REX)'', which is composed of a benchmark of challenging visual reasoning tasks requiring native multi-step exploration and an evaluation protocol. V-REX covers rich application scenarios across diverse domains. V-REX casts the multi-step exploratory reasoning into a Chain-of-Questions (CoQ) and disentangles VLMs' capability to (1) Planning: breaking down an open-ended task by selecting a chain of exploratory questions; and (2) Following: answering curated CoQ sequentially to collect information for deriving the final answer. By curating finite options of questions and answers per step, V-REX achieves a reliable quantitative and fine-grained analysis of the intermediate steps. By assessing SOTA proprietary and open-sourced VLMs, we reveal consistent scaling trends, significant differences between planning and following abilities, and substantial room for improvement in multi-step exploratory reasoning.",
    "translation": "标题：V-REX：基于问题链的探索性视觉推理基准测试\n\n摘要：尽管许多视觉语言模型（VLMs）被开发用于回答如大多数基准测试中那样目标明确、直接的问题，但在实践中，它们往往难以应对复杂的开放式任务。这类任务通常需要在视觉空间中进行多轮探索与推理。此类视觉思维路径不仅能够像AI侦探一样提供逐步探索与验证，还能为最终答案生成更优的解释。然而，由于中间步骤的探索空间庞大，这些路径的评估颇具挑战。为弥合这一差距，我们开发了一个评估套件——“多步探索视觉推理（V-REX）”，它包含一个需要原生多步探索的挑战性视觉推理任务基准以及一套评估协议。V-REX涵盖了跨领域的丰富应用场景。该框架将多步探索性推理转化为“问题链”（CoQ），并解构了VLMs在以下两方面的能力：（1）规划：通过选择一系列探索性问题来分解开放式任务；（2）执行：依次回答精心设计的问题链，以收集信息并推导最终答案。通过为每一步精心设计有限的问题与答案选项，V-REX实现了对中间步骤可靠、定量且细粒度的分析。通过对当前最先进的专有及开源VLMs进行评估，我们揭示了其一致的能力扩展趋势、规划与执行能力间的显著差异，以及多步探索性推理方面存在的巨大改进空间。",
    "url": "https://huggingface.co/papers/2512.11995",
    "arxiv_url": "https://arxiv.org/abs/2512.11995"
  },
  {
    "title": "Toward Ambulatory Vision: Learning Visually-Grounded Active View Selection",
    "summary": "Vision Language Models (VLMs) excel at visual question answering (VQA) but remain limited to snapshot vision, reasoning from static images. In contrast, embodied agents require ambulatory vision, actively moving to obtain more informative views. We introduce Visually Grounded Active View Selection (VG-AVS), a task that selects the most informative next viewpoint using only the visual information in the current image, without relying on scene memory or external knowledge. To support this task, we construct a synthetic dataset with automatically generated paired query-target views and question-answer prompts. We also propose a framework that fine-tunes pretrained VLMs through supervised fine-tuning (SFT) followed by RL-based policy optimization. Our approach achieves strong question answering performance based on viewpoint selection and generalizes robustly to unseen synthetic and real scenes. Furthermore, incorporating our learned VG-AVS framework into existing scene-exploration-based EQA systems improves downstream question-answering accuracy.",
    "translation": "标题：迈向动态视觉：学习基于视觉的主动视角选择\n\n摘要：视觉语言模型在视觉问答任务中表现出色，但仍受限于静态视觉感知，仅能基于单张图像进行推理。相比之下，具身智能体需要动态视觉能力，通过主动移动获取信息量更丰富的观察视角。本文提出视觉驱动的主动视角选择任务，该任务仅利用当前图像的视觉信息选择信息量最大的下一观测视角，无需依赖场景记忆或外部知识。为支持该任务研究，我们构建了一个合成数据集，其中包含自动生成的配对查询-目标视角及问答提示。同时，我们提出一个通过监督微调与基于强化学习的策略优化两阶段框架来微调预训练视觉语言模型。该方法在基于视角选择的问答任务中取得优异性能，并能稳健地泛化至未见过的合成场景与真实场景。此外，将本框架集成至现有基于场景探索的具身问答系统中，可有效提升下游问答任务的准确率。",
    "url": "https://huggingface.co/papers/2512.13250",
    "arxiv_url": "https://arxiv.org/abs/2512.13250"
  },
  {
    "title": "Image Diffusion Preview with Consistency Solver",
    "summary": "The slow inference process of image diffusion models significantly degrades interactive user experiences. To address this, we introduce Diffusion Preview, a novel paradigm employing rapid, low-step sampling to generate preliminary outputs for user evaluation, deferring full-step refinement until the preview is deemed satisfactory. Existing acceleration methods, including training-free solvers and post-training distillation, struggle to deliver high-quality previews or ensure consistency between previews and final outputs. We propose ConsistencySolver derived from general linear multistep methods, a lightweight, trainable high-order solver optimized via Reinforcement Learning, that enhances preview quality and consistency. Experimental results demonstrate that ConsistencySolver significantly improves generation quality and consistency in low-step scenarios, making it ideal for efficient preview-and-refine workflows. Notably, it achieves FID scores on-par with Multistep DPM-Solver using 47% fewer steps, while outperforming distillation baselines. Furthermore, user studies indicate our approach reduces overall user interaction time by nearly 50% while maintaining generation quality. Code is available at https://github.com/G-U-N/consolver.",
    "translation": "标题：基于一致性求解器的图像扩散预览方法\n\n摘要：图像扩散模型的缓慢推理过程严重影响了交互式用户体验。为此，我们提出扩散预览这一新范式，通过快速低步数采样生成初步输出供用户评估，仅在预览结果满意后才进行全步数精细化处理。现有加速方法（包括免训练求解器和训练后蒸馏技术）难以同时实现高质量预览并确保预览与最终输出的一致性。我们提出基于通用线性多步方法推导的一致性求解器，这是一种通过强化学习优化的轻量级可训练高阶求解器，能够显著提升预览质量与一致性。实验结果表明，一致性求解器在低步数场景下显著提高了生成质量与一致性，使其成为高效预览-优化工作流程的理想选择。该方法仅需比多步DPM-Solver减少47%的步数即可达到相当的FID分数，同时性能优于蒸馏基线方法。此外，用户研究表明我们的方法在保持生成质量的同时，将整体用户交互时间减少近50%。代码已开源：https://github.com/G-U-N/consolver。",
    "url": "https://huggingface.co/papers/2512.13592",
    "arxiv_url": "https://arxiv.org/abs/2512.13592"
  },
  {
    "title": "VLSA: Vision-Language-Action Models with Plug-and-Play Safety Constraint Layer",
    "summary": "Vision-Language-Action (VLA) models have demonstrated remarkable capabilities in generalizing across diverse robotic manipulation tasks. However, deploying these models in unstructured environments remains challenging due to the critical need for simultaneous task compliance and safety assurance, particularly in preventing potential collisions during physical interactions. In this work, we introduce a Vision-Language-Safe Action (VLSA) architecture, named AEGIS, which contains a plug-and-play safety constraint (SC) layer formulated via control barrier functions. AEGIS integrates directly with existing VLA models to improve safety with theoretical guarantees, while maintaining their original instruction-following performance. To evaluate the efficacy of our architecture, we construct a comprehensive safety-critical benchmark SafeLIBERO, spanning distinct manipulation scenarios characterized by varying degrees of spatial complexity and obstacle intervention. Extensive experiments demonstrate the superiority of our method over state-of-the-art baselines. Notably, AEGIS achieves a 59.16% improvement in obstacle avoidance rate while substantially increasing the task execution success rate by 17.25%. To facilitate reproducibility and future research, we make our code, models, and the benchmark datasets publicly available at https://vlsa-aegis.github.io/.",
    "translation": "标题：VLSA：具备即插即用安全约束层的视觉-语言-动作模型\n\n摘要：视觉-语言-动作模型在多样化机器人操作任务中展现出卓越的泛化能力。然而，由于需同时满足任务执行与安全保障的严格要求（特别是在物理交互中防止潜在碰撞），在非结构化环境中部署此类模型仍面临挑战。本研究提出一种名为AEGIS的视觉-语言-安全动作架构，该架构通过控制屏障函数构建了即插即用式安全约束层。AEGIS可直接与现有VLA模型集成，在保持原有指令跟随性能的同时，通过理论保障提升系统安全性。为评估该架构效能，我们构建了涵盖不同空间复杂度与障碍物干预特征的综合性安全关键基准测试集SafeLIBERO。大量实验证明，该方法优于当前最先进的基线模型。值得注意的是，AEGIS在障碍物规避率上实现59.16%的提升，同时将任务执行成功率显著提高17.25%。为促进可复现性与后续研究，我们已通过https://vlsa-aegis.github.io/公开代码、模型及基准数据集。",
    "url": "https://huggingface.co/papers/2512.11891",
    "arxiv_url": "https://arxiv.org/abs/2512.11891"
  },
  {
    "title": "Aesthetic Alignment Risks Assimilation: How Image Generation and Reward Models Reinforce Beauty Bias and Ideological \"Censorship\"",
    "summary": "Over-aligning image generation models to a generalized aesthetic preference conflicts with user intent, particularly when ``anti-aesthetic\" outputs are requested for artistic or critical purposes. This adherence prioritizes developer-centered values, compromising user autonomy and aesthetic pluralism. We test this bias by constructing a wide-spectrum aesthetics dataset and evaluating state-of-the-art generation and reward models. We find that aesthetic-aligned generation models frequently default to conventionally beautiful outputs, failing to respect instructions for low-quality or negative imagery. Crucially, reward models penalize anti-aesthetic images even when they perfectly match the explicit user prompt. We confirm this systemic bias through image-to-image editing and evaluation against real abstract artworks.",
    "translation": "标题：美学对齐风险同化：图像生成与奖励模型如何强化审美偏见与意识形态“审查”\n\n摘要：将图像生成模型过度对齐于广义审美偏好会与用户意图相冲突，尤其在用户出于艺术或批判目的要求生成“反审美”内容时。这种对齐机制优先考虑以开发者为中心的价值观，损害了用户自主权与审美多元性。我们通过构建广谱美学数据集并评估前沿生成模型与奖励模型来检验这一偏见。研究发现，经美学对齐的生成模型常默认输出符合传统审美的内容，无法响应生成低质量或负面意象的指令。关键问题在于，即使反审美图像完全符合用户明确指令，奖励模型仍会对其施加惩罚。我们通过图像到图像编辑任务，并对照真实抽象艺术作品进行评估，证实了这一系统性偏见的存在。",
    "url": "https://huggingface.co/papers/2512.11883",
    "arxiv_url": "https://arxiv.org/abs/2512.11883"
  },
  {
    "title": "Towards Interactive Intelligence for Digital Humans",
    "summary": "We introduce Interactive Intelligence, a novel paradigm of digital human that is capable of personality-aligned expression, adaptive interaction, and self-evolution. To realize this, we present Mio (Multimodal Interactive Omni-Avatar), an end-to-end framework composed of five specialized modules: Thinker, Talker, Face Animator, Body Animator, and Renderer. This unified architecture integrates cognitive reasoning with real-time multimodal embodiment to enable fluid, consistent interaction. Furthermore, we establish a new benchmark to rigorously evaluate the capabilities of interactive intelligence. Extensive experiments demonstrate that our framework achieves superior performance compared to state-of-the-art methods across all evaluated dimensions. Together, these contributions move digital humans beyond superficial imitation toward intelligent interaction.",
    "translation": "标题：迈向数字人的交互式智能\n\n摘要：本文提出“交互式智能”这一数字人新范式，其具备人格对齐的表达能力、自适应交互特性与自我进化机制。为实现该范式，我们构建了Mio（多模态交互式全能化身）——一个由五大专用模块组成的端到端框架：思维模块、语音模块、面部动画模块、身体动画模块与渲染模块。该统一架构将认知推理与实时多模态具身表达相融合，实现了流畅且连贯的交互体验。此外，我们建立了一套全新基准体系，用于系统评估交互式智能的核心能力。大量实验表明，本框架在所有评估维度上均优于当前最先进方法。这些成果共同推动数字人从表层模仿迈向真正的智能交互。\n\n标题：迈向数字人的交互式智能\n摘要：本文提出“交互式智能”这一数字人新范式，其具备人格对齐的表达能力、自适应交互特性与自我进化机制。为实现该范式，我们构建了Mio（多模态交互式全能化身）——一个由五大专用模块组成的端到端框架：思维模块、语音模块、面部动画模块、身体动画模块与渲染模块。该统一架构将认知推理与实时多模态具身表达相融合，实现了流畅且连贯的交互体验。此外，我们建立了一套全新基准体系，用于系统评估交互式智能的核心能力。大量实验表明，本框架在所有评估维度上均优于当前最先进方法。这些成果共同推动数字人从表层模仿迈向真正的智能交互。",
    "url": "https://huggingface.co/papers/2512.13674",
    "arxiv_url": "https://arxiv.org/abs/2512.13674"
  },
  {
    "title": "GenieDrive: Towards Physics-Aware Driving World Model with 4D Occupancy Guided Video Generation",
    "summary": "Physics-aware driving world model is essential for drive planning, out-of-distribution data synthesis, and closed-loop evaluation. However, existing methods often rely on a single diffusion model to directly map driving actions to videos, which makes learning difficult and leads to physically inconsistent outputs. To overcome these challenges, we propose GenieDrive, a novel framework designed for physics-aware driving video generation. Our approach starts by generating 4D occupancy, which serves as a physics-informed foundation for subsequent video generation. 4D occupancy contains rich physical information, including high-resolution 3D structures and dynamics. To facilitate effective compression of such high-resolution occupancy, we propose a VAE that encodes occupancy into a latent tri-plane representation, reducing the latent size to only 58% of that used in previous methods. We further introduce Mutual Control Attention (MCA) to accurately model the influence of control on occupancy evolution, and we jointly train the VAE and the subsequent prediction module in an end-to-end manner to maximize forecasting accuracy. Together, these designs yield a 7.2% improvement in forecasting mIoU at an inference speed of 41 FPS, while using only 3.47 M parameters. Additionally, a Normalized Multi-View Attention is introduced in the video generation model to generate multi-view driving videos with guidance from our 4D occupancy, significantly improving video quality with a 20.7% reduction in FVD. Experiments demonstrate that GenieDrive enables highly controllable, multi-view consistent, and physics-aware driving video generation.",
    "translation": "标题：GenieDrive：基于四维占据栅格引导视频生成的物理感知驾驶世界模型\n\n摘要：物理感知的驾驶世界模型对于驾驶规划、分布外数据合成以及闭环评估至关重要。然而，现有方法通常依赖单一扩散模型直接将驾驶动作映射为视频，这使得学习过程困难并导致物理不一致的输出。为克服这些挑战，我们提出了GenieDrive，一种专为物理感知驾驶视频生成设计的新型框架。我们的方法首先生成四维占据栅格，以此为后续视频生成提供物理信息基础。四维占据栅格包含丰富的物理信息，包括高分辨率三维结构与动态特性。为有效压缩此类高分辨率占据栅格，我们提出一种变分自编码器，将占据栅格编码为潜在三平面表示，将潜在空间尺寸降至先前方法的58%。我们进一步引入互控注意力机制，以精确建模控制信号对占据栅格演化的影响，并以端到端方式联合训练变分自编码器与后续预测模块，从而最大化预测精度。这些设计共同实现了在41 FPS推理速度下预测mIoU指标提升7.2%，且仅使用347万参数。此外，我们在视频生成模型中引入归一化多视角注意力机制，通过四维占据栅格引导生成多视角驾驶视频，使FVD指标降低20.7%，显著提升视频质量。实验表明，GenieDrive能够实现高度可控、多视角一致且具有物理感知的驾驶视频生成。",
    "url": "https://huggingface.co/papers/2512.12751",
    "arxiv_url": "https://arxiv.org/abs/2512.12751"
  },
  {
    "title": "What matters for Representation Alignment: Global Information or Spatial Structure?",
    "summary": "Representation alignment (REPA) guides generative training by distilling representations from a strong, pretrained vision encoder to intermediate diffusion features. We investigate a fundamental question: what aspect of the target representation matters for generation, its global semantic information (e.g., measured by ImageNet-1K accuracy) or its spatial structure (i.e. pairwise cosine similarity between patch tokens)? Prevalent wisdom holds that stronger global semantic performance leads to better generation as a target representation. To study this, we first perform a large-scale empirical analysis across 27 different vision encoders and different model scales. The results are surprising; spatial structure, rather than global performance, drives the generation performance of a target representation. To further study this, we introduce two straightforward modifications, which specifically accentuate the transfer of spatial information. We replace the standard MLP projection layer in REPA with a simple convolution layer and introduce a spatial normalization layer for the external representation. Surprisingly, our simple method (implemented in <4 lines of code), termed iREPA, consistently improves convergence speed of REPA, across a diverse set of vision encoders, model sizes, and training variants (such as REPA, REPA-E, Meanflow, JiT etc). %, etc. Our work motivates revisiting the fundamental working mechanism of representational alignment and how it can be leveraged for improved training of generative models. The code and project page are available at https://end2end-diffusion.github.io/irepa",
    "translation": "标题：表征对齐的关键：全局信息还是空间结构？\n\n摘要：表征对齐（REPA）通过将预训练视觉编码器中的表征蒸馏至扩散模型的中间特征，以指导生成式训练。本研究探讨一个核心问题：目标表征的哪个方面对生成任务更为关键——是其全局语义信息（例如通过ImageNet-1K准确率衡量），还是其空间结构（即图像块标记之间的成对余弦相似度）？普遍观点认为，目标表征的全局语义性能越强，生成效果越好。为验证此观点，我们首先对27种不同视觉编码器及不同模型规模进行了大规模实证分析。结果出人意料：驱动目标表征生成性能的主要因素是空间结构，而非全局性能。为进一步探究，我们提出两种简洁的改进方法，专门强化空间信息的传递：将REPA中标准的MLP投影层替换为简单卷积层，并为外部表征引入空间归一化层。令人惊讶的是，这种被命名为iREPA的简易方法（代码实现少于4行），在多种视觉编码器、模型规模和训练变体（如REPA、REPA-E、Meanflow、JiT等）中均能持续提升REPA的收敛速度。本研究促使我们重新审视表征对齐的基本工作机制，并探索如何利用该机制改进生成模型的训练。代码与项目页面详见：https://end2end-diffusion.github.io/irepa",
    "url": "https://huggingface.co/papers/2512.10794",
    "arxiv_url": "https://arxiv.org/abs/2512.10794"
  },
  {
    "title": "RecTok: Reconstruction Distillation along Rectified Flow",
    "summary": "Visual tokenizers play a crucial role in diffusion models. The dimensionality of latent space governs both reconstruction fidelity and the semantic expressiveness of the latent feature. However, a fundamental trade-off is inherent between dimensionality and generation quality, constraining existing methods to low-dimensional latent spaces. Although recent works have leveraged vision foundation models to enrich the semantics of visual tokenizers and accelerate convergence, high-dimensional tokenizers still underperform their low-dimensional counterparts. In this work, we propose RecTok, which overcomes the limitations of high-dimensional visual tokenizers through two key innovations: flow semantic distillation and reconstruction--alignment distillation. Our key insight is to make the forward flow in flow matching semantically rich, which serves as the training space of diffusion transformers, rather than focusing on the latent space as in previous works. Specifically, our method distills the semantic information in VFMs into the forward flow trajectories in flow matching. And we further enhance the semantics by introducing a masked feature reconstruction loss. Our RecTok achieves superior image reconstruction, generation quality, and discriminative performance. It achieves state-of-the-art results on the gFID-50K under both with and without classifier-free guidance settings, while maintaining a semantically rich latent space structure. Furthermore, as the latent dimensionality increases, we observe consistent improvements. Code and model are available at https://shi-qingyu.github.io/rectok.github.io.",
    "translation": "标题：RecTok：沿修正流的重建蒸馏\n\n摘要：视觉分词器在扩散模型中发挥着关键作用。潜在空间的维度同时决定了重建保真度和潜在特征的语义表达能力。然而，维度与生成质量之间存在固有的权衡关系，这限制了现有方法只能采用低维潜在空间。尽管近期研究利用视觉基础模型来增强视觉分词器的语义并加速收敛，但高维分词器的性能仍落后于低维版本。本研究提出RecTok方法，通过两项关键创新克服高维视觉分词器的局限性：流语义蒸馏和重建对齐蒸馏。我们的核心思路是使流匹配中的前向流具备丰富的语义信息，将其作为扩散变换器的训练空间，而非如先前研究那样聚焦于潜在空间本身。具体而言，我们的方法将视觉基础模型中的语义信息蒸馏至流匹配的前向流轨迹中，并通过引入掩码特征重建损失进一步强化语义表达。RecTok在图像重建质量、生成效果和判别性能方面均表现优异。在有无分类器引导的两种设置下，该方法在gFID-50K指标上均达到最先进水平，同时保持了语义丰富的潜在空间结构。此外，随着潜在维度的增加，我们观察到性能的持续提升。代码与模型已发布于https://shi-qingyu.github.io/rectok.github.io。",
    "url": "https://huggingface.co/papers/2512.13421",
    "arxiv_url": "https://arxiv.org/abs/2512.13421"
  },
  {
    "title": "Few-Step Distillation for Text-to-Image Generation: A Practical Guide",
    "summary": "Diffusion distillation has dramatically accelerated class-conditional image synthesis, but its applicability to open-ended text-to-image (T2I) generation is still unclear. We present the first systematic study that adapts and compares state-of-the-art distillation techniques on a strong T2I teacher model, FLUX.1-lite. By casting existing methods into a unified framework, we identify the key obstacles that arise when moving from discrete class labels to free-form language prompts. Beyond a thorough methodological analysis, we offer practical guidelines on input scaling, network architecture, and hyperparameters, accompanied by an open-source implementation and pretrained student models. Our findings establish a solid foundation for deploying fast, high-fidelity, and resource-efficient diffusion generators in real-world T2I applications. Code is available on github.com/alibaba-damo-academy/T2I-Distill.",
    "translation": "标题：文本到图像生成的少步蒸馏：实用指南\n\n摘要：扩散蒸馏技术已显著加速了类别条件图像合成，但其在开放式文本到图像生成中的应用前景仍不明朗。本研究首次系统性地将前沿蒸馏技术适配并应用于强大的文本到图像教师模型FLUX.1-lite。通过将现有方法纳入统一框架，我们揭示了从离散类别标签转向自由形式语言提示时出现的关键障碍。除深入的方法论分析外，本研究还提供了关于输入缩放、网络架构和超参数设置的实用指南，并同步开源了实现代码与预训练学生模型。我们的研究成果为在实际文本到图像应用中部署快速、高保真且资源高效的扩散生成器奠定了坚实基础。代码已发布于github.com/alibaba-damo-academy/T2I-Distill。",
    "url": "https://huggingface.co/papers/2512.13006",
    "arxiv_url": "https://arxiv.org/abs/2512.13006"
  },
  {
    "title": "AutoMV: An Automatic Multi-Agent System for Music Video Generation",
    "summary": "Music-to-Video (M2V) generation for full-length songs faces significant challenges. Existing methods produce short, disjointed clips, failing to align visuals with musical structure, beats, or lyrics, and lack temporal consistency. We propose AutoMV, a multi-agent system that generates full music videos (MVs) directly from a song. AutoMV first applies music processing tools to extract musical attributes, such as structure, vocal tracks, and time-aligned lyrics, and constructs these features as contextual inputs for following agents. The screenwriter Agent and director Agent then use this information to design short script, define character profiles in a shared external bank, and specify camera instructions. Subsequently, these agents call the image generator for keyframes and different video generators for \"story\" or \"singer\" scenes. A Verifier Agent evaluates their output, enabling multi-agent collaboration to produce a coherent longform MV. To evaluate M2V generation, we further propose a benchmark with four high-level categories (Music Content, Technical, Post-production, Art) and twelve ine-grained criteria. This benchmark was applied to compare commercial products, AutoMV, and human-directed MVs with expert human raters: AutoMV outperforms current baselines significantly across all four categories, narrowing the gap to professional MVs. Finally, we investigate using large multimodal models as automatic MV judges; while promising, they still lag behind human expert, highlighting room for future work.",
    "translation": "标题：AutoMV：一种用于音乐视频生成的自动多智能体系统\n\n摘要：针对完整歌曲的音乐到视频（M2V）生成面临重大挑战。现有方法仅能生成简短、不连贯的片段，无法将视觉内容与音乐结构、节拍或歌词对齐，且缺乏时间一致性。本文提出AutoMV，一种能够直接从歌曲生成完整音乐视频（MV）的多智能体系统。AutoMV首先应用音乐处理工具提取音乐属性（如结构、人声音轨和时序对齐的歌词），并将这些特征构建为后续智能体的上下文输入。随后，编剧智能体与导演智能体利用该信息设计简短剧本，在共享外部库中定义角色档案，并指定镜头指令。接着，这些智能体调用图像生成器生成关键帧，并调用不同的视频生成器分别生成“故事”场景和“歌手”场景。验证智能体对输出进行评估，通过多智能体协作生成连贯的长篇音乐视频。为评估M2V生成效果，我们进一步提出包含四大高级类别（音乐内容、技术、后期制作、艺术）和十二项细粒度指标的基准体系。应用该基准对商业产品、AutoMV及人工执导音乐视频进行专家人工评分比较：AutoMV在全部四个类别上均显著优于现有基线方法，缩小了与专业音乐视频的差距。最后，我们探索使用大型多模态模型作为自动音乐视频评估工具；虽然前景可观，但其表现仍落后于人类专家，凸显了未来研究的改进空间。",
    "url": "https://huggingface.co/papers/2512.12196",
    "arxiv_url": "https://arxiv.org/abs/2512.12196"
  },
  {
    "title": "Flowception: Temporally Expansive Flow Matching for Video Generation",
    "summary": "We present Flowception, a novel non-autoregressive and variable-length video generation framework. Flowception learns a probability path that interleaves discrete frame insertions with continuous frame denoising. Compared to autoregressive methods, Flowception alleviates error accumulation/drift as the frame insertion mechanism during sampling serves as an efficient compression mechanism to handle long-term context. Compared to full-sequence flows, our method reduces FLOPs for training three-fold, while also being more amenable to local attention variants, and allowing to learn the length of videos jointly with their content. Quantitative experimental results show improved FVD and VBench metrics over autoregressive and full-sequence baselines, which is further validated with qualitative results. Finally, by learning to insert and denoise frames in a sequence, Flowception seamlessly integrates different tasks such as image-to-video generation and video interpolation.",
    "translation": "标题：Flowception：面向视频生成的时序扩展流匹配方法\n\n摘要：本文提出Flowception，一种新颖的非自回归可变长度视频生成框架。该方法通过概率路径学习机制，将离散帧插入与连续帧去噪过程进行交错融合。相较于自回归方法，Flowception在采样过程中通过帧插入机制形成高效压缩机制以处理长时序上下文，从而有效缓解误差累积/漂移问题。与全序列流方法相比，本方法将训练FLOPs降低至三分之一，同时更适配局部注意力变体，并能实现视频时长与内容的联合学习。定量实验结果显示，该方法在FVD和VBench指标上均优于自回归与全序列基线模型，定性实验结果进一步验证了该结论。通过序列中的帧插入与去噪学习机制，Flowception可无缝集成图像到视频生成与视频插帧等多样化任务。\n\n请按照以下格式返回：\n标题：Flowception：面向视频生成的时序扩展流匹配方法\n摘要：本文提出Flowception，一种新颖的非自回归可变长度视频生成框架。该方法通过概率路径学习机制，将离散帧插入与连续帧去噪过程进行交错融合。相较于自回归方法，Flowception在采样过程中通过帧插入机制形成高效压缩机制以处理长时序上下文，从而有效缓解误差累积/漂移问题。与全序列流方法相比，本方法将训练FLOPs降低至三分之一，同时更适配局部注意力变体，并能实现视频时长与内容的联合学习。定量实验结果显示，该方法在FVD和VBench指标上均优于自回归与全序列基线模型，定性实验结果进一步验证了该结论。通过序列中的帧插入与去噪学习机制，Flowception可无缝集成图像到视频生成与视频插帧等多样化任务。",
    "url": "https://huggingface.co/papers/2512.11438",
    "arxiv_url": "https://arxiv.org/abs/2512.11438"
  },
  {
    "title": "CAPTAIN: Semantic Feature Injection for Memorization Mitigation in Text-to-Image Diffusion Models",
    "summary": "Diffusion models can unintentionally reproduce training examples, raising privacy and copyright concerns as these systems are increasingly deployed at scale. Existing inference-time mitigation methods typically manipulate classifier-free guidance (CFG) or perturb prompt embeddings; however, they often struggle to reduce memorization without compromising alignment with the conditioning prompt. We introduce CAPTAIN, a training-free framework that mitigates memorization by directly modifying latent features during denoising. CAPTAIN first applies frequency-based noise initialization to reduce the tendency to replicate memorized patterns early in the denoising process. It then identifies the optimal denoising timesteps for feature injection and localizes memorized regions. Finally, CAPTAIN injects semantically aligned features from non-memorized reference images into localized latent regions, suppressing memorization while preserving prompt fidelity and visual quality. Our experiments show that CAPTAIN achieves substantial reductions in memorization compared to CFG-based baselines while maintaining strong alignment with the intended prompt.",
    "translation": "标题：CAPTAIN：面向文本到图像扩散模型记忆缓解的语义特征注入方法\n\n摘要：扩散模型可能无意中复现训练样本，随着此类系统的大规模部署，引发了隐私与版权方面的担忧。现有的推理阶段缓解方法通常通过操纵无分类器引导机制或扰动提示嵌入来实现，但这些方法往往难以在不降低与条件提示对齐度的前提下有效减少记忆现象。本文提出CAPTAIN，一种无需重新训练即可缓解记忆的框架，其通过在去噪过程中直接修改潜在特征来实现。CAPTAIN首先应用基于频率的噪声初始化，以降低去噪过程早期复制记忆模式的倾向；随后识别特征注入的最佳去噪时间步并定位记忆区域；最后，将来自非记忆参考图像的语义对齐特征注入定位的潜在区域，从而在抑制记忆的同时保持提示忠实度与视觉质量。实验表明，相较于基于无分类器引导的基线方法，CAPTAIN在保持与目标提示强对齐的前提下，实现了记忆现象的显著减少。\n\n摘要：[中文摘要]",
    "url": "https://huggingface.co/papers/2512.10655",
    "arxiv_url": "https://arxiv.org/abs/2512.10655"
  },
  {
    "title": "DiffusionBrowser: Interactive Diffusion Previews via Multi-Branch Decoders",
    "summary": "Video diffusion models have revolutionized generative video synthesis, but they are imprecise, slow, and can be opaque during generation -- keeping users in the dark for a prolonged period. In this work, we propose DiffusionBrowser, a model-agnostic, lightweight decoder framework that allows users to interactively generate previews at any point (timestep or transformer block) during the denoising process. Our model can generate multi-modal preview representations that include RGB and scene intrinsics at more than 4times real-time speed (less than 1 second for a 4-second video) that convey consistent appearance and motion to the final video. With the trained decoder, we show that it is possible to interactively guide the generation at intermediate noise steps via stochasticity reinjection and modal steering, unlocking a new control capability. Moreover, we systematically probe the model using the learned decoders, revealing how scene, object, and other details are composed and assembled during the otherwise black-box denoising process.",
    "translation": "标题：DiffusionBrowser：基于多分支解码器的交互式扩散预览框架\n\n摘要：视频扩散模型已彻底改变生成式视频合成领域，但其生成过程存在精度不足、速度缓慢且透明度低的问题，导致用户在生成期间需长时间处于不可知状态。本研究提出DiffusionBrowser——一种与模型无关的轻量级解码器框架，允许用户在去噪过程中的任意节点（时间步或Transformer模块）交互式生成预览。该模型能以超实时4倍速（4秒视频生成耗时不足1秒）生成包含RGB与场景本征属性的多模态预览表征，其外观与运动特征与最终视频保持高度一致。通过训练后的解码器，我们证明了在中间噪声步骤中可通过随机性重注入与模态导向实现交互式生成引导，从而解锁全新的控制能力。此外，我们利用学习得到的解码器对模型进行系统性探测，揭示了在原本黑箱化的去噪过程中场景、物体及其他细节是如何逐步组合构建的。",
    "url": "https://huggingface.co/papers/2512.13690",
    "arxiv_url": "https://arxiv.org/abs/2512.13690"
  },
  {
    "title": "LitePT: Lighter Yet Stronger Point Transformer",
    "summary": "Modern neural architectures for 3D point cloud processing contain both convolutional layers and attention blocks, but the best way to assemble them remains unclear. We analyse the role of different computational blocks in 3D point cloud networks and find an intuitive behaviour: convolution is adequate to extract low-level geometry at high-resolution in early layers, where attention is expensive without bringing any benefits; attention captures high-level semantics and context in low-resolution, deep layers more efficiently. Guided by this design principle, we propose a new, improved 3D point cloud backbone that employs convolutions in early stages and switches to attention for deeper layers. To avoid the loss of spatial layout information when discarding redundant convolution layers, we introduce a novel, training-free 3D positional encoding, PointROPE. The resulting LitePT model has 3.6times fewer parameters, runs 2times faster, and uses 2times less memory than the state-of-the-art Point Transformer V3, but nonetheless matches or even outperforms it on a range of tasks and datasets. Code and models are available at: https://github.com/prs-eth/LitePT.",
    "translation": "标题：LitePT：更轻量且更强大的点云Transformer\n\n摘要：当前用于三维点云处理的神经架构同时包含卷积层和注意力模块，但如何最优组合这些模块仍不明确。本文分析了三维点云网络中不同计算模块的作用，发现一种直观规律：卷积层适合在早期高分辨率阶段提取低层次几何特征，此时注意力机制因计算代价高昂而未带来明显收益；而注意力机制能更高效地在深层低分辨率阶段捕获高层次语义与上下文信息。基于此设计原则，我们提出一种改进的三维点云主干网络，在浅层阶段采用卷积运算，在深层阶段切换至注意力机制。为避免丢弃冗余卷积层时损失空间布局信息，我们引入一种无需训练的新型三维位置编码方法PointROPE。最终构建的LitePT模型与当前最先进的Point Transformer V3相比，参数量减少至1/3.6，运行速度提升2倍，内存占用降低50%，同时在一系列任务和数据集上达到相当甚至更优的性能。代码与模型已开源：https://github.com/prs-eth/LitePT。",
    "url": "https://huggingface.co/papers/2512.13689",
    "arxiv_url": "https://arxiv.org/abs/2512.13689"
  },
  {
    "title": "I-Scene: 3D Instance Models are Implicit Generalizable Spatial Learners",
    "summary": "Generalization remains the central challenge for interactive 3D scene generation. Existing learning-based approaches ground spatial understanding in limited scene dataset, restricting generalization to new layouts. We instead reprogram a pre-trained 3D instance generator to act as a scene level learner, replacing dataset-bounded supervision with model-centric spatial supervision. This reprogramming unlocks the generator transferable spatial knowledge, enabling generalization to unseen layouts and novel object compositions. Remarkably, spatial reasoning still emerges even when the training scenes are randomly composed objects. This demonstrates that the generator's transferable scene prior provides a rich learning signal for inferring proximity, support, and symmetry from purely geometric cues. Replacing widely used canonical space, we instantiate this insight with a view-centric formulation of the scene space, yielding a fully feed-forward, generalizable scene generator that learns spatial relations directly from the instance model. Quantitative and qualitative results show that a 3D instance generator is an implicit spatial learner and reasoner, pointing toward foundation models for interactive 3D scene understanding and generation. Project page: https://luling06.github.io/I-Scene-project/",
    "translation": "标题：I-Scene：三维实例模型作为隐式可泛化空间学习器\n\n摘要：泛化能力仍是交互式三维场景生成的核心挑战。现有基于学习的方法将空间理解局限于有限场景数据集，限制了其对新布局的泛化能力。本文通过重新编程预训练的三维实例生成器，使其作为场景级学习器运行，以模型为中心的空间监督替代数据集受限的监督。这种重新编程释放了生成器的可迁移空间知识，使其能够泛化至未见过的布局和新型物体组合。值得注意的是，即使训练场景由随机组合的物体构成，空间推理能力依然能够涌现。这表明生成器的可迁移场景先验为从纯几何线索推断邻近性、支撑关系和对称性提供了丰富的学习信号。我们摒弃广泛使用的规范空间，采用以视角为中心的场景空间构建方法，实例化了这一洞见，从而实现了完全前馈、可泛化的场景生成器，能够直接从实例模型学习空间关系。定量与定性实验结果表明，三维实例生成器是一种隐式的空间学习与推理器，为交互式三维场景理解与生成的基础模型指明了方向。项目页面：https://luling06.github.io/I-Scene-project/",
    "url": "https://huggingface.co/papers/2512.13683",
    "arxiv_url": "https://arxiv.org/abs/2512.13683"
  },
  {
    "title": "Directional Textual Inversion for Personalized Text-to-Image Generation",
    "summary": "Textual Inversion (TI) is an efficient approach to text-to-image personalization but often fails on complex prompts. We trace these failures to embedding norm inflation: learned tokens drift to out-of-distribution magnitudes, degrading prompt conditioning in pre-norm Transformers. Empirically, we show semantics are primarily encoded by direction in CLIP token space, while inflated norms harm contextualization; theoretically, we analyze how large magnitudes attenuate positional information and hinder residual updates in pre-norm blocks. We propose Directional Textual Inversion (DTI), which fixes the embedding magnitude to an in-distribution scale and optimizes only direction on the unit hypersphere via Riemannian SGD. We cast direction learning as MAP with a von Mises-Fisher prior, yielding a constant-direction prior gradient that is simple and efficient to incorporate. Across personalization tasks, DTI improves text fidelity over TI and TI-variants while maintaining subject similarity. Crucially, DTI's hyperspherical parameterization enables smooth, semantically coherent interpolation between learned concepts (slerp), a capability that is absent in standard TI. Our findings suggest that direction-only optimization is a robust and scalable path for prompt-faithful personalization.",
    "translation": "标题：面向个性化文本到图像生成的方向性文本反演\n\n摘要：文本反演是一种高效的文本到图像个性化方法，但在复杂提示词上常表现不佳。我们将其失败归因于嵌入范数膨胀现象：学习得到的标记向量会偏离原始分布范围，从而损害预归一化Transformer中的提示条件控制。实证研究表明，CLIP标记空间中的语义信息主要由方向编码，而膨胀的范数会破坏上下文关联性；理论分析揭示，过大的向量模长会弱化位置信息并阻碍预归一化模块中的残差更新。我们提出方向性文本反演方法，该方法将嵌入向量模长固定于分布内尺度，并通过黎曼随机梯度下降在单位超球面上仅优化方向。我们将方向学习建模为具有冯·米塞斯-费希尔先验的最大后验估计，推导出具有恒定方向先验梯度的简洁高效优化形式。在多类个性化任务中，DTI在保持主体相似性的同时，较原始TI及其变体显著提升了文本忠实度。关键的是，DTI的超球面参数化支持学习概念间的平滑语义连贯插值，这是标准TI所不具备的能力。我们的研究结果表明，纯方向优化是实现提示忠实个性化的稳健且可扩展路径。\n\n摘要：文本反演是一种高效的文本到图像个性化方法，但在处理复杂提示时经常失效。我们将这些失败归因于嵌入范数膨胀现象：学习到的标记向量会漂移到分布外幅度，从而降低预归一化Transformer中的提示条件控制效果。实证研究表明，CLIP标记空间中的语义信息主要由方向编码，而膨胀的范数会损害上下文关联性；理论分析揭示，过大的向量幅度会衰减位置信息并阻碍预归一化块中的残差更新。我们提出方向性文本反演方法，该方法将嵌入幅度固定于分布内尺度，并通过黎曼随机梯度下降在单位超球面上仅优化方向。我们将方向学习建模为具有冯·米塞斯-费希尔先验的最大后验估计，推导出具有恒定方向先验梯度的简洁高效优化形式。在多类个性化任务中，DTI在保持主体相似性的同时，较原始TI及其变体显著提升了文本忠实度。关键的是，DTI的超球面参数化支持学习概念间的平滑语义连贯插值，这是标准TI所不具备的能力。我们的研究结果表明，纯方向优化是实现提示忠实个性化的稳健且可扩展路径。",
    "url": "https://huggingface.co/papers/2512.13672",
    "arxiv_url": "https://arxiv.org/abs/2512.13672"
  },
  {
    "title": "FoundationMotion: Auto-Labeling and Reasoning about Spatial Movement in Videos",
    "summary": "Motion understanding is fundamental to physical reasoning, enabling models to infer dynamics and predict future states. However, state-of-the-art models still struggle on recent motion benchmarks, primarily due to the scarcity of large-scale, fine-grained motion datasets. Existing motion datasets are often constructed from costly manual annotation, severely limiting scalability. To address this challenge, we introduce FoundationMotion, a fully automated data curation pipeline that constructs large-scale motion datasets. Our approach first detects and tracks objects in videos to extract their trajectories, then leverages these trajectories and video frames with Large Language Models (LLMs) to generate fine-grained captions and diverse question-answer pairs about motion and spatial reasoning. Using datasets produced by this pipeline, we fine-tune open-source models including NVILA-Video-15B and Qwen2.5-7B, achieving substantial improvements in motion understanding without compromising performance on other tasks. Notably, our models outperform strong closed-source baselines like Gemini-2.5 Flash and large open-source models such as Qwen2.5-VL-72B across diverse motion understanding datasets and benchmarks. FoundationMotion thus provides a scalable solution for curating fine-grained motion datasets that enable effective fine-tuning of diverse models to enhance motion understanding and spatial reasoning capabilities.",
    "translation": "标题：FoundationMotion：视频空间运动的自动标注与推理\n\n摘要：运动理解是物理推理的基础，它使模型能够推断动态并预测未来状态。然而，当前最先进的模型在最新的运动基准测试中仍面临困难，主要原因是缺乏大规模、细粒度的运动数据集。现有的运动数据集通常依赖昂贵的人工标注构建，严重限制了可扩展性。为应对这一挑战，我们提出了FoundationMotion，一个全自动的数据构建流程，用于创建大规模运动数据集。该方法首先通过检测与跟踪视频中的物体以提取其运动轨迹，随后结合这些轨迹与视频帧，利用大语言模型生成关于运动与空间推理的细粒度描述文本及多样化问答对。基于此流程构建的数据集，我们对包括NVILA-Video-15B和Qwen2.5-7B在内的开源模型进行微调，在显著提升运动理解能力的同时，未损害其他任务上的性能。值得注意的是，在多种运动理解数据集与基准测试中，我们的模型表现优于Gemini-2.5 Flash等强大的闭源基线模型以及Qwen2.5-VL-72B等大型开源模型。因此，FoundationMotion为构建细粒度运动数据集提供了一个可扩展的解决方案，能够有效微调多样化模型，从而增强运动理解与空间推理能力。",
    "url": "https://huggingface.co/papers/2512.10927",
    "arxiv_url": "https://arxiv.org/abs/2512.10927"
  },
  {
    "title": "START: Spatial and Textual Learning for Chart Understanding",
    "summary": "Chart understanding is crucial for deploying multimodal large language models (MLLMs) in real-world scenarios such as analyzing scientific papers and technical reports. Unlike natural images, charts pair a structured visual layout (spatial property) with an underlying data representation (textual property) -- grasping both is essential for precise, fine-grained chart reasoning. Motivated by this observation, we propose START, the Spatial and Textual learning for chART understanding. Specifically, we introduce (i) chart-element grounding and (ii) chart-to-code generation to strengthen an MLLM's understanding of both chart visual layout and data details. To facilitate spatial and textual learning, we propose the START-Dataset generated with a novel data-generation pipeline that first leverages an MLLM to translate real chart images into executable chart code, recovering the underlying data representation while preserving the visual distribution of real-world charts. We then evolve the code with a Large Language Model (LLM) to ascertain the positions of chart elements that capture the chart's visual structure, addressing challenges that existing methods cannot handle. To evaluate a model's ability to understand chart spatial structures, we propose the Chart Spatial understanding Benchmark (CS-Bench), filling a critical gap in comprehensive chart understanding evaluation. Leveraging spatial and textual learning, START delivers consistent gains across model sizes and benchmarks over the base models and surpasses prior state-of-the-art by a clear margin. Code, data and models will be publicly available.",
    "translation": "标题：START：面向图表理解的空间与文本学习\n\n摘要：图表理解对于在多模态大语言模型（MLLMs）应用于科学论文分析、技术报告解读等现实场景中至关重要。与自然图像不同，图表同时包含结构化的视觉布局（空间属性）和隐含的数据表征（文本属性）——准确、细粒度的图表推理需要同时把握这两方面特性。基于这一观察，我们提出START（面向图表理解的空间与文本学习框架）。具体而言，我们通过引入（1）图表元素定位与（2）图表到代码生成两项任务，以增强MLLM对图表视觉布局与数据细节的双重理解。为支撑空间与文本学习，我们构建了START数据集，该数据集通过创新的数据生成流程构建：首先利用MLLM将真实图表图像转换为可执行的图表代码，在还原底层数据表征的同时保持真实图表的视觉分布特征；随后通过大语言模型（LLM）对代码进行演化，以精准确定描述图表视觉结构的元素空间位置，从而解决现有方法难以处理的技术挑战。为评估模型对图表空间结构的理解能力，我们提出了图表空间理解基准测试（CS-Bench），填补了当前图表综合理解评估体系的关键空白。实验表明，基于空间与文本联合学习的START框架在不同模型规模与基准测试中均显著超越基线模型，并以明显优势刷新了现有最优性能。相关代码、数据与模型将公开发布。",
    "url": "https://huggingface.co/papers/2512.07186",
    "arxiv_url": "https://arxiv.org/abs/2512.07186"
  },
  {
    "title": "Inferring Compositional 4D Scenes without Ever Seeing One",
    "summary": "Scenes in the real world are often composed of several static and dynamic objects. Capturing their 4-dimensional structures, composition and spatio-temporal configuration in-the-wild, though extremely interesting, is equally hard. Therefore, existing works often focus on one object at a time, while relying on some category-specific parametric shape model for dynamic objects. This can lead to inconsistent scene configurations, in addition to being limited to the modeled object categories. We propose COM4D (Compositional 4D), a method that consistently and jointly predicts the structure and spatio-temporal configuration of 4D/3D objects using only static multi-object or dynamic single object supervision. We achieve this by a carefully designed training of spatial and temporal attentions on 2D video input. The training is disentangled into learning from object compositions on the one hand, and single object dynamics throughout the video on the other, thus completely avoiding reliance on 4D compositional training data. At inference time, our proposed attention mixing mechanism combines these independently learned attentions, without requiring any 4D composition examples. By alternating between spatial and temporal reasoning, COM4D reconstructs complete and persistent 4D scenes with multiple interacting objects directly from monocular videos. Furthermore, COM4D provides state-of-the-art results in existing separate problems of 4D object and composed 3D reconstruction despite being purely data-driven.",
    "translation": "标题：无需观测即可推断组合式四维场景\n\n摘要：现实世界中的场景通常由多个静态与动态物体组合而成。尽管捕捉这些物体在真实环境中的四维结构、组合方式及时空配置极具研究价值，但其实现难度同样巨大。现有研究往往依赖特定类别的参数化动态物体模型，每次仅聚焦于单一对象，这可能导致场景配置不一致，且受限于已建模的物体类别。本文提出COM4D（组合式四维场景重建）方法，该方法仅需静态多物体或动态单物体的监督数据，即可实现四维/三维物体的结构与时空配置的协同一致性预测。我们通过对二维视频输入的空间与时间注意力机制进行精细化设计训练达成此目标：训练过程解耦为从物体组合中学习空间关系，以及从视频序列中学习单物体动态特征两个独立阶段，从而完全避免对四维组合训练数据的依赖。在推理阶段，我们提出的注意力混合机制能融合这些独立学习的注意力特征，且无需任何四维组合示例。通过空间推理与时间推理的交替迭代，COM4D可直接从单目视频中重建出包含多个交互物体的完整且持续的四维场景。此外，尽管采用纯数据驱动方法，COM4D在现有的四维物体重建与组合式三维重建等独立任务中仍取得了最先进的成果。",
    "url": "https://huggingface.co/papers/2512.05272",
    "arxiv_url": "https://arxiv.org/abs/2512.05272"
  },
  {
    "title": "FIN-bench-v2: A Unified and Robust Benchmark Suite for Evaluating Finnish Large Language Models",
    "summary": "We introduce FIN-bench-v2, a unified benchmark suite for evaluating large language models in Finnish. FIN-bench-v2 consolidates Finnish versions of widely used benchmarks together with an updated and expanded version of the original FIN-bench into a single, consistently formatted collection, covering multiple-choice and generative tasks across reading comprehension, commonsense reasoning, sentiment analysis, world knowledge, and alignment. All datasets are converted to HuggingFace Datasets, which include both cloze and multiple-choice prompt formulations with five variants per task, and we incorporate human annotation or review for machine-translated resources such as GoldenSwag and XED. To select robust tasks, we pretrain a set of 2.15B-parameter decoder-only models and use their learning curves to compute monotonicity, signal-to-noise, non-random performance, and model ordering consistency, retaining only tasks that satisfy all criteria. We further evaluate a set of larger instruction-tuned models to characterize performance across tasks and prompt formulations. All datasets, prompts, and evaluation configurations are publicly available via our fork of the Language Model Evaluation Harness at https://github.com/LumiOpen/lm-evaluation-harness. Supplementary resources are released in a separate repository at https://github.com/TurkuNLP/FIN-bench-v2.",
    "translation": "标题：FIN-bench-v2：用于评估芬兰语大语言模型的统一鲁棒基准套件\n\n摘要：本文介绍FIN-bench-v2，这是一个用于评估芬兰语大语言模型的统一基准套件。该套件将广泛使用的基准测试芬兰语版本与原始FIN-bench的更新扩展版整合为格式统一的数据集集合，涵盖阅读理解、常识推理、情感分析、世界知识和对齐任务中的多项选择与生成式任务。所有数据集均转换为HuggingFace Datasets格式，包含完形填空和多项选择题提示模板（每项任务设五种变体），并对GoldenSwag、XED等机器翻译资源进行了人工标注或审核。为筛选鲁棒性任务，我们预训练了一组21.5亿参数的仅解码器模型，通过其学习曲线计算单调性、信噪比、非随机性能及模型排序一致性，仅保留满足所有标准的任务。我们进一步评估了更大规模的指令微调模型，以分析不同任务和提示模板下的性能表现。所有数据集、提示模板和评估配置已通过我们分叉的Language Model Evaluation Harness开源发布（https://github.com/LumiOpen/lm-evaluation-harness）。补充资源发布于独立代码库（https://github.com/TurkuNLP/FIN-bench-v2）。",
    "url": "https://huggingface.co/papers/2512.13330",
    "arxiv_url": "https://arxiv.org/abs/2512.13330"
  },
  {
    "title": "State over Tokens: Characterizing the Role of Reasoning Tokens",
    "summary": "Large Language Models (LLMs) can generate reasoning tokens before their final answer to boost performance on complex tasks. While these sequences seem like human thought processes, empirical evidence reveals that they are not a faithful explanation of the model's actual reasoning process. To address this gap between appearance and function, we introduce the State over Tokens (SoT) conceptual framework. SoT reframes reasoning tokens not as a linguistic narrative, but as an externalized computational state -- the sole persistent information carrier across the model's stateless generation cycles. This explains how the tokens can drive correct reasoning without being a faithful explanation when read as text and surfaces previously overlooked research questions on these tokens. We argue that to truly understand the process that LLMs do, research must move beyond reading the reasoning tokens as text and focus on decoding them as state.",
    "translation": "标题：状态于词元之上：推理词元角色特性分析\n\n摘要：大型语言模型（LLM）能够在生成最终答案前输出推理词元序列以提升复杂任务性能。尽管这些序列看似模拟人类思维过程，实证研究表明其并非模型真实推理过程的可靠解释。为弥合表象与功能之间的认知鸿沟，本文提出“状态于词元之上”（SoT）概念框架。该框架将推理词元重新定义为外部化的计算状态——作为模型无状态生成周期中唯一持续存在的信息载体，而非语言叙述。这一视角解释了为何推理词元在驱动正确推理的同时，却无法在文本层面提供忠实解释，并揭示了此前被忽视的相关研究问题。我们认为，要真正理解大型语言模型的运行机制，研究必须超越将推理词元作为文本来解读的范式，转向将其作为状态信息进行解码的新路径。",
    "url": "https://huggingface.co/papers/2512.12777",
    "arxiv_url": "https://arxiv.org/abs/2512.12777"
  },
  {
    "title": "CoRe3D: Collaborative Reasoning as a Foundation for 3D Intelligence",
    "summary": "Recent advances in large multimodal models suggest that explicit reasoning mechanisms play a critical role in improving model reliability, interpretability, and cross-modal alignment. While such reasoning-centric approaches have been proven effective in language and vision tasks, their extension to 3D remains underdeveloped. CoRe3D introduces a unified 3D understanding and generation reasoning framework that jointly operates over semantic and spatial abstractions, enabling high-level intent inferred from language to directly guide low-level 3D content formation. Central to this design is a spatially grounded reasoning representation that decomposes 3D latent space into localized regions, allowing the model to reason over geometry in a compositional and procedural manner. By tightly coupling semantic chain-of-thought inference with structured spatial reasoning, CoRe3D produces 3D outputs that exhibit strong local consistency and faithful alignment with linguistic descriptions.",
    "translation": "标题：CoRe3D：作为三维智能基础的协同推理框架\n\n摘要：近期大规模多模态模型的研究进展表明，显式推理机制对于提升模型可靠性、可解释性及跨模态对齐能力具有关键作用。尽管此类以推理为核心的方法已在语言与视觉任务中被证明有效，但其向三维领域的扩展仍显不足。CoRe3D提出了一种统一的三维理解与生成推理框架，该框架在语义与空间抽象层面协同运作，使得从语言中推断出的高层意图能够直接指导底层三维内容的生成。该设计的核心在于一种空间锚定的推理表征，它将三维潜在空间分解为局部化区域，使模型能够以组合化、程序化的方式对几何结构进行推理。通过将语义思维链推理与结构化空间推理紧密耦合，CoRe3D生成的三维输出展现出强烈的局部一致性，并与语言描述保持高度忠实对齐。",
    "url": "https://huggingface.co/papers/2512.12768",
    "arxiv_url": "https://arxiv.org/abs/2512.12768"
  },
  {
    "title": "Rethinking Expert Trajectory Utilization in LLM Post-training",
    "summary": "While effective post-training integrates Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL), the optimal mechanism for utilizing expert trajectories remains unresolved. We propose the Plasticity-Ceiling Framework to theoretically ground this landscape, decomposing performance into foundational SFT performance and the subsequent RL plasticity. Through extensive benchmarking, we establish the Sequential SFT-then-RL pipeline as the superior standard, overcoming the stability deficits of synchronized approaches. Furthermore, we derive precise scaling guidelines: (1) Transitioning to RL at the SFT Stable or Mild Overfitting Sub-phase maximizes the final ceiling by securing foundational SFT performance without compromising RL plasticity; (2) Refuting ``Less is More'' in the context of SFT-then-RL scaling, we demonstrate that Data Scale determines the primary post-training potential, while Trajectory Difficulty acts as a performance multiplier; and (3) Identifying that the Minimum SFT Validation Loss serves as a robust indicator for selecting the expert trajectories that maximize the final performance ceiling. Our findings provide actionable guidelines for maximizing the value extracted from expert trajectories.",
    "translation": "标题：重新思考专家轨迹在大语言模型后训练中的利用方式\n\n摘要：尽管有效的后训练整合了监督微调（SFT）与强化学习（RL），但利用专家轨迹的最优机制仍未明确。我们提出“可塑性-上限”框架为此领域提供理论依据，将性能分解为基础SFT性能与后续RL可塑性。通过广泛的基准测试，我们确立了“先SFT后RL”的序列化流程为更优标准，克服了同步方法的稳定性缺陷。此外，我们推导出精确的扩展准则：（1）在SFT稳定或轻度过拟合子阶段转向RL，可通过确保基础SFT性能且不损害RL可塑性，最大化最终性能上限；（2）在“先SFT后RL”的扩展背景下驳斥“少即是多”的观点，我们证明数据规模决定后训练的主要潜力，而轨迹难度则作为性能倍增器；（3）发现最小SFT验证损失可作为筛选专家轨迹的稳健指标，以最大化最终性能上限。我们的研究结果为从专家轨迹中提取最大价值提供了可操作的指导原则。",
    "url": "https://huggingface.co/papers/2512.11470",
    "arxiv_url": "https://arxiv.org/abs/2512.11470"
  },
  {
    "title": "Learning Robot Manipulation from Audio World Models",
    "summary": "World models have demonstrated impressive performance on robotic learning tasks. Many such tasks inherently demand multimodal reasoning; for example, filling a bottle with water will lead to visual information alone being ambiguous or incomplete, thereby requiring reasoning over the temporal evolution of audio, accounting for its underlying physical properties and pitch patterns. In this paper, we propose a generative latent flow matching model to anticipate future audio observations, enabling the system to reason about long-term consequences when integrated into a robot policy. We demonstrate the superior capabilities of our system through two manipulation tasks that require perceiving in-the-wild audio or music signals, compared to methods without future lookahead. We further emphasize that successful robot action learning for these tasks relies not merely on multi-modal input, but critically on the accurate prediction of future audio states that embody intrinsic rhythmic patterns.",
    "translation": "标题：基于音频世界模型的机器人操作学习\n\n摘要：世界模型在机器人学习任务中展现出卓越性能。许多此类任务本质需要多模态推理能力；例如，仅凭视觉信息判断水瓶注水过程往往存在模糊性或信息缺失，这要求系统能够结合音频信号的时序演化进行推理，并解析其内在物理属性与音高模式。本文提出一种生成式潜在流匹配模型，用于预测未来音频观测数据，使系统在融入机器人策略时能够推理长期行为后果。通过两项需要感知真实环境音频或音乐信号的操作任务实验，本系统相较于无前瞻预测的方法展现出显著优势。我们进一步强调，此类任务的成功学习不仅依赖于多模态输入，更关键取决于对未来音频状态的精准预测——这些状态承载着内在的节奏模式特征。",
    "url": "https://huggingface.co/papers/2512.08405",
    "arxiv_url": "https://arxiv.org/abs/2512.08405"
  },
  {
    "title": "Towards Visual Re-Identification of Fish using Fine-Grained Classification for Electronic Monitoring in Fisheries",
    "summary": "Accurate fisheries data are crucial for effective and sustainable marine resource management. With the recent adoption of Electronic Monitoring (EM) systems, more video data is now being collected than can be feasibly reviewed manually. This paper addresses this challenge by developing an optimized deep learning pipeline for automated fish re-identification (Re-ID) using the novel AutoFish dataset, which simulates EM systems with conveyor belts with six similarly looking fish species. We demonstrate that key Re-ID metrics (R1 and mAP@k) are substantially improved by using hard triplet mining in conjunction with a custom image transformation pipeline that includes dataset-specific normalization. By employing these strategies, we demonstrate that the Vision Transformer-based Swin-T architecture consistently outperforms the Convolutional Neural Network-based ResNet-50, achieving peak performance of 41.65% mAP@k and 90.43% Rank-1 accuracy. An in-depth analysis reveals that the primary challenge is distinguishing visually similar individuals of the same species (Intra-species errors), where viewpoint inconsistency proves significantly more detrimental than partial occlusion. The source code and documentation are available at: https://github.com/msamdk/Fish_Re_Identification.git",
    "translation": "标题：基于细粒度分类的渔业电子监控鱼类视觉重识别研究\n\n摘要：准确的渔业数据对于实现有效且可持续的海洋资源管理至关重要。随着电子监控系统在渔业中的推广应用，当前采集的视频数据量已远超人工审阅的可行范围。本文通过构建优化的深度学习流程应对这一挑战，利用新型AutoFish数据集实现自动化鱼类重识别。该数据集模拟配备传送带的电子监控系统，包含六种外观相似的鱼类物种。研究表明，通过结合困难三元组挖掘与定制化图像处理流程（包含数据集特异性归一化方法），关键重识别指标（R1与mAP@k）得到显著提升。采用上述策略后，基于视觉Transformer的Swin-T架构持续优于基于卷积神经网络的ResNet-50模型，最高达到41.65%的mAP@k与90.43%的Rank-1准确率。深入分析表明，主要挑战在于区分同物种内视觉相似的个体（种内误差），其中视角不一致性造成的识别障碍远高于局部遮挡问题。源代码及文档详见：https://github.com/msamdk/Fish_Re_Identification.git",
    "url": "https://huggingface.co/papers/2512.08400",
    "arxiv_url": "https://arxiv.org/abs/2512.08400"
  },
  {
    "title": "KD-OCT: Efficient Knowledge Distillation for Clinical-Grade Retinal OCT Classification",
    "summary": "Age-related macular degeneration (AMD) and choroidal neovascularization (CNV)-related conditions are leading causes of vision loss worldwide, with optical coherence tomography (OCT) serving as a cornerstone for early detection and management. However, deploying state-of-the-art deep learning models like ConvNeXtV2-Large in clinical settings is hindered by their computational demands. Therefore, it is desirable to develop efficient models that maintain high diagnostic performance while enabling real-time deployment. In this study, a novel knowledge distillation framework, termed KD-OCT, is proposed to compress a high-performance ConvNeXtV2-Large teacher model, enhanced with advanced augmentations, stochastic weight averaging, and focal loss, into a lightweight EfficientNet-B2 student for classifying normal, drusen, and CNV cases. KD-OCT employs real-time distillation with a combined loss balancing soft teacher knowledge transfer and hard ground-truth supervision. The effectiveness of the proposed method is evaluated on the Noor Eye Hospital (NEH) dataset using patient-level cross-validation. Experimental results demonstrate that KD-OCT outperforms comparable multi-scale or feature-fusion OCT classifiers in efficiency- accuracy balance, achieving near-teacher performance with substantial reductions in model size and inference time. Despite the compression, the student model exceeds most existing frameworks, facilitating edge deployment for AMD screening. Code is available at https://github.com/erfan-nourbakhsh/KD- OCT.",
    "translation": "标题：KD-OCT：面向临床级视网膜OCT分类的高效知识蒸馏方法\n\n摘要：年龄相关性黄斑变性（AMD）与脉络膜新生血管（CNV）相关疾病是全球范围内视力丧失的主要原因，而光学相干断层扫描（OCT）是其早期检测与管理的核心手段。然而，在临床环境中部署如ConvNeXtV2-Large等先进深度学习模型时，其高计算需求构成实际障碍。因此，亟需开发在保持高诊断性能的同时支持实时部署的高效模型。本研究提出一种新颖的知识蒸馏框架KD-OCT，通过结合高级数据增强、随机权重平均与焦点损失优化的高性能ConvNeXtV2-Large教师模型，将其压缩为轻量级EfficientNet-B2学生模型，用于正常视网膜、玻璃膜疣及CNV病例的分类。KD-OCT采用实时蒸馏策略，通过融合损失函数平衡教师模型的软知识迁移与真实标签的硬监督。在努尔眼科医院数据集上采用患者级交叉验证评估该方法有效性。实验结果表明，KD-OCT在效率-精度平衡性上优于同类多尺度或特征融合OCT分类器，在模型体积与推理时间显著降低的同时达到接近教师模型的性能。尽管经过压缩，学生模型仍超越现有大多数框架，为AMD筛查的边缘部署提供了可行方案。代码已开源：https://github.com/erfan-nourbakhsh/KD-OCT。",
    "url": "https://huggingface.co/papers/2512.09069",
    "arxiv_url": "https://arxiv.org/abs/2512.09069"
  }
]