[
  {
    "title": "ReFusion: A Diffusion Large Language Model with Parallel Autoregressive Decoding",
    "summary": "Autoregressive models (ARMs) are hindered by slow sequential inference. While masked diffusion models (MDMs) offer a parallel alternative, they suffer from critical drawbacks: high computational overhead from precluding Key-Value (KV) caching, and incoherent generation arising from learning dependencies over an intractable space of token combinations. To address these limitations, we introduce ReFusion, a novel masked diffusion model that achieves superior performance and efficiency by elevating parallel decoding from the token level to a higher slot level, where each slot is a fixed-length, contiguous sub-sequence. This is achieved through an iterative ``plan-and-infill'' decoding process: a diffusion-based planning step first identifies a set of weakly dependent slots, and an autoregressive infilling step then decodes these selected slots in parallel. The slot-based design simultaneously unlocks full KV cache reuse with a unified causal framework and reduces the learning complexity from the token combination space to a manageable slot-level permutation space. Extensive experiments on seven diverse benchmarks show that ReFusion not only overwhelmingly surpasses prior MDMs with 34% performance gains and an over 18times speedup on average, but also bridges the performance gap to strong ARMs while maintaining a 2.33times average speedup.",
    "translation": "标题：ReFusion：一种基于并行自回归解码的扩散大语言模型\n\n摘要：自回归模型因顺序推理速度缓慢而受到限制。尽管掩码扩散模型提供了并行化的替代方案，但其存在两个关键缺陷：一是因无法使用键值缓存而导致计算开销高昂，二是在学习难以处理的词元组合空间上的依赖关系时，会产生不连贯的生成结果。为解决这些局限性，本文提出ReFusion——一种新颖的掩码扩散模型，通过将并行解码从词元层级提升至更高层级的“槽位”（每个槽位为固定长度的连续子序列），实现了更优的性能与效率。该模型采用迭代式的“规划-填充”解码流程：首先通过基于扩散的规划步骤识别出一组弱依赖的槽位，随后通过自回归填充步骤并行解码这些选定槽位。这种基于槽位的设计在统一因果框架下实现了完整的键值缓存复用，同时将学习复杂度从词元组合空间降低至可管理的槽位排列空间。在七个多样化基准测试上的大量实验表明，ReFusion不仅以34%的性能提升和平均超过18倍的加速比显著超越现有掩码扩散模型，更在保持平均2.33倍加速优势的同时，大幅缩小了与强自回归模型的性能差距。",
    "url": "https://huggingface.co/papers/2512.13586",
    "arxiv_url": "https://arxiv.org/abs/2512.13586"
  },
  {
    "title": "Towards Scalable Pre-training of Visual Tokenizers for Generation",
    "summary": "The quality of the latent space in visual tokenizers (e.g., VAEs) is crucial for modern generative models. However, the standard reconstruction-based training paradigm produces a latent space that is biased towards low-level information, leading to a foundation flaw: better pixel-level accuracy does not lead to higher-quality generation. This implies that pouring extensive compute into visual tokenizer pre-training translates poorly to improved performance in generation. We identify this as the ``pre-training scaling problem`` and suggest a necessary shift: to be effective for generation, a latent space must concisely represent high-level semantics. We present VTP, a unified visual tokenizer pre-training framework, pioneering the joint optimization of image-text contrastive, self-supervised, and reconstruction losses. Our large-scale study reveals two principal findings: (1) understanding is a key driver of generation, and (2) much better scaling properties, where generative performance scales effectively with compute, parameters, and data allocated to the pretraining of the visual tokenizer. After large-scale pre-training, our tokenizer delivers a competitive profile (78.2 zero-shot accuracy and 0.36 rFID on ImageNet) and 4.1 times faster convergence on generation compared to advanced distillation methods. More importantly, it scales effectively: without modifying standard DiT training specs, solely investing more FLOPS in pretraining VTP achieves 65.8\\% FID improvement in downstream generation, while conventional autoencoder stagnates very early at 1/10 FLOPS. Our pre-trained models are available at https://github.com/MiniMax-AI/VTP.",
    "translation": "标题：面向生成任务的可扩展视觉分词器预训练研究\n\n摘要：视觉分词器（如变分自编码器）的潜在空间质量对现代生成模型至关重要。然而，基于标准重建的训练范式产生的潜在空间偏向于低层次信息，这导致了一个根本性缺陷：更高的像素级精度并不能带来更高质量的生成结果。这意味着将大量计算资源投入视觉分词器预训练对生成性能的提升效果有限。我们将此问题定义为“预训练扩展困境”，并提出关键转变思路：要有效支持生成任务，潜在空间必须简洁地表征高层次语义信息。本文提出VTP——一个统一的视觉分词器预训练框架，率先实现了图像-文本对比损失、自监督损失与重建损失的联合优化。我们的大规模实验揭示了两项核心发现：（1）理解能力是驱动生成性能的关键因素；（2）该框架具备更优越的扩展特性，生成性能可随预训练投入的计算量、参数量和数据量有效提升。经过大规模预训练后，我们的分词器展现出卓越性能（在ImageNet数据集上达到78.2%的零样本分类准确率和0.36的rFID指标），且在生成任务上比先进蒸馏方法快4.1倍收敛。更重要的是，该框架具备显著扩展优势：在不改变标准DiT训练配置的情况下，仅通过增加VTP预训练的计算量即可实现下游生成任务65.8%的FID指标提升，而传统自编码器在仅使用1/10计算量时性能便过早停滞。预训练模型已开源：https://github.com/MiniMax-AI/VTP。",
    "url": "https://huggingface.co/papers/2512.13687",
    "arxiv_url": "https://arxiv.org/abs/2512.13687"
  },
  {
    "title": "Memory in the Age of AI Agents",
    "summary": "Memory has emerged, and will continue to remain, a core capability of foundation model-based agents. As research on agent memory rapidly expands and attracts unprecedented attention, the field has also become increasingly fragmented. Existing works that fall under the umbrella of agent memory often differ substantially in their motivations, implementations, and evaluation protocols, while the proliferation of loosely defined memory terminologies has further obscured conceptual clarity. Traditional taxonomies such as long/short-term memory have proven insufficient to capture the diversity of contemporary agent memory systems. This work aims to provide an up-to-date landscape of current agent memory research. We begin by clearly delineating the scope of agent memory and distinguishing it from related concepts such as LLM memory, retrieval augmented generation (RAG), and context engineering. We then examine agent memory through the unified lenses of forms, functions, and dynamics. From the perspective of forms, we identify three dominant realizations of agent memory, namely token-level, parametric, and latent memory. From the perspective of functions, we propose a finer-grained taxonomy that distinguishes factual, experiential, and working memory. From the perspective of dynamics, we analyze how memory is formed, evolved, and retrieved over time. To support practical development, we compile a comprehensive summary of memory benchmarks and open-source frameworks. Beyond consolidation, we articulate a forward-looking perspective on emerging research frontiers, including memory automation, reinforcement learning integration, multimodal memory, multi-agent memory, and trustworthiness issues. We hope this survey serves not only as a reference for existing work, but also as a conceptual foundation for rethinking memory as a first-class primitive in the design of future agentic intelligence.",
    "translation": "标题：人工智能智能体时代中的记忆研究\n\n摘要：记忆已成为基于基础模型的智能体核心能力，并将持续发挥关键作用。随着智能体记忆研究快速扩张并受到前所未有的关注，该领域也日益呈现碎片化态势。现有关于智能体记忆的研究在动机、实现方式和评估标准上存在显著差异，而定义松散的记忆术语泛滥进一步模糊了概念清晰度。传统分类法（如长/短期记忆）已不足以涵盖当代智能体记忆系统的多样性。本研究旨在系统梳理当前智能体记忆研究的最新图景。首先明确界定智能体记忆的范畴，并将其与大型语言模型记忆、检索增强生成（RAG）及上下文工程等相关概念进行区分。随后通过形式、功能与动态演化的统一视角审视智能体记忆：在形式层面，归纳出符号级记忆、参数化记忆与潜在记忆三种主流实现方式；在功能层面，提出更细粒度的事实记忆、经验记忆与工作记忆分类体系；在动态层面，解析记忆随时间推移的形成、演化与检索机制。为支撑实际开发，本文汇编了完整的记忆基准测试集与开源框架综述。在整合现有成果基础上，进一步展望了记忆自动化、强化学习融合、多模态记忆、多智能体记忆及可信度问题等新兴前沿研究方向。本研究不仅可作为现有工作的参考指南，更希望为将记忆重新定位为未来智能体设计中的核心基础要素提供概念框架。",
    "url": "https://huggingface.co/papers/2512.13564",
    "arxiv_url": "https://arxiv.org/abs/2512.13564"
  },
  {
    "title": "QwenLong-L1.5: Post-Training Recipe for Long-Context Reasoning and Memory Management",
    "summary": "We introduce QwenLong-L1.5, a model that achieves superior long-context reasoning capabilities through systematic post-training innovations. The key technical breakthroughs of QwenLong-L1.5 are as follows: (1) Long-Context Data Synthesis Pipeline: We develop a systematic synthesis framework that generates challenging reasoning tasks requiring multi-hop grounding over globally distributed evidence. By deconstructing documents into atomic facts and their underlying relationships, and then programmatically composing verifiable reasoning questions, our approach creates high-quality training data at scale, moving substantially beyond simple retrieval tasks to enable genuine long-range reasoning capabilities. (2) Stabilized Reinforcement Learning for Long-Context Training: To overcome the critical instability in long-context RL, we introduce task-balanced sampling with task-specific advantage estimation to mitigate reward bias, and propose Adaptive Entropy-Controlled Policy Optimization (AEPO) that dynamically regulates exploration-exploitation trade-offs. (3) Memory-Augmented Architecture for Ultra-Long Contexts: Recognizing that even extended context windows cannot accommodate arbitrarily long sequences, we develop a memory management framework with multi-stage fusion RL training that seamlessly integrates single-pass reasoning with iterative memory-based processing for tasks exceeding 4M tokens. Based on Qwen3-30B-A3B-Thinking, QwenLong-L1.5 achieves performance comparable to GPT-5 and Gemini-2.5-Pro on long-context reasoning benchmarks, surpassing its baseline by 9.90 points on average. On ultra-long tasks (1M~4M tokens), QwenLong-L1.5's memory-agent framework yields a 9.48-point gain over the agent baseline. Additionally, the acquired long-context reasoning ability translates to enhanced performance in general domains like scientific reasoning, memory tool using, and extended dialogue.",
    "translation": "标题：QwenLong-L1.5：面向长上下文推理与记忆管理的后训练方案\n\n摘要：本文介绍QwenLong-L1.5模型，该模型通过系统性的后训练创新实现了卓越的长上下文推理能力。QwenLong-L1.5的关键技术突破如下：（1）长上下文数据合成流程：我们开发了系统性合成框架，可生成需要基于全局分布证据进行多跳溯因的复杂推理任务。通过将文档解构为原子事实及其内在关联，再以可编程方式组合可验证的推理问题，本方法实现了高质量训练数据的大规模生成，显著超越简单检索任务，真正实现了长距离推理能力。（2）面向长上下文训练的稳定强化学习：为克服长上下文强化学习中的关键不稳定性，我们提出基于任务平衡采样与任务特定优势估计的奖励偏差缓解机制，并设计自适应熵控制策略优化算法，动态调节探索与利用的平衡。（3）超长上下文记忆增强架构：针对扩展上下文窗口仍无法容纳无限长序列的瓶颈，我们开发了具有多阶段融合强化训练的记忆管理框架，通过单次推理与基于记忆的迭代处理无缝协同，可处理超过400万标记的超长任务。基于Qwen3-30B-A3B-Thinking构建的QwenLong-L1.5在长上下文推理基准测试中达到与GPT-5和Gemini-2.5-Pro相当的性能，较基线模型平均提升9.90分。在超长任务（100万至400万标记）中，其记忆智能体框架相比智能体基线获得9.48分的性能增益。此外，所获得的长上下文推理能力可迁移提升科学推理、记忆工具使用及长程对话等通用领域的表现。",
    "url": "https://huggingface.co/papers/2512.12967",
    "arxiv_url": "https://arxiv.org/abs/2512.12967"
  },
  {
    "title": "LongVie 2: Multimodal Controllable Ultra-Long Video World Model",
    "summary": "Building video world models upon pretrained video generation systems represents an important yet challenging step toward general spatiotemporal intelligence. A world model should possess three essential properties: controllability, long-term visual quality, and temporal consistency. To this end, we take a progressive approach-first enhancing controllability and then extending toward long-term, high-quality generation. We present LongVie 2, an end-to-end autoregressive framework trained in three stages: (1) Multi-modal guidance, which integrates dense and sparse control signals to provide implicit world-level supervision and improve controllability; (2) Degradation-aware training on the input frame, bridging the gap between training and long-term inference to maintain high visual quality; and (3) History-context guidance, which aligns contextual information across adjacent clips to ensure temporal consistency. We further introduce LongVGenBench, a comprehensive benchmark comprising 100 high-resolution one-minute videos covering diverse real-world and synthetic environments. Extensive experiments demonstrate that LongVie 2 achieves state-of-the-art performance in long-range controllability, temporal coherence, and visual fidelity, and supports continuous video generation lasting up to five minutes, marking a significant step toward unified video world modeling.",
    "translation": "标题：LongVie 2：多模态可控超长视频世界模型\n\n摘要：在预训练视频生成系统基础上构建视频世界模型，是实现通用时空智能的重要且具有挑战性的一步。一个理想的世界模型应具备三个关键特性：可控性、长期视觉质量与时间一致性。为此，我们采用渐进式方法——先提升可控性，再向长期高质量生成扩展。我们提出LongVie 2，这是一个端到端自回归框架，通过三阶段训练实现：（1）多模态引导，融合稠密与稀疏控制信号以提供隐式世界级监督，提升可控性；（2）输入帧的退化感知训练，弥合训练与长期推理间的差距以保持高视觉质量；（3）历史上下文引导，对齐相邻片段间的上下文信息以确保时间一致性。我们进一步提出LongVGenBench，这是一个包含100段高分辨率一分钟视频的综合基准数据集，涵盖多样化的真实世界与合成场景。大量实验表明，LongVie 2在长程可控性、时间连贯性与视觉保真度方面均达到领先水平，并支持持续生成长达五分钟的视频，标志着向统一视频世界建模迈出了重要一步。",
    "url": "https://huggingface.co/papers/2512.13604",
    "arxiv_url": "https://arxiv.org/abs/2512.13604"
  },
  {
    "title": "Finch: Benchmarking Finance & Accounting across Spreadsheet-Centric Enterprise Workflows",
    "summary": "We introduce a finance & accounting benchmark (Finch) for evaluating AI agents on real-world, enterprise-grade professional workflows -- interleaving data entry, structuring, formatting, web search, cross-file retrieval, calculation, modeling, validation, translation, visualization, and reporting. Finch is sourced from authentic enterprise workspaces at Enron (15,000 spreadsheets and 500,000 emails from 150 employees) and other financial institutions, preserving in-the-wild messiness across multimodal artifacts (text, tables, formulas, charts, code, and images) and spanning diverse domains such as budgeting, trading, and asset management.\n  We propose a workflow construction process that combines LLM-assisted discovery with expert annotation: (1) LLM-assisted, expert-verified derivation of workflows from real-world email threads and version histories of spreadsheet files, and (2) meticulous expert annotation for workflows, requiring over 700 hours of domain-expert effort. This yields 172 composite workflows with 384 tasks, involving 1,710 spreadsheets with 27 million cells, along with PDFs and other artifacts, capturing the intrinsically messy, long-horizon, knowledge-intensive, and collaborative nature of real-world enterprise work.\n  We conduct both human and automated evaluations of frontier AI systems including GPT 5.1, Claude Sonnet 4.5, Gemini 3 Pro, Grok 4, and Qwen 3 Max, and GPT 5.1 Pro spends 48 hours in total yet passes only 38.4% of workflows, while Claude Sonnet 4.5 passes just 25.0%. Comprehensive case studies further surface the challenges that real-world enterprise workflows pose for AI agents.",
    "translation": "标题：Finch：面向以电子表格为核心的企业工作流程的财务与会计基准测试\n\n摘要：本文提出一个财务与会计基准测试框架（Finch），用于评估人工智能代理在真实企业级专业工作流程中的表现——这些流程交织着数据录入、结构化处理、格式调整、网络搜索、跨文件检索、计算、建模、验证、翻译、可视化及报告生成等任务。Finch的数据源自安然公司（涵盖150名员工的15,000份电子表格和50万封电子邮件）及其他金融机构的真实企业工作环境，完整保留了多模态材料（文本、表格、公式、图表、代码和图像）在真实场景中的杂乱特性，覆盖预算编制、交易执行和资产管理等多个领域。\n\n我们提出一种结合大语言模型辅助发现与专家标注的工作流程构建方法：（1）通过大语言模型辅助、专家验证的方式，从真实邮件线程和电子表格文件版本历史中推导工作流程；（2）由领域专家对工作流程进行精细标注，累计投入超过700小时的专业工作量。该方法最终构建出包含384项任务的172个复合工作流程，涉及1,710个总计2700万个单元格的电子表格，以及PDF等其他材料，真实呈现了企业工作中固有的杂乱性、长期性、知识密集性与协作性特征。\n\n我们对包括GPT 5.1、Claude Sonnet 4.5、Gemini 3 Pro、Grok 4和Qwen 3 Max在内的前沿人工智能系统进行了人工与自动化评估。其中GPT 5.1 Pro耗时48小时仅能通过38.4%的工作流程，而Claude Sonnet 4.5仅通过25.0%。深入的案例研究进一步揭示了真实企业工作流程对人工智能代理提出的核心挑战。",
    "url": "https://huggingface.co/papers/2512.13168",
    "arxiv_url": "https://arxiv.org/abs/2512.13168"
  },
  {
    "title": "NL2Repo-Bench: Towards Long-Horizon Repository Generation Evaluation of Coding Agents",
    "summary": "Recent advances in coding agents suggest rapid progress toward autonomous software development, yet existing benchmarks fail to rigorously evaluate the long-horizon capabilities required to build complete software systems. Most prior evaluations focus on localized code generation, scaffolded completion, or short-term repair tasks, leaving open the question of whether agents can sustain coherent reasoning, planning, and execution over the extended horizons demanded by real-world repository construction. To address this gap, we present NL2Repo Bench, a benchmark explicitly designed to evaluate the long-horizon repository generation ability of coding agents. Given only a single natural-language requirements document and an empty workspace, agents must autonomously design the architecture, manage dependencies, implement multi-module logic, and produce a fully installable Python library. Our experiments across state-of-the-art open- and closed-source models reveal that long-horizon repository generation remains largely unsolved: even the strongest agents achieve below 40% average test pass rates and rarely complete an entire repository correctly. Detailed analysis uncovers fundamental long-horizon failure modes, including premature termination, loss of global coherence, fragile cross-file dependencies, and inadequate planning over hundreds of interaction steps. NL2Repo Bench establishes a rigorous, verifiable testbed for measuring sustained agentic competence and highlights long-horizon reasoning as a central bottleneck for the next generation of autonomous coding agents.",
    "translation": "标题：NL2Repo-Bench：面向代码智能体长周期仓库生成能力的评估基准\n\n摘要：代码智能体的最新进展表明，自主软件开发正快速发展，但现有基准测试未能严格评估构建完整软件系统所需的长周期能力。以往评估大多聚焦于局部代码生成、框架补全或短期修复任务，导致智能体能否在现实仓库构建所需的长周期中保持连贯推理、规划与执行能力仍存疑问。为填补这一空白，我们提出NL2Repo-Bench——一个专门用于评估代码智能体长周期仓库生成能力的基准测试。该测试仅提供一个自然语言需求文档和空白工作空间，要求智能体自主完成架构设计、依赖管理、多模块逻辑实现，并最终生成可完整安装的Python库。通过对当前最先进的开源与闭源模型进行实验，我们发现长周期仓库生成任务仍远未解决：即使性能最强的智能体平均测试通过率也低于40%，且极少能完整正确地生成整个仓库。深入分析揭示了长周期任务中的根本性失效模式，包括过早终止、全局一致性丧失、脆弱的跨文件依赖关系，以及在数百个交互步骤中规划能力不足等问题。NL2Repo-Bench为衡量智能体持续自主能力建立了严谨可验证的测试平台，并揭示长周期推理能力是下一代自主代码智能体发展的核心瓶颈。",
    "url": "https://huggingface.co/papers/2512.12730",
    "arxiv_url": "https://arxiv.org/abs/2512.12730"
  },
  {
    "title": "Error-Free Linear Attention is a Free Lunch: Exact Solution from Continuous-Time Dynamics",
    "summary": "Linear-time attention and State Space Models (SSMs) promise to solve the quadratic cost bottleneck in long-context language models employing softmax attention. We introduce Error-Free Linear Attention (EFLA), a numerically stable, fully parallelism and generalized formulation of the delta rule. Specifically, we formulate the online learning update as a continuous-time dynamical system and prove that its exact solution is not only attainable but also computable in linear time with full parallelism. By leveraging the rank-1 structure of the dynamics matrix, we directly derive the exact closed-form solution effectively corresponding to the infinite-order Runge-Kutta method. This attention mechanism is theoretically free from error accumulation, perfectly capturing the continuous dynamics while preserving the linear-time complexity. Through an extensive suite of experiments, we show that EFLA enables robust performance in noisy environments, achieving lower language modeling perplexity and superior downstream benchmark performance than DeltaNet without introducing additional parameters. Our work provides a new theoretical foundation for building high-fidelity, scalable linear-time attention models.",
    "translation": "标题：无误差线性注意力是免费午餐：基于连续时间动力学的精确解\n\n摘要：线性时间注意力与状态空间模型有望解决采用softmax注意力的长上下文语言模型中的二次计算成本瓶颈。本文提出无误差线性注意力——一种数值稳定、完全并行化且具有普适性的增量规则形式化方法。具体而言，我们将在线学习更新过程构建为连续时间动力系统，并证明其精确解不仅可获取，还能以线性时间复杂度和完全并行化方式计算。通过利用动力学矩阵的秩-1结构特性，我们直接推导出等效于无限阶龙格-库塔方法的精确闭式解。该注意力机制理论上不存在误差累积，在保持线性时间复杂度的同时完美捕捉连续动力学特性。通过大量实验验证，EFLA在噪声环境中表现出鲁棒性能，在不引入额外参数的情况下，相比DeltaNet实现了更低的语言建模困惑度和更优的下游基准性能。本研究为构建高保真、可扩展的线性时间注意力模型提供了新的理论基础。",
    "url": "https://huggingface.co/papers/2512.12602",
    "arxiv_url": "https://arxiv.org/abs/2512.12602"
  },
  {
    "title": "KlingAvatar 2.0 Technical Report",
    "summary": "Avatar video generation models have achieved remarkable progress in recent years. However, prior work exhibits limited efficiency in generating long-duration high-resolution videos, suffering from temporal drifting, quality degradation, and weak prompt following as video length increases. To address these challenges, we propose KlingAvatar 2.0, a spatio-temporal cascade framework that performs upscaling in both spatial resolution and temporal dimension. The framework first generates low-resolution blueprint video keyframes that capture global semantics and motion, and then refines them into high-resolution, temporally coherent sub-clips using a first-last frame strategy, while retaining smooth temporal transitions in long-form videos. To enhance cross-modal instruction fusion and alignment in extended videos, we introduce a Co-Reasoning Director composed of three modality-specific large language model (LLM) experts. These experts reason about modality priorities and infer underlying user intent, converting inputs into detailed storylines through multi-turn dialogue. A Negative Director further refines negative prompts to improve instruction alignment. Building on these components, we extend the framework to support ID-specific multi-character control. Extensive experiments demonstrate that our model effectively addresses the challenges of efficient, multimodally aligned long-form high-resolution video generation, delivering enhanced visual clarity, realistic lip-teeth rendering with accurate lip synchronization, strong identity preservation, and coherent multimodal instruction following.",
    "translation": "标题：KlingAvatar 2.0 技术报告\n\n摘要：近年来，虚拟形象视频生成模型取得了显著进展。然而，现有方法在生成长时长高分辨率视频时效率有限，随着视频长度增加，常出现时间漂移、质量下降及提示跟随能力弱等问题。为应对这些挑战，我们提出 KlingAvatar 2.0——一种在空间分辨率与时间维度上实现双重升级的时空级联框架。该框架首先生成捕捉全局语义与运动的低分辨率蓝图视频关键帧，随后通过首尾帧策略将其细化为高分辨率、时间连贯的视频片段，同时保持长视频中平滑的时间过渡。为增强长视频中的跨模态指令融合与对齐，我们引入了由三个模态专用大语言模型专家组成的协同推理指导器。这些专家通过多轮对话推理模态优先级并推断用户潜在意图，将输入转化为详细的故事线。负面指导器进一步优化负面提示以提升指令对齐效果。基于这些组件，我们将框架扩展至支持特定身份的多角色控制。大量实验表明，我们的模型能有效应对高效、多模态对齐的长时长高分辨率视频生成挑战，在视觉清晰度、具有精准唇形同步的逼真唇齿渲染、强身份保持力以及连贯的多模态指令跟随方面均表现出显著提升。",
    "url": "https://huggingface.co/papers/2512.13313",
    "arxiv_url": "https://arxiv.org/abs/2512.13313"
  },
  {
    "title": "MentraSuite: Post-Training Large Language Models for Mental Health Reasoning and Assessment",
    "summary": "Mental health disorders affect hundreds of millions globally, and the Web now serves as a primary medium for accessing support, information, and assessment. Large language models (LLMs) offer scalable and accessible assistance, yet their deployment in mental-health settings remains risky when their reasoning is incomplete, inconsistent, or ungrounded. Existing psychological LLMs emphasize emotional understanding or knowledge recall but overlook the step-wise, clinically aligned reasoning required for appraisal, diagnosis, intervention planning, abstraction, and verification. To address these issues, we introduce MentraSuite, a unified framework for advancing reliable mental-health reasoning. We propose MentraBench, a comprehensive benchmark spanning five core reasoning aspects, six tasks, and 13 datasets, evaluating both task performance and reasoning quality across five dimensions: conciseness, coherence, hallucination avoidance, task understanding, and internal consistency. We further present Mindora, a post-trained model optimized through a hybrid SFT-RL framework with an inconsistency-detection reward to enforce faithful and coherent reasoning. To support training, we construct high-quality trajectories using a novel reasoning trajectory generation strategy, that strategically filters difficult samples and applies a structured, consistency-oriented rewriting process to produce concise, readable, and well-balanced trajectories. Across 20 evaluated LLMs, Mindora achieves the highest average performance on MentraBench and shows remarkable performances in reasoning reliability, demonstrating its effectiveness for complex mental-health scenarios.",
    "translation": "标题：MentraSuite：面向心理健康推理与评估的大语言模型后训练框架\n\n摘要：心理健康障碍影响着全球数亿人口，而网络已成为获取支持、信息与评估的主要渠道。大语言模型能够提供可扩展且易于获取的辅助支持，但当其推理过程存在不完整、不一致或缺乏依据时，在心理健康场景中的部署仍存在风险。现有心理学大语言模型侧重于情感理解或知识复现，却忽视了评估、诊断、干预规划、抽象归纳及验证所需的渐进式、临床导向的推理能力。为解决这些问题，我们提出MentraSuite——一个推进可靠心理健康推理的统一框架。我们构建了MentraBench综合评估基准，涵盖五个核心推理维度、六类任务及13个数据集，从简洁性、连贯性、幻觉规避、任务理解与内部一致性五个维度系统评估任务表现与推理质量。进一步，我们提出通过混合SFT-RL框架后训练优化的Mindora模型，该框架采用不一致性检测奖励机制以确保忠实连贯的推理。为支持训练过程，我们通过创新的推理轨迹生成策略构建高质量轨迹数据，该策略通过筛选困难样本并实施结构化、一致性导向的重写流程，生成简洁可读且均衡优化的推理轨迹。在评估的20个大语言模型中，Mindora在MentraBench上取得最高平均性能，并在推理可靠性方面表现突出，证明了其在复杂心理健康场景中的有效性。",
    "url": "https://huggingface.co/papers/2512.09636",
    "arxiv_url": "https://arxiv.org/abs/2512.09636"
  },
  {
    "title": "Openpi Comet: Competition Solution For 2025 BEHAVIOR Challenge",
    "summary": "The 2025 BEHAVIOR Challenge is designed to rigorously track progress toward solving long-horizon tasks by physical agents in simulated environments. BEHAVIOR-1K focuses on everyday household tasks that people most want robots to assist with and these tasks introduce long-horizon mobile manipulation challenges in realistic settings, bridging the gap between current research and real-world, human-centric applications. This report presents our solution to the 2025 BEHAVIOR Challenge in a very close 2nd place and substantially outperforms the rest of the submissions. Building on π_{0.5}, we focus on systematically building our solution by studying the effects of training techniques and data. Through careful ablations, we show the scaling power in pre-training and post-training phases for competitive performance. We summarize our practical lessons and design recommendations that we hope will provide actionable insights for the broader embodied AI community when adapting powerful foundation models to complex embodied scenarios.",
    "translation": "标题：Openpi Comet：2025年BEHAVIOR挑战赛竞赛方案\n\n摘要：2025年BEHAVIOR挑战赛旨在严格追踪物理智能体在仿真环境中解决长程任务的研究进展。BEHAVIOR-1K聚焦于人们最期望机器人协助完成的日常家庭任务，这些任务在真实场景中引入了长程移动操作挑战，从而弥合了当前研究与现实世界、以人为中心的应用之间的差距。本报告介绍了我们在2025年BEHAVIOR挑战赛中获得亚军（与冠军成绩极为接近）的解决方案，其性能显著优于其他参赛方案。我们在π_{0.5}的基础上，通过系统研究训练技术与数据的影响来构建解决方案。经过细致的消融实验，我们证明了预训练与后训练阶段的扩展能力对提升竞赛性能的关键作用。我们总结了实践心得与设计建议，期望能为更广泛的具身智能社区在将强大基础模型适配复杂具身场景时提供可操作的参考。",
    "url": "https://huggingface.co/papers/2512.10071",
    "arxiv_url": "https://arxiv.org/abs/2512.10071"
  },
  {
    "title": "Spatial-Aware VLA Pretraining through Visual-Physical Alignment from Human Videos",
    "summary": "Vision-Language-Action (VLA) models provide a promising paradigm for robot learning by integrating visual perception with language-guided policy learning. However, most existing approaches rely on 2D visual inputs to perform actions in 3D physical environments, creating a significant gap between perception and action grounding. To bridge this gap, we propose a Spatial-Aware VLA Pretraining paradigm that performs explicit alignment between visual space and physical space during pretraining, enabling models to acquire 3D spatial understanding before robot policy learning. Starting from pretrained vision-language models, we leverage large-scale human demonstration videos to extract 3D visual and 3D action annotations, forming a new source of supervision that aligns 2D visual observations with 3D spatial reasoning. We instantiate this paradigm with VIPA-VLA, a dual-encoder architecture that incorporates a 3D visual encoder to augment semantic visual representations with 3D-aware features. When adapted to downstream robot tasks, VIPA-VLA achieves significantly improved grounding between 2D vision and 3D action, resulting in more robust and generalizable robotic policies.",
    "translation": "标题：基于人类视频视觉-物理对齐的空间感知视觉-语言-动作预训练\n\n摘要：视觉-语言-动作模型通过将视觉感知与语言引导的策略学习相结合，为机器人学习提供了有前景的范式。然而，现有方法大多依赖二维视觉输入在三维物理环境中执行动作，导致感知与动作落地之间存在显著鸿沟。为弥合这一差距，本文提出一种空间感知的视觉-语言-动作预训练范式，通过在预训练阶段显式对齐视觉空间与物理空间，使模型在机器人策略学习前即可获得三维空间理解能力。基于预训练的视觉-语言模型，我们利用大规模人类示范视频提取三维视觉标注与三维动作标注，构建出能够对齐二维视觉观测与三维空间推理的新型监督信号。基于该范式，我们构建了VIPA-VLA模型——采用双编码器架构，通过引入三维视觉编码器将空间感知特征融入语义视觉表征。在下游机器人任务适配中，VIPA-VLA显著提升了二维视觉与三维动作的对接能力，从而产生更具鲁棒性与泛化性的机器人策略。",
    "url": "https://huggingface.co/papers/2512.13080",
    "arxiv_url": "https://arxiv.org/abs/2512.13080"
  },
  {
    "title": "WebOperator: Action-Aware Tree Search for Autonomous Agents in Web Environment",
    "summary": "LLM-based agents often operate in a greedy, step-by-step manner, selecting actions solely based on the current observation without considering long-term consequences or alternative paths. This lack of foresight is particularly problematic in web environments, which are only partially observable-limited to browser-visible content (e.g., DOM and UI elements)-where a single misstep often requires complex and brittle navigation to undo. Without an explicit backtracking mechanism, agents struggle to correct errors or systematically explore alternative paths. Tree-search methods provide a principled framework for such structured exploration, but existing approaches lack mechanisms for safe backtracking, making them prone to unintended side effects. They also assume that all actions are reversible, ignoring the presence of irreversible actions-limitations that reduce their effectiveness in realistic web tasks. To address these challenges, we introduce WebOperator, a tree-search framework that enables reliable backtracking and strategic exploration. Our method incorporates a best-first search strategy that ranks actions by both reward estimates and safety considerations, along with a robust backtracking mechanism that verifies the feasibility of previously visited paths before replaying them, preventing unintended side effects. To further guide exploration, WebOperator generates action candidates from multiple, varied reasoning contexts to ensure diverse and robust exploration, and subsequently curates a high-quality action set by filtering out invalid actions pre-execution and merging semantically equivalent ones. Experimental results on WebArena and WebVoyager demonstrate the effectiveness of WebOperator. On WebArena, WebOperator achieves a state-of-the-art 54.6% success rate with gpt-4o, underscoring the critical advantage of integrating strategic foresight with safe execution.",
    "translation": "标题：WebOperator：面向网络环境中自主智能体的动作感知树搜索方法\n\n摘要：基于大语言模型的智能体通常以贪婪的逐步方式运行，仅根据当前观察选择动作，而不考虑长期后果或替代路径。这种前瞻性的缺失在网络环境中尤为突出——由于环境仅部分可观测（仅限于浏览器可见内容，如DOM和UI元素），单个失误往往需要通过复杂且脆弱的导航操作才能撤销。若缺乏显式的回溯机制，智能体难以纠正错误或系统性地探索替代路径。树搜索方法为此类结构化探索提供了原则性框架，但现有方法缺乏安全回溯机制，容易引发非预期的副作用。同时，这些方法假设所有动作皆可逆，忽略了实际网络任务中不可逆动作的存在，从而降低了其在真实场景中的有效性。为应对这些挑战，我们提出了WebOperator——一种支持可靠回溯与策略性探索的树搜索框架。该方法融合了最佳优先搜索策略，通过奖励估计与安全性评估对动作进行排序，并配备鲁棒的回溯机制，在重放历史路径前验证其可行性，从而避免非预期副作用。为进一步引导探索过程，WebOperator从多样化推理语境中生成候选动作集以保证探索的多样性与鲁棒性，随后通过预执行过滤无效动作、合并语义等价动作的方式，构建高质量动作集合。在WebArena和WebVoyager平台上的实验结果表明了WebOperator的有效性。在WebArena测试中，WebOperator结合gpt-4o实现了54.6%的最优成功率，凸显了策略性前瞻与安全执行相结合的关键优势。",
    "url": "https://huggingface.co/papers/2512.12692",
    "arxiv_url": "https://arxiv.org/abs/2512.12692"
  },
  {
    "title": "DrivePI: Spatial-aware 4D MLLM for Unified Autonomous Driving Understanding, Perception, Prediction and Planning",
    "summary": "Although multi-modal large language models (MLLMs) have shown strong capabilities across diverse domains, their application in generating fine-grained 3D perception and prediction outputs in autonomous driving remains underexplored. In this paper, we propose DrivePI, a novel spatial-aware 4D MLLM that serves as a unified Vision-Language-Action (VLA) framework that is also compatible with vision-action (VA) models. Our method jointly performs spatial understanding, 3D perception (i.e., 3D occupancy), prediction (i.e., occupancy flow), and planning (i.e., action outputs) in parallel through end-to-end optimization. To obtain both precise geometric information and rich visual appearance, our approach integrates point clouds, multi-view images, and language instructions within a unified MLLM architecture. We further develop a data engine to generate text-occupancy and text-flow QA pairs for 4D spatial understanding. Remarkably, with only a 0.5B Qwen2.5 model as MLLM backbone, DrivePI as a single unified model matches or exceeds both existing VLA models and specialized VA models. Specifically, compared to VLA models, DrivePI outperforms OpenDriveVLA-7B by 2.5% mean accuracy on nuScenes-QA and reduces collision rate by 70% over ORION (from 0.37% to 0.11%) on nuScenes. Against specialized VA models, DrivePI surpasses FB-OCC by 10.3 RayIoU for 3D occupancy on OpenOcc, reduces the mAVE from 0.591 to 0.509 for occupancy flow on OpenOcc, and achieves 32% lower L2 error than VAD (from 0.72m to 0.49m) for planning on nuScenes. Code will be available at https://github.com/happinesslz/DrivePI",
    "translation": "标题：DrivePI：面向统一自动驾驶理解、感知、预测与规划的空间感知四维多模态大语言模型\n\n摘要：尽管多模态大语言模型（MLLMs）已在多个领域展现出强大能力，但其在自动驾驶中生成细粒度三维感知与预测输出方面的应用仍待深入探索。本文提出DrivePI，一种新颖的空间感知四维MLLM，它作为一个统一的视觉-语言-动作框架，同时兼容视觉-动作模型。我们的方法通过端到端优化并行实现空间理解、三维感知（即三维占据）、预测（即占据流）与规划（即动作输出）。为同时获取精确的几何信息与丰富的视觉外观，本方法将点云、多视角图像及语言指令整合于统一的MLLM架构中。我们进一步开发了数据引擎，用于生成面向四维空间理解的文本-占据与文本-流问答对。值得注意的是，仅以0.5B参数的Qwen2.5模型作为MLLM骨干，DrivePI作为单一统一模型在性能上达到或超越了现有视觉-语言-动作模型及专用视觉-动作模型。具体而言，相较于视觉-语言-动作模型，DrivePI在nuScenes-QA数据集上的平均准确率较OpenDriveVLA-7B提升2.5%，在nuScenes数据集上的碰撞率较ORION降低70%（从0.37%降至0.11%）。与专用视觉-动作模型相比，DrivePI在OpenOcc数据集上的三维占据任务中RayIoU指标超越FB-OCC达10.3，在占据流任务中将mAVE从0.591降至0.509，并在nuScenes规划任务中较VAD实现32%的L2误差降低（从0.72米降至0.49米）。代码将在https://github.com/happinesslz/DrivePI 公开。",
    "url": "https://huggingface.co/papers/2512.12799",
    "arxiv_url": "https://arxiv.org/abs/2512.12799"
  },
  {
    "title": "V-REX: Benchmarking Exploratory Visual Reasoning via Chain-of-Questions",
    "summary": "While many vision-language models (VLMs) are developed to answer well-defined, straightforward questions with highly specified targets, as in most benchmarks, they often struggle in practice with complex open-ended tasks, which usually require multiple rounds of exploration and reasoning in the visual space. Such visual thinking paths not only provide step-by-step exploration and verification as an AI detective but also produce better interpretations of the final answers. However, these paths are challenging to evaluate due to the large exploration space of intermediate steps. To bridge the gap, we develop an evaluation suite, ``Visual Reasoning with multi-step EXploration (V-REX)'', which is composed of a benchmark of challenging visual reasoning tasks requiring native multi-step exploration and an evaluation protocol. V-REX covers rich application scenarios across diverse domains. V-REX casts the multi-step exploratory reasoning into a Chain-of-Questions (CoQ) and disentangles VLMs' capability to (1) Planning: breaking down an open-ended task by selecting a chain of exploratory questions; and (2) Following: answering curated CoQ sequentially to collect information for deriving the final answer. By curating finite options of questions and answers per step, V-REX achieves a reliable quantitative and fine-grained analysis of the intermediate steps. By assessing SOTA proprietary and open-sourced VLMs, we reveal consistent scaling trends, significant differences between planning and following abilities, and substantial room for improvement in multi-step exploratory reasoning.",
    "translation": "标题：V-REX：基于问题链的探索性视觉推理基准测试\n\n摘要：尽管许多视觉语言模型（VLM）被开发用于回答定义明确、目标具体的直接问题（如大多数基准测试所示），但在实践中，它们往往难以应对复杂的开放式任务。这类任务通常需要在视觉空间中进行多轮探索与推理。此类视觉思维路径不仅能够像AI侦探一样提供逐步探索与验证，还能对最终答案产生更好的解释。然而，由于中间步骤的探索空间巨大，这些路径的评估极具挑战性。为弥合这一差距，我们开发了一个评估套件——“多步探索视觉推理（V-REX）”，该套件包含一个需要原生多步探索的挑战性视觉推理任务基准以及相应的评估协议。V-REX涵盖了跨领域的丰富应用场景。它将多步探索性推理转化为“问题链”（CoQ），并解构了VLM的两方面能力：（1）规划能力：通过选择一系列探索性问题来分解开放式任务；（2）执行能力：依次回答精心设计的问题链，以收集信息并推导最终答案。通过为每个步骤设计有限的问题与答案选项，V-REX实现了对中间步骤可靠、定量且细粒度的分析。通过对当前最先进的专有及开源VLM进行评估，我们揭示了其一致的能力扩展趋势、规划与执行能力间的显著差异，以及多步探索性推理方面存在的巨大改进空间。",
    "url": "https://huggingface.co/papers/2512.11995",
    "arxiv_url": "https://arxiv.org/abs/2512.11995"
  },
  {
    "title": "Toward Ambulatory Vision: Learning Visually-Grounded Active View Selection",
    "summary": "Vision Language Models (VLMs) excel at visual question answering (VQA) but remain limited to snapshot vision, reasoning from static images. In contrast, embodied agents require ambulatory vision, actively moving to obtain more informative views. We introduce Visually Grounded Active View Selection (VG-AVS), a task that selects the most informative next viewpoint using only the visual information in the current image, without relying on scene memory or external knowledge. To support this task, we construct a synthetic dataset with automatically generated paired query-target views and question-answer prompts. We also propose a framework that fine-tunes pretrained VLMs through supervised fine-tuning (SFT) followed by RL-based policy optimization. Our approach achieves strong question answering performance based on viewpoint selection and generalizes robustly to unseen synthetic and real scenes. Furthermore, incorporating our learned VG-AVS framework into existing scene-exploration-based EQA systems improves downstream question-answering accuracy.",
    "translation": "标题：迈向动态视觉：基于视觉感知的主动视角选择学习\n\n摘要：视觉语言模型在视觉问答任务中表现出色，但其仍局限于静态视觉感知，仅能基于单张图像进行推理。相比之下，具身智能体需要动态视觉能力，通过主动移动获取信息量更丰富的观察视角。本文提出视觉感知的主动视角选择任务，该任务仅利用当前图像的视觉信息选择信息量最大的下一观察视角，无需依赖场景记忆或外部知识。为支持该任务，我们构建了包含自动生成的配对查询-目标视角及问答提示的合成数据集。同时提出一种通过监督微调与基于强化学习的策略优化相结合、对预训练视觉语言模型进行微调的框架。该方法在基于视角选择的问答任务中表现出色，并能稳健地泛化至未见过的合成场景与真实场景。此外，将学习得到的视觉感知主动视角选择框架整合至现有基于场景探索的具身问答系统中，可有效提升下游问答任务的准确率。",
    "url": "https://huggingface.co/papers/2512.13250",
    "arxiv_url": "https://arxiv.org/abs/2512.13250"
  },
  {
    "title": "Image Diffusion Preview with Consistency Solver",
    "summary": "The slow inference process of image diffusion models significantly degrades interactive user experiences. To address this, we introduce Diffusion Preview, a novel paradigm employing rapid, low-step sampling to generate preliminary outputs for user evaluation, deferring full-step refinement until the preview is deemed satisfactory. Existing acceleration methods, including training-free solvers and post-training distillation, struggle to deliver high-quality previews or ensure consistency between previews and final outputs. We propose ConsistencySolver derived from general linear multistep methods, a lightweight, trainable high-order solver optimized via Reinforcement Learning, that enhances preview quality and consistency. Experimental results demonstrate that ConsistencySolver significantly improves generation quality and consistency in low-step scenarios, making it ideal for efficient preview-and-refine workflows. Notably, it achieves FID scores on-par with Multistep DPM-Solver using 47% fewer steps, while outperforming distillation baselines. Furthermore, user studies indicate our approach reduces overall user interaction time by nearly 50% while maintaining generation quality. Code is available at https://github.com/G-U-N/consolver.",
    "translation": "标题：基于一致性求解器的图像扩散预览方法\n\n摘要：图像扩散模型的缓慢推理过程严重影响了交互式用户体验。为此，我们提出扩散预览这一新范式，通过快速低步数采样生成初步输出供用户评估，仅在预览结果达到要求后才进行全步数精细化处理。现有加速方法（包括免训练求解器与训练后蒸馏技术）难以同时实现高质量预览并确保预览与最终输出的一致性。我们提出一致性求解器——一种源自通用线性多步方法的轻量级可训练高阶求解器，通过强化学习进行优化，以提升预览质量与一致性。实验结果表明，一致性求解器在低步数场景下显著提升了生成质量与一致性，使其成为高效预览-优化工作流程的理想选择。该方法仅需减少47%的步数即可达到与多步DPM-Solver相当的FID分数，同时优于蒸馏基线方法。此外，用户研究表明我们的方法在保持生成质量的同时，将整体用户交互时间减少了近50%。代码已发布于https://github.com/G-U-N/consolver。",
    "url": "https://huggingface.co/papers/2512.13592",
    "arxiv_url": "https://arxiv.org/abs/2512.13592"
  },
  {
    "title": "VLSA: Vision-Language-Action Models with Plug-and-Play Safety Constraint Layer",
    "summary": "Vision-Language-Action (VLA) models have demonstrated remarkable capabilities in generalizing across diverse robotic manipulation tasks. However, deploying these models in unstructured environments remains challenging due to the critical need for simultaneous task compliance and safety assurance, particularly in preventing potential collisions during physical interactions. In this work, we introduce a Vision-Language-Safe Action (VLSA) architecture, named AEGIS, which contains a plug-and-play safety constraint (SC) layer formulated via control barrier functions. AEGIS integrates directly with existing VLA models to improve safety with theoretical guarantees, while maintaining their original instruction-following performance. To evaluate the efficacy of our architecture, we construct a comprehensive safety-critical benchmark SafeLIBERO, spanning distinct manipulation scenarios characterized by varying degrees of spatial complexity and obstacle intervention. Extensive experiments demonstrate the superiority of our method over state-of-the-art baselines. Notably, AEGIS achieves a 59.16% improvement in obstacle avoidance rate while substantially increasing the task execution success rate by 17.25%. To facilitate reproducibility and future research, we make our code, models, and the benchmark datasets publicly available at https://vlsa-aegis.github.io/.",
    "translation": "标题：VLSA：具备即插即用安全约束层的视觉-语言-动作模型\n\n摘要：视觉-语言-动作（VLA）模型在多样化机器人操作任务中展现出卓越的泛化能力。然而，由于需同时满足任务执行与安全保障的严格要求（特别是在物理交互中防止潜在碰撞），在非结构化环境中部署此类模型仍面临挑战。本研究提出一种名为AEGIS的视觉-语言-安全动作（VLSA）架构，该架构通过控制屏障函数构建了即插即用的安全约束层。AEGIS可直接与现有VLA模型集成，在保持其原始指令跟随性能的同时，以理论保证提升系统安全性。为评估该架构效能，我们构建了涵盖不同空间复杂度与障碍物干预特征的综合性安全关键基准测试集SafeLIBERO。大量实验证明，本方法显著优于现有先进基线模型。值得注意的是，AEGIS在障碍物规避率上实现59.16%的提升，同时将任务执行成功率大幅提高17.25%。为促进可复现性与未来研究，我们已将代码、模型及基准数据集公开于https://vlsa-aegis.github.io/。",
    "url": "https://huggingface.co/papers/2512.11891",
    "arxiv_url": "https://arxiv.org/abs/2512.11891"
  },
  {
    "title": "Aesthetic Alignment Risks Assimilation: How Image Generation and Reward Models Reinforce Beauty Bias and Ideological \"Censorship\"",
    "summary": "Over-aligning image generation models to a generalized aesthetic preference conflicts with user intent, particularly when ``anti-aesthetic\" outputs are requested for artistic or critical purposes. This adherence prioritizes developer-centered values, compromising user autonomy and aesthetic pluralism. We test this bias by constructing a wide-spectrum aesthetics dataset and evaluating state-of-the-art generation and reward models. We find that aesthetic-aligned generation models frequently default to conventionally beautiful outputs, failing to respect instructions for low-quality or negative imagery. Crucially, reward models penalize anti-aesthetic images even when they perfectly match the explicit user prompt. We confirm this systemic bias through image-to-image editing and evaluation against real abstract artworks.",
    "translation": "标题：审美对齐风险同化：图像生成与奖励模型如何强化审美偏见与意识形态“审查”\n\n摘要：将图像生成模型过度对齐于广义审美偏好会与用户意图产生冲突，尤其在用户出于艺术或批判目的请求“反审美”输出时。这种对齐机制以开发者中心价值观为优先，损害了用户自主性与审美多元性。我们通过构建广谱审美数据集并评估前沿生成模型与奖励模型来检验这一偏见。研究发现，经过审美对齐的生成模型常默认输出符合传统审美的图像，无法有效响应关于低质量或负面意象的生成指令。关键问题在于，奖励模型会对反审美图像施加惩罚，即使这些图像完全符合用户的显式指令。我们通过图像编辑实验以及与真实抽象艺术作品的对比评估，证实了这一系统性偏见的存在。",
    "url": "https://huggingface.co/papers/2512.11883",
    "arxiv_url": "https://arxiv.org/abs/2512.11883"
  },
  {
    "title": "Towards Interactive Intelligence for Digital Humans",
    "summary": "We introduce Interactive Intelligence, a novel paradigm of digital human that is capable of personality-aligned expression, adaptive interaction, and self-evolution. To realize this, we present Mio (Multimodal Interactive Omni-Avatar), an end-to-end framework composed of five specialized modules: Thinker, Talker, Face Animator, Body Animator, and Renderer. This unified architecture integrates cognitive reasoning with real-time multimodal embodiment to enable fluid, consistent interaction. Furthermore, we establish a new benchmark to rigorously evaluate the capabilities of interactive intelligence. Extensive experiments demonstrate that our framework achieves superior performance compared to state-of-the-art methods across all evaluated dimensions. Together, these contributions move digital humans beyond superficial imitation toward intelligent interaction.",
    "translation": "标题：迈向数字人的交互智能\n\n摘要：本文提出“交互智能”这一数字人新范式，其具备人格对齐表达、自适应交互与自主进化能力。为实现该目标，我们构建了Mio（多模态交互全能化身）——一个由五大专业模块构成的端到端框架：思维模块、语音模块、面部动画模块、身体动画模块与渲染模块。该统一架构将认知推理与实时多模态具身化相结合，实现了流畅且连贯的交互体验。此外，我们建立了全新基准体系以系统评估交互智能的核心能力。大量实验表明，本框架在所有评估维度上均优于当前最先进方法。这些成果共同推动数字人从表层模仿迈向智能交互的新阶段。",
    "url": "https://huggingface.co/papers/2512.13674",
    "arxiv_url": "https://arxiv.org/abs/2512.13674"
  },
  {
    "title": "GenieDrive: Towards Physics-Aware Driving World Model with 4D Occupancy Guided Video Generation",
    "summary": "Physics-aware driving world model is essential for drive planning, out-of-distribution data synthesis, and closed-loop evaluation. However, existing methods often rely on a single diffusion model to directly map driving actions to videos, which makes learning difficult and leads to physically inconsistent outputs. To overcome these challenges, we propose GenieDrive, a novel framework designed for physics-aware driving video generation. Our approach starts by generating 4D occupancy, which serves as a physics-informed foundation for subsequent video generation. 4D occupancy contains rich physical information, including high-resolution 3D structures and dynamics. To facilitate effective compression of such high-resolution occupancy, we propose a VAE that encodes occupancy into a latent tri-plane representation, reducing the latent size to only 58% of that used in previous methods. We further introduce Mutual Control Attention (MCA) to accurately model the influence of control on occupancy evolution, and we jointly train the VAE and the subsequent prediction module in an end-to-end manner to maximize forecasting accuracy. Together, these designs yield a 7.2% improvement in forecasting mIoU at an inference speed of 41 FPS, while using only 3.47 M parameters. Additionally, a Normalized Multi-View Attention is introduced in the video generation model to generate multi-view driving videos with guidance from our 4D occupancy, significantly improving video quality with a 20.7% reduction in FVD. Experiments demonstrate that GenieDrive enables highly controllable, multi-view consistent, and physics-aware driving video generation.",
    "translation": "标题：GenieDrive：基于四维占据栅格引导视频生成的物理感知驾驶世界模型\n\n摘要：物理感知的驾驶世界模型对于驾驶规划、分布外数据合成以及闭环评估至关重要。然而，现有方法通常依赖单一扩散模型直接将驾驶动作映射为视频，这导致学习困难并产生物理不一致的输出。为克服这些挑战，我们提出GenieDrive——一个专为物理感知驾驶视频生成设计的新型框架。我们的方法首先生成四维占据栅格，以此为后续视频生成提供物理信息基础。四维占据栅格包含丰富的物理信息，包括高分辨率三维结构与动态特性。为有效压缩此类高分辨率占据栅格，我们提出一种变分自编码器，将占据栅格编码为潜在三平面表示，将潜在空间尺寸降至先前方法的58%。我们进一步引入互控注意力机制，以精确建模控制信号对占据栅格演化的影响，并以端到端方式联合训练变分自编码器与后续预测模块，从而最大化预测精度。这些设计共同实现了推理速度41 FPS下预测mIoU指标7.2%的提升，且仅使用347万参数。此外，我们在视频生成模型中引入归一化多视角注意力机制，通过四维占据栅格引导生成多视角驾驶视频，使FVD指标降低20.7%，显著提升视频质量。实验表明，GenieDrive能够实现高度可控、多视角一致且具有物理感知的驾驶视频生成。",
    "url": "https://huggingface.co/papers/2512.12751",
    "arxiv_url": "https://arxiv.org/abs/2512.12751"
  },
  {
    "title": "What matters for Representation Alignment: Global Information or Spatial Structure?",
    "summary": "Representation alignment (REPA) guides generative training by distilling representations from a strong, pretrained vision encoder to intermediate diffusion features. We investigate a fundamental question: what aspect of the target representation matters for generation, its global semantic information (e.g., measured by ImageNet-1K accuracy) or its spatial structure (i.e. pairwise cosine similarity between patch tokens)? Prevalent wisdom holds that stronger global semantic performance leads to better generation as a target representation. To study this, we first perform a large-scale empirical analysis across 27 different vision encoders and different model scales. The results are surprising; spatial structure, rather than global performance, drives the generation performance of a target representation. To further study this, we introduce two straightforward modifications, which specifically accentuate the transfer of spatial information. We replace the standard MLP projection layer in REPA with a simple convolution layer and introduce a spatial normalization layer for the external representation. Surprisingly, our simple method (implemented in <4 lines of code), termed iREPA, consistently improves convergence speed of REPA, across a diverse set of vision encoders, model sizes, and training variants (such as REPA, REPA-E, Meanflow, JiT etc). %, etc. Our work motivates revisiting the fundamental working mechanism of representational alignment and how it can be leveraged for improved training of generative models. The code and project page are available at https://end2end-diffusion.github.io/irepa",
    "translation": "标题：表征对齐的关键：全局信息还是空间结构？\n\n摘要：表征对齐（REPA）通过将预训练的强视觉编码器中的表征蒸馏至扩散模型的中间特征，从而指导生成式模型的训练。本文探究一个核心问题：目标表征的哪个方面对生成任务更为关键——是其全局语义信息（例如通过ImageNet-1K准确率衡量），还是其空间结构（即图像块标记之间的成对余弦相似度）？普遍观点认为，作为目标表征，更强的全局语义性能会带来更好的生成效果。为研究此问题，我们首先对27种不同的视觉编码器及不同模型规模进行了大规模实证分析。结果令人意外：驱动目标表征生成性能的主要因素是空间结构，而非全局性能。为进一步探究，我们引入了两种直接改进方法，专门强化空间信息的传递：将REPA中标准的MLP投影层替换为简单卷积层，并为外部表征引入空间归一化层。令人惊讶的是，我们提出的简洁方法（代码实现少于4行），称为iREPA，在多种视觉编码器、模型规模和训练变体（如REPA、REPA-E、Meanflow、JiT等）中均能持续提升REPA的收敛速度。本研究促使我们重新审视表征对齐的基本工作机制，并探索如何利用其改进生成模型的训练。代码与项目页面详见：https://end2end-diffusion.github.io/irepa",
    "url": "https://huggingface.co/papers/2512.10794",
    "arxiv_url": "https://arxiv.org/abs/2512.10794"
  },
  {
    "title": "RecTok: Reconstruction Distillation along Rectified Flow",
    "summary": "Visual tokenizers play a crucial role in diffusion models. The dimensionality of latent space governs both reconstruction fidelity and the semantic expressiveness of the latent feature. However, a fundamental trade-off is inherent between dimensionality and generation quality, constraining existing methods to low-dimensional latent spaces. Although recent works have leveraged vision foundation models to enrich the semantics of visual tokenizers and accelerate convergence, high-dimensional tokenizers still underperform their low-dimensional counterparts. In this work, we propose RecTok, which overcomes the limitations of high-dimensional visual tokenizers through two key innovations: flow semantic distillation and reconstruction--alignment distillation. Our key insight is to make the forward flow in flow matching semantically rich, which serves as the training space of diffusion transformers, rather than focusing on the latent space as in previous works. Specifically, our method distills the semantic information in VFMs into the forward flow trajectories in flow matching. And we further enhance the semantics by introducing a masked feature reconstruction loss. Our RecTok achieves superior image reconstruction, generation quality, and discriminative performance. It achieves state-of-the-art results on the gFID-50K under both with and without classifier-free guidance settings, while maintaining a semantically rich latent space structure. Furthermore, as the latent dimensionality increases, we observe consistent improvements. Code and model are available at https://shi-qingyu.github.io/rectok.github.io.",
    "translation": "标题：RecTok：沿整流流的重建蒸馏\n\n摘要：视觉分词器在扩散模型中起着关键作用。潜在空间的维度同时决定了重建保真度和潜在特征的语义表达能力。然而，维度与生成质量之间存在着固有的权衡，这限制了现有方法只能采用低维潜在空间。尽管近期研究利用视觉基础模型来增强视觉分词器的语义并加速收敛，但高维分词器的性能仍逊于低维版本。本研究提出RecTok方法，通过两项关键创新克服高维视觉分词器的局限性：流语义蒸馏和重建对齐蒸馏。我们的核心思路是使流匹配中的前向流具有丰富的语义，并将其作为扩散变换器的训练空间，而非如以往研究那样聚焦于潜在空间。具体而言，我们的方法将视觉基础模型中的语义信息蒸馏至流匹配的前向流轨迹中，并通过引入掩码特征重建损失进一步增强语义表达能力。RecTok在图像重建、生成质量和判别性能方面均表现优异，在有无分类器引导的gFID-50K基准测试中均取得最先进的结果，同时保持了语义丰富的潜在空间结构。此外，随着潜在维度的增加，我们观察到性能的持续提升。代码与模型已发布于https://shi-qingyu.github.io/rectok.github.io。",
    "url": "https://huggingface.co/papers/2512.13421",
    "arxiv_url": "https://arxiv.org/abs/2512.13421"
  },
  {
    "title": "Few-Step Distillation for Text-to-Image Generation: A Practical Guide",
    "summary": "Diffusion distillation has dramatically accelerated class-conditional image synthesis, but its applicability to open-ended text-to-image (T2I) generation is still unclear. We present the first systematic study that adapts and compares state-of-the-art distillation techniques on a strong T2I teacher model, FLUX.1-lite. By casting existing methods into a unified framework, we identify the key obstacles that arise when moving from discrete class labels to free-form language prompts. Beyond a thorough methodological analysis, we offer practical guidelines on input scaling, network architecture, and hyperparameters, accompanied by an open-source implementation and pretrained student models. Our findings establish a solid foundation for deploying fast, high-fidelity, and resource-efficient diffusion generators in real-world T2I applications. Code is available on github.com/alibaba-damo-academy/T2I-Distill.",
    "translation": "标题：文本到图像生成的少步蒸馏：实用指南\n\n摘要：扩散蒸馏技术已显著加速了类别条件图像合成，但其在开放式文本到图像生成中的应用前景仍不明朗。本文首次系统性地研究并比较了在强大的文本到图像教师模型FLUX.1-lite上适配的先进蒸馏技术。通过将现有方法纳入统一框架，我们揭示了从离散类别标签转向自由形式语言提示时出现的关键障碍。除深入的方法论分析外，我们还提供了关于输入缩放、网络架构和超参数的实用指南，并同步开源了实现代码与预训练学生模型。本研究为在实际文本到图像应用中部署快速、高保真且资源高效的扩散生成器奠定了坚实基础。代码发布于github.com/alibaba-damo-academy/T2I-Distill。",
    "url": "https://huggingface.co/papers/2512.13006",
    "arxiv_url": "https://arxiv.org/abs/2512.13006"
  },
  {
    "title": "AutoMV: An Automatic Multi-Agent System for Music Video Generation",
    "summary": "Music-to-Video (M2V) generation for full-length songs faces significant challenges. Existing methods produce short, disjointed clips, failing to align visuals with musical structure, beats, or lyrics, and lack temporal consistency. We propose AutoMV, a multi-agent system that generates full music videos (MVs) directly from a song. AutoMV first applies music processing tools to extract musical attributes, such as structure, vocal tracks, and time-aligned lyrics, and constructs these features as contextual inputs for following agents. The screenwriter Agent and director Agent then use this information to design short script, define character profiles in a shared external bank, and specify camera instructions. Subsequently, these agents call the image generator for keyframes and different video generators for \"story\" or \"singer\" scenes. A Verifier Agent evaluates their output, enabling multi-agent collaboration to produce a coherent longform MV. To evaluate M2V generation, we further propose a benchmark with four high-level categories (Music Content, Technical, Post-production, Art) and twelve ine-grained criteria. This benchmark was applied to compare commercial products, AutoMV, and human-directed MVs with expert human raters: AutoMV outperforms current baselines significantly across all four categories, narrowing the gap to professional MVs. Finally, we investigate using large multimodal models as automatic MV judges; while promising, they still lag behind human expert, highlighting room for future work.",
    "translation": "标题：AutoMV：一种用于音乐视频生成的自动化多智能体系统\n\n摘要：针对完整歌曲的音乐到视频（M2V）生成面临重大挑战。现有方法生成的视频片段短且不连贯，无法将视觉内容与音乐结构、节拍或歌词对齐，并缺乏时间一致性。本文提出AutoMV，一种能够直接从歌曲生成完整音乐视频（MV）的多智能体系统。AutoMV首先应用音乐处理工具提取音乐属性（如结构、人声音轨和时间对齐的歌词），并将这些特征构建为后续智能体的上下文输入。随后，编剧智能体与导演智能体利用这些信息设计简短剧本、在共享外部库中定义角色档案，并指定镜头指令。接着，这些智能体调用图像生成器生成关键帧，并调用不同的视频生成器分别生成“故事”场景和“歌手”场景。验证智能体对输出结果进行评估，通过多智能体协作生成连贯的长篇音乐视频。为评估M2V生成效果，我们进一步提出包含四大高级类别（音乐内容、技术、后期制作、艺术）和十二项细粒度指标的基准体系。应用该基准对商业产品、AutoMV及人工执导的MV进行专家人工评分比较：AutoMV在全部四个类别上均显著优于现有基线方法，缩小了与专业MV的差距。最后，我们探索使用大型多模态模型作为自动MV评估工具；虽然该方法前景可观，但仍落后于人类专家，这为未来研究指明了改进方向。",
    "url": "https://huggingface.co/papers/2512.12196",
    "arxiv_url": "https://arxiv.org/abs/2512.12196"
  },
  {
    "title": "Flowception: Temporally Expansive Flow Matching for Video Generation",
    "summary": "We present Flowception, a novel non-autoregressive and variable-length video generation framework. Flowception learns a probability path that interleaves discrete frame insertions with continuous frame denoising. Compared to autoregressive methods, Flowception alleviates error accumulation/drift as the frame insertion mechanism during sampling serves as an efficient compression mechanism to handle long-term context. Compared to full-sequence flows, our method reduces FLOPs for training three-fold, while also being more amenable to local attention variants, and allowing to learn the length of videos jointly with their content. Quantitative experimental results show improved FVD and VBench metrics over autoregressive and full-sequence baselines, which is further validated with qualitative results. Finally, by learning to insert and denoise frames in a sequence, Flowception seamlessly integrates different tasks such as image-to-video generation and video interpolation.",
    "translation": "标题：Flowception：面向视频生成的时序扩展流匹配方法\n\n摘要：本文提出Flowception，一种新颖的非自回归可变长度视频生成框架。该框架通过学习交织离散帧插入与连续帧去噪的概率路径实现视频生成。相较于自回归方法，Flowception通过采样过程中的帧插入机制有效缓解误差累积/漂移问题，该机制可作为处理长期上下文的高效压缩策略。与全序列流方法相比，本方法将训练浮点运算量降低至三分之一，同时更适配局部注意力变体，并能实现视频时长与内容的联合学习。定量实验结果显示，本方法在FVD和VBench指标上均优于自回归与全序列基线模型，定性分析结果进一步验证了其有效性。通过序列中的帧插入与去噪协同学习机制，Flowception可无缝集成图像到视频生成、视频插帧等多样化任务。\n\n请按照以下格式返回：\n标题：Flowception：面向视频生成的时序扩展流匹配方法\n摘要：本文提出Flowception，一种新颖的非自回归可变长度视频生成框架。该框架通过学习交织离散帧插入与连续帧去噪的概率路径实现视频生成。相较于自回归方法，Flowception通过采样过程中的帧插入机制有效缓解误差累积/漂移问题，该机制可作为处理长期上下文的高效压缩策略。与全序列流方法相比，本方法将训练浮点运算量降低至三分之一，同时更适配局部注意力变体，并能实现视频时长与内容的联合学习。定量实验结果显示，本方法在FVD和VBench指标上均优于自回归与全序列基线模型，定性分析结果进一步验证了其有效性。通过序列中的帧插入与去噪协同学习机制，Flowception可无缝集成图像到视频生成、视频插帧等多样化任务。",
    "url": "https://huggingface.co/papers/2512.11438",
    "arxiv_url": "https://arxiv.org/abs/2512.11438"
  },
  {
    "title": "CAPTAIN: Semantic Feature Injection for Memorization Mitigation in Text-to-Image Diffusion Models",
    "summary": "Diffusion models can unintentionally reproduce training examples, raising privacy and copyright concerns as these systems are increasingly deployed at scale. Existing inference-time mitigation methods typically manipulate classifier-free guidance (CFG) or perturb prompt embeddings; however, they often struggle to reduce memorization without compromising alignment with the conditioning prompt. We introduce CAPTAIN, a training-free framework that mitigates memorization by directly modifying latent features during denoising. CAPTAIN first applies frequency-based noise initialization to reduce the tendency to replicate memorized patterns early in the denoising process. It then identifies the optimal denoising timesteps for feature injection and localizes memorized regions. Finally, CAPTAIN injects semantically aligned features from non-memorized reference images into localized latent regions, suppressing memorization while preserving prompt fidelity and visual quality. Our experiments show that CAPTAIN achieves substantial reductions in memorization compared to CFG-based baselines while maintaining strong alignment with the intended prompt.",
    "translation": "标题：CAPTAIN：面向文本到图像扩散模型中记忆缓解的语义特征注入方法\n\n摘要：扩散模型可能无意中复现训练数据，随着此类系统的大规模部署，引发了隐私与版权方面的担忧。现有的推理阶段缓解方法通常通过操纵无分类器引导机制或扰动提示嵌入来实现，但这些方法往往难以在降低记忆复现的同时保持与条件提示的良好对齐。本文提出CAPTAIN，一种无需重新训练即可缓解记忆复现的框架，其通过在去噪过程中直接修改潜在特征来实现。CAPTAIN首先应用基于频率的噪声初始化，以降低去噪早期阶段复制记忆模式的倾向；随后识别特征注入的最佳去噪时间步并定位记忆区域；最后，将非记忆参考图像中语义对齐的特征注入定位的潜在区域，在抑制记忆复现的同时保持提示忠实度与视觉质量。实验表明，相较于基于无分类器引导的基线方法，CAPTAIN在维持与目标提示强对齐的前提下，实现了记忆复现现象的显著减少。\n\n摘要：扩散模型可能无意中复现训练数据，随着此类系统的大规模部署，引发了隐私与版权方面的担忧。现有的推理阶段缓解方法通常通过操纵无分类器引导机制或扰动提示嵌入来实现，但这些方法往往难以在降低记忆复现的同时保持与条件提示的良好对齐。本文提出CAPTAIN，一种无需重新训练即可缓解记忆复现的框架，其通过在去噪过程中直接修改潜在特征来实现。CAPTAIN首先应用基于频率的噪声初始化，以降低去噪早期阶段复制记忆模式的倾向；随后识别特征注入的最佳去噪时间步并定位记忆区域；最后，将非记忆参考图像中语义对齐的特征注入定位的潜在区域，在抑制记忆复现的同时保持提示忠实度与视觉质量。实验表明，相较于基于无分类器引导的基线方法，CAPTAIN在维持与目标提示强对齐的前提下，实现了记忆复现现象的显著减少。",
    "url": "https://huggingface.co/papers/2512.10655",
    "arxiv_url": "https://arxiv.org/abs/2512.10655"
  },
  {
    "title": "DiffusionBrowser: Interactive Diffusion Previews via Multi-Branch Decoders",
    "summary": "Video diffusion models have revolutionized generative video synthesis, but they are imprecise, slow, and can be opaque during generation -- keeping users in the dark for a prolonged period. In this work, we propose DiffusionBrowser, a model-agnostic, lightweight decoder framework that allows users to interactively generate previews at any point (timestep or transformer block) during the denoising process. Our model can generate multi-modal preview representations that include RGB and scene intrinsics at more than 4times real-time speed (less than 1 second for a 4-second video) that convey consistent appearance and motion to the final video. With the trained decoder, we show that it is possible to interactively guide the generation at intermediate noise steps via stochasticity reinjection and modal steering, unlocking a new control capability. Moreover, we systematically probe the model using the learned decoders, revealing how scene, object, and other details are composed and assembled during the otherwise black-box denoising process.",
    "translation": "标题：DiffusionBrowser：基于多分支解码器的交互式扩散预览\n\n摘要：视频扩散模型已彻底改变了生成式视频合成技术，但其生成过程存在不精确、速度慢且透明度低的问题——导致用户在生成期间长时间处于未知状态。本研究提出DiffusionBrowser，一个与模型无关的轻量级解码器框架，允许用户在去噪过程中的任意节点（时间步或Transformer模块）交互式生成预览。该模型能以超过实时速度4倍（4秒视频生成时间低于1秒）生成包含RGB与场景本征特征的多模态预览表征，这些预览与最终视频保持外观与运动的一致性。通过训练后的解码器，我们证明了可通过随机性重注入与模态引导在中间噪声步骤中实现交互式生成控制，从而解锁新的调控能力。此外，我们利用学习到的解码器对模型进行系统性探查，揭示了在原本黑箱化的去噪过程中场景、物体及其他细节是如何逐步组合构建的。",
    "url": "https://huggingface.co/papers/2512.13690",
    "arxiv_url": "https://arxiv.org/abs/2512.13690"
  },
  {
    "title": "LitePT: Lighter Yet Stronger Point Transformer",
    "summary": "Modern neural architectures for 3D point cloud processing contain both convolutional layers and attention blocks, but the best way to assemble them remains unclear. We analyse the role of different computational blocks in 3D point cloud networks and find an intuitive behaviour: convolution is adequate to extract low-level geometry at high-resolution in early layers, where attention is expensive without bringing any benefits; attention captures high-level semantics and context in low-resolution, deep layers more efficiently. Guided by this design principle, we propose a new, improved 3D point cloud backbone that employs convolutions in early stages and switches to attention for deeper layers. To avoid the loss of spatial layout information when discarding redundant convolution layers, we introduce a novel, training-free 3D positional encoding, PointROPE. The resulting LitePT model has 3.6times fewer parameters, runs 2times faster, and uses 2times less memory than the state-of-the-art Point Transformer V3, but nonetheless matches or even outperforms it on a range of tasks and datasets. Code and models are available at: https://github.com/prs-eth/LitePT.",
    "translation": "标题：LitePT：更轻量且更强大的点云Transformer\n\n摘要：现代三维点云处理神经网络架构同时包含卷积层和注意力模块，但如何最优组合这些模块仍不明确。本文分析了三维点云网络中不同计算模块的作用，发现一种直观规律：卷积层适合在早期高分辨率阶段提取低层次几何特征，此时注意力机制因计算代价高昂而未带来明显优势；而注意力机制在低分辨率的深层网络中能更高效地捕获高层次语义和上下文信息。基于此设计原则，我们提出一种改进的三维点云骨干网络，在浅层采用卷积运算，在深层切换至注意力机制。为避免丢弃冗余卷积层时损失空间布局信息，我们引入了一种无需训练的三维位置编码方法PointROPE。最终构建的LitePT模型与当前最先进的Point Transformer V3相比，参数量减少3.6倍，运行速度提升2倍，内存消耗降低2倍，同时在一系列任务和数据集上达到相当甚至更优的性能。代码与模型已开源：https://github.com/prs-eth/LitePT。",
    "url": "https://huggingface.co/papers/2512.13689",
    "arxiv_url": "https://arxiv.org/abs/2512.13689"
  },
  {
    "title": "I-Scene: 3D Instance Models are Implicit Generalizable Spatial Learners",
    "summary": "Generalization remains the central challenge for interactive 3D scene generation. Existing learning-based approaches ground spatial understanding in limited scene dataset, restricting generalization to new layouts. We instead reprogram a pre-trained 3D instance generator to act as a scene level learner, replacing dataset-bounded supervision with model-centric spatial supervision. This reprogramming unlocks the generator transferable spatial knowledge, enabling generalization to unseen layouts and novel object compositions. Remarkably, spatial reasoning still emerges even when the training scenes are randomly composed objects. This demonstrates that the generator's transferable scene prior provides a rich learning signal for inferring proximity, support, and symmetry from purely geometric cues. Replacing widely used canonical space, we instantiate this insight with a view-centric formulation of the scene space, yielding a fully feed-forward, generalizable scene generator that learns spatial relations directly from the instance model. Quantitative and qualitative results show that a 3D instance generator is an implicit spatial learner and reasoner, pointing toward foundation models for interactive 3D scene understanding and generation. Project page: https://luling06.github.io/I-Scene-project/",
    "translation": "标题：I-Scene：三维实例模型作为隐式可泛化空间学习器\n\n摘要：泛化能力仍是交互式三维场景生成的核心挑战。现有基于学习的方法将空间理解局限于有限场景数据集，限制了其对新布局的泛化能力。本研究通过重构预训练的三维实例生成器，使其作为场景级学习器，将以数据集为中心的监督替换为以模型为中心的空间监督。这种重构释放了生成器的可迁移空间知识，使其能够泛化至未见布局及新颖物体组合。值得注意的是，即使训练场景由随机组合的物体构成，空间推理能力依然能够涌现。这表明生成器的可迁移场景先验为从纯几何线索推断邻近性、支撑关系和对称性提供了丰富的学习信号。我们摒弃广泛使用的规范空间，采用以视角为中心的场景空间构建方法实例化这一思路，从而构建了一个完全前馈、可泛化的场景生成器，能够直接从实例模型中学习空间关系。定量与定性实验表明，三维实例生成器可作为隐式空间学习与推理器，为交互式三维场景理解与生成的基座模型指明了方向。项目页面：https://luling06.github.io/I-Scene-project/",
    "url": "https://huggingface.co/papers/2512.13683",
    "arxiv_url": "https://arxiv.org/abs/2512.13683"
  },
  {
    "title": "Directional Textual Inversion for Personalized Text-to-Image Generation",
    "summary": "Textual Inversion (TI) is an efficient approach to text-to-image personalization but often fails on complex prompts. We trace these failures to embedding norm inflation: learned tokens drift to out-of-distribution magnitudes, degrading prompt conditioning in pre-norm Transformers. Empirically, we show semantics are primarily encoded by direction in CLIP token space, while inflated norms harm contextualization; theoretically, we analyze how large magnitudes attenuate positional information and hinder residual updates in pre-norm blocks. We propose Directional Textual Inversion (DTI), which fixes the embedding magnitude to an in-distribution scale and optimizes only direction on the unit hypersphere via Riemannian SGD. We cast direction learning as MAP with a von Mises-Fisher prior, yielding a constant-direction prior gradient that is simple and efficient to incorporate. Across personalization tasks, DTI improves text fidelity over TI and TI-variants while maintaining subject similarity. Crucially, DTI's hyperspherical parameterization enables smooth, semantically coherent interpolation between learned concepts (slerp), a capability that is absent in standard TI. Our findings suggest that direction-only optimization is a robust and scalable path for prompt-faithful personalization.",
    "translation": "标题：面向个性化文本到图像生成的方向性文本反演\n\n摘要：文本反演是一种高效的文本到图像个性化方法，但在复杂提示词上常表现不佳。我们发现其失败根源在于嵌入范数膨胀：学习得到的词元偏离至分布外尺度，导致预归一化Transformer中的提示条件作用退化。实证研究表明，CLIP词元空间中的语义信息主要由方向编码，而膨胀的范数会损害上下文关联性；理论分析揭示，过大的幅度会削弱位置信息并阻碍预归一化模块中的残差更新。为此，我们提出方向性文本反演方法，该方法将嵌入幅度固定于分布内尺度，并通过黎曼随机梯度下降在单位超球面上仅优化方向。我们将方向学习建模为具有冯·米塞斯-费希尔先验的最大后验估计，由此产生恒定方向先验梯度，该方法简单高效。在各类个性化任务中，方向性文本反演在保持主体相似性的同时，较传统文本反演及其变体显著提升了文本忠实度。关键的是，该方法的超球面参数化支持学习概念间的平滑语义连贯插值，这是标准文本反演所不具备的能力。我们的研究结果表明，纯方向优化是实现提示忠实个性化的稳健且可扩展路径。",
    "url": "https://huggingface.co/papers/2512.13672",
    "arxiv_url": "https://arxiv.org/abs/2512.13672"
  },
  {
    "title": "FoundationMotion: Auto-Labeling and Reasoning about Spatial Movement in Videos",
    "summary": "Motion understanding is fundamental to physical reasoning, enabling models to infer dynamics and predict future states. However, state-of-the-art models still struggle on recent motion benchmarks, primarily due to the scarcity of large-scale, fine-grained motion datasets. Existing motion datasets are often constructed from costly manual annotation, severely limiting scalability. To address this challenge, we introduce FoundationMotion, a fully automated data curation pipeline that constructs large-scale motion datasets. Our approach first detects and tracks objects in videos to extract their trajectories, then leverages these trajectories and video frames with Large Language Models (LLMs) to generate fine-grained captions and diverse question-answer pairs about motion and spatial reasoning. Using datasets produced by this pipeline, we fine-tune open-source models including NVILA-Video-15B and Qwen2.5-7B, achieving substantial improvements in motion understanding without compromising performance on other tasks. Notably, our models outperform strong closed-source baselines like Gemini-2.5 Flash and large open-source models such as Qwen2.5-VL-72B across diverse motion understanding datasets and benchmarks. FoundationMotion thus provides a scalable solution for curating fine-grained motion datasets that enable effective fine-tuning of diverse models to enhance motion understanding and spatial reasoning capabilities.",
    "translation": "标题：FoundationMotion：视频空间运动的自动标注与推理\n\n摘要：运动理解是物理推理的基础，它使模型能够推断动态并预测未来状态。然而，现有先进模型在近期的运动基准测试中仍面临困难，主要原因是缺乏大规模、细粒度的运动数据集。现有的运动数据集通常依赖成本高昂的人工标注构建，严重限制了其可扩展性。为应对这一挑战，我们提出了FoundationMotion，一种全自动的数据构建流程，用于创建大规模运动数据集。该方法首先在视频中检测并跟踪物体以提取其运动轨迹，随后利用这些轨迹、视频帧以及大型语言模型（LLLMs）生成关于运动与空间推理的细粒度描述文本和多样化问答对。通过使用该流程构建的数据集，我们对包括NVILA-Video-15B和Qwen2.5-7B在内的开源模型进行微调，在保持其他任务性能的同时，实现了运动理解能力的显著提升。值得注意的是，在多种运动理解数据集和基准测试中，我们的模型表现优于Gemini-2.5 Flash等强大的闭源基线模型以及Qwen2.5-VL-72B等大型开源模型。因此，FoundationMotion为构建细粒度运动数据集提供了一个可扩展的解决方案，能够有效微调多样化模型，从而增强其运动理解与空间推理能力。",
    "url": "https://huggingface.co/papers/2512.10927",
    "arxiv_url": "https://arxiv.org/abs/2512.10927"
  },
  {
    "title": "START: Spatial and Textual Learning for Chart Understanding",
    "summary": "Chart understanding is crucial for deploying multimodal large language models (MLLMs) in real-world scenarios such as analyzing scientific papers and technical reports. Unlike natural images, charts pair a structured visual layout (spatial property) with an underlying data representation (textual property) -- grasping both is essential for precise, fine-grained chart reasoning. Motivated by this observation, we propose START, the Spatial and Textual learning for chART understanding. Specifically, we introduce (i) chart-element grounding and (ii) chart-to-code generation to strengthen an MLLM's understanding of both chart visual layout and data details. To facilitate spatial and textual learning, we propose the START-Dataset generated with a novel data-generation pipeline that first leverages an MLLM to translate real chart images into executable chart code, recovering the underlying data representation while preserving the visual distribution of real-world charts. We then evolve the code with a Large Language Model (LLM) to ascertain the positions of chart elements that capture the chart's visual structure, addressing challenges that existing methods cannot handle. To evaluate a model's ability to understand chart spatial structures, we propose the Chart Spatial understanding Benchmark (CS-Bench), filling a critical gap in comprehensive chart understanding evaluation. Leveraging spatial and textual learning, START delivers consistent gains across model sizes and benchmarks over the base models and surpasses prior state-of-the-art by a clear margin. Code, data and models will be publicly available.",
    "translation": "标题：START：面向图表理解的空间与文本学习\n\n摘要：图表理解对于在多模态大语言模型（MLLMs）中部署至现实场景（如分析科学论文与技术报告）至关重要。与自然图像不同，图表同时包含结构化的视觉布局（空间属性）与底层数据表征（文本属性）——准确理解二者是实现精确、细粒度图表推理的关键。基于此观察，我们提出START（面向图表理解的空间与文本学习）。具体而言，我们引入（1）图表元素定位与（2）图表到代码生成两项任务，以增强MLLM对图表视觉布局与数据细节的双重理解。为促进空间与文本学习，我们构建了START数据集，该数据集通过创新的数据生成流程构建：首先利用MLLM将真实图表图像转换为可执行的图表代码，在还原底层数据表征的同时保持真实图表的视觉分布特征；随后通过大语言模型（LLM）对代码进行演化，以确定捕捉图表视觉结构的关键元素位置，从而解决现有方法难以应对的挑战。为评估模型对图表空间结构的理解能力，我们提出了图表空间理解基准（CS-Bench），填补了当前图表综合理解评估的关键空白。依托空间与文本学习机制，START在不同模型规模与基准测试中均较基线模型取得稳定提升，并以显著优势超越现有最优方法。代码、数据与模型将公开发布。",
    "url": "https://huggingface.co/papers/2512.07186",
    "arxiv_url": "https://arxiv.org/abs/2512.07186"
  },
  {
    "title": "Inferring Compositional 4D Scenes without Ever Seeing One",
    "summary": "Scenes in the real world are often composed of several static and dynamic objects. Capturing their 4-dimensional structures, composition and spatio-temporal configuration in-the-wild, though extremely interesting, is equally hard. Therefore, existing works often focus on one object at a time, while relying on some category-specific parametric shape model for dynamic objects. This can lead to inconsistent scene configurations, in addition to being limited to the modeled object categories. We propose COM4D (Compositional 4D), a method that consistently and jointly predicts the structure and spatio-temporal configuration of 4D/3D objects using only static multi-object or dynamic single object supervision. We achieve this by a carefully designed training of spatial and temporal attentions on 2D video input. The training is disentangled into learning from object compositions on the one hand, and single object dynamics throughout the video on the other, thus completely avoiding reliance on 4D compositional training data. At inference time, our proposed attention mixing mechanism combines these independently learned attentions, without requiring any 4D composition examples. By alternating between spatial and temporal reasoning, COM4D reconstructs complete and persistent 4D scenes with multiple interacting objects directly from monocular videos. Furthermore, COM4D provides state-of-the-art results in existing separate problems of 4D object and composed 3D reconstruction despite being purely data-driven.",
    "translation": "标题：无需观测即可推断组合式四维场景\n\n摘要：现实世界中的场景通常由多个静态与动态物体组合而成。捕捉这些物体在自然状态下的四维结构、组合方式及时空配置虽然极具研究价值，却也异常困难。现有研究往往依赖特定类别的参数化动态物体模型，每次仅聚焦单个物体进行分析。这种方法不仅受限于已建模的物体类别，还可能导致场景配置不一致。我们提出COM4D（组合式四维场景重建）方法，该方法仅需静态多物体或动态单物体的监督数据，即可一致性地联合预测四维/三维物体的结构及时空配置。我们通过对二维视频输入的空间与时间注意力机制进行精心设计的训练实现这一目标：训练过程被解耦为物体组合学习与视频中单物体动态学习两个独立阶段，从而完全避免对四维组合训练数据的依赖。在推理阶段，我们提出的注意力混合机制能够融合这些独立学习的注意力权重，且无需任何四维组合示例。通过交替进行空间推理与时间推理，COM4D可直接从单目视频中重建包含多个交互物体的完整且持续的四维场景。此外，尽管采用纯数据驱动方法，COM4D在现有四维物体重建与组合式三维重建这两个独立问题上均取得了最先进的研究成果。",
    "url": "https://huggingface.co/papers/2512.05272",
    "arxiv_url": "https://arxiv.org/abs/2512.05272"
  },
  {
    "title": "FIN-bench-v2: A Unified and Robust Benchmark Suite for Evaluating Finnish Large Language Models",
    "summary": "We introduce FIN-bench-v2, a unified benchmark suite for evaluating large language models in Finnish. FIN-bench-v2 consolidates Finnish versions of widely used benchmarks together with an updated and expanded version of the original FIN-bench into a single, consistently formatted collection, covering multiple-choice and generative tasks across reading comprehension, commonsense reasoning, sentiment analysis, world knowledge, and alignment. All datasets are converted to HuggingFace Datasets, which include both cloze and multiple-choice prompt formulations with five variants per task, and we incorporate human annotation or review for machine-translated resources such as GoldenSwag and XED. To select robust tasks, we pretrain a set of 2.15B-parameter decoder-only models and use their learning curves to compute monotonicity, signal-to-noise, non-random performance, and model ordering consistency, retaining only tasks that satisfy all criteria. We further evaluate a set of larger instruction-tuned models to characterize performance across tasks and prompt formulations. All datasets, prompts, and evaluation configurations are publicly available via our fork of the Language Model Evaluation Harness at https://github.com/LumiOpen/lm-evaluation-harness. Supplementary resources are released in a separate repository at https://github.com/TurkuNLP/FIN-bench-v2.",
    "translation": "标题：FIN-bench-v2：用于评估芬兰语大语言模型的统一鲁棒基准套件\n\n摘要：本文介绍FIN-bench-v2——一个用于评估芬兰语大语言模型的统一基准套件。该套件将广泛使用的基准测试芬兰语版本与原始FIN-bench的更新扩展版整合为格式统一的数据集合，涵盖阅读理解、常识推理、情感分析、世界知识和对齐任务中的选择题与生成式任务。所有数据集均转换为HuggingFace Datasets格式，包含完形填空和选择题两种提示模板（每项任务设五种变体），并对GoldenSwag、XED等机器翻译资源进行了人工标注或审核。为筛选鲁棒性任务，我们预训练了一组21.5亿参数的仅解码器模型，通过其学习曲线计算单调性、信噪比、非随机性能及模型排序一致性指标，仅保留满足全部标准的任务。进一步通过指令微调的大规模模型评估，系统刻画了不同任务与提示模板下的性能表现。所有数据集、提示模板及评估配置已通过我们分叉的Language Model Evaluation Harness开源（https://github.com/LumiOpen/lm-evaluation-harness），补充资源发布于独立存储库（https://github.com/TurkuNLP/FIN-bench-v2）。",
    "url": "https://huggingface.co/papers/2512.13330",
    "arxiv_url": "https://arxiv.org/abs/2512.13330"
  },
  {
    "title": "State over Tokens: Characterizing the Role of Reasoning Tokens",
    "summary": "Large Language Models (LLMs) can generate reasoning tokens before their final answer to boost performance on complex tasks. While these sequences seem like human thought processes, empirical evidence reveals that they are not a faithful explanation of the model's actual reasoning process. To address this gap between appearance and function, we introduce the State over Tokens (SoT) conceptual framework. SoT reframes reasoning tokens not as a linguistic narrative, but as an externalized computational state -- the sole persistent information carrier across the model's stateless generation cycles. This explains how the tokens can drive correct reasoning without being a faithful explanation when read as text and surfaces previously overlooked research questions on these tokens. We argue that to truly understand the process that LLMs do, research must move beyond reading the reasoning tokens as text and focus on decoding them as state.",
    "translation": "标题：状态于词元之上：推理词元的作用特征分析\n\n摘要：大型语言模型（LLM）能够在生成最终答案前输出推理词元，以提升复杂任务的表现。尽管这些序列看似与人类思维过程相似，但实证研究表明它们并非对模型实际推理过程的真实解释。为弥合表象与功能之间的鸿沟，本文提出“状态于词元之上”（SoT）概念框架。SoT将推理词元重新定义为一种外化的计算状态——而非语言叙述，它是模型在无状态生成周期中唯一持续存在的信息载体。这一框架解释了为何这些词元在驱动正确推理的同时，若被视作文本解读则无法成为真实解释，并揭示了此前被忽视的关于此类词元的研究问题。我们认为，要真正理解大型语言模型的运行机制，研究必须超越将推理词元作为文本解读的范式，转而聚焦于将其作为状态进行解码。",
    "url": "https://huggingface.co/papers/2512.12777",
    "arxiv_url": "https://arxiv.org/abs/2512.12777"
  },
  {
    "title": "CoRe3D: Collaborative Reasoning as a Foundation for 3D Intelligence",
    "summary": "Recent advances in large multimodal models suggest that explicit reasoning mechanisms play a critical role in improving model reliability, interpretability, and cross-modal alignment. While such reasoning-centric approaches have been proven effective in language and vision tasks, their extension to 3D remains underdeveloped. CoRe3D introduces a unified 3D understanding and generation reasoning framework that jointly operates over semantic and spatial abstractions, enabling high-level intent inferred from language to directly guide low-level 3D content formation. Central to this design is a spatially grounded reasoning representation that decomposes 3D latent space into localized regions, allowing the model to reason over geometry in a compositional and procedural manner. By tightly coupling semantic chain-of-thought inference with structured spatial reasoning, CoRe3D produces 3D outputs that exhibit strong local consistency and faithful alignment with linguistic descriptions.",
    "translation": "标题：CoRe3D：作为三维智能基础的协同推理框架\n\n摘要：近期大规模多模态模型的研究进展表明，显式推理机制对于提升模型可靠性、可解释性以及跨模态对齐能力具有关键作用。尽管此类以推理为核心的方法已在语言与视觉任务中被证明有效，但其向三维领域的拓展仍处于初级阶段。CoRe3D提出了一种统一的三维理解与生成推理框架，该框架在语义与空间抽象层面进行协同运算，使得从语言中推断出的高层意图能够直接指导底层三维内容的生成。该设计的核心在于一种基于空间锚定的推理表征，它将三维潜在空间分解为局部化区域，使模型能够以组合化、程序化的方式对几何结构进行推理。通过将语义思维链推理与结构化空间推理紧密耦合，CoRe3D生成的三维输出展现出显著的局部一致性，并与语言描述保持高度忠实对齐。",
    "url": "https://huggingface.co/papers/2512.12768",
    "arxiv_url": "https://arxiv.org/abs/2512.12768"
  },
  {
    "title": "Rethinking Expert Trajectory Utilization in LLM Post-training",
    "summary": "While effective post-training integrates Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL), the optimal mechanism for utilizing expert trajectories remains unresolved. We propose the Plasticity-Ceiling Framework to theoretically ground this landscape, decomposing performance into foundational SFT performance and the subsequent RL plasticity. Through extensive benchmarking, we establish the Sequential SFT-then-RL pipeline as the superior standard, overcoming the stability deficits of synchronized approaches. Furthermore, we derive precise scaling guidelines: (1) Transitioning to RL at the SFT Stable or Mild Overfitting Sub-phase maximizes the final ceiling by securing foundational SFT performance without compromising RL plasticity; (2) Refuting ``Less is More'' in the context of SFT-then-RL scaling, we demonstrate that Data Scale determines the primary post-training potential, while Trajectory Difficulty acts as a performance multiplier; and (3) Identifying that the Minimum SFT Validation Loss serves as a robust indicator for selecting the expert trajectories that maximize the final performance ceiling. Our findings provide actionable guidelines for maximizing the value extracted from expert trajectories.",
    "translation": "标题：重新思考专家轨迹在大语言模型后训练中的利用方式\n\n摘要：尽管有效的后训练整合了监督微调（SFT）与强化学习（RL），但利用专家轨迹的最优机制仍未明确。我们提出“可塑性-上限”框架为此领域提供理论依据，将性能分解为基础SFT性能与后续RL可塑性。通过广泛的基准测试，我们确立了“先SFT后RL”的序列化流程为更优标准，克服了同步方法的稳定性缺陷。此外，我们推导出精确的扩展准则：（1）在SFT稳定期或轻度过拟合子阶段转向RL，可通过确保基础SFT性能且不损害RL可塑性，最大化最终性能上限；（2）在“先SFT后RL”的扩展背景下驳斥“少即是多”的观点，我们证明数据规模决定后训练的主要潜力，而轨迹难度则作为性能倍增器；（3）发现最小SFT验证损失可作为筛选专家轨迹的稳健指标，以最大化最终性能上限。本研究为从专家轨迹中提取最大价值提供了可操作的指导原则。",
    "url": "https://huggingface.co/papers/2512.11470",
    "arxiv_url": "https://arxiv.org/abs/2512.11470"
  },
  {
    "title": "Learning Robot Manipulation from Audio World Models",
    "summary": "World models have demonstrated impressive performance on robotic learning tasks. Many such tasks inherently demand multimodal reasoning; for example, filling a bottle with water will lead to visual information alone being ambiguous or incomplete, thereby requiring reasoning over the temporal evolution of audio, accounting for its underlying physical properties and pitch patterns. In this paper, we propose a generative latent flow matching model to anticipate future audio observations, enabling the system to reason about long-term consequences when integrated into a robot policy. We demonstrate the superior capabilities of our system through two manipulation tasks that require perceiving in-the-wild audio or music signals, compared to methods without future lookahead. We further emphasize that successful robot action learning for these tasks relies not merely on multi-modal input, but critically on the accurate prediction of future audio states that embody intrinsic rhythmic patterns.",
    "translation": "标题：基于音频世界模型的机器人操作学习\n\n摘要：世界模型在机器人学习任务中展现出卓越性能。此类任务往往天然需要多模态推理能力；例如，仅凭视觉信息判断水瓶注水过程可能存在模糊性或信息缺失，因此需要结合音频信号的时序演变进行推理，并考量其底层物理特性与音高模式。本文提出一种生成式潜在流匹配模型，用于预测未来音频观测数据，使系统在融入机器人策略时能够推理长期行为后果。通过两项需要感知真实环境音频或音乐信号的操作任务，我们证明了本系统相较于无前瞻预测的方法具有更优越的性能。我们进一步强调，成功实现此类任务的机器人动作学习不仅依赖于多模态输入，更关键的是对未来音频状态的精准预测——这些状态本质上承载着内在的节奏模式。",
    "url": "https://huggingface.co/papers/2512.08405",
    "arxiv_url": "https://arxiv.org/abs/2512.08405"
  },
  {
    "title": "Towards Visual Re-Identification of Fish using Fine-Grained Classification for Electronic Monitoring in Fisheries",
    "summary": "Accurate fisheries data are crucial for effective and sustainable marine resource management. With the recent adoption of Electronic Monitoring (EM) systems, more video data is now being collected than can be feasibly reviewed manually. This paper addresses this challenge by developing an optimized deep learning pipeline for automated fish re-identification (Re-ID) using the novel AutoFish dataset, which simulates EM systems with conveyor belts with six similarly looking fish species. We demonstrate that key Re-ID metrics (R1 and mAP@k) are substantially improved by using hard triplet mining in conjunction with a custom image transformation pipeline that includes dataset-specific normalization. By employing these strategies, we demonstrate that the Vision Transformer-based Swin-T architecture consistently outperforms the Convolutional Neural Network-based ResNet-50, achieving peak performance of 41.65% mAP@k and 90.43% Rank-1 accuracy. An in-depth analysis reveals that the primary challenge is distinguishing visually similar individuals of the same species (Intra-species errors), where viewpoint inconsistency proves significantly more detrimental than partial occlusion. The source code and documentation are available at: https://github.com/msamdk/Fish_Re_Identification.git",
    "translation": "标题：基于细粒度分类的渔业电子监控鱼类视觉重识别研究\n\n摘要：准确的渔业数据对实现有效且可持续的海洋资源管理至关重要。随着电子监控系统近年来的推广应用，当前采集的视频数据量已远超人工审阅的可行范围。本文通过构建基于新型AutoFish数据集（模拟配备传送带的电子监控系统，包含六种形态相似的鱼类）的优化深度学习流程，以应对鱼类自动重识别任务中的挑战。研究表明，结合针对数据集特点设计的归一化图像预处理流程，并采用困难三元组挖掘策略，可显著提升重识别关键指标（R1与mAP@k）。应用该策略后，基于视觉Transformer的Swin-T架构持续优于基于卷积神经网络的ResNet-50模型，最高达到41.65%的mAP@k与90.43%的Rank-1准确率。深入分析表明，主要挑战在于区分同物种中视觉特征相似的个体（种内误差），其中视角不一致问题的影响显著大于局部遮挡。源代码及技术文档详见：https://github.com/msamdk/Fish_Re_Identification.git",
    "url": "https://huggingface.co/papers/2512.08400",
    "arxiv_url": "https://arxiv.org/abs/2512.08400"
  },
  {
    "title": "KD-OCT: Efficient Knowledge Distillation for Clinical-Grade Retinal OCT Classification",
    "summary": "Age-related macular degeneration (AMD) and choroidal neovascularization (CNV)-related conditions are leading causes of vision loss worldwide, with optical coherence tomography (OCT) serving as a cornerstone for early detection and management. However, deploying state-of-the-art deep learning models like ConvNeXtV2-Large in clinical settings is hindered by their computational demands. Therefore, it is desirable to develop efficient models that maintain high diagnostic performance while enabling real-time deployment. In this study, a novel knowledge distillation framework, termed KD-OCT, is proposed to compress a high-performance ConvNeXtV2-Large teacher model, enhanced with advanced augmentations, stochastic weight averaging, and focal loss, into a lightweight EfficientNet-B2 student for classifying normal, drusen, and CNV cases. KD-OCT employs real-time distillation with a combined loss balancing soft teacher knowledge transfer and hard ground-truth supervision. The effectiveness of the proposed method is evaluated on the Noor Eye Hospital (NEH) dataset using patient-level cross-validation. Experimental results demonstrate that KD-OCT outperforms comparable multi-scale or feature-fusion OCT classifiers in efficiency- accuracy balance, achieving near-teacher performance with substantial reductions in model size and inference time. Despite the compression, the student model exceeds most existing frameworks, facilitating edge deployment for AMD screening. Code is available at https://github.com/erfan-nourbakhsh/KD- OCT.",
    "translation": "标题：KD-OCT：面向临床级视网膜OCT分类的高效知识蒸馏方法\n\n摘要：年龄相关性黄斑变性（AMD）和脉络膜新生血管（CNV）相关疾病是全球范围内导致视力丧失的主要原因，而光学相干断层扫描（OCT）是其早期检测与管理的基石。然而，在临床环境中部署如ConvNeXtV2-Large等先进深度学习模型受到其高计算需求的限制。因此，开发能够在保持高诊断性能的同时实现实时部署的高效模型具有重要意义。本研究提出了一种新颖的知识蒸馏框架KD-OCT，旨在将经过高级数据增强、随机权重平均和焦点损失优化的高性能ConvNeXtV2-Large教师模型，压缩为轻量级的EfficientNet-B2学生模型，用于分类正常、玻璃膜疣及CNV病例。KD-OCT采用实时蒸馏策略，通过结合软教师知识迁移与硬真实标签监督的混合损失函数实现平衡优化。该方法的有效性在Noor眼科医院（NEH）数据集上采用患者级交叉验证进行评估。实验结果表明，KD-OCT在效率与准确性的平衡上优于同类多尺度或特征融合OCT分类器，在模型大小和推理时间大幅降低的同时实现了接近教师模型的性能。尽管经过压缩，学生模型仍超越了大多数现有框架，为AMD筛查的边缘部署提供了便利。代码公开于：https://github.com/erfan-nourbakhsh/KD-OCT。",
    "url": "https://huggingface.co/papers/2512.09069",
    "arxiv_url": "https://arxiv.org/abs/2512.09069"
  }
]