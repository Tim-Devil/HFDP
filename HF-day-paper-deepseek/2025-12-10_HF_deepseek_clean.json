[
  {
    "title": "Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance",
    "summary": "We present Wan-Move, a simple and scalable framework that brings motion control to video generative models. Existing motion-controllable methods typically suffer from coarse control granularity and limited scalability, leaving their outputs insufficient for practical use. We narrow this gap by achieving precise and high-quality motion control. Our core idea is to directly make the original condition features motion-aware for guiding video synthesis. To this end, we first represent object motions with dense point trajectories, allowing fine-grained control over the scene. We then project these trajectories into latent space and propagate the first frame's features along each trajectory, producing an aligned spatiotemporal feature map that tells how each scene element should move. This feature map serves as the updated latent condition, which is naturally integrated into the off-the-shelf image-to-video model, e.g., Wan-I2V-14B, as motion guidance without any architecture change. It removes the need for auxiliary motion encoders and makes fine-tuning base models easily scalable. Through scaled training, Wan-Move generates 5-second, 480p videos whose motion controllability rivals Kling 1.5 Pro's commercial Motion Brush, as indicated by user studies. To support comprehensive evaluation, we further design MoveBench, a rigorously curated benchmark featuring diverse content categories and hybrid-verified annotations. It is distinguished by larger data volume, longer video durations, and high-quality motion annotations. Extensive experiments on MoveBench and the public dataset consistently show Wan-Move's superior motion quality. Code, models, and benchmark data are made publicly available.",
    "translation": "标题：Wan-Move：基于潜在轨迹引导的运动可控视频生成\n\n摘要：本文提出Wan-Move，一个简洁且可扩展的框架，旨在为视频生成模型实现运动控制。现有运动可控方法通常存在控制粒度粗糙和可扩展性有限的问题，导致其输出难以满足实际应用需求。我们通过实现精确且高质量的运动控制来缩小这一差距。核心思想是直接使原始条件特征具备运动感知能力，从而指导视频合成。为此，我们首先通过密集点轨迹表示物体运动，实现对场景的细粒度控制。随后将这些轨迹映射到潜在空间，并沿每条轨迹传播首帧特征，生成对齐的时空特征图以指示每个场景元素的运动方式。该特征图作为更新后的潜在条件，无需修改架构即可无缝集成至现成的图像到视频模型（如Wan-I2V-14B）中作为运动引导。该方法无需辅助运动编码器，并使基础模型的微调易于扩展。通过规模化训练，Wan-Move可生成5秒时长的480p视频，用户研究表明其运动控制能力与Kling 1.5 Pro的商业化\"运动笔刷\"功能相当。为支持全面评估，我们进一步设计了MoveBench基准测试集，该数据集经过严格筛选，涵盖多样化的内容类别并采用混合验证标注，具有数据量更大、视频时长更长、运动标注质量更高的特点。在MoveBench和公开数据集上的大量实验一致表明，Wan-Move在运动质量方面具有显著优势。代码、模型及基准数据均已公开。",
    "url": "https://huggingface.co/papers/2512.08765",
    "arxiv_url": "https://arxiv.org/abs/2512.08765"
  },
  {
    "title": "Visionary: The World Model Carrier Built on WebGPU-Powered Gaussian Splatting Platform",
    "summary": "Neural rendering, particularly 3D Gaussian Splatting (3DGS), has evolved rapidly and become a key component for building world models. However, existing viewer solutions remain fragmented, heavy, or constrained by legacy pipelines, resulting in high deployment friction and limited support for dynamic content and generative models. In this work, we present Visionary, an open, web-native platform for real-time various Gaussian Splatting and meshes rendering. Built on an efficient WebGPU renderer with per-frame ONNX inference, Visionary enables dynamic neural processing while maintaining a lightweight, \"click-to-run\" browser experience. It introduces a standardized Gaussian Generator contract, which not only supports standard 3DGS rendering but also allows plug-and-play algorithms to generate or update Gaussians each frame. Such inference also enables us to apply feedforward generative post-processing. The platform further offers a plug in three.js library with a concise TypeScript API for seamless integration into existing web applications. Experiments show that, under identical 3DGS assets, Visionary achieves superior rendering efficiency compared to current Web viewers due to GPU-based primitive sorting. It already supports multiple variants, including MLP-based 3DGS, 4DGS, neural avatars, and style transformation or enhancement networks. By unifying inference and rendering directly in the browser, Visionary significantly lowers the barrier to reproduction, comparison, and deployment of 3DGS-family methods, serving as a unified World Model Carrier for both reconstructive and generative paradigms.",
    "translation": "标题：Visionary：基于WebGPU驱动高斯溅射平台的世界模型载体\n\n摘要：神经渲染技术，特别是三维高斯溅射（3DGS），近年来快速发展并成为构建世界模型的关键组件。然而，现有的可视化方案仍存在碎片化、架构臃肿或受传统流程限制等问题，导致部署门槛高且对动态内容与生成模型的支持有限。本研究提出Visionary——一个开放的、面向Web原生的实时高斯溅射与网格渲染平台。该平台基于高效的WebGPU渲染器构建，支持逐帧ONNX推理，在保持轻量化“即点即用”浏览器体验的同时实现了动态神经处理。我们设计了标准化的高斯生成器协议，该协议不仅支持标准3DGS渲染，还允许通过即插即用算法逐帧生成或更新高斯分布。这种推理机制进一步支持前馈生成式后处理流程。平台同时提供了兼容three.js的插件库，其简洁的TypeScript接口可实现与现有Web应用的无缝集成。实验表明，在相同3DGS资源条件下，Visionary凭借基于GPU的图元排序机制，其渲染效率优于现有Web可视化方案。目前平台已支持多种衍生技术，包括基于MLP的3DGS、4DGS、神经数字人以及风格转换与增强网络。通过将推理与渲染统一集成至浏览器环境，Visionary显著降低了3DGS系列方法的复现、比较与部署门槛，成为重建与生成双范式下的统一世界模型载体。",
    "url": "https://huggingface.co/papers/2512.08478",
    "arxiv_url": "https://arxiv.org/abs/2512.08478"
  },
  {
    "title": "Preserving Source Video Realism: High-Fidelity Face Swapping for Cinematic Quality",
    "summary": "Video face swapping is crucial in film and entertainment production, where achieving high fidelity and temporal consistency over long and complex video sequences remains a significant challenge. Inspired by recent advances in reference-guided image editing, we explore whether rich visual attributes from source videos can be similarly leveraged to enhance both fidelity and temporal coherence in video face swapping. Building on this insight, this work presents LivingSwap, the first video reference guided face swapping model. Our approach employs keyframes as conditioning signals to inject the target identity, enabling flexible and controllable editing. By combining keyframe conditioning with video reference guidance, the model performs temporal stitching to ensure stable identity preservation and high-fidelity reconstruction across long video sequences. To address the scarcity of data for reference-guided training, we construct a paired face-swapping dataset, Face2Face, and further reverse the data pairs to ensure reliable ground-truth supervision. Extensive experiments demonstrate that our method achieves state-of-the-art results, seamlessly integrating the target identity with the source video's expressions, lighting, and motion, while significantly reducing manual effort in production workflows. Project webpage: https://aim-uofa.github.io/LivingSwap",
    "translation": "标题：保持源视频真实感：面向电影级画质的高保真人脸替换\n\n摘要：视频人脸替换技术在影视娱乐制作中至关重要，然而在长时复杂视频序列中实现高保真度与时间一致性仍面临重大挑战。受近期参考引导图像编辑进展的启发，本研究探索是否可类似地利用源视频丰富的视觉属性来增强视频人脸替换的保真度与时间连贯性。基于此洞见，本文提出首个视频参考引导的人脸替换模型LivingSwap。该方法以关键帧作为条件信号注入目标身份特征，实现灵活可控的编辑。通过将关键帧条件化与视频参考引导相结合，模型执行时序拼接以确保长视频序列中稳定的身份保持与高保真重建。针对参考引导训练数据稀缺的问题，我们构建了配对人脸替换数据集Face2Face，并通过数据对反转机制确保可靠的监督信号。大量实验表明，本方法在无缝融合目标身份与源视频的表情、光照及运动特征方面达到最优性能，同时显著减少制作流程中的人工干预。项目主页：https://aim-uofa.github.io/LivingSwap",
    "url": "https://huggingface.co/papers/2512.07951",
    "arxiv_url": "https://arxiv.org/abs/2512.07951"
  },
  {
    "title": "OneStory: Coherent Multi-Shot Video Generation with Adaptive Memory",
    "summary": "Storytelling in real-world videos often unfolds through multiple shots -- discontinuous yet semantically connected clips that together convey a coherent narrative. However, existing multi-shot video generation (MSV) methods struggle to effectively model long-range cross-shot context, as they rely on limited temporal windows or single keyframe conditioning, leading to degraded performance under complex narratives. In this work, we propose OneStory, enabling global yet compact cross-shot context modeling for consistent and scalable narrative generation. OneStory reformulates MSV as a next-shot generation task, enabling autoregressive shot synthesis while leveraging pretrained image-to-video (I2V) models for strong visual conditioning. We introduce two key modules: a Frame Selection module that constructs a semantically-relevant global memory based on informative frames from prior shots, and an Adaptive Conditioner that performs importance-guided patchification to generate compact context for direct conditioning. We further curate a high-quality multi-shot dataset with referential captions to mirror real-world storytelling patterns, and design effective training strategies under the next-shot paradigm. Finetuned from a pretrained I2V model on our curated 60K dataset, OneStory achieves state-of-the-art narrative coherence across diverse and complex scenes in both text- and image-conditioned settings, enabling controllable and immersive long-form video storytelling.",
    "translation": "标题：OneStory：基于自适应记忆的连贯多镜头视频生成\n\n摘要：现实世界视频中的叙事通常通过多个镜头展开——这些镜头在时间上不连续但语义相互关联，共同构成连贯的叙事流。然而，现有多镜头视频生成方法因依赖有限时间窗口或单关键帧条件约束，难以有效建模长程跨镜头上下文关系，导致复杂叙事场景下性能下降。本研究提出OneStory框架，通过全局且紧凑的跨镜头上下文建模实现连贯可扩展的叙事生成。该方法将多镜头视频生成重构为\"下一镜头生成\"任务，在利用预训练图像到视频模型实现强视觉条件约束的同时，支持自回归式镜头合成。我们引入两个核心模块：基于历史镜头信息帧构建语义相关全局记忆的帧选择模块，以及通过重要性引导分块生成紧凑上下文条件的自适应调节器。为进一步模拟现实叙事模式，我们构建了包含指代性描述的高质量多镜头数据集，并在下一镜头范式下设计了高效训练策略。通过在自建的6万规模数据集上对预训练图像到视频模型进行微调，OneStory在文本条件与图像条件设置下，针对多样化复杂场景均实现了当前最优的叙事连贯性，为可控沉浸式长视频叙事提供了有效解决方案。",
    "url": "https://huggingface.co/papers/2512.07802",
    "arxiv_url": "https://arxiv.org/abs/2512.07802"
  },
  {
    "title": "ThreadWeaver: Adaptive Threading for Efficient Parallel Reasoning in Language Models",
    "summary": "Scaling inference-time computation has enabled Large Language Models (LLMs) to achieve strong reasoning performance, but inherently sequential decoding leads to substantial latency, especially on complex tasks. Recent work on adaptive parallel reasoning aims to improve inference efficiency by decomposing the problem-solving process into concurrent reasoning threads when beneficial. However, existing methods on realistic tasks are either limited to supervised behavior cloning or exhibit significant accuracy drops compared to widely-used sequential long chain-of-thought (CoT) baselines. Moreover, many require customized inference engines, complicating deployment. We introduce ThreadWeaver, a framework for adaptive parallel reasoning that achieves accuracy on par with popular sequential reasoning models of comparable size while significantly reducing inference latency. ThreadWeaver's performance stems from three key innovations: 1) a two-stage parallel trajectory generator that produces large-scale, high-quality CoT data with parallel annotations for supervised fine-tuning; 2) a trie-based training-inference co-design that enables parallel reasoning on any off-the-shelf autoregressive inference engine without modifying position embeddings or KV caches; and 3) a parallelization-aware reinforcement learning framework that teaches the model to balance accuracy with effective parallelization. Across six challenging mathematical reasoning benchmarks, ThreadWeaver trained atop Qwen3-8B achieves accuracy comparable to cutting-edge sequential reasoning models (71.9% on average and 79.9% on AIME24) while delivering up to 1.53x average speedup in token latency, establishing a new Pareto frontier between accuracy and efficiency.",
    "translation": "标题：ThreadWeaver：面向语言模型高效并行推理的自适应线程技术\n\n摘要：扩展推理时计算能力使大语言模型（LLMs）获得了强大的推理性能，但其固有的顺序解码机制会导致显著延迟，尤其在复杂任务中。近期自适应并行推理研究旨在通过将解题过程分解为并发推理线程来提升推理效率，但现有方法在现实任务中要么局限于监督行为克隆，要么相比广泛使用的顺序长思维链基线出现显著准确率下降。此外，许多方法需要定制化推理引擎，增加了部署复杂度。本文提出ThreadWeaver框架，该自适应并行推理框架在保持与同规模主流顺序推理模型相当准确率的同时，显著降低推理延迟。其性能源于三项关键创新：1）两阶段并行轨迹生成器，可产生带并行标注的大规模高质量思维链数据用于监督微调；2）基于字典树的训练-推理协同设计，无需修改位置编码或KV缓存即可在任何现成自回归推理引擎上实现并行推理；3）并行感知强化学习框架，指导模型在准确率与有效并行化间取得平衡。在六项具有挑战性的数学推理基准测试中，基于Qwen3-8B构建的ThreadWeaver达到与前沿顺序推理模型相当的准确率（平均71.9%，AIME24数据集79.9%），同时实现最高达1.53倍的平均词元延迟加速，在准确率与效率间建立了新的帕累托前沿。",
    "url": "https://huggingface.co/papers/2512.07843",
    "arxiv_url": "https://arxiv.org/abs/2512.07843"
  },
  {
    "title": "Boosting Unsupervised Video Instance Segmentation with Automatic Quality-Guided Self-Training",
    "summary": "Video Instance Segmentation (VIS) faces significant annotation challenges due to its dual requirements of pixel-level masks and temporal consistency labels. While recent unsupervised methods like VideoCutLER eliminate optical flow dependencies through synthetic data, they remain constrained by the synthetic-to-real domain gap. We present AutoQ-VIS, a novel unsupervised framework that bridges this gap through quality-guided self-training. Our approach establishes a closed-loop system between pseudo-label generation and automatic quality assessment, enabling progressive adaptation from synthetic to real videos. Experiments demonstrate state-of-the-art performance with 52.6 AP_{50} on YouTubeVIS-2019 val set, surpassing the previous state-of-the-art VideoCutLER by 4.4%, while requiring no human annotations. This demonstrates the viability of quality-aware self-training for unsupervised VIS. We will release the code at https://github.com/wcbup/AutoQ-VIS.",
    "translation": "标题：基于自动质量引导自训练的无监督视频实例分割性能提升\n\n摘要：视频实例分割任务因同时需要像素级掩码标注与时序一致性标签而面临显著的标注挑战。现有无监督方法（如VideoCutLER）虽通过合成数据消除了光流依赖，但仍受限于合成数据与真实场景间的领域差异。本文提出AutoQ-VIS——一种通过质量引导自训练弥合领域差异的新型无监督框架。该方法在伪标签生成与自动质量评估之间构建闭环系统，实现从合成视频到真实视频的渐进式自适应。实验表明，本方法在YouTubeVIS-2019验证集上达到52.6 AP_{50}的先进性能，较此前最优方法VideoCutLER提升4.4%，且无需任何人工标注。这证明了质量感知自训练在无监督视频实例分割中的可行性。代码将在https://github.com/wcbup/AutoQ-VIS开源。",
    "url": "https://huggingface.co/papers/2512.06864",
    "arxiv_url": "https://arxiv.org/abs/2512.06864"
  },
  {
    "title": "Arbitrage: Efficient Reasoning via Advantage-Aware Speculation",
    "summary": "Modern Large Language Models achieve impressive reasoning capabilities with long Chain of Thoughts, but they incur substantial computational cost during inference, and this motivates techniques to improve the performance-cost ratio. Among these techniques, Speculative Decoding accelerates inference by employing a fast but inaccurate draft model to autoregressively propose tokens, which are then verified in parallel by a more capable target model. However, due to unnecessary rejections caused by token mismatches in semantically equivalent steps, traditional token-level Speculative Decoding struggles in reasoning tasks. Although recent works have shifted to step-level semantic verification, which improve efficiency by accepting or rejecting entire reasoning steps, existing step-level methods still regenerate many rejected steps with little improvement, wasting valuable target compute. To address this challenge, we propose Arbitrage, a novel step-level speculative generation framework that routes generation dynamically based on the relative advantage between draft and target models. Instead of applying a fixed acceptance threshold, Arbitrage uses a lightweight router trained to predict when the target model is likely to produce a meaningfully better step. This routing approximates an ideal Arbitrage Oracle that always chooses the higher-quality step, achieving near-optimal efficiency-accuracy trade-offs. Across multiple mathematical reasoning benchmarks, Arbitrage consistently surpasses prior step-level Speculative Decoding baselines, reducing inference latency by up to sim2times at matched accuracy.",
    "translation": "标题：套利推理：基于优势感知推测的高效推理方法\n\n摘要：现代大语言模型通过长链思维展现出卓越的推理能力，但其推理过程需消耗大量计算资源，这促使研究者探索提升性能成本比的技术。在各类加速技术中，推测解码通过采用快速但不精确的草稿模型自回归地生成候选标记，再由更强目标模型并行验证，以此提升推理效率。然而，传统基于标记级别的推测解码在语义等价步骤中常因标记不匹配产生不必要的拒绝，导致其在推理任务中表现受限。尽管近期研究转向基于步骤级别的语义验证——通过接受或拒绝完整推理步骤提升效率，但现有方法仍会因大量被拒步骤的重复生成而改进有限，造成目标模型计算资源的浪费。为应对这一挑战，本文提出“套利推理”——一种新型步骤级推测生成框架，该框架根据草稿模型与目标模型的相对优势动态路由生成过程。与采用固定接受阈值的方法不同，套利推理通过轻量级路由模块预测目标模型何时可能生成显著更优的步骤，其路由机制近似于始终选择更高质量步骤的理想套利预言机，实现了接近最优的效率-精度平衡。在多个数学推理基准测试中，套利推理均优于现有步骤级推测解码基线方法，在保持同等精度条件下将推理延迟降低最高约2倍。",
    "url": "https://huggingface.co/papers/2512.05033",
    "arxiv_url": "https://arxiv.org/abs/2512.05033"
  },
  {
    "title": "MIND-V: Hierarchical Video Generation for Long-Horizon Robotic Manipulation with RL-based Physical Alignment",
    "summary": "Embodied imitation learning is constrained by the scarcity of diverse, long-horizon robotic manipulation data. Existing video generation models for this domain are limited to synthesizing short clips of simple actions and often rely on manually defined trajectories. To this end, we introduce MIND-V, a hierarchical framework designed to synthesize physically plausible and logically coherent videos of long-horizon robotic manipulation. Inspired by cognitive science, MIND-V bridges high-level reasoning with pixel-level synthesis through three core components: a Semantic Reasoning Hub (SRH) that leverages a pre-trained vision-language model for task planning; a Behavioral Semantic Bridge (BSB) that translates abstract instructions into domain-invariant representations; and a Motor Video Generator (MVG) for conditional video rendering. MIND-V employs Staged Visual Future Rollouts, a test-time optimization strategy to enhance long-horizon robustness. To align the generated videos with physical laws, we introduce a GRPO reinforcement learning post-training phase guided by a novel Physical Foresight Coherence (PFC) reward. PFC leverages the V-JEPA world model to enforce physical plausibility by aligning the predicted and actual dynamic evolutions in the feature space. MIND-V demonstrates state-of-the-art performance in long-horizon robotic manipulation video generation, establishing a scalable and controllable paradigm for embodied data synthesis.",
    "translation": "标题：MIND-V：基于强化学习物理对齐的长时程机器人操作分层视频生成框架\n\n摘要：具身模仿学习受限于多样化、长时程机器人操作数据的稀缺性。该领域现有的视频生成模型仅能合成简单动作的短片段，且常依赖人工定义的轨迹。为此，我们提出MIND-V——一种分层框架，旨在合成物理合理且逻辑连贯的长时程机器人操作视频。受认知科学启发，MIND-V通过三个核心组件将高层推理与像素级合成相连接：语义推理中心（SRH）利用预训练视觉-语言模型进行任务规划；行为语义桥（BSB）将抽象指令转化为领域无关的表征；运动视频生成器（MVG）实现条件视频渲染。MIND-V采用分阶段视觉未来推演策略，这是一种测试时优化方法，以增强长时程鲁棒性。为使生成视频符合物理规律，我们引入基于新型物理前瞻一致性（PFC）奖励的GRPO强化学习后训练阶段。PFC利用V-JEPA世界模型，通过在特征空间中对齐预测动态演化与实际动态演化来保证物理合理性。实验表明，MIND-V在长时程机器人操作视频生成任务中达到最先进性能，为具身数据合成建立了可扩展且可控的范式。",
    "url": "https://huggingface.co/papers/2512.06628",
    "arxiv_url": "https://arxiv.org/abs/2512.06628"
  },
  {
    "title": "DeepCode: Open Agentic Coding",
    "summary": "Recent advances in large language models (LLMs) have given rise to powerful coding agents, making it possible for code assistants to evolve into code engineers. However, existing methods still face significant challenges in achieving high-fidelity document-to-codebase synthesis--such as scientific papers to code--primarily due to a fundamental conflict between information overload and the context bottlenecks of LLMs. In this work, we introduce DeepCode, a fully autonomous framework that fundamentally addresses this challenge through principled information-flow management. By treating repository synthesis as a channel optimization problem, DeepCode seamlessly orchestrates four information operations to maximize task-relevant signals under finite context budgets: source compression via blueprint distillation, structured indexing using stateful code memory, conditional knowledge injection via retrieval-augmented generation, and closed-loop error correction. Extensive evaluations on the PaperBench benchmark demonstrate that DeepCode achieves state-of-the-art performance, decisively outperforming leading commercial agents such as Cursor and Claude Code, and crucially, surpassing PhD-level human experts from top institutes on key reproduction metrics. By systematically transforming paper specifications into production-grade implementations comparable to human expert quality, this work establishes new foundations for autonomous scientific reproduction that can accelerate research evaluation and discovery.",
    "translation": "标题：DeepCode：开放式智能体编码\n\n摘要：大型语言模型（LLM）的最新进展催生了强大的编码智能体，使得代码助手有望演变为代码工程师。然而，现有方法在实现高保真度的文档到代码库合成（例如从科学论文生成代码）方面仍面临重大挑战，这主要源于信息过载与LLM上下文容量限制之间的根本矛盾。本研究提出DeepCode，一个完全自主的框架，通过系统化的信息流管理从根本上解决这一难题。该框架将代码库合成视为信道优化问题，通过协调四种信息操作在有限上下文预算下最大化任务相关信号：基于蓝图提炼的源文档压缩、采用状态化代码记忆的结构化索引、通过检索增强生成的条件知识注入，以及闭环纠错机制。在PaperBench基准测试中的广泛评估表明，DeepCode实现了最先进的性能，显著超越Cursor、Claude Code等主流商业智能体，并在关键复现指标上超越顶尖机构的博士级人类专家。通过系统化地将论文规范转化为可媲美人类专家水准的生产级实现，本研究为自主科学复现奠定了新基础，有望加速研究评估与科学发现进程。",
    "url": "https://huggingface.co/papers/2512.07921",
    "arxiv_url": "https://arxiv.org/abs/2512.07921"
  },
  {
    "title": "From Next-Token to Next-Block: A Principled Adaptation Path for Diffusion LLMs",
    "summary": "Large language models (LLMs) excel at generation but dominant autoregressive (AR) decoding is inherently sequential, creating a throughput bottleneck. Diffusion Language Models (DLMs)--especially block-wise variants--enable parallel generation and intra-block bidirectional reasoning, yet training large DLMs from scratch is costly and wastes the knowledge in mature AR checkpoints. Prior \"adaptation\" attempts either modify logits or randomly grow attention masks to full-sequence diffusion, or simply transplant AR weights into a block-diffusion recipe, leaving a fundamental mismatch between AR causality and block-wise bidirectionality unaddressed. We reframe adaptation as a intra-paradigm path from AR to Block-Diffusion by viewing AR as Block-Diffusion with blocksize=1. Concretely, we design the pathway of adaptation as follows: we use a context-causal attention mask (causal in context, bidirectional only within the active block), an efficient parallel adaptation procedure, an auxiliary AR loss to maximize data utilization and retain pretrained knowledge, and gradual increment of the generation block size. The recipe integrates cleanly with masked block-diffusion and maintains train-inference consistency. Built on these components, NBDiff-7B (Base and Instruct) could inherit the long-context modeling and reasoning capabilities, and achieve state-of-the-art performance among the 7B-class DLMs, delivering strong gains on general-knowledge, math, and code benchmarks over strong baselines. These results demonstrate that principled AR-to-block-diffusion adaptation is an effective and compute-efficient alternative to training DLMs from scratch. Codes: https://github.com/YuchuanTian/NBDiff.",
    "translation": "标题：从下一词元到下一文本块：扩散大语言模型的原则性适配路径\n\n摘要：大语言模型（LLM）在生成任务上表现卓越，但主流的自回归解码方式本质上是顺序执行的，这造成了吞吐量瓶颈。扩散语言模型（DLM）——尤其是基于文本块的变体——能够实现并行生成和块内双向推理，然而从头训练大型DLM成本高昂，且浪费了成熟自回归模型检查点中的知识。先前的“适配”尝试要么修改逻辑值或将注意力掩码随机扩展至全序列扩散，要么简单地将自回归权重移植到块扩散框架中，均未解决自回归因果性与块内双向性之间的根本性失配问题。我们通过将自回归模型视为块大小等于1的块扩散模型，将适配重新定义为从自回归到块扩散的范式内路径。具体而言，我们设计了如下适配路径：采用上下文因果注意力掩码（在上下文维度保持因果性，仅在当前活动块内允许双向注意力）、高效的并行适配流程、辅助自回归损失函数以最大化数据利用并保留预训练知识，以及逐步增加生成块大小。该方法与掩码块扩散框架无缝集成，并保持了训练与推理的一致性。基于这些组件构建的NBDiff-7B（基础版与指导版）能够继承长上下文建模与推理能力，在7B参数量级的DLM中达到最先进的性能，在通用知识、数学和代码基准测试上相较于强基线模型取得显著提升。这些结果表明，遵循原则的自回归到块扩散适配策略是替代从头训练DLM的有效且计算高效的方法。代码地址：https://github.com/YuchuanTian/NBDiff。",
    "url": "https://huggingface.co/papers/2512.06776",
    "arxiv_url": "https://arxiv.org/abs/2512.06776"
  },
  {
    "title": "See, Hear, and Understand: Benchmarking Audiovisual Human Speech Understanding in Multimodal Large Language Models",
    "summary": "Multimodal large language models (MLLMs) are expected to jointly interpret vision, audio, and language, yet existing video benchmarks rarely assess fine-grained reasoning about human speech. Many tasks remain visually solvable or only coarsely evaluate speech, offering limited insight into whether models can align who speaks, what is said, and when it occurs. We introduce AV-SpeakerBench, a curated benchmark of 3,212 multiple-choice questions focused on speaker-centric audiovisual reasoning in real-world videos. It features: (1) a speaker-centered formulation that treats speakers-not scenes-as the core reasoning unit; (2) fusion-grounded question design embedding audiovisual dependencies into question semantics; and (3) expert-curated annotations ensuring temporal precision and cross-modal validity. Comprehensive evaluations show that the Gemini family consistently outperforms open-source systems, with Gemini 2.5 Pro achieving the best results. Among open models, Qwen3-Omni-30B approaches Gemini 2.0 Flash but remains far behind Gemini 2.5 Pro, primarily due to weaker audiovisual fusion rather than visual perception. We believe AV-SpeakerBench establishes a rigorous foundation for advancing fine-grained audiovisual reasoning in future multimodal systems.",
    "translation": "标题：视听理解：多模态大语言模型中人类语音理解的基准测试\n\n摘要：多模态大语言模型（MLLMs）被期望能够协同解读视觉、音频和语言信息，然而现有的视频基准测试很少评估对人类语音的细粒度推理能力。许多任务仍可通过视觉信息单独解决，或仅对语音进行粗略评估，难以深入揭示模型是否能够准确关联说话者身份、话语内容及其发生时间。为此，我们提出了AV-SpeakerBench，一个精心构建的基准测试集，包含3,212道基于真实世界视频的、以说话者为中心的视听推理选择题。该基准具有以下特点：（1）采用以说话者而非场景为核心的推理单元设计；（2）通过融合式问题设计，将视听依赖关系嵌入问题语义中；（3）经过专家标注，确保时间精度与跨模态有效性。综合评估结果表明，Gemini系列模型持续优于开源系统，其中Gemini 2.5 Pro取得最佳性能。在开源模型中，Qwen3-Omni-30B接近Gemini 2.0 Flash的水平，但仍远落后于Gemini 2.5 Pro，其主要差距源于视听融合能力较弱而非视觉感知不足。我们相信AV-SpeakerBench为推进未来多模态系统的细粒度视听推理研究奠定了严谨的基础。",
    "url": "https://huggingface.co/papers/2512.02231",
    "arxiv_url": "https://arxiv.org/abs/2512.02231"
  },
  {
    "title": "COREA: Coarse-to-Fine 3D Representation Alignment Between Relightable 3D Gaussians and SDF via Bidirectional 3D-to-3D Supervision",
    "summary": "We present COREA, the first unified framework that jointly learns relightable 3D Gaussians and a Signed Distance Field (SDF) for accurate geometry reconstruction and faithful relighting. While recent 3D Gaussian Splatting (3DGS) methods have extended toward mesh reconstruction and physically-based rendering (PBR), their geometry is still learned from 2D renderings, leading to coarse surfaces and unreliable BRDF-lighting decomposition. To address these limitations, COREA introduces a coarse-to-fine bidirectional 3D-to-3D alignment strategy that allows geometric signals to be learned directly in 3D space. Within this strategy, depth provides coarse alignment between the two representations, while depth gradients and normals refine fine-scale structure, and the resulting geometry supports stable BRDF-lighting decomposition. A density-control mechanism further stabilizes Gaussian growth, balancing geometric fidelity with memory efficiency. Experiments on standard benchmarks demonstrate that COREA achieves superior performance in novel-view synthesis, mesh reconstruction, and PBR within a unified framework.",
    "translation": "标题：COREA：基于双向三维到三维监督的可重光照三维高斯与符号距离场的从粗到精三维表示对齐\n\n摘要：本文提出COREA，首个联合学习可重光照三维高斯与符号距离场的统一框架，旨在实现精确几何重建与真实重光照。尽管近期三维高斯泼溅方法已扩展至网格重建与基于物理的渲染，但其几何信息仍从二维渲染中学习，导致表面粗糙且BRDF-光照分解不可靠。为克服这些局限，COREA引入一种从粗到精的双向三维到三维对齐策略，使几何信号能够在三维空间中直接学习。该策略中，深度信息为两种表示提供粗粒度对齐，深度梯度与法向量则优化细尺度结构，所得几何支撑稳定的BRDF-光照分解。密度控制机制进一步稳定高斯分布增长，在几何保真度与内存效率间取得平衡。在标准基准测试上的实验表明，COREA在统一框架内实现了新颖视角合成、网格重建与基于物理的渲染方面的卓越性能。",
    "url": "https://huggingface.co/papers/2512.07107",
    "arxiv_url": "https://arxiv.org/abs/2512.07107"
  },
  {
    "title": "TreeGRPO: Tree-Advantage GRPO for Online RL Post-Training of Diffusion Models",
    "summary": "Reinforcement learning (RL) post-training is crucial for aligning generative models with human preferences, but its prohibitive computational cost remains a major barrier to widespread adoption. We introduce TreeGRPO, a novel RL framework that dramatically improves training efficiency by recasting the denoising process as a search tree. From shared initial noise samples, TreeGRPO strategically branches to generate multiple candidate trajectories while efficiently reusing their common prefixes. This tree-structured approach delivers three key advantages: (1) High sample efficiency, achieving better performance under same training samples (2) Fine-grained credit assignment via reward backpropagation that computes step-specific advantages, overcoming the uniform credit assignment limitation of trajectory-based methods, and (3) Amortized computation where multi-child branching enables multiple policy updates per forward pass. Extensive experiments on both diffusion and flow-based models demonstrate that TreeGRPO achieves 2.4times faster training while establishing a superior Pareto frontier in the efficiency-reward trade-off space. Our method consistently outperforms GRPO baselines across multiple benchmarks and reward models, providing a scalable and effective pathway for RL-based visual generative model alignment. The project website is available at treegrpo.github.io.",
    "translation": "标题：TreeGRPO：基于树优势的GRPO用于扩散模型在线强化学习后训练\n\n摘要：强化学习后训练对于使生成模型与人类偏好对齐至关重要，但其高昂的计算成本仍是广泛采用的主要障碍。我们提出了TreeGRPO，一种新颖的强化学习框架，通过将去噪过程重构为搜索树，显著提升了训练效率。该方法从共享的初始噪声样本出发，通过策略性分支生成多个候选轨迹，同时高效复用其公共前缀。这种树形结构方法具有三大关键优势：（1）高样本效率，在相同训练样本下实现更优性能；（2）通过奖励反向传播进行细粒度信用分配，计算步骤级优势值，克服了基于轨迹方法中均匀信用分配的局限性；（3）摊销式计算，多子节点分支机制使得单次前向传播即可完成多次策略更新。在扩散模型和流模型上的大量实验表明，TreeGRPO在效率-奖励权衡空间中建立了更优的帕累托边界，同时实现了2.4倍的训练加速。该方法在多个基准测试和奖励模型中持续超越GRPO基线，为基于强化学习的视觉生成模型对齐提供了可扩展且有效的技术路径。项目网站详见treegrpo.github.io。",
    "url": "https://huggingface.co/papers/2512.08153",
    "arxiv_url": "https://arxiv.org/abs/2512.08153"
  },
  {
    "title": "Efficiently Reconstructing Dynamic Scenes One D4RT at a Time",
    "summary": "Understanding and reconstructing the complex geometry and motion of dynamic scenes from video remains a formidable challenge in computer vision. This paper introduces D4RT, a simple yet powerful feedforward model designed to efficiently solve this task. D4RT utilizes a unified transformer architecture to jointly infer depth, spatio-temporal correspondence, and full camera parameters from a single video. Its core innovation is a novel querying mechanism that sidesteps the heavy computation of dense, per-frame decoding and the complexity of managing multiple, task-specific decoders. Our decoding interface allows the model to independently and flexibly probe the 3D position of any point in space and time. The result is a lightweight and highly scalable method that enables remarkably efficient training and inference. We demonstrate that our approach sets a new state of the art, outperforming previous methods across a wide spectrum of 4D reconstruction tasks. We refer to the project webpage for animated results: https://d4rt-paper.github.io/.",
    "translation": "标题：高效动态场景重建：逐D4RT实现\n\n摘要：从视频中理解并重建动态场景的复杂几何结构与运动，一直是计算机视觉领域的一项艰巨挑战。本文提出D4RT，一种简洁而强大的前馈模型，旨在高效解决此任务。D4RT采用统一的Transformer架构，能够从单一视频中联合推断深度、时空对应关系以及完整的相机参数。其核心创新在于一种新颖的查询机制，该机制避免了密集逐帧解码的繁重计算，也规避了管理多个任务特定解码器的复杂性。我们的解码接口使模型能够独立且灵活地探测时空任意点的三维位置。由此产生了一种轻量且高度可扩展的方法，实现了显著高效的训练与推理。实验表明，我们的方法在广泛的4D重建任务中超越了现有技术，确立了新的性能标杆。动态效果请参见项目网页：https://d4rt-paper.github.io/。",
    "url": "https://huggingface.co/papers/2512.08924",
    "arxiv_url": "https://arxiv.org/abs/2512.08924"
  },
  {
    "title": "Modular Neural Image Signal Processing",
    "summary": "This paper presents a modular neural image signal processing (ISP) framework that processes raw inputs and renders high-quality display-referred images. Unlike prior neural ISP designs, our method introduces a high degree of modularity, providing full control over multiple intermediate stages of the rendering process.~This modular design not only achieves high rendering accuracy but also improves scalability, debuggability, generalization to unseen cameras, and flexibility to match different user-preference styles. To demonstrate the advantages of this design, we built a user-interactive photo-editing tool that leverages our neural ISP to support diverse editing operations and picture styles. The tool is carefully engineered to take advantage of the high-quality rendering of our neural ISP and to enable unlimited post-editable re-rendering. Our method is a fully learning-based framework with variants of different capacities, all of moderate size (ranging from ~0.5 M to ~3.9 M parameters for the entire pipeline), and consistently delivers competitive qualitative and quantitative results across multiple test sets. Watch the supplemental video at: https://youtu.be/ByhQjQSjxVM",
    "translation": "标题：模块化神经图像信号处理\n\n摘要：本文提出了一种模块化神经图像信号处理（ISP）框架，该框架处理原始输入并渲染出高质量的显示参考图像。与先前的神经ISP设计不同，我们的方法引入了高度的模块化，实现了对渲染过程中多个中间阶段的完全控制。这种模块化设计不仅实现了高渲染精度，还提升了可扩展性、可调试性、对未见相机的泛化能力以及匹配不同用户偏好风格的灵活性。为展示该设计的优势，我们构建了一个用户交互式照片编辑工具，该工具利用我们的神经ISP支持多样化的编辑操作和图片风格。该工具经过精心设计，以充分利用我们神经ISP的高质量渲染能力，并支持无限次可后期编辑的重新渲染。我们的方法是一个完全基于学习的框架，具有不同容量的变体，所有变体规模适中（整个流程参数量约为0.5M至3.9M），并在多个测试集上持续提供具有竞争力的定性与定量结果。补充视频请访问：https://youtu.be/ByhQjQSjxVM",
    "url": "https://huggingface.co/papers/2512.08564",
    "arxiv_url": "https://arxiv.org/abs/2512.08564"
  },
  {
    "title": "Ground Slow, Move Fast: A Dual-System Foundation Model for Generalizable Vision-and-Language Navigation",
    "summary": "While recent large vision-language models (VLMs) have improved generalization in vision-language navigation (VLN), existing methods typically rely on end-to-end pipelines that map vision-language inputs directly to short-horizon discrete actions. Such designs often produce fragmented motions, incur high latency, and struggle with real-world challenges like dynamic obstacle avoidance. We propose DualVLN, the first dual-system VLN foundation model that synergistically integrates high-level reasoning with low-level action execution. System 2, a VLM-based global planner, \"grounds slowly\" by predicting mid-term waypoint goals via image-grounded reasoning. System 1, a lightweight, multi-modal conditioning Diffusion Transformer policy, \"moves fast\" by leveraging both explicit pixel goals and latent features from System 2 to generate smooth and accurate trajectories. The dual-system design enables robust real-time control and adaptive local decision-making in complex, dynamic environments. By decoupling training, the VLM retains its generalization, while System 1 achieves interpretable and effective local navigation. DualVLN outperforms prior methods across all VLN benchmarks and real-world experiments demonstrate robust long-horizon planning and real-time adaptability in dynamic environments.",
    "translation": "标题：稳扎稳打，快速行动：一种面向可泛化视觉语言导航的双系统基础模型\n\n摘要：尽管近期的大型视觉语言模型（VLMs）提升了视觉语言导航（VLN）的泛化能力，但现有方法通常依赖于端到端管道，直接将视觉语言输入映射为短视程的离散动作。此类设计往往导致动作片段化、延迟较高，且难以应对动态避障等现实挑战。本文提出DualVLN，首个双系统视觉语言导航基础模型，其通过协同整合高层推理与低层动作执行来实现导航。系统2是一个基于VLM的全局规划器，通过基于图像的推理预测中程航点目标，实现“稳扎稳打”；系统1则是一个轻量级、多模态条件扩散Transformer策略，利用系统2提供的显式像素目标与潜在特征生成平滑精准的轨迹，实现“快速行动”。双系统设计使得模型能够在复杂动态环境中实现鲁棒的实时控制与自适应局部决策。通过解耦训练，VLM保持了其泛化能力，而系统1则实现了可解释且高效的局部导航。DualVLN在所有VLN基准测试中均优于现有方法，真实环境实验进一步验证了其在动态环境中具备鲁棒的长视程规划能力与实时适应性。",
    "url": "https://huggingface.co/papers/2512.08186",
    "arxiv_url": "https://arxiv.org/abs/2512.08186"
  },
  {
    "title": "SUCCESS-GS: Survey of Compactness and Compression for Efficient Static and Dynamic Gaussian Splatting",
    "summary": "3D Gaussian Splatting (3DGS) has emerged as a powerful explicit representation enabling real-time, high-fidelity 3D reconstruction and novel view synthesis. However, its practical use is hindered by the massive memory and computational demands required to store and render millions of Gaussians. These challenges become even more severe in 4D dynamic scenes. To address these issues, the field of Efficient Gaussian Splatting has rapidly evolved, proposing methods that reduce redundancy while preserving reconstruction quality. This survey provides the first unified overview of efficient 3D and 4D Gaussian Splatting techniques. For both 3D and 4D settings, we systematically categorize existing methods into two major directions, Parameter Compression and Restructuring Compression, and comprehensively summarize the core ideas and methodological trends within each category. We further cover widely used datasets, evaluation metrics, and representative benchmark comparisons. Finally, we discuss current limitations and outline promising research directions toward scalable, compact, and real-time Gaussian Splatting for both static and dynamic 3D scene representation.",
    "translation": "标题：SUCCESS-GS：高效静态与动态高斯泼溅的紧凑性与压缩技术综述\n\n摘要：3D高斯泼溅（3DGS）作为一种强大的显式表示方法，已能够实现实时、高保真的三维重建与新视角合成。然而，其实际应用受到存储和渲染数百万高斯点所需巨大内存与计算需求的限制。在4D动态场景中，这些挑战更为严峻。为应对这些问题，高效高斯泼溅领域迅速发展，提出了多种在保持重建质量的同时减少冗余的方法。本综述首次对高效3D与4D高斯泼溅技术进行了统一梳理。针对3D与4D场景，我们系统地将现有方法归纳为参数压缩与结构重组压缩两大方向，并全面总结了各类方法的核心思想与技术趋势。此外，我们还涵盖了广泛使用的数据集、评估指标以及代表性基准测试比较。最后，我们探讨了当前局限，并展望了面向静态与动态三维场景表示的可扩展、紧凑且实时的高斯泼溅技术的未来研究方向。",
    "url": "https://huggingface.co/papers/2512.07197",
    "arxiv_url": "https://arxiv.org/abs/2512.07197"
  },
  {
    "title": "EcomBench: Towards Holistic Evaluation of Foundation Agents in E-commerce",
    "summary": "Foundation agents have rapidly advanced in their ability to reason and interact with real environments, making the evaluation of their core capabilities increasingly important. While many benchmarks have been developed to assess agent performance, most concentrate on academic settings or artificially designed scenarios while overlooking the challenges that arise in real applications. To address this issue, we focus on a highly practical real-world setting, the e-commerce domain, which involves a large volume of diverse user interactions, dynamic market conditions, and tasks directly tied to real decision-making processes. To this end, we introduce EcomBench, a holistic E-commerce Benchmark designed to evaluate agent performance in realistic e-commerce environments. EcomBench is built from genuine user demands embedded in leading global e-commerce ecosystems and is carefully curated and annotated through human experts to ensure clarity, accuracy, and domain relevance. It covers multiple task categories within e-commerce scenarios and defines three difficulty levels that evaluate agents on key capabilities such as deep information retrieval, multi-step reasoning, and cross-source knowledge integration. By grounding evaluation in real e-commerce contexts, EcomBench provides a rigorous and dynamic testbed for measuring the practical capabilities of agents in modern e-commerce.",
    "translation": "标题：EcomBench：面向电子商务领域基础智能体的综合性评估框架\n\n摘要：基础智能体在环境推理与交互能力方面发展迅速，对其核心能力进行系统性评估的需求日益凸显。现有评估基准多集中于学术场景或人工设计环境，往往忽略真实应用场景中的复杂挑战。为应对这一问题，本研究聚焦于具有高度实践价值的电子商务领域——该领域涉及海量异构用户交互、动态市场环境以及与真实决策流程紧密关联的任务。为此，我们提出EcomBench：一个基于真实电子商务场景构建的综合性评估基准。该基准源自全球主流电商生态中的真实用户需求，经由领域专家精心标注与校验，确保任务表述的清晰性、准确性与领域相关性。EcomBench涵盖电商场景下的多类任务，并设立三个难度层级，重点评估智能体的深度信息检索、多步推理及跨源知识整合等关键能力。通过将评估体系锚定于真实电商环境，EcomBench为衡量智能体在现代电商场景中的实际应用能力提供了严谨且动态的测试平台。",
    "url": "https://huggingface.co/papers/2512.08868",
    "arxiv_url": "https://arxiv.org/abs/2512.08868"
  },
  {
    "title": "TrackingWorld: World-centric Monocular 3D Tracking of Almost All Pixels",
    "summary": "Monocular 3D tracking aims to capture the long-term motion of pixels in 3D space from a single monocular video and has witnessed rapid progress in recent years. However, we argue that the existing monocular 3D tracking methods still fall short in separating the camera motion from foreground dynamic motion and cannot densely track newly emerging dynamic subjects in the videos. To address these two limitations, we propose TrackingWorld, a novel pipeline for dense 3D tracking of almost all pixels within a world-centric 3D coordinate system. First, we introduce a tracking upsampler that efficiently lifts the arbitrary sparse 2D tracks into dense 2D tracks. Then, to generalize the current tracking methods to newly emerging objects, we apply the upsampler to all frames and reduce the redundancy of 2D tracks by eliminating the tracks in overlapped regions. Finally, we present an efficient optimization-based framework to back-project dense 2D tracks into world-centric 3D trajectories by estimating the camera poses and the 3D coordinates of these 2D tracks. Extensive evaluations on both synthetic and real-world datasets demonstrate that our system achieves accurate and dense 3D tracking in a world-centric coordinate frame.",
    "translation": "标题：TrackingWorld：基于世界坐标系的单目视频中几乎所有像素的三维跟踪\n\n摘要：单目三维跟踪旨在从单目视频中捕获像素在三维空间中的长期运动，近年来取得了快速进展。然而，我们认为现有的单目三维跟踪方法在分离相机运动与前景动态运动方面仍存在不足，且无法对视频中新出现的动态目标进行密集跟踪。针对这两点局限性，我们提出了TrackingWorld——一种在世界中心三维坐标系下对几乎所有像素进行密集三维跟踪的新流程。首先，我们引入一种跟踪上采样器，能够高效地将任意稀疏二维跟踪结果提升为密集二维跟踪。其次，为将现有跟踪方法推广至新出现的物体，我们将上采样器应用于所有帧，并通过消除重叠区域的跟踪轨迹来减少二维跟踪的冗余。最后，我们提出一种基于优化的高效框架，通过估计相机位姿及这些二维跟踪点的三维坐标，将密集二维跟踪结果反投影至世界中心的三维轨迹。在合成数据集与真实数据集上的大量实验表明，我们的系统能够在世界中心坐标系下实现精确且密集的三维跟踪。",
    "url": "https://huggingface.co/papers/2512.08358",
    "arxiv_url": "https://arxiv.org/abs/2512.08358"
  },
  {
    "title": "Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images",
    "summary": "Brain tumors pose a significant threat to human life, therefore it is very much necessary to detect them accurately in the early stages for better diagnosis and treatment. Brain tumors can be detected by the radiologist manually from the MRI scan images of the patients. However, the incidence of brain tumors has risen amongst children and adolescents in recent years, resulting in a substantial volume of data, as a result, it is time-consuming and difficult to detect manually. With the emergence of Artificial intelligence in the modern world and its vast application in the medical field, we can make an approach to the CAD (Computer Aided Diagnosis) system for the early detection of Brain tumors automatically. All the existing models for this task are not completely generalized and perform poorly on the validation data. So, we have proposed two novel Deep Learning Architectures - (a) SAETCN (Self-Attention Enhancement Tumor Classification Network) for the classification of different kinds of brain tumors. We have achieved an accuracy of 99.38% on the validation dataset making it one of the few Novel Deep learning-based architecture that is capable of detecting brain tumors accurately. We have trained the model on the dataset, which contains images of 3 types of tumors (glioma, meningioma, and pituitary tumors) and non-tumor cases. and (b) SAS-Net (Self-Attentive Segmentation Network) for the accurate segmentation of brain tumors. We have achieved an overall pixel accuracy of 99.23%.",
    "translation": "标题：基于MRI图像的脑肿瘤分类与分割新型深度学习架构研究\n\n摘要：脑肿瘤对人类生命构成重大威胁，因此在早期阶段进行精准检测对于优化诊疗方案至关重要。目前脑肿瘤主要通过放射科医生人工判读患者MRI扫描图像进行识别。然而近年来儿童与青少年脑肿瘤发病率持续上升，导致医学影像数据量急剧增长，人工检测方式已面临耗时费力且效率低下的挑战。随着人工智能技术的快速发展及其在医疗领域的广泛应用，我们提出采用计算机辅助诊断系统实现脑肿瘤的自动化早期检测。现有模型普遍存在泛化能力不足的问题，在验证数据集上表现欠佳。为此，本研究提出两种新型深度学习架构：（a）SAETCN（自注意力增强肿瘤分类网络），用于实现多类别脑肿瘤分类。该模型在包含胶质瘤、脑膜瘤、垂体瘤及非肿瘤病例的四类影像数据集上进行训练，在验证集上取得了99.38%的分类准确率，成为少数能够实现脑肿瘤精准识别的创新深度学习架构；（b）SAS-Net（自注意力分割网络），专门用于脑肿瘤的精确区域分割，最终实现了99.23%的整体像素精度。",
    "url": "https://huggingface.co/papers/2512.06531",
    "arxiv_url": "https://arxiv.org/abs/2512.06531"
  },
  {
    "title": "LYNX: Learning Dynamic Exits for Confidence-Controlled Reasoning",
    "summary": "Large reasoning models achieve strong performance on complex tasks by generating extended chains of thought, but they often \"overthink\": continuing to reason long after they have enough information to answer correctly. This wastes inference-time compute and can hurt accuracy. Existing attempts to stop early either manipulate decoding with extra sampling and heuristics, rely on auxiliary verifier models, or operate only as post-hoc analysis pipelines without formal guarantees. We introduce LYNX, an online early-exit mechanism that turns a model's own hidden-state awareness into confidence-controlled stopping decisions. LYNX attaches exit decisions to naturally occurring reasoning cues (e.g., \"hmm\", \"wait\") during generation, trains a lightweight probe on hidden states at those cue tokens using supervision from forced exits, and wraps the resulting scores in split conformal prediction to obtain distribution-free control over premature exits. Crucially, we train and calibrate this probe once on a generic mathematical corpus and reuse it unchanged across benchmarks, decoding temperatures, and even non-mathematical tasks. Across three model families spanning 1.5B to 32B parameters, a single mathematically trained probe per base model yields strong accuracy--efficiency tradeoffs. On GSM8K, LYNX matches or improves baseline accuracy while reducing tokens by 40--65\\%; on MATH-500 it improves accuracy by up to 12 points with roughly 35--60\\% fewer tokens; on AIME 2024 it recovers baseline accuracy with more than 50\\% token savings; and on CommonsenseQA, a non-math benchmark, it transfers zero-shot with modest accuracy gains and up to 70\\% fewer tokens. Compared to state-of-the-art early-exit methods, LYNX offers competitive or superior Pareto frontiers while remaining fully online, requiring no proxy models at inference, and providing explicit, user-tunable confidence guarantees.",
    "translation": "标题：LYNX：面向置信度控制推理的动态出口学习机制\n\n摘要：大型推理模型通过生成扩展的思维链在复杂任务上展现出强大性能，但常存在“过度思考”现象：即在已掌握足够信息给出正确答案后仍持续推理。这不仅浪费推理计算资源，还可能损害准确性。现有早期停止方案或依赖额外采样与启发式方法干预解码过程，或需要辅助验证模型支持，抑或仅作为缺乏形式化保证的事后分析流程。本文提出LYNX——一种在线早期退出机制，将模型自身的隐藏状态感知转化为置信度控制的停止决策。LYNX在生成过程中将退出决策锚定于自然出现的推理线索（如“嗯”“等等”），利用强制退出生成的监督信号在这些线索标记对应的隐藏状态上训练轻量化探针，并通过分割合规预测框架对得分进行封装，从而实现对过早退出的无分布控制。关键创新在于：该探针仅需在通用数学语料上进行一次性训练与校准，即可跨基准测试、解码温度乃至非数学任务直接复用。在涵盖15亿至320亿参数的三个模型系列中，每个基础模型仅需配备单一数学训练探针即可实现优异的准确率-效率权衡。在GSM8K数据集上，LYNX在减少40%-65%标记使用量的同时保持或提升基线准确率；在MATH-500数据集上以约35%-60%的标记缩减实现最高12个百分点的准确率提升；在AIME 2024竞赛题中，以超过50%的标记节省恢复基线准确率；在常识推理基准CommonsenseQA（非数学任务）上，该机制实现零样本迁移，在获得适度准确率提升的同时减少高达70%的标记消耗。相较于前沿早期退出方法，LYNX在保持完全在线运行、无需推理时代理模型、提供用户可调节的显式置信度保证的前提下，呈现出具有竞争力或更优的帕累托前沿。",
    "url": "https://huggingface.co/papers/2512.05325",
    "arxiv_url": "https://arxiv.org/abs/2512.05325"
  },
  {
    "title": "MemLoRA: Distilling Expert Adapters for On-Device Memory Systems",
    "summary": "Memory-augmented Large Language Models (LLMs) have demonstrated remarkable consistency during prolonged dialogues by storing relevant memories and incorporating them as context. Such memory-based personalization is also key in on-device settings that allow users to keep their conversations and data private. However, memory-augmented systems typically rely on LLMs that are too costly for local on-device deployment. Even though Small Language Models (SLMs) are more suitable for on-device inference than LLMs, they cannot achieve sufficient performance. Additionally, these LLM-based systems lack native visual capabilities, limiting their applicability in multimodal contexts. In this paper, we introduce (i) MemLoRA, a novel memory system that enables local deployment by equipping SLMs with specialized memory adapters, and (ii) its vision extension MemLoRA-V, which integrates small Vision-Language Models (SVLMs) to memory systems, enabling native visual understanding. Following knowledge distillation principles, each adapter is trained separately for specific memory operationsx2013knowledge extraction, memory update, and memory-augmented generation. Equipped with memory adapters, small models enable accurate on-device memory operations without cloud dependency. On text-only operations, MemLoRA outperforms 10times larger baseline models (e.g., Gemma2-27B) and achieves performance comparable to 60times larger models (e.g., GPT-OSS-120B) on the LoCoMo benchmark. To evaluate visual understanding operations instead, we extend LoCoMo with challenging Visual Question Answering tasks that require direct visual reasoning. On this, our VLM-integrated MemLoRA-V shows massive improvements over caption-based approaches (81.3 vs. 23.7 accuracy) while keeping strong performance in text-based tasks, demonstrating the efficacy of our method in multimodal contexts.",
    "translation": "标题：MemLoRA：面向设备端内存系统的专家适配器蒸馏方法\n\n摘要：内存增强型大语言模型（LLMs）通过存储相关记忆并将其作为上下文融入对话，在长程对话中展现出卓越的连贯性。这种基于记忆的个性化技术对于允许用户保持对话和数据私密性的设备端场景尤为重要。然而，现有内存增强系统通常依赖计算成本高昂的大语言模型，难以在本地设备端部署。尽管小语言模型（SLMs）比大语言模型更适合设备端推理，但其性能往往不足。此外，现有基于大语言模型的系统缺乏原生视觉理解能力，限制了其在多模态场景中的应用。本文提出：（1）MemLoRA，一种新型内存系统，通过为小语言模型配备专用记忆适配器实现本地部署；（2）其视觉扩展版本MemLoRA-V，通过集成小型视觉语言模型（SVLMs）使内存系统具备原生视觉理解能力。基于知识蒸馏原理，每个适配器针对特定记忆操作（知识提取、记忆更新和记忆增强生成）进行独立训练。配备记忆适配器的小模型能够在无需云端依赖的情况下，实现精确的设备端内存操作。在纯文本任务中，MemLoRA的性能超越规模10倍于自身的基线模型（如Gemma2-27B），并在LoCoMo基准测试中达到与60倍规模模型（如GPT-OSS-120B）相当的水平。为评估视觉理解能力，我们扩展了LoCoMo基准，引入需要直接视觉推理的视觉问答任务。在此测试中，集成视觉语言模型的MemLoRA-V相比基于图像描述的方法取得显著提升（准确率81.3对23.7），同时在文本任务中保持强劲性能，证明了该方法在多模态场景中的有效性。",
    "url": "https://huggingface.co/papers/2512.04763",
    "arxiv_url": "https://arxiv.org/abs/2512.04763"
  },
  {
    "title": "SegEarth-OV3: Exploring SAM 3 for Open-Vocabulary Semantic Segmentation in Remote Sensing Images",
    "summary": "Most existing methods for training-free Open-Vocabulary Semantic Segmentation (OVSS) are based on CLIP. While these approaches have made progress, they often face challenges in precise localization or require complex pipelines to combine separate modules, especially in remote sensing scenarios where numerous dense and small targets are present. Recently, Segment Anything Model 3 (SAM 3) was proposed, unifying segmentation and recognition in a promptable framework. In this paper, we present a preliminary exploration of applying SAM 3 to the remote sensing OVSS task without any training. First, we implement a mask fusion strategy that combines the outputs from SAM 3's semantic segmentation head and the Transformer decoder (instance head). This allows us to leverage the strengths of both heads for better land coverage. Second, we utilize the presence score from the presence head to filter out categories that do not exist in the scene, reducing false positives caused by the vast vocabulary sizes and patch-level processing in geospatial scenes. We evaluate our method on extensive remote sensing datasets. Experiments show that this simple adaptation achieves promising performance, demonstrating the potential of SAM 3 for remote sensing OVSS. Our code is released at https://github.com/earth-insights/SegEarth-OV-3.",
    "translation": "标题：SegEarth-OV3：探索SAM 3在遥感图像开放词汇语义分割中的应用\n\n摘要：目前大多数免训练的开放词汇语义分割方法基于CLIP模型。尽管这些方法已取得进展，但在精确定位方面常面临挑战，或需要复杂的流程来整合独立模块，尤其在包含大量密集小目标的遥感场景中。近期提出的Segment Anything Model 3（SAM 3）在可提示框架中统一了分割与识别任务。本文对SAM 3在遥感开放词汇语义分割任务中的免训练应用进行了初步探索。首先，我们设计了一种掩码融合策略，将SAM 3语义分割头与Transformer解码器（实例头）的输出相结合，从而综合利用两个模块的优势以提升地物覆盖能力。其次，我们利用存在性头生成的存在分数过滤场景中不存在的类别，以减少地理空间场景中庞大词汇量与斑块级处理导致的误判。我们在多个遥感数据集上评估了该方法，实验表明这种轻量化适配方案取得了显著效果，证明了SAM 3在遥感开放词汇语义分割任务中的潜力。代码已发布于https://github.com/earth-insights/SegEarth-OV-3。",
    "url": "https://huggingface.co/papers/2512.08730",
    "arxiv_url": "https://arxiv.org/abs/2512.08730"
  },
  {
    "title": "SAM-Body4D: Training-Free 4D Human Body Mesh Recovery from Videos",
    "summary": "Human Mesh Recovery (HMR) aims to reconstruct 3D human pose and shape from 2D observations and is fundamental to human-centric understanding in real-world scenarios. While recent image-based HMR methods such as SAM 3D Body achieve strong robustness on in-the-wild images, they rely on per-frame inference when applied to videos, leading to temporal inconsistency and degraded performance under occlusions. We address these issues without extra training by leveraging the inherent human continuity in videos. We propose SAM-Body4D, a training-free framework for temporally consistent and occlusion-robust HMR from videos. We first generate identity-consistent masklets using a promptable video segmentation model, then refine them with an Occlusion-Aware module to recover missing regions. The refined masklets guide SAM 3D Body to produce consistent full-body mesh trajectories, while a padding-based parallel strategy enables efficient multi-human inference. Experimental results demonstrate that SAM-Body4D achieves improved temporal stability and robustness in challenging in-the-wild videos, without any retraining. Our code and demo are available at: https://github.com/gaomingqi/sam-body4d.",
    "translation": "标题：SAM-Body4D：无需训练的从视频中恢复四维人体网格\n\n摘要：人体网格恢复旨在从二维观测数据中重建三维人体姿态与形状，是现实场景中以人为中心理解任务的基础。尽管近期基于图像的人体网格恢复方法（如SAM 3D Body）在真实场景图像中展现出较强的鲁棒性，但在处理视频时仍需逐帧推理，导致时间不一致性且在遮挡情况下性能下降。本研究通过利用视频中人体固有的连续性，在不增加额外训练的前提下解决了上述问题。我们提出了SAM-Body4D——一个无需训练的框架，能够从视频中实现时间一致且抗遮挡的人体网格恢复。我们首先使用可提示视频分割模型生成身份一致的掩码片段，随后通过遮挡感知模块进行优化以恢复缺失区域。优化后的掩码片段引导SAM 3D Body生成连贯的全身体网格轨迹，而基于填充的并行策略实现了高效的多人体推理。实验结果表明，SAM-Body4D在具有挑战性的真实场景视频中显著提升了时间稳定性与鲁棒性，且无需任何重新训练。我们的代码与演示已发布于：https://github.com/gaomingqi/sam-body4d。",
    "url": "https://huggingface.co/papers/2512.08406",
    "arxiv_url": "https://arxiv.org/abs/2512.08406"
  },
  {
    "title": "Terrain Diffusion: A Diffusion-Based Successor to Perlin Noise in Infinite, Real-Time Terrain Generation",
    "summary": "For decades, procedural worlds have been built on procedural noise functions such as Perlin noise, which are fast and infinite, yet fundamentally limited in realism and large-scale coherence. We introduce Terrain Diffusion, an AI-era successor to Perlin noise that bridges the fidelity of diffusion models with the properties that made procedural noise indispensable: seamless infinite extent, seed-consistency, and constant-time random access. At its core is InfiniteDiffusion, a novel algorithm for infinite generation, enabling seamless, real-time synthesis of boundless landscapes. A hierarchical stack of diffusion models couples planetary context with local detail, while a compact Laplacian encoding stabilizes outputs across Earth-scale dynamic ranges. An open-source infinite-tensor framework supports constant-memory manipulation of unbounded tensors, and few-step consistency distillation enables efficient generation. Together, these components establish diffusion models as a practical foundation for procedural world generation, capable of synthesizing entire planets coherently, controllably, and without limits.",
    "translation": "标题：地形扩散：基于扩散模型的无限实时地形生成技术——柏林噪声的继任者\n\n摘要：数十年来，程序化生成的世界一直建立在柏林噪声等程序化噪声函数基础上，这些方法虽然快速且能无限生成，但在真实感与大规模连贯性方面存在根本性局限。本文提出“地形扩散”技术，作为柏林噪声在人工智能时代的新继任者，它将扩散模型的高保真度与程序化噪声不可或缺的核心特性——无缝无限延展、种子一致性和恒定时间随机访问——相结合。其核心是“无限扩散”算法，这是一种实现无限生成的新方法，能够无缝实时合成无边界地形景观。通过层级化堆叠的扩散模型，将行星级环境信息与局部细节相耦合，同时采用紧凑的拉普拉斯编码机制，确保在地球尺度的动态范围内保持输出稳定性。开源的无限张量框架支持对无界张量的恒定内存操作，而少步一致性蒸馏技术则实现了高效生成。这些组件共同确立了扩散模型作为程序化世界生成的实践基础，能够以连贯、可控且无限制的方式合成整个行星地貌。",
    "url": "https://huggingface.co/papers/2512.08309",
    "arxiv_url": "https://arxiv.org/abs/2512.08309"
  },
  {
    "title": "Predicting Time-Dependent Flow Over Complex Geometries Using Operator Networks",
    "summary": "Fast, geometry-generalizing surrogates for unsteady flow remain challenging. We present a time-dependent, geometry-aware Deep Operator Network that predicts velocity fields for moderate-Re flows around parametric and non-parametric shapes. The model encodes geometry via a signed distance field (SDF) trunk and flow history via a CNN branch, trained on 841 high-fidelity simulations. On held-out shapes, it attains sim 5% relative L2 single-step error and up to 1000X speedups over CFD. We provide physics-centric rollout diagnostics, including phase error at probes and divergence norms, to quantify long-horizon fidelity. These reveal accurate near-term transients but error accumulation in fine-scale wakes, most pronounced for sharp-cornered geometries. We analyze failure modes and outline practical mitigations. Code, splits, and scripts are openly released at: https://github.com/baskargroup/TimeDependent-DeepONet to support reproducibility and benchmarking.",
    "translation": "标题：基于算子网络预测复杂几何结构上的时变流动\n\n摘要：构建快速且适用于复杂几何结构的非定常流动替代模型仍具挑战性。本文提出一种具有时间依赖性与几何感知能力的深度算子网络，能够预测参数化及非参数化形状周围中等雷诺数流动的速度场。该模型通过符号距离场主干网络编码几何信息，并利用卷积神经网络分支编码流动历史信息，基于841组高精度仿真数据进行训练。在未见过的几何形状上，模型实现了约5%的相对L2单步误差，计算速度较传统计算流体力学方法提升最高达1000倍。我们提供了以物理特性为核心的推演诊断方法，包括监测点相位误差与散度范数，以量化长期预测的保真度。结果显示模型能准确预测短期瞬态过程，但在精细尺度尾流中会出现误差累积，这一现象在尖角几何结构中尤为显著。我们分析了模型的失效模式并提出了实际改进方案。为支持可重复性与基准测试，代码、数据划分及脚本已公开发布于：https://github.com/baskargroup/TimeDependent-DeepONet。",
    "url": "https://huggingface.co/papers/2512.04434",
    "arxiv_url": "https://arxiv.org/abs/2512.04434"
  },
  {
    "title": "Same Content, Different Answers: Cross-Modal Inconsistency in MLLMs",
    "summary": "We introduce two new benchmarks REST and REST+(Render-Equivalence Stress Tests) to enable systematic evaluation of cross-modal inconsistency in multimodal large language models (MLLMs). MLLMs are trained to represent vision and language in the same embedding space, yet they cannot perform the same tasks in both modalities. Our benchmarks contain samples with the same semantic information in three modalities (image, text, mixed) and we show that state-of-the-art MLLMs cannot consistently reason over these different modalities. We evaluate 15 MLLMs and find that the degree of modality inconsistency varies substantially, even when accounting for problems with text recognition (OCR). Neither rendering text as image nor rendering an image as text solves the inconsistency. Even if OCR is correct, we find that visual characteristics (text colour and resolution, but not font) and the number of vision tokens have an impact on model performance. Finally, we find that our consistency score correlates with the modality gap between text and images, highlighting a mechanistic interpretation of cross-modal inconsistent MLLMs.",
    "translation": "标题：相同内容，不同答案：多模态大语言模型中的跨模态不一致性\n\n摘要：我们引入了两个新基准测试REST与REST+（渲染等价性压力测试），以系统评估多模态大语言模型（MLLMs）中的跨模态不一致性。MLLMs被训练用于在同一嵌入空间中表示视觉与语言信息，但它们无法在两种模态中执行相同任务。我们的基准测试包含三种模态（图像、文本、混合）下具有相同语义信息的样本，研究表明当前最先进的MLLMs无法在不同模态间保持一致的推理能力。通过对15个MLLMs的评估发现，即使排除文本识别（OCR）问题的影响，模态不一致程度仍存在显著差异。无论是将文本渲染为图像还是将图像渲染为文本，均无法解决这种不一致性。即使OCR识别正确，视觉特征（文本颜色与分辨率，但字体除外）和视觉标记数量仍会影响模型性能。最后，我们发现一致性评分与文本-图像间的模态差距存在相关性，这为跨模态不一致的MLLMs提供了机制性解释。",
    "url": "https://huggingface.co/papers/2512.08923",
    "arxiv_url": "https://arxiv.org/abs/2512.08923"
  }
]