[
  {
    "title": "TwinFlow: Realizing One-step Generation on Large Models with Self-adversarial Flows",
    "summary": "Recent advances in large multi-modal generative models have demonstrated impressive capabilities in multi-modal generation, including image and video generation. These models are typically built upon multi-step frameworks like diffusion and flow matching, which inherently limits their inference efficiency (requiring 40-100 Number of Function Evaluations (NFEs)). While various few-step methods aim to accelerate the inference, existing solutions have clear limitations. Prominent distillation-based methods, such as progressive and consistency distillation, either require an iterative distillation procedure or show significant degradation at very few steps (< 4-NFE). Meanwhile, integrating adversarial training into distillation (e.g., DMD/DMD2 and SANA-Sprint) to enhance performance introduces training instability, added complexity, and high GPU memory overhead due to the auxiliary trained models. To this end, we propose TwinFlow, a simple yet effective framework for training 1-step generative models that bypasses the need of fixed pretrained teacher models and avoids standard adversarial networks during training, making it ideal for building large-scale, efficient models. On text-to-image tasks, our method achieves a GenEval score of 0.83 in 1-NFE, outperforming strong baselines like SANA-Sprint (a GAN loss-based framework) and RCGM (a consistency-based framework). Notably, we demonstrate the scalability of TwinFlow by full-parameter training on Qwen-Image-20B and transform it into an efficient few-step generator. With just 1-NFE, our approach matches the performance of the original 100-NFE model on both the GenEval and DPG-Bench benchmarks, reducing computational cost by 100times with minor quality degradation. Project page is available at https://zhenglin-cheng.com/twinflow.",
    "translation": "标题：TwinFlow：基于自对抗流实现大模型单步生成\n\n摘要：近年来，大规模多模态生成模型在多模态生成（包括图像与视频生成）方面展现出卓越能力。这类模型通常基于扩散模型或流匹配等多步生成框架构建，其固有的多步推理过程限制了生成效率（通常需要40-100次函数评估）。尽管已有多种少步生成方法旨在加速推理，但现有方案存在明显局限：基于蒸馏的主流方法（如渐进蒸馏与一致性蒸馏）需要迭代蒸馏过程，或在极少数步数（<4步）下出现显著性能衰退；而将对抗训练引入蒸馏过程的方法（如DMD/DMD2和SANA-Sprint）虽能提升性能，却因引入额外训练模型导致训练不稳定、复杂度增加及高昂的GPU内存开销。为此，我们提出TwinFlow——一种简单而有效的单步生成模型训练框架。该框架无需依赖固定的预训练教师模型，在训练过程中避免了传统对抗网络的使用，特别适合构建大规模高效生成模型。在文生图任务中，本方法在单步生成条件下取得0.83的GenEval分数，显著优于SANA-Sprint（基于GAN损失的框架）和RCGM（基于一致性的框架）等强基线模型。值得注意的是，我们通过对Qwen-Image-20B模型进行全参数训练，验证了TwinFlow框架的可扩展性，并将其转化为高效少步生成器。实验表明：在仅使用单步生成的情况下，该方法在GenEval和DPG-Bench基准测试中的表现与原始百步生成模型相当，在几乎保持生成质量的同时将计算成本降低至百分之一。项目页面详见：https://zhenglin-cheng.com/twinflow。",
    "url": "https://huggingface.co/papers/2512.05150",
    "arxiv_url": "https://arxiv.org/abs/2512.05150"
  },
  {
    "title": "EditThinker: Unlocking Iterative Reasoning for Any Image Editor",
    "summary": "Instruction-based image editing has emerged as a prominent research area, which, benefiting from image generation foundation models, have achieved high aesthetic quality, making instruction-following capability the primary challenge. Existing approaches improve instruction adherence via supervised or reinforcement learning, yet single-turn success rates remain limited due to inherent stochasticity and a lack of deliberation. In this work, we propose a deliberative editing framework to 'think' while they edit, which simulates the human cognitive loop by iteratively executing a Think-while-Edit cycle: Critiquing results and Refining instructions , followed by Repeating the generation until satisfactory. Specifically, we train a single MLLM, EditThinker, to act as the reasoning engine of this framework, which jointly produce the critique score, reasoning process, and refined instructions. We employ reinforcement learning to align the EditThinker's thinking with its editing, thereby generating more targeted instruction improvements. Extensive experiments on four benchmarks demonstrate that our approach significantly improves the instruction-following capability of any image editing model by a large margin. We will release our data construction framework, datasets, and models to benefit the community.",
    "translation": "标题：EditThinker：为任意图像编辑器解锁迭代推理能力\n\n摘要：基于指令的图像编辑已成为一个重要研究领域，该方法受益于图像生成基础模型，已能实现较高的美学质量，使得指令跟随能力成为当前的主要挑战。现有方法通过监督学习或强化学习来提升指令遵循度，但由于内在的随机性及缺乏深思熟虑的过程，单轮编辑的成功率仍然有限。本研究提出了一种深思型编辑框架，使模型在编辑过程中能够“思考”，通过迭代执行“边编辑边思考”的认知循环来模拟人类认知过程：即批判生成结果并优化指令，随后重复生成直至获得满意输出。具体而言，我们训练了一个单一的多模态大语言模型——EditThinker，作为该框架的推理引擎，联合生成批判评分、推理过程及优化后的指令。我们采用强化学习方法将EditThinker的思考过程与其编辑行为对齐，从而产生更具针对性的指令改进。在四个基准数据集上的大量实验表明，我们的方法能够显著提升任意图像编辑模型的指令跟随能力，且提升幅度显著。我们将公开数据构建框架、数据集及模型，以促进相关领域的发展。",
    "url": "https://huggingface.co/papers/2512.05965",
    "arxiv_url": "https://arxiv.org/abs/2512.05965"
  },
  {
    "title": "From Imitation to Discrimination: Toward A Generalized Curriculum Advantage Mechanism Enhancing Cross-Domain Reasoning Tasks",
    "summary": "Reinforcement learning has emerged as a paradigm for post-training large language models, boosting their reasoning capabilities. Such approaches compute an advantage value for each sample, reflecting better or worse performance than expected, thereby yielding both positive and negative signals for training. However, the indiscriminate mixing of the two signals in existing methods, especially from the early stages, may lead to ambiguous guidance and limited gains. To address this issue, we propose **CAPO** (**C**urriculum **A**dvantage **P**olicy **O**ptimization), an adaptive curriculum mechanism based on advantage signals. The proposed mechanism bootstraps imitation learning with positive-only advantage samples to establish robust foundations, and subsequently introduces negative signals to cultivate discriminative capabilities, thereby improving generalization across complex scenarios. Compatible with diverse optimization methods including GRPO, PPO, RLOO, and Reinforce++, our method consistently achieves stable and significant improvements in mathematical reasoning tasks, and further generalizes effectively to multimodal Graphical User Interface (GUI) reasoning scenarios, establishing itself as a versatile and robust optimization framework.",
    "translation": "标题：从模仿到判别：一种增强跨域推理任务的广义课程优势机制\n\n摘要：强化学习已成为大语言模型后训练的一种范式，显著提升了其推理能力。此类方法为每个样本计算优势值，反映其表现优于或劣于预期的程度，从而为训练提供正向与负向信号。然而，现有方法中两种信号的混杂使用，尤其是在训练早期阶段，可能导致指导意义模糊且收益有限。为解决这一问题，我们提出 **CAPO**（**C**urriculum **A**dvantage **P**olicy **O**ptimization，课程优势策略优化），一种基于优势信号的自适应课程机制。该机制首先利用纯正向优势样本进行模仿学习，以建立稳健的基础，随后逐步引入负向信号以培养判别能力，从而提升模型在复杂场景中的泛化性能。本方法与包括GRPO、PPO、RLOO和Reinforce++在内的多种优化方法兼容，在数学推理任务中持续取得稳定且显著的性能提升，并进一步有效泛化至多模态图形用户界面（GUI）推理场景，成为一个通用且鲁棒的优化框架。",
    "url": "https://huggingface.co/papers/2512.02580",
    "arxiv_url": "https://arxiv.org/abs/2512.02580"
  },
  {
    "title": "EMMA: Efficient Multimodal Understanding, Generation, and Editing with a Unified Architecture",
    "summary": "We propose EMMA, an efficient and unified architecture for multimodal understanding, generation and editing. Specifically, EMMA primarily consists of 1) An efficient autoencoder with a 32x compression ratio, which significantly reduces the number of tokens required for generation. This also ensures the training balance between understanding and generation tasks by applying the same compression ratio to images. 2) Channel-wise concatenation instead of token-wise concatenation among visual understanding and generation tokens, which further reduces the visual tokens in unified architectures. 3) A shared-and-decoupled network that enables mutual improvements across tasks while meeting the task-specific modeling requirements. 4) A mixture-of-experts mechanism adopted for visual understanding encoder, which substantially improves perceptual capabilities with a few parameters increase. Extensive experiments have shown that EMMA-4B can significantly outperform state-of-the-art unified multimodal approaches (e.g., BAGEL-7B) in both efficiency and performance, while also achieving competitive results compared to recent multimodal understanding and generation experts (e.g., Qwen3-VL and Qwen-Image). We believe that EMMA lays a solid foundation for the future development of unified multimodal architectures.",
    "translation": "标题：EMMA：一种高效多模态理解、生成与编辑的统一架构\n\n摘要：本文提出EMMA，一种面向多模态理解、生成与编辑任务的高效统一架构。该架构的核心创新包括：1）设计具有32倍压缩率的高效自编码器，显著降低生成任务所需的标记数量，同时通过对图像施加相同压缩率确保理解与生成任务的训练平衡；2）在视觉理解与生成标记之间采用通道级拼接而非标记级拼接，进一步减少统一架构中的视觉标记数量；3）构建共享解耦网络，在满足任务特定建模需求的同时实现跨任务协同优化；4）在视觉理解编码器中引入专家混合机制，以少量参数增长显著提升感知能力。大量实验表明，EMMA-4B在效率与性能上均显著优于当前先进统一多模态方法（如BAGEL-7B），同时相较于近期专用多模态理解与生成模型（如Qwen3-VL与Qwen-Image）也展现出竞争优势。我们相信EMMA为未来统一多模态架构的发展奠定了坚实基础。",
    "url": "https://huggingface.co/papers/2512.04810",
    "arxiv_url": "https://arxiv.org/abs/2512.04810"
  },
  {
    "title": "PaCo-RL: Advancing Reinforcement Learning for Consistent Image Generation with Pairwise Reward Modeling",
    "summary": "Consistent image generation requires faithfully preserving identities, styles, and logical coherence across multiple images, which is essential for applications such as storytelling and character design. Supervised training approaches struggle with this task due to the lack of large-scale datasets capturing visual consistency and the complexity of modeling human perceptual preferences. In this paper, we argue that reinforcement learning (RL) offers a promising alternative by enabling models to learn complex and subjective visual criteria in a data-free manner. To achieve this, we introduce PaCo-RL, a comprehensive framework that combines a specialized consistency reward model with an efficient RL algorithm. The first component, PaCo-Reward, is a pairwise consistency evaluator trained on a large-scale dataset constructed via automated sub-figure pairing. It evaluates consistency through a generative, autoregressive scoring mechanism enhanced by task-aware instructions and CoT reasons. The second component, PaCo-GRPO, leverages a novel resolution-decoupled optimization strategy to substantially reduce RL cost, alongside a log-tamed multi-reward aggregation mechanism that ensures balanced and stable reward optimization. Extensive experiments across the two representative subtasks show that PaCo-Reward significantly improves alignment with human perceptions of visual consistency, and PaCo-GRPO achieves state-of-the-art consistency performance with improved training efficiency and stability. Together, these results highlight the promise of PaCo-RL as a practical and scalable solution for consistent image generation. The project page is available at https://x-gengroup.github.io/HomePage_PaCo-RL/.",
    "translation": "标题：PaCo-RL：通过成对奖励建模推进强化学习在一致性图像生成中的应用\n\n摘要：一致性图像生成要求在多幅图像中忠实保持身份、风格和逻辑连贯性，这对于故事叙述和角色设计等应用至关重要。由于缺乏捕捉视觉一致性的大规模数据集，以及建模人类感知偏好的复杂性，监督训练方法在此任务上面临困难。本文认为，强化学习通过使模型能够以无数据方式学习复杂且主观的视觉标准，提供了一种有前景的替代方案。为实现这一目标，我们提出了PaCo-RL，这是一个将专用一致性奖励模型与高效强化学习算法相结合的综合框架。其第一个组件PaCo-Reward是一种基于自动化子图配对构建的大规模数据集训练的成对一致性评估器，它通过生成式自回归评分机制进行评估，该机制通过任务感知指令和思维链推理得到增强。第二个组件PaCo-GRPO采用了一种新颖的解耦分辨率优化策略，显著降低了强化学习成本，同时结合了对数调制的多奖励聚合机制，确保了平衡稳定的奖励优化。在两个代表性子任务上的大量实验表明，PaCo-Reward显著提升了与人类视觉一致性感知的对齐程度，而PaCo-GRPO在提升训练效率和稳定性的同时，实现了最先进的一致性生成性能。这些结果共同凸显了PaCo-RL作为一种实用且可扩展的一致性图像生成解决方案的潜力。项目页面详见：https://x-gengroup.github.io/HomePage_PaCo-RL/。",
    "url": "https://huggingface.co/papers/2512.04784",
    "arxiv_url": "https://arxiv.org/abs/2512.04784"
  },
  {
    "title": "SCAIL: Towards Studio-Grade Character Animation via In-Context Learning of 3D-Consistent Pose Representations",
    "summary": "Achieving character animation that meets studio-grade production standards remains challenging despite recent progress. Existing approaches can transfer motion from a driving video to a reference image, but often fail to preserve structural fidelity and temporal consistency in wild scenarios involving complex motion and cross-identity animations. In this work, we present SCAIL (Studio-grade Character Animation via In-context Learning), a framework designed to address these challenges from two key innovations. First, we propose a novel 3D pose representation, providing a more robust and flexible motion signal. Second, we introduce a full-context pose injection mechanism within a diffusion-transformer architecture, enabling effective spatio-temporal reasoning over full motion sequences. To align with studio-level requirements, we develop a curated data pipeline ensuring both diversity and quality, and establish a comprehensive benchmark for systematic evaluation. Experiments show that SCAIL achieves state-of-the-art performance and advances character animation toward studio-grade reliability and realism.",
    "translation": "标题：SCAIL：通过三维一致性姿态表征的上下文学习实现影视级角色动画\n\n摘要：尽管近期取得进展，实现符合影视级制作标准的角色动画仍具挑战。现有方法可将驱动视频中的动作迁移至参考图像，但在涉及复杂运动与跨身份动画的开放场景中，常难以保持结构保真度与时间一致性。本研究提出SCAIL（基于上下文学习的影视级角色动画框架），该框架通过两项关键创新应对这些挑战：首先，我们提出一种新颖的三维姿态表征方法，提供更鲁棒灵活的运动信号；其次，我们在扩散-变换器架构中引入全上下文姿态注入机制，实现对完整运动序列的有效时空推理。为满足影视级标准，我们开发了兼顾多样性与质量的精选数据流程，并建立系统性评估的综合基准。实验表明，SCAIL实现了最先进的性能表现，将角色动画向影视级可靠性与真实感推进。",
    "url": "https://huggingface.co/papers/2512.05905",
    "arxiv_url": "https://arxiv.org/abs/2512.05905"
  },
  {
    "title": "Entropy Ratio Clipping as a Soft Global Constraint for Stable Reinforcement Learning",
    "summary": "Large language model post-training relies on reinforcement learning to improve model capability and alignment quality. However, the off-policy training paradigm introduces distribution shift, which often pushes the policy beyond the trust region, leading to training instabilities manifested as fluctuations in policy entropy and unstable gradients. Although PPO-Clip mitigates this issue through importance clipping, it still overlooks the global distributional shift of actions. To address these challenges, we propose using the entropy ratio between the current and previous policies as a new global metric that effectively quantifies the relative change in policy exploration throughout updates. Building on this metric, we introduce an Entropy Ratio Clipping (ERC) mechanism that imposes bidirectional constraints on the entropy ratio. This stabilizes policy updates at the global distribution level and compensates for the inability of PPO-clip to regulate probability shifts of un-sampled actions. We integrate ERC into both DAPO and GPPO reinforcement learning algorithms. Experiments across multiple benchmarks show that ERC consistently improves performance.",
    "translation": "标题：熵比裁剪作为一种软性全局约束用于稳定强化学习\n\n摘要：大语言模型的后训练依赖于强化学习来提升模型能力与对齐质量。然而，离策略训练范式会引入分布偏移，这往往使策略超出置信区域，导致训练不稳定，表现为策略熵的波动与梯度不稳定。尽管PPO-Clip通过重要性采样裁剪缓解了这一问题，但其仍忽视了动作的全局分布偏移。为应对这些挑战，我们提出使用当前策略与先前策略之间的熵比作为新的全局度量指标，该指标能有效量化策略在更新过程中探索行为的相对变化。基于此度量，我们引入了熵比裁剪机制，对熵比施加双向约束。这能在全局分布层面稳定策略更新，并弥补PPO-clip无法调节未采样动作概率偏移的不足。我们将ERC机制整合至DAPO与GPPO强化学习算法中。在多基准测试上的实验表明，ERC能持续提升算法性能。",
    "url": "https://huggingface.co/papers/2512.05591",
    "arxiv_url": "https://arxiv.org/abs/2512.05591"
  },
  {
    "title": "Joint 3D Geometry Reconstruction and Motion Generation for 4D Synthesis from a Single Image",
    "summary": "Generating interactive and dynamic 4D scenes from a single static image remains a core challenge. Most existing generate-then-reconstruct and reconstruct-then-generate methods decouple geometry from motion, causing spatiotemporal inconsistencies and poor generalization. To address these, we extend the reconstruct-then-generate framework to jointly perform Motion generation and geometric Reconstruction for 4D Synthesis (MoRe4D). We first introduce TrajScene-60K, a large-scale dataset of 60,000 video samples with dense point trajectories, addressing the scarcity of high-quality 4D scene data. Based on this, we propose a diffusion-based 4D Scene Trajectory Generator (4D-STraG) to jointly generate geometrically consistent and motion-plausible 4D point trajectories. To leverage single-view priors, we design a depth-guided motion normalization strategy and a motion-aware module for effective geometry and dynamics integration. We then propose a 4D View Synthesis Module (4D-ViSM) to render videos with arbitrary camera trajectories from 4D point track representations. Experiments show that MoRe4D generates high-quality 4D scenes with multi-view consistency and rich dynamic details from a single image. Code: https://github.com/Zhangyr2022/MoRe4D.",
    "translation": "标题：基于单张图像的4D合成：联合三维几何重建与运动生成\n\n摘要：从单张静态图像生成具有交互性与动态性的4D场景仍是核心挑战。现有\"先生成后重建\"与\"先重建后生成\"方法大多将几何结构与运动解耦，导致时空不一致性与泛化能力不足。为解决这些问题，本研究扩展\"先重建后生成\"框架，提出面向4D合成的运动生成与几何重建联合方法（MoRe4D）。我们首先构建TrajScene-60K大规模数据集，包含6万个具有密集点轨迹的视频样本，以缓解高质量4D场景数据稀缺问题。基于此，提出基于扩散模型的4D场景轨迹生成器（4D-STraG），能够联合生成几何一致且运动合理的4D点轨迹。为利用单视图先验信息，设计了深度引导的运动归一化策略与运动感知模块，实现几何与动态特征的有效融合。进一步提出4D视角合成模块（4D-ViSM），可从4D点轨迹表征渲染任意相机轨迹的视频。实验表明，MoRe4D能够从单张图像生成具有多视角一致性与丰富动态细节的高质量4D场景。代码地址：https://github.com/Zhangyr2022/MoRe4D。",
    "url": "https://huggingface.co/papers/2512.05044",
    "arxiv_url": "https://arxiv.org/abs/2512.05044"
  },
  {
    "title": "COOPER: A Unified Model for Cooperative Perception and Reasoning in Spatial Intelligence",
    "summary": "Visual Spatial Reasoning is crucial for enabling Multimodal Large Language Models (MLLMs) to understand object properties and spatial relationships, yet current models still struggle with 3D-aware reasoning. Existing approaches typically enhance either perception, by augmenting RGB inputs with auxiliary modalities such as depth and segmentation, or reasoning, by training on spatial VQA datasets and applying reinforcement learning, and thus treat these two aspects in isolation. In this work, we investigate whether a unified MLLM can develop an intrinsic ability to enhance spatial perception and, through adaptive interleaved reasoning, achieve stronger spatial intelligence. We propose COOPER, a unified MLLM that leverages depth and segmentation as auxiliary modalities and is trained in two stages to acquire auxiliary modality generation and adaptive, interleaved reasoning capabilities. COOPER achieves an average 6.91\\% improvement in spatial reasoning while maintaining general performance. Moreover, even a variant trained only for auxiliary modality generation attains a 7.92\\% gain on distance and size estimation, suggesting that learning to generate auxiliary modalities helps internalize spatial knowledge and strengthen spatial understanding.",
    "translation": "标题：COOPER：空间智能中协同感知与推理的统一模型\n\n摘要：视觉空间推理对于使多模态大语言模型理解物体属性与空间关系至关重要，然而现有模型在三维感知推理方面仍面临挑战。当前方法通常通过两种独立路径进行增强：一是在感知层面，通过为RGB输入添加深度与分割等辅助模态；二是在推理层面，通过空间视觉问答数据集训练并结合强化学习。本研究探讨统一的多模态大语言模型能否通过自适应交错推理机制，发展出增强空间感知的内在能力，从而实现更强大的空间智能。我们提出COOPER模型，该统一框架以深度与分割作为辅助模态，通过两阶段训练获得辅助模态生成能力与自适应交错推理能力。实验表明，COOPER在保持通用性能的同时，空间推理任务平均提升6.91%。值得注意的是，仅进行辅助模态生成训练的变体模型在距离与尺寸估计任务上亦获得7.92%的性能增益，这证实学习生成辅助模态有助于模型内化空间知识并强化空间理解能力。",
    "url": "https://huggingface.co/papers/2512.04563",
    "arxiv_url": "https://arxiv.org/abs/2512.04563"
  },
  {
    "title": "RealGen: Photorealistic Text-to-Image Generation via Detector-Guided Rewards",
    "summary": "With the continuous advancement of image generation technology, advanced models such as GPT-Image-1 and Qwen-Image have achieved remarkable text-to-image consistency and world knowledge However, these models still fall short in photorealistic image generation. Even on simple T2I tasks, they tend to produce \" fake\" images with distinct AI artifacts, often characterized by \"overly smooth skin\" and \"oily facial sheens\". To recapture the original goal of \"indistinguishable-from-reality\" generation, we propose RealGen, a photorealistic text-to-image framework. RealGen integrates an LLM component for prompt optimization and a diffusion model for realistic image generation. Inspired by adversarial generation, RealGen introduces a \"Detector Reward\" mechanism, which quantifies artifacts and assesses realism using both semantic-level and feature-level synthetic image detectors. We leverage this reward signal with the GRPO algorithm to optimize the entire generation pipeline, significantly enhancing image realism and detail. Furthermore, we propose RealBench, an automated evaluation benchmark employing Detector-Scoring and Arena-Scoring. It enables human-free photorealism assessment, yielding results that are more accurate and aligned with real user experience. Experiments demonstrate that RealGen significantly outperforms general models like GPT-Image-1 and Qwen-Image, as well as specialized photorealistic models like FLUX-Krea, in terms of realism, detail, and aesthetics. The code is available at https://github.com/yejy53/RealGen.",
    "translation": "标题：RealGen：基于检测器引导奖励的逼真文本到图像生成\n\n摘要：随着图像生成技术的持续进步，GPT-Image-1与Qwen-Image等先进模型已在文本-图像一致性与世界知识掌握方面取得显著成果。然而，这些模型在生成逼真图像方面仍存在不足。即使在简单的文本到图像任务中，它们也倾向于生成带有明显人工智能痕迹的“虚假”图像，常表现为“过度光滑的皮肤”与“油光满面的面部光泽”。为重新实现“以假乱真”的生成目标，我们提出RealGen——一个逼真的文本到图像生成框架。RealGen整合了用于提示词优化的大型语言模型组件与用于逼真图像生成的扩散模型。受对抗生成思想启发，RealGen引入“检测器奖励”机制，通过语义级与特征级的合成图像检测器量化人工痕迹并评估真实感。我们结合GRPO算法利用该奖励信号优化整个生成流程，显著提升了图像的真实感与细节表现。此外，我们提出RealBench自动化评估基准，采用检测器评分与竞技场评分机制，实现了无需人工参与的逼真度评估，其评估结果更精准且符合真实用户体验。实验表明，在真实感、细节与美学维度上，RealGen显著优于GPT-Image-1、Qwen-Image等通用模型以及FLUX-Krea等专业逼真生成模型。代码已开源：https://github.com/yejy53/RealGen。",
    "url": "https://huggingface.co/papers/2512.00473",
    "arxiv_url": "https://arxiv.org/abs/2512.00473"
  },
  {
    "title": "Self-Improving VLM Judges Without Human Annotations",
    "summary": "Effective judges of Vision-Language Models (VLMs) are crucial for model development. Current methods for training VLM judges mainly rely on large-scale human preference annotations. However, such an approach is costly, and the annotations easily become obsolete as models rapidly improve. In this work, we present a framework to self-train a VLM judge model without any human preference annotations, using only self-synthesized data. Our method is iterative and has three stages: (1) generate diverse multimodal instruction-response pairs at varying quality levels, (2) generate reasoning traces and judgments for each pair, removing the ones that do not match our expected quality levels, and (3) training on correct judge answers and their reasoning traces. We evaluate the resulting judge on Multimodal RewardBench and VL-RewardBench across domains: correctness, preference, reasoning, safety, and visual question-answering. Our method improves a Llama-3.2-11B multimodal judge from 0.38 to 0.51 in overall accuracy on VL-RewardBench, often outperforming much larger models including Llama-3.2-90B, GPT-4o, and Claude 3.5 Sonnet, with particularly strong gains in general, hallucination, and reasoning dimensions. The overall strength of these human-annotation-free results suggest the potential for a future self-judge that evolves alongside rapidly improving VLM capabilities.",
    "translation": "标题：无需人工标注的自改进视觉语言模型评判器\n\n摘要：有效的视觉语言模型评判器对于模型开发至关重要。当前训练VLM评判器的方法主要依赖于大规模人工偏好标注。然而，这种方法成本高昂，且随着模型快速迭代，标注数据极易过时。本研究提出一种无需任何人工偏好标注、仅使用自合成数据的VLM评判器自训练框架。该方法采用迭代式三阶段流程：（1）生成具有不同质量层次的多模态指令-响应对；（2）为每对数据生成推理轨迹与评判结果，并剔除不符合预期质量水平的数据；（3）基于正确的评判答案及其推理轨迹进行训练。我们在多模态奖励基准测试和VL奖励基准测试中，从正确性、偏好性、推理能力、安全性和视觉问答五个领域评估所得评判器。实验表明，该方法将Llama-3.2-11B多模态评判器在VL奖励基准测试中的总体准确率从0.38提升至0.51，在通用性、幻觉抑制和推理维度表现尤为突出，其性能常超越包括Llama-3.2-90B、GPT-4o和Claude 3.5 Sonnet在内的更大规模模型。这些无需人工标注的实验结果整体表明，未来有望构建出能随VLM能力快速进化而同步发展的自演进评判系统。",
    "url": "https://huggingface.co/papers/2512.05145",
    "arxiv_url": "https://arxiv.org/abs/2512.05145"
  },
  {
    "title": "World Models That Know When They Don't Know: Controllable Video Generation with Calibrated Uncertainty",
    "summary": "Recent advances in generative video models have led to significant breakthroughs in high-fidelity video synthesis, specifically in controllable video generation where the generated video is conditioned on text and action inputs, e.g., in instruction-guided video editing and world modeling in robotics. Despite these exceptional capabilities, controllable video models often hallucinate - generating future video frames that are misaligned with physical reality - which raises serious concerns in many tasks such as robot policy evaluation and planning. However, state-of-the-art video models lack the ability to assess and express their confidence, impeding hallucination mitigation. To rigorously address this challenge, we propose C3, an uncertainty quantification (UQ) method for training continuous-scale calibrated controllable video models for dense confidence estimation at the subpatch level, precisely localizing the uncertainty in each generated video frame. Our UQ method introduces three core innovations to empower video models to estimate their uncertainty. First, our method develops a novel framework that trains video models for correctness and calibration via strictly proper scoring rules. Second, we estimate the video model's uncertainty in latent space, avoiding training instability and prohibitive training costs associated with pixel-space approaches. Third, we map the dense latent-space uncertainty to interpretable pixel-level uncertainty in the RGB space for intuitive visualization, providing high-resolution uncertainty heatmaps that identify untrustworthy regions. Through extensive experiments on large-scale robot learning datasets (Bridge and DROID) and real-world evaluations, we demonstrate that our method not only provides calibrated uncertainty estimates within the training distribution, but also enables effective out-of-distribution detection.",
    "translation": "标题：自知其不知的世界模型：基于校准不确定性的可控视频生成\n\n摘要：生成式视频模型的最新进展推动了高保真视频合成领域的重大突破，特别是在可控视频生成方面——生成的视频能够以文本和动作输入为条件，例如在指令引导的视频编辑和机器人世界建模中。尽管具备卓越能力，可控视频模型常出现幻觉现象，即生成的未来视频帧与物理现实不符，这在机器人策略评估与规划等任务中引发严重关切。然而，现有先进视频模型缺乏评估和表达自身置信度的能力，阻碍了幻觉缓解的进程。为系统应对这一挑战，我们提出C3方法：一种不确定性量化技术，用于训练连续尺度校准的可控视频模型，实现亚区块级别的密集置信度估计，精准定位每帧生成视频中的不确定性区域。该方法通过三大核心创新赋能视频模型的不确定性估计：首先，构建基于严格恰当评分规则的新型训练框架，使视频模型同时优化正确性与校准度；其次，在潜在空间估计视频模型的不确定性，规避像素空间方法存在的训练不稳定性和过高计算成本；最后，将密集的潜在空间不确定性映射至RGB空间的可解释像素级不确定性，通过高分辨率不确定性热力图直观标识不可信区域，实现可视化分析。基于大规模机器人学习数据集（Bridge与DROID）的广泛实验及现实场景评估表明，该方法不仅能在训练分布内提供校准的不确定性估计，还能实现有效的分布外检测。",
    "url": "https://huggingface.co/papers/2512.05927",
    "arxiv_url": "https://arxiv.org/abs/2512.05927"
  },
  {
    "title": "SpaceControl: Introducing Test-Time Spatial Control to 3D Generative Modeling",
    "summary": "Generative methods for 3D assets have recently achieved remarkable progress, yet providing intuitive and precise control over the object geometry remains a key challenge. Existing approaches predominantly rely on text or image prompts, which often fall short in geometric specificity: language can be ambiguous, and images are cumbersome to edit. In this work, we introduce SpaceControl, a training-free test-time method for explicit spatial control of 3D generation. Our approach accepts a wide range of geometric inputs, from coarse primitives to detailed meshes, and integrates seamlessly with modern pre-trained generative models without requiring any additional training. A controllable parameter lets users trade off between geometric fidelity and output realism. Extensive quantitative evaluation and user studies demonstrate that SpaceControl outperforms both training-based and optimization-based baselines in geometric faithfulness while preserving high visual quality. Finally, we present an interactive user interface that enables online editing of superquadrics for direct conversion into textured 3D assets, facilitating practical deployment in creative workflows. Find our project page at https://spacecontrol3d.github.io/",
    "translation": "标题：SpaceControl：在三维生成建模中引入测试时空间控制\n\n摘要：三维资产的生成方法近年来取得了显著进展，但如何对物体几何形态提供直观且精确的控制仍是关键挑战。现有方法主要依赖文本或图像提示，这些方式在几何特异性上往往存在不足：语言描述可能具有模糊性，而图像编辑则过程繁琐。本研究提出SpaceControl，一种无需训练的测试时方法，用于实现三维生成的显式空间控制。我们的方法能够接受从粗糙几何基元到精细网格的多种几何输入，并可无缝集成于现代预训练生成模型，无需任何额外训练。通过可控参数，用户可在几何保真度与输出真实感之间进行权衡。大量定量评估与用户研究表明，SpaceControl在保持高视觉质量的同时，其几何忠实度优于基于训练和基于优化的基线方法。最后，我们开发了交互式用户界面，支持在线编辑超二次曲面并直接转换为带纹理的三维资产，为创意工作流程的实际应用提供便利。项目页面详见：https://spacecontrol3d.github.io/",
    "url": "https://huggingface.co/papers/2512.05343",
    "arxiv_url": "https://arxiv.org/abs/2512.05343"
  },
  {
    "title": "ReVSeg: Incentivizing the Reasoning Chain for Video Segmentation with Reinforcement Learning",
    "summary": "Reasoning-centric video object segmentation is an inherently complex task: the query often refers to dynamics, causality, and temporal interactions, rather than static appearances. Yet existing solutions generally collapse these factors into simplified reasoning with latent embeddings, rendering the reasoning chain opaque and essentially intractable. We therefore adopt an explicit decomposition perspective and introduce ReVSeg, which executes reasoning as sequential decisions in the native interface of pretrained vision language models (VLMs). Rather than folding all reasoning into a single-step prediction, ReVSeg executes three explicit operations -- semantics interpretation, temporal evidence selection, and spatial grounding -- aligning pretrained capabilities. We further employ reinforcement learning to optimize the multi-step reasoning chain, enabling the model to self-refine its decision quality from outcome-driven signals. Experimental results demonstrate that ReVSeg attains state-of-the-art performances on standard video object segmentation benchmarks and yields interpretable reasoning trajectories. Project page is available at https://clementine24.github.io/ReVSeg/ .",
    "translation": "标题：ReVSeg：基于强化学习的推理链激励视频分割方法\n\n摘要：以推理为核心的视频目标分割本质上是一项复杂任务：查询通常涉及动态变化、因果关系与时间交互，而非静态外观特征。然而现有解决方案往往将这些因素压缩为基于隐式嵌入的简化推理，导致推理链不透明且难以追溯。为此，我们采用显式分解视角提出ReVSeg模型，该模型在预训练视觉语言模型的原生交互界面中，将推理过程转化为序列化决策执行。区别于将全部推理折叠为单步预测的传统方法，ReVSeg通过语义解析、时序证据筛选和空间定位三个显式操作步骤，实现对预训练模型能力的系统性对齐。我们进一步采用强化学习优化多步推理链，使模型能够根据结果驱动信号自主优化决策质量。实验结果表明，ReVSeg在标准视频目标分割基准测试中达到最先进性能，并生成可解释的推理轨迹。项目页面详见：https://clementine24.github.io/ReVSeg/。",
    "url": "https://huggingface.co/papers/2512.02835",
    "arxiv_url": "https://arxiv.org/abs/2512.02835"
  },
  {
    "title": "AI & Human Co-Improvement for Safer Co-Superintelligence",
    "summary": "Self-improvement is a goal currently exciting the field of AI, but is fraught with danger, and may take time to fully achieve. We advocate that a more achievable and better goal for humanity is to maximize co-improvement: collaboration between human researchers and AIs to achieve co-superintelligence. That is, specifically targeting improving AI systems' ability to work with human researchers to conduct AI research together, from ideation to experimentation, in order to both accelerate AI research and to generally endow both AIs and humans with safer superintelligence through their symbiosis. Focusing on including human research improvement in the loop will both get us there faster, and more safely.",
    "translation": "标题：人工智能与人类协同进化以实现更安全的共同超级智能\n\n摘要：自我改进是当前人工智能领域备受关注的目标，但其过程充满风险，且可能需要较长时间才能完全实现。我们认为对人类而言，更可实现且更优的目标是实现协同进化的最大化：即人类研究者与人工智能系统通过协作达成共同超级智能。具体而言，应着力提升人工智能系统与人类研究者协同开展人工智能研究的能力——从构想到实验的全过程——从而既加速人工智能研究进展，又通过人机共生机制赋予双方更安全的超级智能。将人类研究能力的提升纳入这一循环体系，将使我们以更快、更安全的方式实现该目标。",
    "url": "https://huggingface.co/papers/2512.05356",
    "arxiv_url": "https://arxiv.org/abs/2512.05356"
  },
  {
    "title": "M3DR: Towards Universal Multilingual Multimodal Document Retrieval",
    "summary": "Multimodal document retrieval systems have shown strong progress in aligning visual and textual content for semantic search. However, most existing approaches remain heavily English-centric, limiting their effectiveness in multilingual contexts. In this work, we present M3DR (Multilingual Multimodal Document Retrieval), a framework designed to bridge this gap across languages, enabling applicability across diverse linguistic and cultural contexts. M3DR leverages synthetic multilingual document data and generalizes across different vision-language architectures and model sizes, enabling robust cross-lingual and cross-modal alignment. Using contrastive training, our models learn unified representations for text and document images that transfer effectively across languages. We validate this capability on 22 typologically diverse languages, demonstrating consistent performance and adaptability across linguistic and script variations. We further introduce a comprehensive benchmark that captures real-world multilingual scenarios, evaluating models under monolingual, multilingual, and mixed-language settings. M3DR generalizes across both single dense vector and ColBERT-style token-level multi-vector retrieval paradigms. Our models, NetraEmbed and ColNetraEmbed achieve state-of-the-art performance with ~150% relative improvements on cross-lingual retrieval.",
    "translation": "标题：M3DR：迈向通用多语言多模态文档检索\n\n摘要：多模态文档检索系统在对齐视觉与文本内容以进行语义搜索方面已取得显著进展。然而，现有方法大多仍以英语为中心，限制了其在多语言环境中的有效性。本研究提出M3DR（多语言多模态文档检索）框架，旨在跨越语言鸿沟，使其能够适用于多样化的语言及文化场景。M3DR利用合成多语言文档数据，可泛化至不同的视觉-语言架构与模型规模，从而实现鲁棒的跨语言与跨模态对齐。通过对比训练，我们的模型能够学习文本与文档图像的统一表征，并有效迁移至不同语言。我们在22种类型各异的语言上验证了这一能力，证明模型在不同语言及文字变体中均具有稳定的性能与适应性。此外，我们引入了一个涵盖真实多语言场景的综合基准，在单语、多语及混合语言设置下评估模型性能。M3DR可同时适用于单稠密向量与ColBERT风格的令牌级多向量检索范式。我们提出的NetraEmbed与ColNetraEmbed模型实现了最先进的性能，在跨语言检索任务上相对性能提升约150%。",
    "url": "https://huggingface.co/papers/2512.03514",
    "arxiv_url": "https://arxiv.org/abs/2512.03514"
  },
  {
    "title": "Active Video Perception: Iterative Evidence Seeking for Agentic Long Video Understanding",
    "summary": "Long video understanding (LVU) is challenging because answering real-world queries often depends on sparse, temporally dispersed cues buried in hours of mostly redundant and irrelevant content. While agentic pipelines improve video reasoning capabilities, prevailing frameworks rely on a query-agnostic captioner to perceive video information, which wastes computation on irrelevant content and blurs fine-grained temporal and spatial information. Motivated by active perception theory, we argue that LVU agents should actively decide what, when, and where to observe, and continuously assess whether the current observation is sufficient to answer the query. We present Active Video Perception (AVP), an evidence-seeking framework that treats the video as an interactive environment and acquires compact, queryrelevant evidence directly from pixels. Concretely, AVP runs an iterative plan-observe-reflect process with MLLM agents. In each round, a planner proposes targeted video interactions, an observer executes them to extract time-stamped evidence, and a reflector evaluates the sufficiency of the evidence for the query, either halting with an answer or triggering further observation. Across five LVU benchmarks, AVP achieves highest performance with significant improvements. Notably, AVP outperforms the best agentic method by 5.7% in average accuracy while only requires 18.4% inference time and 12.4% input tokens.",
    "translation": "标题：主动视频感知：面向智能体长视频理解的迭代式证据搜寻\n\n摘要：长视频理解（LVU）面临巨大挑战，因为回答现实世界中的查询往往依赖于隐藏在数小时冗余且无关内容中的稀疏、时间分散的线索。尽管智能体流程提升了视频推理能力，但主流框架依赖与查询无关的通用描述器来感知视频信息，这既浪费计算资源处理无关内容，又模糊了细粒度的时间与空间信息。受主动感知理论启发，我们认为LVU智能体应主动决定观察的内容、时机与位置，并持续评估当前观察是否足以回答查询。本文提出主动视频感知（AVP），这是一个将视频视为交互环境、直接从像素中获取紧凑且与查询相关证据的搜寻框架。具体而言，AVP通过多模态大语言模型智能体运行迭代式的“计划-观察-反思”流程：每一轮中，规划器提出有针对性的视频交互指令，观察器执行指令以提取带时间戳的证据，反思器则评估证据对回答查询的充分性，从而决定是终止流程并给出答案，还是触发进一步观察。在五个LVU基准测试中，AVP均取得了最高性能，且提升显著。值得注意的是，AVP在平均准确率上优于现有最佳智能体方法5.7%，同时仅需18.4%的推理时间和12.4%的输入令牌量。",
    "url": "https://huggingface.co/papers/2512.05774",
    "arxiv_url": "https://arxiv.org/abs/2512.05774"
  },
  {
    "title": "From Segments to Scenes: Temporal Understanding in Autonomous Driving via Vision-Language Model",
    "summary": "Temporal understanding in autonomous driving (AD) remains a significant challenge, even for recent state-of-the-art (SoTA) Vision-Language Models (VLMs). Prior work has introduced datasets and benchmarks aimed at improving temporal reasoning, but these have emphasized other video content, including sports, cooking, and movies. No existing benchmark focuses exclusively on the unique challenges of temporal understanding in ego-centric AD footage. To fill this gap, the Temporal Understanding in Autonomous Driving (TAD) benchmark is presented, which evaluates VLMs' ability to capture the dynamic relationships between actions in AD. TAD comprises nearly 6,000 question-answer (QA) pairs, spanning 7 human-designed tasks. In addition, an evaluation is performed that consists of 9 closed- and open-source generalist models as well as SoTA AD specialist models. When applied to TAD, current SoTA models demonstrated substandard accuracies, largely due to imperfect fine-grained motion understanding. To improve motion understanding and overall accuracy on TAD, two novel training-free solutions are proposed: Scene-CoT, that leverages Chain-of-Thought (CoT) and TCogMap, which incorporates an ego-centric temporal cognitive map. The proposed approaches are integrated with existing VLMs and improve average accuracy on TAD by up to 17.72%. By introducing TAD, benchmarking multiple SoTA models, and proposing effective enhancements, this work aims to catalyze future research on temporal understanding in AD. The benchmark and evaluation code are available at https://huggingface.co/datasets/vbdai/TAD{Hugging Face} and https://github.com/vbdi/tad_bench{Github}, respectively.",
    "translation": "标题：从片段到场景：基于视觉语言模型的自动驾驶时序理解研究\n\n摘要：自动驾驶中的时序理解仍然是重大挑战，即使对于当前最先进的视觉语言模型亦是如此。先前研究虽已引入旨在提升时序推理能力的数据集与基准测试，但其重点多集中于体育、烹饪、电影等其他视频内容领域，尚无专门针对以自我为中心视角的自动驾驶视频时序理解特有问题构建的基准测试。为填补这一空白，本研究提出自动驾驶时序理解基准测试，用于评估视觉语言模型捕捉自动驾驶场景中动作间动态关系的能力。该基准包含近6000组问答对，涵盖7项人工设计的任务。此外，我们对9个开源与闭源的通用模型以及最先进的自动驾驶专用模型进行了系统性评估。实验表明，当前最先进模型在该基准测试中表现欠佳，主要归因于其对细粒度运动理解存在不足。为提升运动理解能力及整体测试准确率，本文提出两种无需训练的创新解决方案：基于思维链的场景推理方法，以及融合自我中心时序认知地图的时序认知映射方法。将所提方法与现有视觉语言模型结合后，在自动驾驶时序理解基准上的平均准确率最高提升17.72%。通过构建该基准测试、评估多类先进模型并提出有效增强方法，本研究旨在推动自动驾驶时序理解领域的后续研究。基准测试数据与评估代码已分别发布于https://huggingface.co/datasets/vbdai/TAD与https://github.com/vbdi/tad_bench。",
    "url": "https://huggingface.co/papers/2512.05277",
    "arxiv_url": "https://arxiv.org/abs/2512.05277"
  },
  {
    "title": "ProPhy: Progressive Physical Alignment for Dynamic World Simulation",
    "summary": "Recent advances in video generation have shown remarkable potential for constructing world simulators. However, current models still struggle to produce physically consistent results, particularly when handling large-scale or complex dynamics. This limitation arises primarily because existing approaches respond isotropically to physical prompts and neglect the fine-grained alignment between generated content and localized physical cues. To address these challenges, we propose ProPhy, a Progressive Physical Alignment Framework that enables explicit physics-aware conditioning and anisotropic generation. ProPhy employs a two-stage Mixture-of-Physics-Experts (MoPE) mechanism for discriminative physical prior extraction, where Semantic Experts infer semantic-level physical principles from textual descriptions, and Refinement Experts capture token-level physical dynamics. This mechanism allows the model to learn fine-grained, physics-aware video representations that better reflect underlying physical laws. Furthermore, we introduce a physical alignment strategy that transfers the physical reasoning capabilities of vision-language models (VLMs) into the Refinement Experts, facilitating a more accurate representation of dynamic physical phenomena. Extensive experiments on physics-aware video generation benchmarks demonstrate that ProPhy produces more realistic, dynamic, and physically coherent results than existing state-of-the-art methods.",
    "translation": "标题：ProPhy：面向动态世界模拟的渐进式物理对齐框架\n\n摘要：视频生成领域的最新进展在构建世界模拟器方面展现出显著潜力。然而，当前模型在生成物理一致性结果方面仍存在困难，特别是在处理大规模或复杂动态场景时。这一局限主要源于现有方法对物理提示的响应呈各向同性，且忽视了生成内容与局部物理线索之间的细粒度对齐。为应对这些挑战，我们提出ProPhy——一种渐进式物理对齐框架，能够实现显式的物理感知条件化与各向异性生成。ProPhy采用两阶段物理专家混合机制进行判别式物理先验提取：语义专家从文本描述中推断语义级物理原理，细化专家则捕捉词元级物理动态。该机制使模型能够学习细粒度的物理感知视频表征，从而更好地反映底层物理规律。此外，我们提出一种物理对齐策略，将视觉语言模型的物理推理能力迁移至细化专家模块，以提升动态物理现象的表征精度。在物理感知视频生成基准上的大量实验表明，ProPhy相比现有先进方法能生成更真实、动态且物理连贯的结果。",
    "url": "https://huggingface.co/papers/2512.05564",
    "arxiv_url": "https://arxiv.org/abs/2512.05564"
  },
  {
    "title": "SQ-format: A Unified Sparse-Quantized Hardware-friendly Data Format for LLMs",
    "summary": "Post-training quantization (PTQ) plays a crucial role in the democratization of large language models (LLMs). However, existing low-bit quantization and sparsification techniques are difficult to balance accuracy and efficiency due to the limited hardware support. For example, W4A8 can only achieve the same peak TOPS as W8A8 whereas the GPU-supported sparse data format (2:4 semi-structure sparse) is seldomly adopted due to the loss of accuracy. To bridge this gap, in this paper, we propose the Sparse-Quantized Format (SQ-format), which is a unified data format for quantization and sparsification potentially easily supported by new hardware and existing GPUs. SQ-format makes use of the fact that sparse matrix can be accelerated in high-precision, and low-precision matrix multiplication can also be accelerated accordingly. As such, SQ-format is proposed to achieve Pareto improvement between performance and throughput. This format is particularly suitable for activations with outlier inequality status and makes their static compression possible. We show the state-of-the-art PTQ performance with SQ-format, propose the hardware required to support it, and further offer the design exploration and insights for the next-generation AI accelerators.",
    "translation": "标题：SQ-format：一种面向大语言模型的统一稀疏量化硬件友好数据格式\n\n摘要：训练后量化（PTQ）在大语言模型（LLM）的普及中起着至关重要的作用。然而，由于硬件支持有限，现有的低位量化和稀疏化技术难以在精度与效率之间取得平衡。例如，W4A8量化仅能实现与W8A8相当的峰值TOPS，而GPU支持的稀疏数据格式（如2:4半结构化稀疏）则因精度损失问题较少被采用。为弥合这一差距，本文提出稀疏量化格式（SQ-format），这是一种统一的量化与稀疏化数据格式，可被新型硬件及现有GPU潜在支持。SQ-format基于稀疏矩阵可在高精度下加速、而低精度矩阵乘法亦可相应加速的特性，旨在实现性能与吞吐量的帕累托改进。该格式特别适用于具有异常值非均衡分布的激活张量，并使其静态压缩成为可能。我们展示了SQ-format在训练后量化中的先进性能，提出了支持该格式所需的硬件架构，并进一步为下一代AI加速器提供了设计探索与前瞻洞见。",
    "url": "https://huggingface.co/papers/2512.05409",
    "arxiv_url": "https://arxiv.org/abs/2512.05409"
  },
  {
    "title": "TimesNet-Gen: Deep Learning-based Site Specific Strong Motion Generation",
    "summary": "Effective earthquake risk reduction relies on accurate site-specific evaluations. This requires models that can represent the influence of local site conditions on ground motion characteristics. In this context, data driven approaches that learn site controlled signatures from recorded ground motions offer a promising direction. We address strong ground motion generation from time-domain accelerometer records and introduce the TimesNet-Gen, a time-domain conditional generator. The approach uses a station specific latent bottleneck. We evaluate generation by comparing HVSR curves and fundamental site-frequency f_0 distributions between real and generated records per station, and summarize station specificity with a score based on the f_0 distribution confusion matrices. TimesNet-Gen achieves strong station-wise alignment and compares favorably with a spectrogram-based conditional VAE baseline for site-specific strong motion synthesis. Our codes are available via https://github.com/brsylmz23/TimesNet-Gen.",
    "translation": "标题：TimesNet-Gen：基于深度学习的场地特定强震动生成方法\n\n摘要：有效的地震风险降低依赖于准确的场地特定评估，这需要能够表征局部场地条件对地震动特征影响的模型。在此背景下，从记录的地震动中学习场地控制特征的数据驱动方法提供了一个有前景的研究方向。本文针对时域加速度计记录进行强震动生成，提出了TimesNet-Gen——一种时域条件生成器。该方法采用站点特定的潜在瓶颈结构进行评估。我们通过比较各站点真实记录与生成记录的HVSR曲线及场地基本频率f_0分布来评估生成效果，并基于f_0分布的混淆矩阵计算得分以量化站点特异性。TimesNet-Gen在站点层面展现出高度一致性，在场地特定强震动合成任务中优于基于频谱图的条件变分自编码器基线模型。相关代码已发布于https://github.com/brsylmz23/TimesNet-Gen。",
    "url": "https://huggingface.co/papers/2512.04694",
    "arxiv_url": "https://arxiv.org/abs/2512.04694"
  },
  {
    "title": "Colon-X: Advancing Intelligent Colonoscopy from Multimodal Understanding to Clinical Reasoning",
    "summary": "In this study, we present Colon-X, an open initiative aimed at advancing multimodal intelligence in colonoscopy. We begin by constructing ColonVQA, the most comprehensive multimodal dataset ever built for colonoscopy, featuring over 1.1M+ visual question answering entries across 76 clinical findings and 18 multimodal tasks. Beyond serving as a community-wide data foundation, we further investigate a critical yet underexplored transition in colonoscopy - evolving from multimodal understanding to clinical reasoning: (a) To capture the current landscape of multimodal understanding behaviors, we systematically assess the generalizability of 22 multimodal large language models and examine their reliability under human-induced perturbations. The results reveal that clinical outputs from leading MLLMs remain far from robust and trustworthy. (b) To narrow this gap, we further explore reasoning-centric intelligence tailored for colonoscopy. Specifically, we curate ColonReason, a clinically grounded reasoning dataset annotated through a multi-expert debating pipeline, and develop ColonR1, the first R1-styled model incorporating task-adaptive rewarding and gradient-stable optimization techniques. Under data-scarce conditions, our ColonR1 achieves 56.61% overall accuracy, outperforming supervised fine-tuning by 25.22%, and sets a new reasoning-enabled baseline for multimodal colonoscopy analysis. All data and model resources are publicly available at https://github.com/ai4colonoscopy/Colon-X.",
    "translation": "标题：Colon-X：从多模态理解到临床推理的智能结肠镜技术演进\n\n摘要：本研究提出Colon-X开放计划，旨在推动结肠镜多模态智能技术的发展。我们首先构建了ColonVQA——迄今为止最全面的结肠镜多模态数据集，涵盖76种临床发现和18项多模态任务，包含超过110万条视觉问答数据。该数据集不仅为学界提供基础数据资源，我们进一步深入研究了结肠镜领域关键但尚未充分探索的转型路径——从多模态理解向临床推理的演进：（a）为评估当前多模态理解能力的发展现状，我们系统测试了22个多模态大语言模型的泛化性能，并考察其在人工干预扰动下的可靠性。结果表明，当前主流MLLM模型的临床输出仍远未达到稳健可信的标准。（b）为弥合这一差距，我们进一步探索面向结肠镜的推理核心智能技术。具体而言，我们构建了基于临床实践的推理数据集ColonReason（通过多专家辩论流程完成标注），并开发了首款R1架构模型ColonR1——该模型融合任务自适应奖励机制与梯度稳定优化技术。在数据稀缺条件下，ColonR1实现了56.61%的综合准确率，较监督微调方法提升25.22%，为多模态结肠镜分析建立了全新的推理能力基准。所有数据与模型资源已通过https://github.com/ai4colonoscopy/Colon-X开源发布。",
    "url": "https://huggingface.co/papers/2512.03667",
    "arxiv_url": "https://arxiv.org/abs/2512.03667"
  },
  {
    "title": "From FLOPs to Footprints: The Resource Cost of Artificial Intelligence",
    "summary": "As computational demands continue to rise, assessing the environmental footprint of AI requires moving beyond energy and water consumption to include the material demands of specialized hardware. This study quantifies the material footprint of AI training by linking computational workloads to physical hardware needs. The elemental composition of the Nvidia A100 SXM 40 GB graphics processing unit (GPU) was analyzed using inductively coupled plasma optical emission spectroscopy, which identified 32 elements. The results show that AI hardware consists of about 90% heavy metals and only trace amounts of precious metals. The elements copper, iron, tin, silicon, and nickel dominate the GPU composition by mass. In a multi-step methodology, we integrate these measurements with computational throughput per GPU across varying lifespans, accounting for the computational requirements of training specific AI models at different training efficiency regimes. Scenario-based analyses reveal that, depending on Model FLOPs Utilization (MFU) and hardware lifespan, training GPT-4 requires between 1,174 and 8,800 A100 GPUs, corresponding to the extraction and eventual disposal of up to 7 tons of toxic elements. Combined software and hardware optimization strategies can reduce material demands: increasing MFU from 20% to 60% lowers GPU requirements by 67%, while extending lifespan from 1 to 3 years yields comparable savings; implementing both measures together reduces GPU needs by up to 93%. Our findings highlight that incremental performance gains, such as those observed between GPT-3.5 and GPT-4, come at disproportionately high material costs. The study underscores the necessity of incorporating material resource considerations into discussions of AI scalability, emphasizing that future progress in AI must align with principles of resource efficiency and environmental responsibility.",
    "translation": "标题：从浮点运算到资源足迹：人工智能的资源成本\n\n摘要：随着计算需求持续攀升，评估人工智能的环境足迹需要超越能耗与水耗，将专用硬件的材料需求纳入考量。本研究通过关联计算负载与物理硬件需求，量化了人工智能训练的材料足迹。采用电感耦合等离子体发射光谱法对英伟达A100 SXM 40 GB图形处理器（GPU）进行元素分析，共检测出32种元素。结果表明，AI硬件约90%由重金属构成，贵金属含量仅为痕量。按质量计，铜、铁、锡、硅和镍是GPU的主要组成元素。通过多步骤研究方法，我们将这些测量数据与不同使用周期内单GPU的计算吞吐量相结合，并纳入不同训练效率模式下特定AI模型的训练计算需求。基于情景的分析显示，根据模型浮点运算利用率（MFU）与硬件使用寿命，训练GPT-4需要1,174至8,800块A100 GPU，对应最高达7吨有毒元素的开采与最终废弃。软硬件协同优化策略可降低材料需求：将MFU从20%提升至60%可使GPU需求量减少67%，而将使用寿命从1年延长至3年可实现同等幅度的节约；同时实施这两项措施最多可降低93%的GPU需求。我们的研究结果表明，如GPT-3.5到GPT-4所体现的渐进式性能提升，伴随着不成比例的高昂材料成本。本研究强调必须将材料资源考量纳入人工智能可扩展性的讨论，并指出未来AI发展必须符合资源效率与环境责任原则。",
    "url": "https://huggingface.co/papers/2512.04142",
    "arxiv_url": "https://arxiv.org/abs/2512.04142"
  },
  {
    "title": "Taxonomy-Adaptive Moderation Model with Robust Guardrails for Large Language Models",
    "summary": "Large Language Models (LLMs) are typically aligned for safety during the post-training phase; however, they may still generate inappropriate outputs that could potentially pose risks to users. This challenge underscores the need for robust safeguards that operate across both model inputs and outputs. In this work, we introduce Roblox Guard 1.0, a state-of-the-art instruction fine-tuned LLM designed to enhance the safety of LLM systems through comprehensive input-output moderation, using a pipeline of LLMs to enhance moderation capability. Built on the Llama-3.1-8B-Instruct backbone, our model is instruction fine-tuned to generalize across previously unseen safety taxonomies and demonstrates strong performance on out-of-domain safety benchmarks. The instruction fine-tuning process uses a mix of synthetic and open-source safety datasets, augmented with chain-of-thought (CoT) rationales and input inversion to enhance contextual understanding and decision making. To support systematic evaluation, we also release RobloxGuard-Eval, a new benchmark featuring an extensible safety taxonomy to assess the effectiveness of LLM guardrails and moderation frameworks.",
    "translation": "标题：具有鲁棒护栏的适应性分类大语言模型内容审核系统\n\n摘要：大语言模型通常在训练后阶段进行安全性对齐，但仍可能生成不当输出，对用户构成潜在风险。这一挑战凸显了在模型输入和输出两端建立鲁棒安全防护机制的必要性。本研究提出Roblox Guard 1.0模型——一种基于指令微调的前沿大语言模型，通过构建多阶段大语言模型处理流程，实现输入输出的全面内容审核以增强系统安全性。该模型以Llama-3.1-8B-Instruct为基座，通过指令微调使其能够泛化至未见过的安全分类体系，并在领域外安全基准测试中表现出卓越性能。指令微调过程融合了合成与开源安全数据集，并采用思维链推理机制及输入反转技术以增强上下文理解与决策能力。为支持系统性评估，我们同步发布RobloxGuard-Eval基准测试集，该数据集具备可扩展的安全分类体系，专门用于评估大语言模型护栏机制与内容审核框架的有效性。",
    "url": "https://huggingface.co/papers/2512.05339",
    "arxiv_url": "https://arxiv.org/abs/2512.05339"
  }
]