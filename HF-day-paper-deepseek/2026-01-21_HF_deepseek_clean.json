[
  {
    "title": "Being-H0.5: Scaling Human-Centric Robot Learning for Cross-Embodiment Generalization",
    "summary": "We introduce Being-H0.5, a foundational Vision-Language-Action (VLA) model designed for robust cross-embodiment generalization across diverse robotic platforms. While existing VLAs often struggle with morphological heterogeneity and data scarcity, we propose a human-centric learning paradigm that treats human interaction traces as a universal \"mother tongue\" for physical interaction. To support this, we present UniHand-2.0, the largest embodied pre-training recipe to date, comprising over 35,000 hours of multimodal data across 30 distinct robotic embodiments. Our approach introduces a Unified Action Space that maps heterogeneous robot controls into semantically aligned slots, enabling low-resource robots to bootstrap skills from human data and high-resource platforms. Built upon this human-centric foundation, we design a unified sequential modeling and multi-task pre-training paradigm to bridge human demonstrations and robotic execution. Architecturally, Being-H0.5 utilizes a Mixture-of-Transformers design featuring a novel Mixture-of-Flow (MoF) framework to decouple shared motor primitives from specialized embodiment-specific experts. Finally, to make cross-embodiment policies stable in the real world, we introduce Manifold-Preserving Gating for robustness under sensory shift and Universal Async Chunking to universalize chunked control across embodiments with different latency and control profiles. We empirically demonstrate that Being-H0.5 achieves state-of-the-art results on simulated benchmarks, such as LIBERO (98.9%) and RoboCasa (53.9%), while also exhibiting strong cross-embodiment capabilities on five robotic platforms.",
    "translation": "标题：Being-H0.5：面向跨具身泛化的人本机器人学习规模化框架\n\n摘要：本文提出Being-H0.5，这是一个为跨多样机器人平台实现鲁棒跨具身泛化而设计的基础视觉-语言-动作模型。针对现有视觉-语言-动作模型常受形态异构性与数据稀缺性制约的问题，我们提出一种以人为中心的学习范式，将人类交互轨迹视为物理交互的通用“母语”。为此，我们构建了迄今最大规模的具身预训练方案UniHand-2.0，涵盖30种不同机器人具身形态的超过35,000小时多模态数据。该方法通过引入统一动作空间，将异构机器人控制映射至语义对齐的槽位，使低资源机器人能够从人类数据及高资源平台中引导技能学习。基于此以人为本的框架，我们设计了统一的序列建模与多任务预训练范式，以桥接人类示范与机器人执行。在架构层面，Being-H0.5采用混合Transformer设计，创新性地提出混合流框架，将共享运动基元与特定具身专家解耦。最后，为保障跨具身策略在现实环境中的稳定性，我们引入流形保持门控机制以增强感知偏移下的鲁棒性，并提出通用异步分块方法，使分块控制能适配不同延迟与控制特性的具身体系。实验表明，Being-H0.5在仿真基准测试（如LIBERO达到98.9%，RoboCasa达到53.9%）中取得最先进性能，同时在五种机器人平台上展现出强大的跨具身泛化能力。",
    "url": "https://huggingface.co/papers/2601.12993",
    "arxiv_url": "https://arxiv.org/abs/2601.12993"
  },
  {
    "title": "Advances and Frontiers of LLM-based Issue Resolution in Software Engineering: A Comprehensive Survey",
    "summary": "Issue resolution, a complex Software Engineering (SWE) task integral to real-world development, has emerged as a compelling challenge for artificial intelligence. The establishment of benchmarks like SWE-bench revealed this task as profoundly difficult for large language models, thereby significantly accelerating the evolution of autonomous coding agents. This paper presents a systematic survey of this emerging domain. We begin by examining data construction pipelines, covering automated collection and synthesis approaches. We then provide a comprehensive analysis of methodologies, spanning training-free frameworks with their modular components to training-based techniques, including supervised fine-tuning and reinforcement learning. Subsequently, we discuss critical analyses of data quality and agent behavior, alongside practical applications. Finally, we identify key challenges and outline promising directions for future research. An open-source repository is maintained at https://github.com/DeepSoftwareAnalytics/Awesome-Issue-Resolution to serve as a dynamic resource in this field.",
    "translation": "标题：基于大语言模型的软件工程问题解决进展与前沿：一项全面综述\n\n摘要：问题解决作为现实软件开发中一项复杂的软件工程任务，已成为人工智能领域备受关注的研究挑战。SWE-bench等基准测试的建立表明，该任务对大语言模型而言极具难度，从而显著加速了自主编码智能体的发展进程。本文对这一新兴领域进行了系统性综述。首先，我们考察数据构建流程，涵盖自动化收集与合成方法。其次，我们对方法论体系展开全面分析，包括基于模块化组件的免训练框架，以及基于监督微调与强化学习等训练技术的方法体系。随后，我们探讨数据质量与智能体行为的关键分析维度，并结合实际应用场景展开论述。最后，本文指出当前面临的核心挑战，并展望未来研究的潜在方向。本领域动态资源库持续维护于 https://github.com/DeepSoftwareAnalytics/Awesome-Issue-Resolution。",
    "url": "https://huggingface.co/papers/2601.11655",
    "arxiv_url": "https://arxiv.org/abs/2601.11655"
  },
  {
    "title": "Think3D: Thinking with Space for Spatial Reasoning",
    "summary": "Understanding and reasoning about the physical world requires spatial intelligence: the ability to interpret geometry, perspective, and spatial relations beyond 2D perception. While recent vision large models (VLMs) excel at visual understanding, they remain fundamentally 2D perceivers and struggle with genuine 3D reasoning. We introduce Think3D, a framework that enables VLM agents to think with 3D space. By leveraging 3D reconstruction models that recover point clouds and camera poses from images or videos, Think3D allows the agent to actively manipulate space through camera-based operations and ego/global-view switching, transforming spatial reasoning into an interactive 3D chain-of-thought process. Without additional training, Think3D significantly improves the spatial reasoning performance of advanced models such as GPT-4.1 and Gemini 2.5 Pro, yielding average gains of +7.8% on BLINK Multi-view and MindCube, and +4.7% on VSI-Bench. We further show that smaller models, which struggle with spatial exploration, benefit significantly from a reinforcement learning policy that enables the model to select informative viewpoints and operations. With RL, the benefit from tool usage increases from +0.7% to +6.8%. Our findings demonstrate that training-free, tool-augmented spatial exploration is a viable path toward more flexible and human-like 3D reasoning in multimodal agents, establishing a new dimension of multimodal intelligence. Code and weights are released at https://github.com/zhangzaibin/spagent.",
    "translation": "标题：Think3D：利用空间进行空间推理的思维框架\n\n摘要：理解和推理物理世界需要空间智能：即超越二维感知，能够解读几何、透视和空间关系的能力。尽管当前的视觉大模型在视觉理解方面表现出色，但其本质上仍是二维感知器，难以进行真正的三维推理。我们提出了Think3D框架，使视觉大模型代理能够利用三维空间进行思考。该框架通过利用从图像或视频中恢复点云和相机姿态的三维重建模型，使代理能够通过基于相机的操作以及自我/全局视角切换来主动操控空间，从而将空间推理转化为交互式的三维思维链过程。在无需额外训练的情况下，Think3D显著提升了如GPT-4.1和Gemini 2.5 Pro等先进模型的空间推理性能，在BLINK Multi-view和MindCube基准上平均提升+7.8%，在VSI-Bench上提升+4.7%。我们进一步发现，对于在空间探索方面存在困难的小型模型，通过强化学习策略使其能够选择信息丰富的视角和操作，可带来显著收益。借助强化学习，工具使用带来的性能提升从+0.7%增加至+6.8%。我们的研究结果表明，无需训练、工具增强的空间探索是实现多模态代理更灵活、更类人三维推理的可行路径，从而确立了多模态智能的一个新维度。代码和权重已发布于https://github.com/zhangzaibin/spagent。",
    "url": "https://huggingface.co/papers/2601.13029",
    "arxiv_url": "https://arxiv.org/abs/2601.13029"
  },
  {
    "title": "OmniTransfer: All-in-one Framework for Spatio-temporal Video Transfer",
    "summary": "Videos convey richer information than images or text, capturing both spatial and temporal dynamics. However, most existing video customization methods rely on reference images or task-specific temporal priors, failing to fully exploit the rich spatio-temporal information inherent in videos, thereby limiting flexibility and generalization in video generation. To address these limitations, we propose OmniTransfer, a unified framework for spatio-temporal video transfer. It leverages multi-view information across frames to enhance appearance consistency and exploits temporal cues to enable fine-grained temporal control. To unify various video transfer tasks, OmniTransfer incorporates three key designs: Task-aware Positional Bias that adaptively leverages reference video information to improve temporal alignment or appearance consistency; Reference-decoupled Causal Learning separating reference and target branches to enable precise reference transfer while improving efficiency; and Task-adaptive Multimodal Alignment using multimodal semantic guidance to dynamically distinguish and tackle different tasks. Extensive experiments show that OmniTransfer outperforms existing methods in appearance (ID and style) and temporal transfer (camera movement and video effects), while matching pose-guided methods in motion transfer without using pose, establishing a new paradigm for flexible, high-fidelity video generation.",
    "translation": "标题：OmniTransfer：面向时空视频迁移的一体化框架\n\n摘要：视频相比图像或文本能传达更丰富的信息，同时捕捉空间与时间动态。然而，现有视频定制方法大多依赖参考图像或特定任务的时间先验，未能充分利用视频固有的丰富时空信息，从而限制了视频生成的灵活性与泛化能力。为应对这些局限，本文提出OmniTransfer——一个统一的时空视频迁移框架。该框架通过利用跨帧的多视角信息以增强外观一致性，并挖掘时序线索以实现细粒度的时间控制。为统一各类视频迁移任务，OmniTransfer包含三项关键设计：任务感知位置偏置，可自适应利用参考视频信息以提升时序对齐或外观一致性；参考解耦因果学习，通过分离参考与目标分支实现精准参考迁移并提升效率；以及任务自适应多模态对齐，借助多模态语义引导动态区分并处理不同任务。大量实验表明，OmniTransfer在外观（身份与风格）迁移和时序（摄像机运动与视频特效）迁移任务上均优于现有方法，同时在不使用姿态信息的情况下达到与姿态引导方法相当的运动迁移效果，为灵活、高保真的视频生成建立了新范式。",
    "url": "https://huggingface.co/papers/2601.14250",
    "arxiv_url": "https://arxiv.org/abs/2601.14250"
  },
  {
    "title": "Toward Efficient Agents: Memory, Tool learning, and Planning",
    "summary": "Recent years have witnessed increasing interest in extending large language models into agentic systems. While the effectiveness of agents has continued to improve, efficiency, which is crucial for real-world deployment, has often been overlooked. This paper therefore investigates efficiency from three core components of agents: memory, tool learning, and planning, considering costs such as latency, tokens, steps, etc. Aimed at conducting comprehensive research addressing the efficiency of the agentic system itself, we review a broad range of recent approaches that differ in implementation yet frequently converge on shared high-level principles including but not limited to bounding context via compression and management, designing reinforcement learning rewards to minimize tool invocation, and employing controlled search mechanisms to enhance efficiency, which we discuss in detail. Accordingly, we characterize efficiency in two complementary ways: comparing effectiveness under a fixed cost budget, and comparing cost at a comparable level of effectiveness. This trade-off can also be viewed through the Pareto frontier between effectiveness and cost. From this perspective, we also examine efficiency oriented benchmarks by summarizing evaluation protocols for these components and consolidating commonly reported efficiency metrics from both benchmark and methodological studies. Moreover, we discuss the key challenges and future directions, with the goal of providing promising insights.",
    "translation": "标题：迈向高效智能体：记忆、工具学习与规划\n\n摘要：近年来，将大型语言模型扩展为智能体系统的研究日益受到关注。尽管智能体的性能持续提升，但对其实际部署至关重要的效率问题却常被忽视。本文从智能体的三个核心组成部分——记忆、工具学习与规划——出发，结合延迟、计算量、步骤数等成本因素，系统探讨效率优化问题。为全面研究智能体系统自身的效率，我们综述了近年来各类实现方式不同但常遵循共同高层原则的研究方法，包括但不限于：通过压缩与管理机制限制上下文范围、设计强化学习奖励函数以最小化工具调用、采用受控搜索机制提升效率等，并对这些方法展开详细讨论。基于此，我们从两个互补维度界定效率：在固定成本预算下比较性能表现，以及在相当性能水平下比较资源消耗。这种权衡关系亦可从性能与成本的帕累托前沿视角进行解读。基于该视角，我们通过总结各模块的评估方案、整合基准测试与方法研究中常用的效率指标，系统梳理了面向效率的评估体系。最后，本文讨论了当前面临的关键挑战与未来研究方向，旨在为相关领域提供前瞻性见解。",
    "url": "https://huggingface.co/papers/2601.14192",
    "arxiv_url": "https://arxiv.org/abs/2601.14192"
  },
  {
    "title": "FutureOmni: Evaluating Future Forecasting from Omni-Modal Context for Multimodal LLMs",
    "summary": "Although Multimodal Large Language Models (MLLMs) demonstrate strong omni-modal perception, their ability to forecast future events from audio-visual cues remains largely unexplored, as existing benchmarks focus mainly on retrospective understanding. To bridge this gap, we introduce FutureOmni, the first benchmark designed to evaluate omni-modal future forecasting from audio-visual environments. The evaluated models are required to perform cross-modal causal and temporal reasoning, as well as effectively leverage internal knowledge to predict future events. FutureOmni is constructed via a scalable LLM-assisted, human-in-the-loop pipeline and contains 919 videos and 1,034 multiple-choice QA pairs across 8 primary domains. Evaluations on 13 omni-modal and 7 video-only models show that current systems struggle with audio-visual future prediction, particularly in speech-heavy scenarios, with the best accuracy of 64.8% achieved by Gemini 3 Flash. To mitigate this limitation, we curate a 7K-sample instruction-tuning dataset and propose an Omni-Modal Future Forecasting (OFF) training strategy. Evaluations on FutureOmni and popular audio-visual and video-only benchmarks demonstrate that OFF enhances future forecasting and generalization. We publicly release all code (https://github.com/OpenMOSS/FutureOmni) and datasets (https://huggingface.co/datasets/OpenMOSS-Team/FutureOmni).",
    "translation": "标题：FutureOmni：评估多模态大语言模型基于全模态上下文的未来预测能力\n\n摘要：尽管多模态大语言模型（MLLMs）展现出强大的全模态感知能力，但其基于视听线索预测未来事件的能力仍鲜有探索，现有基准主要集中于回顾性理解。为填补这一空白，我们提出了FutureOmni——首个用于评估基于视听环境进行全模态未来预测的基准。该基准要求被评估模型能够执行跨模态的因果与时序推理，并有效利用内部知识来预测未来事件。FutureOmni通过可扩展的大语言模型辅助、人机协同流程构建而成，涵盖8个主要领域，包含919个视频和1,034个多项选择题对。对13个全模态模型和7个纯视频模型的评估表明，当前系统在视听未来预测任务上表现欠佳，尤其在语音密集型场景中，最佳准确率（由Gemini 3 Flash实现）仅为64.8%。为改善这一局限，我们构建了一个包含7千样本的指令微调数据集，并提出了一种全模态未来预测训练策略。在FutureOmni及主流视听与纯视频基准上的评估表明，该策略有效提升了未来预测能力与泛化性能。我们已公开所有代码（https://github.com/OpenMOSS/FutureOmni）与数据集（https://huggingface.co/datasets/OpenMOSS-Team/FutureOmni）。",
    "url": "https://huggingface.co/papers/2601.13836",
    "arxiv_url": "https://arxiv.org/abs/2601.13836"
  },
  {
    "title": "MemoryRewardBench: Benchmarking Reward Models for Long-Term Memory Management in Large Language Models",
    "summary": "Existing works increasingly adopt memory-centric mechanisms to process long contexts in a segment manner, and effective memory management is one of the key capabilities that enables large language models to effectively propagate information across the entire sequence. Therefore, leveraging reward models (RMs) to automatically and reliably evaluate memory quality is critical. In this work, we introduce MemoryRewardBench, the first benchmark to systematically study the ability of RMs to evaluate long-term memory management processes. MemoryRewardBench covers both long-context comprehension and long-form generation tasks, featuring 10 distinct settings with different memory management patterns, with context length ranging from 8K to 128K tokens. Evaluations on 13 cutting-edge RMs indicate a diminishing performance gap between open-source and proprietary models, with newer-generation models consistently outperforming their predecessors regardless of parameter count. We further expose the capabilities and fundamental limitations of current RMs in evaluating LLM memory management across diverse settings.",
    "translation": "标题：MemoryRewardBench：面向大语言模型长时记忆管理的奖励模型基准测试\n\n摘要：现有研究越来越多地采用以记忆为核心的机制来分段处理长上下文，而有效的记忆管理是大语言模型在整个序列中有效传递信息的关键能力之一。因此，利用奖励模型来自动、可靠地评估记忆质量至关重要。本研究提出了MemoryRewardBench，这是首个系统研究奖励模型评估长时记忆管理过程的基准测试。MemoryRewardBench涵盖长上下文理解与长文本生成任务，包含10种具有不同记忆管理模式的场景，上下文长度范围从8K到128K词元。对13个前沿奖励模型的评估表明，开源模型与专有模型之间的性能差距正在缩小，且新一代模型无论参数量大小均持续优于前代模型。我们进一步揭示了当前奖励模型在评估大语言模型跨多样化场景的记忆管理能力方面的优势与根本局限。",
    "url": "https://huggingface.co/papers/2601.11969",
    "arxiv_url": "https://arxiv.org/abs/2601.11969"
  },
  {
    "title": "Locate, Steer, and Improve: A Practical Survey of Actionable Mechanistic Interpretability in Large Language Models",
    "summary": "Mechanistic Interpretability (MI) has emerged as a vital approach to demystify the opaque decision-making of Large Language Models (LLMs). However, existing reviews primarily treat MI as an observational science, summarizing analytical insights while lacking a systematic framework for actionable intervention. To bridge this gap, we present a practical survey structured around the pipeline: \"Locate, Steer, and Improve.\" We formally categorize Localizing (diagnosis) and Steering (intervention) methods based on specific Interpretable Objects to establish a rigorous intervention protocol. Furthermore, we demonstrate how this framework enables tangible improvements in Alignment, Capability, and Efficiency, effectively operationalizing MI as an actionable methodology for model optimization. The curated paper list of this work is available at https://github.com/rattlesnakey/Awesome-Actionable-MI-Survey.",
    "translation": "标题：定位、引导与改进：大语言模型中可操作的机制可解释性实用综述\n\n摘要：机制可解释性已成为揭示大语言模型不透明决策过程的关键方法。然而，现有综述多将机制可解释性视为观测性科学，主要聚焦于分析性见解的总结，缺乏系统化的可操作干预框架。为弥补这一空白，本文提出以“定位、引导与改进”为流程结构的实用综述框架。我们基于特定可解释对象，对定位（诊断）与引导（干预）方法进行形式化分类，以建立严谨的干预规范。进一步地，我们论证了该框架如何在对齐性、能力与效率三个维度实现实质性改进，从而将机制可解释性有效转化为可操作的模型优化方法论。本工作的精选文献列表发布于：https://github.com/rattlesnakey/Awesome-Actionable-MI-Survey。",
    "url": "https://huggingface.co/papers/2601.14004",
    "arxiv_url": "https://arxiv.org/abs/2601.14004"
  },
  {
    "title": "UniX: Unifying Autoregression and Diffusion for Chest X-Ray Understanding and Generation",
    "summary": "Despite recent progress, medical foundation models still struggle to unify visual understanding and generation, as these tasks have inherently conflicting goals: semantic abstraction versus pixel-level reconstruction. Existing approaches, typically based on parameter-shared autoregressive architectures, frequently lead to compromised performance in one or both tasks. To address this, we present UniX, a next-generation unified medical foundation model for chest X-ray understanding and generation. UniX decouples the two tasks into an autoregressive branch for understanding and a diffusion branch for high-fidelity generation. Crucially, a cross-modal self-attention mechanism is introduced to dynamically guide the generation process with understanding features. Coupled with a rigorous data cleaning pipeline and a multi-stage training strategy, this architecture enables synergistic collaboration between tasks while leveraging the strengths of diffusion models for superior generation. On two representative benchmarks, UniX achieves a 46.1% improvement in understanding performance (Micro-F1) and a 24.2% gain in generation quality (FD-RadDino), using only a quarter of the parameters of LLM-CXR. By achieving performance on par with task-specific models, our work establishes a scalable paradigm for synergistic medical image understanding and generation. Codes and models are available at https://github.com/ZrH42/UniX.",
    "translation": "标题：UniX：统一自回归与扩散模型用于胸部X光影像理解与生成\n\n摘要：尽管近期取得进展，医学基础模型在统一视觉理解与生成任务方面仍面临挑战，因为这两类任务存在固有目标冲突：语义抽象与像素级重建。现有方法通常基于参数共享的自回归架构，往往导致一项或两项任务性能受损。为此，我们提出UniX——新一代面向胸部X光影像理解与生成的统一医学基础模型。UniX将两项任务解耦为理解任务的自回归分支与高保真生成任务的扩散分支，关键之处在于引入跨模态自注意力机制，通过理解特征动态引导生成过程。结合严格的数据清洗流程与多阶段训练策略，该架构在充分发挥扩散模型生成优势的同时，实现了任务间的协同协作。在两个代表性基准测试中，UniX仅使用LLM-CXR四分之一参数量，即在理解性能（Micro-F1）上提升46.1%，在生成质量（FD-RadDino）上提升24.2%。通过与专用模型相媲美的性能表现，本研究为协同医学影像理解与生成建立了可扩展的范式。代码与模型已发布于https://github.com/ZrH42/UniX。",
    "url": "https://huggingface.co/papers/2601.11522",
    "arxiv_url": "https://arxiv.org/abs/2601.11522"
  },
  {
    "title": "ToolPRMBench: Evaluating and Advancing Process Reward Models for Tool-using Agents",
    "summary": "Reward-guided search methods have demonstrated strong potential in enhancing tool-using agents by effectively guiding sampling and exploration over complex action spaces. As a core design, those search methods utilize process reward models (PRMs) to provide step-level rewards, enabling more fine-grained monitoring. However, there is a lack of systematic and reliable evaluation benchmarks for PRMs in tool-using settings. In this paper, we introduce ToolPRMBench, a large-scale benchmark specifically designed to evaluate PRMs for tool-using agents. ToolPRMBench is built on top of several representative tool-using benchmarks and converts agent trajectories into step-level test cases. Each case contains the interaction history, a correct action, a plausible but incorrect alternative, and relevant tool metadata. We respectively utilize offline sampling to isolate local single-step errors and online sampling to capture realistic multi-step failures from full agent rollouts. A multi-LLM verification pipeline is proposed to reduce label noise and ensure data quality. We conduct extensive experiments across large language models, general PRMs, and tool-specialized PRMs on ToolPRMBench. The results reveal clear differences in PRM effectiveness and highlight the potential of specialized PRMs for tool-using. Code and data will be released at https://github.com/David-Li0406/ToolPRMBench.",
    "translation": "标题：ToolPRMBench：面向工具使用智能体的过程奖励模型评估与改进\n\n摘要：奖励引导的搜索方法通过有效指导在复杂动作空间中的采样与探索，已展现出增强工具使用智能体的强大潜力。这些搜索方法以过程奖励模型为核心设计，通过提供步骤级奖励实现更细粒度的监控。然而，当前在工具使用场景中仍缺乏系统且可靠的过程奖励模型评估基准。本文提出了ToolPRMBench，这是一个专门用于评估工具使用智能体过程奖励模型的大规模基准。该基准基于多个代表性工具使用基准构建，并将智能体轨迹转化为步骤级测试用例。每个案例包含交互历史、正确动作、合理但错误的替代动作以及相关工具元数据。我们分别采用离线采样以隔离局部单步错误，并通过在线采样从完整智能体推演中捕捉真实的多步失败情况。为降低标注噪声并确保数据质量，本文提出了一个多大型语言模型验证流程。我们在ToolPRMBench上对大型语言模型、通用过程奖励模型和工具专用过程奖励模型进行了广泛实验。结果表明不同过程奖励模型效能存在显著差异，并凸显了专用过程奖励模型在工具使用场景中的潜力。代码与数据将在https://github.com/David-Li0406/ToolPRMBench发布。",
    "url": "https://huggingface.co/papers/2601.12294",
    "arxiv_url": "https://arxiv.org/abs/2601.12294"
  },
  {
    "title": "DARC: Decoupled Asymmetric Reasoning Curriculum for LLM Evolution",
    "summary": "Self-play with large language models has emerged as a promising paradigm for achieving self-improving artificial intelligence. However, existing self-play frameworks often suffer from optimization instability, due to (i) non-stationary objectives induced by solver-dependent reward feedback for the Questioner, and (ii) bootstrapping errors from self-generated pseudo-labels used to supervise the Solver. To mitigate these challenges, we introduce DARC (Decoupled Asymmetric Reasoning Curriculum), a two-stage framework that stabilizes the self-evolution process. First, we train the Questioner to synthesize difficulty-calibrated questions, conditioned on explicit difficulty levels and external corpora. Second, we train the Solver with an asymmetric self-distillation mechanism, where a document-augmented teacher generates high-quality pseudo-labels to supervise the student Solver that lacks document access. Empirical results demonstrate that DARC is model-agnostic, yielding an average improvement of 10.9 points across nine reasoning benchmarks and three backbone models. Moreover, DARC consistently outperforms all baselines and approaches the performance of fully supervised models without relying on human annotations.The code is available at https://github.com/RUCBM/DARC.",
    "translation": "标题：DARC：面向大语言模型进化的解耦非对称推理课程框架\n\n摘要：基于大语言模型的自我博弈已成为实现自我改进人工智能的重要范式。然而，现有自我博弈框架常因以下问题面临优化不稳定性：（1）提问者依赖求解器反馈的奖励目标具有非平稳性；（2）求解器训练时采用自生成伪标签会引入自举误差。为应对这些挑战，我们提出DARC（解耦非对称推理课程框架），该两阶段框架能有效稳定自我进化过程。第一阶段，我们训练提问者根据显式难度分级和外部语料库生成难度校准的问题。第二阶段，我们通过非对称自蒸馏机制训练求解器：具备文档增强能力的教师模型生成高质量伪标签，用以监督无法访问文档的学生求解器。实验结果表明，DARC具有模型无关性，在九项推理基准测试和三种骨干模型上平均提升10.9个性能点。此外，DARC在无需人工标注的情况下持续超越所有基线模型，性能接近全监督模型水平。代码已开源：https://github.com/RUCBM/DARC。",
    "url": "https://huggingface.co/papers/2601.13761",
    "arxiv_url": "https://arxiv.org/abs/2601.13761"
  },
  {
    "title": "Aligning Agentic World Models via Knowledgeable Experience Learning",
    "summary": "Current Large Language Models (LLMs) exhibit a critical modal disconnect: they possess vast semantic knowledge but lack the procedural grounding to respect the immutable laws of the physical world. Consequently, while these agents implicitly function as world models, their simulations often suffer from physical hallucinations-generating plans that are logically sound but physically unexecutable. Existing alignment strategies predominantly rely on resource-intensive training or fine-tuning, which attempt to compress dynamic environmental rules into static model parameters. However, such parametric encapsulation is inherently rigid, struggling to adapt to the open-ended variability of physical dynamics without continuous, costly retraining. To bridge this gap, we introduce WorldMind, a framework that autonomously constructs a symbolic World Knowledge Repository by synthesizing environmental feedback. Specifically, it unifies Process Experience to enforce physical feasibility via prediction errors and Goal Experience to guide task optimality through successful trajectories. Experiments on EB-ALFRED and EB-Habitat demonstrate that WorldMind achieves superior performance compared to baselines with remarkable cross-model and cross-environment transferability.",
    "translation": "标题：基于知识化经验学习的具身世界模型对齐\n\n摘要：当前大语言模型存在关键模态割裂问题：它们拥有海量语义知识，却缺乏遵循物理世界恒定法则的程序性基础。这导致这些智能体虽隐式发挥着世界模型的功能，其模拟过程常出现物理幻觉——生成逻辑合理但物理上不可执行的计划。现有对齐策略主要依赖资源密集的训练或微调，试图将动态环境规则压缩为静态模型参数。然而这种参数化封装本质上是僵化的，难以适应物理动态的开放可变性，且需持续投入高昂的再训练成本。为弥合这一鸿沟，我们提出WorldMind框架，通过综合环境反馈自主构建符号化世界知识库。具体而言，该框架统一了两种经验机制：通过预测误差强化物理可行性的过程经验，以及借助成功轨迹指导任务最优性的目标经验。在EB-ALFRED和EB-Habitat平台上的实验表明，WorldMind在保持卓越跨模型、跨环境迁移能力的同时，实现了优于基线模型的性能表现。",
    "url": "https://huggingface.co/papers/2601.13247",
    "arxiv_url": "https://arxiv.org/abs/2601.13247"
  },
  {
    "title": "Agentic-R: Learning to Retrieve for Agentic Search",
    "summary": "Agentic search has recently emerged as a powerful paradigm, where an agent interleaves multi-step reasoning with on-demand retrieval to solve complex questions. Despite its success, how to design a retriever for agentic search remains largely underexplored. Existing search agents typically rely on similarity-based retrievers, while similar passages are not always useful for final answer generation. In this paper, we propose a novel retriever training framework tailored for agentic search. Unlike retrievers designed for single-turn retrieval-augmented generation (RAG) that only rely on local passage utility, we propose to use both local query-passage relevance and global answer correctness to measure passage utility in a multi-turn agentic search. We further introduce an iterative training strategy, where the search agent and the retriever are optimized bidirectionally and iteratively. Different from RAG retrievers that are only trained once with fixed questions, our retriever is continuously improved using evolving and higher-quality queries from the agent. Extensive experiments on seven single-hop and multi-hop QA benchmarks demonstrate that our retriever, termed , consistently outperforms strong baselines across different search agents. Our codes are available at: https://github.com/8421BCD/Agentic-R.",
    "translation": "标题：Agentic-R：面向智能体搜索的检索学习\n\n摘要：智能体搜索作为一种新兴的强大范式，其核心在于智能体通过多步推理与按需检索的交替执行来解决复杂问题。尽管该范式已取得显著成效，但如何为其设计专用检索器仍缺乏深入探索。现有搜索智能体通常依赖基于相似度的检索器，然而相似文本片段并不总能有效支撑最终答案的生成。本文提出一种专为智能体搜索设计的新型检索器训练框架。与面向单轮检索增强生成（RAG）且仅依赖局部片段效用的检索器不同，本框架提出在多轮智能体搜索中综合使用局部查询-片段相关性与全局答案正确性来评估片段效用。我们进一步引入迭代训练策略，实现搜索智能体与检索器的双向迭代优化。相较于仅通过固定问题单次训练的RAG检索器，本方法能持续利用智能体生成的动态演进且更高质量的查询来改进检索器。在七个单跳与多跳问答基准上的大量实验表明，我们提出的检索器（命名为Agentic-R）在不同搜索智能体中均稳定优于现有强基线模型。代码已开源：https://github.com/8421BCD/Agentic-R。",
    "url": "https://huggingface.co/papers/2601.11888",
    "arxiv_url": "https://arxiv.org/abs/2601.11888"
  },
  {
    "title": "A BERTology View of LLM Orchestrations: Token- and Layer-Selective Probes for Efficient Single-Pass Classification",
    "summary": "Production LLM systems often rely on separate models for safety and other classification-heavy steps, increasing latency, VRAM footprint, and operational complexity. We instead reuse computation already paid for by the serving LLM: we train lightweight probes on its hidden states and predict labels in the same forward pass used for generation. We frame classification as representation selection over the full token-layer hidden-state tensor, rather than committing to a fixed token or fixed layer (e.g., first-token logits or final-layer pooling). To implement this, we introduce a two-stage aggregator that (i) summarizes tokens within each layer and (ii) aggregates across layer summaries to form a single representation for classification. We instantiate this template with direct pooling, a 100K-parameter scoring-attention gate, and a downcast multi-head self-attention (MHA) probe with up to 35M trainable parameters. Across safety and sentiment benchmarks our probes improve over logit-only reuse (e.g., MULI) and are competitive with substantially larger task-specific baselines, while preserving near-serving latency and avoiding the VRAM and latency costs of a separate guard-model pipeline.",
    "translation": "标题：LLM编排的BERTology视角：面向高效单次分类的令牌与层级选择性探针\n\n摘要：生产级大型语言模型系统通常依赖独立模型处理安全性及其他分类密集型任务，这会导致延迟增加、显存占用扩大及操作复杂性上升。我们提出复用服务LLM已完成的计算：在其隐藏状态上训练轻量级探针，并在生成所用的同一次前向传播中完成标签预测。我们将分类任务重新定义为对完整令牌-层级隐藏状态张量的表征选择，而非固定采用特定令牌或特定层级（如首令牌逻辑值或最终层池化）。为实现这一目标，我们设计了一种两阶段聚合器：（1）在各层级内部汇总令牌信息；（2）跨层级聚合摘要以形成单一分类表征。我们通过三种方式实例化该框架：直接池化法、10万参数规模的评分注意力门控机制，以及最多包含3500万可训练参数的降维多头自注意力探针。在安全性与情感分析基准测试中，我们的探针方法相较于仅复用逻辑值的方案（如MULI）表现更优，并与参数量显著更大的任务专用基线模型性能相当，同时保持了接近服务部署的推理速度，避免了独立防护模型管道带来的显存与延迟开销。",
    "url": "https://huggingface.co/papers/2601.13288",
    "arxiv_url": "https://arxiv.org/abs/2601.13288"
  },
  {
    "title": "KAGE-Bench: Fast Known-Axis Visual Generalization Evaluation for Reinforcement Learning",
    "summary": "Pixel-based reinforcement learning agents often fail under purely visual distribution shift even when latent dynamics and rewards are unchanged, but existing benchmarks entangle multiple sources of shift and hinder systematic analysis. We introduce KAGE-Env, a JAX-native 2D platformer that factorizes the observation process into independently controllable visual axes while keeping the underlying control problem fixed. By construction, varying a visual axis affects performance only through the induced state-conditional action distribution of a pixel policy, providing a clean abstraction for visual generalization. Building on this environment, we define KAGE-Bench, a benchmark of six known-axis suites comprising 34 train-evaluation configuration pairs that isolate individual visual shifts. Using a standard PPO-CNN baseline, we observe strong axis-dependent failures, with background and photometric shifts often collapsing success, while agent-appearance shifts are comparatively benign. Several shifts preserve forward motion while breaking task completion, showing that return alone can obscure generalization failures. Finally, the fully vectorized JAX implementation enables up to 33M environment steps per second on a single GPU, enabling fast and reproducible sweeps over visual factors. Code: https://avanturist322.github.io/KAGEBench/.",
    "translation": "标题：KAGE-Bench：面向强化学习的快速已知视觉轴泛化评估基准\n\n摘要：基于像素的强化学习智能体即使在潜在动态与奖励函数保持不变的情况下，也常因纯粹的视觉分布偏移而失效。然而，现有基准测试往往混杂多种偏移来源，阻碍了系统性分析。为此，我们提出了KAGE-Env——一个基于JAX原生开发的2D平台游戏环境，该环境将观测过程分解为可独立控制的视觉轴，同时保持底层控制问题固定。通过这种设计，改变任一视觉轴仅会通过影响像素策略的状态条件动作分布来改变性能，从而为视觉泛化研究提供了清晰的抽象框架。基于此环境，我们构建了KAGE-Bench基准测试，包含六个已知视觉轴测试集，共计34组训练-评估配置对，能够隔离单一视觉偏移的影响。采用标准PPO-CNN基线进行实验，我们观察到显著的轴依赖性失效现象：背景偏移与光度偏移常导致任务成功率骤降，而智能体外貌偏移的影响相对较小。部分偏移在保持前进运动的同时破坏了任务完成能力，这表明仅依赖回报指标可能掩盖泛化失败问题。此外，完全向量化的JAX实现使得单GPU上每秒可执行高达3300万环境步，实现了对视觉影响因素的快速、可重复扫描。代码地址：https://avanturist322.github.io/KAGEBench/。",
    "url": "https://huggingface.co/papers/2601.14232",
    "arxiv_url": "https://arxiv.org/abs/2601.14232"
  },
  {
    "title": "LightOnOCR: A 1B End-to-End Multilingual Vision-Language Model for State-of-the-Art OCR",
    "summary": "We present LightOnOCR-2-1B, a 1B-parameter end-to-end multilingual vision--language model that converts document images (e.g., PDFs) into clean, naturally ordered text without brittle OCR pipelines. Trained on a large-scale, high-quality distillation mix with strong coverage of scans, French documents, and scientific PDFs, LightOnOCR-2 achieves state-of-the-art results on OlmOCR-Bench while being 9times smaller and substantially faster than prior best-performing models. We further extend the output format to predict normalized bounding boxes for embedded images, introducing localization during pretraining via a resume strategy and refining it with RLVR using IoU-based rewards. Finally, we improve robustness with checkpoint averaging and task-arithmetic merging. We release model checkpoints under Apache 2.0, and publicly release the dataset and LightOnOCR-bbox-bench evaluation under their respective licenses.",
    "translation": "标题：LightOnOCR：一个10亿参数端到端多语言视觉语言模型，实现最先进OCR性能\n\n摘要：本文提出LightOnOCR-2-1B模型，这是一个拥有10亿参数的端到端多语言视觉-语言模型，能够直接将文档图像（如PDF文件）转换为整洁、自然排序的文本，无需依赖脆弱的传统OCR处理流程。该模型通过大规模高质量蒸馏混合数据进行训练，全面覆盖扫描文档、法语文档及科学类PDF文件。LightOnOCR-2在OlmOCR-Bench评测中取得了最先进的性能表现，其参数量较先前最佳模型缩小9倍且推理速度显著提升。我们进一步扩展输出格式以预测嵌入图像的归一化边界框，通过课程学习策略在预训练阶段引入定位能力，并采用基于交并比奖励的RLVR方法进行细化优化。最后，我们通过检查点平均与任务算术融合技术增强了模型鲁棒性。本模型检查点依据Apache 2.0协议开源发布，相关数据集及LightOnOCR-bbox-bench评估基准亦在对应许可下公开。",
    "url": "https://huggingface.co/papers/2601.14251",
    "arxiv_url": "https://arxiv.org/abs/2601.14251"
  },
  {
    "title": "PRiSM: Benchmarking Phone Realization in Speech Models",
    "summary": "Phone recognition (PR) serves as the atomic interface for language-agnostic modeling for cross-lingual speech processing and phonetic analysis. Despite prolonged efforts in developing PR systems, current evaluations only measure surface-level transcription accuracy. We introduce PRiSM, the first open-source benchmark designed to expose blind spots in phonetic perception through intrinsic and extrinsic evaluation of PR systems. PRiSM standardizes transcription-based evaluation and assesses downstream utility in clinical, educational, and multilingual settings with transcription and representation probes. We find that diverse language exposure during training is key to PR performance, encoder-CTC models are the most stable, and specialized PR models still outperform Large Audio Language Models. PRiSM releases code, recipes, and datasets to move the field toward multilingual speech models with robust phonetic ability: https://github.com/changelinglab/prism.",
    "translation": "标题：PRiSM：语音模型中音素实现性能的基准测试\n\n摘要：音素识别（PR）作为跨语言语音处理与语音学分析中语言无关建模的基础接口。尽管音素识别系统的开发已历经长期努力，当前评估仅关注表层转写准确度。本文提出PRiSM——首个通过音素识别系统的内在与外在评估揭示语音感知盲点的开源基准。PRiSM标准化了基于转写的评估方法，并通过转写与表征探针，在临床、教育及多语言场景中评估其下游应用价值。研究发现：训练过程中的多语言暴露是提升音素识别性能的关键；编码器-CTC模型表现最为稳定；专用音素识别模型仍优于大型音频语言模型。PRiSM公开了代码、训练方案与数据集，以推动领域向具备强健语音能力的多语言语音模型发展：https://github.com/changelinglab/prism。",
    "url": "https://huggingface.co/papers/2601.14046",
    "arxiv_url": "https://arxiv.org/abs/2601.14046"
  },
  {
    "title": "FantasyVLN: Unified Multimodal Chain-of-Thought Reasoning for Vision-Language Navigation",
    "summary": "Achieving human-level performance in Vision-and-Language Navigation (VLN) requires an embodied agent to jointly understand multimodal instructions and visual-spatial context while reasoning over long action sequences. Recent works, such as NavCoT and NavGPT-2, demonstrate the potential of Chain-of-Thought (CoT) reasoning for improving interpretability and long-horizon planning. Moreover, multimodal extensions like OctoNav-R1 and CoT-VLA further validate CoT as a promising pathway toward human-like navigation reasoning. However, existing approaches face critical drawbacks: purely textual CoTs lack spatial grounding and easily overfit to sparse annotated reasoning steps, while multimodal CoTs incur severe token inflation by generating imagined visual observations, making real-time navigation impractical. In this work, we propose FantasyVLN, a unified implicit reasoning framework that preserves the benefits of CoT reasoning without explicit token overhead. Specifically, imagined visual tokens are encoded into a compact latent space using a pretrained Visual AutoRegressor (VAR) during CoT reasoning training, and the model jointly learns from textual, visual, and multimodal CoT modes under a unified multi-CoT strategy. At inference, our model performs direct instruction-to-action mapping while still enjoying reasoning-aware representations. Extensive experiments on LH-VLN show that our approach achieves reasoning-aware yet real-time navigation, improving success rates and efficiency while reducing inference latency by an order of magnitude compared to explicit CoT methods.",
    "translation": "标题：FantasyVLN：面向视觉语言导航的统一多模态思维链推理框架\n\n摘要：在视觉语言导航（VLN）任务中实现人类水平的性能，要求智能体能够同时理解多模态指令与视觉空间上下文，并在长动作序列中进行推理。近期研究如NavCoT与NavGPT-2展示了思维链（CoT）推理在提升可解释性与长程规划能力方面的潜力。此外，OctoNav-R1与CoT-VLA等多模态扩展工作进一步验证了CoT是实现类人导航推理的有效路径。然而，现有方法存在明显局限：纯文本型CoT缺乏空间 grounding 且易对稀疏标注的推理步骤过拟合，而多模态CoT因生成虚拟视觉观测导致严重的标记膨胀，使得实时导航难以实现。本文提出FantasyVLN——一个统一的隐式推理框架，在保留CoT推理优势的同时避免了显式的标记开销。具体而言，在CoT推理训练阶段，我们通过预训练的视觉自回归编码器将虚拟视觉标记压缩至紧凑的潜在空间表示，并基于统一的多CoT策略联合学习文本、视觉及多模态三种推理模式。在推理阶段，模型直接实现从指令到动作的映射，同时仍具备推理感知的表征能力。在LH-VLN数据集上的大量实验表明，本方法实现了兼具推理感知与实时性的导航，在提升成功率与效率的同时，将推理延迟较显式CoT方法降低了一个数量级。",
    "url": "https://huggingface.co/papers/2601.13976",
    "arxiv_url": "https://arxiv.org/abs/2601.13976"
  },
  {
    "title": "Which Reasoning Trajectories Teach Students to Reason Better? A Simple Metric of Informative Alignment",
    "summary": "Long chain-of-thought (CoT) trajectories provide rich supervision signals for distilling reasoning from teacher to student LLMs. However, both prior work and our experiments show that trajectories from stronger teachers do not necessarily yield better students, highlighting the importance of data-student suitability in distillation. Existing methods assess suitability primarily through student likelihood, favoring trajectories that closely align with the model's current behavior but overlooking more informative ones. Addressing this, we propose Rank-Surprisal Ratio (RSR), a simple metric that captures both alignment and informativeness to assess the suitability of a reasoning trajectory. RSR is motivated by the observation that effective trajectories typically combine low absolute probability with relatively high-ranked tokens under the student model, balancing learning signal strength and behavioral alignment. Concretely, RSR is defined as the ratio of a trajectory's average token-wise rank to its average negative log-likelihood, and is straightforward to compute and interpret. Across five student models and reasoning trajectories from 11 diverse teachers, RSR strongly correlates with post-training performance (average Spearman 0.86), outperforming existing metrics. We further demonstrate its practical utility in both trajectory selection and teacher selection.",
    "translation": "标题：哪些推理轨迹能更好地教会学生推理？一种衡量信息对齐的简单指标\n\n摘要：长链思维轨迹为从教师大语言模型向学生大语言模型蒸馏推理能力提供了丰富的监督信号。然而，先前研究及我们的实验均表明，来自更强教师的轨迹未必能培养出更优秀的学生，这凸显了蒸馏过程中数据与学生模型适配性的重要性。现有方法主要通过学生模型的似然度评估适配性，倾向于选择与模型当前行为高度一致的轨迹，却可能忽略更具信息量的轨迹。针对这一问题，我们提出秩-惊异比，这是一个同时捕捉对齐性和信息量的简单指标，用于评估推理轨迹的适配性。RSR的提出基于以下观察：有效轨迹通常结合了较低绝对概率与学生模型中相对较高排名的词元，从而在学习信号强度与行为对齐之间取得平衡。具体而言，RSR定义为轨迹的平均词元秩与其平均负对数似然的比值，其计算和解释均较为直观。在五种学生模型和来自11位不同教师的推理轨迹上的实验表明，RSR与训练后性能呈现强相关性（平均斯皮尔曼相关系数0.86），优于现有评估指标。我们进一步展示了该指标在轨迹选择和教师选择两个场景中的实际应用价值。",
    "url": "https://huggingface.co/papers/2601.14249",
    "arxiv_url": "https://arxiv.org/abs/2601.14249"
  },
  {
    "title": "InT: Self-Proposed Interventions Enable Credit Assignment in LLM Reasoning",
    "summary": "Outcome-reward reinforcement learning (RL) has proven effective at improving the reasoning capabilities of large language models (LLMs). However, standard RL assigns credit only at the level of the final answer, penalizing entire reasoning traces when the outcome is incorrect and uniformly reinforcing all steps when it is correct. As a result, correct intermediate steps may be discouraged in failed traces, while spurious steps may be reinforced in successful ones. We refer to this failure mode as the problem of credit assignment. While a natural remedy is to train a process reward model, accurately optimizing such models to identify corrective reasoning steps remains challenging. We introduce Intervention Training (InT), a training paradigm in which the model performs fine-grained credit assignment on its own reasoning traces by proposing short, targeted corrections that steer trajectories toward higher reward. Using reference solutions commonly available in mathematical reasoning datasets and exploiting the fact that verifying a model-generated solution is easier than generating a correct one from scratch, the model identifies the first error in its reasoning and proposes a single-step intervention to redirect the trajectory toward the correct solution. We then apply supervised fine-tuning (SFT) to the on-policy rollout up to the point of error concatenated with the intervention, localizing error to the specific step that caused failure. We show that the resulting model serves as a far better initialization for RL training. After running InT and subsequent fine-tuning with RL, we improve accuracy by nearly 14% over a 4B-parameter base model on IMO-AnswerBench, outperforming larger open-source models such as gpt-oss-20b.",
    "translation": "标题：InT：自提议干预实现大语言模型推理中的信用分配\n\n摘要：结果奖励强化学习（RL）已被证明能有效提升大语言模型（LLM）的推理能力。然而，标准RL仅将信用分配至最终答案层面：当结果错误时惩罚整个推理轨迹，结果正确时则均匀强化所有步骤。这导致错误轨迹中的正确中间步骤可能被抑制，而成功轨迹中的无效步骤却可能被强化。我们将此失效模式称为信用分配问题。尽管训练过程奖励模型是一种自然解决方案，但准确优化此类模型以识别纠正性推理步骤仍具挑战性。本文提出干预训练（InT），该训练范式使模型能够通过提出简短、有针对性的修正建议（引导轨迹获得更高奖励），对其自身推理轨迹进行细粒度信用分配。利用数学推理数据集中普遍存在的参考答案，并基于“验证模型生成解比从头生成正确解更容易”这一事实，模型可识别其推理中的首个错误，并提出单步干预以将轨迹导向正确解。随后，我们对策略内执行轨迹（截至错误发生点）与干预建议进行拼接，并实施监督微调（SFT），从而将错误定位至导致失败的具体步骤。实验表明，所得模型能为后续RL训练提供更优的初始化基础。在InT及后续RL微调后，我们在IMO-AnswerBench上将4B参数基模型的准确率提升近14%，性能超越gpt-oss-20b等更大规模开源模型。",
    "url": "https://huggingface.co/papers/2601.14209",
    "arxiv_url": "https://arxiv.org/abs/2601.14209"
  },
  {
    "title": "Uncertainty-Aware Gradient Signal-to-Noise Data Selection for Instruction Tuning",
    "summary": "Instruction tuning is a standard paradigm for adapting large language models (LLMs), but modern instruction datasets are large, noisy, and redundant, making full-data fine-tuning costly and often unnecessary. Existing data selection methods either build expensive gradient datastores or assign static scores from a weak proxy, largely ignoring evolving uncertainty, and thus missing a key source of LLM interpretability. We propose GRADFILTERING, an objective-agnostic, uncertainty-aware data selection framework that utilizes a small GPT-2 proxy with a LoRA ensemble and aggregates per-example gradients into a Gradient Signal-to-Noise Ratio (G-SNR) utility. Our method matches or surpasses random subsets and strong baselines in most LLM-as-a-judge evaluations as well as in human assessment. Moreover, GRADFILTERING-selected subsets converge faster than competitive filters under the same compute budget, reflecting the benefit of uncertainty-aware scoring.",
    "translation": "标题：面向指令微调的不确定性感知梯度信噪比数据选择方法\n\n摘要：指令微调是适配大语言模型的标准范式，但现代指令数据集普遍存在规模庞大、噪声显著和冗余度高的问题，导致全数据微调成本高昂且往往不必要。现有数据选择方法或需构建高成本的梯度数据存储库，或依赖弱代理模型分配静态评分，大多忽略了模型训练过程中动态变化的不确定性，因而错失了大语言模型可解释性的关键来源。本文提出GRADFILTERING框架——一种与优化目标无关的不确定性感知数据选择方法，该方法采用集成LoRA模块的小型GPT-2代理模型，通过将样本级梯度聚合为梯度信噪比效用指标进行评估。在多数基于大语言模型作为评判器的评估及人工评估中，本方法所选数据子集的表现均达到或超越随机子集与现有强基线方法。此外，在相同计算预算下，经GRADFILTERING筛选的数据子集比竞争性过滤方法收敛更快，这印证了不确定性感知评分机制的有效性。",
    "url": "https://huggingface.co/papers/2601.13697",
    "arxiv_url": "https://arxiv.org/abs/2601.13697"
  },
  {
    "title": "On the Evidentiary Limits of Membership Inference for Copyright Auditing",
    "summary": "As large language models (LLMs) are trained on increasingly opaque corpora, membership inference attacks (MIAs) have been proposed to audit whether copyrighted texts were used during training, despite growing concerns about their reliability under realistic conditions. We ask whether MIAs can serve as admissible evidence in adversarial copyright disputes where an accused model developer may obfuscate training data while preserving semantic content, and formalize this setting through a judge-prosecutor-accused communication protocol. To test robustness under this protocol, we introduce SAGE (Structure-Aware SAE-Guided Extraction), a paraphrasing framework guided by Sparse Autoencoders (SAEs) that rewrites training data to alter lexical structure while preserving semantic content and downstream utility. Our experiments show that state-of-the-art MIAs degrade when models are fine-tuned on SAGE-generated paraphrases, indicating that their signals are not robust to semantics-preserving transformations. While some leakage remains in certain fine-tuning regimes, these results suggest that MIAs are brittle in adversarial settings and insufficient, on their own, as a standalone mechanism for copyright auditing of LLMs.",
    "translation": "标题：论成员推理在版权审计中的证据局限性\n\n摘要：随着大语言模型（LLM）在日益不透明的语料库上进行训练，尽管在实际条件下其可靠性备受质疑，成员推理攻击（MIA）仍被提议用于审计训练过程中是否使用了受版权保护的文本。本文探讨在对抗性版权纠纷中，当被指控的模型开发者可能对训练数据进行语义保留的模糊处理时，MIA能否作为可采信的证据。我们通过构建法官-控方-被告三方通信协议的形式化框架来定义这一场景。为测试该协议下的鲁棒性，我们提出了SAGE（结构感知的稀疏自编码器引导提取）框架——一种基于稀疏自编码器（SAE）的复述生成方法，该框架能在保留语义内容与下游效用的前提下重构训练数据的词汇结构。实验表明，当模型在SAGE生成的复述文本上进行微调时，最先进的MIA性能显著下降，这证明其检测信号对语义保持的文本变换不具备鲁棒性。尽管在某些微调机制中仍存在信息残留，但这些结果表明：在对抗性场景下，MIA具有脆弱性，其本身不足以作为LLM版权审计的独立机制。",
    "url": "https://huggingface.co/papers/2601.12937",
    "arxiv_url": "https://arxiv.org/abs/2601.12937"
  },
  {
    "title": "Fundamental Limitations of Favorable Privacy-Utility Guarantees for DP-SGD",
    "summary": "Differentially Private Stochastic Gradient Descent (DP-SGD) is the dominant paradigm for private training, but its fundamental limitations under worst-case adversarial privacy definitions remain poorly understood. We analyze DP-SGD in the f-differential privacy framework, which characterizes privacy via hypothesis-testing trade-off curves, and study shuffled sampling over a single epoch with M gradient updates. We derive an explicit suboptimal upper bound on the achievable trade-off curve. This result induces a geometric lower bound on the separation κ which is the maximum distance between the mechanism's trade-off curve and the ideal random-guessing line. Because a large separation implies significant adversarial advantage, meaningful privacy requires small κ. However, we prove that enforcing a small separation imposes a strict lower bound on the Gaussian noise multiplier σ, which directly limits the achievable utility. In particular, under the standard worst-case adversarial model, shuffled DP-SGD must satisfy\n  σge 1{2ln M} quadorquad κge 1{8}!left(1-1{4πln M}right),\n  and thus cannot simultaneously achieve strong privacy and high utility. Although this bound vanishes asymptotically as M to infty, the convergence is extremely slow: even for practically relevant numbers of updates the required noise magnitude remains substantial. We further show that the same limitation extends to Poisson subsampling up to constant factors. Our experiments confirm that the noise levels implied by this bound leads to significant accuracy degradation at realistic training settings, thus showing a critical bottleneck in DP-SGD under standard worst-case adversarial assumptions.",
    "translation": "标题：差分隐私随机梯度下降中有利隐私-效用保证的基本限制\n\n摘要：差分隐私随机梯度下降（DP-SGD）是隐私训练的主流范式，但其在最坏情况对抗性隐私定义下的基本限制仍未得到充分理解。我们在f-差分隐私框架下分析DP-SGD（该框架通过假设检验权衡曲线刻画隐私特性），并研究单轮训练周期内进行M次梯度更新的混洗采样机制。我们推导出可实现的权衡曲线存在显式的次优上界，该结果导出了分离度κ的几何下界（κ表示机制权衡曲线与理想随机猜测线之间的最大距离）。由于较大的分离度意味着显著的对抗性优势，有意义的隐私保护需要较小的κ值。然而，我们证明强制保持较小分离度会对高斯噪声乘数σ施加严格下界，从而直接限制可实现的效用。具体而言，在标准最坏情况对抗性模型下，混洗DP-SGD必须满足：\nσ ≥ 1/(2√ln M) 或 κ ≥ 1/8 (1 - 1/(4π ln M))，\n因此无法同时实现强隐私保护与高效用。尽管该界限在M→∞时渐近消失，但其收敛速度极慢：即使在实践相关的更新次数下，所需噪声量级仍然非常显著。我们进一步证明该限制在常数因子范围内同样适用于泊松子采样。实验证实，该界限所隐含的噪声水平会导致实际训练场景中模型精度显著下降，从而揭示了标准最坏情况对抗性假设下DP-SGD存在关键瓶颈。\n\n请按照以下格式返回：\n标题：[中文标题]\n摘要：[中文摘要]",
    "url": "https://huggingface.co/papers/2601.10237",
    "arxiv_url": "https://arxiv.org/abs/2601.10237"
  },
  {
    "title": "DSAEval: Evaluating Data Science Agents on a Wide Range of Real-World Data Science Problems",
    "summary": "Recent LLM-based data agents aim to automate data science tasks ranging from data analysis to deep learning. However, the open-ended nature of real-world data science problems, which often span multiple taxonomies and lack standard answers, poses a significant challenge for evaluation. To address this, we introduce DSAEval, a benchmark comprising 641 real-world data science problems grounded in 285 diverse datasets, covering both structured and unstructured data (e.g., vision and text). DSAEval incorporates three distinctive features: (1) Multimodal Environment Perception, which enables agents to interpret observations from multiple modalities including text and vision; (2) Multi-Query Interactions, which mirror the iterative and cumulative nature of real-world data science projects; and (3) Multi-Dimensional Evaluation, which provides a holistic assessment across reasoning, code, and results. We systematically evaluate 11 advanced agentic LLMs using DSAEval. Our results show that Claude-Sonnet-4.5 achieves the strongest overall performance, GPT-5.2 is the most efficient, and MiMo-V2-Flash is the most cost-effective. We further demonstrate that multimodal perception consistently improves performance on vision-related tasks, with gains ranging from 2.04% to 11.30%. Overall, while current data science agents perform well on structured data and routine data anlysis workflows, substantial challenges remain in unstructured domains. Finally, we offer critical insights and outline future research directions to advance the development of data science agents.",
    "translation": "标题：DSAEval：基于广泛真实世界数据科学问题的数据科学智能体评估框架\n\n摘要：当前基于大语言模型的数据智能体致力于实现从数据分析到深度学习的数据科学任务自动化。然而，真实世界数据科学问题具有开放性的特点，通常跨越多个分类体系且缺乏标准答案，这为评估工作带来了重大挑战。为此，我们提出了DSAEval评估基准，该基准包含基于285个多样化数据集的641个真实世界数据科学问题，涵盖结构化与非结构化数据（如视觉与文本数据）。DSAEval具备三个显著特征：（1）多模态环境感知能力，使智能体能够解析文本、视觉等多模态观测信息；（2）多轮次交互机制，模拟真实数据科学项目中迭代与累积的工作特性；（3）多维度评估体系，从推理过程、代码实现与结果输出三个维度进行综合评估。我们使用DSAEval对11个先进的智能体大语言模型进行了系统评估。结果表明：Claude-Sonnet-4.5在综合性能上表现最优，GPT-5.2具有最高执行效率，而MiMo-V2-Flash则具备最佳成本效益。我们进一步验证了多模态感知能力能持续提升视觉相关任务的表现，性能增益范围达2.04%至11.30%。总体而言，当前数据科学智能体在结构化数据和常规数据分析流程中表现良好，但在非结构化数据领域仍面临重大挑战。最后，我们提出了关键见解并展望了未来研究方向，以推动数据科学智能体的发展。",
    "url": "https://huggingface.co/papers/2601.13591",
    "arxiv_url": "https://arxiv.org/abs/2601.13591"
  },
  {
    "title": "A Hybrid Protocol for Large-Scale Semantic Dataset Generation in Low-Resource Languages: The Turkish Semantic Relations Corpus",
    "summary": "We present a hybrid methodology for generating large-scale semantic relationship datasets in low-resource languages, demonstrated through a comprehensive Turkish semantic relations corpus. Our approach integrates three phases: (1) FastText embeddings with Agglomerative Clustering to identify semantic clusters, (2) Gemini 2.5-Flash for automated semantic relationship classification, and (3) integration with curated dictionary sources. The resulting dataset comprises 843,000 unique Turkish semantic pairs across three relationship types (synonyms, antonyms, co-hyponyms) representing a 10x scale increase over existing resources at minimal cost ($65). We validate the dataset through two downstream tasks: an embedding model achieving 90% top-1 retrieval accuracy and a classification model attaining 90% F1-macro. Our scalable protocol addresses critical data scarcity in Turkish NLP and demonstrates applicability to other low-resource languages. We publicly release the dataset and models.",
    "translation": "标题：一种面向低资源语言大规模语义数据集生成的混合协议：土耳其语语义关系语料库\n\n摘要：本文提出了一种用于生成低资源语言大规模语义关系数据集的混合方法，并通过构建一个全面的土耳其语语义关系语料库进行验证。我们的方法整合了三个阶段：(1) 利用FastText词嵌入与凝聚层次聚类识别语义簇，(2) 采用Gemini 2.5-Flash进行自动化语义关系分类，(3) 与精选词典资源进行整合。最终构建的数据集包含843,000个独特的土耳其语语义对，涵盖同义词、反义词和共下位词三种关系类型，其规模以极低成本（65美元）达到现有资源的10倍。我们通过两项下游任务验证了数据集质量：词嵌入模型在Top-1检索任务中达到90%准确率，分类模型获得90%的宏观F1分数。这一可扩展的协议有效缓解了土耳其语自然语言处理领域的数据稀缺问题，并证明了其适用于其他低资源语言的潜力。我们已公开数据集与相关模型。",
    "url": "https://huggingface.co/papers/2601.13253",
    "arxiv_url": "https://arxiv.org/abs/2601.13253"
  },
  {
    "title": "Beyond Cosine Similarity: Taming Semantic Drift and Antonym Intrusion in a 15-Million Node Turkish Synonym Graph",
    "summary": "Neural embeddings have a notorious blind spot: they can't reliably tell synonyms apart from antonyms. Consequently, increasing similarity thresholds often fails to prevent opposites from being grouped together. We've built a large-scale semantic clustering system specifically designed to tackle this problem head on. Our pipeline chews through 15 million lexical items, evaluates a massive 520 million potential relationships, and ultimately generates 2.9 million high-precision semantic clusters. The system makes three primary contributions. First, we introduce a labeled dataset of 843,000 concept pairs spanning synonymy, antonymy, and co-hyponymy, constructed via Gemini 2.5-Flash LLM augmentation and verified using human-curated dictionary resources. Second, we propose a specialized three-way semantic relation discriminator that achieves 90% macro-F1, enabling robust disambiguation beyond raw embedding similarity. Third, we introduce a novel soft-to-hard clustering algorithm that mitigates semantic drift preventing erroneous transitive chains (e.g., hot -> spicy -> pain -> depression) while simultaneously resolving polysemy. Our approach employs a topology-aware two-stage expansion-pruning procedure with topological voting, ensuring that each term is assigned to exactly one semantically coherent cluster. The resulting resource enables high-precision semantic search and retrieval-augmented generation, particularly for morphologically rich and low-resource languages where existing synonym databases remain sparse.",
    "translation": "标题：超越余弦相似度：在一个1500万节点的土耳其语同义词图中控制语义漂移与反义词干扰\n\n摘要：神经嵌入模型存在一个显著的盲点：其无法可靠地区分同义词与反义词。因此，单纯提高相似度阈值往往无法阻止反义词被错误归为一类。我们构建了一个专门针对此问题的大规模语义聚类系统。该流程处理了1500万个词汇单元，评估了5.2亿个潜在语义关系，最终生成了290万个高精度语义聚类簇。本系统主要有三点贡献：首先，我们构建了一个包含84.3万对概念（涵盖同义、反义及同级关系）的标注数据集，该数据集通过Gemini 2.5-Flash大语言模型进行数据增强，并利用人工校勘的词典资源进行验证。其次，我们提出了一种专用的三元语义关系判别器，其宏观F1值达到90%，能够实现超越原始嵌入相似度的鲁棒消歧。第三，我们设计了一种新颖的软聚类到硬聚类算法，该算法通过拓扑感知的两阶段扩展-剪枝流程结合拓扑投票机制，在消解一词多义现象的同时，有效抑制了导致错误传递链（例如：热→辣→疼痛→抑郁）的语义漂移，确保每个术语被精确分配至一个语义连贯的聚类簇。最终构建的资源能够支持高精度语义搜索与检索增强生成，尤其适用于形态丰富且现有同义词数据库稀缺的低资源语言。",
    "url": "https://huggingface.co/papers/2601.13251",
    "arxiv_url": "https://arxiv.org/abs/2601.13251"
  },
  {
    "title": "METIS: Mentoring Engine for Thoughtful Inquiry & Solutions",
    "summary": "Many students lack access to expert research mentorship. We ask whether an AI mentor can move undergraduates from an idea to a paper. We build METIS, a tool-augmented, stage-aware assistant with literature search, curated guidelines, methodology checks, and memory. We evaluate METIS against GPT-5 and Claude Sonnet 4.5 across six writing stages using LLM-as-a-judge pairwise preferences, student-persona rubrics, short multi-turn tutoring, and evidence/compliance checks. On 90 single-turn prompts, LLM judges preferred METIS to Claude Sonnet 4.5 in 71% and to GPT-5 in 54%. Student scores (clarity/actionability/constraint-fit; 90 prompts x 3 judges) are higher across stages. In multi-turn sessions (five scenarios/agent), METIS yields slightly higher final quality than GPT-5. Gains concentrate in document-grounded stages (D-F), consistent with stage-aware routing and groundings failure modes include premature tool routing, shallow grounding, and occasional stage misclassification.",
    "translation": "标题：METIS：面向审慎探究与解决方案的导师引擎\n\n摘要：当前许多学生难以获得专业的研究指导。本研究探讨人工智能导师能否帮助本科生从研究构思推进至论文成稿。为此我们开发了METIS系统——一个具备文献检索、精选指南、方法论核查与记忆功能的工具增强型分阶段智能助手。通过LLM作为评判者的配对偏好评估、学生角色量规、短对话多轮辅导及证据/合规性检查，我们在六个写作阶段将METIS与GPT-5和Claude Sonnet 4.5进行对比评估。在90个单轮提示测试中，LLM评判者认为METIS优于Claude Sonnet 4.5的比例达71%，优于GPT-5的比例为54%。分阶段评估显示（基于清晰度/可操作性/约束匹配度指标，90提示×3评委），METIS在各阶段均获得更高评分。在多轮对话场景中（五类情境/智能体），METIS最终产出质量略高于GPT-5。性能提升主要集中在文献支撑阶段（D-F），这与分阶段路由机制的设计相符；系统失效模式包括工具过早路由、文献支撑深度不足及偶发的阶段误判。",
    "url": "https://huggingface.co/papers/2601.13075",
    "arxiv_url": "https://arxiv.org/abs/2601.13075"
  },
  {
    "title": "SciCoQA: Quality Assurance for Scientific Paper--Code Alignment",
    "summary": "We present SciCoQA, a dataset for detecting discrepancies between scientific publications and their codebases to ensure faithful implementations. We construct SciCoQA from GitHub issues and reproducibility papers, and to scale our dataset, we propose a synthetic data generation method for constructing paper-code discrepancies. We analyze the paper-code discrepancies in detail and propose discrepancy types and categories to better understand the occurring mismatches. In total, our dataset consists of 611 paper-code discrepancies (81 real, 530 synthetic), spanning diverse computational science disciplines, including AI, Physics, Quantitative Biology, and others. Our evaluation of 21 LLMs highlights the difficulty of SciCoQA, particularly for instances involving omitted paper details, long-context inputs, and data outside the models' pre-training corpus. The best performing model in our evaluation, GPT-5, can only detect 45.7\\% of real-world paper-code discrepancies.",
    "translation": "标题：SciCoQA：科学论文与代码对齐的质量保证\n\n摘要：本文提出SciCoQA数据集，用于检测科学出版物与其代码库之间的差异，以确保实现过程的忠实性。我们基于GitHub议题和可复现性论文构建了SciCoQA，并提出一种合成数据生成方法以规模化构建论文-代码差异数据。我们详细分析了论文与代码之间的差异，提出了差异类型与分类体系，以深入理解实际中出现的错配现象。该数据集共包含611个论文-代码差异实例（81个真实案例，530个合成案例），涵盖人工智能、物理学、定量生物学等多个计算科学领域。通过对21个大语言模型的评估，我们发现SciCoQA任务具有较高难度，特别是在涉及论文细节缺失、长上下文输入及模型预训练语料外数据的案例中表现明显。评估中表现最佳的GPT-5模型仅能检测出45.7%的真实世界论文-代码差异。",
    "url": "https://huggingface.co/papers/2601.12910",
    "arxiv_url": "https://arxiv.org/abs/2601.12910"
  },
  {
    "title": "LIBERTy: A Causal Framework for Benchmarking Concept-Based Explanations of LLMs with Structural Counterfactuals",
    "summary": "Concept-based explanations quantify how high-level concepts (e.g., gender or experience) influence model behavior, which is crucial for decision-makers in high-stakes domains. Recent work evaluates the faithfulness of such explanations by comparing them to reference causal effects estimated from counterfactuals. In practice, existing benchmarks rely on costly human-written counterfactuals that serve as an imperfect proxy. To address this, we introduce a framework for constructing datasets containing structural counterfactual pairs: LIBERTy (LLM-based Interventional Benchmark for Explainability with Reference Targets). LIBERTy is grounded in explicitly defined Structured Causal Models (SCMs) of the text generation, interventions on a concept propagate through the SCM until an LLM generates the counterfactual. We introduce three datasets (disease detection, CV screening, and workplace violence prediction) together with a new evaluation metric, order-faithfulness. Using them, we evaluate a wide range of methods across five models and identify substantial headroom for improving concept-based explanations. LIBERTy also enables systematic analysis of model sensitivity to interventions: we find that proprietary LLMs show markedly reduced sensitivity to demographic concepts, likely due to post-training mitigation. Overall, LIBERTy provides a much-needed benchmark for developing faithful explainability methods.",
    "translation": "标题：LIBERTy：基于结构反事实的LLM概念解释基准测试因果框架\n\n摘要：基于概念的解释通过量化高级概念（如性别或经验）对模型行为的影响，为高风险领域的决策者提供关键洞察。现有研究通过将此类解释与基于反事实估计的参考因果效应进行比较，以评估其忠实性。然而，当前基准测试依赖成本高昂的人工编写反事实作为不完善的代理指标。为此，我们提出了一个构建包含结构反事实对数据集的框架：LIBERTy（基于LLM的可解释性干预基准测试参考目标）。该框架以文本生成的显式结构化因果模型为基础，通过对概念进行干预，使效应沿SCM传递直至LLM生成反事实文本。我们构建了三个数据集（疾病检测、简历筛选和工作场所暴力预测）并提出了新的评估指标——顺序忠实性。基于这些资源，我们对五种模型中的多种方法进行了评估，发现现有概念解释方法仍有显著改进空间。LIBERTy还能系统分析模型对干预的敏感性：研究发现，经过后训练缓解的专有LLM对人口统计概念的敏感性显著降低。总体而言，LIBERTy为开发忠实可靠的可解释性方法提供了亟需的基准测试体系。",
    "url": "https://huggingface.co/papers/2601.10700",
    "arxiv_url": "https://arxiv.org/abs/2601.10700"
  },
  {
    "title": "Finally Outshining the Random Baseline: A Simple and Effective Solution for Active Learning in 3D Biomedical Imaging",
    "summary": "Active learning (AL) has the potential to drastically reduce annotation costs in 3D biomedical image segmentation, where expert labeling of volumetric data is both time-consuming and expensive. Yet, existing AL methods are unable to consistently outperform improved random sampling baselines adapted to 3D data, leaving the field without a reliable solution. We introduce Class-stratified Scheduled Power Predictive Entropy (ClaSP PE), a simple and effective query strategy that addresses two key limitations of standard uncertainty-based AL methods: class imbalance and redundancy in early selections. ClaSP PE combines class-stratified querying to ensure coverage of underrepresented structures and log-scale power noising with a decaying schedule to enforce query diversity in early-stage AL and encourage exploitation later. In our evaluation on 24 experimental settings using four 3D biomedical datasets within the comprehensive nnActive benchmark, ClaSP PE is the only method that generally outperforms improved random baselines in terms of both segmentation quality with statistically significant gains, whilst remaining annotation efficient. Furthermore, we explicitly simulate the real-world application by testing our method on four previously unseen datasets without manual adaptation, where all experiment parameters are set according to predefined guidelines. The results confirm that ClaSP PE robustly generalizes to novel tasks without requiring dataset-specific tuning. Within the nnActive framework, we present compelling evidence that an AL method can consistently outperform random baselines adapted to 3D segmentation, in terms of both performance and annotation efficiency in a realistic, close-to-production scenario. Our open-source implementation and clear deployment guidelines make it readily applicable in practice. Code is at https://github.com/MIC-DKFZ/nnActive.",
    "translation": "标题：终于超越随机基线：三维生物医学影像主动学习的一种简单有效解决方案\n\n摘要：主动学习（AL）有望显著降低三维生物医学图像分割的标注成本，因为专家对体数据的手动标注既耗时又昂贵。然而，现有主动学习方法始终无法稳定超越针对三维数据优化的改进型随机采样基线，导致该领域缺乏可靠的解决方案。本文提出类分层调度幂预测熵（ClaSP PE）方法，这是一种简单而有效的查询策略，它解决了标准基于不确定性的主动学习方法的两大关键局限：类别不平衡和早期选择冗余。ClaSP PE 融合了类分层查询机制以确保对低代表性结构的覆盖，同时采用对数尺度幂噪声结合衰减调度策略，在主动学习早期阶段强制查询多样性，并在后期促进针对性挖掘。我们在综合性 nnActive 基准测试中，使用四个三维生物医学数据集构建的 24 种实验设置进行评估，结果表明 ClaSP PE 是唯一能在分割质量上以统计显著优势普遍超越改进型随机基线的方法，同时保持标注高效性。此外，我们通过在四个未见数据集上不进行人工适配地测试方法，明确模拟了实际应用场景，所有实验参数均依据预设指南设置。结果证实 ClaSP PE 能够稳健地泛化至新任务，无需针对特定数据集进行调优。在 nnActive 框架内，我们提供了有力证据，表明在接近实际生产的现实场景中，主动学习方法可以在性能与标注效率两方面持续超越适用于三维分割的随机基线。我们的开源实现与清晰部署指南使其能够直接应用于实践。代码位于 https://github.com/MIC-DKFZ/nnActive。",
    "url": "https://huggingface.co/papers/2601.13677",
    "arxiv_url": "https://arxiv.org/abs/2601.13677"
  },
  {
    "title": "Towards Efficient and Robust Linguistic Emotion Diagnosis for Mental Health via Multi-Agent Instruction Refinement",
    "summary": "Linguistic expressions of emotions such as depression, anxiety, and trauma-related states are pervasive in clinical notes, counseling dialogues, and online mental health communities, and accurate recognition of these emotions is essential for clinical triage, risk assessment, and timely intervention. Although large language models (LLMs) have demonstrated strong generalization ability in emotion analysis tasks, their diagnostic reliability in high-stakes, context-intensive medical settings remains highly sensitive to prompt design. Moreover, existing methods face two key challenges: emotional comorbidity, in which multiple intertwined emotional states complicate prediction, and inefficient exploration of clinically relevant cues. To address these challenges, we propose APOLO (Automated Prompt Optimization for Linguistic Emotion Diagnosis), a framework that systematically explores a broader and finer-grained prompt space to improve diagnostic efficiency and robustness. APOLO formulates instruction refinement as a Partially Observable Markov Decision Process and adopts a multi-agent collaboration mechanism involving Planner, Teacher, Critic, Student, and Target roles. Within this closed-loop framework, the Planner defines an optimization trajectory, while the Teacher-Critic-Student agents iteratively refine prompts to enhance reasoning stability and effectiveness, and the Target agent determines whether to continue optimization based on performance evaluation. Experimental results show that APOLO consistently improves diagnostic accuracy and robustness across domain-specific and stratified benchmarks, demonstrating a scalable and generalizable paradigm for trustworthy LLM applications in mental healthcare.",
    "translation": "标题：基于多智能体指令优化的高效鲁棒性语言情感诊断在心理健康领域的应用研究\n\n摘要：抑郁、焦虑及创伤相关状态等情感的语言表达广泛存在于临床记录、咨询对话及在线心理健康社区中，对这些情感的准确识别对于临床分诊、风险评估和及时干预至关重要。尽管大语言模型在情感分析任务中展现出强大的泛化能力，但在高风险、强语境的医疗场景中，其诊断可靠性仍高度依赖于提示设计。现有方法面临两大关键挑战：一是情感共病现象，即多种交织的情感状态使预测复杂化；二是对临床相关线索的探索效率不足。为应对这些挑战，本研究提出APOLO框架（面向语言情感诊断的自动化提示优化），通过系统探索更广泛且细粒度的提示空间来提升诊断效率与鲁棒性。APOLO将指令优化建模为部分可观测马尔可夫决策过程，采用包含规划者、教师、评判者、学生与目标角色的多智能体协作机制。在此闭环框架中，规划者定义优化轨迹，教师-评判者-学生智能体通过迭代优化提示以增强推理稳定性与有效性，目标智能体则根据性能评估决定是否继续优化。实验结果表明，APOLO在领域特定及分层基准测试中持续提升诊断准确率与鲁棒性，为心理健康领域可信赖的大语言模型应用提供了可扩展、可推广的范式。",
    "url": "https://huggingface.co/papers/2601.13481",
    "arxiv_url": "https://arxiv.org/abs/2601.13481"
  },
  {
    "title": "RemoteVAR: Autoregressive Visual Modeling for Remote Sensing Change Detection",
    "summary": "Remote sensing change detection aims to localize and characterize scene changes between two time points and is central to applications such as environmental monitoring and disaster assessment. Meanwhile, visual autoregressive models (VARs) have recently shown impressive image generation capability, but their adoption for pixel-level discriminative tasks remains limited due to weak controllability, suboptimal dense prediction performance and exposure bias. We introduce RemoteVAR, a new VAR-based change detection framework that addresses these limitations by conditioning autoregressive prediction on multi-resolution fused bi-temporal features via cross-attention, and by employing an autoregressive training strategy designed specifically for change map prediction. Extensive experiments on standard change detection benchmarks show that RemoteVAR delivers consistent and significant improvements over strong diffusion-based and transformer-based baselines, establishing a competitive autoregressive alternative for remote sensing change detection. Code will be available https://github.com/yilmazkorkmaz1/RemoteVAR{here}.",
    "translation": "标题：RemoteVAR：面向遥感变化检测的自回归视觉建模\n\n摘要：遥感变化检测旨在定位并描述两个时间点之间的场景变化，是环境监测与灾害评估等应用的核心任务。与此同时，视觉自回归模型（VARs）近期展现出卓越的图像生成能力，但由于可控性较弱、密集预测性能欠佳以及曝光偏差等问题，其在像素级判别任务中的应用仍较为有限。本文提出RemoteVAR，一种基于VAR的新型变化检测框架，该框架通过跨注意力机制将自回归预测条件化于多分辨率融合的双时相特征，并采用专为变化图预测设计的自回归训练策略，从而有效克服了上述限制。在标准变化检测基准数据集上的大量实验表明，RemoteVAR相较于基于扩散模型和基于Transformer的强基线模型均取得了持续且显著的性能提升，为遥感变化检测领域提供了一种具有竞争力的自回归解决方案。代码将在 https://github.com/yilmazkorkmaz1/RemoteVAR 公开。",
    "url": "https://huggingface.co/papers/2601.11898",
    "arxiv_url": "https://arxiv.org/abs/2601.11898"
  }
]