[
  {
    "title": "From Code Foundation Models to Agents and Applications: A Practical Guide to Code Intelligence",
    "summary": "Large language models (LLMs) have fundamentally transformed automated software development by enabling direct translation of natural language descriptions into functional code, driving commercial adoption through tools like Github Copilot (Microsoft), Cursor (Anysphere), Trae (ByteDance), and Claude Code (Anthropic). While the field has evolved dramatically from rule-based systems to Transformer-based architectures, achieving performance improvements from single-digit to over 95\\% success rates on benchmarks like HumanEval. In this work, we provide a comprehensive synthesis and practical guide (a series of analytic and probing experiments) about code LLMs, systematically examining the complete model life cycle from data curation to post-training through advanced prompting paradigms, code pre-training, supervised fine-tuning, reinforcement learning, and autonomous coding agents. We analyze the code capability of the general LLMs (GPT-4, Claude, LLaMA) and code-specialized LLMs (StarCoder, Code LLaMA, DeepSeek-Coder, and QwenCoder), critically examining the techniques, design decisions, and trade-offs. Further, we articulate the research-practice gap between academic research (e.g., benchmarks and tasks) and real-world deployment (e.g., software-related code tasks), including code correctness, security, contextual awareness of large codebases, and integration with development workflows, and map promising research directions to practical needs. Last, we conduct a series of experiments to provide a comprehensive analysis of code pre-training, supervised fine-tuning, and reinforcement learning, covering scaling law, framework selection, hyperparameter sensitivity, model architectures, and dataset comparisons.",
    "translation": "标题：从代码基础模型到智能体与应用：代码智能实践指南\n\n摘要：大型语言模型（LLM）通过实现自然语言描述到功能代码的直接转换，从根本上改变了自动化软件开发的面貌，并借助GitHub Copilot（微软）、Cursor（Anysphere）、Trae（字节跳动）和Claude Code（Anthropic）等工具推动了商业应用。该领域已从基于规则的系统显著演进至基于Transformer的架构，在HumanEval等基准测试中的性能从个位数成功率提升至超过95%。本文针对代码大型语言模型，提供了全面的综述与实践指南（包含一系列分析与探测实验），系统性地审视了从数据构建到后训练的完整模型生命周期，涵盖高级提示范式、代码预训练、监督微调、强化学习及自主编码智能体。我们分析了通用大型语言模型（GPT-4、Claude、LLaMA）与代码专用大型语言模型（StarCoder、Code LLaMA、DeepSeek-Coder、QwenCoder）的代码能力，并对相关技术、设计决策与权衡进行了批判性考察。进一步地，我们阐明了学术研究（如基准测试与任务）与实际部署（如软件相关代码任务）之间的研究-实践差距，涉及代码正确性、安全性、大型代码库的上下文感知以及与开发流程的集成，并将有前景的研究方向与实际需求进行映射。最后，我们通过一系列实验对代码预训练、监督微调与强化学习进行了综合分析，涵盖缩放定律、框架选择、超参数敏感性、模型架构及数据集比较。",
    "url": "https://huggingface.co/papers/2511.18538",
    "arxiv_url": "https://arxiv.org/abs/2511.18538"
  },
  {
    "title": "LongVT: Incentivizing \"Thinking with Long Videos\" via Native Tool Calling",
    "summary": "Large multimodal models (LMMs) have shown great potential for video reasoning with textual Chain-of-Thought. However, they remain vulnerable to hallucinations, especially when processing long-form videos where evidence is sparse and temporally dispersed. Inspired by how humans comprehend long videos - by first skimming globally and then examining relevant clips for details - we introduce LongVT, an end-to-end agentic framework that enables \"Thinking with Long Videos\" via interleaved Multimodal Chain-of-Tool-Thought. Specifically, we exploit LMMs' inherent temporal grounding ability as a native video cropping tool to zoom in on a specific video clip and resample finer-grained video frames. This global-to-local reasoning loop continues until answers are grounded in retrieved visual evidence. Given the scarcity of fine-grained question-answering (QA) data for the long video reasoning task, we curate and will release a data suite named VideoSIAH to facilitate both training and evaluation. Specifically, our training dataset consists of 247.9K samples for tool-integrated cold-start supervised fine-tuning, 1.6K samples for agentic reinforcement learning, and 15.4K samples for agentic reinforcement fine-tuning, respectively. Our evaluation benchmark consists of 1,280 QA pairs that are carefully curated through a semi-automatic data pipeline with human-in-the-loop validation. With a meticulously designed three-stage training strategy and extensive empirical validation, LongVT consistently outperforms existing strong baselines across four challenging long-video understanding and reasoning benchmarks. Our codes, data, and model checkpoints are publicly available at https://github.com/EvolvingLMMs-Lab/LongVT .",
    "translation": "标题：LongVT：通过原生工具调用激励“长视频思考”\n\n摘要：大型多模态模型（LMMs）在结合文本思维链进行视频推理方面展现出巨大潜力。然而，它们仍易产生幻觉，尤其在处理证据稀疏且时间分散的长视频时。受人类理解长视频方式（先全局浏览，再细查相关片段）的启发，我们提出了LongVT——一个端到端的智能体框架，通过交错的多模态工具-思维链实现“长视频思考”。具体而言，我们利用LMMs固有的时序定位能力作为原生视频裁剪工具，以聚焦特定视频片段并重采样更细粒度的视频帧。这种从全局到局部的推理循环持续进行，直至答案基于检索到的视觉证据得到验证。针对长视频推理任务中细粒度问答数据稀缺的问题，我们构建并将发布名为VideoSIAH的数据套件，以支持训练与评估。具体来说，我们的训练数据集包含24.79万个样本用于工具集成的冷启动监督微调、1.6千个样本用于智能体强化学习，以及15.4千个样本用于智能体强化微调。评估基准包含1280个经过半自动化数据流程精心构建、并经过人机协同验证的问答对。通过精心设计的三阶段训练策略和大量实证验证，LongVT在四个具有挑战性的长视频理解与推理基准测试中均持续超越现有强基线模型。我们的代码、数据及模型检查点已公开于https://github.com/EvolvingLMMs-Lab/LongVT。",
    "url": "https://huggingface.co/papers/2511.20785",
    "arxiv_url": "https://arxiv.org/abs/2511.20785"
  },
  {
    "title": "Envision: Benchmarking Unified Understanding & Generation for Causal World Process Insights",
    "summary": "Current multimodal models aim to transcend the limitations of single-modality representations by unifying understanding and generation, often using text-to-image (T2I) tasks to calibrate semantic consistency. However, their reliance on static, single-image generation in training and evaluation leads to overfitting to static pattern matching and semantic fusion, while fundamentally hindering their ability to model dynamic processes that unfold over time. To address these constraints, we propose Envision-a causal event progression benchmark for chained text-to-multi-image generation. Grounded in world knowledge and structured by spatiotemporal causality, it reorganizes existing evaluation dimensions and includes 1,000 four-stage prompts spanning six scientific and humanities domains. To transition evaluation from single images to sequential frames and assess whether models truly internalize world knowledge while adhering to causal-temporal constraints, we introduce Envision-Score, a holistic metric integrating multi-dimensional consistency, physicality, and aesthetics. Comprehensive evaluation of 15 models (10 specialized T2I models, 5 unified models) uncovers: specialized T2I models demonstrate proficiency in aesthetic rendering yet lack intrinsic world knowledge. Unified multimodal models bridge this gap, consistently outperforming specialized counterparts in causal narrative coherence. However, even these unified architectures remain subordinate to closed-source models and struggle to overcome the core challenge of spatiotemporal consistency. This demonstrates that a focus on causally-isolated single images impedes multi-frame reasoning and generation, promoting static pattern matching over dynamic world modeling-ultimately limiting world knowledge internalization, generation.",
    "translation": "标题：Envision：面向因果世界过程洞察的统一理解与生成基准评测\n\n摘要：当前多模态模型旨在通过统一理解与生成能力突破单模态表征的局限，常采用文本到图像（T2I）任务校准语义一致性。然而，其在训练与评估中对静态单图像生成的依赖，导致模型过度拟合静态模式匹配与语义融合，从根本上制约了对随时间演化的动态过程建模能力。为突破这些限制，我们提出Envision——一个面向链式文本到多图像生成的因果事件演进基准。该基准以世界知识为基础，通过时空因果结构进行组织，重构了现有评估维度，涵盖六大科学与人文领域的1000个四阶段提示。为实现从单图像到序列帧的评估转型，并检验模型是否在遵循因果时序约束的同时真正内化了世界知识，我们提出Envision-Score综合评估指标，整合了多维一致性、物理合理性与美学表现。对15个模型（10个专用T2I模型，5个统一模型）的系统评估发现：专用T2I模型虽在美学渲染方面表现熟练，但缺乏内在世界知识；统一多模态模型弥补了这一差距，在因果叙事连贯性上持续优于专用模型。然而，即使这些统一架构仍逊色于闭源模型，且在克服时空一致性的核心挑战方面存在困难。这表明，对因果孤立单图像的关注会阻碍多帧推理与生成，促使模型偏向静态模式匹配而非动态世界建模，最终限制了世界知识的内化与生成能力。",
    "url": "https://huggingface.co/papers/2512.01816",
    "arxiv_url": "https://arxiv.org/abs/2512.01816"
  },
  {
    "title": "Stabilizing Reinforcement Learning with LLMs: Formulation and Practices",
    "summary": "This paper proposes a novel formulation for reinforcement learning (RL) with large language models, explaining why and under what conditions the true sequence-level reward can be optimized via a surrogate token-level objective in policy gradient methods such as REINFORCE. Specifically, through a first-order approximation, we show that this surrogate becomes increasingly valid only when both the training-inference discrepancy and policy staleness are minimized. This insight provides a principled explanation for the crucial role of several widely adopted techniques in stabilizing RL training, including importance sampling correction, clipping, and particularly Routing Replay for Mixture-of-Experts (MoE) models. Through extensive experiments with a 30B MoE model totaling hundreds of thousands of GPU hours, we show that for on-policy training, the basic policy gradient algorithm with importance sampling correction achieves the highest training stability. When off-policy updates are introduced to accelerate convergence, combining clipping and Routing Replay becomes essential to mitigate the instability caused by policy staleness. Notably, once training is stabilized, prolonged optimization consistently yields comparable final performance regardless of cold-start initialization. We hope that the shared insights and the developed recipes for stable RL training will facilitate future research.",
    "translation": "标题：基于大语言模型的强化学习稳定性：理论框架与实践方法\n\n摘要：本文提出了一种基于大语言模型的强化学习新框架，从理论层面阐释了在REINFORCE等策略梯度方法中，为何及在何种条件下可通过代理词级目标函数优化真实序列级奖励。具体而言，通过一阶近似分析，我们证明仅当训练-推断差异与策略陈旧性同时最小化时，该代理目标的有效性才会显著提升。这一发现为多项广泛采用的强化学习稳定技术提供了理论依据，包括重要性采样校正、梯度裁剪，以及特别针对专家混合模型的路由重放机制。通过对参数量达300亿的专家混合模型开展累计数十万GPU小时的实验，我们发现：在在线策略训练中，结合重要性采样校正的基础策略梯度算法能实现最高的训练稳定性；当引入离线策略更新以加速收敛时，梯度裁剪与路由重放技术的结合对于缓解策略陈旧性引起的不稳定性至关重要。值得注意的是，一旦训练趋于稳定，无论采用何种冷启动初始化方式，持续优化最终都能获得相当的性能表现。我们期望所分享的理论洞见与开发的稳定训练方案能为未来研究提供参考。\n\n标题：基于大语言模型的强化学习稳定性：理论框架与实践方法\n摘要：本文提出了一种基于大语言模型的强化学习新框架，从理论层面阐释了在REINFORCE等策略梯度方法中，为何及在何种条件下可通过代理词级目标函数优化真实序列级奖励。具体而言，通过一阶近似分析，我们证明仅当训练-推断差异与策略陈旧性同时最小化时，该代理目标的有效性才会显著提升。这一发现为多项广泛采用的强化学习稳定技术提供了理论依据，包括重要性采样校正、梯度裁剪，以及特别针对专家混合模型的路由重放机制。通过对参数量达300亿的专家混合模型开展累计数十万GPU小时的实验，我们发现：在在线策略训练中，结合重要性采样校正的基础策略梯度算法能实现最高的训练稳定性；当引入离线策略更新以加速收敛时，梯度裁剪与路由重放技术的结合对于缓解策略陈旧性引起的不稳定性至关重要。值得注意的是，一旦训练趋于稳定，无论采用何种冷启动初始化方式，持续优化最终都能获得相当的性能表现。我们期望所分享的理论洞见与开发的稳定训练方案能为未来研究提供参考。",
    "url": "https://huggingface.co/papers/2512.01374",
    "arxiv_url": "https://arxiv.org/abs/2512.01374"
  },
  {
    "title": "How Far Are We from Genuinely Useful Deep Research Agents?",
    "summary": "Deep Research Agents (DRAs) aim to automatically produce analyst-level reports through iterative information retrieval and synthesis. However, most existing DRAs were validated on question-answering benchmarks, while research on generating comprehensive reports remains overlooked. Worse, current benchmarks for report synthesis suffer from task complexity and subjective metrics -- this fails to reflect user demands and limits the practical utility of generated reports. To address these gaps, we present Fine-grained DEepResearch bench (FINDER), an enhanced benchmark consisting of 100 human-curated research tasks with 419 structured checklist items that standardize report structure, analytical depth, and factual grounding. Based on approximately 1,000 reports produced by mainstream DRAs, we further propose Deep rEsearch Failure Taxonomy (DEFT), the first failure taxonomy for deep research agents. DEFT contains 14 fine-grained failure modes across reasoning, retrieval, and generation, and is built upon grounded theory with human-LLM co-annotating and inter-annotator reliability validation. Our experimental findings reveal that current DRAs struggle not with task comprehension but with evidence integration, verification, and reasoning-resilient planning.",
    "translation": "标题：我们距离真正实用的深度研究智能体还有多远？\n\n摘要：深度研究智能体旨在通过迭代式信息检索与综合，自动生成分析师级别的报告。然而，现有大多数深度研究智能体仅在问答基准测试中得到验证，而针对生成综合性报告的研究仍被忽视。更严重的是，当前报告综合的基准测试存在任务复杂度高与评估指标主观性强的问题——这既无法反映用户真实需求，也限制了生成报告的实际效用。为填补这些空白，我们提出了细粒度深度研究基准测试，这是一个包含100项人工策划研究任务的增强型基准，涵盖419项结构化检查清单条目，用于标准化报告结构、分析深度与事实依据。基于主流深度研究智能体生成的约1000份报告，我们进一步提出了深度研究失败分类法，这是首个针对深度研究智能体的失败类型学体系。该分类法包含推理、检索与生成三大维度下的14种细粒度失败模式，并基于扎根理论构建，采用人机协同标注与标注者间信度验证。实验结果表明，当前深度研究智能体的主要瓶颈不在于任务理解，而在于证据整合、事实核查以及具备推理韧性的规划能力。",
    "url": "https://huggingface.co/papers/2512.01948",
    "arxiv_url": "https://arxiv.org/abs/2512.01948"
  },
  {
    "title": "What about gravity in video generation? Post-Training Newton's Laws with Verifiable Rewards",
    "summary": "Recent video diffusion models can synthesize visually compelling clips, yet often violate basic physical laws-objects float, accelerations drift, and collisions behave inconsistently-revealing a persistent gap between visual realism and physical realism. We propose NewtonRewards, the first physics-grounded post-training framework for video generation based on verifiable rewards. Instead of relying on human or VLM feedback, NewtonRewards extracts measurable proxies from generated videos using frozen utility models: optical flow serves as a proxy for velocity, while high-level appearance features serve as a proxy for mass. These proxies enable explicit enforcement of Newtonian structure through two complementary rewards: a Newtonian kinematic constraint enforcing constant-acceleration dynamics, and a mass conservation reward preventing trivial, degenerate solutions. We evaluate NewtonRewards on five Newtonian Motion Primitives (free fall, horizontal/parabolic throw, and ramp sliding down/up) using our newly constructed large-scale benchmark, NewtonBench-60K. Across all primitives in visual and physics metrics, NewtonRewards consistently improves physical plausibility, motion smoothness, and temporal coherence over prior post-training methods. It further maintains strong performance under out-of-distribution shifts in height, speed, and friction. Our results show that physics-grounded verifiable rewards offer a scalable path toward physics-aware video generation.",
    "translation": "标题：视频生成中的重力问题如何解决？基于可验证奖励的后训练牛顿定律应用\n\n摘要：当前视频扩散模型虽能合成视觉上逼真的片段，却常违反基本物理定律——物体漂浮、加速度漂移、碰撞行为不一致——这揭示了视觉真实感与物理真实感之间的持续差距。本文提出NewtonRewards，首个基于可验证奖励的物理基础视频生成后训练框架。该方法不依赖人类或视觉语言模型反馈，而是通过冻结的效用模型从生成视频中提取可测量代理：光流作为速度代理，高层外观特征作为质量代理。这些代理通过两种互补奖励实现牛顿力学结构的显式强化：牛顿运动学约束确保恒定加速度动力学，质量守恒奖励防止平凡退化解。我们在新构建的大规模基准数据集NewtonBench-60K上，针对五种牛顿运动基本形式（自由落体、水平/抛物线抛射、斜面下滑/上滑）进行评估。在视觉与物理指标上，NewtonRewards在所有基本形式中均持续提升物理合理性、运动平滑度与时间连贯性，优于现有后训练方法。该框架在高度、速度与摩擦力的分布外变化下仍保持强劲性能。实验结果表明，基于物理的可验证奖励为物理感知的视频生成提供了可扩展的路径。\n\n摘要：当前视频扩散模型虽能合成视觉上逼真的片段，却常违反基本物理定律——物体漂浮、加速度漂移、碰撞行为不一致——这揭示了视觉真实感与物理真实感之间的持续差距。本文提出NewtonRewards，首个基于可验证奖励的物理基础视频生成后训练框架。该方法不依赖人类或视觉语言模型反馈，而是通过冻结的效用模型从生成视频中提取可测量代理：光流作为速度代理，高层外观特征作为质量代理。这些代理通过两种互补奖励实现牛顿力学结构的显式强化：牛顿运动学约束确保恒定加速度动力学，质量守恒奖励防止平凡退化解。我们在新构建的大规模基准数据集NewtonBench-60K上，针对五种牛顿运动基本形式（自由落体、水平/抛物线抛射、斜面下滑/上滑）进行评估。在视觉与物理指标上，NewtonRewards在所有基本形式中均持续提升物理合理性、运动平滑度与时间连贯性，优于现有后训练方法。该框架在高度、速度与摩擦力的分布外变化下仍保持强劲性能。实验结果表明，基于物理的可验证奖励为物理感知的视频生成提供了可扩展的路径。",
    "url": "https://huggingface.co/papers/2512.00425",
    "arxiv_url": "https://arxiv.org/abs/2512.00425"
  },
  {
    "title": "Infinity-RoPE: Action-Controllable Infinite Video Generation Emerges From Autoregressive Self-Rollout",
    "summary": "Current autoregressive video diffusion models are constrained by three core bottlenecks: (i) the finite temporal horizon imposed by the base model's 3D Rotary Positional Embedding (3D-RoPE), (ii) slow prompt responsiveness in maintaining fine-grained action control during long-form rollouts, and (iii) the inability to realize discontinuous cinematic transitions within a single generation stream. We introduce infty-RoPE, a unified inference-time framework that addresses all three limitations through three interconnected components: Block-Relativistic RoPE, KV Flush, and RoPE Cut. Block-Relativistic RoPE reformulates temporal encoding as a moving local reference frame, where each newly generated latent block is rotated relative to the base model's maximum frame horizon while earlier blocks are rotated backward to preserve relative temporal geometry. This relativistic formulation eliminates fixed temporal positions, enabling continuous video generation far beyond the base positional limits. To obtain fine-grained action control without re-encoding, KV Flush renews the KV cache by retaining only two latent frames, the global sink and the last generated latent frame, thereby ensuring immediate prompt responsiveness. Finally, RoPE Cut introduces controlled discontinuities in temporal RoPE coordinates, enabling multi-cut scene transitions within a single continuous rollout. Together, these components establish infty-RoPE as a training-free foundation for infinite-horizon, controllable, and cinematic video diffusion. Comprehensive experiments show that infty-RoPE consistently surpasses previous autoregressive models in overall VBench scores.",
    "translation": "标题：Infinity-RoPE：自回归自展开中涌现的动作可控无限视频生成\n\n摘要：当前的自回归视频扩散模型受限于三个核心瓶颈：(i) 基础模型的三维旋转位置编码（3D-RoPE）所施加的有限时间跨度，(ii) 在长序列展开过程中维持细粒度动作控制的缓慢提示响应性，以及(iii) 无法在单一生成流中实现非连续的影视化转场。我们提出了Infinity-RoPE，一个统一的推理时框架，通过三个相互关联的组件——块相对论RoPE、KV刷新与RoPE截断——共同解决了上述所有限制。块相对论RoPE将时间编码重新表述为一个移动的局部参考系，其中每个新生成的潜在块相对于基础模型的最大帧跨度进行旋转，而较早的块则向后旋转以保持相对的时间几何关系。这种相对论表述消除了固定的时间位置，使得视频生成能够持续进行，远超基础位置编码的限制。为了在不重新编码的情况下实现细粒度的动作控制，KV刷新通过仅保留两个潜在帧（全局汇点与最后生成的潜在帧）来更新KV缓存，从而确保即时的提示响应性。最后，RoPE截断在时间RoPE坐标中引入受控的不连续性，使得在单一连续展开中能够实现多镜头场景转场。这些组件共同构成了Infinity-RoPE，作为一个无需训练的基础框架，支持无限跨度、可控且具有影视化效果的视频扩散。综合实验表明，Infinity-RoPE在整体VBench评分上持续超越先前的自回归模型。",
    "url": "https://huggingface.co/papers/2511.20649",
    "arxiv_url": "https://arxiv.org/abs/2511.20649"
  },
  {
    "title": "The Consistency Critic: Correcting Inconsistencies in Generated Images via Reference-Guided Attentive Alignment",
    "summary": "Previous works have explored various customized generation tasks given a reference image, but they still face limitations in generating consistent fine-grained details. In this paper, our aim is to solve the inconsistency problem of generated images by applying a reference-guided post-editing approach and present our ImageCritic. We first construct a dataset of reference-degraded-target triplets obtained via VLM-based selection and explicit degradation, which effectively simulates the common inaccuracies or inconsistencies observed in existing generation models. Furthermore, building on a thorough examination of the model's attention mechanisms and intrinsic representations, we accordingly devise an attention alignment loss and a detail encoder to precisely rectify inconsistencies. ImageCritic can be integrated into an agent framework to automatically detect inconsistencies and correct them with multi-round and local editing in complex scenarios. Extensive experiments demonstrate that ImageCritic can effectively resolve detail-related issues in various customized generation scenarios, providing significant improvements over existing methods.",
    "translation": "标题：一致性评判器：通过参考引导的注意力对齐修正生成图像中的不一致性\n\n摘要：先前研究已探索了基于参考图像的多种定制化生成任务，但这些方法在生成具有一致性的细粒度细节方面仍存在局限。本文旨在通过采用参考引导的后编辑方法解决生成图像的不一致性问题，并提出我们的ImageCritic系统。我们首先通过基于视觉语言模型的选择与显式退化处理，构建了包含参考图像-退化图像-目标图像的三元组数据集，该数据集有效模拟了现有生成模型中常见的细节偏差与不一致现象。在此基础上，通过对模型注意力机制与内在表征的深入分析，我们相应设计了注意力对齐损失函数与细节编码器，以实现对不一致区域的精准修正。ImageCritic可集成至智能体框架中，在复杂场景下通过多轮局部编辑自动检测并修正不一致区域。大量实验表明，ImageCritic能在多种定制化生成场景中有效解决细节相关问题，相较于现有方法取得了显著提升。",
    "url": "https://huggingface.co/papers/2511.20614",
    "arxiv_url": "https://arxiv.org/abs/2511.20614"
  },
  {
    "title": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models",
    "summary": "Unified multimodal models (UMMs) aim to jointly perform multimodal understanding and generation within a single framework. We present TUNA, a native UMM that builds a unified continuous visual representation by cascading a VAE encoder with a representation encoder. This unified representation space allows end-to-end processing of images and videos for both understanding and generation tasks. Compared to prior UMMs with decoupled representations, TUNA's unified visual space avoids representation format mismatches introduced by separate encoders, outperforming decoupled alternatives in both understanding and generation. Moreover, we observe that stronger pretrained representation encoders consistently yield better performance across all multimodal tasks, highlighting the importance of the representation encoder. Finally, in this unified setting, jointly training on both understanding and generation data allows the two tasks to benefit from each other rather than interfere. Our extensive experiments on multimodal understanding and generation benchmarks show that TUNA achieves state-of-the-art results in image and video understanding, image and video generation, and image editing, demonstrating the effectiveness and scalability of its unified representation design.",
    "translation": "标题：TUNA：面向原生统一多模态模型的统一视觉表征驯化\n\n摘要：统一多模态模型旨在单一框架内联合实现多模态理解与生成任务。本文提出TUNA——一种原生统一多模态模型，通过级联变分自编码器编码器与表征编码器构建统一的连续视觉表征空间。该统一表征空间支持对图像与视频进行端到端的理解与生成处理。相较于采用解耦表征的现有统一多模态模型，TUNA的统一视觉空间避免了因独立编码器引入的表征格式失配问题，在理解与生成任务上均优于解耦方案。此外，我们发现更强的预训练表征编码器能在所有多模态任务中持续提升性能，这凸显了表征编码器的重要性。最终，在此统一框架下，联合训练理解与生成数据能使两项任务相互促进而非相互干扰。我们在多模态理解与生成基准测试中的大量实验表明，TUNA在图像/视频理解、图像/视频生成及图像编辑任务上均取得最先进性能，验证了其统一表征设计的有效性与可扩展性。",
    "url": "https://huggingface.co/papers/2512.02014",
    "arxiv_url": "https://arxiv.org/abs/2512.02014"
  },
  {
    "title": "LFM2 Technical Report",
    "summary": "We present LFM2, a family of Liquid Foundation Models designed for efficient on-device deployment and strong task capabilities. Using hardware-in-the-loop architecture search under edge latency and memory constraints, we obtain a compact hybrid backbone that combines gated short convolutions with a small number of grouped query attention blocks, delivering up to 2x faster prefill and decode on CPUs compared to similarly sized models. The LFM2 family covers 350M-8.3B parameters, including dense models (350M, 700M, 1.2B, 2.6B) and a mixture-of-experts variant (8.3B total, 1.5B active), all with 32K context length. LFM2's training pipeline includes a tempered, decoupled Top-K knowledge distillation objective that avoids support mismatch; curriculum learning with difficulty-ordered data; and a three-stage post-training recipe of supervised fine-tuning, length-normalized preference optimization, and model merging. Pre-trained on 10-12T tokens, LFM2 models achieve strong results across diverse benchmarks; for example, LFM2-2.6B reaches 79.56% on IFEval and 82.41% on GSM8K. We further build multimodal and retrieval variants: LFM2-VL for vision-language tasks, LFM2-Audio for speech, and LFM2-ColBERT for retrieval. LFM2-VL supports tunable accuracy-latency tradeoffs via token-efficient visual processing, while LFM2-Audio separates audio input and output pathways to enable real-time speech-to-speech interaction competitive with models 3x larger. LFM2-ColBERT provides a low-latency encoder for queries and documents, enabling high-performance retrieval across multiple languages. All models are released with open weights and deployment packages for ExecuTorch, llama.cpp, and vLLM, making LFM2 a practical base for edge applications that need fast, memory-efficient inference and strong task capabilities.",
    "translation": "标题：LFM2技术报告\n\n摘要：本文提出LFM2系列液态基础模型，该模型家族专为高效的设备端部署与强大的任务能力而设计。通过在边缘延迟和内存约束下进行硬件在环架构搜索，我们获得了一个紧凑的混合主干网络，该网络将门控短卷积与少量分组查询注意力模块相结合，在CPU上相比同等规模模型实现了最高2倍的预填充和解码速度提升。LFM2系列涵盖3.5亿至83亿参数规模，包括稠密模型（3.5亿/7亿/12亿/26亿参数）和专家混合变体（83亿总参数/15亿激活参数），所有模型均支持32K上下文长度。LFM2的训练流程包含：采用避免支持失配的温和解耦Top-K知识蒸馏目标、基于难度排序数据的课程学习，以及包含监督微调、长度归一化偏好优化和模型融合的三阶段后训练方案。基于10-12万亿token预训练的LFM2模型在多样化基准测试中表现优异，例如LFM2-26B在IFEval达到79.56%，在GSM8K达到82.41%。我们进一步构建了多模态与检索变体：面向视觉语言任务的LFM2-VL、面向语音的LFM2-Audio，以及面向检索的LFM2-ColBERT。LFM2-VL通过令牌高效的视觉处理支持可调节的精度-延迟权衡；LFM2-Audio分离音频输入输出路径，可实现与三倍规模模型相媲美的实时语音交互；LFM2-ColBERT提供面向查询和文档的低延迟编码器，支持跨语言高性能检索。所有模型均以开放权重形式发布，并提供适用于ExecuTorch、llama.cpp和vLLM的部署套件，使LFM2成为需要快速、内存高效推理与强大任务能力的边缘应用的实用基础框架。",
    "url": "https://huggingface.co/papers/2511.23404",
    "arxiv_url": "https://arxiv.org/abs/2511.23404"
  },
  {
    "title": "Wikontic: Constructing Wikidata-Aligned, Ontology-Aware Knowledge Graphs with Large Language Models",
    "summary": "Knowledge graphs (KGs) provide structured, verifiable grounding for large language models (LLMs), but current LLM-based systems commonly use KGs as auxiliary structures for text retrieval, leaving their intrinsic quality underexplored. In this work, we propose Wikontic, a multi-stage pipeline that constructs KGs from open-domain text by extracting candidate triplets with qualifiers, enforcing Wikidata-based type and relation constraints, and normalizing entities to reduce duplication. The resulting KGs are compact, ontology-consistent, and well-connected; on MuSiQue, the correct answer entity appears in 96% of generated triplets. On HotpotQA, our triplets-only setup achieves 76.0 F1, and on MuSiQue 59.8 F1, matching or surpassing several retrieval-augmented generation baselines that still require textual context. In addition, Wikontic attains state-of-the-art information-retention performance on the MINE-1 benchmark (86%), outperforming prior KG construction methods. Wikontic is also efficient at build time: KG construction uses less than 1,000 output tokens, about 3times fewer than AriGraph and <1/20 of GraphRAG. The proposed pipeline enhances the quality of the generated KG and offers a scalable solution for leveraging structured knowledge in LLMs.",
    "translation": "标题：Wikontic：利用大语言模型构建与Wikidata对齐、本体感知的知识图谱\n\n摘要：知识图谱为大型语言模型提供了结构化、可验证的基础，但当前基于大语言模型的系统通常仅将知识图谱作为文本检索的辅助结构，其内在质量尚未得到充分探索。本研究提出Wikontic，一个多阶段处理流程，能够从开放域文本中构建知识图谱。该流程通过提取带限定条件的候选三元组、实施基于Wikidata的类型与关系约束，并对实体进行规范化以减少重复，最终生成紧凑、符合本体论且连接性良好的知识图谱。在MuSiQue数据集上，正确答案实体出现在96%的生成三元组中。在HotpotQA任务中，仅使用三元组的设置取得了76.0的F1分数，在MuSiQue上达到59.8 F1，匹配或超越了多个仍需依赖文本上下文的检索增强生成基线方法。此外，Wikontic在MINE-1基准测试中取得了最先进的信息保留性能（86%），优于先前的知识图谱构建方法。Wikontic在构建阶段也表现出高效性：知识图谱构建消耗少于1,000个输出标记，约为AriGraph的三分之一，不足GraphRAG的二十分之一。所提出的流程提升了生成知识图谱的质量，并为在大语言模型中利用结构化知识提供了可扩展的解决方案。",
    "url": "https://huggingface.co/papers/2512.00590",
    "arxiv_url": "https://arxiv.org/abs/2512.00590"
  },
  {
    "title": "Rectifying LLM Thought from Lens of Optimization",
    "summary": "Recent advancements in large language models (LLMs) have been driven by their emergent reasoning capabilities, particularly through long chain-of-thought (CoT) prompting, which enables thorough exploration and deliberation. Despite these advances, long-CoT LLMs often exhibit suboptimal reasoning behaviors, such as overthinking and excessively protracted reasoning chains, which can impair performance. In this paper, we analyze reasoning processes through an optimization lens, framing CoT as a gradient descent procedure where each reasoning step constitutes an update toward problem resolution. Building on this perspective, we introduce RePro (Rectifying Process-level Reward), a novel approach to refine LLM reasoning during post-training. RePro defines a surrogate objective function to assess the optimization process underlying CoT, utilizing a dual scoring mechanism to quantify its intensity and stability. These scores are aggregated into a composite process-level reward, seamlessly integrated into reinforcement learning with verifiable rewards (RLVR) pipelines to optimize LLMs. Extensive experiments across multiple reinforcement learning algorithms and diverse LLMs, evaluated on benchmarks spanning mathematics, science, and coding, demonstrate that RePro consistently enhances reasoning performance and mitigates suboptimal reasoning behaviors.",
    "translation": "标题：基于优化视角的大语言模型思维校正\n\n摘要：大语言模型（LLM）的最新进展主要得益于其涌现的推理能力，尤其是通过长链思维（CoT）提示实现的深入探索与思考。尽管取得这些进步，采用长链CoT的LLM仍常表现出次优推理行为，例如过度思考与推理链过长等问题，这可能损害模型性能。本文从优化视角分析推理过程，将CoT框架化为梯度下降过程，其中每个推理步骤构成向问题解决的迭代更新。基于此视角，我们提出RePro（校正过程级奖励）方法，这是一种在训练后阶段优化LLM推理的新途径。RePro通过定义代理目标函数来评估CoT背后的优化过程，采用双重评分机制量化其强度与稳定性。这些评分被整合为复合过程级奖励，无缝集成至带可验证奖励的强化学习（RLVR）流程中以优化LLM。通过在数学、科学与编程等多领域基准测试上，对多种强化学习算法及不同LLM开展的大量实验表明，RePro能持续提升推理性能并有效缓解次优推理行为。",
    "url": "https://huggingface.co/papers/2512.01925",
    "arxiv_url": "https://arxiv.org/abs/2512.01925"
  },
  {
    "title": "Flash-DMD: Towards High-Fidelity Few-Step Image Generation with Efficient Distillation and Joint Reinforcement Learning",
    "summary": "Diffusion Models have emerged as a leading class of generative models, yet their iterative sampling process remains computationally expensive. Timestep distillation is a promising technique to accelerate generation, but it often requires extensive training and leads to image quality degradation. Furthermore, fine-tuning these distilled models for specific objectives, such as aesthetic appeal or user preference, using Reinforcement Learning (RL) is notoriously unstable and easily falls into reward hacking. In this work, we introduce Flash-DMD, a novel framework that enables fast convergence with distillation and joint RL-based refinement. Specifically, we first propose an efficient timestep-aware distillation strategy that significantly reduces training cost with enhanced realism, outperforming DMD2 with only 2.1% its training cost. Second, we introduce a joint training scheme where the model is fine-tuned with an RL objective while the timestep distillation training continues simultaneously. We demonstrate that the stable, well-defined loss from the ongoing distillation acts as a powerful regularizer, effectively stabilizing the RL training process and preventing policy collapse. Extensive experiments on score-based and flow matching models show that our proposed Flash-DMD not only converges significantly faster but also achieves state-of-the-art generation quality in the few-step sampling regime, outperforming existing methods in visual quality, human preference, and text-image alignment metrics. Our work presents an effective paradigm for training efficient, high-fidelity, and stable generative models. Codes are coming soon.",
    "translation": "标题：Flash-DMD：通过高效蒸馏与联合强化学习实现高保真少步图像生成\n\n摘要：扩散模型已成为生成模型中的主导类别，但其迭代采样过程计算成本高昂。时间步蒸馏是一种有前景的加速生成技术，但通常需要大量训练并导致图像质量下降。此外，使用强化学习针对特定目标（如审美吸引力或用户偏好）对这些蒸馏模型进行微调，存在众所周知的训练不稳定问题，且极易陷入奖励破解。本文提出Flash-DMD，一种通过蒸馏实现快速收敛并结合基于强化学习的联合优化的新型框架。具体而言，我们首先提出一种高效的时间步感知蒸馏策略，该策略在显著降低训练成本的同时增强了生成真实性，仅需DMD2模型2.1%的训练成本即可实现更优性能。其次，我们引入一种联合训练方案，在模型通过强化学习目标进行微调的同时，时间步蒸馏训练持续并行进行。我们证明，来自持续蒸馏过程的稳定、定义明确的损失函数可作为强大的正则化器，有效稳定强化学习训练过程并防止策略崩溃。在基于分数和流匹配模型上的大量实验表明，我们所提出的Flash-DMD不仅收敛速度显著加快，而且在少步采样机制下实现了最先进的生成质量，在视觉质量、人类偏好和图文对齐指标上均优于现有方法。本研究为训练高效、高保真且稳定的生成模型提供了一种有效范式。代码即将发布。",
    "url": "https://huggingface.co/papers/2511.20549",
    "arxiv_url": "https://arxiv.org/abs/2511.20549"
  },
  {
    "title": "VLASH: Real-Time VLAs via Future-State-Aware Asynchronous Inference",
    "summary": "Vision-Language-Action models (VLAs) are becoming increasingly capable across diverse robotic tasks. However, their real-world deployment remains slow and inefficient: demonstration videos are often sped up by 5-10x to appear smooth, with noticeable action stalls and delayed reactions to environmental changes. Asynchronous inference offers a promising solution to achieve continuous and low-latency control by enabling robots to execute actions and perform inference simultaneously. However, because the robot and environment continue to evolve during inference, a temporal misalignment arises between the prediction and execution intervals. This leads to significant action instability, while existing methods either degrade accuracy or introduce runtime overhead to mitigate it. We propose VLASH, a general asynchronous inference framework for VLAs that delivers smooth, accurate, and fast reaction control without additional overhead or architectural changes. VLASH estimates the future execution-time state by rolling the robot state forward with the previously generated action chunk, thereby bridging the gap between prediction and execution. Experiments show that VLASH achieves up to 2.03x speedup and reduces reaction latency by up to 17.4x compared to synchronous inference while fully preserving the original accuracy. Moreover, it empowers VLAs to handle fast-reaction, high-precision tasks such as playing ping-pong and playing whack-a-mole, where traditional synchronous inference fails. Code is available at https://github.com/mit-han-lab/vlash",
    "translation": "标题：VLASH：通过未来状态感知异步推理实现实时视觉语言动作模型\n\n摘要：视觉-语言-动作模型（VLAs）在多样化机器人任务中正展现出日益强大的能力。然而，其在实际部署中仍存在延迟高、效率低的问题：演示视频常需加速5-10倍才能呈现流畅效果，且存在明显的动作停滞与环境响应延迟。异步推理通过使机器人能够同时执行动作与进行推理，为实现连续低延迟控制提供了可行方案。但受推理过程中机器人与环境持续演化的影响，预测区间与执行区间会产生时序错位，导致显著的动作失稳现象。现有方法往往以牺牲准确性或增加运行时开销为代价来缓解该问题。本文提出VLASH——一种通用的VLA异步推理框架，无需额外开销或架构修改即可实现平滑、准确、快速的反应控制。VLASH通过将机器人状态与先前生成的动作片段进行前向推演，预估未来执行时刻的状态，从而弥合预测与执行间的时序鸿沟。实验表明，相较于同步推理，VLASH在完全保持原始准确性的同时，最高可实现2.03倍的加速效果，并将反应延迟降低达17.4倍。此外，该框架使VLAs能够胜任乒乓球对战、打地鼠等需要快速反应与高精度操控的任务，而传统同步推理方法在此类任务中均告失效。代码已开源：https://github.com/mit-han-lab/vlash",
    "url": "https://huggingface.co/papers/2512.01031",
    "arxiv_url": "https://arxiv.org/abs/2512.01031"
  },
  {
    "title": "GR-RL: Going Dexterous and Precise for Long-Horizon Robotic Manipulation",
    "summary": "We present GR-RL, a robotic learning framework that turns a generalist vision-language-action (VLA) policy into a highly capable specialist for long-horizon dexterous manipulation. Assuming the optimality of human demonstrations is core to existing VLA policies. However, we claim that in highly dexterous and precise manipulation tasks, human demonstrations are noisy and suboptimal. GR-RL proposes a multi-stage training pipeline that filters, augments, and reinforces the demonstrations by reinforcement learning. First, GR-RL learns a vision-language-conditioned task progress, filters the demonstration trajectories, and only keeps the transitions that contribute positively to the progress. Specifically, we show that by directly applying offline RL with sparse reward, the resulting Q-values can be treated as a robust progress function. Next, we introduce morphological symmetry augmentation that greatly improves the generalization and performance of GR-RL. Lastly, to better align the VLA policy with its deployment behaviors for high-precision control, we perform online RL by learning a latent space noise predictor. With this pipeline, GR-RL is, to our knowledge, the first learning-based policy that can autonomously lace up a shoe by threading shoelaces through multiple eyelets with an 83.3% success rate, a task requiring long-horizon reasoning, millimeter-level precision, and compliant soft-body interaction. We hope GR-RL provides a step toward enabling generalist robot foundations models to specialize into reliable real-world experts.",
    "translation": "标题：GR-RL：面向长时序机器人灵巧操控的精细控制框架\n\n摘要：本文提出GR-RL——一种机器人学习框架，可将通用视觉-语言-动作策略转化为擅长长时序灵巧操控的专用策略。现有视觉-语言-动作策略的核心假设是人类演示具有最优性，但我们指出在高度灵巧且需精密控制的任务中，人类演示存在噪声且非最优。GR-RL设计了一个多阶段训练流程，通过强化学习对演示数据进行筛选、增强与优化：首先学习视觉-语言条件化的任务进度函数，筛选演示轨迹并仅保留对进度有积极贡献的状态转移。具体而言，我们证明通过直接应用稀疏奖励的离线强化学习，所得Q值可作为鲁棒的进度函数。其次，引入形态对称增强方法，显著提升GR-RL的泛化能力与性能。最后，为更好地对齐视觉-语言-动作策略与高精度控制场景下的部署行为，我们通过隐空间噪声预测器进行在线强化学习。该框架使GR-RL成为首个基于学习的策略，能够以83.3%的成功率自主完成穿鞋带任务——该任务需贯穿多个鞋孔，要求长时序推理、毫米级精度及柔性体顺应交互。我们希望GR-RL能为通用机器人基础模型向可靠现实场景专家系统的转化提供技术路径。",
    "url": "https://huggingface.co/papers/2512.01801",
    "arxiv_url": "https://arxiv.org/abs/2512.01801"
  },
  {
    "title": "InternVideo-Next: Towards General Video Foundation Models without Video-Text Supervision",
    "summary": "Large-scale video-text pretraining achieves strong performance but depends on noisy, synthetic captions with limited semantic coverage, often overlooking implicit world knowledge such as object motion, 3D geometry, and physical cues. In contrast, masked video modeling (MVM) directly exploits spatiotemporal structures but trails text-supervised methods on general tasks. We find this gap arises from overlooked architectural issues: pixel-level reconstruction struggles with convergence and its low-level requirement often conflicts with semantics, while latent prediction often encourages shortcut learning. To address these, we disentangle the traditional encoder-decoder design into an Encoder-Predictor-Decoder (EPD) framework, where the predictor acts as a latent world model, and propose InternVideo-Next, a two-stage pretraining scheme that builds a semantically consistent yet detail-preserving latent space for this world model. First, conventional linear decoder in pixel MVM enforces the predictor output latent to be linearly projected to, thus separable in pixel space, causing the conflict with semantic abstraction. Our Stage 1 proposes a conditional diffusion decoder and injects reliable image-level semantic priors to enhance semantics and convergence, thus bridging pixel-level fidelity with high-level semantic abstraction. Stage 2 further learns world knowledge by predicting frozen Stage 1 targets within this space, mitigating shortcut learning. Trained on public, unlabeled videos, InternVideo-Next achieves state-of-the-art results across benchmarks and provides a scalable path toward general video representation learning.",
    "translation": "标题：InternVideo-Next：迈向无需视频-文本监督的通用视频基础模型\n\n摘要：大规模视频-文本预训练虽能实现强劲性能，但依赖于语义覆盖有限且带有噪声的合成描述，往往忽略了物体运动、三维几何与物理线索等隐含世界知识。相比之下，掩码视频建模方法直接利用时空结构，但在通用任务上仍落后于文本监督方法。我们发现这一差距源于被忽视的架构问题：像素级重建面临收敛困难，其低层次要求常与语义目标冲突；而潜在特征预测则易引发捷径学习。为解决这些问题，我们将传统编码器-解码器架构解耦为编码器-预测器-解码器框架，其中预测器充当潜在世界模型，并提出InternVideo-Next——一个两阶段预训练方案，为该世界模型构建语义一致且细节保留的潜在空间。首先，像素级掩码视频建模中传统的线性解码器强制预测器输出特征需线性映射至像素空间，导致其与语义抽象目标产生冲突。我们的第一阶段提出条件扩散解码器，并注入可靠的图像级语义先验以增强语义表达与收敛性，从而在像素级保真度与高层语义抽象间建立桥梁。第二阶段通过在该空间内预测冻结的第一阶段目标来进一步学习世界知识，有效缓解捷径学习问题。在公开无标注视频数据上训练的InternVideo-Next在多项基准测试中取得最先进成果，为通用视频表征学习提供了可扩展的技术路径。",
    "url": "https://huggingface.co/papers/2512.01342",
    "arxiv_url": "https://arxiv.org/abs/2512.01342"
  },
  {
    "title": "SpeContext: Enabling Efficient Long-context Reasoning with Speculative Context Sparsity in LLMs",
    "summary": "In this paper, we point out that the objective of the retrieval algorithms is to align with the LLM, which is similar to the objective of knowledge distillation in LLMs. We analyze the similarity in information focus between the distilled language model(DLM) and the original LLM from the perspective of information theory, and thus propose a novel paradigm that leverages a DLM as the retrieval algorithm. Based on the insight, we present SpeContext, an algorithm and system co-design for long-context reasoning. (1) At the algorithm level, SpeContext proposes lightweight retrieval head based on the head-level attention weights of DLM, achieving > 90% parameters reduction by pruning the redundancy. (2) At the system level, SpeContext designs an asynchronous prefetch dataflow via the elastic loading strategy, effectively overlapping KV cache retrieval with the LLM computation. (3) At the compilation level, SpeContext constructs the theoretical memory model and implements an adaptive memory management system to achieve acceleration by maximizing GPU memory utilization. We deploy and evaluate SpeContext in two resourceconstrained environments, cloud and edge. Extensive experiments show that, compared with the Huggingface framework, SpeContext achieves up to 24.89x throughput improvement in cloud and 10.06x speedup in edge with negligible accuracy loss, pushing the Pareto frontier of accuracy and throughput.",
    "translation": "标题：SpeContext：通过推测性上下文稀疏性实现大语言模型的高效长上下文推理\n\n摘要：本文指出，检索算法的目标与大语言模型（LLM）对齐，这与大语言模型中知识蒸馏的目标相似。我们从信息论角度分析了蒸馏语言模型（DLM）与原始大语言模型在信息关注点上的相似性，进而提出一种利用DLM作为检索算法的新范式。基于这一洞见，我们提出了SpeContext——一种面向长上下文推理的算法与系统协同设计框架。（1）在算法层面，SpeContext基于DLM的头部注意力权重设计了轻量级检索头，通过剪枝冗余参数实现了超过90%的参数压缩。（2）在系统层面，SpeContext通过弹性加载策略设计了异步预取数据流，有效实现了KV缓存检索与大语言模型计算的重叠执行。（3）在编译层面，SpeContext构建了理论内存模型并实现了自适应内存管理系统，通过最大化GPU内存利用率实现加速。我们在云端和边缘两种资源受限环境中部署并评估了SpeContext。大量实验表明，与Huggingface框架相比，SpeContext在云端实现了最高24.89倍的吞吐量提升，在边缘端实现了10.06倍的加速，且精度损失可忽略不计，从而推动了精度与吞吐量的帕累托前沿边界。",
    "url": "https://huggingface.co/papers/2512.00722",
    "arxiv_url": "https://arxiv.org/abs/2512.00722"
  },
  {
    "title": "Flow Straighter and Faster: Efficient One-Step Generative Modeling via MeanFlow on Rectified Trajectories",
    "summary": "Flow-based generative models have recently demonstrated strong performance, yet sampling typically relies on expensive numerical integration of ordinary differential equations (ODEs). Rectified Flow enables one-step sampling by learning nearly straight probability paths, but achieving such straightness requires multiple computationally intensive reflow iterations. MeanFlow achieves one-step generation by directly modeling the average velocity over time; however, when trained on highly curved flows, it suffers from slow convergence and noisy supervision. To address these limitations, we propose Rectified MeanFlow, a framework that models the mean velocity field along the rectified trajectory using only a single reflow step. This eliminates the need for perfectly straightened trajectories while enabling efficient training. Furthermore, we introduce a simple yet effective truncation heuristic that aims to reduce residual curvature and further improve performance. Extensive experiments on ImageNet at 64, 256, and 512 resolutions show that Re-MeanFlow consistently outperforms prior one-step flow distillation and Rectified Flow methods in both sample quality and training efficiency. Code is available at https://github.com/Xinxi-Zhang/Re-MeanFlow.",
    "translation": "标题：流更直更快：基于整流轨迹MeanFlow的高效一步生成建模\n\n摘要：基于流的生成模型近期展现出强大性能，但其采样过程通常依赖于昂贵的常微分方程数值积分。整流流方法通过学习近似直线的概率路径实现一步采样，但获得这种直线性需要多次计算密集的整流迭代。MeanFlow通过直接建模时间平均速度实现一步生成，然而在高度弯曲的流上训练时，其收敛速度缓慢且监督信号存在噪声。为克服这些局限，我们提出整流平均流框架，该框架仅需单次整流步骤即可沿整流轨迹建模平均速度场。这种方法在避免完美直线轨迹需求的同时实现了高效训练。此外，我们引入一种简单有效的截断启发式方法，旨在减少残余曲率并进一步提升性能。在64×64、256×256和512×512分辨率ImageNet数据集上的大量实验表明，Re-MeanFlow在样本质量和训练效率方面均优于现有的一步流蒸馏方法与整流流方法。代码已发布于https://github.com/Xinxi-Zhang/Re-MeanFlow。",
    "url": "https://huggingface.co/papers/2511.23342",
    "arxiv_url": "https://arxiv.org/abs/2511.23342"
  },
  {
    "title": "Accelerating Streaming Video Large Language Models via Hierarchical Token Compression",
    "summary": "Streaming Video Large Language Models (VideoLLMs) have demonstrated impressive performance across various video understanding tasks, but they face significant challenges in real-time deployment due to the high computational cost of processing dense visual tokens from continuous video streams. In streaming video scenarios, the primary bottleneck lies in the Vision Transformer (ViT) encoding stage, where redundant processing of temporally similar frames leads to inefficiency. Additionally, inflated token sequences during LLM pre-filling further exacerbate latency and memory overhead. To address these challenges, we propose Streaming Token Compression (STC), a plug-and-play hierarchical framework that seamlessly integrates into existing streaming VideoLLMs, optimizing both ViT encoding and LLM pre-filling stages to accelerate processing. STC introduces two token-level accelerators: STC-Cacher, which reduces ViT encoding overhead by caching and reusing features from temporally similar frames, and STC-Pruner, which compresses the visual token sequence before it enters the LLM, preserving only the most salient tokens based on both spatial and temporal relevance. Extensive experiments on four baseline streaming VideoLLMs across five benchmarks demonstrate that STC outperforms other compression methods. Notably, STC retains up to 99\\% of accuracy on the ReKV framework while reducing ViT encoding latency and LLM pre-filling latency by 24.5\\% and 45.3\\%.",
    "translation": "标题：基于分层令牌压缩的流式视频大语言模型加速方法\n\n摘要：流式视频大语言模型（VideoLLMs）在各种视频理解任务中展现出卓越性能，但由于处理连续视频流中密集视觉令牌的高计算成本，其在实时部署中面临重大挑战。在流式视频场景中，主要瓶颈在于视觉变换器（ViT）编码阶段——对时间相似帧的冗余处理导致效率低下。此外，大语言模型预填充阶段膨胀的令牌序列进一步加剧了延迟和内存开销。为应对这些挑战，我们提出流式令牌压缩（STC），一种即插即用的分层框架，可无缝集成到现有流式VideoLLMs中，通过优化ViT编码和大语言模型预填充阶段实现处理加速。STC引入两个令牌级加速器：STC-Cacher通过缓存并重用时间相似帧的特征降低ViT编码开销；STC-Pruner在视觉令牌序列输入大语言模型前对其进行压缩，基于时空相关性仅保留最显著的令牌。在五个基准测试中对四种主流流式VideoLLMs的广泛实验表明，STC性能优于其他压缩方法。值得注意的是，STC在ReKV框架上保持高达99%的准确率，同时将ViT编码延迟和大语言模型预填充延迟分别降低24.5%和45.3%。",
    "url": "https://huggingface.co/papers/2512.00891",
    "arxiv_url": "https://arxiv.org/abs/2512.00891"
  },
  {
    "title": "Where Culture Fades: Revealing the Cultural Gap in Text-to-Image Generation",
    "summary": "Multilingual text-to-image (T2I) models have advanced rapidly in terms of visual realism and semantic alignment, and are now widely utilized. Yet outputs vary across cultural contexts: because language carries cultural connotations, images synthesized from multilingual prompts should preserve cross-lingual cultural consistency. We conduct a comprehensive analysis showing that current T2I models often produce culturally neutral or English-biased results under multilingual prompts. Analyses of two representative models indicate that the issue stems not from missing cultural knowledge but from insufficient activation of culture-related representations. We propose a probing method that localizes culture-sensitive signals to a small set of neurons in a few fixed layers. Guided by this finding, we introduce two complementary alignment strategies: (1) inference-time cultural activation that amplifies the identified neurons without backbone fine-tuned; and (2) layer-targeted cultural enhancement that updates only culturally relevant layers. Experiments on our CultureBench demonstrate consistent improvements over strong baselines in cultural consistency while preserving fidelity and diversity.",
    "translation": "标题：文化褪色之处：揭示文本到图像生成中的文化鸿沟\n\n摘要：多语言文本到图像生成模型在视觉真实性与语义对齐方面进展迅速，现已得到广泛应用。然而，其输出结果在不同文化语境中存在显著差异：由于语言承载着文化内涵，基于多语言提示生成的图像应当保持跨语言的文化一致性。我们通过综合分析发现，当前文本到图像模型在处理多语言提示时，常产生文化中性或偏向英语文化的结果。对两个代表性模型的分析表明，该问题并非源于文化知识的缺失，而是由文化相关表征激活不足所致。我们提出一种探测方法，可将文化敏感信号定位至少数固定层中的特定神经元集合。基于这一发现，我们引入两种互补的对齐策略：（1）无需微调主干模型的推理时文化激活技术，通过增强已识别神经元的响应实现文化表征强化；（2）针对文化相关层的定向增强方法，仅更新与文化关联密切的模型层。在自建文化评估基准CultureBench上的实验表明，该方法在保持生成质量与多样性的同时，相较于现有基线模型实现了文化一致性的持续提升。",
    "url": "https://huggingface.co/papers/2511.17282",
    "arxiv_url": "https://arxiv.org/abs/2511.17282"
  },
  {
    "title": "PromptBridge: Cross-Model Prompt Transfer for Large Language Models",
    "summary": "Large language models (LLMs) underpin applications in code generation, mathematical reasoning, and agent-based workflows. In practice, systems access LLMs via commercial APIs or open-source deployments, and the model landscape (e.g., GPT, Claude, Llama) evolves rapidly. This rapid evolution forces frequent model switches driven by capability, cost, deployment constraints, and privacy. Yet prompts are highly model-sensitive: reusing a prompt engineered for one model on another often yields substantially worse performance than a prompt optimized for the target model. We term this phenomenon Model Drifting. Through extensive empirical analysis across diverse LLM configurations, we show that model drifting is both common and severe. To address this challenge, we introduce PromptBridge, a training-free framework that preserves prompt effectiveness under model switches, enabling cross-model prompt transfer without costly per-task or per-model re-optimization. PromptBridge requires only a small set of alignment tasks for calibration. It first applies Model-Adaptive Reflective Prompt Evolution (MAP-RPE) to obtain task- and model-specific optimal prompts via iterative reflective refinement and quantitative evaluation. Using the resulting calibrated prompt pairs for the source and target models, PromptBridge learns a cross-model prompt mapping. At test time, i.e., for an unseen task, given a source-model prompt, this mapping directly produces an optimized prompt for the target model. Experiments in single-agent and multi-agent settings show that PromptBridge consistently improves downstream accuracy while reducing migration effort. The code will be available soon.",
    "translation": "标题：PromptBridge：面向大语言模型的跨模型提示迁移框架\n\n摘要：大语言模型已成为代码生成、数学推理和智能体工作流等应用的核心支撑。在实际应用中，系统通常通过商业API或开源部署调用大语言模型，而模型生态（如GPT、Claude、Llama等）正经历快速迭代。这种快速演进使得用户需要频繁切换模型，切换动因包括性能差异、成本考量、部署限制及隐私需求等。然而，提示词具有显著的模型敏感性：针对特定模型优化的提示词直接迁移至其他模型时，其性能往往远低于针对目标模型专门优化的提示词。我们将此现象称为“模型漂移”。通过对多种大语言模型配置的广泛实证分析，我们发现模型漂移现象普遍存在且影响显著。为应对这一挑战，本文提出PromptBridge——一种无需训练的框架，旨在保持模型切换时提示词的有效性，实现无需针对每项任务或每个模型进行高成本重新优化的跨模型提示迁移。该框架仅需少量对齐任务进行校准：首先通过“模型自适应反射式提示演进”方法，借助迭代式反射优化与量化评估，获得面向特定任务与模型的最优提示词；随后利用源模型与目标模型校准后的提示词对，学习跨模型提示映射关系。在测试阶段（即面对未见任务时），给定源模型提示词，该映射可直接生成针对目标模型的优化提示词。在单智能体与多智能体场景下的实验表明，PromptBridge能持续提升下游任务准确率，同时显著降低迁移成本。相关代码即将公开。",
    "url": "https://huggingface.co/papers/2512.01420",
    "arxiv_url": "https://arxiv.org/abs/2512.01420"
  },
  {
    "title": "SCALE: Selective Resource Allocation for Overcoming Performance Bottlenecks in Mathematical Test-time Scaling",
    "summary": "Test-time compute scaling has emerged as a powerful paradigm for enhancing mathematical reasoning in large language models (LLMs) by allocating additional computational resources during inference. However, current methods employ uniform resource distribution across all reasoning sub-problems, creating fundamental bottlenecks where challenging sub-problems receive insufficient attention while routine operations consume disproportionate resources. This uniform allocation creates performance bottlenecks where additional computational resources yield diminishing returns. Inspired by dual-process theory, we propose SCALE (Selective Resource Allocation), a framework that selectively allocates computational resources based on sub-problem difficulty. SCALE operates through four stages: (1) problem decomposition into sequential reasoning sub-problems, (2) difficulty assessment of each sub-problem to distinguish between routine operations and computationally challenging sub-problems, (3) selective processing mode assignment between System 1 for simple sub-problems and System 2 for complex ones, and (4) sequential execution with context propagation. By concentrating resources on challenging sub-problems while processing routine operations efficiently, SCALE achieves substantial performance improvements with superior resource utilization. Extensive experiments demonstrate that SCALE significantly outperforms uniform scaling baselines, achieving accuracy improvements of up to 13.75 percentage points (57.50% to 71.25% on AIME25) while reducing computational costs by 33%-53%, representing a major advance in test-time scaling that addresses fundamental limitations of current approaches.",
    "translation": "标题：SCALE：选择性资源分配以克服数学测试时扩展中的性能瓶颈\n\n摘要：测试时计算扩展已成为一种强大的范式，通过在推理过程中分配额外计算资源来增强大型语言模型（LLM）的数学推理能力。然而，现有方法对所有推理子问题采用均匀的资源分配，这造成了根本性瓶颈：具有挑战性的子问题未能获得足够关注，而常规操作却消耗了不成比例的资源。这种均匀分配导致性能瓶颈，使得额外计算资源的投入产生边际效益递减。受双过程理论启发，我们提出SCALE（选择性资源分配）框架，该框架根据子问题难度选择性分配计算资源。SCALE通过四个阶段运行：（1）将问题分解为顺序推理子问题；（2）评估每个子问题的难度，以区分常规操作与计算密集型挑战性子问题；（3）为简单子问题分配系统1处理模式，为复杂子问题分配系统2处理模式；（4）结合上下文传递的顺序执行。通过将资源集中于挑战性子问题，同时高效处理常规操作，SCALE在显著提升性能的同时实现了更优的资源利用率。大量实验表明，SCALE显著优于均匀扩展基线方法，在AIME25数据集上准确率最高提升13.75个百分点（从57.50%提升至71.25%），同时降低33%-53%的计算成本。这代表了测试时扩展领域的重要进展，有效解决了现有方法的根本性局限。",
    "url": "https://huggingface.co/papers/2512.00466",
    "arxiv_url": "https://arxiv.org/abs/2512.00466"
  },
  {
    "title": "MultiBanana: A Challenging Benchmark for Multi-Reference Text-to-Image Generation",
    "summary": "Recent text-to-image generation models have acquired the ability of multi-reference generation and editing; the ability to inherit the appearance of subjects from multiple reference images and re-render them under new contexts. However, the existing benchmark datasets often focus on the generation with single or a few reference images, which prevents us from measuring the progress on how model performance advances or pointing out their weaknesses, under different multi-reference conditions. In addition, their task definitions are still vague, typically limited to axes such as \"what to edit\" or \"how many references are given\", and therefore fail to capture the intrinsic difficulty of multi-reference settings. To address this gap, we introduce MultiBanana, which is carefully designed to assesses the edge of model capabilities by widely covering multi-reference-specific problems at scale: (1) varying the number of references, (2) domain mismatch among references (e.g., photo vs. anime), (3) scale mismatch between reference and target scenes, (4) references containing rare concepts (e.g., a red banana), and (5) multilingual textual references for rendering. Our analysis among a variety of text-to-image models reveals their superior performances, typical failure modes, and areas for improvement. MultiBanana will be released as an open benchmark to push the boundaries and establish a standardized basis for fair comparison in multi-reference image generation. Our data and code are available at https://github.com/matsuolab/multibanana .",
    "translation": "标题：MultiBanana：一个面向多参考文本到图像生成的挑战性基准\n\n摘要：近期的文本到图像生成模型已具备多参考生成与编辑的能力，即能够从多张参考图像中继承目标对象的外观特征，并在新情境下进行重新渲染。然而，现有基准数据集通常侧重于单张或少量参考图像的生成任务，这限制了我们在不同多参考条件下评估模型性能进展或指认其弱点的能力。此外，现有任务定义仍较为模糊，通常局限于“编辑内容”或“参考数量”等单一维度，未能充分捕捉多参考场景的内在复杂性。为填补这一空白，我们提出了MultiBanana基准，其通过广泛覆盖大规模多参考特有问题来系统评估模型的能力边界：（1）参考图像数量的变化，（2）参考图像间的领域不匹配（例如照片与动漫风格），（3）参考场景与目标场景的尺度差异，（4）包含罕见概念的参考图像（例如红色香蕉），以及（5）多语言文本参考的渲染需求。通过对多种文本到图像模型的分析，我们揭示了其优势表现、典型失败模式及改进方向。MultiBanana将作为开放基准发布，旨在推动多参考图像生成领域的发展，并为公平比较建立标准化基础。相关数据与代码已公开于https://github.com/matsuolab/multibanana。",
    "url": "https://huggingface.co/papers/2511.22989",
    "arxiv_url": "https://arxiv.org/abs/2511.22989"
  },
  {
    "title": "Script: Graph-Structured and Query-Conditioned Semantic Token Pruning for Multimodal Large Language Models",
    "summary": "The rapid growth of visual tokens in multimodal large language models (MLLMs) leads to excessive memory consumption and inference latency, especially when handling high-resolution images and videos. Token pruning is a technique used to mitigate this issue by removing redundancy, but existing methods often ignore relevance to the user query or suffer from the limitations of attention mechanisms, reducing their adaptability and effectiveness. To address these challenges, we propose Script, a plug-and-play pruning method that requires no retraining and generalizes across diverse MLLMs. Script comprises two modules: a graph-structured pruning module that removes visually redundant tokens, and a query-conditioned semantic pruning module that preserves query-relevant visual information. Together, they enhance performance on multimodal tasks. Experiments on fourteen benchmarks across image and video understanding tasks show that Script consistently achieves higher model efficiency and predictive accuracy compared to existing pruning methods. On LLaVA-NeXT-7B, it achieves up to 6.8x prefill speedup and 10x FLOP reduction, while retaining 96.88% of the original performance.",
    "translation": "标题：Script：面向多模态大语言模型的图结构及查询条件语义令牌剪枝方法\n\n摘要：多模态大语言模型（MLLMs）中视觉令牌数量的快速增长导致内存消耗和推理延迟显著增加，尤其是在处理高分辨率图像和视频时。令牌剪枝技术通过消除冗余来缓解这一问题，但现有方法往往忽略与用户查询的相关性，或受限于注意力机制的缺陷，从而降低了其适应性和有效性。为应对这些挑战，我们提出Script——一种无需重新训练且可泛化至多种MLLMs的即插即用式剪枝方法。Script包含两个核心模块：图结构剪枝模块用于去除视觉冗余令牌，以及查询条件语义剪枝模块用于保留与查询相关的视觉信息。二者协同工作，显著提升了多模态任务性能。在涵盖图像与视频理解任务的十四个基准测试上的实验表明，相较于现有剪枝方法，Script在模型效率和预测准确性方面均表现更优。在LLaVA-NeXT-7B模型上，该方法实现了最高6.8倍的预填充加速和10倍的浮点运算量削减，同时保持了原模型96.88%的性能表现。",
    "url": "https://huggingface.co/papers/2512.01949",
    "arxiv_url": "https://arxiv.org/abs/2512.01949"
  },
  {
    "title": "Lotus-2: Advancing Geometric Dense Prediction with Powerful Image Generative Model",
    "summary": "Recovering pixel-wise geometric properties from a single image is fundamentally ill-posed due to appearance ambiguity and non-injective mappings between 2D observations and 3D structures. While discriminative regression models achieve strong performance through large-scale supervision, their success is bounded by the scale, quality and diversity of available data and limited physical reasoning. Recent diffusion models exhibit powerful world priors that encode geometry and semantics learned from massive image-text data, yet directly reusing their stochastic generative formulation is suboptimal for deterministic geometric inference: the former is optimized for diverse and high-fidelity image generation, whereas the latter requires stable and accurate predictions. In this work, we propose Lotus-2, a two-stage deterministic framework for stable, accurate and fine-grained geometric dense prediction, aiming to provide an optimal adaption protocol to fully exploit the pre-trained generative priors. Specifically, in the first stage, the core predictor employs a single-step deterministic formulation with a clean-data objective and a lightweight local continuity module (LCM) to generate globally coherent structures without grid artifacts. In the second stage, the detail sharpener performs a constrained multi-step rectified-flow refinement within the manifold defined by the core predictor, enhancing fine-grained geometry through noise-free deterministic flow matching. Using only 59K training samples, less than 1% of existing large-scale datasets, Lotus-2 establishes new state-of-the-art results in monocular depth estimation and highly competitive surface normal prediction. These results demonstrate that diffusion models can serve as deterministic world priors, enabling high-quality geometric reasoning beyond traditional discriminative and generative paradigms.",
    "translation": "标题：Lotus-2：基于强大图像生成模型的几何密集预测进展\n\n摘要：从单张图像中恢复逐像素的几何属性本质上是病态问题，其原因在于外观的模糊性以及二维观测与三维结构之间的非单射映射关系。尽管判别式回归模型通过大规模监督学习取得了优异性能，但其成功受限于可用数据的规模、质量与多样性，以及有限的物理推理能力。近期扩散模型展现出强大的世界先验，能够编码从海量图文数据中学到的几何与语义信息，然而直接沿用其随机生成范式对于确定性几何推理并非最优：前者旨在实现多样且高保真的图像生成，而后者则需要稳定且精确的预测。本研究提出Lotus-2——一个用于实现稳定、精确且细粒度几何密集预测的两阶段确定性框架，旨在提供一种最优适配方案以充分挖掘预训练生成先验的潜力。具体而言，在第一阶段，核心预测器采用基于干净数据目标的单步确定性建模，并配备轻量级局部连续性模块，以生成全局连贯且无网格伪影的结构。在第二阶段，细节锐化器在核心预测器定义的流形内执行约束性多步修正流优化，通过无噪声的确定性流匹配增强细粒度几何细节。仅使用5.9万训练样本（不足现有大规模数据集的1%），Lotus-2在单目深度估计任务中取得了全新的最优性能，并在表面法线预测中达到高度竞争力。这些结果表明，扩散模型能够作为确定性世界先验，实现超越传统判别式与生成范式的高质量几何推理。",
    "url": "https://huggingface.co/papers/2512.01030",
    "arxiv_url": "https://arxiv.org/abs/2512.01030"
  },
  {
    "title": "StreamGaze: Gaze-Guided Temporal Reasoning and Proactive Understanding in Streaming Videos",
    "summary": "Streaming video understanding requires models not only to process temporally incoming frames, but also to anticipate user intention for realistic applications like AR glasses. While prior streaming benchmarks evaluate temporal reasoning, none measure whether MLLMs can interpret or leverage human gaze signals within a streaming setting. To fill this gap, we introduce StreamGaze, the first benchmark designed to evaluate how effectively MLLMs use gaze for temporal and proactive reasoning in streaming videos. StreamGaze introduces gaze-guided past, present, and proactive tasks that comprehensively evaluate streaming video understanding. These tasks assess whether models can use real-time gaze to follow shifting attention and infer user intentions from only past and currently observed frames. To build StreamGaze, we develop a gaze-video QA generation pipeline that aligns egocentric videos with raw gaze trajectories via fixation extraction, region-specific visual prompting, and scanpath construction. This pipeline produces spatio-temporally grounded QA pairs that closely reflect human perceptual dynamics. Across all StreamGaze tasks, we observe substantial performance gaps between state-of-the-art MLLMs and human performance, revealing fundamental limitations in gaze-based temporal reasoning, intention modeling, and proactive prediction. We further provide detailed analyses of gaze-prompting strategies, reasoning behaviors, and task-specific failure modes, offering deeper insight into why current MLLMs struggle and what capabilities future models must develop. All data and code will be publicly released to support continued research in gaze-guided streaming video understanding.",
    "translation": "标题：StreamGaze：流式视频中的视线引导时序推理与前瞻性理解\n\n摘要：流式视频理解不仅要求模型能够处理时序输入的帧序列，还需要在如增强现实眼镜等实际应用中预测用户意图。尽管现有的流式基准测试评估了时序推理能力，但尚未有研究衡量多模态大语言模型（MLLMs）是否能在流式场景中解读或利用人类视线信号。为填补这一空白，我们提出了StreamGaze——首个专门用于评估MLLMs在流式视频中如何利用视线进行时序与前瞻性推理的基准测试。StreamGaze引入了视线引导的过去、现在与前瞻性任务，全面评估流式视频理解能力。这些任务检验模型能否利用实时视线追踪注意力转移，并仅基于过去及当前观测帧推断用户意图。为构建StreamGaze，我们开发了一套视线-视频问答生成流程，通过注视点提取、区域特异性视觉提示和扫描路径构建，将第一人称视频与原始视线轨迹对齐。该流程生成具有时空 grounded 性的问答对，紧密贴合人类感知动态。在所有StreamGaze任务中，我们发现当前最先进的MLLMs与人类表现之间存在显著性能差距，揭示了基于视线的时序推理、意图建模和前瞻预测方面的根本性局限。我们进一步对视线提示策略、推理行为及任务特定失败模式进行了详细分析，深入探讨了当前MLLMs的不足及未来模型需发展的能力。所有数据与代码将公开发布，以支持视线引导流式视频理解领域的持续研究。",
    "url": "https://huggingface.co/papers/2512.01707",
    "arxiv_url": "https://arxiv.org/abs/2512.01707"
  },
  {
    "title": "POLARIS: Projection-Orthogonal Least Squares for Robust and Adaptive Inversion in Diffusion Models",
    "summary": "The Inversion-Denoising Paradigm, which is based on diffusion models, excels in diverse image editing and restoration tasks. We revisit its mechanism and reveal a critical, overlooked factor in reconstruction degradation: the approximate noise error. This error stems from approximating the noise at step t with the prediction at step t-1, resulting in severe error accumulation throughout the inversion process. We introduce Projection-Orthogonal Least Squares for Robust and Adaptive Inversion (POLARIS), which reformulates inversion from an error-compensation problem into an error-origin problem. Rather than optimizing embeddings or latent codes to offset accumulated drift, POLARIS treats the guidance scale ω as a step-wise variable and derives a mathematically grounded formula to minimize inversion error at each step. Remarkably, POLARIS improves inversion latent quality with just one line of code. With negligible performance overhead, it substantially mitigates noise approximation errors and consistently improves the accuracy of downstream tasks.",
    "translation": "标题：POLARIS：基于投影正交最小二乘的扩散模型鲁棒自适应反演方法\n\n摘要：基于扩散模型的反演-去噪范式在多种图像编辑与修复任务中表现卓越。本文重新审视其工作机制，揭示了一个导致重建质量退化的关键但常被忽视的因素：近似噪声误差。该误差源于使用第t-1步的预测值来近似第t步的噪声，从而在整个反演过程中引发严重的误差累积。我们提出一种基于投影正交最小二乘的鲁棒自适应反演方法（POLARIS），该方法将反演问题从误差补偿问题重新定义为误差溯源问题。POLARIS不通过优化嵌入或潜码来抵消累积漂移，而是将引导尺度ω视为步进变量，并推导出具有数学依据的公式以最小化每一步的反演误差。值得注意的是，POLARIS仅需一行代码即可显著提升反演潜码质量。在性能开销可忽略不计的前提下，该方法能有效抑制噪声近似误差，并持续提升下游任务的准确性。",
    "url": "https://huggingface.co/papers/2512.00369",
    "arxiv_url": "https://arxiv.org/abs/2512.00369"
  },
  {
    "title": "ORION: Teaching Language Models to Reason Efficiently in the Language of Thought",
    "summary": "Large Reasoning Models (LRMs) achieve strong performance in mathematics, code generation, and task planning, but their reliance on long chains of verbose \"thinking\" tokens leads to high latency, redundancy, and incoherent reasoning paths. Inspired by the Language of Thought Hypothesis, which posits that human reasoning operates over a symbolic, compositional mental language called Mentalese, we introduce a framework that trains models to reason in a similarly compact style. Mentalese encodes abstract reasoning as ultra-compressed, structured tokens, enabling models to solve complex problems with far fewer steps. To improve both efficiency and accuracy, we propose SHORTER LENGTH PREFERENCE OPTIMIZATION (SLPO), a reinforcement learning method that rewards concise solutions that stay correct, while still allowing longer reasoning when needed. Applied to Mentalese-aligned models, SLPO yields significantly higher compression rates by enabling concise reasoning that preserves the benefits of detailed thinking without the computational overhead. Across benchmarks including AIME 2024 and 2025, MinervaMath, OlympiadBench, Math500, and AMC, our ORION models produce reasoning traces with 4-16x fewer tokens, achieve up to 5x lower inference latency, and reduce training costs by 7-9x relative to the DeepSeek R1 Distilled model, while maintaining 90-98% of its accuracy. ORION also surpasses Claude and ChatGPT-4o by up to 5% in accuracy while maintaining 2x compression. These results show that Mentalese-style compressed reasoning offers a step toward human-like cognitive efficiency, enabling real-time, cost-effective reasoning without sacrificing accuracy.",
    "translation": "标题：ORION：在思维语言中高效训练语言模型推理能力\n\n摘要：大型推理模型在数学、代码生成和任务规划方面表现出色，但其依赖冗长的“思维”标记链导致高延迟、冗余和推理路径不连贯。受思维语言假说启发——该假说认为人类推理基于一种称为“心理语”的符号化、组合性心理语言——我们提出了一个训练模型以类似紧凑风格进行推理的框架。心理语将抽象推理编码为超压缩的结构化标记，使模型能够以更少的步骤解决复杂问题。为提升效率与准确性，我们提出**短长度偏好优化**方法，这是一种强化学习方法，奖励保持正确的简洁解决方案，同时允许必要时进行更长的推理。应用于心理语对齐模型后，该方法通过实现简洁推理显著提高了压缩率，既保留了详细思维的优势，又避免了计算开销。在AIME 2024/2025、MinervaMath、OlympiadBench、Math500和AMC等基准测试中，我们的ORION模型生成的推理轨迹标记数减少4-16倍，推理延迟降低最高达5倍，训练成本相比DeepSeek R1 Distilled模型降低7-9倍，同时保持其90-98%的准确率。ORION的准确率较Claude和ChatGPT-4o最高提升5%，且保持2倍压缩率。这些结果表明，心理语式压缩推理向类人认知效率迈进了一步，实现了在不牺牲准确性的前提下进行实时、高性价比的推理。",
    "url": "https://huggingface.co/papers/2511.22891",
    "arxiv_url": "https://arxiv.org/abs/2511.22891"
  },
  {
    "title": "Asking like Socrates: Socrates helps VLMs understand remote sensing images",
    "summary": "Recent multimodal reasoning models, inspired by DeepSeek-R1, have significantly advanced vision-language systems. However, in remote sensing (RS) tasks, we observe widespread pseudo reasoning: models narrate the process of reasoning rather than genuinely reason toward the correct answer based on visual evidence. We attribute this to the Glance Effect, where a single, coarse perception of large-scale RS imagery results in incomplete understanding and reasoning based on linguistic self-consistency instead of visual evidence. To address this, we propose RS-EoT (Remote Sensing Evidence-of-Thought), a language-driven, iterative visual evidence-seeking paradigm. To instill this paradigm, we propose SocraticAgent, a self-play multi-agent system that synthesizes reasoning traces via alternating cycles of reasoning and visual inspection. To enhance and generalize these patterns, we propose a two-stage progressive RL strategy: first, RL on fine-grained Grounding tasks to enhance RS-EoT capabilities, followed by RL on RS VQA to generalize to broader understanding scenarios. Experiments show RS-EoT achieves state-of-the-art performance on multiple RS VQA and grounding benchmarks. Analyses reveal clear iterative cycles of reasoning and evidence seeking, confirming RS-EoT mitigates the Glance Effect and enables genuine evidence-grounded reasoning. Our code, data, and models are available at https://geox-lab.github.io/Asking_like_Socrates",
    "translation": "标题：如苏格拉底般提问：苏格拉底助力视觉语言模型理解遥感图像\n\n摘要：受DeepSeek-R1启发的多模态推理模型近期显著推动了视觉语言系统的发展。然而在遥感任务中，我们观察到普遍存在的伪推理现象：模型倾向于描述推理过程，而非基于视觉证据进行真正的推理以得出正确答案。我们将此归因于\"一瞥效应\"，即对大规模遥感图像的单次粗略感知导致理解不完整，并促使模型依赖语言自洽性而非视觉证据进行推理。为解决该问题，我们提出RS-EoT（遥感思维证据链），一种语言驱动的迭代式视觉证据搜寻范式。为植入该范式，我们设计了SocraticAgent——一个通过推理与视觉检查交替循环来合成推理轨迹的自博弈多代理系统。为增强并泛化这些模式，我们提出两阶段渐进式强化学习策略：首先通过细粒度定位任务的强化学习提升RS-EoT能力，再通过遥感视觉问答的强化学习泛化至更广泛的理解场景。实验表明RS-EoT在多个遥感视觉问答与定位基准测试中达到最先进性能。分析显示清晰的推理与证据搜寻迭代循环，证实RS-EoT能有效缓解一瞥效应，实现真正基于证据的推理。代码、数据及模型已开源：https://geox-lab.github.io/Asking_like_Socrates",
    "url": "https://huggingface.co/papers/2511.22396",
    "arxiv_url": "https://arxiv.org/abs/2511.22396"
  },
  {
    "title": "Agentic Policy Optimization via Instruction-Policy Co-Evolution",
    "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has advanced the reasoning capability of large language models (LLMs), enabling autonomous agents that can conduct effective multi-turn and tool-integrated reasoning. While instructions serve as the primary protocol for defining agents, RLVR typically relies on static and manually designed instructions. However, those instructions may be suboptimal for the base model, and the optimal instruction may change as the agent's policy improves and explores the interaction with the environment. To bridge the gap, we introduce INSPO, a novel Instruction-Policy co-evolution framework that integrates instruction optimization as a dynamic component of the reinforcement learning (RL) loop. INSPO maintains a dynamic population of instruction candidates that are sampled with questions, where reward signals in RL loops are automatically attributed to each instruction, and low performers are periodically pruned. New instructions are generated and verified through an on-policy reflection mechanism, where an LLM-based optimizer analyzes past experience from a replay buffer and evolves more effective strategies given the current policy. We conduct extensive experiments on multi-turn retrieval and reasoning tasks, demonstrating that INSPO substantially outperforms strong baselines relying on static instructions. INSPO discovers innovative instructions that guide the agent toward more strategic reasoning paths, achieving substantial performance gains with only a marginal increase in computational overhead.",
    "translation": "标题：基于指令-策略协同进化的智能体策略优化\n\n摘要：可验证奖励强化学习（RLVR）显著提升了大型语言模型（LLM）的推理能力，使其能够构建可进行高效多轮交互与工具整合推理的自主智能体。尽管指令是定义智能体的核心协议，但传统RLVR通常依赖于静态的人工设计指令。然而，这些指令对基础模型可能并非最优，且最优指令会随着智能体策略的改进及其与环境交互探索的深入而动态变化。为弥合这一差距，本文提出INSPO——一种创新的指令-策略协同进化框架，将指令优化作为强化学习（RL）循环的动态组成部分。INSPO维护一个动态的指令候选集合，这些指令与问题共同采样；RL循环中的奖励信号会自动归因于每条指令，并定期淘汰低效指令。新指令通过基于策略的反思机制生成与验证：一个基于LLM的优化器分析回放缓冲区中的历史经验，并针对当前策略演化出更有效的策略指导。我们在多轮检索与推理任务上进行了广泛实验，结果表明INSPO显著优于依赖静态指令的强基线方法。INSPO能够发现创新性指令，引导智能体走向更具战略性的推理路径，在仅略微增加计算开销的情况下实现显著的性能提升。",
    "url": "https://huggingface.co/papers/2512.01945",
    "arxiv_url": "https://arxiv.org/abs/2512.01945"
  },
  {
    "title": "HiconAgent: History Context-aware Policy Optimization for GUI Agents",
    "summary": "Graphical User Interface (GUI) agents require effective use of historical context to perform sequential navigation tasks. While incorporating past actions and observations can improve decision making, naive use of full history leads to excessive computational overhead and distraction from irrelevant information. To address this, we introduce HiconAgent, a GUI agent trained with History Context-aware Policy Optimization (HCPO) for efficient and effective utilization of historical information. HCPO optimizes history usage in both sampling and policy updates through two complementary components: (1) Dynamic Context Sampling (DCS) presents the agent with variable length histories during sampling, enabling adaptive use of the most relevant context; (2) Anchor-guided History Compression (AHC) refines the policy update phase with a dual branch strategy where the compressed branch removes history observations while keeping history actions as information flow anchors. The compressed and uncompressed branches are coupled through a history-enhanced alignment loss to enforce consistent history usage while maintaining efficiency. Experiments on mainstream GUI navigation benchmarks demonstrate strong performance. Despite being smaller, HiconAgent-3B outperforms GUI-R1-7B by +8.46 percent grounding accuracy and +11.32 percent step success rate on GUI-Odyssey, while achieving comparable results on AndroidControl and AITW with up to 2.47x computational speedup and 60 percent FLOPs reduction.",
    "translation": "标题：HiconAgent：面向图形用户界面智能体的历史上下文感知策略优化\n\n摘要：图形用户界面（GUI）智能体需要有效利用历史上下文以执行序列化导航任务。虽然整合过往动作与观察信息能够提升决策质量，但直接使用完整历史记录会导致计算开销过大并引入无关信息干扰。为此，我们提出HiconAgent——一种采用历史上下文感知策略优化（HCPO）方法训练的GUI智能体，旨在实现历史信息的高效精准利用。HCPO通过两个互补组件优化历史信息在采样与策略更新阶段的使用：（1）动态上下文采样（DCS）在采样阶段为智能体提供可变长度的历史记录，使其能自适应选择最相关上下文；（2）锚点引导的历史压缩（AHC）采用双分支策略改进策略更新阶段，其中压缩分支在移除历史观察信息的同时保留历史动作作为信息流锚点。压缩分支与未压缩分支通过历史增强对齐损失进行耦合，在保证效率的同时强化历史使用的一致性。主流GUI导航基准测试表明，该方法具有卓越性能：尽管参数量更小，HiconAgent-3B在GUI-Odyssey数据集上的定位准确率与步骤成功率分别超越GUI-R1-7B模型8.46%和11.32%，同时在AndroidControl与AITW数据集上取得可比结果，并实现最高2.47倍的计算加速与60%的浮点运算量削减。",
    "url": "https://huggingface.co/papers/2512.01763",
    "arxiv_url": "https://arxiv.org/abs/2512.01763"
  },
  {
    "title": "Learning Eigenstructures of Unstructured Data Manifolds",
    "summary": "We introduce a novel framework that directly learns a spectral basis for shape and manifold analysis from unstructured data, eliminating the need for traditional operator selection, discretization, and eigensolvers. Grounded in optimal-approximation theory, we train a network to decompose an implicit approximation operator by minimizing the reconstruction error in the learned basis over a chosen distribution of probe functions. For suitable distributions, they can be seen as an approximation of the Laplacian operator and its eigendecomposition, which are fundamental in geometry processing. Furthermore, our method recovers in a unified manner not only the spectral basis, but also the implicit metric's sampling density and the eigenvalues of the underlying operator. Notably, our unsupervised method makes no assumption on the data manifold, such as meshing or manifold dimensionality, allowing it to scale to arbitrary datasets of any dimension. On point clouds lying on surfaces in 3D and high-dimensional image manifolds, our approach yields meaningful spectral bases, that can resemble those of the Laplacian, without explicit construction of an operator. By replacing the traditional operator selection, construction, and eigendecomposition with a learning-based approach, our framework offers a principled, data-driven alternative to conventional pipelines. This opens new possibilities in geometry processing for unstructured data, particularly in high-dimensional spaces.",
    "translation": "标题：非结构化数据流形特征结构学习\n\n摘要：本文提出一种新颖框架，可直接从非结构化数据中学习用于形状与流形分析的谱基，无需传统算子选择、离散化及特征求解过程。基于最优逼近理论，我们通过训练网络在选定探测函数分布上最小化学习基中的重构误差，从而分解隐式逼近算子。对于合适的分布，该框架可视为拉普拉斯算子及其特征分解的近似——这两者在几何处理中具有基础性地位。此外，我们的方法能以统一方式同时恢复谱基、隐式度量的采样密度以及底层算子的特征值。值得注意的是，这种无监督方法无需对数据流形（如网格化或流形维度）进行任何假设，可扩展至任意维度的数据集。针对三维空间表面点云与高维图像流形，本方法无需显式构建算子即可生成有意义的谱基，其性质可与拉普拉斯算子的谱基相媲美。通过用基于学习的方法替代传统算子选择、构建及特征分解流程，本框架为传统处理管线提供了原理清晰、数据驱动的替代方案。这为非结构化数据（特别是高维空间数据）的几何处理开辟了新的可能性。",
    "url": "https://huggingface.co/papers/2512.01103",
    "arxiv_url": "https://arxiv.org/abs/2512.01103"
  },
  {
    "title": "The Art of Scaling Test-Time Compute for Large Language Models",
    "summary": "Test-time scaling (TTS) -- the dynamic allocation of compute during inference -- is a promising direction for improving reasoning in large language models (LLMs). However, a systematic comparison of well-known TTS strategies under identical conditions is missing, and the influence of model type and problem difficulty on performance remains unclear. To address these gaps, we conduct the first large-scale study of TTS, spanning over thirty billion tokens generated using eight open-source LLMs (7B to 235B parameters), across four reasoning datasets. We observe three consistent trends: (1) no single TTS strategy universally dominates; (2) reasoning models exhibit distinct trace-quality patterns across problem difficulty and trace length, forming short-horizon and long-horizon categories; and (3) for a given model type, the optimal TTS performance scales monotonically with compute budget. Based on these insights, we provide a practical recipe for selecting the best TTS strategy, considering problem difficulty, model type, and compute budget, providing a practical guide to effective inference-time scaling.",
    "translation": "标题：大型语言模型测试时计算扩展的艺术\n\n摘要：测试时扩展（TTS）——在推理过程中动态分配计算资源——是提升大型语言模型（LLMs）推理能力的一个前景广阔的方向。然而，目前尚缺乏在相同条件下对主流TTS策略的系统性比较，且模型类型与问题难度对性能的影响仍不明确。为填补这些空白，我们开展了首次大规模的TTS研究，涵盖八个开源LLM（参数量从70亿到2350亿）在四个推理数据集上生成的超过三百亿个标记。我们观察到三个一致趋势：（1）没有单一的TTS策略能在所有情况下占优；（2）推理模型在不同问题难度和推理轨迹长度上表现出明显的轨迹质量差异，可划分为短视野与长视野两类；（3）对于给定模型类型，最优TTS性能随计算预算增加呈单调提升。基于这些发现，我们提出了一个综合考虑问题难度、模型类型和计算预算的实用方案，用于选择最佳TTS策略，为高效的推理时扩展提供了实践指导。",
    "url": "https://huggingface.co/papers/2512.02008",
    "arxiv_url": "https://arxiv.org/abs/2512.02008"
  },
  {
    "title": "OpenREAD: Reinforced Open-Ended Reasoing for End-to-End Autonomous Driving with LLM-as-Critic",
    "summary": "Recently, two-stage fine-tuning strategies, e.g., acquiring essential driving knowledge through supervised fine-tuning (SFT) and further enhancing decision-making and planning via reinforcement fine-tuning (RFT), have shown strong potential in advancing the knowledge-driven autonomous driving (AD) paradigm. However, the learning nature of SFT still limits the generalization of reasoning, thereby constraining the full potential of driving performance. Meanwhile, current RFT approaches are primarily applied to downstream tasks, since scene understanding is an open-ended problem where corresponding rewards are difficult to quantify. To address these limitations, we propose OpenREAD, an OPEN-ended REasoning reinforced vision-language model (VLM)-based autonomous driving (AD) framework that enables end-to-end RFT across the full spectrum from high-level reasoning to low-level trajectory planning. Specifically, we begin by constructing large-scale Chain-of-Thought (CoT) annotations on open-source driving-related knowledge datasets, and employ the powerful Qwen3 large language model (LLM) as the critic in RFT to quantify reasoning quality for open-ended questions during reward modeling. Extensive experiments confirm that joint end-to-end RFT yields substantial improvements in both upstream and downstream tasks, enabling OpenREAD to achieve state-of-the-art performance on reasoning and planning benchmarks.",
    "translation": "标题：OpenREAD：基于LLM作为评判者的端到端自动驾驶强化开放式推理框架\n\n摘要：近期，两阶段微调策略（例如通过监督微调获取关键驾驶知识，再通过强化微调提升决策与规划能力）在推动知识驱动的自动驾驶范式发展中展现出巨大潜力。然而，监督微调的学习本质仍限制了推理能力的泛化，从而制约了驾驶性能的全面提升。同时，由于场景理解属于开放式问题，其对应奖励难以量化，现有强化微调方法主要应用于下游任务。为突破这些局限，本文提出OpenREAD，一种基于强化开放式推理的视觉语言模型自动驾驶框架，能够实现从高层推理到低层轨迹规划的端到端全流程强化微调。具体而言，我们首先在开源驾驶知识数据集上构建大规模思维链标注，并利用强大的Qwen3大语言模型作为强化微调中的评判者，在奖励建模过程中对开放式问题的推理质量进行量化评估。大量实验证实，联合端到端强化微调在上下游任务中均带来显著提升，使OpenREAD在推理与规划基准测试中达到了最先进的性能水平。",
    "url": "https://huggingface.co/papers/2512.01830",
    "arxiv_url": "https://arxiv.org/abs/2512.01830"
  },
  {
    "title": "ChronosObserver: Taming 4D World with Hyperspace Diffusion Sampling",
    "summary": "Although prevailing camera-controlled video generation models can produce cinematic results, lifting them directly to the generation of 3D-consistent and high-fidelity time-synchronized multi-view videos remains challenging, which is a pivotal capability for taming 4D worlds. Some works resort to data augmentation or test-time optimization, but these strategies are constrained by limited model generalization and scalability issues. To this end, we propose ChronosObserver, a training-free method including World State Hyperspace to represent the spatiotemporal constraints of a 4D world scene, and Hyperspace Guided Sampling to synchronize the diffusion sampling trajectories of multiple views using the hyperspace. Experimental results demonstrate that our method achieves high-fidelity and 3D-consistent time-synchronized multi-view videos generation without training or fine-tuning for diffusion models.",
    "translation": "标题：ChronosObserver：基于超空间扩散采样的四维世界构建方法\n\n摘要：尽管当前主流的相机控制视频生成模型能够产生电影级效果，但将其直接应用于生成三维一致且高保真的时间同步多视角视频仍面临挑战，而这正是驾驭四维世界的关键能力。现有研究多采用数据增强或测试时优化策略，但这些方法受限于模型泛化能力不足与可扩展性问题。为此，我们提出ChronosObserver——一种无需训练的方法，其核心包含用于表征四维世界场景时空约束的“世界状态超空间”，以及通过超空间同步多视角扩散采样轨迹的“超空间引导采样”机制。实验结果表明，该方法无需对扩散模型进行训练或微调，即可实现高保真、三维一致的时间同步多视角视频生成。",
    "url": "https://huggingface.co/papers/2512.01481",
    "arxiv_url": "https://arxiv.org/abs/2512.01481"
  },
  {
    "title": "Generalist Large Language Models Outperform Clinical Tools on Medical Benchmarks",
    "summary": "Specialized clinical AI assistants are rapidly entering medical practice, often framed as safer or more reliable than general-purpose large language models (LLMs). Yet, unlike frontier models, these clinical tools are rarely subjected to independent, quantitative evaluation, creating a critical evidence gap despite their growing influence on diagnosis, triage, and guideline interpretation. We assessed two widely deployed clinical AI systems (OpenEvidence and UpToDate Expert AI) against three state-of-the-art generalist LLMs (GPT-5, Gemini 3 Pro, and Claude Sonnet 4.5) using a 1,000-item mini-benchmark combining MedQA (medical knowledge) and HealthBench (clinician-alignment) tasks. Generalist models consistently outperformed clinical tools, with GPT-5 achieving the highest scores, while OpenEvidence and UpToDate demonstrated deficits in completeness, communication quality, context awareness, and systems-based safety reasoning. These findings reveal that tools marketed for clinical decision support may often lag behind frontier LLMs, underscoring the urgent need for transparent, independent evaluation before deployment in patient-facing workflows.",
    "translation": "标题：通用大语言模型在医学基准测试中表现优于临床专用工具\n\n摘要：专业临床人工智能助手正快速进入医疗实践领域，常被宣传为比通用大语言模型更安全可靠。然而与前沿模型不同，这些临床工具很少接受独立的定量评估，尽管其对诊断、分诊和指南解读的影响日益增强，这一证据缺口仍构成关键问题。本研究选取两个广泛部署的临床AI系统（OpenEvidence与UpToDate Expert AI）与三种前沿通用大语言模型（GPT-5、Gemini 3 Pro和Claude Sonnet 4.5），通过整合MedQA（医学知识）与HealthBench（临床对齐）任务的千项微型基准测试进行对比评估。通用模型在所有评估维度上均稳定超越临床工具，其中GPT-5获得最高评分；而OpenEvidence与UpToDate在回答完整性、沟通质量、情境感知及系统化安全推理方面存在明显缺陷。研究结果表明，当前市场上推广的临床决策支持工具可能普遍落后于前沿大语言模型，这凸显了在面向患者的临床工作流部署前，建立透明独立评估机制的紧迫性。",
    "url": "https://huggingface.co/papers/2512.01191",
    "arxiv_url": "https://arxiv.org/abs/2512.01191"
  },
  {
    "title": "Seeing the Wind from a Falling Leaf",
    "summary": "A longstanding goal in computer vision is to model motions from videos, while the representations behind motions, i.e. the invisible physical interactions that cause objects to deform and move, remain largely unexplored. In this paper, we study how to recover the invisible forces from visual observations, e.g., estimating the wind field by observing a leaf falling to the ground. Our key innovation is an end-to-end differentiable inverse graphics framework, which jointly models object geometry, physical properties, and interactions directly from videos. Through backpropagation, our approach enables the recovery of force representations from object motions. We validate our method on both synthetic and real-world scenarios, and the results demonstrate its ability to infer plausible force fields from videos. Furthermore, we show the potential applications of our approach, including physics-based video generation and editing. We hope our approach sheds light on understanding and modeling the physical process behind pixels, bridging the gap between vision and physics. Please check more video results in our https://chaoren2357.github.io/seeingthewind/{project page}.",
    "translation": "标题：从落叶中看见风：基于视频的不可见力场推断\n\n摘要：计算机视觉领域的一个长期目标是从视频中建模物体运动，然而运动背后的表征——即导致物体形变与移动的不可见物理相互作用——在很大程度上仍未得到充分探索。本文研究如何从视觉观测中恢复不可见的力场，例如通过观察一片落叶的运动来估计风场。我们的核心创新在于提出了一种端到端可微分的逆向图形学框架，该框架能够直接从视频中联合建模物体几何结构、物理属性与相互作用关系。通过反向传播机制，该方法实现了从物体运动中恢复力场表征。我们在合成场景与真实场景中验证了方法的有效性，实验结果表明该方法能够从视频中推断出合理的力场分布。此外，我们展示了该方法在基于物理的视频生成与编辑等领域的潜在应用价值。本研究为理解与建模像素背后的物理过程提供了新思路，有助于弥合视觉感知与物理规律之间的认知鸿沟。更多视频结果请访问项目页面：https://chaoren2357.github.io/seeingthewind/。",
    "url": "https://huggingface.co/papers/2512.00762",
    "arxiv_url": "https://arxiv.org/abs/2512.00762"
  },
  {
    "title": "WiseEdit: Benchmarking Cognition- and Creativity-Informed Image Editing",
    "summary": "Recent image editing models boast next-level intelligent capabilities, facilitating cognition- and creativity-informed image editing. Yet, existing benchmarks provide too narrow a scope for evaluation, failing to holistically assess these advanced abilities. To address this, we introduce WiseEdit, a knowledge-intensive benchmark for comprehensive evaluation of cognition- and creativity-informed image editing, featuring deep task depth and broad knowledge breadth. Drawing an analogy to human cognitive creation, WiseEdit decomposes image editing into three cascaded steps, i.e., Awareness, Interpretation, and Imagination, each corresponding to a task that poses a challenge for models to complete at the specific step. It also encompasses complex tasks, where none of the three steps can be finished easily. Furthermore, WiseEdit incorporates three fundamental types of knowledge: Declarative, Procedural, and Metacognitive knowledge. Ultimately, WiseEdit comprises 1,220 test cases, objectively revealing the limitations of SoTA image editing models in knowledge-based cognitive reasoning and creative composition capabilities. The benchmark, evaluation code, and the generated images of each model will be made publicly available soon. Project Page: https://qnancy.github.io/wiseedit_project_page/.",
    "translation": "标题：WiseEdit：面向认知与创意驱动型图像编辑的基准评测框架\n\n摘要：当前图像编辑模型展现出新一代智能特性，能够支持基于认知与创意的图像编辑。然而，现有评测基准的评估范围过于局限，难以全面衡量这些高级能力。为此，我们提出WiseEdit——一个知识密集型的综合性评测基准，用于系统评估认知与创意驱动型图像编辑能力。该基准具有任务深度大、知识覆盖面广的特点。借鉴人类认知创作过程，WiseEdit将图像编辑解构为感知、解析与想象三个级联步骤，每个步骤对应特定任务，旨在检验模型在该环节的处理能力。同时，基准还包含需要综合处理三个步骤的复合型复杂任务。此外，WiseEdit整合了陈述性知识、程序性知识和元认知知识三大基础知识类型。最终构建的评测集包含1,220个测试案例，客观揭示了当前前沿图像编辑模型在基于知识的认知推理与创意构图能力方面的局限性。本基准框架、评估代码及各模型生成图像将于近期公开发布。项目主页：https://qnancy.github.io/wiseedit_project_page/。",
    "url": "https://huggingface.co/papers/2512.00387",
    "arxiv_url": "https://arxiv.org/abs/2512.00387"
  },
  {
    "title": "CauSight: Learning to Supersense for Visual Causal Discovery",
    "summary": "Causal thinking enables humans to understand not just what is seen, but why it happens. To replicate this capability in modern AI systems, we introduce the task of visual causal discovery. It requires models to infer cause-and-effect relations among visual entities across diverse scenarios instead of merely perceiving their presence. To this end, we first construct the Visual Causal Graph dataset (VCG-32K), a large-scale collection of over 32,000 images annotated with entity-level causal graphs, and further develop CauSight, a novel vision-language model to perform visual causal discovery through causally aware reasoning. Our training recipe integrates three components: (1) training data curation from VCG-32K, (2) Tree-of-Causal-Thought (ToCT) for synthesizing reasoning trajectories, and (3) reinforcement learning with a designed causal reward to refine the reasoning policy. Experiments show that CauSight outperforms GPT-4.1 on visual causal discovery, achieving over a threefold performance boost (21% absolute gain). Our code, model, and dataset are fully open-sourced at project page: https://github.com/OpenCausaLab/CauSight.",
    "translation": "标题：CauSight：面向视觉因果发现的学习式超感知\n\n摘要：因果思维使人类不仅能理解所见之物，更能洞悉其发生的原因。为在现代人工智能系统中复现这种能力，我们提出了视觉因果发现这一任务。该任务要求模型在多样场景中推断视觉实体间的因果关系，而非仅仅感知其存在。为此，我们首先构建了视觉因果图数据集（VCG-32K），这是一个包含超过32,000张标注了实体级因果图的大规模图像集合；进而开发了CauSight——一种通过因果感知推理进行视觉因果发现的新型视觉语言模型。我们的训练方案整合了三个核心部分：（1）基于VCG-32K的训练数据构建，（2）用于合成推理轨迹的因果思维树（ToCT），以及（3）通过设计的因果奖励进行强化学习以优化推理策略。实验表明，CauSight在视觉因果发现任务上显著优于GPT-4.1，实现了超过三倍的性能提升（绝对增益达21%）。我们的代码、模型及数据集已在项目页面全面开源：https://github.com/OpenCausaLab/CauSight。",
    "url": "https://huggingface.co/papers/2512.01827",
    "arxiv_url": "https://arxiv.org/abs/2512.01827"
  },
  {
    "title": "DreamingComics: A Story Visualization Pipeline via Subject and Layout Customized Generation using Video Models",
    "summary": "Current story visualization methods tend to position subjects solely by text and face challenges in maintaining artistic consistency. To address these limitations, we introduce DreamingComics, a layout-aware story visualization framework. We build upon a pretrained video diffusion-transformer (DiT) model, leveraging its spatiotemporal priors to enhance identity and style consistency. For layout-based position control, we propose RegionalRoPE, a region-aware positional encoding scheme that re-indexes embeddings based on the target layout. Additionally, we introduce a masked condition loss to further constrain each subject's visual features to their designated region. To infer layouts from natural language scripts, we integrate an LLM-based layout generator trained to produce comic-style layouts, enabling flexible and controllable layout conditioning. We present a comprehensive evaluation of our approach, showing a 29.2% increase in character consistency and a 36.2% increase in style similarity compared to previous methods, while displaying high spatial accuracy. Our project page is available at https://yj7082126.github.io/dreamingcomics/",
    "translation": "标题：DreamingComics：基于视频模型的主体与布局定制化生成的故事可视化流程\n\n摘要：现有故事可视化方法通常仅依赖文本来定位主体，且在维持艺术风格一致性方面面临挑战。为克服这些局限，本文提出DreamingComics——一种具备布局感知能力的故事可视化框架。该框架基于预训练的视频扩散变换器（DiT）模型构建，利用其时空先验知识增强角色身份与风格一致性。为实现基于布局的位置控制，我们提出RegionalRoPE区域感知位置编码方案，该方案根据目标布局对嵌入表示进行重索引。此外，我们引入掩码条件损失函数，进一步将每个主体的视觉特征约束至其指定区域。为从自然语言脚本推断布局，我们集成基于大语言模型的布局生成器，该生成器经训练可生成漫画风格布局，从而实现灵活可控的布局条件控制。通过系统性评估表明：相较于现有方法，本方法在角色一致性指标上提升29.2%，在风格相似度指标上提升36.2%，同时展现出较高的空间准确性。项目页面详见：https://yj7082126.github.io/dreamingcomics/",
    "url": "https://huggingface.co/papers/2512.01686",
    "arxiv_url": "https://arxiv.org/abs/2512.01686"
  },
  {
    "title": "IndicParam: Benchmark to evaluate LLMs on low-resource Indic Languages",
    "summary": "While large language models excel on high-resource multilingual tasks, low- and extremely low-resource Indic languages remain severely under-evaluated. We present IndicParam, a human-curated benchmark of over 13,000 multiple-choice questions covering 11 such languages (Nepali, Gujarati, Marathi, Odia as low-resource; Dogri, Maithili, Rajasthani, Sanskrit, Bodo, Santali, Konkani as extremely low-resource) plus Sanskrit-English code-mixed set. We evaluated 19 LLMs, both proprietary and open-weights, which reveals that even the top-performing GPT-5 reaches only 45.0% average accuracy, followed by DeepSeek-3.2 (43.1) and Claude-4.5 (42.7). We additionally label each question as knowledge-oriented or purely linguistic to discriminate factual recall from grammatical proficiency. Further, we assess the ability of LLMs to handle diverse question formats-such as list-based matching, assertion-reason pairs, and sequence ordering-alongside conventional multiple-choice questions. IndicParam provides insights into limitations of cross-lingual transfer and establishes a challenging benchmark for Indic languages. The dataset is available at https://huggingface.co/datasets/bharatgenai/IndicParam. Scripts to run benchmark are present at https://github.com/ayushbits/IndicParam.",
    "translation": "标题：IndicParam：面向低资源印度语言的大语言模型评估基准\n\n摘要：尽管大语言模型在高资源多语言任务中表现优异，但针对低资源及极低资源印度语言的系统性评估仍严重不足。本文提出IndicParam——一个包含超过13,000道人工标注选择题的评估基准，涵盖11种印度语言（低资源语言：尼泊尔语、古吉拉特语、马拉地语、奥里亚语；极低资源语言：多格拉语、迈蒂利语、拉贾斯坦语、梵语、博多语、桑塔利语、孔卡尼语）以及梵英混合语料集。通过对19个开源与闭源大语言模型的评估，我们发现即使表现最佳的GPT-5平均准确率也仅达45.0%，其次为DeepSeek-3.2（43.1%）和Claude-4.5（42.7%）。我们进一步将问题标注为知识导向型或纯语言型，以区分事实记忆能力与语法掌握能力。此外，除了传统选择题外，我们还评估了大语言模型处理多样化题型的能力，包括列表匹配题、论断推理对题和序列排序题。IndicParam揭示了跨语言迁移的局限性，为印度语言建立了具有挑战性的评估基准。数据集发布于https://huggingface.co/datasets/bharatgenai/IndicParam，基准测试脚本可通过https://github.com/ayushbits/IndicParam获取。",
    "url": "https://huggingface.co/papers/2512.00333",
    "arxiv_url": "https://arxiv.org/abs/2512.00333"
  },
  {
    "title": "A Hierarchical Framework for Humanoid Locomotion with Supernumerary Limbs",
    "summary": "The integration of Supernumerary Limbs (SLs) on humanoid robots poses a significant stability challenge due to the dynamic perturbations they introduce. This thesis addresses this issue by designing a novel hierarchical control architecture to improve humanoid locomotion stability with SLs. The core of this framework is a decoupled strategy that combines learning-based locomotion with model-based balancing. The low-level component consists of a walking gait for a Unitree H1 humanoid through imitation learning and curriculum learning. The high-level component actively utilizes the SLs for dynamic balancing. The effectiveness of the system is evaluated in a physics-based simulation under three conditions: baseline gait for an unladen humanoid (baseline walking), walking with a static SL payload (static payload), and walking with the active dynamic balancing controller (dynamic balancing). Our evaluation shows that the dynamic balancing controller improves stability. Compared to the static payload condition, the balancing strategy yields a gait pattern closer to the baseline and decreases the Dynamic Time Warping (DTW) distance of the CoM trajectory by 47\\%. The balancing controller also improves the re-stabilization within gait cycles and achieves a more coordinated anti-phase pattern of Ground Reaction Forces (GRF). The results demonstrate that a decoupled, hierarchical design can effectively mitigate the internal dynamic disturbances arising from the mass and movement of the SLs, enabling stable locomotion for humanoids equipped with functional limbs. Code and videos are available here: https://github.com/heyzbw/HuSLs.",
    "translation": "标题：一种用于超限肢体人形机器人步态的分层控制框架\n\n摘要：在人形机器人上集成超限肢体（SLs）会引入动态扰动，从而带来显著的稳定性挑战。本研究通过设计一种新颖的分层控制架构来解决这一问题，以提升配备超限肢体的人形机器人的步态稳定性。该框架的核心是一种解耦策略，将基于学习的步态控制与基于模型的平衡控制相结合。底层组件通过模仿学习与课程学习，为Unitree H1人形机器人生成行走步态。高层组件则主动利用超限肢体进行动态平衡调节。该系统在基于物理的仿真环境中通过三种条件进行评估：无负载人形机器人的基准步态（基准行走）、携带静态超限肢体负载行走（静态负载）以及使用主动动态平衡控制器行走（动态平衡）。评估结果表明，动态平衡控制器有效提升了稳定性。与静态负载条件相比，该平衡策略产生的步态模式更接近基准步态，并将质心轨迹的动态时间规整（DTW）距离降低了47%。此外，平衡控制器改善了步态周期内的再稳定能力，并实现了更协调的反相地面反作用力（GRF）模式。研究结果证明，这种解耦的分层设计能够有效缓解由超限肢体的质量与运动产生的内部动态干扰，从而使配备功能化超限肢体的人形机器人实现稳定运动。代码与演示视频可见：https://github.com/heyzbw/HuSLs。",
    "url": "https://huggingface.co/papers/2512.00077",
    "arxiv_url": "https://arxiv.org/abs/2512.00077"
  },
  {
    "title": "Generative Video Motion Editing with 3D Point Tracks",
    "summary": "Camera and object motions are central to a video's narrative. However, precisely editing these captured motions remains a significant challenge, especially under complex object movements. Current motion-controlled image-to-video (I2V) approaches often lack full-scene context for consistent video editing, while video-to-video (V2V) methods provide viewpoint changes or basic object translation, but offer limited control over fine-grained object motion. We present a track-conditioned V2V framework that enables joint editing of camera and object motion. We achieve this by conditioning a video generation model on a source video and paired 3D point tracks representing source and target motions. These 3D tracks establish sparse correspondences that transfer rich context from the source video to new motions while preserving spatiotemporal coherence. Crucially, compared to 2D tracks, 3D tracks provide explicit depth cues, allowing the model to resolve depth order and handle occlusions for precise motion editing. Trained in two stages on synthetic and real data, our model supports diverse motion edits, including joint camera/object manipulation, motion transfer, and non-rigid deformation, unlocking new creative potential in video editing.",
    "translation": "标题：基于三维点轨迹的生成式视频运动编辑\n\n摘要：相机与物体运动是视频叙事的关键要素。然而，对这些已捕捉运动进行精确编辑仍面临重大挑战，尤其在复杂物体运动场景下更为突出。现有运动控制的图像到视频方法常因缺乏完整场景上下文而难以实现一致性视频编辑，而视频到视频方法虽能提供视角变换或基础物体位移，却对细粒度物体运动的控制能力有限。本文提出一种轨迹条件化的视频到视频框架，能够实现相机与物体运动的联合编辑。该框架通过将视频生成模型约束于源视频及其配对的、表征源运动与目标运动的三维点轨迹来实现编辑功能。这些三维轨迹建立的稀疏对应关系，可在保持时空连贯性的同时，将丰富上下文信息从源视频迁移至新运动状态。关键之处在于，相较于二维轨迹，三维轨迹能提供显式深度线索，使模型能够解析深度顺序并处理遮挡问题，从而实现精确运动编辑。通过合成数据与真实数据的双阶段训练，本模型支持多种运动编辑任务，包括相机/物体联合操控、运动迁移及非刚性形变，为视频编辑领域开拓了新的创作可能性。",
    "url": "https://huggingface.co/papers/2512.02015",
    "arxiv_url": "https://arxiv.org/abs/2512.02015"
  },
  {
    "title": "MEGConformer: Conformer-Based MEG Decoder for Robust Speech and Phoneme Classification",
    "summary": "We present Conformer-based decoders for the LibriBrain 2025 PNPL competition, targeting two foundational MEG tasks: Speech Detection and Phoneme Classification. Our approach adapts a compact Conformer to raw 306-channel MEG signals, with a lightweight convolutional projection layer and task-specific heads. For Speech Detection, a MEG-oriented SpecAugment provided a first exploration of MEG-specific augmentation. For Phoneme Classification, we used inverse-square-root class weighting and a dynamic grouping loader to handle 100-sample averaged examples. In addition, a simple instance-level normalization proved critical to mitigate distribution shifts on the holdout split. Using the official Standard track splits and F1-macro for model selection, our best systems achieved 88.9% (Speech) and 65.8% (Phoneme) on the leaderboard, surpassing the competition baselines and ranking within the top-10 in both tasks. For further implementation details, the technical documentation, source code, and checkpoints are available at https://github.com/neural2speech/libribrain-experiments.",
    "translation": "标题：MEGConformer：基于Conformer的稳健语音与音素分类MEG解码器\n\n摘要：本文针对LibriBrain 2025 PNPL竞赛提出基于Conformer架构的解码器，聚焦于脑磁图（MEG）信号处理的两项基础任务：语音检测与音素分类。该方法将轻量化Conformer模型适配于原始306通道MEG信号，通过轻量卷积投影层与任务专用输出头实现特征提取。在语音检测任务中，我们首次探索了面向MEG信号的SpecAugment数据增强策略；在音素分类任务中，采用逆平方根类别加权与动态分组加载器处理百样本平均数据。此外，简单的实例级归一化策略对抑制预留数据集分布偏移具有关键作用。基于官方标准赛道划分方案与宏平均F1分数进行模型选择，我们的最优系统在排行榜上分别取得88.9%（语音检测）与65.8%（音素分类）的评估结果，超越竞赛基线模型并在两项任务中均位列前十。完整技术文档、源代码与模型检查点可通过https://github.com/neural2speech/libribrain-experiments获取。",
    "url": "https://huggingface.co/papers/2512.01443",
    "arxiv_url": "https://arxiv.org/abs/2512.01443"
  },
  {
    "title": "Doppler-Enhanced Deep Learning: Improving Thyroid Nodule Segmentation with YOLOv5 Instance Segmentation",
    "summary": "The increasing prevalence of thyroid cancer globally has led to the development of various computer-aided detection methods. Accurate segmentation of thyroid nodules is a critical first step in the development of AI-assisted clinical decision support systems. This study focuses on instance segmentation of thyroid nodules using YOLOv5 algorithms on ultrasound images. We evaluated multiple YOLOv5 variants (Nano, Small, Medium, Large, and XLarge) across two dataset versions, with and without doppler images. The YOLOv5-Large algorithm achieved the highest performance with a dice score of 91\\% and mAP of 0.87 on the dataset including doppler images. Notably, our results demonstrate that doppler images, typically excluded by physicians, can significantly improve segmentation performance. The YOLOv5-Small model achieved 79\\% dice score when doppler images were excluded, while including them improved performance across all model variants. These findings suggest that instance segmentation with YOLOv5 provides an effective real-time approach for thyroid nodule detection, with potential clinical applications in automated diagnostic systems.",
    "translation": "标题：多普勒增强深度学习：基于YOLOv5实例分割的甲状腺结节分割性能提升研究\n\n摘要：全球甲状腺癌发病率的持续上升推动了各类计算机辅助检测方法的发展。在人工智能辅助临床决策支持系统的开发中，甲状腺结节的精确分割是关键的第一步。本研究聚焦于利用YOLOv5算法对超声图像进行甲状腺结节实例分割。我们在包含与不包含多普勒图像的两个数据集版本上评估了多种YOLOv5变体（Nano、Small、Medium、Large及XLarge）。实验结果表明，在包含多普勒图像的数据集上，YOLOv5-Large算法取得了最佳性能，其Dice相似系数达91%，平均精度（mAP）为0.87。值得注意的是，通常被医师排除在外的多普勒图像能显著提升分割性能：当排除多普勒图像时，YOLOv5-Small模型的Dice系数为79%，而引入多普勒图像后所有模型变体的性能均得到提升。这些发现表明，基于YOLOv5的实例分割为甲状腺结节检测提供了有效的实时解决方案，在自动化诊断系统中具有临床转化潜力。",
    "url": "https://huggingface.co/papers/2512.00639",
    "arxiv_url": "https://arxiv.org/abs/2512.00639"
  },
  {
    "title": "OmniFusion: Simultaneous Multilingual Multimodal Translations via Modular Fusion",
    "summary": "There has been significant progress in open-source text-only translation large language models (LLMs) with better language coverage and quality. However, these models can be only used in cascaded pipelines for speech translation (ST), performing automatic speech recognition first followed by translation. This introduces additional latency, which is particularly critical in simultaneous ST (SimulST), and prevents the model from exploiting multimodal context, such as images, which can aid disambiguation. Pretrained multimodal foundation models (MMFMs) already possess strong perception and reasoning capabilities across multiple modalities, but generally lack the multilingual coverage and specialized translation performance of dedicated translation LLMs. To build an effective multimodal translation system, we propose an end-to-end approach that fuses MMFMs with translation LLMs. We introduce a novel fusion strategy that connects hidden states from multiple layers of a pretrained MMFM to a translation LLM, enabling joint end-to-end training. The resulting model, OmniFusion, built on Omni 2.5-7B as the MMFM and SeedX PPO-7B as the translation LLM, can perform speech-to-text, speech-and-image-to-text, and text-and-image-to-text translation. Experiments demonstrate that OmniFusion effectively leverages both audio and visual inputs, achieves a 1-second latency reduction in SimulST compared to cascaded pipelines and also improves the overall translation qualityCode is available at https://github.com/saikoneru/OmniFusion.",
    "translation": "标题：OmniFusion：基于模块化融合的多语言多模态同步翻译模型\n\n摘要：开源纯文本翻译大语言模型（LLMs）在语言覆盖范围与翻译质量方面已取得显著进展。然而，这些模型仅能通过级联流程应用于语音翻译（ST），即先进行自动语音识别再进行文本翻译。这种处理方式会引入额外延迟，在同步语音翻译（SimulST）场景中尤为关键，且无法利用多模态上下文（如图像）来辅助消歧。预训练多模态基础模型（MMFMs）已具备跨模态的强感知与推理能力，但通常缺乏专用翻译大语言模型的多语言覆盖能力与专业化翻译性能。为构建高效的多模态翻译系统，本文提出一种端到端方法，将多模态基础模型与翻译大语言模型进行融合。我们设计了一种新颖的融合策略，将预训练多模态基础模型多个隐藏层的状态连接至翻译大语言模型，从而实现联合端到端训练。基于Omni 2.5-7B作为多模态基础模型、SeedX PPO-7B作为翻译大语言模型构建的OmniFusion模型，能够执行语音到文本、语音与图像到文本、以及文本与图像到文本的翻译任务。实验表明，OmniFusion能有效利用音频与视觉输入，在同步语音翻译任务中相比级联流程降低1秒延迟，同时提升整体翻译质量。代码已开源：https://github.com/saikoneru/OmniFusion。",
    "url": "https://huggingface.co/papers/2512.00234",
    "arxiv_url": "https://arxiv.org/abs/2512.00234"
  },
  {
    "title": "Structured Extraction from Business Process Diagrams Using Vision-Language Models",
    "summary": "Business Process Model and Notation (BPMN) is a widely adopted standard for representing complex business workflows. While BPMN diagrams are often exchanged as visual images, existing methods primarily rely on XML representations for computational analysis. In this work, we present a pipeline that leverages Vision-Language Models (VLMs) to extract structured JSON representations of BPMN diagrams directly from images, without requiring source model files or textual annotations. We also incorporate optical character recognition (OCR) for textual enrichment and evaluate the generated element lists against ground truth data derived from the source XML files. Our approach enables robust component extraction in scenarios where original source files are unavailable. We benchmark multiple VLMs and observe performance improvements in several models when OCR is used for text enrichment. In addition, we conducted extensive statistical analyses of OCR-based enrichment methods and prompt ablation studies, providing a clearer understanding of their impact on model performance.",
    "translation": "标题：基于视觉语言模型的业务流程图表结构化信息提取\n\n摘要：业务流程模型与标注（BPMN）是广泛采用的复杂业务工作流表示标准。尽管BPMN图表通常以视觉图像形式交换，现有分析方法主要依赖XML表示进行计算处理。本研究提出一种处理流程，利用视觉语言模型（VLMs）直接从图像中提取BPMN图表的结构化JSON表示，无需源模型文件或文本标注。我们整合光学字符识别（OCR）技术进行文本增强，并通过源XML文件生成的真实数据评估所生成元素列表的准确性。该方法能够在原始源文件不可获取的场景下实现稳健的组件提取。我们对多种VLM模型进行基准测试，发现使用OCR进行文本增强时多个模型性能得到提升。此外，我们对基于OCR的增强方法进行了深入的统计分析及提示消融研究，从而更清晰地理解这些方法对模型性能的影响。",
    "url": "https://huggingface.co/papers/2511.22448",
    "arxiv_url": "https://arxiv.org/abs/2511.22448"
  }
]