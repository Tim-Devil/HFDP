[
  {
    "title": "Too Good to be Bad: On the Failure of LLMs to Role-Play Villains",
    "summary": "Large Language Models (LLMs) are increasingly tasked with creative\ngeneration, including the simulation of fictional characters. However, their\nability to portray non-prosocial, antagonistic personas remains largely\nunexamined. We hypothesize that the safety alignment of modern LLMs creates a\nfundamental conflict with the task of authentically role-playing morally\nambiguous or villainous characters. To investigate this, we introduce the Moral\nRolePlay benchmark, a new dataset featuring a four-level moral alignment scale\nand a balanced test set for rigorous evaluation. We task state-of-the-art LLMs\nwith role-playing characters from moral paragons to pure villains. Our\nlarge-scale evaluation reveals a consistent, monotonic decline in role-playing\nfidelity as character morality decreases. We find that models struggle most\nwith traits directly antithetical to safety principles, such as ``Deceitful''\nand ``Manipulative'', often substituting nuanced malevolence with superficial\naggression. Furthermore, we demonstrate that general chatbot proficiency is a\npoor predictor of villain role-playing ability, with highly safety-aligned\nmodels performing particularly poorly. Our work provides the first systematic\nevidence of this critical limitation, highlighting a key tension between model\nsafety and creative fidelity. Our benchmark and findings pave the way for\ndeveloping more nuanced, context-aware alignment methods.",
    "translation": "标题：过于良善难以为恶：大型语言模型在反派角色扮演中的失效研究\n\n摘要：大型语言模型正被日益应用于创造性生成任务，包括模拟虚构角色。然而，其在塑造非亲社会性对抗角色方面的能力仍待深入探究。我们提出假设：现代LLMs的安全对齐机制与真实扮演道德模糊或反派角色的任务存在根本性冲突。为此，我们引入道德角色扮演基准测试，该数据集包含四级道德对齐量表和平衡测试集以进行严格评估。我们要求前沿LLMs扮演从道德典范到纯粹反派的不同角色。大规模评估显示，随着角色道德水平的降低，角色扮演保真度呈现持续单调下降趋势。研究发现模型在表现与安全原则直接对立的特质（如“欺诈性”和“操纵性”）时最为困难，往往以浅层攻击性替代复杂的恶意刻画。此外，我们证明通用聊天机器人能力无法有效预测反派扮演表现，高度安全对齐的模型在此方面表现尤为不佳。本研究首次系统论证了这一关键局限性，揭示了模型安全性与创作保真度之间的核心矛盾。我们提出的基准测试与研究发现在开发更精细化、情境感知的对齐方法方面具有开创性意义。",
    "url": "https://huggingface.co/papers/2511.04962",
    "arxiv_url": "https://arxiv.org/abs/2511.04962"
  },
  {
    "title": "DeepEyesV2: Toward Agentic Multimodal Model",
    "summary": "Agentic multimodal models should not only comprehend text and images, but\nalso actively invoke external tools, such as code execution environments and\nweb search, and integrate these operations into reasoning. In this work, we\nintroduce DeepEyesV2 and explore how to build an agentic multimodal model from\nthe perspectives of data construction, training methods, and model evaluation.\nWe observe that direct reinforcement learning alone fails to induce robust\ntool-use behavior. This phenomenon motivates a two-stage training pipeline: a\ncold-start stage to establish tool-use patterns, and reinforcement learning\nstage to further refine tool invocation. We curate a diverse, moderately\nchallenging training dataset, specifically including examples where tool use is\nbeneficial. We further introduce RealX-Bench, a comprehensive benchmark\ndesigned to evaluate real-world multimodal reasoning, which inherently requires\nthe integration of multiple capabilities, including perception, search, and\nreasoning. We evaluate DeepEyesV2 on RealX-Bench and other representative\nbenchmarks, demonstrating its effectiveness across real-world understanding,\nmathematical reasoning, and search-intensive tasks. Moreover, DeepEyesV2\nexhibits task-adaptive tool invocation, tending to use image operations for\nperception tasks and numerical computations for reasoning tasks. Reinforcement\nlearning further enables complex tool combinations and allows model to\nselectively invoke tools based on context. We hope our study can provide\nguidance for community in developing agentic multimodal models.",
    "translation": "标题：DeepEyesV2：迈向具身化多模态模型\n\n摘要：具身化多模态模型不仅需要理解文本与图像，更应主动调用外部工具（如代码执行环境与网络搜索），并将这些操作融入推理过程。本研究提出DeepEyesV2，从数据构建、训练方法与模型评估三个维度系统探索具身化多模态模型的构建路径。我们发现单纯使用强化学习难以形成稳定的工具使用行为，这一现象促使我们设计两阶段训练流程：通过冷启动阶段建立工具使用范式，再经由强化学习阶段优化工具调用机制。我们构建了兼具多样性与适度挑战性的训练数据集，特别纳入工具使用具有显著效益的实例。同时提出RealX-Bench综合评估基准，该基准针对现实场景中的多模态推理任务设计，天然要求模型融合感知、搜索与推理等多元能力。在RealX-Bench及代表性基准测试中，DeepEyesV2在现实场景理解、数学推理及搜索密集型任务中均展现卓越性能。值得注意的是，该模型表现出任务自适应的工具调用特性：在感知任务中倾向使用图像操作，在推理任务中偏好数值计算。强化学习进一步促成了复杂工具组合能力，使模型能根据语境选择性调用工具。本研究期望为学界开发具身化多模态模型提供系统性参考。",
    "url": "https://huggingface.co/papers/2511.05271",
    "arxiv_url": "https://arxiv.org/abs/2511.05271"
  },
  {
    "title": "Visual Spatial Tuning",
    "summary": "Capturing spatial relationships from visual inputs is a cornerstone of\nhuman-like general intelligence. Several previous studies have tried to enhance\nthe spatial awareness of Vision-Language Models (VLMs) by adding extra expert\nencoders, which brings extra overhead and usually harms general capabilities.\nTo enhance the spatial ability in general architectures, we introduce Visual\nSpatial Tuning (VST), a comprehensive framework to cultivate VLMs with\nhuman-like visuospatial abilities, from spatial perception to reasoning. We\nfirst attempt to enhance spatial perception in VLMs by constructing a\nlarge-scale dataset termed VST-P, which comprises 4.1 million samples spanning\n19 skills across single views, multiple images, and videos. Then, we present\nVST-R, a curated dataset with 135K samples that instruct models to reason in\nspace. In particular, we adopt a progressive training pipeline: supervised\nfine-tuning to build foundational spatial knowledge, followed by reinforcement\nlearning to further improve spatial reasoning abilities. Without the\nside-effect to general capabilities, the proposed VST consistently achieves\nstate-of-the-art results on several spatial benchmarks, including 34.8% on\nMMSI-Bench and 61.2% on VSIBench. It turns out that the\nVision-Language-Action models can be significantly enhanced with the proposed\nspatial tuning paradigm, paving the way for more physically grounded AI.",
    "translation": "标题：视觉空间调优\n\n摘要：从视觉输入中捕捉空间关系是实现类人通用智能的基石。先前研究多通过引入额外专家编码器来增强视觉语言模型的空间感知能力，但这不仅增加了计算开销，往往还会损害模型的通用性能。为在通用架构中提升空间能力，我们提出视觉空间调优（VST）——一个培育视觉语言模型具备从空间感知到推理的类人视觉空间能力的完整框架。我们首先通过构建包含410万样本的大规模数据集VST-P来增强视觉语言模型的空间感知能力，该数据集涵盖单视图、多图像和视频三大范畴的19项空间技能。继而推出VST-R数据集，其13.5万条样本可指导模型进行空间推理。我们特别采用渐进式训练流程：先通过监督微调建立基础空间知识，再通过强化学习进一步提升空间推理能力。在保持通用性能不受影响的前提下，所提出的VST框架在多项空间基准测试中持续取得最优结果，包括在MMSI-Bench达到34.8%的准确率，在VSIBench达到61.2%的准确率。研究表明，通过本文提出的空间调优范式可显著增强视觉-语言-动作模型的能力，为开发更具物理现实感的人工智能铺平道路。",
    "url": "https://huggingface.co/papers/2511.05491",
    "arxiv_url": "https://arxiv.org/abs/2511.05491"
  },
  {
    "title": "VeriCoT: Neuro-symbolic Chain-of-Thought Validation via Logical\n  Consistency Checks",
    "summary": "LLMs can perform multi-step reasoning through Chain-of-Thought (CoT), but\nthey cannot reliably verify their own logic. Even when they reach correct\nanswers, the underlying reasoning may be flawed, undermining trust in\nhigh-stakes scenarios. To mitigate this issue, we introduce VeriCoT, a\nneuro-symbolic method that extracts and verifies formal logical arguments from\nCoT reasoning. VeriCoT formalizes each CoT reasoning step into first-order\nlogic and identifies premises that ground the argument in source context,\ncommonsense knowledge, or prior reasoning steps. The symbolic representation\nenables automated solvers to verify logical validity while the NL premises\nallow humans and systems to identify ungrounded or fallacious reasoning steps.\nExperiments on the ProofWriter, LegalBench, and BioASQ datasets show VeriCoT\neffectively identifies flawed reasoning, and serves as a strong predictor of\nfinal answer correctness. We also leverage VeriCoT's verification signal for\n(1) inference-time self-reflection, (2) supervised fine-tuning (SFT) on\nVeriCoT-distilled datasets and (3) preference fine-tuning (PFT) with direct\npreference optimization (DPO) using verification-based pairwise rewards,\nfurther improving reasoning validity and accuracy.",
    "translation": "标题：VeriCoT：基于逻辑一致性检验的神经符号思维链验证方法\n\n摘要：大型语言模型能够通过思维链进行多步推理，但其无法可靠地验证自身逻辑。即使在获得正确答案的情况下，底层推理过程仍可能存在缺陷，这在高风险场景中会削弱可信度。为缓解该问题，我们提出VeriCoT——一种从思维链推理中提取并验证形式逻辑论证的神经符号方法。该方法将思维链的每个推理步骤形式化为一阶逻辑，并识别使论证植根于源语境、常识知识或先前推理步骤的前提条件。符号化表征支持自动化求解器验证逻辑有效性，而自然语言前提则允许人类和系统识别未扎根或存在谬误的推理步骤。在ProofWriter、LegalBench和BioASQ数据集上的实验表明，VeriCoT能有效识别缺陷推理，并作为最终答案正确性的强预测指标。我们进一步利用VeriCoT的验证信号实现：(1) 推理时自反思机制，(2) 基于VeriCoT蒸馏数据集的监督微调，以及(3) 采用带验证导向配对奖励的直接偏好优化进行偏好微调，从而持续提升推理的有效性与准确性。",
    "url": "https://huggingface.co/papers/2511.04662",
    "arxiv_url": "https://arxiv.org/abs/2511.04662"
  },
  {
    "title": "Dense Motion Captioning",
    "summary": "Recent advances in 3D human motion and language integration have primarily\nfocused on text-to-motion generation, leaving the task of motion understanding\nrelatively unexplored. We introduce Dense Motion Captioning, a novel task that\naims to temporally localize and caption actions within 3D human motion\nsequences. Current datasets fall short in providing detailed temporal\nannotations and predominantly consist of short sequences featuring few actions.\nTo overcome these limitations, we present the Complex Motion Dataset (CompMo),\nthe first large-scale dataset featuring richly annotated, complex motion\nsequences with precise temporal boundaries. Built through a carefully designed\ndata generation pipeline, CompMo includes 60,000 motion sequences, each\ncomposed of multiple actions ranging from at least two to ten, accurately\nannotated with their temporal extents. We further present DEMO, a model that\nintegrates a large language model with a simple motion adapter, trained to\ngenerate dense, temporally grounded captions. Our experiments show that DEMO\nsubstantially outperforms existing methods on CompMo as well as on adapted\nbenchmarks, establishing a robust baseline for future research in 3D motion\nunderstanding and captioning.",
    "translation": "标题：稠密运动描述生成\n\n摘要：当前三维人体运动与语言融合的研究主要集中在文本到动作生成任务，而对运动理解任务的探索相对不足。本文提出稠密运动描述生成这一新型任务，旨在对三维人体运动序列中的动作进行时序定位与描述。现有数据集普遍存在时序标注细节不足的问题，且主要包含动作数量有限的短序列。为突破这些局限，我们构建了复杂运动数据集——首个包含精确时序边界标注的大规模复杂运动序列数据集。通过精心设计的数据生成流程，该数据集包含60,000个运动序列，每个序列由至少2个至多10个动作组合而成，并配有精确的时序范围标注。我们进一步提出DEMO模型，该模型通过简易运动适配器整合大语言模型，经过训练可生成具有时序锚点的稠密描述。实验结果表明，DEMO在CompMo数据集及经适配的基准测试中均显著优于现有方法，为三维运动理解与描述任务的后续研究建立了坚实的基准。",
    "url": "https://huggingface.co/papers/2511.05369",
    "arxiv_url": "https://arxiv.org/abs/2511.05369"
  },
  {
    "title": "Towards Mitigating Hallucinations in Large Vision-Language Models by\n  Refining Textual Embeddings",
    "summary": "In this work, we identify an inherent bias in prevailing LVLM architectures\ntoward the language modality, largely resulting from the common practice of\nsimply appending visual embeddings to the input text sequence. To address this,\nwe propose a simple yet effective method that refines textual embeddings by\nintegrating average-pooled visual features. Our approach demonstrably improves\nvisual grounding and significantly reduces hallucinations on established\nbenchmarks. While average pooling offers a straightforward, robust, and\nefficient means of incorporating visual information, we believe that more\nsophisticated fusion methods could further enhance visual grounding and\ncross-modal alignment. Given that the primary focus of this work is to\nhighlight the modality imbalance and its impact on hallucinations -- and to\nshow that refining textual embeddings with visual information mitigates this\nissue -- we leave exploration of advanced fusion strategies for future work.",
    "translation": "标题：通过优化文本嵌入缓解大型视觉语言模型中的幻觉现象\n\n摘要：本研究揭示了主流LVLM架构中存在的对语言模态的内在偏好，这主要源于当前普遍将视觉嵌入简单附加到输入文本序列的做法。针对这一问题，我们提出了一种简单而有效的方法，通过整合平均池化后的视觉特征来优化文本嵌入。实验证明，我们的方法能显著提升视觉基础能力，并在权威基准测试中有效减少幻觉现象。虽然平均池化提供了简单、稳健且高效的视觉信息融合方式，但我们认为更复杂的融合方法有望进一步强化视觉基础与跨模态对齐。鉴于本研究的核心目标是揭示模态不平衡及其对幻觉现象的影响，并论证通过视觉信息优化文本嵌入能够缓解此问题，我们将更先进的融合策略探索留待未来研究。\n\n请按照以下格式返回：\n标题：[通过优化文本嵌入缓解大型视觉语言模型中的幻觉现象]\n摘要：[本研究揭示了主流LVLM架构中存在的对语言模态的内在偏好，这主要源于当前普遍将视觉嵌入简单附加到输入文本序列的做法。针对这一问题，我们提出了一种简单而有效的方法，通过整合平均池化后的视觉特征来优化文本嵌入。实验证明，我们的方法能显著提升视觉基础能力，并在权威基准测试中有效减少幻觉现象。虽然平均池化提供了简单、稳健且高效的视觉信息融合方式，但我们认为更复杂的融合方法有望进一步强化视觉基础与跨模态对齐。鉴于本研究的核心目标是揭示模态不平衡及其对幻觉现象的影响，并论证通过视觉信息优化文本嵌入能够缓解此问题，我们将更先进的融合策略探索留待未来研究。]",
    "url": "https://huggingface.co/papers/2511.05017",
    "arxiv_url": "https://arxiv.org/abs/2511.05017"
  },
  {
    "title": "Real-Time Reasoning Agents in Evolving Environments",
    "summary": "Agents in the real world must make not only logical but also timely\njudgments. This requires continuous awareness of the dynamic environment:\nhazards emerge, opportunities arise, and other agents act, while the agent's\nreasoning is still unfolding. Despite advances in language model reasoning,\nexisting approaches fail to account for this dynamic nature. We introduce\nreal-time reasoning as a new problem formulation for agents in evolving\nenvironments and build Real-Time Reasoning Gym to demonstrate it. We study two\nparadigms for deploying language models in agents: (1) reactive agents, which\nemploy language models with bounded reasoning computation for rapid responses,\nand (2) planning agents, which allow extended reasoning computation for complex\nproblems. Our experiments show that even state-of-the-art models struggle with\nmaking logical and timely judgments in either paradigm. To address this\nlimitation, we propose AgileThinker, which simultaneously engages both\nreasoning paradigms. AgileThinker consistently outperforms agents engaging only\none reasoning paradigm as the task difficulty and time pressure rise,\neffectively balancing reasoning depth and response latency. Our work\nestablishes real-time reasoning as a critical testbed for developing practical\nagents and provides a foundation for research in temporally constrained AI\nsystems, highlighting a path toward real-time capable agents.",
    "translation": "标题：动态环境中的实时推理智能体\n\n摘要：现实世界中的智能体不仅需要做出符合逻辑的判断，更需确保决策的时效性。这要求智能体持续感知动态环境的变化：危险可能突然出现，机遇转瞬即逝，其他智能体也在同时行动，而智能体自身的推理过程仍在进行。尽管语言模型推理技术已取得显著进展，现有方法仍未能充分考虑这种动态特性。我们提出了实时推理这一针对动态环境智能体的新问题框架，并构建了实时推理验证平台予以实证。我们研究了两种在智能体中部署语言模型的范式：（1）反应式智能体——采用有限计算资源的语言模型实现快速响应；（2）规划式智能体——允许消耗更多计算资源以解决复杂问题。实验表明，即使最先进的模型在两种范式下都难以同时保证逻辑正确性与时效性。为此，我们提出AgileThinker框架，通过并行运行双重推理范式来突破这一局限。随着任务难度和时间压力的提升，AgileThinker始终优于单一推理范式的智能体，有效实现了推理深度与响应延迟的平衡。本研究将实时推理确立为开发实用智能体的关键测试基准，为时间约束下人工智能系统的研究奠定基础，指明了实现实时智能体的发展路径。",
    "url": "https://huggingface.co/papers/2511.04898",
    "arxiv_url": "https://arxiv.org/abs/2511.04898"
  },
  {
    "title": "Jailbreaking in the Haystack",
    "summary": "Recent advances in long-context language models (LMs) have enabled\nmillion-token inputs, expanding their capabilities across complex tasks like\ncomputer-use agents. Yet, the safety implications of these extended contexts\nremain unclear. To bridge this gap, we introduce NINJA (short for\nNeedle-in-haystack jailbreak attack), a method that jailbreaks aligned LMs by\nappending benign, model-generated content to harmful user goals. Critical to\nour method is the observation that the position of harmful goals play an\nimportant role in safety. Experiments on standard safety benchmark, HarmBench,\nshow that NINJA significantly increases attack success rates across\nstate-of-the-art open and proprietary models, including LLaMA, Qwen, Mistral,\nand Gemini. Unlike prior jailbreaking methods, our approach is low-resource,\ntransferable, and less detectable. Moreover, we show that NINJA is\ncompute-optimal -- under a fixed compute budget, increasing context length can\noutperform increasing the number of trials in best-of-N jailbreak. These\nfindings reveal that even benign long contexts -- when crafted with careful\ngoal positioning -- introduce fundamental vulnerabilities in modern LMs.",
    "translation": "标题：大海捞针式越狱攻击  \n摘要：长上下文语言模型的最新进展已支持百万级令牌输入，显著提升了其在计算机使用代理等复杂任务中的能力。然而，这种扩展上下文的安全影响尚不明确。为填补这一空白，我们提出NINJA（\"大海捞针\"越狱攻击的简称），该方法通过将模型生成的良性内容附加到恶意用户目标上，实现对对齐语言模型的越狱。我们方法的关键在于发现恶意目标在上下文中的位置对安全性具有重要影响。在标准安全基准测试HarmBench上的实验表明，NINJA显著提升了包括LLaMA、Qwen、Mistral和Gemini在内的前沿开源与专有模型的攻击成功率。与现有越狱方法不同，本方法具有低资源消耗、强可迁移性和低可检测性的特点。此外，我们证明NINJA具有计算最优性——在固定计算预算下，增加上下文长度优于增加N次尝试中的最优越狱次数。这些发现表明，即便是良性长上下文——若经过精心的目标位置设计——也会在现代语言模型中引发根本性安全漏洞。",
    "url": "https://huggingface.co/papers/2511.04707",
    "arxiv_url": "https://arxiv.org/abs/2511.04707"
  },
  {
    "title": "HAFixAgent: History-Aware Automated Program Repair Agent",
    "summary": "Automated program repair (APR) has recently shifted toward large language\nmodels and agent-based systems, yet most systems rely on local snapshot\ncontext, overlooking repository history. Prior work shows that repository\nhistory helps repair single-line bugs, since the last commit touching the buggy\nline is often the bug-introducing one. In this paper, we investigate whether\nrepository history can also improve agentic APR systems at scale, especially\nfor complex multi-hunk bugs. We present HAFixAgent, a History-Aware Bug-Fixing\nAgent that injects blame-derived repository heuristics into its repair loop. A\npreliminary study of all 854 real-world bugs from Defects4J motivates our\ndesign, showing that bug-relevant history is both widely available and highly\nconcentrated. Empirical comparison of HAFixAgent with two state-of-the-art\nbaselines shows: (1) Effectiveness: HAFixAgent significantly improves over the\nagent-based baseline (by 212.3%) and the multi-hunk baseline (by 29.9%). (2)\nEfficiency: history does not significantly increase agent steps and keeps token\ncosts comparable, with notably lower median costs for complex\nmulti-file-multi-hunk bugs. (3) Practicality: combining different historical\nheuristics repairs more bugs, offering a clear cost-benefit trade-off.\nHAFixAgent offers a practical recipe for history-aware agentic APR: ground the\nagent in version control history, prioritize diff-based historical context, and\nintegrate complementary heuristics when needed.",
    "translation": "标题：HAFixAgent：具备历史感知能力的自动化程序修复智能体\n\n摘要：自动化程序修复领域近期正朝着大语言模型与智能体系统方向发展，但现有系统大多依赖本地快照上下文，忽视了代码仓库的历史信息。已有研究表明，仓库历史有助于修复单行缺陷，因为最近一次修改缺陷代码的提交往往就是引入该缺陷的提交。本文系统性地探究仓库历史能否在更大范围内提升智能体式APR系统的修复能力，特别是针对复杂的多代码块缺陷。我们提出HAFixAgent——一种具备历史感知能力的缺陷修复智能体，该智能体将基于代码溯源信息的仓库启发式规则注入其修复循环。通过对Defects4J数据集中854个真实缺陷的初步研究，我们发现与缺陷相关的历史信息不仅广泛存在且高度集中，这一发现为系统设计提供了理论依据。HAFixAgent与两种最先进基准系统的实证对比表明：（1）有效性：相较基于智能体的基准系统提升212.3%，针对多代码块缺陷的基准系统提升29.9%；（2）效率：历史信息未显著增加智能体执行步骤，且保持相当的令牌成本，对于复杂的多文件-多代码块缺陷其中位数成本显著更低；（3）实用性：结合不同历史启发式规则可修复更多缺陷，形成明确的成本效益权衡。HAFixAgent为构建历史感知的智能体式APR系统提供了实用方案：将智能体锚定于版本控制历史，优先采用基于差异对比的历史上下文，并在需要时整合互补性启发式规则。",
    "url": "https://huggingface.co/papers/2511.01047",
    "arxiv_url": "https://arxiv.org/abs/2511.01047"
  },
  {
    "title": "CritiCal: Can Critique Help LLM Uncertainty or Confidence Calibration?",
    "summary": "Accurate confidence calibration in Large Language Models (LLMs) is critical\nfor safe use in high-stakes domains, where clear verbalized confidence enhances\nuser trust. Traditional methods that mimic reference confidence expressions\noften fail to capture the reasoning needed for accurate confidence assessment.\nWe propose natural language critiques as a solution, ideally suited for\nconfidence calibration, as precise gold confidence labels are hard to obtain\nand often require multiple generations. This paper studies how natural language\ncritiques can enhance verbalized confidence, addressing: (1) What to critique:\nuncertainty (question-focused) or confidence (answer-specific)? Analysis shows\nconfidence suits multiple-choice tasks, while uncertainty excels in open-ended\nscenarios. (2) How to critique: self-critique or critique calibration training?\nWe propose Self-Critique, enabling LLMs to critique and optimize their\nconfidence beyond mere accuracy, and CritiCal, a novel Critique Calibration\ntraining method that leverages natural language critiques to improve confidence\ncalibration, moving beyond direct numerical optimization. Experiments show that\nCritiCal significantly outperforms Self-Critique and other competitive\nbaselines, even surpassing its teacher model, GPT-4o, in complex reasoning\ntasks. CritiCal also shows robust generalization in out-of-distribution\nsettings, advancing LLM's reliability.",
    "translation": "标题：CritiCal：批判性反馈能否提升大语言模型的不确定性或置信度校准？\n\n摘要：大语言模型（LLM）的精确置信度校准对其在高风险领域的安全应用至关重要，清晰的言语化置信度能有效增强用户信任。传统方法通过模仿参考置信度表达往往难以捕捉准确置信评估所需的推理过程。我们提出采用自然语言批判作为解决方案，该方法特别适合置信度校准——由于精确的黄金置信标签难以获取且常需多次生成。本文研究自然语言批判如何增强言语化置信度，重点探讨：（1）批判对象：应针对不确定性（问题导向）还是置信度（答案特定）？分析表明置信度适用于多项选择题任务，而不确定性在开放式场景中表现更优；（2）批判方式：采用自我批判还是批判校准训练？我们提出自我批判方法使LLM能超越准确率指标对自身置信度进行批判优化，同时创新性地提出CritiCal——一种基于自然语言批判的校准训练方法，突破直接数值优化的局限。实验表明，CritiCal在复杂推理任务中显著优于自我批判及其他竞争基线，甚至超越其教师模型GPT-4o。在分布外场景中，CritiCal也展现出强大的泛化能力，为提升LLM可靠性提供了新路径。",
    "url": "https://huggingface.co/papers/2510.24505",
    "arxiv_url": "https://arxiv.org/abs/2510.24505"
  }
]