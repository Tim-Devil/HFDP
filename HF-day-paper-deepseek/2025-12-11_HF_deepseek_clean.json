[
  {
    "title": "StereoWorld: Geometry-Aware Monocular-to-Stereo Video Generation",
    "summary": "The growing adoption of XR devices has fueled strong demand for high-quality stereo video, yet its production remains costly and artifact-prone. To address this challenge, we present StereoWorld, an end-to-end framework that repurposes a pretrained video generator for high-fidelity monocular-to-stereo video generation. Our framework jointly conditions the model on the monocular video input while explicitly supervising the generation with a geometry-aware regularization to ensure 3D structural fidelity. A spatio-temporal tiling scheme is further integrated to enable efficient, high-resolution synthesis. To enable large-scale training and evaluation, we curate a high-definition stereo video dataset containing over 11M frames aligned to natural human interpupillary distance (IPD). Extensive experiments demonstrate that StereoWorld substantially outperforms prior methods, generating stereo videos with superior visual fidelity and geometric consistency. The project webpage is available at https://ke-xing.github.io/StereoWorld/.",
    "translation": "标题：StereoWorld：几何感知的单目到立体视频生成框架\n\n摘要：随着XR设备的日益普及，市场对高质量立体视频的需求持续增长，但其制作仍面临成本高昂且易产生视觉伪影的挑战。为解决这一问题，我们提出了StereoWorld——一个端到端的框架，通过改造预训练视频生成模型，实现高保真的单目到立体视频生成。该框架在模型中以单目视频输入为联合条件，同时通过几何感知正则化对生成过程进行显式监督，以确保三维结构的保真度。我们还进一步整合了时空分块机制，以实现高效的高分辨率合成。为支持大规模训练与评估，我们构建了一个高清立体视频数据集，包含超过1100万帧与自然人眼瞳距（IPD）对齐的视频帧。大量实验表明，StereoWorld在视觉保真度与几何一致性方面显著优于现有方法，能够生成质量更优的立体视频。项目页面详见：https://ke-xing.github.io/StereoWorld/。",
    "url": "https://huggingface.co/papers/2512.09363",
    "arxiv_url": "https://arxiv.org/abs/2512.09363"
  },
  {
    "title": "BrainExplore: Large-Scale Discovery of Interpretable Visual Representations in the Human Brain",
    "summary": "Understanding how the human brain represents visual concepts, and in which brain regions these representations are encoded, remains a long-standing challenge. Decades of work have advanced our understanding of visual representations, yet brain signals remain large and complex, and the space of possible visual concepts is vast. As a result, most studies remain small-scale, rely on manual inspection, focus on specific regions and properties, and rarely include systematic validation. We present a large-scale, automated framework for discovering and explaining visual representations across the human cortex. Our method comprises two main stages. First, we discover candidate interpretable patterns in fMRI activity through unsupervised, data-driven decomposition methods. Next, we explain each pattern by identifying the set of natural images that most strongly elicit it and generating a natural-language description of their shared visual meaning. To scale this process, we introduce an automated pipeline that tests multiple candidate explanations, assigns quantitative reliability scores, and selects the most consistent description for each voxel pattern. Our framework reveals thousands of interpretable patterns spanning many distinct visual concepts, including fine-grained representations previously unreported.",
    "translation": "标题：BrainExplore：大规模发现人脑中可解释的视觉表征\n\n摘要：理解人脑如何表征视觉概念以及这些表征在哪些脑区编码，仍然是一个长期存在的挑战。数十年的研究增进了我们对视觉表征的理解，但脑信号依然庞大而复杂，且可能的视觉概念空间极为广阔。因此，大多数研究仍局限于小规模、依赖人工检查、聚焦于特定区域和属性，并很少包含系统性验证。本文提出一个大规模、自动化的框架，用于发现和解释跨越人类大脑皮层的视觉表征。我们的方法包含两个主要阶段：首先，通过无监督的数据驱动分解方法，从功能磁共振成像活动中发现候选的可解释模式；其次，通过识别最能激发该模式的自然图像集，并生成对其共享视觉意义的自然语言描述，来解释每个模式。为实现规模化处理，我们引入了一个自动化流程，用于测试多个候选解释、分配定量可靠性评分，并为每个体素模式选择最一致的描述。我们的框架揭示了数千个可解释模式，涵盖许多不同的视觉概念，包括此前未报道过的细粒度表征。",
    "url": "https://huggingface.co/papers/2512.08560",
    "arxiv_url": "https://arxiv.org/abs/2512.08560"
  },
  {
    "title": "OmniPSD: Layered PSD Generation with Diffusion Transformer",
    "summary": "Recent advances in diffusion models have greatly improved image generation and editing, yet generating or reconstructing layered PSD files with transparent alpha channels remains highly challenging. We propose OmniPSD, a unified diffusion framework built upon the Flux ecosystem that enables both text-to-PSD generation and image-to-PSD decomposition through in-context learning. For text-to-PSD generation, OmniPSD arranges multiple target layers spatially into a single canvas and learns their compositional relationships through spatial attention, producing semantically coherent and hierarchically structured layers. For image-to-PSD decomposition, it performs iterative in-context editing, progressively extracting and erasing textual and foreground components to reconstruct editable PSD layers from a single flattened image. An RGBA-VAE is employed as an auxiliary representation module to preserve transparency without affecting structure learning. Extensive experiments on our new RGBA-layered dataset demonstrate that OmniPSD achieves high-fidelity generation, structural consistency, and transparency awareness, offering a new paradigm for layered design generation and decomposition with diffusion transformers.",
    "translation": "标题：OmniPSD：基于扩散Transformer的分层PSD生成方法\n\n摘要：扩散模型的最新进展显著提升了图像生成与编辑能力，然而生成或重建具有透明Alpha通道的分层PSD文件仍极具挑战性。本文提出OmniPSD——一个基于Flux生态系统的统一扩散框架，通过上下文学习同时实现文本到PSD生成与图像到PSD分解。在文本到PSD生成任务中，OmniPSD将多个目标图层空间排列至单一画布，并通过空间注意力机制学习其组合关系，从而生成语义连贯且层次结构清晰的图层。在图像到PSD分解任务中，该框架执行迭代式上下文编辑，逐步提取并擦除文本与前景元素，从单张扁平化图像重建可编辑的PSD图层。我们引入RGBA-VAE作为辅助表征模块，在不影响结构学习的前提下保持透明度信息。基于新建的RGBA分层数据集的大量实验表明，OmniPSD能够实现高保真生成、结构一致性与透明度感知，为基于扩散Transformer的分层设计生成与分解提供了新范式。",
    "url": "https://huggingface.co/papers/2512.09247",
    "arxiv_url": "https://arxiv.org/abs/2512.09247"
  },
  {
    "title": "Composing Concepts from Images and Videos via Concept-prompt Binding",
    "summary": "Visual concept composition, which aims to integrate different elements from images and videos into a single, coherent visual output, still falls short in accurately extracting complex concepts from visual inputs and flexibly combining concepts from both images and videos. We introduce Bind & Compose, a one-shot method that enables flexible visual concept composition by binding visual concepts with corresponding prompt tokens and composing the target prompt with bound tokens from various sources. It adopts a hierarchical binder structure for cross-attention conditioning in Diffusion Transformers to encode visual concepts into corresponding prompt tokens for accurate decomposition of complex visual concepts. To improve concept-token binding accuracy, we design a Diversify-and-Absorb Mechanism that uses an extra absorbent token to eliminate the impact of concept-irrelevant details when training with diversified prompts. To enhance the compatibility between image and video concepts, we present a Temporal Disentanglement Strategy that decouples the training process of video concepts into two stages with a dual-branch binder structure for temporal modeling. Evaluations demonstrate that our method achieves superior concept consistency, prompt fidelity, and motion quality over existing approaches, opening up new possibilities for visual creativity.",
    "translation": "标题：通过概念提示绑定实现图像与视频的概念组合\n\n摘要：视觉概念组合旨在将图像和视频中的不同元素整合为单一、连贯的视觉输出，但现有方法在从视觉输入中准确提取复杂概念、灵活组合图像与视频概念方面仍存在不足。本文提出Bind & Compose方法，这是一种单次学习技术，通过将视觉概念与对应的提示词绑定，并利用来自多源的绑定词组合目标提示，实现灵活的视觉概念组合。该方法采用分层绑定器结构，在扩散变换器中通过跨注意力机制将视觉概念编码为对应的提示词，从而实现对复杂视觉概念的精确解构。为提高概念-词绑定的准确性，我们设计了\"多样化-吸收机制\"，通过引入额外的吸收词来消除训练过程中多样化提示带来的无关细节干扰。为增强图像与视频概念的兼容性，我们提出\"时序解耦策略\"，通过双分支绑定器结构将视频概念训练解耦为两个阶段，以优化时序建模。实验评估表明，本方法在概念一致性、提示保真度和运动质量方面均优于现有技术，为视觉创意应用开辟了新可能。",
    "url": "https://huggingface.co/papers/2512.09824",
    "arxiv_url": "https://arxiv.org/abs/2512.09824"
  },
  {
    "title": "InfiniteVL: Synergizing Linear and Sparse Attention for Highly-Efficient, Unlimited-Input Vision-Language Models",
    "summary": "Window attention and linear attention represent two principal strategies for mitigating the quadratic complexity and ever-growing KV cache in Vision-Language Models (VLMs). However, we observe that window-based VLMs suffer performance degradation when sequence length exceeds the window size, while linear attention underperforms on information-intensive tasks such as OCR and document understanding. To overcome these limitations, we propose InfiniteVL, a linear-complexity VLM architecture that synergizes sliding window attention (SWA) with Gated DeltaNet. For achieving competitive multimodal performance under constrained resources, we design a three-stage training strategy comprising distillation pretraining, instruction tuning, and long-sequence SFT. Remarkably, using less than 2\\% of the training data required by leading VLMs, InfiniteVL not only substantially outperforms previous linear-complexity VLMs but also matches the performance of leading Transformer-based VLMs, while demonstrating effective long-term memory retention. Compared to similar-sized Transformer-based VLMs accelerated by FlashAttention-2, InfiniteVL achieves over 3.6\\times inference speedup while maintaining constant latency and memory footprint. In streaming video understanding scenarios, it sustains a stable 24 FPS real-time prefill speed while preserving long-term memory cache. Code and models are available at https://github.com/hustvl/InfiniteVL.",
    "translation": "标题：InfiniteVL：融合线性与稀疏注意力实现高效、无限输入的视觉语言模型\n\n摘要：窗口注意力和线性注意力是缓解视觉语言模型（VLMs）中二次复杂度与持续增长的KV缓存的两类主要策略。然而，我们观察到基于窗口的VLMs在序列长度超过窗口大小时会出现性能下降，而线性注意力在OCR和文档理解等信息密集型任务上表现欠佳。为克服这些限制，本文提出InfiniteVL，一种线性复杂度的VLM架构，它将滑动窗口注意力（SWA）与门控DeltaNet相融合。为了在有限资源下实现具有竞争力的多模态性能，我们设计了一个包含蒸馏预训练、指令微调和长序列监督微调的三阶段训练策略。值得注意的是，在使用不到主流VLM所需训练数据2%的情况下，InfiniteVL不仅显著超越了以往的线性复杂度VLMs，而且达到了基于Transformer的主流VLMs的性能水平，同时展现出有效的长期记忆保持能力。与通过FlashAttention-2加速的同类规模Transformer VLM相比，InfiniteVL在保持恒定延迟和内存占用的同时，实现了超过3.6倍的推理加速。在流式视频理解场景中，它能够维持稳定的24 FPS实时预填充速度，同时保持长期记忆缓存。代码与模型已发布于https://github.com/hustvl/InfiniteVL。",
    "url": "https://huggingface.co/papers/2512.08829",
    "arxiv_url": "https://arxiv.org/abs/2512.08829"
  },
  {
    "title": "Rethinking Chain-of-Thought Reasoning for Videos",
    "summary": "Chain-of-thought (CoT) reasoning has been highly successful in solving complex tasks in natural language processing, and recent multimodal large language models (MLLMs) have extended this paradigm to video reasoning. However, these models typically build on lengthy reasoning chains and large numbers of input visual tokens. Motivated by empirical observations from our benchmark study, we hypothesize that concise reasoning combined with a reduced set of visual tokens can be sufficient for effective video reasoning. To evaluate this hypothesis, we design and validate an efficient post-training and inference framework that enhances a video MLLM's reasoning capability. Our framework enables models to operate on compressed visual tokens and generate brief reasoning traces prior to answering. The resulting models achieve substantially improved inference efficiency, deliver competitive performance across diverse benchmarks, and avoid reliance on manual CoT annotations or supervised fine-tuning. Collectively, our results suggest that long, human-like CoT reasoning may not be necessary for general video reasoning, and that concise reasoning can be both effective and efficient. Our code will be released at https://github.com/LaVi-Lab/Rethink_CoT_Video.",
    "translation": "标题：重新思考视频中的思维链推理\n\n摘要：思维链推理在解决自然语言处理中的复杂任务方面取得了显著成功，而近期出现的多模态大语言模型将这一范式扩展至视频推理领域。然而，现有模型通常依赖于冗长的推理链和大量的输入视觉标记。基于基准研究的实证观察，我们提出假设：结合精简视觉标记的简洁推理可能足以实现有效的视频推理。为验证这一假设，我们设计并验证了一种高效的训练后处理与推理框架，该框架能够增强视频多模态大语言模型的推理能力。我们的框架使模型能够在压缩视觉标记上运行，并在回答问题前生成简明的推理轨迹。实验结果表明，优化后的模型在显著提升推理效率的同时，在多样化基准测试中展现出具有竞争力的性能，且无需依赖人工标注的思维链数据或有监督微调。综合而言，我们的研究结果表明，对于通用视频推理任务，冗长类人思维链推理可能并非必要，而简洁推理既能保持有效性又能提升效率。代码将在 https://github.com/LaVi-Lab/Rethink_CoT_Video 发布。",
    "url": "https://huggingface.co/papers/2512.09616",
    "arxiv_url": "https://arxiv.org/abs/2512.09616"
  },
  {
    "title": "HiF-VLA: Hindsight, Insight and Foresight through Motion Representation for Vision-Language-Action Models",
    "summary": "Vision-Language-Action (VLA) models have recently enabled robotic manipulation by grounding visual and linguistic cues into actions. However, most VLAs assume the Markov property, relying only on the current observation and thus suffering from temporal myopia that degrades long-horizon coherence. In this work, we view motion as a more compact and informative representation of temporal context and world dynamics, capturing inter-state changes while filtering static pixel-level noise. Building on this idea, we propose HiF-VLA (Hindsight, Insight, and Foresight for VLAs), a unified framework that leverages motion for bidirectional temporal reasoning. HiF-VLA encodes past dynamics through hindsight priors, anticipates future motion via foresight reasoning, and integrates both through a hindsight-modulated joint expert to enable a ''think-while-acting'' paradigm for long-horizon manipulation. As a result, HiF-VLA surpasses strong baselines on LIBERO-Long and CALVIN ABC-D benchmarks, while incurring negligible additional inference latency. Furthermore, HiF-VLA achieves substantial improvements in real-world long-horizon manipulation tasks, demonstrating its broad effectiveness in practical robotic settings.",
    "translation": "标题：HiF-VLA：基于运动表征的视觉-语言-动作模型后视、洞察与前瞻框架\n\n摘要：视觉-语言-动作模型近期通过将视觉与语言线索映射为动作，实现了机器人操控任务。然而，现有模型大多遵循马尔可夫假设，仅依赖当前观测状态，因而存在时序短视问题，导致长时序任务中的连贯性下降。本研究提出将运动视为一种更紧凑且信息丰富的时序上下文与世界动态表征，其能捕捉状态间变化并过滤静态像素级噪声。基于此，我们构建了HiF-VLA（视觉-语言-动作模型的后视、洞察与前瞻框架），该统一框架利用运动信息实现双向时序推理。HiF-VLA通过后验先验编码历史动态，借助前瞻推理预测未来运动，并通过后验调制联合专家模块将二者融合，从而构建适用于长时序操控的“行动中思考”范式。实验表明，HiF-VLA在LIBERO-Long与CALVIN ABC-D基准测试中均超越现有强基线模型，且仅带来可忽略的额外推理延迟。此外，该模型在现实长时序操控任务中取得显著性能提升，证明了其在实际机器人应用场景中的广泛有效性。",
    "url": "https://huggingface.co/papers/2512.09928",
    "arxiv_url": "https://arxiv.org/abs/2512.09928"
  },
  {
    "title": "Fast-Decoding Diffusion Language Models via Progress-Aware Confidence Schedules",
    "summary": "Diffusion large language models (dLLMs) offer a promising alternative to autoregressive models, but their practical utility is severely hampered by slow, iterative sampling. We present SchED, a training-free, model-agnostic early-exit algorithm that aggregates full-span logit margins and halts decoding once a smooth, progress-dependent confidence threshold is met. We evaluated SchED on two dLLM families (Dream and LLaDA), in base and instruction-tuned variants across ten benchmarks spanning downstream tasks including multiple-choice question answering (MCQ), math, long-form QA/summarization, and translation. SchED delivers large, stable accelerations: on instruction-tuned models, it achieves 3.8-4.0times speedups while retaining 99.8-100% of the baseline score on average. On base models, SchED yields consistent speedup gains with 99.1-100% performance retention, with up to 2.34times under more aggressive settings. Using a conservative speed metric that heavily penalizes quality loss (QPS, γ{=}4), we show that SchED is robust and clearly outperforms prior confidence-based early-exit methods, which break down on long-form generation. An entropy analysis of the model's token predictions reveals that instruction tuning speeds up the decay of predictive entropy. By turning genuine confidence stabilization into computational savings, SchED makes dLLM decoding substantially more efficient.",
    "translation": "标题：基于进度感知置信度调度的扩散语言模型快速解码方法\n\n摘要：扩散大语言模型（dLLMs）为自回归模型提供了一种有前景的替代方案，但其缓慢的迭代采样过程严重限制了实际应用。本文提出SchED算法，这是一种无需训练、与模型无关的提前退出算法，通过聚合全跨度对数边际值，并在达到平滑的进度相关置信度阈值时停止解码。我们在两个dLLM系列（Dream和LLaDA）上评估了SchED，涵盖基础版本和指令微调版本，并在十个基准测试中进行验证，包括多项选择题回答（MCQ）、数学、长文本问答/摘要和翻译等下游任务。SchED实现了显著且稳定的加速效果：在指令微调模型上，平均加速比达到3.8-4.0倍，同时保持基线得分的99.8%-100%；在基础模型上，SchED在保持99.1%-100%性能的前提下获得稳定的加速增益，在更激进的设置下加速比最高可达2.34倍。采用严格惩罚质量损失的速度评估指标（QPS, γ=4）表明，SchED具有强鲁棒性，明显优于先前基于置信度的提前退出方法（后者在长文本生成任务中失效）。对模型令牌预测的熵分析显示，指令微调会加速预测熵的衰减。通过将真实的置信度稳定转化为计算效率提升，SchED显著提高了dLLM的解码效率。",
    "url": "https://huggingface.co/papers/2512.02892",
    "arxiv_url": "https://arxiv.org/abs/2512.02892"
  },
  {
    "title": "UniUGP: Unifying Understanding, Generation, and Planing For End-to-end Autonomous Driving",
    "summary": "Autonomous driving (AD) systems struggle in long-tail scenarios due to limited world knowledge and weak visual dynamic modeling. Existing vision-language-action (VLA)-based methods cannot leverage unlabeled videos for visual causal learning, while world model-based methods lack reasoning capabilities from large language models. In this paper, we construct multiple specialized datasets providing reasoning and planning annotations for complex scenarios. Then, a unified Understanding-Generation-Planning framework, named UniUGP, is proposed to synergize scene reasoning, future video generation, and trajectory planning through a hybrid expert architecture. By integrating pre-trained VLMs and video generation models, UniUGP leverages visual dynamics and semantic reasoning to enhance planning performance. Taking multi-frame observations and language instructions as input, it produces interpretable chain-of-thought reasoning, physically consistent trajectories, and coherent future videos. We introduce a four-stage training strategy that progressively builds these capabilities across multiple existing AD datasets, along with the proposed specialized datasets. Experiments demonstrate state-of-the-art performance in perception, reasoning, and decision-making, with superior generalization to challenging long-tail situations.",
    "translation": "标题：UniUGP：面向端到端自动驾驶的理解、生成与规划统一框架\n\n摘要：自动驾驶系统因世界知识有限与视觉动态建模能力不足，在长尾场景中面临挑战。现有基于视觉-语言-动作的方法难以利用无标注视频进行视觉因果学习，而基于世界模型的方法缺乏大语言模型的推理能力。本文构建了多个专用数据集，为复杂场景提供推理与规划标注。在此基础上，提出名为UniUGP的统一理解-生成-规划框架，通过混合专家架构协同实现场景推理、未来视频生成与轨迹规划。该框架整合预训练的视觉语言模型与视频生成模型，利用视觉动态与语义推理提升规划性能。系统以多帧观测数据与语言指令作为输入，可生成可解释的思维链推理、物理一致的轨迹以及连贯的未来场景视频。我们设计了四阶段训练策略，在多个现有自动驾驶数据集及新建专用数据集上逐步构建上述能力。实验结果表明，该系统在感知、推理与决策任务上达到最先进性能，并在具有挑战性的长尾场景中展现出卓越的泛化能力。",
    "url": "https://huggingface.co/papers/2512.09864",
    "arxiv_url": "https://arxiv.org/abs/2512.09864"
  },
  {
    "title": "WonderZoom: Multi-Scale 3D World Generation",
    "summary": "We present WonderZoom, a novel approach to generating 3D scenes with contents across multiple spatial scales from a single image. Existing 3D world generation models remain limited to single-scale synthesis and cannot produce coherent scene contents at varying granularities. The fundamental challenge is the lack of a scale-aware 3D representation capable of generating and rendering content with largely different spatial sizes. WonderZoom addresses this through two key innovations: (1) scale-adaptive Gaussian surfels for generating and real-time rendering of multi-scale 3D scenes, and (2) a progressive detail synthesizer that iteratively generates finer-scale 3D contents. Our approach enables users to \"zoom into\" a 3D region and auto-regressively synthesize previously non-existent fine details from landscapes to microscopic features. Experiments demonstrate that WonderZoom significantly outperforms state-of-the-art video and 3D models in both quality and alignment, enabling multi-scale 3D world creation from a single image. We show video results and an interactive viewer of generated multi-scale 3D worlds in https://wonderzoom.github.io/",
    "translation": "标题：WonderZoom：多尺度三维世界生成\n\n摘要：本文提出WonderZoom，这是一种从单张图像生成跨多空间尺度三维场景内容的新方法。现有三维世界生成模型仍局限于单尺度合成，无法在不同粒度上生成连贯的场景内容。其根本挑战在于缺乏能够生成和渲染空间尺寸差异巨大内容的尺度感知三维表征。WonderZoom通过两项关键创新解决此问题：（1）用于多尺度三维场景生成与实时渲染的尺度自适应高斯面元；（2）可迭代生成更精细尺度三维内容的渐进式细节合成器。该方法使用户能够\"放大\"三维区域，并自回归地合成从景观到微观特征等原本不存在的精细细节。实验表明，WonderZoom在生成质量和对齐度上显著优于当前最先进的视频与三维模型，实现了从单张图像创建多尺度三维世界。生成的多尺度三维世界视频结果与交互式查看器详见 https://wonderzoom.github.io/",
    "url": "https://huggingface.co/papers/2512.09164",
    "arxiv_url": "https://arxiv.org/abs/2512.09164"
  },
  {
    "title": "EtCon: Edit-then-Consolidate for Reliable Knowledge Editing",
    "summary": "Knowledge editing aims to update specific facts in large language models (LLMs) without full retraining. Prior efforts sought to tune the knowledge layers of LLMs, proving effective for making selective edits. However, a significant gap exists between their performance in controlled, teacher-forcing evaluations and their real-world effectiveness in lifelong learning scenarios, which greatly limits their practical applicability. This work's empirical analysis reveals two recurring issues associated with this gap: (1) Most traditional methods lead the edited model to overfit to the new fact, thereby degrading pre-trained capabilities; (2) There is a critical absence of a knowledge consolidation stage, leaving new facts insufficiently integrated into LLMs' inference-time behavior under autoregressive generation, thereby leading to a mismatch between parametric knowledge and actual generation behavior. To this end, we propose Edit-then-Consolidate, a novel knowledge editing paradigm that aims to bridge the gap between theoretical knowledge editing methods and their real-world applicability. Specifically, (1) our framework mitigates overfitting via Targeted Proximal Supervised Fine-Tuning (TPSFT) that localizes the edit via a trust-region objective to limit policy drift; (2) Then, a consolidation stage using Group Relative Policy Optimization (GRPO) aligns the edited knowledge with CoT-based inference policy by optimizing trajectory-level behavior under comprehensive reward signals. Extensive experiments demonstrate our framework consistently improves editing reliability and generalization under real-world evaluations, while better preserving locality and pre-trained capabilities.",
    "translation": "标题：EtCon：先编辑后巩固——一种可靠的知识编辑方法\n\n摘要：知识编辑旨在无需完整重训练的情况下更新大语言模型（LLM）中的特定事实。先前研究主要通过调整LLM的知识层来实现选择性编辑，并在受控的教师强制评估中取得了良好效果。然而，这类方法在受控评估中的表现与其在终身学习场景中的实际效能之间存在显著差距，这严重限制了其实际应用价值。本文的实证分析揭示了导致该差距的两个常见问题：（1）多数传统方法会使编辑后的模型过度拟合新事实，从而损害其预训练能力；（2）关键缺乏知识巩固阶段，导致新知识未能充分整合到LLM在自回归生成时的推理行为中，造成参数化知识与实际生成行为之间的错配。为此，我们提出“先编辑后巩固”这一新型知识编辑范式，旨在弥合理论性知识编辑方法与实际应用之间的鸿沟。具体而言：（1）本框架通过“目标近端监督微调”缓解过拟合问题，该技术基于信任区域目标定位编辑范围以限制策略漂移；（2）随后采用“群体相对策略优化”进行知识巩固，通过在多维度奖励信号下优化轨迹级行为，将编辑后的知识与基于思维链的推理策略对齐。大量实验表明，本框架在现实场景评估中能持续提升编辑的可靠性与泛化能力，同时更好地保持了编辑的局部性与模型的预训练能力。",
    "url": "https://huggingface.co/papers/2512.04753",
    "arxiv_url": "https://arxiv.org/abs/2512.04753"
  },
  {
    "title": "Learning Unmasking Policies for Diffusion Language Models",
    "summary": "Diffusion (Large) Language Models (dLLMs) now match the downstream performance of their autoregressive counterparts on many tasks, while holding the promise of being more efficient during inference. One particularly successful variant is masked discrete diffusion, in which a buffer filled with special mask tokens is progressively replaced with tokens sampled from the model's vocabulary. Efficiency can be gained by unmasking several tokens in parallel, but doing too many at once risks degrading the generation quality. Thus, one critical design aspect of dLLMs is the sampling procedure that selects, at each step of the diffusion process, which tokens to replace. Indeed, recent work has found that heuristic strategies such as confidence thresholding lead to both higher quality and token throughput compared to random unmasking. However, such heuristics have downsides: they require manual tuning, and we observe that their performance degrades with larger buffer sizes. In this work, we instead propose to train sampling procedures using reinforcement learning. Specifically, we formalize masked diffusion sampling as a Markov decision process in which the dLLM serves as the environment, and propose a lightweight policy architecture based on a single-layer transformer that maps dLLM token confidences to unmasking decisions. Our experiments show that these trained policies match the performance of state-of-the-art heuristics when combined with semi-autoregressive generation, while outperforming them in the full diffusion setting. We also examine the transferability of these policies, finding that they can generalize to new underlying dLLMs and longer sequence lengths. However, we also observe that their performance degrades when applied to out-of-domain data, and that fine-grained tuning of the accuracy-efficiency trade-off can be challenging with our approach.",
    "translation": "标题：扩散语言模型中的解掩码策略学习\n\n摘要：扩散（大）语言模型（dLLMs）目前在多项任务的下游性能上已与自回归模型相当，同时在推理效率方面展现出更大潜力。其中一种尤为成功的变体是掩码离散扩散模型，该模型通过逐步将填充特殊掩码标记的缓冲区替换为从模型词汇表中采样的标记来实现生成。通过并行解掩码多个标记可提升效率，但一次性解掩过多标记可能导致生成质量下降。因此，dLLMs的一个关键设计环节是在扩散过程的每一步选择待替换标记的采样策略。近期研究已发现，与随机解掩码相比，基于置信度阈值等启发式策略能同时提升生成质量和标记吞吐量。然而，此类启发式方法存在局限：需要人工调参，且我们发现其性能会随缓冲区规模扩大而下降。本研究提出采用强化学习训练采样策略的新方法。具体而言，我们将掩码扩散采样形式化为马尔可夫决策过程，其中dLLM作为环境，并设计了一种基于单层Transformer的轻量级策略架构，该架构可将dLLM的标记置信度映射为解掩码决策。实验表明，经过训练的采样策略在半自回归生成场景中能达到最先进启发式方法的性能水平，并在完整扩散设定中表现更优。我们还验证了策略的可迁移性，发现其能泛化至新的底层dLLM和更长序列。但研究同时指出，这些策略在处理域外数据时性能会下降，且通过本方法精细调节精度-效率权衡仍存在挑战。",
    "url": "https://huggingface.co/papers/2512.09106",
    "arxiv_url": "https://arxiv.org/abs/2512.09106"
  },
  {
    "title": "Towards a Science of Scaling Agent Systems",
    "summary": "Agents, language model (LM)-based systems that are capable of reasoning, planning, and acting are becoming the dominant paradigm for real-world AI applications. Despite this widespread adoption, the principles that determine their performance remain underexplored, leaving practitioners to rely on heuristics rather than principled design choices. We address this gap by deriving quantitative scaling principles for agent systems. We evaluate this across four diverse benchmarks: Finance-Agent, BrowseComp-Plus, PlanCraft, and Workbench. Using five canonical architectures (Single, Independent, Centralized, Decentralized, Hybrid) instantiated across three LLM families, we perform a controlled evaluation spanning 180 configurations with standardized tools and token budgets. We derive a predictive model using empirical coordination metrics, including efficiency, overhead, error amplification, and redundancy, that achieves cross-validated R^2=0.513. We identify three dominant effects: (1) a tool-coordination trade-off: under fixed computational budgets, tool-heavy tasks suffer disproportionately from multi-agent overhead. (2) a capability saturation: coordination yields diminishing or negative returns (beta=-0.408, p<0.001) once single-agent baselines exceed ~45%. (3) topology-dependent error amplification: independent agents amplify errors 17.2x through unchecked propagation, while centralized coordination contains this to 4.4x. Centralized coordination improves performance by 80.9% on parallelizable tasks like financial reasoning, while decentralized coordination excels on dynamic web navigation (+9.2% vs. +0.2%). Yet for sequential reasoning tasks, all multi-agent variants degraded performance by 39-70%. The framework predicts the optimal coordination strategy for 87% of held-out configurations, providing a predictive principle of agentic scaling based on measurable task properties.",
    "translation": "标题：迈向智能体系统规模化科学\n\n摘要：基于语言模型的智能体系统——具备推理、规划与行动能力的系统——正成为现实世界人工智能应用的主导范式。尽管其应用日益广泛，决定其性能的内在原理仍未得到充分探索，导致实践者往往依赖经验法则而非基于原理的设计选择。本研究通过推导智能体系统的定量规模化原理来填补这一空白。我们在四个多样化基准测试（Finance-Agent、BrowseComp-Plus、PlanCraft和Workbench）中进行评估，采用五种典型架构（单智能体、独立智能体、集中式、分布式及混合式），并在三大语言模型系列中实例化，通过标准化工具与计算资源预算对180种配置进行受控实验。我们利用经验性协调指标（包括效率、开销、误差放大和冗余度）构建预测模型，其交叉验证R²达到0.513。研究发现三大主导效应：（1）工具-协调权衡：在固定计算预算下，工具密集型任务会因多智能体协调开销而承受不成比例的损失；（2）能力饱和效应：当单智能体基线性能超过约45%时，协调带来的收益呈现递减甚至负回报（β=-0.408，p<0.001）；（3）拓扑依赖的误差放大：独立智能体因未受控的误差传播使误差放大17.2倍，而集中式协调可将其控制在4.4倍。在金融推理等可并行任务中，集中式协调使性能提升80.9%；在动态网络导航任务中，分布式协调表现更优（+9.2%对比+0.2%）。然而对于顺序推理任务，所有多智能体架构均导致性能下降39-70%。该框架对87%的预留配置能预测最优协调策略，为基于可测量任务特性的智能体规模化提供了可预测的设计原则。",
    "url": "https://huggingface.co/papers/2512.08296",
    "arxiv_url": "https://arxiv.org/abs/2512.08296"
  },
  {
    "title": "IF-Bench: Benchmarking and Enhancing MLLMs for Infrared Images with Generative Visual Prompting",
    "summary": "Recent advances in multimodal large language models (MLLMs) have led to impressive progress across various benchmarks. However, their capability in understanding infrared images remains unexplored. To address this gap, we introduce IF-Bench, the first high-quality benchmark designed for evaluating multimodal understanding of infrared images. IF-Bench consists of 499 images sourced from 23 infrared datasets and 680 carefully curated visual question-answer pairs, covering 10 essential dimensions of image understanding. Based on this benchmark, we systematically evaluate over 40 open-source and closed-source MLLMs, employing cyclic evaluation, bilingual assessment, and hybrid judgment strategies to enhance the reliability of the results. Our analysis reveals how model scale, architecture, and inference paradigms affect infrared image comprehension, providing valuable insights for this area. Furthermore, we propose a training-free generative visual prompting (GenViP) method, which leverages advanced image editing models to translate infrared images into semantically and spatially aligned RGB counterparts, thereby mitigating domain distribution shifts. Extensive experiments demonstrate that our method consistently yields significant performance improvements across a wide range of MLLMs. The benchmark and code are available at https://github.com/casiatao/IF-Bench.",
    "translation": "标题：IF-Bench：基于生成式视觉提示的红外图像多模态大语言模型评测与增强\n\n摘要：多模态大语言模型（MLLMs）的最新进展已在各类评测基准上取得显著成果，然而其在红外图像理解方面的能力尚未得到充分探索。为填补这一空白，我们提出了IF-Bench——首个面向红外图像多模态理解评估的高质量基准。该基准包含从23个红外数据集中精选的499幅图像，以及680组精心构建的视觉问答对，涵盖图像理解的十个核心维度。基于此基准，我们系统评估了超过40个开源与闭源MLLMs，采用循环评估、双语测评和混合判读策略以提升结果可靠性。分析揭示了模型规模、架构及推理范式对红外图像理解的影响，为该领域提供了重要洞见。此外，我们提出了一种免训练的生成式视觉提示方法（GenViP），该方法利用先进图像编辑模型将红外图像转换为语义与空间对齐的RGB对应图像，从而缓解领域分布偏移问题。大量实验表明，该方法在多种MLLMs上均能带来显著的性能提升。基准数据与代码已公开于：https://github.com/casiatao/IF-Bench。",
    "url": "https://huggingface.co/papers/2512.09663",
    "arxiv_url": "https://arxiv.org/abs/2512.09663"
  },
  {
    "title": "TED-4DGS: Temporally Activated and Embedding-based Deformation for 4DGS Compression",
    "summary": "Building on the success of 3D Gaussian Splatting (3DGS) in static 3D scene representation, its extension to dynamic scenes, commonly referred to as 4DGS or dynamic 3DGS, has attracted increasing attention. However, designing more compact and efficient deformation schemes together with rate-distortion-optimized compression strategies for dynamic 3DGS representations remains an underexplored area. Prior methods either rely on space-time 4DGS with overspecified, short-lived Gaussian primitives or on canonical 3DGS with deformation that lacks explicit temporal control. To address this, we present TED-4DGS, a temporally activated and embedding-based deformation scheme for rate-distortion-optimized 4DGS compression that unifies the strengths of both families. TED-4DGS is built on a sparse anchor-based 3DGS representation. Each canonical anchor is assigned learnable temporal-activation parameters to specify its appearance and disappearance transitions over time, while a lightweight per-anchor temporal embedding queries a shared deformation bank to produce anchor-specific deformation. For rate-distortion compression, we incorporate an implicit neural representation (INR)-based hyperprior to model anchor attribute distributions, along with a channel-wise autoregressive model to capture intra-anchor correlations. With these novel elements, our scheme achieves state-of-the-art rate-distortion performance on several real-world datasets. To the best of our knowledge, this work represents one of the first attempts to pursue a rate-distortion-optimized compression framework for dynamic 3DGS representations.",
    "translation": "标题：TED-4DGS：面向4DGS压缩的时间激活与嵌入式形变方法\n\n摘要：基于三维高斯泼溅（3DGS）在静态三维场景表示中的成功，其向动态场景的扩展（通常称为4DGS或动态3DGS）正受到日益广泛的关注。然而，如何为动态3DGS表示设计更紧凑高效的形变方案，并结合率失真优化的压缩策略，仍是一个尚未充分探索的领域。现有方法要么依赖具有过度参数化、短生命周期高斯基元的时空4DGS，要么采用缺乏显式时间控制的规范3DGS形变框架。为此，我们提出TED-4DGS——一种面向率失真优化4DGS压缩的时间激活与嵌入式形变方案，该方案融合了两类方法的优势。TED-4DGS建立在基于稀疏锚点的3DGS表示基础上：每个规范锚点被赋予可学习的时间激活参数，以控制其随时间推移的出现与消失状态；同时通过轻量化的锚点时序嵌入查询共享形变库，生成锚点特异性形变。为实现率失真压缩，我们引入基于隐式神经表示的超先验模型来建模锚点属性分布，并结合通道自回归模型以捕捉锚点内部关联。凭借这些创新设计，本方案在多个真实世界数据集上实现了领先的率失真性能。据我们所知，本研究是首个针对动态3DGS表示构建率失真优化压缩框架的探索性工作之一。",
    "url": "https://huggingface.co/papers/2512.05446",
    "arxiv_url": "https://arxiv.org/abs/2512.05446"
  },
  {
    "title": "MotionEdit: Benchmarking and Learning Motion-Centric Image Editing",
    "summary": "We introduce MotionEdit, a novel dataset for motion-centric image editing-the task of modifying subject actions and interactions while preserving identity, structure, and physical plausibility. Unlike existing image editing datasets that focus on static appearance changes or contain only sparse, low-quality motion edits, MotionEdit provides high-fidelity image pairs depicting realistic motion transformations extracted and verified from continuous videos. This new task is not only scientifically challenging but also practically significant, powering downstream applications such as frame-controlled video synthesis and animation.\n  To evaluate model performance on the novel task, we introduce MotionEdit-Bench, a benchmark that challenges models on motion-centric edits and measures model performance with generative, discriminative, and preference-based metrics. Benchmark results reveal that motion editing remains highly challenging for existing state-of-the-art diffusion-based editing models. To address this gap, we propose MotionNFT (Motion-guided Negative-aware Fine Tuning), a post-training framework that computes motion alignment rewards based on how well the motion flow between input and model-edited images matches the ground-truth motion, guiding models toward accurate motion transformations. Extensive experiments on FLUX.1 Kontext and Qwen-Image-Edit show that MotionNFT consistently improves editing quality and motion fidelity of both base models on the motion editing task without sacrificing general editing ability, demonstrating its effectiveness.",
    "translation": "标题：MotionEdit：面向运动中心图像编辑的基准构建与学习方法\n\n摘要：本文提出MotionEdit，一个面向运动中心图像编辑任务的新型数据集——该任务旨在修改主体动作与交互关系，同时保持身份特征、场景结构和物理合理性。与现有专注于静态外观修改或仅包含稀疏低质量运动编辑的数据集不同，MotionEdit通过从连续视频中提取并验证的真实运动变换，提供了描绘高保真运动转换的图像对。这一新任务不仅具有科学挑战性，更具备实际应用价值，可为帧控视频合成与动画生成等下游应用提供支持。\n\n为评估模型在该新任务上的性能，我们构建了MotionEdit-Bench基准测试平台，通过生成式、判别式和基于偏好的多维度指标，系统评估模型在运动中心编辑任务上的表现。基准测试结果表明，当前基于扩散模型的先进编辑方法在运动编辑任务上仍面临巨大挑战。为应对这一局限，我们提出MotionNFT（运动引导的负向感知微调）框架，该训练后优化框架通过计算输入图像与模型编辑后图像间的运动流与真实运动场的匹配度，构建运动对齐奖励机制，引导模型实现精准的运动转换。在FLUX.1 Kontext和Qwen-Image-Edit模型上的大量实验表明，MotionNFT在保持通用编辑能力的同时，能持续提升基础模型在运动编辑任务中的编辑质量与运动保真度，验证了该框架的有效性。",
    "url": "https://huggingface.co/papers/2512.10284",
    "arxiv_url": "https://arxiv.org/abs/2512.10284"
  },
  {
    "title": "Beyond Unified Models: A Service-Oriented Approach to Low Latency, Context Aware Phonemization for Real Time TTS",
    "summary": "Lightweight, real-time text-to-speech systems are crucial for accessibility. However, the most efficient TTS models often rely on lightweight phonemizers that struggle with context-dependent challenges. In contrast, more advanced phonemizers with a deeper linguistic understanding typically incur high computational costs, which prevents real-time performance.\n  This paper examines the trade-off between phonemization quality and inference speed in G2P-aided TTS systems, introducing a practical framework to bridge this gap. We propose lightweight strategies for context-aware phonemization and a service-oriented TTS architecture that executes these modules as independent services. This design decouples heavy context-aware components from the core TTS engine, effectively breaking the latency barrier and enabling real-time use of high-quality phonemization models. Experimental results confirm that the proposed system improves pronunciation soundness and linguistic accuracy while maintaining real-time responsiveness, making it well-suited for offline and end-device TTS applications.",
    "translation": "标题：超越统一模型：面向服务的低延迟上下文感知音素化方法在实时TTS中的应用\n\n摘要：轻量级实时文本转语音系统对于信息无障碍至关重要。然而最高效的TTS模型通常依赖轻量级音素转换器，这类系统在处理上下文相关挑战时表现欠佳。相比之下，具有更深层语言理解能力的先进音素转换器往往会产生高昂计算成本，从而无法实现实时性能。本文研究了G2P辅助TTS系统中音素化质量与推理速度之间的权衡关系，提出了一种弥合该差距的实用框架。我们设计了轻量级上下文感知音素化策略，并构建了面向服务的TTS架构，将这些模块作为独立服务执行。该设计将高计算负载的上下文感知组件与核心TTS引擎解耦，有效突破了延迟瓶颈，实现了高质量音素化模型的实时调用。实验结果表明，所提系统在保持实时响应能力的同时，显著提升了发音合理性与语言准确性，特别适用于离线及终端设备的TTS应用场景。",
    "url": "https://huggingface.co/papers/2512.08006",
    "arxiv_url": "https://arxiv.org/abs/2512.08006"
  },
  {
    "title": "VideoSSM: Autoregressive Long Video Generation with Hybrid State-Space Memory",
    "summary": "Autoregressive (AR) diffusion enables streaming, interactive long-video generation by producing frames causally, yet maintaining coherence over minute-scale horizons remains challenging due to accumulated errors, motion drift, and content repetition. We approach this problem from a memory perspective, treating video synthesis as a recurrent dynamical process that requires coordinated short- and long-term context. We propose VideoSSM, a Long Video Model that unifies AR diffusion with a hybrid state-space memory. The state-space model (SSM) serves as an evolving global memory of scene dynamics across the entire sequence, while a context window provides local memory for motion cues and fine details. This hybrid design preserves global consistency without frozen, repetitive patterns, supports prompt-adaptive interaction, and scales in linear time with sequence length. Experiments on short- and long-range benchmarks demonstrate state-of-the-art temporal consistency and motion stability among autoregressive video generator especially at minute-scale horizons, enabling content diversity and interactive prompt-based control, thereby establishing a scalable, memory-aware framework for long video generation.",
    "translation": "标题：VideoSSM：基于混合状态空间记忆的自回归长视频生成\n\n摘要：自回归扩散模型通过因果式逐帧生成实现了流式、交互式的长视频生成，但由于误差累积、运动漂移和内容重复等问题，在分钟级时间跨度上保持视频连贯性仍具挑战。本文从记忆视角切入，将视频合成视为一种需要协调短期与长期上下文的循环动态过程。我们提出VideoSSM——一种将自回归扩散与混合状态空间记忆相统一的长视频生成模型。其中状态空间模型作为演化中的全局记忆，持续追踪整个序列的场景动态；而局部上下文窗口则为运动线索与细节特征提供短期记忆。这种混合设计能够在避免画面冻结与模式重复的前提下保持全局一致性，支持基于提示的自适应交互，并以序列长度的线性时间复杂度实现扩展。在短期与长期基准测试上的实验表明，该模型在自回归视频生成器中实现了最先进的时间连贯性与运动稳定性，尤其在分钟级跨度上表现突出，能够同时保障内容多样性与基于提示的交互控制，从而为长视频生成建立了一个可扩展的、具备记忆感知能力的框架。",
    "url": "https://huggingface.co/papers/2512.04519",
    "arxiv_url": "https://arxiv.org/abs/2512.04519"
  },
  {
    "title": "GimbalDiffusion: Gravity-Aware Camera Control for Video Generation",
    "summary": "Recent progress in text-to-video generation has achieved remarkable realism, yet fine-grained control over camera motion and orientation remains elusive. Existing approaches typically encode camera trajectories through relative or ambiguous representations, limiting explicit geometric control. We introduce GimbalDiffusion, a framework that enables camera control grounded in physical-world coordinates, using gravity as a global reference. Instead of describing motion relative to previous frames, our method defines camera trajectories in an absolute coordinate system, allowing precise and interpretable control over camera parameters without requiring an initial reference frame. We leverage panoramic 360-degree videos to construct a wide variety of camera trajectories, well beyond the predominantly straight, forward-facing trajectories seen in conventional video data. To further enhance camera guidance, we introduce null-pitch conditioning, an annotation strategy that reduces the model's reliance on text content when conflicting with camera specifications (e.g., generating grass while the camera points towards the sky). Finally, we establish a benchmark for camera-aware video generation by rebalancing SpatialVID-HQ for comprehensive evaluation under wide camera pitch variation. Together, these contributions advance the controllability and robustness of text-to-video models, enabling precise, gravity-aligned camera manipulation within generative frameworks.",
    "translation": "标题：GimbalDiffusion：面向视频生成的重力感知相机控制框架\n\n摘要：当前文本到视频生成技术已取得显著的真实感进展，但对相机运动与朝向的细粒度控制仍面临挑战。现有方法通常通过相对或模糊的表征编码相机轨迹，限制了显式几何控制能力。本文提出GimbalDiffusion框架，该框架基于物理世界坐标系实现相机控制，并以重力作为全局参考基准。与依赖相邻帧相对运动的传统方法不同，本方法在绝对坐标系中定义相机轨迹，无需初始参考帧即可实现对相机参数精确且可解释的控制。我们利用全景360度视频构建多样化的相机运动轨迹，其范围远超传统视频数据中主要存在的直线前向运动模式。为增强相机引导效果，我们提出零俯仰条件标注策略，该策略能在相机参数与文本描述冲突时（例如相机朝向天空时需生成草地场景）降低模型对文本内容的依赖。此外，我们通过重构SpatialVID-HQ数据集建立相机感知视频生成基准测试平台，支持大范围相机俯仰变化下的综合性能评估。这些创新共同提升了文本到视频模型的可控性与鲁棒性，实现在生成框架内进行精确且重力对齐的相机运动控制。",
    "url": "https://huggingface.co/papers/2512.09112",
    "arxiv_url": "https://arxiv.org/abs/2512.09112"
  },
  {
    "title": "Pay Less Attention to Function Words for Free Robustness of Vision-Language Models",
    "summary": "To address the trade-off between robustness and performance for robust VLM, we observe that function words could incur vulnerability of VLMs against cross-modal adversarial attacks, and propose Function-word De-Attention (FDA) accordingly to mitigate the impact of function words. Similar to differential amplifiers, our FDA calculates the original and the function-word cross-attention within attention heads, and differentially subtracts the latter from the former for more aligned and robust VLMs. Comprehensive experiments include 2 SOTA baselines under 6 different attacks on 2 downstream tasks, 3 datasets, and 3 models. Overall, our FDA yields an average 18/13/53% ASR drop with only 0.2/0.3/0.6% performance drops on the 3 tested models on retrieval, and a 90% ASR drop with a 0.3% performance gain on visual grounding. We demonstrate the scalability, generalization, and zero-shot performance of FDA experimentally, as well as in-depth ablation studies and analysis. Code will be made publicly at https://github.com/michaeltian108/FDA.",
    "translation": "标题：减少对功能词的关注以提升视觉语言模型的鲁棒性\n\n摘要：为解决鲁棒视觉语言模型（VLM）中鲁棒性与性能之间的权衡问题，本文发现功能词可能导致VLM在跨模态对抗攻击下产生脆弱性，并据此提出功能词去关注（FDA）机制以减轻功能词的影响。类似于差分放大器的工作原理，FDA在注意力头内分别计算原始跨注意力与功能词跨注意力，并通过差分减法将后者从前者中去除，从而构建更对齐且更鲁棒的VLM。综合实验涵盖2个下游任务、3个数据集和3种模型，在6种不同攻击下与2种前沿基线方法进行对比。总体而言，在检索任务中，FDA在3个测试模型上平均实现18%/13%/53%的攻击成功率下降，性能仅下降0.2%/0.3%/0.6%；在视觉定位任务中实现90%的攻击成功率下降，同时性能提升0.3%。实验进一步验证了FDA的可扩展性、泛化能力和零样本性能，并提供了深入的消融研究与分析。代码将公开于https://github.com/michaeltian108/FDA。",
    "url": "https://huggingface.co/papers/2512.07222",
    "arxiv_url": "https://arxiv.org/abs/2512.07222"
  },
  {
    "title": "Smart Timing for Mining: A Deep Learning Framework for Bitcoin Hardware ROI Prediction",
    "summary": "Bitcoin mining hardware acquisition requires strategic timing due to volatile markets, rapid technological obsolescence, and protocol-driven revenue cycles. Despite mining's evolution into a capital-intensive industry, there is little guidance on when to purchase new Application-Specific Integrated Circuit (ASIC) hardware, and no prior computational frameworks address this decision problem. We address this gap by formulating hardware acquisition as a time series classification task, predicting whether purchasing ASIC machines yields profitable (Return on Investment (ROI) >= 1), marginal (0 < ROI < 1), or unprofitable (ROI <= 0) returns within one year. We propose MineROI-Net, an open source Transformer-based architecture designed to capture multi-scale temporal patterns in mining profitability. Evaluated on data from 20 ASIC miners released between 2015 and 2024 across diverse market regimes, MineROI-Net outperforms LSTM-based and TSLANet baselines, achieving 83.7% accuracy and 83.1% macro F1-score. The model demonstrates strong economic relevance, achieving 93.6% precision in detecting unprofitable periods and 98.5% precision for profitable ones, while avoiding misclassification of profitable scenarios as unprofitable and vice versa. These results indicate that MineROI-Net offers a practical, data-driven tool for timing mining hardware acquisitions, potentially reducing financial risk in capital-intensive mining operations. The model is available through: https://github.com/AMAAI-Lab/MineROI-Net.",
    "translation": "标题：智能挖矿时机选择：一种基于深度学习的比特币硬件投资回报率预测框架\n\n摘要：由于市场波动剧烈、技术迭代迅速以及协议驱动的收益周期特性，比特币挖矿硬件的购置需要战略性择时。尽管挖矿已演变为资本密集型产业，但目前关于何时购置新型专用集成电路（ASIC）硬件的指导极为有限，且尚无先前的计算框架能够系统解决这一决策问题。为填补这一空白，本研究将硬件购置问题构建为时间序列分类任务，旨在预测购置ASIC矿机是否能在一年内产生盈利（投资回报率（ROI）≥1）、边际收益（0 < ROI < 1）或亏损（ROI ≤ 0）。我们提出了MineROI-Net，一种基于Transformer架构的开源模型，专门设计用于捕捉挖矿收益的多尺度时序模式。通过在2015年至2024年间发布的20款ASIC矿机数据上进行测试，并覆盖多种市场行情，MineROI-Net的表现优于基于LSTM和TSLANet的基线模型，实现了83.7%的准确率和83.1%的宏观F1分数。该模型展现出显著的经济实用性：在识别亏损时段时精确率达到93.6%，识别盈利时段时精确率达98.5%，且能有效避免将盈利场景误判为亏损（反之亦然）。结果表明，MineROI-Net为挖矿硬件购置时机选择提供了一个实用、数据驱动的工具，有望降低资本密集型挖矿运营的财务风险。模型可通过以下链接获取：https://github.com/AMAAI-Lab/MineROI-Net。",
    "url": "https://huggingface.co/papers/2512.05402",
    "arxiv_url": "https://arxiv.org/abs/2512.05402"
  },
  {
    "title": "Reinventing Clinical Dialogue: Agentic Paradigms for LLM Enabled Healthcare Communication",
    "summary": "Clinical dialogue represents a complex duality requiring both the empathetic fluency of natural conversation and the rigorous precision of evidence-based medicine. While Large Language Models possess unprecedented linguistic capabilities, their architectural reliance on reactive and stateless processing often favors probabilistic plausibility over factual veracity. This structural limitation has catalyzed a paradigm shift in medical AI from generative text prediction to agentic autonomy, where the model functions as a central reasoning engine capable of deliberate planning and persistent memory. Moving beyond existing reviews that primarily catalog downstream applications, this survey provides a first-principles analysis of the cognitive architecture underpinning this shift. We introduce a novel taxonomy structured along the orthogonal axes of knowledge source and agency objective to delineate the provenance of clinical knowledge against the system's operational scope. This framework facilitates a systematic analysis of the intrinsic trade-offs between creativity and reliability by categorizing methods into four archetypes: Latent Space Clinicians, Emergent Planners, Grounded Synthesizers, and Verifiable Workflow Automators. For each paradigm, we deconstruct the technical realization across the entire cognitive pipeline, encompassing strategic planning, memory management, action execution, collaboration, and evolution to reveal how distinct architectural choices balance the tension between autonomy and safety.",
    "translation": "标题：重塑临床对话：基于大语言模型的医疗沟通智能体范式\n\n摘要：临床对话呈现复杂的双重性，既需要自然交流的共情流畅性，又要求循证医学的严谨精确性。尽管大语言模型具备前所未有的语言能力，但其依赖反应式无状态处理的架构特点往往更倾向于概率合理性而非事实准确性。这一结构性局限推动了医疗人工智能领域的范式转变——从生成式文本预测转向智能体自主架构，使模型能够作为具备审慎规划与持久记忆能力的核心推理引擎。本文超越现有主要罗列下游应用的综述，从第一性原理出发分析了支撑这一转变的认知架构。我们提出了一种基于知识来源与智能体目标正交轴构建的新型分类体系，用以界定临床知识的溯源与系统操作范围的对应关系。该框架通过将现有方法归纳为四大原型——潜在空间临床医师、涌现式规划器、知识锚定合成器及可验证工作流自动化系统，系统性地揭示了创造力与可靠性之间的内在权衡。针对每种范式，我们解构了其在完整认知流程（涵盖战略规划、记忆管理、行动执行、协作与演进）中的技术实现路径，从而阐明不同架构选择如何平衡自主性与安全性之间的张力。",
    "url": "https://huggingface.co/papers/2512.01453",
    "arxiv_url": "https://arxiv.org/abs/2512.01453"
  }
]