[
  {
    "title": "Kling-Omni Technical Report",
    "summary": "We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds.",
    "translation": "标题：Kling-Omni 技术报告\n\n摘要：本文介绍 Kling-Omni，这是一个通用的生成式框架，旨在直接从多模态视觉语言输入中合成高保真度视频。Kling-Omni 采用端到端视角，弥合了多样化视频生成、编辑与智能推理任务之间的功能分离，将其整合为一个完整的系统。与割裂的流水线方法不同，Kling-Omni 支持包括文本指令、参考图像和视频上下文在内的多种用户输入，并将其处理为统一的多模态表示，以提供电影级画质且高度智能的视频内容创作。为支撑这些能力，我们构建了一个全面的数据系统，作为多模态视频创作的基础。该框架还通过高效的大规模预训练策略和针对推理的基础设施优化得到进一步增强。综合评估表明，Kling-Omni 在上下文生成、基于推理的编辑以及多模态指令跟随方面展现出卓越性能。我们相信，Kling-Omni 不仅是一个内容创作工具，更是迈向能够感知、推理、生成并与动态复杂世界交互的多模态世界模拟器的关键进展。",
    "url": "https://huggingface.co/papers/2512.16776",
    "arxiv_url": "https://arxiv.org/abs/2512.16776"
  },
  {
    "title": "Adaptation of Agentic AI",
    "summary": "Cutting-edge agentic AI systems are built on foundation models that can be adapted to plan, reason, and interact with external tools to perform increasingly complex and specialized tasks. As these systems grow in capability and scope, adaptation becomes a central mechanism for improving performance, reliability, and generalization. In this paper, we unify the rapidly expanding research landscape into a systematic framework that spans both agent adaptations and tool adaptations. We further decompose these into tool-execution-signaled and agent-output-signaled forms of agent adaptation, as well as agent-agnostic and agent-supervised forms of tool adaptation. We demonstrate that this framework helps clarify the design space of adaptation strategies in agentic AI, makes their trade-offs explicit, and provides practical guidance for selecting or switching among strategies during system design. We then review the representative approaches in each category, analyze their strengths and limitations, and highlight key open challenges and future opportunities. Overall, this paper aims to offer a conceptual foundation and practical roadmap for researchers and practitioners seeking to build more capable, efficient, and reliable agentic AI systems.",
    "translation": "标题：智能体人工智能的适应性研究\n\n摘要：前沿的智能体人工智能系统建立在基础模型之上，这些模型可通过调整以实现规划、推理及与外部工具交互，从而执行日益复杂和专门化的任务。随着此类系统在能力和范围上的扩展，适应性已成为提升性能、可靠性和泛化能力的核心机制。本文通过构建一个系统化框架，将快速扩展的研究领域统一为智能体适应性与工具适应性两大维度，并进一步将其分解为工具执行信号驱动型和智能体输出信号驱动型的智能体适应性，以及智能体无关型和智能体监督型的工具适应性。研究表明，该框架有助于厘清智能体人工智能中适应性策略的设计空间，明确其权衡关系，并为系统设计过程中策略的选择与切换提供实践指导。在此基础上，本文综述了各类别中的代表性方法，分析其优势与局限，并指出当前面临的关键开放挑战与未来机遇。总体而言，本文旨在为寻求构建更强大、高效和可靠的智能体人工智能系统的研究者与实践者提供概念基础和实践路线图。",
    "url": "https://huggingface.co/papers/2512.16301",
    "arxiv_url": "https://arxiv.org/abs/2512.16301"
  },
  {
    "title": "LLaDA2.0: Scaling Up Diffusion Language Models to 100B",
    "summary": "This paper presents LLaDA2.0 -- a tuple of discrete diffusion large language models (dLLM) scaling up to 100B total parameters through systematic conversion from auto-regressive (AR) models -- establishing a new paradigm for frontier-scale deployment. Instead of costly training from scratch, LLaDA2.0 upholds knowledge inheritance, progressive adaption and efficiency-aware design principle, and seamless converts a pre-trained AR model into dLLM with a novel 3-phase block-level WSD based training scheme: progressive increasing block-size in block diffusion (warm-up), large-scale full-sequence diffusion (stable) and reverting back to compact-size block diffusion (decay). Along with post-training alignment with SFT and DPO, we obtain LLaDA2.0-mini (16B) and LLaDA2.0-flash (100B), two instruction-tuned Mixture-of-Experts (MoE) variants optimized for practical deployment. By preserving the advantages of parallel decoding, these models deliver superior performance and efficiency at the frontier scale. Both models were open-sourced.",
    "translation": "标题：LLaDA2.0：将扩散语言模型扩展至千亿参数规模\n\n摘要：本文提出LLaDA2.0——一组通过自回归模型系统化转换构建、总参数量达千亿规模的离散扩散大语言模型，为前沿规模部署建立了新范式。该方法摒弃了成本高昂的从头训练，秉持知识继承、渐进适应与效率优先的设计原则，通过创新的三阶段块级加权序列扩散训练方案，将预训练的自回归模型无缝转换为扩散语言模型：该方案包含块扩散中逐步增加块大小的预热阶段、大规模全序列扩散的稳定阶段，以及回归紧凑块扩散的衰减阶段。结合监督微调与直接偏好优化的训练后对齐，我们获得了LLaDA2.0-mini（160亿参数）和LLaDA2.0-flash（千亿参数）两个经过指令调优的混合专家模型变体，专为实际部署优化。通过保持并行解码的优势，这些模型在前沿规模上实现了卓越的性能与效率。两个模型均已开源发布。",
    "url": "https://huggingface.co/papers/2512.15745",
    "arxiv_url": "https://arxiv.org/abs/2512.15745"
  },
  {
    "title": "Next-Embedding Prediction Makes Strong Vision Learners",
    "summary": "Inspired by the success of generative pretraining in natural language, we ask whether the same principles can yield strong self-supervised visual learners. Instead of training models to output features for downstream use, we train them to generate embeddings to perform predictive tasks directly. This work explores such a shift from learning representations to learning models. Specifically, models learn to predict future patch embeddings conditioned on past ones, using causal masking and stop gradient, which we refer to as Next-Embedding Predictive Autoregression (NEPA). We demonstrate that a simple Transformer pretrained on ImageNet-1k with next embedding prediction as its sole learning objective is effective - no pixel reconstruction, discrete tokens, contrastive loss, or task-specific heads. This formulation retains architectural simplicity and scalability, without requiring additional design complexity. NEPA achieves strong results across tasks, attaining 83.8% and 85.3% top-1 accuracy on ImageNet-1K with ViT-B and ViT-L backbones after fine-tuning, and transferring effectively to semantic segmentation on ADE20K. We believe generative pretraining from embeddings provides a simple, scalable, and potentially modality-agnostic alternative to visual self-supervised learning.",
    "translation": "标题：下一嵌入预测构建强视觉学习器\n\n摘要：受生成式预训练在自然语言领域成功的启发，我们探究相同原理能否构建强大的自监督视觉学习器。不同于训练模型输出特征供下游任务使用，我们训练模型直接生成嵌入以执行预测任务。本研究探索了这种从学习表征到学习模型的范式转变。具体而言，模型通过因果掩码和梯度截断技术，学习基于历史图像块嵌入预测未来嵌入，我们将其称为“下一嵌入预测自回归”（NEPA）。实验证明，在ImageNet-1k数据集上仅以下一嵌入预测为学习目标预训练的简单Transformer模型即具有显著效果——无需像素重建、离散标记、对比损失或任务特定头设计。该方案保持了架构简洁性与可扩展性，无需引入额外设计复杂度。NEPA在多项任务中表现优异：基于ViT-B和ViT-L骨干网络微调后，在ImageNet-1K上分别达到83.8%和85.3%的Top-1准确率，并能有效迁移至ADE20K语义分割任务。我们相信基于嵌入的生成式预训练为视觉自监督学习提供了一种简洁、可扩展且可能模态无关的替代方案。",
    "url": "https://huggingface.co/papers/2512.16922",
    "arxiv_url": "https://arxiv.org/abs/2512.16922"
  },
  {
    "title": "StereoPilot: Learning Unified and Efficient Stereo Conversion via Generative Priors",
    "summary": "The rapid growth of stereoscopic displays, including VR headsets and 3D cinemas, has led to increasing demand for high-quality stereo video content. However, producing 3D videos remains costly and complex, while automatic Monocular-to-Stereo conversion is hindered by the limitations of the multi-stage ``Depth-Warp-Inpaint'' (DWI) pipeline. This paradigm suffers from error propagation, depth ambiguity, and format inconsistency between parallel and converged stereo configurations. To address these challenges, we introduce UniStereo, the first large-scale unified dataset for stereo video conversion, covering both stereo formats to enable fair benchmarking and robust model training. Building upon this dataset, we propose StereoPilot, an efficient feed-forward model that directly synthesizes the target view without relying on explicit depth maps or iterative diffusion sampling. Equipped with a learnable domain switcher and a cycle consistency loss, StereoPilot adapts seamlessly to different stereo formats and achieves improved consistency. Extensive experiments demonstrate that StereoPilot significantly outperforms state-of-the-art methods in both visual fidelity and computational efficiency. Project page: https://hit-perfect.github.io/StereoPilot/.",
    "translation": "标题：StereoPilot：基于生成先验学习统一高效立体转换方法\n\n摘要：随着立体显示设备（如VR头显和3D影院）的快速发展，市场对高质量立体视频内容的需求日益增长。然而，3D视频制作成本高昂且流程复杂，而自动化的单目到立体转换技术受限于多阶段“深度-形变-修复”（DWI）流程的固有缺陷。该范式存在误差传递、深度歧义以及平行与汇聚立体格式不一致等问题。为解决这些挑战，本文首次构建了UniStereo——一个覆盖两种立体格式的大规模统一立体视频转换数据集，以支持公平基准测试与鲁棒的模型训练。基于此数据集，我们提出StereoPilot模型，该高效前馈模型无需依赖显式深度图或迭代扩散采样，可直接合成目标视角视图。通过可学习的域切换模块与循环一致性损失函数，StereoPilot能够自适应不同立体格式并提升视觉一致性。大量实验表明，StereoPilot在视觉保真度与计算效率方面均显著优于现有先进方法。项目页面：https://hit-perfect.github.io/StereoPilot/。",
    "url": "https://huggingface.co/papers/2512.16915",
    "arxiv_url": "https://arxiv.org/abs/2512.16915"
  },
  {
    "title": "Seedance 1.5 pro: A Native Audio-Visual Joint Generation Foundation Model",
    "summary": "Recent strides in video generation have paved the way for unified audio-visual generation. In this work, we present Seedance 1.5 pro, a foundational model engineered specifically for native, joint audio-video generation. Leveraging a dual-branch Diffusion Transformer architecture, the model integrates a cross-modal joint module with a specialized multi-stage data pipeline, achieving exceptional audio-visual synchronization and superior generation quality. To ensure practical utility, we implement meticulous post-training optimizations, including Supervised Fine-Tuning (SFT) on high-quality datasets and Reinforcement Learning from Human Feedback (RLHF) with multi-dimensional reward models. Furthermore, we introduce an acceleration framework that boosts inference speed by over 10X. Seedance 1.5 pro distinguishes itself through precise multilingual and dialect lip-syncing, dynamic cinematic camera control, and enhanced narrative coherence, positioning it as a robust engine for professional-grade content creation. Seedance 1.5 pro is now accessible on Volcano Engine at https://console.volcengine.com/ark/region:ark+cn-beijing/experience/vision?type=GenVideo.",
    "translation": "标题：Seedance 1.5 pro：原生音视频联合生成基础模型\n\n摘要：近期视频生成领域的进展为统一的音视频生成奠定了基础。本研究提出了Seedance 1.5 pro——一个专为原生音视频联合生成设计的基础模型。该模型采用双分支扩散Transformer架构，通过跨模态联合模块与专门设计的多阶段数据流程相结合，实现了卓越的音画同步效果与生成质量。为提升实用价值，我们实施了精细的后训练优化策略，包括基于高质量数据集的有监督微调，以及结合多维度奖励模型的人类反馈强化学习。此外，我们引入了加速推理框架，将生成速度提升超过10倍。Seedance 1.5 pro在多语言/方言口型同步、动态电影级镜头控制及叙事连贯性增强方面表现突出，使其成为专业级内容创作的强大引擎。该模型现已通过火山引擎平台开放使用，访问地址：https://console.volcengine.com/ark/region:ark+cn-beijing/experience/vision?type=GenVideo。",
    "url": "https://huggingface.co/papers/2512.13507",
    "arxiv_url": "https://arxiv.org/abs/2512.13507"
  },
  {
    "title": "Depth Any Panoramas: A Foundation Model for Panoramic Depth Estimation",
    "summary": "In this work, we present a panoramic metric depth foundation model that generalizes across diverse scene distances. We explore a data-in-the-loop paradigm from the view of both data construction and framework design. We collect a large-scale dataset by combining public datasets, high-quality synthetic data from our UE5 simulator and text-to-image models, and real panoramic images from the web. To reduce domain gaps between indoor/outdoor and synthetic/real data, we introduce a three-stage pseudo-label curation pipeline to generate reliable ground truth for unlabeled images. For the model, we adopt DINOv3-Large as the backbone for its strong pre-trained generalization, and introduce a plug-and-play range mask head, sharpness-centric optimization, and geometry-centric optimization to improve robustness to varying distances and enforce geometric consistency across views. Experiments on multiple benchmarks (e.g., Stanford2D3D, Matterport3D, and Deep360) demonstrate strong performance and zero-shot generalization, with particularly robust and stable metric predictions in diverse real-world scenes. The project page can be found at: https://insta360-research-team.github.io/DAP_website/ {https://insta360-research-team.github.io/DAP\\_website/}",
    "translation": "标题：深度全景：全景深度估计的基础模型\n\n摘要：本研究提出了一种适用于多样化场景距离的全景度量深度基础模型。我们从数据构建与框架设计的双重维度探索了数据闭环范式。通过整合公开数据集、基于UE5模拟器的高质量合成数据、文本到图像生成模型以及网络采集的真实全景图像，我们构建了大规模训练数据集。为缩减室内/室外与合成/真实数据间的域差异，我们设计了包含三阶段的伪标签优化流程，为未标注图像生成可靠真值。模型架构方面，采用具有强预训练泛化能力的DINOv3-Large作为主干网络，并创新性地引入即插即用式距离掩码头模块、以清晰度为核心的优化策略以及以几何一致性为导向的优化方法，从而提升模型对多尺度距离的鲁棒性并强化跨视角几何一致性。在多个基准数据集（如Stanford2D3D、Matterport3D和Deep360）上的实验表明，该模型在保持优异性能与零样本泛化能力的同时，能够在复杂现实场景中实现鲁棒且稳定的度量深度预测。项目页面详见：https://insta360-research-team.github.io/DAP_website/",
    "url": "https://huggingface.co/papers/2512.16913",
    "arxiv_url": "https://arxiv.org/abs/2512.16913"
  },
  {
    "title": "Generative Refocusing: Flexible Defocus Control from a Single Image",
    "summary": "Depth-of-field control is essential in photography, but getting the perfect focus often takes several tries or special equipment. Single-image refocusing is still difficult. It involves recovering sharp content and creating realistic bokeh. Current methods have significant drawbacks. They need all-in-focus inputs, depend on synthetic data from simulators, and have limited control over aperture. We introduce Generative Refocusing, a two-step process that uses DeblurNet to recover all-in-focus images from various inputs and BokehNet for creating controllable bokeh. Our main innovation is semi-supervised training. This method combines synthetic paired data with unpaired real bokeh images, using EXIF metadata to capture real optical characteristics beyond what simulators can provide. Our experiments show we achieve top performance in defocus deblurring, bokeh synthesis, and refocusing benchmarks. Additionally, our Generative Refocusing allows text-guided adjustments and custom aperture shapes.",
    "translation": "标题：生成式重聚焦：基于单张图像的灵活散焦控制\n\n摘要：景深控制在摄影中至关重要，但获得完美对焦通常需要多次尝试或特殊设备。单图像重聚焦技术仍面临挑战，其涉及恢复清晰内容并生成逼真的焦外虚化效果。现有方法存在明显局限：需要全对焦输入图像、依赖仿真器生成的合成数据，且对光圈的控制能力有限。本文提出生成式重聚焦技术，采用包含去模糊网络与虚化网络的两阶段流程，前者可从多样化输入中恢复全对焦图像，后者用于生成可控虚化效果。本研究的核心创新在于半监督训练方法：通过结合合成配对数据与未配对真实虚化图像，并利用EXIF元数据捕捉仿真器无法提供的真实光学特性。实验表明，我们的方法在散焦去模糊、虚化合成和重聚焦基准测试中均达到最优性能。此外，所提出的生成式重聚焦系统支持文本引导的参数调整与自定义光圈形状控制。",
    "url": "https://huggingface.co/papers/2512.16923",
    "arxiv_url": "https://arxiv.org/abs/2512.16923"
  },
  {
    "title": "DeContext as Defense: Safe Image Editing in Diffusion Transformers",
    "summary": "In-context diffusion models allow users to modify images with remarkable ease and realism. However, the same power raises serious privacy concerns: personal images can be easily manipulated for identity impersonation, misinformation, or other malicious uses, all without the owner's consent. While prior work has explored input perturbations to protect against misuse in personalized text-to-image generation, the robustness of modern, large-scale in-context DiT-based models remains largely unexamined. In this paper, we propose DeContext, a new method to safeguard input images from unauthorized in-context editing. Our key insight is that contextual information from the source image propagates to the output primarily through multimodal attention layers. By injecting small, targeted perturbations that weaken these cross-attention pathways, DeContext breaks this flow, effectively decouples the link between input and output. This simple defense is both efficient and robust. We further show that early denoising steps and specific transformer blocks dominate context propagation, which allows us to concentrate perturbations where they matter most. Experiments on Flux Kontext and Step1X-Edit show that DeContext consistently blocks unwanted image edits while preserving visual quality. These results highlight the effectiveness of attention-based perturbations as a powerful defense against image manipulation.",
    "translation": "标题：DeContext 防御法：扩散变换器中的安全图像编辑\n\n摘要：上下文扩散模型使用户能够以极高的便捷性和真实感修改图像。然而，这种能力也引发了严重的隐私担忧：个人图像可能被轻易用于身份冒充、虚假信息传播或其他恶意用途，且均未经所有者同意。尽管已有研究探索通过输入扰动来防范个性化文生图模型中的滥用行为，但现代大规模基于上下文扩散变换器（DiT）模型的鲁棒性仍缺乏深入检验。本文提出DeContext方法，旨在保护输入图像免遭未经授权的上下文编辑。我们的核心发现是：源图像的上下文信息主要通过多模态注意力层传播至输出结果。通过注入微小且有针对性的扰动以削弱这些跨注意力路径，DeContext能够阻断信息流，有效解耦输入与输出之间的关联。这种简洁的防御机制兼具高效性与鲁棒性。我们进一步证明，早期去噪步骤和特定变换器模块主导着上下文传播过程，这使得我们可以将扰动集中在最关键的位置。在Flux Kontext和Step1X-Edit数据集上的实验表明，DeContext能持续阻断非预期的图像编辑，同时保持视觉质量。这些结果凸显了基于注意力机制的扰动策略作为抵御图像篡改的有效防御手段。",
    "url": "https://huggingface.co/papers/2512.16625",
    "arxiv_url": "https://arxiv.org/abs/2512.16625"
  },
  {
    "title": "REGLUE Your Latents with Global and Local Semantics for Entangled Diffusion",
    "summary": "Latent diffusion models (LDMs) achieve state-of-the-art image synthesis, yet their reconstruction-style denoising objective provides only indirect semantic supervision: high-level semantics emerge slowly, requiring longer training and limiting sample quality. Recent works inject semantics from Vision Foundation Models (VFMs) either externally via representation alignment or internally by jointly modeling only a narrow slice of VFM features inside the diffusion process, under-utilizing the rich, nonlinear, multi-layer spatial semantics available. We introduce REGLUE (Representation Entanglement with Global-Local Unified Encoding), a unified latent diffusion framework that jointly models (i) VAE image latents, (ii) compact local (patch-level) VFM semantics, and (iii) a global (image-level) [CLS] token within a single SiT backbone. A lightweight convolutional semantic compressor nonlinearly aggregates multi-layer VFM features into a low-dimensional, spatially structured representation, which is entangled with the VAE latents in the diffusion process. An external alignment loss further regularizes internal representations toward frozen VFM targets. On ImageNet 256x256, REGLUE consistently improves FID and accelerates convergence over SiT-B/2 and SiT-XL/2 baselines, as well as over REPA, ReDi, and REG. Extensive experiments show that (a) spatial VFM semantics are crucial, (b) non-linear compression is key to unlocking their full benefit, and (c) global tokens and external alignment act as complementary, lightweight enhancements within our global-local-latent joint modeling framework. The code is available at https://github.com/giorgospets/reglue .",
    "translation": "标题：REGLUE：融合全局与局部语义的隐变量纠缠扩散方法\n\n摘要：隐扩散模型（LDMs）在图像合成领域取得了最先进的性能，但其重建式去噪目标仅提供间接的语义监督：高级语义特征形成缓慢，导致训练周期延长并限制生成质量。近期研究尝试通过视觉基础模型（VFMs）注入语义信息，或采用外部表征对齐方式，或仅在扩散过程中内部联合建模有限的VFM特征层，未能充分利用VFM中丰富、非线性、多层级的空间语义特征。本文提出REGLUE（全局-局部统一编码的表征纠缠框架），该统一隐扩散框架在单个SiT主干网络中联合建模：（i）VAE图像隐变量，（ii）紧凑的局部（图像块级）VFM语义，以及（iii）全局（图像级）[CLS]标记。通过轻量级卷积语义压缩器对多层VFM特征进行非线性聚合，生成低维且具有空间结构的表征，该表征在扩散过程中与VAE隐变量形成纠缠。外部对齐损失进一步约束内部表征向冻结的VFM目标对齐。在ImageNet 256×256数据集上，REGLUE相较于SiT-B/2和SiT-XL/2基线模型，以及REPA、ReDi和REG方法，在FID指标上持续提升并加速收敛。大量实验表明：（a）空间VFM语义至关重要，（b）非线性压缩是充分发挥其效益的关键，（c）全局标记与外部对齐在本框架的全局-局部-隐变量联合建模中起到互补的轻量化增强作用。代码已开源：https://github.com/giorgospets/reglue。",
    "url": "https://huggingface.co/papers/2512.16636",
    "arxiv_url": "https://arxiv.org/abs/2512.16636"
  },
  {
    "title": "Alchemist: Unlocking Efficiency in Text-to-Image Model Training via Meta-Gradient Data Selection",
    "summary": "Recent advances in Text-to-Image (T2I) generative models, such as Imagen, Stable Diffusion, and FLUX, have led to remarkable improvements in visual quality. However, their performance is fundamentally limited by the quality of training data. Web-crawled and synthetic image datasets often contain low-quality or redundant samples, which lead to degraded visual fidelity, unstable training, and inefficient computation. Hence, effective data selection is crucial for improving data efficiency. Existing approaches rely on costly manual curation or heuristic scoring based on single-dimensional features in Text-to-Image data filtering. Although meta-learning based method has been explored in LLM, there is no adaptation for image modalities. To this end, we propose **Alchemist**, a meta-gradient-based framework to select a suitable subset from large-scale text-image data pairs. Our approach automatically learns to assess the influence of each sample by iteratively optimizing the model from a data-centric perspective. Alchemist consists of two key stages: data rating and data pruning. We train a lightweight rater to estimate each sample's influence based on gradient information, enhanced with multi-granularity perception. We then use the Shift-Gsampling strategy to select informative subsets for efficient model training. Alchemist is the first automatic, scalable, meta-gradient-based data selection framework for Text-to-Image model training. Experiments on both synthetic and web-crawled datasets demonstrate that Alchemist consistently improves visual quality and downstream performance. Training on an Alchemist-selected 50% of the data can outperform training on the full dataset.",
    "translation": "标题：Alchemist：基于元梯度的数据选择提升文本到图像模型训练效率\n\n摘要：文本到图像（T2I）生成模型（如Imagen、Stable Diffusion和FLUX）的最新进展显著提升了视觉生成质量。然而，其性能从根本上受限于训练数据的质量。网络爬取和合成图像数据集常包含低质量或冗余样本，导致视觉保真度下降、训练过程不稳定以及计算效率低下。因此，有效的数据选择对于提升数据效率至关重要。现有方法依赖于成本高昂的人工筛选或基于文本到图像数据过滤中单一维度特征的启发式评分。尽管基于元学习的方法已在大型语言模型中得到探索，但尚未适用于图像模态。为此，我们提出**Alchemist**，一个基于元梯度的框架，用于从大规模文本-图像数据对中选择合适的子集。该方法通过以数据为中心的视角迭代优化模型，自动学习评估每个样本的影响。Alchemist包含两个关键阶段：数据评级与数据剪枝。我们训练一个轻量级评级器，基于梯度信息并辅以多粒度感知来估计每个样本的影响，随后采用Shift-G采样策略选择信息丰富的子集以进行高效模型训练。Alchemist是首个面向文本到图像模型训练的自动化、可扩展、基于元梯度的数据选择框架。在合成与网络爬取数据集上的实验表明，Alchemist能持续提升视觉质量与下游任务性能。使用Alchemist选择的50%数据训练，其效果可优于使用完整数据集训练的结果。",
    "url": "https://huggingface.co/papers/2512.16905",
    "arxiv_url": "https://arxiv.org/abs/2512.16905"
  },
  {
    "title": "The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text",
    "summary": "We present WorldCanvas, a framework for promptable world events that enables rich, user-directed simulation by combining text, trajectories, and reference images. Unlike text-only approaches and existing trajectory-controlled image-to-video methods, our multimodal approach combines trajectories -- encoding motion, timing, and visibility -- with natural language for semantic intent and reference images for visual grounding of object identity, enabling the generation of coherent, controllable events that include multi-agent interactions, object entry/exit, reference-guided appearance and counterintuitive events. The resulting videos demonstrate not only temporal coherence but also emergent consistency, preserving object identity and scene despite temporary disappearance. By supporting expressive world events generation, WorldCanvas advances world models from passive predictors to interactive, user-shaped simulators. Our project page is available at: https://worldcanvas.github.io/.",
    "translation": "标题：世界即画布：融合参考图像、轨迹与文本的可提示事件绘制\n\n摘要：本文提出WorldCanvas框架，该框架通过融合文本、轨迹与参考图像，构建了支持丰富用户导向模拟的可提示世界事件系统。相较于纯文本方法及现有轨迹控制的图像转视频技术，我们的多模态方法将编码运动、时序与可见性的轨迹、表达语义意图的自然语言，以及确立物体视觉特征的参考图像相结合，从而能够生成包含多智能体交互、物体进出场、参考图像引导的外观呈现及反直觉事件在内的连贯可控事件。生成的视频不仅具备时序连贯性，更展现出涌现一致性——即使物体暂时消失，其身份特征与场景信息仍得以保持。通过支持富有表现力的世界事件生成，WorldCanvas推动了世界模型从被动预测器向用户可交互式模拟器的演进。项目页面详见：https://worldcanvas.github.io/。",
    "url": "https://huggingface.co/papers/2512.16924",
    "arxiv_url": "https://arxiv.org/abs/2512.16924"
  },
  {
    "title": "N3D-VLM: Native 3D Grounding Enables Accurate Spatial Reasoning in Vision-Language Models",
    "summary": "While current multimodal models can answer questions based on 2D images, they lack intrinsic 3D object perception, limiting their ability to comprehend spatial relationships and depth cues in 3D scenes. In this work, we propose N3D-VLM, a novel unified framework that seamlessly integrates native 3D object perception with 3D-aware visual reasoning, enabling both precise 3D grounding and interpretable spatial understanding. Unlike conventional end-to-end models that directly predict answers from RGB/RGB-D inputs, our approach equips the model with native 3D object perception capabilities, enabling it to directly localize objects in 3D space based on textual descriptions. Building upon accurate 3D object localization, the model further performs explicit reasoning in 3D, achieving more interpretable and structured spatial understanding. To support robust training for these capabilities, we develop a scalable data construction pipeline that leverages depth estimation to lift large-scale 2D annotations into 3D space, significantly increasing the diversity and coverage for 3D object grounding data, yielding over six times larger than the largest existing single-image 3D detection dataset. Moreover, the pipeline generates spatial question-answering datasets that target chain-of-thought (CoT) reasoning in 3D, facilitating joint training for both 3D object localization and 3D spatial reasoning. Experimental results demonstrate that our unified framework not only achieves state-of-the-art performance on 3D grounding tasks, but also consistently surpasses existing methods in 3D spatial reasoning in vision-language model.",
    "translation": "标题：N3D-VLM：原生三维感知赋能视觉语言模型实现精确空间推理\n\n摘要：当前的多模态模型虽能基于二维图像回答问题，但缺乏内在的三维物体感知能力，限制了其理解三维场景中空间关系与深度信息的能力。本研究提出N3D-VLM——一种将原生三维物体感知与三维视觉推理无缝整合的新型统一框架，既能实现精确的三维定位，又能获得可解释的空间理解。与传统端到端模型直接从RGB/RGB-D输入预测答案不同，我们的方法赋予模型原生三维物体感知能力，使其能够依据文本描述直接在三维空间中定位物体。在实现精确三维物体定位的基础上，模型进一步在三维空间中进行显式推理，从而获得更具可解释性和结构化的空间理解。为支撑这些能力的鲁棒性训练，我们开发了可扩展的数据构建流程：通过深度估计技术将大规模二维标注提升至三维空间，显著增强了三维物体定位数据的多样性与覆盖范围，生成的数据集规模达到现有最大单图像三维检测数据集的六倍以上。该流程还能生成针对三维思维链推理的空间问答数据集，促进三维物体定位与三维空间推理的联合训练。实验结果表明，我们的统一框架不仅在三维定位任务上达到最先进性能，在视觉语言模型的三维空间推理任务中也持续超越现有方法。",
    "url": "https://huggingface.co/papers/2512.16561",
    "arxiv_url": "https://arxiv.org/abs/2512.16561"
  },
  {
    "title": "JustRL: Scaling a 1.5B LLM with a Simple RL Recipe",
    "summary": "Recent advances in reinforcement learning for large language models have converged on increasing complexity: multi-stage training pipelines, dynamic hyperparameter schedules, and curriculum learning strategies. This raises a fundamental question: Is this complexity necessary? We present JustRL, a minimal approach using single-stage training with fixed hyperparameters that achieves state-of-the-art performance on two 1.5B reasoning models (54.9\\% and 64.3\\% average accuracy across nine mathematical benchmarks) while using 2times less compute than sophisticated approaches. The same hyperparameters transfer across both models without tuning, and training exhibits smooth, monotonic improvement over 4,000+ steps without the collapses or plateaus that typically motivate interventions. Critically, ablations reveal that adding ``standard tricks'' like explicit length penalties and robust verifiers may degrade performance by collapsing exploration. These results suggest that the field may be adding complexity to solve problems that disappear with a stable, scaled-up baseline. We release our models and code to establish a simple, validated baseline for the community.",
    "translation": "标题：JustRL：采用简易强化学习方案扩展15亿参数大语言模型\n\n摘要：近期大语言模型强化学习领域的发展呈现出日益复杂的趋势：多阶段训练流程、动态超参数调度以及课程学习策略。这引发了一个根本性问题：此类复杂性是否必要？本文提出JustRL——一种采用固定超参数的单阶段训练极简方案，在两个15亿参数推理模型上实现了最先进性能（在九项数学基准测试中平均准确率分别达到54.9%和64.3%），同时计算消耗比复杂方案减少2倍。相同超参数可在两个模型间直接迁移而无需调整，且历经4000余步训练仍保持平滑单调的改进趋势，未出现通常需要干预的崩溃或平台期。关键的是，消融实验表明，添加显式长度惩罚和鲁棒验证器等\"标准技巧\"可能因压缩探索空间而导致性能下降。这些结果表明，当前领域可能正在通过增加复杂性来解决本可通过稳定、规模化基线自然消解的问题。我们公开模型与代码，旨在为学界建立一个经过验证的简易基线。\n\n请按照以下格式返回：\n标题：JustRL：采用简易强化学习方案扩展15亿参数大语言模型\n摘要：近期大语言模型强化学习领域的发展呈现出日益复杂的趋势：多阶段训练流程、动态超参数调度以及课程学习策略。这引发了一个根本性问题：此类复杂性是否必要？本文提出JustRL——一种采用固定超参数的单阶段训练极简方案，在两个15亿参数推理模型上实现了最先进性能（在九项数学基准测试中平均准确率分别达到54.9%和64.3%），同时计算消耗比复杂方案减少2倍。相同超参数可在两个模型间直接迁移而无需调整，且历经4000余步训练仍保持平滑单调的改进趋势，未出现通常需要干预的崩溃或平台期。关键的是，消融实验表明，添加显式长度惩罚和鲁棒验证器等\"标准技巧\"可能因压缩探索空间而导致性能下降。这些结果表明，当前领域可能正在通过增加复杂性来解决本可通过稳定、规模化基线自然消解的问题。我们公开模型与代码，旨在为学界建立一个经过验证的简易基线。",
    "url": "https://huggingface.co/papers/2512.16649",
    "arxiv_url": "https://arxiv.org/abs/2512.16649"
  },
  {
    "title": "AdaTooler-V: Adaptive Tool-Use for Images and Videos",
    "summary": "Recent advances have shown that multimodal large language models (MLLMs) benefit from multimodal interleaved chain-of-thought (CoT) with vision tool interactions. However, existing open-source models often exhibit blind tool-use reasoning patterns, invoking vision tools even when they are unnecessary, which significantly increases inference overhead and degrades model performance. To this end, we propose AdaTooler-V, an MLLM that performs adaptive tool-use by determining whether a visual problem truly requires tools. First, we introduce AT-GRPO, a reinforcement learning algorithm that adaptively adjusts reward scales based on the Tool Benefit Score of each sample, encouraging the model to invoke tools only when they provide genuine improvements. Moreover, we construct two datasets to support training: AdaTooler-V-CoT-100k for SFT cold start and AdaTooler-V-300k for RL with verifiable rewards across single-image, multi-image, and video data. Experiments across twelve benchmarks demonstrate the strong reasoning capability of AdaTooler-V, outperforming existing methods in diverse visual reasoning tasks. Notably, AdaTooler-V-7B achieves an accuracy of 89.8\\% on the high-resolution benchmark V*, surpassing the commercial proprietary model GPT-4o and Gemini 1.5 Pro. All code, models, and data are released.",
    "translation": "标题：AdaTooler-V：面向图像与视频的自适应工具使用\n\n摘要：近期研究表明，多模态大语言模型（MLLMs）能够通过结合视觉工具交互的多模态交错思维链（CoT）获得性能提升。然而，现有开源模型常表现出盲目的工具使用推理模式，即使在无需工具时也会调用视觉工具，这显著增加了推理开销并降低了模型性能。为此，我们提出AdaTooler-V，一种能够通过判断视觉问题是否真正需要工具来实现自适应工具使用的MLLM。首先，我们引入AT-GRPO强化学习算法，该算法根据每个样本的“工具效益评分”自适应调整奖励尺度，鼓励模型仅在工具能带来实质改进时才调用它们。此外，我们构建了两个支持训练的数据集：用于监督微调冷启动的AdaTooler-V-CoT-100k，以及用于覆盖单图像、多图像和视频数据的可验证奖励强化学习的AdaTooler-V-300k。在十二个基准测试上的实验表明，AdaTooler-V具备强大的推理能力，在多样化的视觉推理任务中优于现有方法。值得注意的是，AdaTooler-V-7B在高分辨率基准V*上达到了89.8%的准确率，超越了商业闭源模型GPT-4o和Gemini 1.5 Pro。所有代码、模型和数据均已开源发布。",
    "url": "https://huggingface.co/papers/2512.16918",
    "arxiv_url": "https://arxiv.org/abs/2512.16918"
  },
  {
    "title": "Exploration v.s. Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward",
    "summary": "This paper examines the exploration-exploitation trade-off in reinforcement learning with verifiable rewards (RLVR), a framework for improving the reasoning of Large Language Models (LLMs). Recent studies suggest that RLVR can elicit strong mathematical reasoning in LLMs through two seemingly paradoxical mechanisms: spurious rewards, which suppress exploitation by rewarding outcomes unrelated to the ground truth, and entropy minimization, which suppresses exploration by pushing the model toward more confident and deterministic outputs, highlighting a puzzling dynamic: both discouraging exploitation and discouraging exploration improve reasoning performance, yet the underlying principles that reconcile these effects remain poorly understood. We focus on two fundamental questions: (i) how policy entropy relates to performance, and (ii) whether spurious rewards yield gains, potentially through the interplay of clipping bias and model contamination. Our results show that clipping bias under spurious rewards reduces policy entropy, leading to more confident and deterministic outputs, while entropy minimization alone is insufficient for improvement. We further propose a reward-misalignment model explaining why spurious rewards can enhance performance beyond contaminated settings. Our findings clarify the mechanisms behind spurious-reward benefits and provide principles for more effective RLVR training.",
    "translation": "标题：探索与利用之辨：通过剪裁、熵与伪奖励重思可验证奖励强化学习\n\n摘要：本文研究了可验证奖励强化学习框架中的探索-利用权衡问题，该框架旨在提升大语言模型的推理能力。近期研究表明，RLVR可通过两种看似矛盾的机制激发大语言模型的数学推理能力：伪奖励机制通过奖励与真实答案无关的结果来抑制模型过度利用既有知识，而熵最小化机制则通过推动模型产生更确定性的输出来抑制探索行为。这揭示了一个令人困惑的动态现象：抑制利用与抑制探索均能提升推理性能，但协调这两种效应的内在原理尚不明确。本研究聚焦两个核心问题：（1）策略熵如何影响模型性能；（2）伪奖励是否通过剪裁偏差与模型污染之间的相互作用产生增益。实验结果表明，伪奖励下的剪裁偏差会降低策略熵，促使模型产生更确定性的输出，而单纯的熵最小化并不足以带来性能提升。我们进一步提出奖励错配模型，阐释了伪奖励为何能在非污染场景下提升模型性能。本研究阐明了伪奖励产生增益的内在机制，并为更有效的RLVR训练提供了理论依据。",
    "url": "https://huggingface.co/papers/2512.16912",
    "arxiv_url": "https://arxiv.org/abs/2512.16912"
  },
  {
    "title": "Multimodal RewardBench 2: Evaluating Omni Reward Models for Interleaved Text and Image",
    "summary": "Reward models (RMs) are essential for training large language models (LLMs), but remain underexplored for omni models that handle interleaved image and text sequences. We introduce Multimodal RewardBench 2 (MMRB2), the first comprehensive benchmark for reward models on multimodal understanding and (interleaved) generation. MMRB2 spans four tasks: text-to-image, image editing, interleaved generation, and multimodal reasoning (\"thinking-with-images\"), providing 1,000 expert-annotated preference pairs per task from 23 models and agents across 21 source tasks. MMRB2 is designed with: (1) practical but challenging prompts; (2) responses from state-of-the-art models and agents; and (3) preference pairs with strong human-expert consensus, curated via an ensemble filtering strategy. Using MMRB2, we study existing judges for each subtask, including multimodal LLM-as-a-judge and models trained with human preferences. The latest Gemini 3 Pro attains 75-80% accuracy. GPT-5 and Gemini 2.5 Pro reach 66-75% accuracy, compared to >90% for humans, yet surpass the widely used GPT-4o (59%). The best performing open-source model Qwen3-VL-32B achieves similar accuracies as Gemini 2.5 Flash (64%). We also show that MMRB2 performance strongly correlates with downstream task success using Best-of-N sampling and conduct an in-depth analysis that shows key areas to improve the reward models going forward.",
    "translation": "标题：多模态奖励基准2：评估交错文本与图像的全能奖励模型\n\n摘要：奖励模型对于训练大语言模型至关重要，但在处理交错图像与文本序列的全能模型领域仍研究不足。本文提出多模态奖励基准2，这是首个针对多模态理解与（交错）生成任务的奖励模型综合基准。该基准涵盖四大任务：文本到图像生成、图像编辑、交错内容生成以及多模态推理（“图像思维”），每个任务提供来自23个模型与智能体在21个源任务中产生的1000组专家标注偏好对。MMRB2的设计具备三大特点：（1）实用且具有挑战性的提示；（2）汇集前沿模型与智能体的响应；（3）通过集成过滤策略构建具有强专家共识的偏好对。基于MMRB2，我们系统评估了各子任务的现有评判器，包括多模态大语言模型作为评判器以及经人类偏好训练的模型。最新Gemini 3 Pro模型达到75-80%的准确率，GPT-5与Gemini 2.5 Pro达到66-75%的准确率（人类专家准确率＞90%），但仍显著超越广泛使用的GPT-4o（59%）。性能最佳的开源模型Qwen3-VL-32B达到与Gemini 2.5 Flash相当的准确率（64%）。研究进一步表明，通过N选一采样策略，MMRB2评估结果与下游任务表现呈强相关性，深入分析揭示了奖励模型未来改进的关键方向。",
    "url": "https://huggingface.co/papers/2512.16899",
    "arxiv_url": "https://arxiv.org/abs/2512.16899"
  },
  {
    "title": "EasyV2V: A High-quality Instruction-based Video Editing Framework",
    "summary": "While image editing has advanced rapidly, video editing remains less explored, facing challenges in consistency, control, and generalization. We study the design space of data, architecture, and control, and introduce EasyV2V, a simple and effective framework for instruction-based video editing. On the data side, we compose existing experts with fast inverses to build diverse video pairs, lift image edit pairs into videos via single-frame supervision and pseudo pairs with shared affine motion, mine dense-captioned clips for video pairs, and add transition supervision to teach how edits unfold. On the model side, we observe that pretrained text-to-video models possess editing capability, motivating a simplified design. Simple sequence concatenation for conditioning with light LoRA fine-tuning suffices to train a strong model. For control, we unify spatiotemporal control via a single mask mechanism and support optional reference images. Overall, EasyV2V works with flexible inputs, e.g., video+text, video+mask+text, video+mask+reference+text, and achieves state-of-the-art video editing results, surpassing concurrent and commercial systems. Project page: https://snap-research.github.io/easyv2v/",
    "translation": "标题：EasyV2V：一种基于指令的高质量视频编辑框架\n\n摘要：尽管图像编辑技术发展迅速，视频编辑领域仍相对缺乏探索，在一致性、控制能力和泛化性方面面临挑战。本研究系统探讨了数据、架构与控制机制的设计空间，并提出了EasyV2V——一个简洁高效的基于指令的视频编辑框架。在数据层面，我们通过快速逆变换整合现有专家模型构建多样化视频对，借助单帧监督与共享仿射运动的伪配对将图像编辑对提升至视频维度，挖掘密集标注视频片段生成视频训练对，并引入过渡监督以指导编辑过程的动态演化。在模型架构方面，我们发现预训练的文本到视频模型本身具备编辑潜力，从而启发了简化设计思路：仅需通过序列拼接进行条件控制并配合轻量级LoRA微调，即可训练出高性能模型。在控制机制上，我们通过统一的单掩码机制实现时空协同控制，并支持可选参考图像输入。总体而言，EasyV2V支持灵活输入组合（如视频+文本、视频+掩码+文本、视频+掩码+参考图像+文本），在视频编辑效果上达到当前最优水平，超越了同期学术成果与商业系统。项目主页：https://snap-research.github.io/easyv2v/",
    "url": "https://huggingface.co/papers/2512.16920",
    "arxiv_url": "https://arxiv.org/abs/2512.16920"
  },
  {
    "title": "FlashPortrait: 6x Faster Infinite Portrait Animation with Adaptive Latent Prediction",
    "summary": "Current diffusion-based acceleration methods for long-portrait animation struggle to ensure identity (ID) consistency. This paper presents FlashPortrait, an end-to-end video diffusion transformer capable of synthesizing ID-preserving, infinite-length videos while achieving up to 6x acceleration in inference speed. In particular, FlashPortrait begins by computing the identity-agnostic facial expression features with an off-the-shelf extractor. It then introduces a Normalized Facial Expression Block to align facial features with diffusion latents by normalizing them with their respective means and variances, thereby improving identity stability in facial modeling. During inference, FlashPortrait adopts a dynamic sliding-window scheme with weighted blending in overlapping areas, ensuring smooth transitions and ID consistency in long animations. In each context window, based on the latent variation rate at particular timesteps and the derivative magnitude ratio among diffusion layers, FlashPortrait utilizes higher-order latent derivatives at the current timestep to directly predict latents at future timesteps, thereby skipping several denoising steps and achieving 6x speed acceleration. Experiments on benchmarks show the effectiveness of FlashPortrait both qualitatively and quantitatively.",
    "translation": "标题：FlashPortrait：基于自适应潜在预测实现6倍加速的无限时长肖像动画生成\n\n摘要：当前基于扩散模型的长时长肖像动画加速方法难以确保身份（ID）一致性。本文提出FlashPortrait，一种端到端的视频扩散Transformer模型，能够合成保持身份一致性的无限时长视频，并在推理速度上实现高达6倍的加速。具体而言，FlashPortrait首先利用现成的特征提取器计算与身份无关的面部表情特征。随后，通过引入归一化面部表情模块，将面部特征与扩散潜在变量对齐，即利用各自的均值与方差进行归一化处理，从而提升面部建模中的身份稳定性。在推理阶段，FlashPortrait采用动态滑动窗口策略，并在重叠区域进行加权融合，以确保长动画中的平滑过渡与身份一致性。在每个上下文窗口中，基于特定时间步的潜在变量变化率以及扩散层间的导数幅值比，FlashPortrait利用当前时间步的高阶潜在导数直接预测未来时间步的潜在变量，从而跳过多个去噪步骤，实现6倍速度加速。基准测试实验表明，FlashPortrait在定性与定量评估中均表现出显著有效性。",
    "url": "https://huggingface.co/papers/2512.16900",
    "arxiv_url": "https://arxiv.org/abs/2512.16900"
  },
  {
    "title": "RePlan: Reasoning-guided Region Planning for Complex Instruction-based Image Editing",
    "summary": "Instruction-based image editing enables natural-language control over visual modifications, yet existing models falter under Instruction-Visual Complexity (IV-Complexity), where intricate instructions meet cluttered or ambiguous scenes. We introduce RePlan (Region-aligned Planning), a plan-then-execute framework that couples a vision-language planner with a diffusion editor. The planner decomposes instructions via step-by-step reasoning and explicitly grounds them to target regions; the editor then applies changes using a training-free attention-region injection mechanism, enabling precise, parallel multi-region edits without iterative inpainting. To strengthen planning, we apply GRPO-based reinforcement learning using 1K instruction-only examples, yielding substantial gains in reasoning fidelity and format reliability. We further present IV-Edit, a benchmark focused on fine-grained grounding and knowledge-intensive edits. Across IV-Complex settings, RePlan consistently outperforms strong baselines trained on far larger datasets, improving regional precision and overall fidelity. Our project page: https://replan-iv-edit.github.io",
    "translation": "标题：RePlan：基于推理引导的区域规划用于复杂指令图像编辑\n\n摘要：基于指令的图像编辑技术实现了对视觉修改的自然语言控制，但现有模型在指令-视觉复杂性（IV-Complexity）场景下表现不佳——即当复杂指令遇到杂乱或模糊的图像内容时。本文提出RePlan（区域对齐规划），一种“先规划后执行”的框架，通过结合视觉语言规划器与扩散编辑模型实现编辑任务。规划器通过逐步推理分解指令，并将其显式定位至目标区域；编辑模型随后采用无需训练的注意力区域注入机制实施修改，从而实现无需迭代修复的精准、并行多区域编辑。为增强规划能力，我们基于GRPO强化学习方法，仅使用1,000条纯指令样本进行训练，显著提升了推理准确性与格式可靠性。我们还提出了IV-Edit基准数据集，专注于细粒度区域定位与知识密集型编辑任务。在多种IV-Complex场景下的实验表明，RePlan在区域精度与整体保真度上均优于使用更大规模数据集训练的基线模型，展现出优越性能。项目页面：https://replan-iv-edit.github.io",
    "url": "https://huggingface.co/papers/2512.16864",
    "arxiv_url": "https://arxiv.org/abs/2512.16864"
  },
  {
    "title": "VenusBench-GD: A Comprehensive Multi-Platform GUI Benchmark for Diverse Grounding Tasks",
    "summary": "GUI grounding is a critical component in building capable GUI agents. However, existing grounding benchmarks suffer from significant limitations: they either provide insufficient data volume and narrow domain coverage, or focus excessively on a single platform and require highly specialized domain knowledge. In this work, we present VenusBench-GD, a comprehensive, bilingual benchmark for GUI grounding that spans multiple platforms, enabling hierarchical evaluation for real-word applications. VenusBench-GD contributes as follows: (i) we introduce a large-scale, cross-platform benchmark with extensive coverage of applications, diverse UI elements, and rich annotated data, (ii) we establish a high-quality data construction pipeline for grounding tasks, achieving higher annotation accuracy than existing benchmarks, and (iii) we extend the scope of element grounding by proposing a hierarchical task taxonomy that divides grounding into basic and advanced categories, encompassing six distinct subtasks designed to evaluate models from complementary perspectives. Our experimental findings reveal critical insights: general-purpose multimodal models now match or even surpass specialized GUI models on basic grounding tasks. In contrast, advanced tasks, still favor GUI-specialized models, though they exhibit significant overfitting and poor robustness. These results underscore the necessity of comprehensive, multi-tiered evaluation frameworks.",
    "translation": "标题：VenusBench-GD：面向多样化落地任务的多平台综合图形用户界面基准测试\n\n摘要：图形用户界面（GUI）落地是构建高效能GUI智能体的关键组成部分。然而，现有的落地基准测试存在显著局限性：它们要么数据量不足且领域覆盖狭窄，要么过度聚焦单一平台并需要高度专业化的领域知识。本研究提出VenusBench-GD——一个跨越多平台的双语GUI落地综合基准测试，支持面向实际应用场景的层次化评估。本基准测试的贡献包括：（一）构建了大规模跨平台基准数据集，涵盖广泛的应用类型、多样化的UI元素及丰富的标注数据；（二）建立了针对落地任务的高质量数据构建流程，其标注精度超越现有基准测试；（三）通过提出层次化任务分类体系拓展了元素落地的评估维度，将落地任务划分为基础与高级两大类别，涵盖六个设计互补的子任务以多角度评估模型性能。实验结果表明：通用多模态模型在基础落地任务上已媲美甚至超越专用GUI模型，而高级任务仍更适配GUI专用模型，但后者存在明显过拟合与鲁棒性不足的问题。这些发现凸显了建立全面多层次评估框架的必要性。",
    "url": "https://huggingface.co/papers/2512.16501",
    "arxiv_url": "https://arxiv.org/abs/2512.16501"
  },
  {
    "title": "ModelTables: A Corpus of Tables about Models",
    "summary": "We present ModelTables, a benchmark of tables in Model Lakes that captures the structured semantics of performance and configuration tables often overlooked by text only retrieval. The corpus is built from Hugging Face model cards, GitHub READMEs, and referenced papers, linking each table to its surrounding model and publication context. Compared with open data lake tables, model tables are smaller yet exhibit denser inter table relationships, reflecting tightly coupled model and benchmark evolution. The current release covers over 60K models and 90K tables. To evaluate model and table relatedness, we construct a multi source ground truth using three complementary signals: (1) paper citation links, (2) explicit model card links and inheritance, and (3) shared training datasets. We present one extensive empirical use case for the benchmark which is table search. We compare canonical Data Lake search operators (unionable, joinable, keyword) and Information Retrieval baselines (dense, sparse, hybrid retrieval) on this benchmark. Union based semantic table retrieval attains 54.8 % P@1 overall (54.6 % on citation, 31.3 % on inheritance, 30.6 % on shared dataset signals); table based dense retrieval reaches 66.5 % P@1, and metadata hybrid retrieval achieves 54.1 %. This evaluation indicates clear room for developing better table search methods. By releasing ModelTables and its creation protocol, we provide the first large scale benchmark of structured data describing AI model. Our use case of table discovery in Model Lakes, provides intuition and evidence for developing more accurate semantic retrieval, structured comparison, and principled organization of structured model knowledge. Source code, data, and other artifacts have been made available at https://github.com/RJMillerLab/ModelTables.",
    "translation": "标题：ModelTables：关于模型的表格语料库\n\n摘要：本文提出ModelTables，这是一个针对模型湖中表格的基准数据集，旨在捕捉性能与配置表格的结构化语义信息，这些信息在纯文本检索中常被忽视。该语料库基于Hugging Face模型卡片、GitHub README文档及相关论文构建，将每个表格与其对应的模型及出版背景进行关联。与开放数据湖中的表格相比，模型表格规模较小但表现出更密集的表格间关联，反映了模型与基准测试紧密耦合的演进过程。当前版本涵盖超过6万个模型和9万个表格。为评估模型与表格的相关性，我们通过三种互补信号构建了多源真实标注数据：（1）论文引用关系，（2）显式的模型卡片链接与继承关系，（3）共享的训练数据集。我们以表格搜索为例展示了该基准数据集的一个综合性实证应用案例，比较了经典数据湖搜索操作（可合并、可连接、关键词搜索）与信息检索基线方法（稠密检索、稀疏检索、混合检索）在该基准上的表现。基于合并操作的语义表格检索整体P@1达到54.8%（引用关系54.6%，继承关系31.3%，共享数据集30.6%）；基于表格的稠密检索达到66.5% P@1；元数据混合检索达到54.1%。评估结果表明现有表格搜索方法仍有明显改进空间。通过发布ModelTables及其构建流程，我们首次提供了描述人工智能模型的大规模结构化数据基准。我们在模型湖中开展的表格发现应用案例，为开发更精确的语义检索、结构化比较及结构化模型知识的系统化组织提供了理论依据与实践证明。相关源代码、数据及其他材料已公开于https://github.com/RJMillerLab/ModelTables。",
    "url": "https://huggingface.co/papers/2512.16106",
    "arxiv_url": "https://arxiv.org/abs/2512.16106"
  },
  {
    "title": "Hearing to Translate: The Effectiveness of Speech Modality Integration into LLMs",
    "summary": "As Large Language Models (LLMs) expand beyond text, integrating speech as a native modality has given rise to SpeechLLMs, which aim to translate spoken language directly, thereby bypassing traditional transcription-based pipelines. Whether this integration improves speech-to-text translation quality over established cascaded architectures, however, remains an open question. We present Hearing to Translate, the first comprehensive test suite rigorously benchmarking 5 state-of-the-art SpeechLLMs against 16 strong direct and cascade systems that couple leading speech foundation models (SFM), with multilingual LLMs. Our analysis spans 16 benchmarks, 13 language pairs, and 9 challenging conditions, including disfluent, noisy, and long-form speech. Across this extensive evaluation, we find that cascaded systems remain the most reliable overall, while current SpeechLLMs only match cascades in selected settings and SFMs lag behind both, highlighting that integrating an LLM, either within the model or in a pipeline, is essential for high-quality speech translation.",
    "translation": "标题：听觉翻译：语音模态集成于大语言模型的有效性研究\n\n摘要：随着大语言模型（LLMs）的应用范围超越纯文本领域，将语音作为原生模态进行集成催生了语音大语言模型（SpeechLLMs），其旨在直接翻译口语，从而绕过传统的基于转写的处理流程。然而，这种集成是否比成熟的级联架构更能提升语音到文本的翻译质量，仍是一个悬而未决的问题。本研究提出“听觉翻译”评估框架，首次构建了全面的测试集，将5个前沿的SpeechLLMs与16个强大的直接及级联系统（结合领先的语音基础模型与多语言大语言模型）进行严格基准比较。我们的分析涵盖16个基准数据集、13种语言对以及9种具有挑战性的条件（包括不流畅语音、含噪语音和长篇幅语音）。在这项广泛评估中，我们发现级联系统整体上仍是最可靠的方案，而当前的SpeechLLMs仅在特定场景下与级联系统表现相当，语音基础模型则落后于两者。这突出表明，无论是通过模型内部集成还是构建处理流程，融入大语言模型对于实现高质量的语音翻译至关重要。",
    "url": "https://huggingface.co/papers/2512.16378",
    "arxiv_url": "https://arxiv.org/abs/2512.16378"
  },
  {
    "title": "Differences That Matter: Auditing Models for Capability Gap Discovery and Rectification",
    "summary": "Conventional evaluation methods for multimodal LLMs (MLLMs) lack interpretability and are often insufficient to fully disclose significant capability gaps across models. To address this, we introduce AuditDM, an automated framework that actively discovers and rectifies MLLM failure modes by auditing their divergence. AuditDM fine-tunes an MLLM as an auditor via reinforcement learning to generate challenging questions and counterfactual images that maximize disagreement among target models. Once trained, the auditor uncovers diverse, interpretable exemplars that reveal model weaknesses and serve as annotation-free data for rectification. When applied to SoTA models like Gemma-3 and PaliGemma-2, AuditDM discovers more than 20 distinct failure types. Fine-tuning on these discoveries consistently improves all models across 16 benchmarks, and enables a 3B model to surpass its 28B counterpart. Our results suggest that as data scaling hits diminishing returns, targeted model auditing offers an effective path to model diagnosis and improvement.",
    "translation": "标题：关键差异：面向能力差距发现与修正的模型审计方法\n\n摘要：传统多模态大语言模型（MLLMs）评估方法缺乏可解释性，往往难以充分揭示模型间显著的能力差距。为此，我们提出AuditDM——一种通过主动审计模型分歧来发现并修正MLLM失效模式的自动化框架。该框架通过强化学习微调MLLM作为审计器，使其生成能最大化目标模型间分歧的挑战性问题和反事实图像。训练完成后，审计器可挖掘出多样化的可解释示例，这些示例既能揭示模型缺陷，又可作为无需标注的修正数据。在Gemma-3和PaliGemma-2等前沿模型上的实验表明，AuditDM成功识别出20余种不同的失效类型。基于这些发现进行微调后，所有模型在16个基准测试中均获得持续提升，甚至使30亿参数模型超越其280亿参数的对照模型。研究结果表明，当数据扩展效益递减时，定向模型审计为模型诊断与优化提供了有效路径。",
    "url": "https://huggingface.co/papers/2512.16921",
    "arxiv_url": "https://arxiv.org/abs/2512.16921"
  },
  {
    "title": "Insight Miner: A Time Series Analysis Dataset for Cross-Domain Alignment with Natural Language",
    "summary": "Time-series data is critical across many scientific and industrial domains, including environmental analysis, agriculture, transportation, and finance. However, mining insights from this data typically requires deep domain expertise, a process that is both time-consuming and labor-intensive. In this paper, we propose Insight Miner, a large-scale multimodal model (LMM) designed to generate high-quality, comprehensive time-series descriptions enriched with domain-specific knowledge. To facilitate this, we introduce TS-InsightsAvailable at \\href{https://huggingface.co/datasets/zhykoties/time-series-language-alignment{https://huggingface.co/datasets/zhykoties/time-series-language-alignment}.}, the first general-domain dataset for time series and language alignment. TS-Insights contains 100k time-series windows sampled from 20 forecasting datasets. We construct this dataset using a novel agentic workflow, where we use statistical tools to extract features from raw time series before synthesizing them into coherent trend descriptions with GPT-4. Following instruction tuning on TS-Insights, Insight Miner outperforms state-of-the-art multimodal models, such as LLaVA liu2023llava and GPT-4, in generating time-series descriptions and insights. Our findings suggest a promising direction for leveraging LMMs in time series analysis, and serve as a foundational step toward enabling LLMs to interpret time series as a native input modality.",
    "translation": "标题：Insight Miner：面向跨领域自然语言对齐的时间序列分析数据集\n\n摘要：时间序列数据在环境分析、农业、交通和金融等诸多科学与工业领域中至关重要。然而，从这类数据中挖掘洞见通常需要深厚的领域专业知识，这一过程既耗时又费力。本文提出Insight Miner，一种旨在生成高质量、综合性时间序列描述的大规模多模态模型，其描述内容融合了领域专业知识。为支持该模型，我们推出了TS-Insights（可通过\\href{https://huggingface.co/datasets/zhykoties/time-series-language-alignment}{https://huggingface.co/datasets/zhykoties/time-series-language-alignment}获取），这是首个面向时间序列与语言对齐的通用领域数据集。TS-Insights包含从20个预测数据集中采样的10万个时间序列窗口，其构建采用了一种新型智能体工作流程：先通过统计工具从原始时间序列中提取特征，再使用GPT-4将其合成为连贯的趋势描述。基于TS-Insights进行指令微调后，Insight Miner在生成时间序列描述与洞见方面超越了LLaVA（liu2023llava）和GPT-4等先进多模态模型。我们的研究结果为利用多模态模型进行时间序列分析指明了可行方向，并为使大语言模型将时间序列作为原生输入模态进行解读奠定了重要基础。",
    "url": "https://huggingface.co/papers/2512.11251",
    "arxiv_url": "https://arxiv.org/abs/2512.11251"
  },
  {
    "title": "Make-It-Poseable: Feed-forward Latent Posing Model for 3D Humanoid Character Animation",
    "summary": "Posing 3D characters is a fundamental task in computer graphics and vision. However, existing methods like auto-rigging and pose-conditioned generation often struggle with challenges such as inaccurate skinning weight prediction, topological imperfections, and poor pose conformance, limiting their robustness and generalizability. To overcome these limitations, we introduce Make-It-Poseable, a novel feed-forward framework that reformulates character posing as a latent-space transformation problem. Instead of deforming mesh vertices as in traditional pipelines, our method reconstructs the character in new poses by directly manipulating its latent representation. At the core of our method is a latent posing transformer that manipulates shape tokens based on skeletal motion. This process is facilitated by a dense pose representation for precise control. To ensure high-fidelity geometry and accommodate topological changes, we also introduce a latent-space supervision strategy and an adaptive completion module. Our method demonstrates superior performance in posing quality. It also naturally extends to 3D editing applications like part replacement and refinement.",
    "translation": "标题：Make-It-Poseable：面向三维类人角色动画的前馈式潜在姿态生成模型\n\n摘要：三维角色姿态生成是计算机图形学与视觉领域的一项基础任务。然而，现有方法如自动绑定与姿态条件生成常面临蒙皮权重预测不准确、拓扑结构缺陷及姿态贴合度差等挑战，限制了其鲁棒性与泛化能力。为克服这些局限，本文提出Make-It-Poseable——一种创新的前馈式框架，将角色姿态生成重新定义为潜在空间变换问题。与传统流程中直接变形网格顶点不同，本方法通过操纵角色的潜在表征直接重建新姿态下的角色模型。其核心是一个基于骨骼运动操纵形状标记的潜在姿态变换器，该过程通过密集姿态表征实现精确控制。为确保高保真几何质量并适应拓扑结构变化，我们还引入了潜在空间监督策略与自适应补全模块。实验表明，本方法在姿态生成质量上具有显著优势，并能自然扩展到部件替换与精细化等三维编辑任务中。",
    "url": "https://huggingface.co/papers/2512.16767",
    "arxiv_url": "https://arxiv.org/abs/2512.16767"
  },
  {
    "title": "FrameDiffuser: G-Buffer-Conditioned Diffusion for Neural Forward Frame Rendering",
    "summary": "Neural rendering for interactive applications requires translating geometric and material properties (G-buffer) to photorealistic images with realistic lighting on a frame-by-frame basis. While recent diffusion-based approaches show promise for G-buffer-conditioned image synthesis, they face critical limitations: single-image models like RGBX generate frames independently without temporal consistency, while video models like DiffusionRenderer are too computationally expensive for most consumer gaming sets ups and require complete sequences upfront, making them unsuitable for interactive applications where future frames depend on user input. We introduce FrameDiffuser, an autoregressive neural rendering framework that generates temporally consistent, photorealistic frames by conditioning on G-buffer data and the models own previous output. After an initial frame, FrameDiffuser operates purely on incoming G-buffer data, comprising geometry, materials, and surface properties, while using its previously generated frame for temporal guidance, maintaining stable, temporal consistent generation over hundreds to thousands of frames. Our dual-conditioning architecture combines ControlNet for structural guidance with ControlLoRA for temporal coherence. A three-stage training strategy enables stable autoregressive generation. We specialize our model to individual environments, prioritizing consistency and inference speed over broad generalization, demonstrating that environment-specific training achieves superior photorealistic quality with accurate lighting, shadows, and reflections compared to generalized approaches.",
    "translation": "标题：FrameDiffuser：基于G-Buffer条件扩散的神经前向帧渲染\n\n摘要：面向交互应用的神经渲染需要逐帧将几何与材质属性（G-Buffer）转换为具有真实光照效果的逼真图像。尽管近期基于扩散模型的方法在G-Buffer条件图像合成中展现出潜力，但仍面临关键局限：如RGBX等单帧模型独立生成各帧而缺乏时序一致性；而DiffusionRenderer等视频模型计算开销过大，难以适配多数消费级游戏设备，且需预先获取完整序列，无法适应未来帧依赖用户输入的交互场景。本文提出FrameDiffuser——一种自回归神经渲染框架，通过融合G-Buffer数据与模型自身历史输出来生成时序一致、视觉逼真的帧序列。在生成首帧后，FrameDiffuser仅依赖输入的G-Buffer数据（包含几何结构、材质与表面属性），同时利用已生成的前一帧进行时序引导，从而在数百至数千帧范围内保持稳定且时序一致的生成效果。我们的双条件架构结合了ControlNet的结构引导与ControlLoRA的时序连贯性增强机制，并通过三阶段训练策略实现稳定的自回归生成。该模型针对特定场景环境进行专门化训练，在保持广泛泛化能力的同时，优先保障时序一致性与推理速度。实验表明，相较于通用化方法，场景专用训练能在光照、阴影与反射等视觉要素上实现更优越的逼真度与物理准确性。",
    "url": "https://huggingface.co/papers/2512.16670",
    "arxiv_url": "https://arxiv.org/abs/2512.16670"
  },
  {
    "title": "Trainable Log-linear Sparse Attention for Efficient Diffusion Transformers",
    "summary": "Diffusion Transformers (DiTs) set the state of the art in visual generation, yet their quadratic self-attention cost fundamentally limits scaling to long token sequences. Recent Top-K sparse attention approaches reduce the computation of DiTs by compressing tokens into block-wise representation and selecting a small set of relevant key blocks, but still suffer from (i) quadratic selection cost on compressed tokens and (ii) increasing K required to maintain model quality as sequences grow. We identify that their inefficiency is due to the single-level design, as a single coarse level is insufficient to represent the global structure. In this paper, we introduce Log-linear Sparse Attention (LLSA), a trainable sparse attention mechanism for extremely long token sequences that reduces both selection and attention costs from quadratic to log-linear complexity by utilizing a hierarchical structure. LLSA performs hierarchical Top-K selection, progressively adopting sparse Top-K selection with the indices found at the previous level, and introduces a Hierarchical KV Enrichment mechanism that preserves global context while using fewer tokens of different granularity during attention computation. To support efficient training, we develop a high-performance GPU implementation that uses only sparse indices for both the forward and backward passes, eliminating the need for dense attention masks. We evaluate LLSA on high-resolution pixel-space image generation without using patchification and VAE encoding. LLSA accelerates attention inference by 28.27x and DiT training by 6.09x on 256x256 pixel token sequences, while maintaining generation quality. The results demonstrate that LLSA offers a promising direction for training long-sequence DiTs efficiently. Code is available at: https://github.com/SingleZombie/LLSA",
    "translation": "标题：可训练的对数线性稀疏注意力机制用于高效扩散变换器\n\n摘要：扩散变换器（DiTs）在视觉生成领域取得了最先进的性能，但其二次方的自注意力计算成本从根本上限制了其向长令牌序列的扩展。近期提出的Top-K稀疏注意力方法通过将令牌压缩为块级表示并选择少量相关关键块来减少DiTs的计算量，但仍存在以下问题：（1）在压缩令牌上仍需二次方选择成本；（2）随着序列增长，为保持模型质量需要不断增加K值。我们发现其效率低下的根源在于单层级设计，因为单一粗粒度层级不足以有效表征全局结构。本文提出对数线性稀疏注意力（LLSA），这是一种针对极长令牌序列的可训练稀疏注意力机制，通过利用层次化结构将选择成本和注意力成本从二次方降低至对数线性复杂度。LLSA执行层次化Top-K选择，基于前一层级发现的索引逐步采用稀疏Top-K选择，并引入层次化键值增强机制，在注意力计算过程中使用更少的不同粒度令牌同时保持全局上下文信息。为支持高效训练，我们开发了高性能GPU实现方案，在前向和反向传播中仅使用稀疏索引，无需构建稠密注意力掩码。我们在不使用分块化和VAE编码的高分辨率像素空间图像生成任务上评估LLSA。在256×256像素令牌序列上，LLSA将注意力推理速度提升28.27倍，将DiT训练速度提升6.09倍，同时保持生成质量。实验结果表明，LLSA为高效训练长序列DiTs提供了有前景的技术路径。代码已开源：https://github.com/SingleZombie/LLSA",
    "url": "https://huggingface.co/papers/2512.16615",
    "arxiv_url": "https://arxiv.org/abs/2512.16615"
  },
  {
    "title": "Coupled Variational Reinforcement Learning for Language Model General Reasoning",
    "summary": "While reinforcement learning have achieved impressive progress in language model reasoning, they are constrained by the requirement for verifiable rewards. Recent verifier-free RL methods address this limitation by utilizing the intrinsic probabilities of LLMs generating reference answers as reward signals. However, these approaches typically sample reasoning traces conditioned only on the question. This design decouples reasoning-trace sampling from answer information, leading to inefficient exploration and incoherence between traces and final answers. In this paper, we propose \\b{Coupled Variational Reinforcement Learning} (CoVRL), which bridges variational inference and reinforcement learning by coupling prior and posterior distributions through a hybrid sampling strategy. By constructing and optimizing a composite distribution that integrates these two distributions, CoVRL enables efficient exploration while preserving strong thought-answer coherence. Extensive experiments on mathematical and general reasoning benchmarks show that CoVRL improves performance by 12.4\\% over the base model and achieves an additional 2.3\\% improvement over strong state-of-the-art verifier-free RL baselines, providing a principled framework for enhancing the general reasoning capabilities of language models.",
    "translation": "标题：面向语言模型通用推理的耦合变分强化学习方法\n\n摘要：尽管强化学习在语言模型推理领域取得了显著进展，但其发展仍受限于可验证奖励信号的需求。近期出现的免验证器强化学习方法通过利用大语言模型生成参考答案的内在概率作为奖励信号，缓解了这一限制。然而，这些方法通常仅基于问题条件对推理轨迹进行采样。这种设计导致推理轨迹采样与答案信息解耦，从而引发低效探索以及轨迹与最终答案间的不一致问题。本文提出耦合变分强化学习方法，该方法通过混合采样策略耦合先验分布与后验分布，构建了变分推断与强化学习之间的桥梁。通过构建并优化融合这两种分布的复合分布，该方法在保持强思维-答案一致性的同时实现了高效探索。在数学推理与通用推理基准测试上的大量实验表明，该方法相比基线模型性能提升12.4%，较当前先进的免验证器强化学习基线方法额外提升2.3%，为增强语言模型的通用推理能力提供了理论严谨的框架。",
    "url": "https://huggingface.co/papers/2512.12576",
    "arxiv_url": "https://arxiv.org/abs/2512.12576"
  },
  {
    "title": "MomaGraph: State-Aware Unified Scene Graphs with Vision-Language Model for Embodied Task Planning",
    "summary": "Mobile manipulators in households must both navigate and manipulate. This requires a compact, semantically rich scene representation that captures where objects are, how they function, and which parts are actionable. Scene graphs are a natural choice, yet prior work often separates spatial and functional relations, treats scenes as static snapshots without object states or temporal updates, and overlooks information most relevant for accomplishing the current task. To address these limitations, we introduce MomaGraph, a unified scene representation for embodied agents that integrates spatial-functional relationships and part-level interactive elements. However, advancing such a representation requires both suitable data and rigorous evaluation, which have been largely missing. We thus contribute MomaGraph-Scenes, the first large-scale dataset of richly annotated, task-driven scene graphs in household environments, along with MomaGraph-Bench, a systematic evaluation suite spanning six reasoning capabilities from high-level planning to fine-grained scene understanding. Built upon this foundation, we further develop MomaGraph-R1, a 7B vision-language model trained with reinforcement learning on MomaGraph-Scenes. MomaGraph-R1 predicts task-oriented scene graphs and serves as a zero-shot task planner under a Graph-then-Plan framework. Extensive experiments demonstrate that our model achieves state-of-the-art results among open-source models, reaching 71.6% accuracy on the benchmark (+11.4% over the best baseline), while generalizing across public benchmarks and transferring effectively to real-robot experiments.",
    "translation": "标题：MomaGraph：面向具身任务规划的视觉语言模型状态感知统一场景图\n\n摘要：家庭环境中的移动机械臂需同时具备导航与操作能力，这要求一种紧凑且语义丰富的场景表征，能够捕捉物体位置、功能属性及可操作部件。场景图虽为自然选择，但现有研究常将空间关系与功能关系割裂，将场景视为缺乏物体状态或时序更新的静态快照，并忽视与当前任务最相关的信息。为突破这些局限，我们提出MomaGraph——一种面向具身智能体的统一场景表征，整合了空间-功能关系与部件级交互元素。然而，推进此类表征需要适配的数据集与严谨的评估体系，而这两者长期缺失。为此，我们贡献了MomaGraph-Scenes（首个大规模家庭环境任务驱动精细标注场景图数据集）与MomaGraph-Bench（涵盖从高层规划到细粒度场景理解六类推理能力的系统化评估套件）。基于此基础，我们进一步开发了MomaGraph-R1——一个通过强化学习在MomaGraph-Scenes上训练的70亿参数视觉语言模型。该模型能够预测任务导向场景图，并在“先构图后规划”框架下实现零样本任务规划。大量实验表明，我们的模型在开源模型中达到最先进水平：在基准测试中取得71.6%的准确率（较最佳基线提升11.4%），同时能泛化至公共基准测试，并有效迁移至真实机器人实验。",
    "url": "https://huggingface.co/papers/2512.16909",
    "arxiv_url": "https://arxiv.org/abs/2512.16909"
  },
  {
    "title": "TabReX : Tabular Referenceless eXplainable Evaluation",
    "summary": "Evaluating the quality of tables generated by large language models (LLMs) remains an open challenge: existing metrics either flatten tables into text, ignoring structure, or rely on fixed references that limit generalization. We present TabReX, a reference-less, property-driven framework for evaluating tabular generation via graph-based reasoning. TabReX converts both source text and generated tables into canonical knowledge graphs, aligns them through an LLM-guided matching process, and computes interpretable, rubric-aware scores that quantify structural and factual fidelity. The resulting metric provides controllable trade-offs between sensitivity and specificity, yielding human-aligned judgments and cell-level error traces. To systematically asses metric robustness, we introduce TabReX-Bench, a large-scale benchmark spanning six domains and twelve planner-driven perturbation types across three difficulty tiers. Empirical results show that TabReX achieves the highest correlation with expert rankings, remains stable under harder perturbations, and enables fine-grained model-vs-prompt analysis establishing a new paradigm for trustworthy, explainable evaluation of structured generation systems.",
    "translation": "标题：TabReX：一种无参考、可解释的表格生成评估框架\n\n摘要：评估大语言模型（LLM）生成的表格质量仍是一个开放挑战：现有方法或将表格扁平化为文本而忽略其结构，或依赖固定参考从而限制泛化能力。本文提出TabReX，一种基于图推理、无需参考且以属性驱动的表格生成评估框架。TabReX将源文本与生成表格均转换为规范化知识图谱，通过LLM引导的匹配过程进行对齐，并计算可解释的、基于评估量规的分数，以量化结构与事实一致性。该评估指标可在敏感度与特异性之间实现可控权衡，提供与人工判断一致的结果及单元格级别的错误追溯。为系统评估指标的鲁棒性，我们构建了TabReX-Bench大规模基准数据集，涵盖六个领域、十二种规划驱动的扰动类型及三个难度层级。实验结果表明，TabReX与专家评价的相关性最高，在强扰动下保持稳定，并能支持细粒度的模型与提示词对比分析，为结构化生成系统的可信可解释评估建立了新范式。",
    "url": "https://huggingface.co/papers/2512.15907",
    "arxiv_url": "https://arxiv.org/abs/2512.15907"
  },
  {
    "title": "Vibe Spaces for Creatively Connecting and Expressing Visual Concepts",
    "summary": "Creating new visual concepts often requires connecting distinct ideas through their most relevant shared attributes -- their vibe. We introduce Vibe Blending, a novel task for generating coherent and meaningful hybrids that reveals these shared attributes between images. Achieving such blends is challenging for current methods, which struggle to identify and traverse nonlinear paths linking distant concepts in latent space. We propose Vibe Space, a hierarchical graph manifold that learns low-dimensional geodesics in feature spaces like CLIP, enabling smooth and semantically consistent transitions between concepts. To evaluate creative quality, we design a cognitively inspired framework combining human judgments, LLM reasoning, and a geometric path-based difficulty score. We find that Vibe Space produces blends that humans consistently rate as more creative and coherent than current methods.",
    "translation": "标题：用于创造性连接与表达视觉概念的“氛围空间”\n\n摘要：创造新的视觉概念通常需要通过最相关的共享属性——即其“氛围”——来连接不同的想法。本文提出“氛围融合”这一新颖任务，旨在生成连贯且有意义的混合图像，以揭示图像间的共享属性。现有方法在实现此类融合时面临挑战，难以识别并遍历潜在空间中连接远距离概念的非线性路径。我们提出“氛围空间”——一种层次化图流形结构，能够在CLIP等特征空间中学习低维测地线，从而实现概念间平滑且语义一致的过渡。为评估创意质量，我们设计了一个融合人类判断、大语言模型推理与基于几何路径的难度评分的认知启发式评估框架。实验表明，相较于现有方法，“氛围空间”生成的融合结果被人类评价者一致认为更具创造性与连贯性。",
    "url": "https://huggingface.co/papers/2512.14884",
    "arxiv_url": "https://arxiv.org/abs/2512.14884"
  },
  {
    "title": "Reasoning Within the Mind: Dynamic Multimodal Interleaving in Latent Space",
    "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have significantly enhanced cross-modal understanding and reasoning by incorporating Chain-of-Thought (CoT) reasoning in the semantic space. Building upon this, recent studies extend the CoT mechanism to the visual modality, enabling models to integrate visual information during reasoning through external tools or explicit image generation. However, these methods remain dependent on explicit step-by-step reasoning, unstable perception-reasoning interaction and notable computational overhead. Inspired by human cognition, we posit that thinking unfolds not linearly but through the dynamic interleaving of reasoning and perception within the mind. Motivated by this perspective, we propose DMLR, a test-time Dynamic Multimodal Latent Reasoning framework that employs confidence-guided latent policy gradient optimization to refine latent think tokens for in-depth reasoning. Furthermore, a Dynamic Visual Injection Strategy is introduced, which retrieves the most relevant visual features at each latent think token and updates the set of best visual patches. The updated patches are then injected into latent think token to achieve dynamic visual-textual interleaving. Experiments across seven multimodal reasoning benchmarks and various model architectures demonstrate that DMLR significantly improves reasoning and perception performance while maintaining high inference efficiency.",
    "translation": "标题：心智内推理：潜在空间中的动态多模态交错\n\n摘要：多模态大语言模型（MLLMs）的最新进展通过在语义空间中引入思维链推理，显著提升了跨模态理解与推理能力。在此基础上，近期研究将思维链机制扩展至视觉模态，使模型能够借助外部工具或显式图像生成在推理过程中整合视觉信息。然而，这些方法仍依赖于显式的分步推理，存在感知-推理交互不稳定及显著计算开销的问题。受人类认知机制启发，我们认为思维并非线性展开，而是通过心智中推理与感知的动态交错进行。基于这一视角，我们提出DMLR——一种测试时动态多模态潜在推理框架，该框架采用置信度引导的潜在策略梯度优化方法，通过精炼潜在思维标记实现深度推理。此外，我们引入了动态视觉注入策略，该策略在每个潜在思维标记处检索最相关的视觉特征并更新最优视觉片段集合，随后将更新后的片段注入潜在思维标记，实现动态的视觉-文本交错。在七个多模态推理基准测试及多种模型架构上的实验表明，DMLR在保持高效推理的同时，显著提升了模型的推理与感知性能。",
    "url": "https://huggingface.co/papers/2512.12623",
    "arxiv_url": "https://arxiv.org/abs/2512.12623"
  },
  {
    "title": "Bidirectional Normalizing Flow: From Data to Noise and Back",
    "summary": "Normalizing Flows (NFs) have been established as a principled framework for generative modeling. Standard NFs consist of a forward process and a reverse process: the forward process maps data to noise, while the reverse process generates samples by inverting it. Typical NF forward transformations are constrained by explicit invertibility, ensuring that the reverse process can serve as their exact analytic inverse. Recent developments in TARFlow and its variants have revitalized NF methods by combining Transformers and autoregressive flows, but have also exposed causal decoding as a major bottleneck. In this work, we introduce Bidirectional Normalizing Flow (BiFlow), a framework that removes the need for an exact analytic inverse. BiFlow learns a reverse model that approximates the underlying noise-to-data inverse mapping, enabling more flexible loss functions and architectures. Experiments on ImageNet demonstrate that BiFlow, compared to its causal decoding counterpart, improves generation quality while accelerating sampling by up to two orders of magnitude. BiFlow yields state-of-the-art results among NF-based methods and competitive performance among single-evaluation (\"1-NFE\") methods. Following recent encouraging progress on NFs, we hope our work will draw further attention to this classical paradigm.",
    "translation": "标题：双向归一化流：从数据到噪声及其逆过程\n\n摘要：归一化流已发展成为生成建模的理论框架。标准归一化流包含前向过程与反向过程：前向过程将数据映射为噪声，反向过程则通过逆向映射生成样本。传统归一化流的前向变换受显式可逆性约束，确保反向过程能作为其精确解析逆。近期TARFlow及其变体通过结合Transformer与自回归流重振了归一化流方法，但也暴露出因果解码成为主要瓶颈。本研究提出双向归一化流框架，该框架无需依赖精确解析逆。双向归一化流通过学习近似底层噪声到数据逆映射的反向模型，实现了更灵活的损失函数与架构设计。在ImageNet数据集上的实验表明，相较于因果解码方法，双向归一化流在将采样速度提升两个数量级的同时提高了生成质量。该框架在基于归一化流的方法中取得了最优结果，并在单次评估方法中展现出具有竞争力的性能。随着归一化流领域近期取得鼓舞性进展，我们希望本研究能进一步激发对这一经典范式的关注。",
    "url": "https://huggingface.co/papers/2512.10953",
    "arxiv_url": "https://arxiv.org/abs/2512.10953"
  },
  {
    "title": "EmoCaliber: Advancing Reliable Visual Emotion Comprehension via Confidence Verbalization and Calibration",
    "summary": "Visual Emotion Comprehension (VEC) aims to infer sentiment polarities or emotion categories from affective cues embedded in images. In recent years, Multimodal Large Language Models (MLLMs) have established a popular paradigm in VEC, leveraging their generalizability to unify VEC tasks defined under diverse emotion taxonomies. While this paradigm achieves notable success, it typically formulates VEC as a deterministic task, requiring the model to output a single, definitive emotion label for each image. Such a formulation insufficiently accounts for the inherent subjectivity of emotion perception, overlooking alternative interpretations that may be equally plausible to different viewers. To address this limitation, we propose equipping MLLMs with capabilities to verbalize their confidence in emotion predictions. This additional signal provides users with an estimate of both the plausibility of alternative interpretations and the MLLMs' self-assessed competence, thereby enhancing reliability in practice. Building on this insight, we introduce a three-stage training framework that progressively endows with structured reasoning, teaches to verbalize confidence, and calibrates confidence expression, culminating in EmoCaliber, a confidence-aware MLLM for VEC. Through fair and comprehensive evaluations on the unified benchmark VECBench, EmoCaliber demonstrates overall superiority against existing methods in both emotion prediction and confidence estimation. These results validate the effectiveness of our approach and mark a feasible step toward more reliable VEC systems. Project page: https://github.com/wdqqdw/EmoCaliber.",
    "translation": "标题：EmoCaliber：通过置信度言语化与校准推进可靠的视觉情感理解\n\n摘要：视觉情感理解旨在从图像中蕴含的情感线索推断情感极性或情绪类别。近年来，多模态大语言模型凭借其泛化能力，成为视觉情感理解的主流范式，能够统一不同情感分类体系下的任务定义。尽管该范式取得了显著成功，但其通常将视觉情感理解视为确定性任务，要求模型为每张图像输出单一、明确的情感标签。这种设定未能充分考虑情感感知固有的主观性，忽视了对于不同观察者而言可能同样合理的其他解释。为应对这一局限，本文提出为多模态大语言模型赋予情感预测置信度言语化的能力。这一附加信号既为用户提供了替代性解释的合理性评估，也揭示了模型对自身能力的评估，从而在实践中增强系统可靠性。基于此洞见，我们提出一个三阶段训练框架：逐步赋予结构化推理能力、教授置信度言语化技巧、校准置信度表达，最终构建出面向视觉情感理解的置信度感知模型EmoCaliber。通过在统一基准测试集VECBench上进行公平全面的评估，EmoCaliber在情感预测与置信度估计两方面均展现出相较于现有方法的整体优越性。这些结果验证了我们方法的有效性，标志着向更可靠的视觉情感理解系统迈出了切实一步。项目页面：https://github.com/wdqqdw/EmoCaliber。",
    "url": "https://huggingface.co/papers/2512.15528",
    "arxiv_url": "https://arxiv.org/abs/2512.15528"
  },
  {
    "title": "Nemotron-Math: Efficient Long-Context Distillation of Mathematical Reasoning from Multi-Mode Supervision",
    "summary": "High-quality mathematical reasoning supervision requires diverse reasoning styles, long-form traces, and effective tool integration, capabilities that existing datasets provide only in limited form. Leveraging the multi-mode generation ability of gpt-oss-120b, we introduce Nemotron-Math, a large-scale mathematical reasoning dataset containing 7.5M solution traces across high, medium, and low reasoning modes, each available both with and without Python tool-integrated reasoning (TIR).\n  The dataset integrates 85K curated AoPS problems with 262K community-sourced StackExchange-Math problems, combining structured competition tasks with diverse real-world mathematical queries. We conduct controlled evaluations to assess the dataset quality.\n  Nemotron-Math consistently outperforms the original OpenMathReasoning on matched AoPS problems. Incorporating StackExchange-Math substantially improves robustness and generalization, especially on HLE-Math, while preserving accuracy on math competition benchmarks.\n  To support efficient long-context training, we develop a sequential bucketed strategy that accelerates 128K context-length fine-tuning by 2--3times without significant accuracy loss. Overall, Nemotron-Math enables state-of-the-art performance, including 100\\% maj@16 accuracy on AIME 2024 and 2025 with Python TIR.",
    "translation": "标题：Nemotron-Math：基于多模态监督的高效长上下文数学推理蒸馏\n\n摘要：高质量的数学推理监督需要多样化的推理风格、长篇幅的解题过程以及有效的工具集成能力，而现有数据集仅能提供有限形式的支持。利用gpt-oss-120b的多模态生成能力，我们提出了Nemotron-Math——一个大规模数学推理数据集，包含750万条涵盖高、中、低三种推理模式的解题过程，每种模式均提供包含与不包含Python工具集成推理（TIR）的版本。该数据集整合了8.5万道精选的AoPS竞赛题与26.2万道社区来源的StackExchange-Math问题，将结构化竞赛任务与多样化的现实数学问题相结合。我们通过受控评估验证了数据集质量：在匹配的AoPS问题上，Nemotron-Math持续优于原始OpenMathReasoning数据集；引入StackExchange-Math数据显著提升了模型的鲁棒性与泛化能力（尤其在HLE-Math基准上），同时保持了数学竞赛基准的准确性。为支持高效的长上下文训练，我们开发了序列分桶策略，使128K上下文长度的微调加速2-3倍且未造成显著精度损失。总体而言，Nemotron-Math实现了最先进的性能表现，在使用Python TIR时于AIME 2024和2025测试中达到100%的maj@16准确率。",
    "url": "https://huggingface.co/papers/2512.15489",
    "arxiv_url": "https://arxiv.org/abs/2512.15489"
  },
  {
    "title": "Sharing State Between Prompts and Programs",
    "summary": "The rise of large language models (LLMs) has introduced a new type of programming: natural language programming. By writing prompts that direct LLMs to perform natural language processing, code generation, reasoning, etc., users are writing code in natural language -- natural language code -- for the LLM to execute.\n  An emerging area of research enables interoperability between natural language code and formal languages such as Python. We present a novel programming abstraction, shared program state, that removes the manual work required to enable interoperability between natural language code and program state. With shared program state, programmers can write natural code that directly writes program variables, computes with program objects, and implements control flow in the program. We present a schema for specifying natural function interfaces that extend programming systems to support natural code and leverage this schema to specify shared program state as a natural function interface.\n  We implement shared program state in the Nightjar programming system. Nightjar enables programmers to write Python programs that contain natural code that shares the Python program state. We show that Nightjar programs achieve comparable or higher task accuracy than manually written implementations (+4-19%), while decreasing the lines of code by 39.6% on average. The tradeoff to using Nightjar is that it may incur runtime overhead (0.4-4.3x runtime of manual implementations).",
    "translation": "标题：提示与程序间的状态共享\n\n摘要：大型语言模型（LLM）的兴起催生了一种新型编程范式：自然语言编程。通过编写提示词来引导LLM执行自然语言处理、代码生成、推理等任务，用户实际上是在用自然语言编写代码——即自然语言代码——供LLM执行。\n\n当前新兴研究领域致力于实现自然语言代码与Python等形式化语言之间的互操作性。本文提出一种新颖的编程抽象概念——共享程序状态，该机制消除了实现自然语言代码与程序状态互操作性所需的手动操作。借助共享程序状态，程序员能够编写直接写入程序变量、使用程序对象进行计算，并在程序中实现控制流的自然语言代码。我们提出一种用于规范自然函数接口的架构，该架构可扩展编程系统以支持自然语言代码，并利用此架构将共享程序状态定义为自然函数接口。\n\n我们在Nightjar编程系统中实现了共享程序状态。Nightjar使程序员能够编写包含自然语言代码的Python程序，这些代码可与Python程序状态直接交互。实验表明，Nightjar程序在任务准确率上达到甚至超越手动编写实现的水平（提升4-19%），同时平均减少39.6%的代码量。使用Nightjar的代价是可能产生运行时开销（达到手动实现运行时间的0.4-4.3倍）。",
    "url": "https://huggingface.co/papers/2512.14805",
    "arxiv_url": "https://arxiv.org/abs/2512.14805"
  },
  {
    "title": "Improving Recursive Transformers with Mixture of LoRAs",
    "summary": "Parameter sharing in recursive transformers reduces model size but collapses layer-wise expressivity. We propose Mixture of LoRAs (MoL), a lightweight conditional-computation mechanism that inserts Low-Rank Adaptation (LoRA) experts inside a shared feed-forward network (FFN). MoL enables token-conditional weight-space modulation of the shared FFN without untying backbone parameters, unlike prior approaches that add fixed or externally attached adapters. We pretrain a modernised recursive architecture, ModernALBERT, integrating rotary embeddings, GeGLU, FlashAttention, and a distillation-based initialisation. Across GLUE, SQuAD-v2, and BEIR, ModernALBERT (50M--120M) achieves state-of-the-art performance among compact models and surpasses larger fully parameterised baselines. We also propose an expert-merging procedure that compresses MoL into a single adapter at inference while preserving accuracy, enabling efficient deployment. Our results show that conditional weight-space modulation effectively restores the expressivity lost under aggressive parameter sharing in recursive transformers.",
    "translation": "标题：基于混合LoRA的递归Transformer性能提升研究\n\n摘要：递归Transformer中的参数共享机制虽能缩减模型规模，却导致层间表达能力退化。本研究提出混合低秩适配器（MoL）方案，通过在共享前馈网络内部嵌入低秩自适应专家模块，构建轻量级条件计算机制。相较于以往采用固定或外挂适配器的方法，MoL能在保持主干参数绑定的前提下实现基于令牌条件的权重空间调制。我们预训练了现代化递归架构ModernALBERT，集成旋转位置编码、GeGLU激活函数、FlashAttention加速机制及基于知识蒸馏的初始化策略。在GLUE、SQuAD-v2和BEIR基准测试中，参数量为50M-120M的ModernALBERT在紧凑模型中达到最优性能，并超越完全参数化的大型基线模型。同时提出专家融合算法，在推理阶段将MoL压缩为单一适配器且保持精度，实现高效部署。实验结果表明，条件权重空间调制能有效恢复递归Transformer在激进参数共享策略下损失的表达能力。",
    "url": "https://huggingface.co/papers/2512.12880",
    "arxiv_url": "https://arxiv.org/abs/2512.12880"
  }
]