[
  {
    "title": "Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows",
    "summary": "Despite advances in scientific AI, a coherent framework for Scientific General Intelligence (SGI)-the ability to autonomously conceive, investigate, and reason across scientific domains-remains lacking. We present an operational SGI definition grounded in the Practical Inquiry Model (PIM: Deliberation, Conception, Action, Perception) and operationalize it via four scientist-aligned tasks: deep research, idea generation, dry/wet experiments, and experimental reasoning. SGI-Bench comprises over 1,000 expert-curated, cross-disciplinary samples inspired by Science's 125 Big Questions, enabling systematic evaluation of state-of-the-art LLMs. Results reveal gaps: low exact match (10--20%) in deep research despite step-level alignment; ideas lacking feasibility and detail; high code executability but low execution result accuracy in dry experiments; low sequence fidelity in wet protocols; and persistent multimodal comparative-reasoning challenges. We further introduce Test-Time Reinforcement Learning (TTRL), which optimizes retrieval-augmented novelty rewards at inference, enhancing hypothesis novelty without reference answer. Together, our PIM-grounded definition, workflow-centric benchmark, and empirical insights establish a foundation for AI systems that genuinely participate in scientific discovery.",
    "translation": "标题：基于科学家对齐工作流程的大语言模型科学通用智能评估\n\n摘要：尽管科学人工智能领域取得进展，但科学通用智能（SGI）——即跨科学领域自主构思、探究与推理的能力——仍缺乏连贯的评估框架。本研究基于实践探究模型（PIM：审思、构思、行动、感知）提出可操作的SGI定义，并通过四项科学家对齐任务进行具象化：深度研究、创意生成、干/湿实验设计与实验推理。我们构建的SGI-Bench包含1000余个受《科学》杂志\"125个重大科学问题\"启发的跨学科专家评审样本，可系统评估前沿大语言模型。评估结果显示多重不足：深度研究任务中步骤对齐度虽高但精确匹配率仅10-20%；生成创意缺乏可行性与细节；干实验代码可执行率高但执行结果准确率低；湿实验协议序列保真度不足；多模态比较推理任务仍存在持续挑战。我们进一步提出推理时强化学习方法，通过在推理阶段优化检索增强的新颖性奖励，在无参考答案条件下提升假设生成的新颖度。本研究通过PIM理论框架、以工作流程为核心的评估体系及实证分析，为真正参与科学发现的人工智能系统奠定了理论基础。",
    "url": "https://huggingface.co/papers/2512.16969",
    "arxiv_url": "https://arxiv.org/abs/2512.16969"
  },
  {
    "title": "PhysBrain: Human Egocentric Data as a Bridge from Vision Language Models to Physical Intelligence",
    "summary": "Robotic generalization relies on physical intelligence: the ability to reason about state changes, contact-rich interactions, and long-horizon planning under egocentric perception and action. However, most VLMs are trained primarily on third-person data, creating a fundamental viewpoint mismatch for humanoid robots. Scaling robot egocentric data collection remains impractical due to high cost and limited diversity, whereas large-scale human egocentric videos offer a scalable alternative that naturally capture rich interaction context and causal structure. The key challenge is to convert raw egocentric videos into structured and reliable embodiment training supervision. Accordingly, we propose an Egocentric2Embodiment translation pipeline that transforms first-person videos into multi-level, schema-driven VQA supervision with enforced evidence grounding and temporal consistency, enabling the construction of the Egocentric2Embodiment dataset (E2E-3M) at scale. An egocentric-aware embodied brain, termed PhysBrain, is obtained by training on the E2E-3M dataset. PhysBrain exhibits substantially improved egocentric understanding, particularly for planning on EgoThink. It provides an egocentric-aware initialization that enables more sample-efficient VLA fine-tuning and higher SimplerEnv success rates (53.9\\%), demonstrating effective transfer from human egocentric supervision to downstream robot control.",
    "translation": "标题：PhysBrain：以人类自我中心数据为桥梁实现从视觉语言模型到物理智能的过渡\n\n摘要：机器人泛化能力依赖于物理智能，即在自我中心感知与行动下进行状态变化推理、密集接触交互以及长时程规划的能力。然而，当前大多数视觉语言模型主要基于第三人称数据进行训练，这导致其人形机器人应用存在根本性的视角错配问题。由于成本高昂与多样性有限，大规模采集机器人自我中心数据仍不现实；而大规模人类自我中心视频则提供了可扩展的替代方案，其天然蕴含丰富的交互语境与因果结构。核心挑战在于如何将原始自我中心视频转化为结构化、可靠的具身训练监督信号。为此，我们提出一种自我中心到具身转换流程，通过强化证据锚定与时间一致性机制，将第一人称视频转化为多层次、模式驱动的视觉问答监督数据，从而规模化构建自我中心到具身数据集（E2E-3M）。基于该数据集训练得到的自我中心感知具身大脑模型（命名为PhysBrain）展现出显著增强的自我中心理解能力，尤其在EgoThink任务中的规划表现突出。该模型提供的自我中心感知初始化权重，能够实现更高效的视觉语言动作模型微调，并在SimplerEnv环境中达到53.9%的成功率，有效验证了从人类自我中心监督到下游机器人控制的知识迁移机制。",
    "url": "https://huggingface.co/papers/2512.16793",
    "arxiv_url": "https://arxiv.org/abs/2512.16793"
  },
  {
    "title": "When Reasoning Meets Its Laws",
    "summary": "Despite the superior performance of Large Reasoning Models (LRMs), their reasoning behaviors are often counterintuitive, leading to suboptimal reasoning capabilities. To theoretically formalize the desired reasoning behaviors, this paper presents the Laws of Reasoning (LoRe), a unified framework that characterizes intrinsic reasoning patterns in LRMs. We first propose compute law with the hypothesis that the reasoning compute should scale linearly with question complexity. Beyond compute, we extend LoRe with a supplementary accuracy law. Since the question complexity is difficult to quantify in practice, we examine these hypotheses by two properties of the laws, monotonicity and compositionality. We therefore introduce LoRe-Bench, a benchmark that systematically measures these two tractable properties for large reasoning models. Evaluation shows that most reasoning models exhibit reasonable monotonicity but lack compositionality. In response, we develop an effective finetuning approach that enforces compute-law compositionality. Extensive empirical studies demonstrate that better compliance with compute laws yields consistently improved reasoning performance on multiple benchmarks, and uncovers synergistic effects across properties and laws. Project page: https://lore-project.github.io/",
    "translation": "标题：当推理遇见其定律\n\n摘要：尽管大型推理模型（LRMs）表现出卓越的性能，但其推理行为往往违背直觉，导致推理能力未能达到最优。为从理论上形式化理想的推理行为，本文提出推理定律（LoRe）这一统一框架，用以刻画大型推理模型的内在推理模式。我们首先提出计算定律，其核心假设是推理计算量应与问题复杂度呈线性比例关系。除计算维度外，我们进一步通过补充的准确率定律扩展推理定律框架。由于问题复杂度在实践中难以量化，我们通过定律的两个可验证属性——单调性与组合性——来检验这些假设。为此，我们构建了LoRe-Bench基准测试集，系统化地衡量大型推理模型在这两个可度量属性上的表现。评估结果表明，大多数推理模型展现出合理的单调性，但缺乏组合性。针对此问题，我们开发了一种有效的微调方法，以强化计算定律的组合性。大量实证研究表明，更好地遵循计算定律能在多个基准测试中持续提升推理性能，并揭示不同属性与定律之间的协同效应。项目页面：https://lore-project.github.io/",
    "url": "https://huggingface.co/papers/2512.17901",
    "arxiv_url": "https://arxiv.org/abs/2512.17901"
  },
  {
    "title": "Seed-Prover 1.5: Mastering Undergraduate-Level Theorem Proving via Learning from Experience",
    "summary": "Large language models have recently made significant progress to generate rigorous mathematical proofs. In contrast, utilizing LLMs for theorem proving in formal languages (such as Lean) remains challenging and computationally expensive, particularly when addressing problems at the undergraduate level and beyond. In this work, we present Seed-Prover 1.5, a formal theorem-proving model trained via large-scale agentic reinforcement learning, alongside an efficient test-time scaling (TTS) workflow. Through extensive interactions with Lean and other tools, the model continuously accumulates experience during the RL process, substantially enhancing the capability and efficiency of formal theorem proving. Furthermore, leveraging recent advancements in natural language proving, our TTS workflow efficiently bridges the gap between natural and formal languages. Compared to state-of-the-art methods, Seed-Prover 1.5 achieves superior performance with a smaller compute budget. It solves 88\\% of PutnamBench (undergraduate-level), 80\\% of Fate-H (graduate-level), and 33\\% of Fate-X (PhD-level) problems. Notably, using our system, we solved 11 out of 12 problems from Putnam 2025 within 9 hours. Our findings suggest that scaling learning from experience, driven by high-quality formal feedback, holds immense potential for the future of formal mathematical reasoning.",
    "translation": "标题：Seed-Prover 1.5：通过经验学习掌握本科水平定理证明\n\n摘要：近期，大语言模型在生成严谨数学证明方面取得了显著进展。然而，利用大语言模型进行形式化语言（如Lean）中的定理证明仍面临挑战且计算成本高昂，尤其是在处理本科及以上难度的问题时。本研究提出了Seed-Prover 1.5，这是一个通过大规模智能体强化学习训练的形式化定理证明模型，并配套设计了高效的测试时扩展工作流程。该模型通过与Lean等工具的广泛交互，在强化学习过程中持续积累经验，显著提升了形式化定理证明的能力与效率。此外，结合自然语言证明领域的最新进展，我们的测试时扩展工作流程有效弥合了自然语言与形式化语言之间的鸿沟。与现有最优方法相比，Seed-Prover 1.5以更少的计算资源实现了更优的性能：其解决了PutnamBench（本科水平）88%的问题、Fate-H（研究生水平）80%的问题以及Fate-X（博士水平）33%的问题。尤为突出的是，使用本系统我们在9小时内解决了2025年普特南数学竞赛12道题目中的11道。我们的研究表明，在高质量形式化反馈驱动下扩展经验学习，对于形式化数学推理的未来发展具有巨大潜力。",
    "url": "https://huggingface.co/papers/2512.17260",
    "arxiv_url": "https://arxiv.org/abs/2512.17260"
  },
  {
    "title": "4D-RGPT: Toward Region-level 4D Understanding via Perceptual Distillation",
    "summary": "Despite advances in Multimodal LLMs (MLLMs), their ability to reason over 3D structures and temporal dynamics remains limited, constrained by weak 4D perception and temporal understanding. Existing 3D and 4D Video Question Answering (VQA) benchmarks also emphasize static scenes and lack region-level prompting. We tackle these issues by introducing: (a) 4D-RGPT, a specialized MLLM designed to capture 4D representations from video inputs with enhanced temporal perception; (b) Perceptual 4D Distillation (P4D), a training framework that transfers 4D representations from a frozen expert model into 4D-RGPT for comprehensive 4D perception; and (c) R4D-Bench, a benchmark for depth-aware dynamic scenes with region-level prompting, built via a hybrid automated and human-verified pipeline. Our 4D-RGPT achieves notable improvements on both existing 4D VQA benchmarks and the proposed R4D-Bench benchmark.",
    "translation": "标题：4D-RGPT：通过感知蒸馏实现区域级四维场景理解\n\n摘要：尽管多模态大语言模型（MLLMs）取得了进展，但其在三维结构与时间动态推理方面的能力仍受限于薄弱的四维感知与时间理解能力。现有的三维与四维视频问答（VQA）基准亦侧重于静态场景，且缺乏区域级提示机制。为应对这些问题，本文提出：（1）4D-RGPT——一种专为从视频输入中捕捉四维表征而设计的MLLM，具备增强的时间感知能力；（2）感知四维蒸馏（P4D）训练框架，通过将冻结专家模型的四维表征迁移至4D-RGPT，实现全面的四维感知；（3）R4D-Bench——一个基于混合自动化与人机协同验证流程构建的、支持区域级提示的深度感知动态场景基准测试集。实验表明，我们的4D-RGPT在现有四维VQA基准及新提出的R4D-Bench基准上均取得了显著性能提升。",
    "url": "https://huggingface.co/papers/2512.17012",
    "arxiv_url": "https://arxiv.org/abs/2512.17012"
  },
  {
    "title": "Both Semantics and Reconstruction Matter: Making Representation Encoders Ready for Text-to-Image Generation and Editing",
    "summary": "Modern Latent Diffusion Models (LDMs) typically operate in low-level Variational Autoencoder (VAE) latent spaces that are primarily optimized for pixel-level reconstruction. To unify vision generation and understanding, a burgeoning trend is to adopt high-dimensional features from representation encoders as generative latents. However, we empirically identify two fundamental obstacles in this paradigm: (1) the discriminative feature space lacks compact regularization, making diffusion models prone to off-manifold latents that lead to inaccurate object structures; and (2) the encoder's inherently weak pixel-level reconstruction hinders the generator from learning accurate fine-grained geometry and texture. In this paper, we propose a systematic framework to adapt understanding-oriented encoder features for generative tasks. We introduce a semantic-pixel reconstruction objective to regularize the latent space, enabling the compression of both semantic information and fine-grained details into a highly compact representation (96 channels with 16x16 spatial downsampling). This design ensures that the latent space remains semantically rich and achieves state-of-the-art image reconstruction, while remaining compact enough for accurate generation. Leveraging this representation, we design a unified Text-to-Image (T2I) and image editing model. Benchmarking against various feature spaces, we demonstrate that our approach achieves state-of-the-art reconstruction, faster convergence, and substantial performance gains in both T2I and editing tasks, validating that representation encoders can be effectively adapted into robust generative components.",
    "translation": "标题：语义与重建并重：让表征编码器胜任文本到图像生成与编辑任务\n\n摘要：现代潜在扩散模型（LDMs）通常在低层变分自编码器（VAE）潜在空间中运行，该空间主要针对像素级重建进行优化。为统一视觉生成与理解任务，新兴趋势是采用表征编码器提取的高维特征作为生成潜变量。然而，我们通过实验发现该范式存在两个根本性障碍：（1）判别性特征空间缺乏紧凑正则化，导致扩散模型易受流形外潜变量影响，从而生成错误的对象结构；（2）编码器固有的弱像素级重建能力阻碍生成器学习精确的细粒度几何与纹理特征。本文提出一个系统性框架，将面向理解任务的编码器特征适配于生成任务。我们引入语义-像素联合重建目标来正则化潜在空间，使语义信息与细粒度细节能同时压缩到高度紧凑的表征中（96通道且空间下采样16倍）。该设计确保潜在空间既保持语义丰富性，又实现最先进的图像重建性能，同时具备足够紧凑性以支持精确生成。基于此表征，我们设计了统一的文本到图像（T2I）与图像编辑模型。通过对多种特征空间的基准测试，我们证明该方法在重建质量上达到最优，具有更快的收敛速度，并在T2I与编辑任务中取得显著性能提升，验证了表征编码器可有效转化为鲁棒的生成组件。",
    "url": "https://huggingface.co/papers/2512.17909",
    "arxiv_url": "https://arxiv.org/abs/2512.17909"
  },
  {
    "title": "Are We on the Right Way to Assessing LLM-as-a-Judge?",
    "summary": "LLM-as-a-Judge has been widely adopted as an evaluation method and served as supervised rewards in model training. However, existing benchmarks for LLM-as-a-Judge are mainly relying on human-annotated ground truth, which introduces human bias that undermines the assessment of reliability and imposes scalability constraints. To overcome these limitations, we introduce Sage, a novel evaluation suite that assesses the quality of LLM judges without necessitating any human annotation. Inspired by axioms of rational choice theory, Sage introduces two new lenses for measuring LLM-as-a-Judge: local self-consistency (pair-wise preference stability) and global logical consistency (transitivity across a full set of preferences). We curate a dataset of 650 questions by combining structured benchmark problems with real-world user queries. Our experiments demonstrate both the stability of our metrics and their high correlation with supervised benchmarks like LLMBar and RewardBench2, confirming Sage's reliability as an evaluation suite for the robustness and accuracy of LLM-as-a-Judge. Based on Sage, we reveal that current state-of-the-art LLMs exhibit significant reliability problems when acting as judges in both scoring and pairwise settings; even the top-performing models, Gemini-2.5-Pro and GPT-5, fail to maintain consistent preferences in nearly a quarter of difficult cases. We attribute this to a new phenomenon called situational preference, which explains why explicit rubrics or criteria can help the model judge consistently across answer pairs. Our further analysis shows that finetuned LLM-as-a-Judge is a feasible method to boost performance, and the panel-based judge as well as deep reasoning can enhance the judging consistency. We also find substantial inconsistency in human judgments, which indicates that human annotation may not be a reliable gold standard.",
    "translation": "标题：评估“大语言模型即评委”的方法是否走在正确道路上？\n\n摘要：“大语言模型即评委”作为一种评估方法已被广泛采用，并在模型训练中充当监督奖励信号。然而，现有针对该模式的评测基准主要依赖人工标注的真实数据，这引入了人为偏见，不仅削弱了可靠性评估，还带来了可扩展性限制。为克服这些局限，我们提出了Sage——一个无需任何人工标注即可评估大语言模型评委质量的新型评测框架。受理性选择理论公理的启发，Sage引入两个全新评估维度：局部自一致性（成对偏好稳定性）与全局逻辑一致性（全偏好集的传递性）。我们通过整合结构化基准问题与现实用户查询，构建了包含650个问题的数据集。实验证明，我们的评估指标既具有稳定性，又与LLMBar、RewardBench2等监督式基准保持高度相关性，从而验证了Sage作为评估大语言模型评委鲁棒性与准确性的可靠性。基于Sage的评估，我们发现当前最先进的大语言模型在评分和成对比较场景中担任评委时均存在显著可靠性问题；即使是表现最优异的Gemini-2.5-Pro与GPT-5模型，在近四分之一复杂案例中仍无法保持偏好一致性。我们将此归因于一种称为“情境性偏好”的新现象，该现象解释了为何明确的评分标准或准则能帮助模型在不同答案对间保持判断一致性。进一步分析表明：微调大语言模型评委是提升性能的有效途径，而评审团机制与深度推理能增强判断一致性。我们还发现人类判断存在显著不一致性，这提示人工标注可能并非可靠的黄金标准。",
    "url": "https://huggingface.co/papers/2512.16041",
    "arxiv_url": "https://arxiv.org/abs/2512.16041"
  },
  {
    "title": "An Anatomy of Vision-Language-Action Models: From Modules to Milestones and Challenges",
    "summary": "Vision-Language-Action (VLA) models are driving a revolution in robotics, enabling machines to understand instructions and interact with the physical world. This field is exploding with new models and datasets, making it both exciting and challenging to keep pace with. This survey offers a clear and structured guide to the VLA landscape. We design it to follow the natural learning path of a researcher: we start with the basic Modules of any VLA model, trace the history through key Milestones, and then dive deep into the core Challenges that define recent research frontier. Our main contribution is a detailed breakdown of the five biggest challenges in: (1) Representation, (2) Execution, (3) Generalization, (4) Safety, and (5) Dataset and Evaluation. This structure mirrors the developmental roadmap of a generalist agent: establishing the fundamental perception-action loop, scaling capabilities across diverse embodiments and environments, and finally ensuring trustworthy deployment-all supported by the essential data infrastructure. For each of them, we review existing approaches and highlight future opportunities. We position this paper as both a foundational guide for newcomers and a strategic roadmap for experienced researchers, with the dual aim of accelerating learning and inspiring new ideas in embodied intelligence. A live version of this survey, with continuous updates, is maintained on our https://suyuz1.github.io/Survery/{project page}.",
    "translation": "标题：视觉-语言-动作模型剖析：从模块构成到发展里程碑与挑战\n\n摘要：视觉-语言-动作（VLA）模型正在推动机器人技术的革命，使机器能够理解指令并与物理世界进行交互。该领域正涌现大量新模型与数据集，在带来发展机遇的同时也使得跟踪研究进展面临挑战。本综述为VLA研究领域提供清晰的结构化指引。我们按照研究者的自然学习路径设计框架：从VLA模型的基础模块出发，梳理关键发展里程碑，进而深入探讨定义当前研究前沿的核心挑战。我们的主要贡献在于对五大挑战领域进行系统性剖析：（1）表征学习，（2）动作执行，（3）泛化能力，（4）安全可靠性，以及（5）数据集与评估体系。这一结构映射出通用智能体的发展路线图：建立感知-动作的基础闭环，在不同具身形式与环境中扩展能力，最终实现可信部署——所有这些都离不开数据基础设施的支撑。针对每个挑战领域，我们系统评述现有研究方法并指明未来机遇。本文既可作为新研究者的基础指南，亦可作为资深学者的战略路线图，旨在加速具身智能领域的学习进程并激发创新思路。本综述的动态更新版本持续维护于https://suyuz1.github.io/Survery/{项目页面}。",
    "url": "https://huggingface.co/papers/2512.11362",
    "arxiv_url": "https://arxiv.org/abs/2512.11362"
  },
  {
    "title": "RadarGen: Automotive Radar Point Cloud Generation from Cameras",
    "summary": "We present RadarGen, a diffusion model for synthesizing realistic automotive radar point clouds from multi-view camera imagery. RadarGen adapts efficient image-latent diffusion to the radar domain by representing radar measurements in bird's-eye-view form that encodes spatial structure together with radar cross section (RCS) and Doppler attributes. A lightweight recovery step reconstructs point clouds from the generated maps. To better align generation with the visual scene, RadarGen incorporates BEV-aligned depth, semantic, and motion cues extracted from pretrained foundation models, which guide the stochastic generation process toward physically plausible radar patterns. Conditioning on images makes the approach broadly compatible, in principle, with existing visual datasets and simulation frameworks, offering a scalable direction for multimodal generative simulation. Evaluations on large-scale driving data show that RadarGen captures characteristic radar measurement distributions and reduces the gap to perception models trained on real data, marking a step toward unified generative simulation across sensing modalities.",
    "translation": "标题：RadarGen：基于多视角摄像头的汽车雷达点云生成方法\n\n摘要：本文提出RadarGen——一种基于扩散模型的多视角摄像头图像到汽车雷达点云的生成方法。该方法通过将雷达测量数据编码为包含空间结构、雷达散射截面（RCS）和多普勒属性的鸟瞰图形式，将高效的图像潜在扩散模型适配至雷达领域。通过轻量级重建步骤，可从生成的特征图中恢复点云数据。为实现生成结果与视觉场景的精准对齐，RadarGen融合了从预训练基础模型中提取的鸟瞰图对齐深度、语义及运动线索，引导随机生成过程形成物理可信的雷达模式。基于图像的条件生成机制使本方法原则上兼容现有视觉数据集与仿真框架，为多模态生成式仿真提供了可扩展的技术路径。在大规模驾驶数据上的评估表明，RadarGen能够准确捕捉雷达测量的特征分布，并显著缩小基于生成数据与真实数据训练的感知模型性能差距，为实现跨传感模态的统一生成式仿真迈出了重要一步。",
    "url": "https://huggingface.co/papers/2512.17897",
    "arxiv_url": "https://arxiv.org/abs/2512.17897"
  },
  {
    "title": "Robust-R1: Degradation-Aware Reasoning for Robust Visual Understanding",
    "summary": "Multimodal Large Language Models struggle to maintain reliable performance under extreme real-world visual degradations, which impede their practical robustness. Existing robust MLLMs predominantly rely on implicit training/adaptation that focuses solely on visual encoder generalization, suffering from limited interpretability and isolated optimization. To overcome these limitations, we propose Robust-R1, a novel framework that explicitly models visual degradations through structured reasoning chains. Our approach integrates: (i) supervised fine-tuning for degradation-aware reasoning foundations, (ii) reward-driven alignment for accurately perceiving degradation parameters, and (iii) dynamic reasoning depth scaling adapted to degradation intensity. To facilitate this approach, we introduce a specialized 11K dataset featuring realistic degradations synthesized across four critical real-world visual processing stages, each annotated with structured chains connecting degradation parameters, perceptual influence, pristine semantic reasoning chain, and conclusion. Comprehensive evaluations demonstrate state-of-the-art robustness: Robust-R1 outperforms all general and robust baselines on the real-world degradation benchmark R-Bench, while maintaining superior anti-degradation performance under multi-intensity adversarial degradations on MMMB, MMStar, and RealWorldQA.",
    "translation": "标题：Robust-R1：面向鲁棒视觉理解的退化感知推理框架\n\n摘要：多模态大语言模型在极端现实世界视觉退化条件下难以保持可靠性能，这限制了其实际应用的鲁棒性。现有鲁棒性多模态大语言模型主要依赖仅关注视觉编码器泛化能力的隐式训练/适配方法，存在可解释性有限与优化孤立的问题。为突破这些局限，我们提出Robust-R1——一种通过结构化推理链显式建模视觉退化的新型框架。该框架整合了：（1）面向退化感知推理基础的监督微调；（2）用于精准感知退化参数的奖励驱动对齐机制；（3）适应退化强度的动态推理深度缩放策略。为支撑该方法，我们构建了一个包含11K样本的专用数据集，其通过四个关键现实世界视觉处理阶段合成具有真实感的退化数据，每个样本均标注了连接退化参数、感知影响、原始语义推理链与结论的结构化链条。综合实验表明本方法达到最先进的鲁棒性水平：Robust-R1在现实世界退化基准R-Bench上超越所有通用及鲁棒性基线模型，同时在MMMB、MMStar和RealWorldQA数据集的多强度对抗性退化测试中保持卓越的抗退化性能。",
    "url": "https://huggingface.co/papers/2512.17532",
    "arxiv_url": "https://arxiv.org/abs/2512.17532"
  },
  {
    "title": "GroundingME: Exposing the Visual Grounding Gap in MLLMs through Multi-Dimensional Evaluation",
    "summary": "Visual grounding, localizing objects from natural language descriptions, represents a critical bridge between language and vision understanding. While multimodal large language models (MLLMs) achieve impressive scores on existing benchmarks, a fundamental question remains: can MLLMs truly ground language in vision with human-like sophistication, or are they merely pattern-matching on simplified datasets? Current benchmarks fail to capture real-world complexity where humans effortlessly navigate ambiguous references and recognize when grounding is impossible. To rigorously assess MLLMs' true capabilities, we introduce GroundingME, a benchmark that systematically challenges models across four critical dimensions: (1) Discriminative, distinguishing highly similar objects, (2) Spatial, understanding complex relational descriptions, (3) Limited, handling occlusions or tiny objects, and (4) Rejection, recognizing ungroundable queries. Through careful curation combining automated generation with human verification, we create 1,005 challenging examples mirroring real-world complexity. Evaluating 25 state-of-the-art MLLMs reveals a profound capability gap: the best model achieves only 45.1% accuracy, while most score 0% on rejection tasks, reflexively hallucinating objects rather than acknowledging their absence, raising critical safety concerns for deployment. We explore two strategies for improvements: (1) test-time scaling selects optimal response by thinking trajectory to improve complex grounding by up to 2.9%, and (2) data-mixture training teaches models to recognize ungroundable queries, boosting rejection accuracy from 0% to 27.9%. GroundingME thus serves as both a diagnostic tool revealing current limitations in MLLMs and a roadmap toward human-level visual grounding.",
    "translation": "标题：GroundingME：通过多维评估揭示多模态大语言模型中的视觉定位能力差距\n\n摘要：视觉定位（即根据自然语言描述定位目标物体）是连接语言理解与视觉理解的关键桥梁。尽管多模态大语言模型（MLLMs）在现有基准测试中取得了令人瞩目的分数，但一个根本问题依然存在：MLLMs 是否真能像人类一样精细地将语言锚定于视觉信息，还是仅仅在简化数据集上进行模式匹配？现有基准测试未能捕捉现实世界的复杂性，而人类却能轻松处理模糊指代并识别何时无法进行视觉定位。为严格评估 MLLMs 的真实能力，我们提出了 GroundingME 基准测试，从四个关键维度系统性地挑战模型：（1）判别性——区分高度相似的物体；（2）空间性——理解复杂关系描述；（3）受限性——处理遮挡或微小物体；（4）拒识性——识别无法定位的查询。通过自动化生成与人工验证相结合的精心构建，我们创建了 1,005 个反映现实复杂性的挑战性样本。对 25 个前沿 MLLMs 的评估揭示了显著的能力差距：最佳模型准确率仅为 45.1%，而多数模型在拒识任务中得分为 0%，它们会反射性地幻觉出不存在物体而非承认其缺失，这为实际部署带来了重大安全隐患。我们探索了两种改进策略：（1）测试时扩展技术通过思维轨迹选择最优响应，将复杂场景定位准确率最高提升 2.9%；（2）混合数据训练使模型学会识别不可定位查询，将拒识准确率从 0% 提升至 27.9%。因此，GroundingME 既可作为揭示当前 MLLMs 局限性的诊断工具，也为实现人类水平的视觉定位能力提供了发展路线图。",
    "url": "https://huggingface.co/papers/2512.17495",
    "arxiv_url": "https://arxiv.org/abs/2512.17495"
  },
  {
    "title": "Physics of Language Models: Part 4.1, Architecture Design and the Magic of Canon Layers",
    "summary": "Understanding architectural differences in language models is challenging, especially at academic-scale pretraining (e.g., 1.3B parameters, 100B tokens), where results are often dominated by noise and randomness. To overcome this, we introduce controlled synthetic pretraining tasks that isolate and evaluate core model capabilities. Within this framework, we discover CANON LAYERS: lightweight architectural components -- named after the musical term \"canon\" -- that promote horizontal information flow across neighboring tokens. Canon layers compute weighted sums of nearby token representations and integrate seamlessly into Transformers, linear attention, state-space models, or any sequence architecture.\n  We present 12 key results. This includes how Canon layers enhance reasoning depth (e.g., by 2times), reasoning breadth, knowledge manipulation, etc. They lift weak architectures like NoPE to match RoPE, and linear attention to rival SOTA linear models like Mamba2/GDN -- validated both through synthetic tasks and real-world academic-scale pretraining. This synthetic playground offers an economical, principled path to isolate core model capabilities often obscured at academic scales. Equipped with infinite high-quality data, it may even PREDICT how future architectures will behave as training pipelines improve -- e.g., through better data curation or RL-based post-training -- unlocking deeper reasoning and hierarchical inference.",
    "translation": "标题：语言模型的物理学：第四部分第一节，架构设计与规范层的神奇之处\n\n摘要：理解语言模型间的架构差异具有挑战性，尤其是在学术规模的预训练场景中（例如13亿参数、1000亿标记），其结果往往受到噪声和随机性的主导。为克服这一难题，我们引入了受控的合成预训练任务，以隔离并评估模型的核心能力。在此框架下，我们发现了**规范层**：一种轻量级的架构组件——其命名源自音乐术语“卡农”——能够促进相邻标记间的横向信息流动。规范层通过计算邻近标记表示的加权和，可无缝集成至Transformer、线性注意力、状态空间模型或任何序列架构中。\n\n我们展示了12项关键结果。其中包括规范层如何提升推理深度（例如提升2倍）、拓展推理广度、增强知识处理能力等。它们能使弱架构（如NoPE）提升至与RoPE相当的水平，并使线性注意力模型达到与Mamba2/GDN等先进线性模型相媲美的性能——这些结论均通过合成任务和真实学术规模预训练得到验证。这一合成实验环境提供了一条经济且系统化的路径，以揭示在学术规模下常被掩盖的核心模型能力。借助无限的高质量数据，它甚至能够**预测**未来架构在训练流程改进（例如通过更优的数据筛选或基于强化学习的后训练）后的表现，从而解锁更深层次的推理与分层推断能力。",
    "url": "https://huggingface.co/papers/2512.17351",
    "arxiv_url": "https://arxiv.org/abs/2512.17351"
  },
  {
    "title": "Turn-PPO: Turn-Level Advantage Estimation with PPO for Improved Multi-Turn RL in Agentic LLMs",
    "summary": "Reinforcement learning (RL) has re-emerged as a natural approach for training interactive LLM agents in real-world environments. However, directly applying the widely used Group Relative Policy Optimization (GRPO) algorithm to multi-turn tasks exposes notable limitations, particularly in scenarios requiring long-horizon reasoning. To address these challenges, we investigate more stable and effective advantage estimation strategies, especially for multi-turn settings. We first explore Proximal Policy Optimization (PPO) as an alternative and find it to be more robust than GRPO. To further enhance PPO in multi-turn scenarios, we introduce turn-PPO, a variant that operates on a turn-level MDP formulation, as opposed to the commonly used token-level MDP. Our results on the WebShop and Sokoban datasets demonstrate the effectiveness of turn-PPO, both with and without long reasoning components.",
    "translation": "标题：Turn-PPO：基于PPO的回合级优势估计以改进智能大语言模型中的多轮强化学习\n\n摘要：强化学习已重新成为在现实环境中训练交互式大语言模型智能体的自然方法。然而，直接将广泛使用的组相对策略优化算法应用于多轮任务时，暴露出明显的局限性，尤其是在需要长程推理的场景中。为应对这些挑战，我们研究了更稳定、更有效的优势估计策略，特别是针对多轮交互设置。我们首先探索了近端策略优化作为替代方案，并发现其比GRPO更具鲁棒性。为了在多轮场景中进一步增强PPO，我们提出了turn-PPO，这是一种基于回合级马尔可夫决策过程建模的变体，而非常用的令牌级MDP。我们在WebShop和Sokoban数据集上的实验结果证明了turn-PPO的有效性，无论是否包含长推理组件。",
    "url": "https://huggingface.co/papers/2512.17008",
    "arxiv_url": "https://arxiv.org/abs/2512.17008"
  },
  {
    "title": "HERBench: A Benchmark for Multi-Evidence Integration in Video Question Answering",
    "summary": "Video Large Language Models (Video-LLMs) are rapidly improving, yet current Video Question Answering (VideoQA) benchmarks often allow questions to be answered from a single salient cue, under-testing reasoning that must aggregate multiple, temporally separated visual evidence. We present HERBench, a VideoQA benchmark purpose-built to assess multi-evidence integration across time. Each question requires aggregating at least three non-overlapping evidential cues across distinct video segments, so neither language priors nor a single snapshot can suffice. HERBench comprises 26K five-way multiple-choice questions organized into twelve compositional tasks that probe identity binding, cross-entity relations, temporal ordering, co-occurrence verification, and counting. To make evidential demand measurable, we introduce the Minimum Required Frame-Set (MRFS), the smallest number of frames a model must fuse to answer correctly, and show that HERBench imposes substantially higher demand than prior datasets (mean MRFS 5.5 vs. 2.6-4.2). Evaluating 13 state-of-the-art Video-LLMs on HERBench reveals pervasive failures: accuracies of 31-42% are only slightly above the 20% random-guess baseline. We disentangle this failure into two critical bottlenecks: (1) a retrieval deficit, where frame selectors overlook key evidence, and (2) a fusion deficit, where models fail to integrate information even when all necessary evidence is provided. By making cross-time evidence both unavoidable and quantifiable, HERBench establishes a principled target for advancing robust, compositional video understanding.",
    "translation": "标题：HERBench：视频问答中多证据整合的基准测试框架\n\n摘要：视频大语言模型（Video-LLMs）正快速发展，然而当前的视频问答（VideoQA）基准测试常允许仅凭单一显著线索回答问题，未能充分检验模型对多个时间分散的视觉证据进行整合推理的能力。本文提出HERBench，这是一个专门用于评估跨时间多证据整合能力的VideoQA基准测试集。其中每个问题均要求整合至少三个来自不同视频片段的非重叠证据线索，因此仅依赖语言先验或单一视频快照均无法正确作答。HERBench包含2.6万个五选一多项选择题，划分为十二类组合式任务，涵盖身份绑定、跨实体关系、时序排序、共现验证及计数推理等维度。为量化证据需求，我们引入“最小必需帧集”（MRFS）指标，即模型必须融合的最小帧数才能正确回答问题。实验表明HERBench对证据整合的需求显著高于现有数据集（平均MRFS为5.5帧，对比基准数据集的2.6-4.2帧）。通过对13个前沿Video-LLMs的评估，发现普遍存在能力缺陷：模型准确率仅为31%-42%，略高于20%的随机猜测基线。我们将这种缺陷归因于两个关键瓶颈：（1）检索缺陷——帧选择器易遗漏关键证据；（2）融合缺陷——即使提供全部必要证据，模型仍无法有效整合信息。通过构建跨时间证据的强制性需求与可量化评估体系，HERBench为推进鲁棒性、组合式的视频理解研究确立了原则性目标。",
    "url": "https://huggingface.co/papers/2512.14870",
    "arxiv_url": "https://arxiv.org/abs/2512.14870"
  },
  {
    "title": "Animate Any Character in Any World",
    "summary": "Recent advances in world models have greatly enhanced interactive environment simulation. Existing methods mainly fall into two categories: (1) static world generation models, which construct 3D environments without active agents, and (2) controllable-entity models, which allow a single entity to perform limited actions in an otherwise uncontrollable environment. In this work, we introduce AniX, leveraging the realism and structural grounding of static world generation while extending controllable-entity models to support user-specified characters capable of performing open-ended actions. Users can provide a 3DGS scene and a character, then direct the character through natural language to perform diverse behaviors from basic locomotion to object-centric interactions while freely exploring the environment. AniX synthesizes temporally coherent video clips that preserve visual fidelity with the provided scene and character, formulated as a conditional autoregressive video generation problem. Built upon a pre-trained video generator, our training strategy significantly enhances motion dynamics while maintaining generalization across actions and characters. Our evaluation covers a broad range of aspects, including visual quality, character consistency, action controllability, and long-horizon coherence.",
    "translation": "标题：任意世界中的角色动画生成\n\n摘要：世界模型的最新进展显著提升了交互式环境模拟能力。现有方法主要分为两类：(1)静态世界生成模型——构建无主动智能体的三维环境；(2)可控实体模型——允许单个实体在不可控环境中执行有限动作。本研究提出AniX系统，在保持静态世界生成模型真实性与结构基础的同时，扩展可控实体模型以支持用户指定角色执行开放式动作。用户可提供3D高斯泼溅场景与角色模型，通过自然语言指令驱动角色完成从基础移动到以对象为中心的交互等多样化行为，并自由探索环境。AniX将任务构建为条件自回归视频生成问题，能够合成时间连贯且与输入场景、角色视觉保真度一致的视频片段。基于预训练视频生成器构建的训练策略在保持跨动作与角色泛化能力的同时，显著提升了运动动态表现。评估体系涵盖视觉质量、角色一致性、动作可控性及长时序连贯性等多维度指标。",
    "url": "https://huggingface.co/papers/2512.17796",
    "arxiv_url": "https://arxiv.org/abs/2512.17796"
  },
  {
    "title": "SWE-Bench++: A Framework for the Scalable Generation of Software Engineering Benchmarks from Open-Source Repositories",
    "summary": "Benchmarks like SWE-bench have standardized the evaluation of Large Language Models (LLMs) on repository-level software engineering tasks. However, these efforts remain limited by manual curation, static datasets, and a focus on Python-based bug fixes. We introduce SWE-Bench++, an automated framework that generates repository-level coding tasks from open-source GitHub projects. Unlike synthetic approaches, our pipeline harvests live pull requests to cover both bug fixes and feature requests across 11 languages. SWE-Bench++ turns GitHub pull requests (PRs) into reproducible, execution-based tasks via four stages: programmatic sourcing, environment synthesis, test oracle extraction, and quality assurance. A final hint-guided trajectory synthesis step converts instances that strong models fail on into training trajectories. Our initial benchmark consists of 11,133 instances from 3,971 repositories across 11 languages. On a subset of 1,782 instances of this benchmark, today's strongest models perform as follows: claude-sonnet-4.5 achieves 36.20% pass@10, gpt-5-2025-08-07 34.57%, gemini/gemini-2.5-pro 24.92%, and gpt-4o 16.89%. We further demonstrate the utility of our dataset by showing that fine-tuning on SWE-Bench++ instances yields measurable improvements on the SWE-bench Multilingual benchmark. SWE-Bench++ provides a scalable, multilingual benchmark for evaluating and improving repository-level code generation.",
    "translation": "标题：SWE-Bench++：一种基于开源仓库的可扩展软件工程基准生成框架\n\n摘要：诸如SWE-bench等基准测试已在仓库级软件工程任务上实现了对大语言模型（LLM）评估的标准化。然而，现有工作仍受限于人工整理、静态数据集以及专注于基于Python的错误修复。本文提出SWE-Bench++，这是一个从开源GitHub项目中自动生成仓库级编码任务的框架。与合成方法不同，我们的流水线通过采集实时拉取请求，覆盖了11种编程语言的错误修复与功能需求两类任务。SWE-Bench++通过四个阶段将GitHub拉取请求转化为可复现的、基于执行的任务：程序化采集、环境合成、测试预言提取与质量保证。最后通过提示引导的轨迹合成步骤，将强模型未能解决的任务实例转化为训练轨迹。我们的初始基准包含来自3,971个仓库、涵盖11种语言的11,133个任务实例。在该基准的1,782个实例子集上，当前最强模型的表现如下：claude-sonnet-4.5达到36.20% pass@10，gpt-5-2025-08-07为34.57%，gemini/gemini-2.5-pro为24.92%，gpt-4o为16.89%。我们进一步通过实验证明，在SWE-Bench++实例上进行微调可在SWE-bench多语言基准上带来可测量的性能提升。SWE-Bench++为评估和改进仓库级代码生成提供了一个可扩展、多语言的基准测试框架。",
    "url": "https://huggingface.co/papers/2512.17419",
    "arxiv_url": "https://arxiv.org/abs/2512.17419"
  },
  {
    "title": "Bolmo: Byteifying the Next Generation of Language Models",
    "summary": "We introduce Bolmo, the first family of competitive fully open byte-level language models (LMs) at the 1B and 7B parameter scales. In contrast to prior research on byte-level LMs, which focuses predominantly on training from scratch, we train Bolmo by byteifying existing subword-level LMs. Byteification enables overcoming the limitations of subword tokenization - such as insufficient character understanding and efficiency constraints due to the fixed subword vocabulary - while performing at the level of leading subword-level LMs. Bolmo is specifically designed for byteification: our architecture resolves a mismatch between the expressivity of prior byte-level architectures and subword-level LMs, which makes it possible to employ an effective exact distillation objective between Bolmo and the source subword model. This allows for converting a subword-level LM to a byte-level LM by investing less than 1\\% of a typical pretraining token budget. Bolmo substantially outperforms all prior byte-level LMs of comparable size, and outperforms the source subword-level LMs on character understanding and, in some cases, coding, while coming close to matching the original LMs' performance on other tasks. Furthermore, we show that Bolmo can achieve inference speeds competitive with subword-level LMs by training with higher token compression ratios, and can be cheaply and effectively post-trained by leveraging the existing ecosystem around the source subword-level LM. Our results finally make byte-level LMs a practical choice competitive with subword-level LMs across a wide set of use cases.",
    "translation": "标题：Bolmo：字节化新一代语言模型\n\n摘要：我们推出Bolmo，这是首个在10亿和70亿参数规模上具有竞争力的全开放字节级语言模型系列。与先前主要关注从头训练的字节级语言模型研究不同，我们通过对现有子词级语言模型进行字节化来训练Bolmo。字节化能够克服子词分词的局限性——例如对字符理解不足以及固定子词词汇表导致的效率限制——同时达到领先子词级语言模型的性能水平。Bolmo专为字节化设计：我们的架构解决了先前字节级架构与子词级语言模型在表达能力上的不匹配问题，从而能够在Bolmo与源子词模型之间采用有效的精确蒸馏目标。这使得仅需投入不到典型预训练标记预算的1%，即可将子词级语言模型转换为字节级语言模型。Bolmo在性能上显著超越所有同类规模的先前字节级语言模型，并在字符理解任务上优于源子词级语言模型，在某些编码任务中也表现更佳，同时在其他任务上接近原始语言模型的性能。此外，我们证明Bolmo通过采用更高的标记压缩比进行训练，能够实现与子词级语言模型相竞争的推理速度，并且可以借助围绕源子词级语言模型的现有生态系统进行低成本、高效的后续训练。我们的研究成果最终使字节级语言模型成为广泛用例中与子词级语言模型相竞争的实用选择。",
    "url": "https://huggingface.co/papers/2512.15586",
    "arxiv_url": "https://arxiv.org/abs/2512.15586"
  },
  {
    "title": "StageVAR: Stage-Aware Acceleration for Visual Autoregressive Models",
    "summary": "Visual Autoregressive (VAR) modeling departs from the next-token prediction paradigm of traditional Autoregressive (AR) models through next-scale prediction, enabling high-quality image generation. However, the VAR paradigm suffers from sharply increased computational complexity and running time at large-scale steps. Although existing acceleration methods reduce runtime for large-scale steps, but rely on manual step selection and overlook the varying importance of different stages in the generation process. To address this challenge, we present StageVAR, a systematic study and stage-aware acceleration framework for VAR models. Our analysis shows that early steps are critical for preserving semantic and structural consistency and should remain intact, while later steps mainly refine details and can be pruned or approximated for acceleration. Building on these insights, StageVAR introduces a plug-and-play acceleration strategy that exploits semantic irrelevance and low-rank properties in late-stage computations, without requiring additional training. Our proposed StageVAR achieves up to 3.4x speedup with only a 0.01 drop on GenEval and a 0.26 decrease on DPG, consistently outperforming existing acceleration baselines. These results highlight stage-aware design as a powerful principle for efficient visual autoregressive image generation.",
    "translation": "标题：StageVAR：面向视觉自回归模型的分阶段感知加速方法\n\n摘要：视觉自回归模型通过“下一尺度预测”突破了传统自回归模型“下一标记预测”的范式，实现了高质量图像生成。然而，该范式在大规模生成步骤中面临计算复杂度与运行时间急剧增加的问题。现有加速方法虽能减少大规模步骤的运行时间，但依赖人工步骤选择，且忽视了生成过程中不同阶段的重要性差异。为应对这一挑战，本文提出StageVAR——一个针对视觉自回归模型的系统性研究与分阶段感知加速框架。我们的分析表明：早期步骤对保持语义与结构一致性至关重要，应完整保留；而后期步骤主要进行细节优化，可通过剪枝或近似计算实现加速。基于此发现，StageVAR提出一种即插即用的加速策略，利用后期计算中的语义无关性与低秩特性实现加速，且无需额外训练。实验表明，StageVAR在GenEval基准上仅损失0.01分、在DPG基准上仅降低0.26分的情况下，最高可实现3.4倍加速效果，性能持续优于现有加速基线。这些结果证明，分阶段感知设计是实现高效视觉自回归图像生成的有效原则。",
    "url": "https://huggingface.co/papers/2512.16483",
    "arxiv_url": "https://arxiv.org/abs/2512.16483"
  },
  {
    "title": "Meta-RL Induces Exploration in Language Agents",
    "summary": "Reinforcement learning (RL) has enabled the training of large language model (LLM) agents to interact with the environment and to solve multi-turn long-horizon tasks. However, the RL-trained agents often struggle in tasks that require active exploration and fail to efficiently adapt from trial-and-error experiences. In this paper, we present LaMer, a general Meta-RL framework that enables LLM agents to actively explore and learn from the environment feedback at test time. LaMer consists of two key components: (i) a cross-episode training framework to encourage exploration and long-term rewards optimization; and (ii) in-context policy adaptation via reflection, allowing the agent to adapt their policy from task feedback signal without gradient update. Experiments across diverse environments show that LaMer significantly improves performance over RL baselines, with 11%, 14%, and 19% performance gains on Sokoban, MineSweeper and Webshop, respectively. Moreover, LaMer also demonstrates better generalization to more challenging or previously unseen tasks compared to the RL-trained agents. Overall, our results demonstrate that Meta-RL provides a principled approach to induce exploration in language agents, enabling more robust adaptation to novel environments through learned exploration strategies.",
    "translation": "标题：元强化学习在语言智能体中引导探索行为\n\n摘要：强化学习（RL）已能够训练大型语言模型（LLM）智能体与环境交互，以解决多轮次长周期任务。然而，经强化学习训练的智能体在需要主动探索的任务中往往表现不佳，且难以从试错经验中高效适应。本文提出LaMer——一个通用的元强化学习框架，使LLM智能体能够在测试阶段主动探索并从环境反馈中学习。LaMer包含两个关键组成部分：（一）跨回合训练框架，以鼓励探索并优化长期奖励；（二）通过反思实现上下文策略自适应，使智能体无需梯度更新即可根据任务反馈信号调整策略。在多样化环境中的实验表明，LaMer性能显著优于强化学习基线方法，在推箱子、扫雷和网络购物任务中分别实现了11%、14%和19%的性能提升。此外，与强化学习训练的智能体相比，LaMer在更具挑战性或先前未见任务上也表现出更好的泛化能力。总体而言，我们的研究结果表明，元强化学习为语言智能体提供了一种引导探索行为的理论方法，使其能够通过习得的探索策略更稳健地适应新环境。",
    "url": "https://huggingface.co/papers/2512.16848",
    "arxiv_url": "https://arxiv.org/abs/2512.16848"
  },
  {
    "title": "3D-RE-GEN: 3D Reconstruction of Indoor Scenes with a Generative Framework",
    "summary": "Recent advances in 3D scene generation produce visually appealing output, but current representations hinder artists' workflows that require modifiable 3D textured mesh scenes for visual effects and game development. Despite significant advances, current textured mesh scene reconstruction methods are far from artist ready, suffering from incorrect object decomposition, inaccurate spatial relationships, and missing backgrounds. We present 3D-RE-GEN, a compositional framework that reconstructs a single image into textured 3D objects and a background. We show that combining state of the art models from specific domains achieves state of the art scene reconstruction performance, addressing artists' requirements.\n  Our reconstruction pipeline integrates models for asset detection, reconstruction, and placement, pushing certain models beyond their originally intended domains. Obtaining occluded objects is treated as an image editing task with generative models to infer and reconstruct with scene level reasoning under consistent lighting and geometry. Unlike current methods, 3D-RE-GEN generates a comprehensive background that spatially constrains objects during optimization and provides a foundation for realistic lighting and simulation tasks in visual effects and games. To obtain physically realistic layouts, we employ a novel 4-DoF differentiable optimization that aligns reconstructed objects with the estimated ground plane. 3D-RE-GEN~achieves state of the art performance in single image 3D scene reconstruction, producing coherent, modifiable scenes through compositional generation guided by precise camera recovery and spatial optimization.",
    "translation": "标题：3D-RE-GEN：基于生成框架的室内场景三维重建\n\n摘要：当前三维场景生成技术虽能产生视觉效果出色的成果，但其现有表征形式难以满足视觉特效与游戏开发领域艺术家对可编辑纹理化三维网格场景的工作流程需求。尽管相关技术已取得显著进展，现有纹理网格场景重建方法仍存在物体分解错误、空间关系不准确及背景缺失等问题，远未达到艺术创作实用标准。本文提出3D-RE-GEN——一种能够将单张图像重建为纹理化三维物体与背景的组合式框架。研究表明，通过整合特定领域的前沿模型，本框架在满足艺术创作需求的同时实现了当前最优的场景重建性能。\n\n我们的重建管线集成了资产检测、重建与布局模型，并将部分模型的应用范围拓展至原设计领域之外。针对被遮挡物体的重建问题，我们将其转化为生成式模型驱动的图像编辑任务，通过在一致光照与几何约束下进行场景级推理来实现物体推断与重建。与现有方法不同，3D-RE-GEN能够生成完整的背景环境，在优化过程中为物体提供空间约束，并为视觉特效与游戏中的真实光照模拟任务奠定基础。为获得物理合理的场景布局，我们提出了一种新颖的四自由度可微分优化方法，将重建物体与估计的地平面进行精确对齐。通过精确相机恢复与空间优化指导的组合式生成，3D-RE-GEN在单图像三维场景重建中实现了当前最优性能，能够生成具有空间一致性与可编辑性的完整场景。",
    "url": "https://huggingface.co/papers/2512.17459",
    "arxiv_url": "https://arxiv.org/abs/2512.17459"
  },
  {
    "title": "A Benchmark and Agentic Framework for Omni-Modal Reasoning and Tool Use in Long Videos",
    "summary": "Long-form multimodal video understanding requires integrating vision, speech, and ambient audio with coherent long-range reasoning. Existing benchmarks emphasize either temporal length or multimodal richness, but rarely both and while some incorporate open-ended questions and advanced metrics, they mostly rely on single-score accuracy, obscuring failure modes. We introduce LongShOTBench, a diagnostic benchmark with open-ended, intent-driven questions; single- and multi-turn dialogues; and tasks requiring multimodal reasoning and agentic tool use across video, audio, and speech. Each item includes a reference answer and graded rubric for interpretable, and traceable evaluation. LongShOTBench is produced via a scalable, human-validated pipeline to ensure coverage and reproducibility. All samples in our LongShOTBench are human-verified and corrected. Furthermore, we present LongShOTAgent, an agentic system that analyzes long videos via preprocessing, search, and iterative refinement. On LongShOTBench, state-of-the-art MLLMs show large gaps: Gemini-2.5-Flash achieves 52.95%, open-source models remain below 30%, and LongShOTAgent attains 44.66%. These results underscore the difficulty of real-world long-form video understanding. LongShOTBench provides a practical, reproducible foundation for evaluating and improving MLLMs. All resources are available on GitHub: https://github.com/mbzuai-oryx/longshot.",
    "translation": "标题：面向长视频全模态推理与工具使用的基准与智能体框架\n\n摘要：长模态视频理解需要融合视觉、语音与环境音频信息，并进行连贯的长程推理。现有基准往往侧重时序长度或多模态丰富性中的单一维度，鲜有兼顾两者；尽管部分基准引入了开放式问题与高级评估指标，但仍主要依赖单一分数准确率，难以揭示模型失效的具体模式。本文提出LongShOTBench——一个包含开放式意图驱动问题、单轮/多轮对话，以及需要跨视频、音频和语音进行多模态推理与智能体工具使用的诊断性基准。每个评估项均配有参考答案和分级评分标准，支持可解释、可追溯的评估。该基准通过可扩展的人工验证流程构建，确保覆盖度与可复现性，所有样本均经过人工核验与修正。此外，我们提出LongShOTAgent智能体系统，通过预处理、检索和迭代优化实现长视频分析。在LongShOTBench上的实验表明，当前先进多模态大语言模型存在显著差距：Gemini-2.5-Flash仅达到52.95%，开源模型普遍低于30%，而LongShOTAgent获得44.66%的得分。这些结果凸显了现实场景中长模态视频理解的挑战性。LongShOTBench为评估和改进多模态大语言模型提供了实用、可复现的基础框架。所有资源已在GitHub开源：https://github.com/mbzuai-oryx/longshot。",
    "url": "https://huggingface.co/papers/2512.16978",
    "arxiv_url": "https://arxiv.org/abs/2512.16978"
  },
  {
    "title": "MineTheGap: Automatic Mining of Biases in Text-to-Image Models",
    "summary": "Text-to-Image (TTI) models generate images based on text prompts, which often leave certain aspects of the desired image ambiguous. When faced with these ambiguities, TTI models have been shown to exhibit biases in their interpretations. These biases can have societal impacts, e.g., when showing only a certain race for a stated occupation. They can also affect user experience when creating redundancy within a set of generated images instead of spanning diverse possibilities. Here, we introduce MineTheGap - a method for automatically mining prompts that cause a TTI model to generate biased outputs. Our method goes beyond merely detecting bias for a given prompt. Rather, it leverages a genetic algorithm to iteratively refine a pool of prompts, seeking for those that expose biases. This optimization process is driven by a novel bias score, which ranks biases according to their severity, as we validate on a dataset with known biases. For a given prompt, this score is obtained by comparing the distribution of generated images to the distribution of LLM-generated texts that constitute variations on the prompt. Code and examples are available on the project's webpage.",
    "translation": "标题：MineTheGap：文本到图像模型中偏见的自动挖掘\n\n摘要：文本到图像模型根据文本提示生成图像，而文本提示往往对期望图像的某些方面存在模糊性。面对这些模糊性时，文本到图像模型已显示出在解释过程中存在偏见。这些偏见可能产生社会影响，例如在描述特定职业时仅呈现某一特定种族。当一组生成图像内部出现冗余而非覆盖多样可能性时，这些偏见也会影响用户体验。本文提出MineTheGap——一种自动挖掘导致文本到图像模型产生偏见输出的提示词方法。我们的方法不仅限于检测给定提示词的偏见，更通过遗传算法迭代优化提示词池，寻找能够暴露偏见的提示词。这一优化过程由新颖的偏见评分驱动，该评分根据偏见严重程度进行排序（我们在已知偏见数据集上验证了其有效性）。对于给定提示词，该评分通过比较生成图像的分布与基于提示词变体生成的大语言模型文本分布而获得。代码和示例详见项目网页。",
    "url": "https://huggingface.co/papers/2512.13427",
    "arxiv_url": "https://arxiv.org/abs/2512.13427"
  }
]