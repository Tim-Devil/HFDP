[
  {
    "title": "Lumine: An Open Recipe for Building Generalist Agents in 3D Open Worlds",
    "summary": "We introduce Lumine, the first open recipe for developing generalist agents capable of completing hours-long complex missions in real time within challenging 3D open-world environments. Lumine adopts a human-like interaction paradigm that unifies perception, reasoning, and action in an end-to-end manner, powered by a vision-language model. It processes raw pixels at 5 Hz to produce precise 30 Hz keyboard-mouse actions and adaptively invokes reasoning only when necessary. Trained in Genshin Impact, Lumine successfully completes the entire five-hour Mondstadt main storyline on par with human-level efficiency and follows natural language instructions to perform a broad spectrum of tasks in both 3D open-world exploration and 2D GUI manipulation across collection, combat, puzzle-solving, and NPC interaction. In addition to its in-domain performance, Lumine demonstrates strong zero-shot cross-game generalization. Without any fine-tuning, it accomplishes 100-minute missions in Wuthering Waves and the full five-hour first chapter of Honkai: Star Rail. These promising results highlight Lumine's effectiveness across distinct worlds and interaction dynamics, marking a concrete step toward generalist agents in open-ended environments.",
    "translation": "标题：Lumine：构建3D开放世界通用智能体的开放方案\n\n摘要：本文提出Lumine——首个用于开发通用智能体的开放方案，该方案能在具有挑战性的3D开放世界环境中实时完成长达数小时的复杂任务。Lumine采用类人交互范式，通过视觉语言模型驱动，以端到端方式统一感知、推理与行动。该系统以5赫兹频率处理原始像素数据，生成精确的30赫兹键鼠操作，并仅在必要时自适应触发推理机制。通过在《原神》中的训练，Lumine成功以媲美人类效率的水平完成长达五小时的蒙德城主线剧情，并能依据自然语言指令在3D开放世界探索与2D图形界面操作中执行采集、战斗、解谜及NPC交互等多样化任务。除领域内优异表现外，Lumine展现出强大的零样本跨游戏泛化能力：未经微调即成功完成《鸣潮》中100分钟的任务流程与《崩坏：星穹铁道》首章五小时的全部内容。这些突破性成果彰显了Lumine在不同世界架构与交互机制中的卓越适应性，标志着开放环境下通用智能体研究迈出实质性一步。",
    "url": "https://huggingface.co/papers/2511.08892",
    "arxiv_url": "https://arxiv.org/abs/2511.08892"
  },
  {
    "title": "MADD: Multi-Agent Drug Discovery Orchestra",
    "summary": "Hit identification is a central challenge in early drug discovery, traditionally requiring substantial experimental resources. Recent advances in artificial intelligence, particularly large language models (LLMs), have enabled virtual screening methods that reduce costs and improve efficiency. However, the growing complexity of these tools has limited their accessibility to wet-lab researchers. Multi-agent systems offer a promising solution by combining the interpretability of LLMs with the precision of specialized models and tools. In this work, we present MADD, a multi-agent system that builds and executes customized hit identification pipelines from natural language queries. MADD employs four coordinated agents to handle key subtasks in de novo compound generation and screening. We evaluate MADD across seven drug discovery cases and demonstrate its superior performance compared to existing LLM-based solutions. Using MADD, we pioneer the application of AI-first drug design to five biological targets and release the identified hit molecules. Finally, we introduce a new benchmark of query-molecule pairs and docking scores for over three million compounds to contribute to the agentic future of drug design.",
    "translation": "标题：MADD：多智能体药物发现协同系统\n\n摘要：苗头化合物识别是早期药物发现的核心挑战，传统方法需要大量实验资源。人工智能尤其是大语言模型的最新进展，已实现可降低成本和提升效率的虚拟筛选方法。然而，这些工具日益增长的复杂性限制了湿实验室研究人员的使用。多智能体系统通过结合大语言模型的可解释性与专业模型工具的精确性，提供了具有前景的解决方案。本研究提出MADD多智能体系统，能够根据自然语言查询构建并执行定制化的苗头化合物识别流程。该系统采用四个协同智能体处理从头化合物生成与筛选中的关键子任务。通过对七个药物发现案例的评估，我们证明MADD相较现有基于大语言模型的解决方案具有更优性能。借助MADD，我们率先将AI优先的药物设计方法应用于五个生物靶点，并公开发布已识别的苗头化合物。最后，我们建立了包含三百余万种化合物的查询-分子对与对接评分新基准，以推动药物设计向智能体化未来发展。",
    "url": "https://huggingface.co/papers/2511.08217",
    "arxiv_url": "https://arxiv.org/abs/2511.08217"
  },
  {
    "title": "Time-to-Move: Training-Free Motion Controlled Video Generation via Dual-Clock Denoising",
    "summary": "Diffusion-based video generation can create realistic videos, yet existing image- and text-based conditioning fails to offer precise motion control. Prior methods for motion-conditioned synthesis typically require model-specific fine-tuning, which is computationally expensive and restrictive. We introduce Time-to-Move (TTM), a training-free, plug-and-play framework for motion- and appearance-controlled video generation with image-to-video (I2V) diffusion models. Our key insight is to use crude reference animations obtained through user-friendly manipulations such as cut-and-drag or depth-based reprojection. Motivated by SDEdit's use of coarse layout cues for image editing, we treat the crude animations as coarse motion cues and adapt the mechanism to the video domain. We preserve appearance with image conditioning and introduce dual-clock denoising, a region-dependent strategy that enforces strong alignment in motion-specified regions while allowing flexibility elsewhere, balancing fidelity to user intent with natural dynamics. This lightweight modification of the sampling process incurs no additional training or runtime cost and is compatible with any backbone. Extensive experiments on object and camera motion benchmarks show that TTM matches or exceeds existing training-based baselines in realism and motion control. Beyond this, TTM introduces a unique capability: precise appearance control through pixel-level conditioning, exceeding the limits of text-only prompting. Visit our project page for video examples and code: https://time-to-move.github.io/.",
    "translation": "标题：时序运动：基于双时钟去噪的无训练运动控制视频生成方法\n\n摘要：基于扩散模型的视频生成技术能够创建逼真的视频内容，但现有基于图像和文本的条件控制方法难以实现精确的运动控制。先前基于运动条件合成的方法通常需要进行模型特定微调，这种方法计算成本高昂且具有局限性。我们提出时序运动（TTM）——一种无需训练、即插即用的视频生成框架，通过图像到视频（I2V）扩散模型实现运动与外观的联合控制。我们的核心思路是利用通过用户友好操作（如剪切拖拽或基于深度的重投影）获取的粗略参考动画。受SDEdit使用粗略布局线索进行图像编辑的启发，我们将这些粗略动画视为运动提示线索，并将该机制适配到视频领域。通过图像条件保持外观一致性，并引入双时钟去噪策略——这种区域依赖方法在运动指定区域强制实现强对齐，同时在其余区域保持灵活性，有效平衡用户意图保真度与自然动态表现。这种轻量级采样过程修改无需额外训练或运行时成本，且兼容任何骨干网络。在物体运动和相机运动基准上的大量实验表明，TTM在真实感和运动控制方面达到或超越了现有基于训练的基线方法。此外，TTM还引入了独特能力：通过像素级条件实现精确外观控制，突破了纯文本提示的局限性。欢迎访问我们的项目页面查看视频示例和代码：https://time-to-move.github.io/。",
    "url": "https://huggingface.co/papers/2511.08633",
    "arxiv_url": "https://arxiv.org/abs/2511.08633"
  },
  {
    "title": "TiDAR: Think in Diffusion, Talk in Autoregression",
    "summary": "Diffusion language models hold the promise of fast parallel generation, while autoregressive (AR) models typically excel in quality due to their causal structure aligning naturally with language modeling. This raises a fundamental question: can we achieve a synergy with high throughput, higher GPU utilization, and AR level quality? Existing methods fail to effectively balance these two aspects, either prioritizing AR using a weaker model for sequential drafting (speculative decoding), leading to lower drafting efficiency, or using some form of left-to-right (AR-like) decoding logic for diffusion, which still suffers from quality degradation and forfeits its potential parallelizability. We introduce TiDAR, a sequence-level hybrid architecture that drafts tokens (Thinking) in Diffusion and samples final outputs (Talking) AutoRegressively - all within a single forward pass using specially designed structured attention masks. This design exploits the free GPU compute density, achieving a strong balance between drafting and verification capacity. Moreover, TiDAR is designed to be serving-friendly (low overhead) as a standalone model. We extensively evaluate TiDAR against AR models, speculative decoding, and diffusion variants across generative and likelihood tasks at 1.5B and 8B scales. Thanks to the parallel drafting and sampling as well as exact KV cache support, TiDAR outperforms speculative decoding in measured throughput and surpasses diffusion models like Dream and Llada in both efficiency and quality. Most notably, TiDAR is the first architecture to close the quality gap with AR models while delivering 4.71x to 5.91x more tokens per second.",
    "translation": "标题：TiDAR：扩散式思考，自回归式表达\n\n摘要：扩散语言模型具备快速并行生成的潜力，而自回归模型因其因果结构与语言建模天然契合，通常在生成质量上更胜一筹。这引出一个根本性问题：我们能否实现高吞吐量、高GPU利用率与自回归级质量的三重协同？现有方法未能有效平衡这两个方面：要么采用较弱模型进行序列草拟（如推测解码）而优先保障自回归特性，导致草拟效率低下；要么为扩散模型引入某种左到右（类自回归）解码逻辑，仍存在质量下降问题并丧失其并行潜力。我们提出TiDAR——一种序列级混合架构，通过特殊设计的结构化注意力掩码在单次前向传播中实现扩散式令牌草拟（思考）与自回归式最终输出采样（表达）。该设计充分利用空闲GPU计算密度，在草拟与验证能力间达成强力平衡。此外，TiDAR作为独立模型具备服务友好特性（低开销）。我们在1.5B和8B规模上，针对生成与似然任务对TiDAR、自回归模型、推测解码及扩散变体进行了全面评估。得益于并行草拟采样机制及精确KV缓存支持，TiDAR在实测吞吐量上超越推测解码，在效率与质量上均优于Dream、Llada等扩散模型。最显著的是，TiDAR成为首个在保持与自回归模型质量持平的同时，实现每秒生成令牌数提升4.71至5.91倍的架构。",
    "url": "https://huggingface.co/papers/2511.08923",
    "arxiv_url": "https://arxiv.org/abs/2511.08923"
  },
  {
    "title": "LoopTool: Closing the Data-Training Loop for Robust LLM Tool Calls",
    "summary": "Augmenting Large Language Models (LLMs) with external tools enables them to execute complex, multi-step tasks. However, tool learning is hampered by the static synthetic data pipelines where data generation and model training are executed as two separate, non-interactive processes. This approach fails to adaptively focus on a model's specific weaknesses and allows noisy labels to persist, degrading training efficiency. We introduce LoopTool, a fully automated, model-aware data evolution framework that closes this loop by tightly integrating data synthesis and model training. LoopTool iteratively refines both the data and the model through three synergistic modules: (1) Greedy Capability Probing (GCP) diagnoses the model's mastered and failed capabilities; (2) Judgement-Guided Label Verification (JGLV) uses an open-source judge model to find and correct annotation errors, progressively purifying the dataset; and (3) Error-Driven Data Expansion (EDDE) generates new, challenging samples based on identified failures. This closed-loop process operates within a cost-effective, open-source ecosystem, eliminating dependence on expensive closed-source APIs. Experiments show that our 8B model trained with LoopTool significantly surpasses its 32B data generator and achieves new state-of-the-art results on the BFCL-v3 and ACEBench benchmarks for its scale. Our work demonstrates that closed-loop, self-refining data pipelines can dramatically enhance the tool-use capabilities of LLMs.",
    "translation": "标题：LoopTool：构建数据-训练闭环以实现鲁棒的大语言模型工具调用\n\n摘要：通过外部工具增强大语言模型（LLMs）使其能够执行复杂的多步骤任务。然而当前工具学习受限于静态合成数据流程——数据生成与模型训练被割裂为两个独立非交互的过程。这种方法既无法自适应聚焦于模型的特定弱点，又放任噪声标签持续存在，最终降低训练效率。我们提出LoopTool这一全自动、模型感知的数据演化框架，通过紧密集成数据合成与模型训练构建完整闭环。该框架通过三个协同模块迭代优化数据与模型：（1）贪婪能力探测（GCP）诊断模型已掌握与失败的能力；（2）判别引导的标签验证（JGLV）使用开源评判模型发现并修正标注错误，逐步净化数据集；（3）错误驱动的数据扩展（EDDE）基于已识别的失败案例生成具有挑战性的新样本。这种闭环流程在成本可控的开源生态中运行，摆脱了对昂贵闭源API的依赖。实验表明，采用LoopTool训练的80亿参数模型显著超越其320亿参数的数据生成器，并在BFCL-v3和ACEBench基准测试中达到同规模模型的最优性能。我们的工作证明，闭环自优化的数据流程能显著增强大语言模型的工具使用能力。",
    "url": "https://huggingface.co/papers/2511.09148",
    "arxiv_url": "https://arxiv.org/abs/2511.09148"
  },
  {
    "title": "WMPO: World Model-based Policy Optimization for Vision-Language-Action Models",
    "summary": "Vision-Language-Action (VLA) models have shown strong potential for general-purpose robotic manipulation, but their reliance on expert demonstrations limits their ability to learn from failures and perform self-corrections. Reinforcement learning (RL) addresses these through self-improving interactions with the physical environment, but suffers from high sample complexity on real robots. We introduce World-Model-based Policy Optimization (WMPO), a principled framework for on-policy VLA RL without interacting with the real environment. In contrast to widely used latent world models, WMPO focuses on pixel-based predictions that align the \"imagined\" trajectories with the VLA features pretrained with web-scale images. Crucially, WMPO enables the policy to perform on-policy GRPO that provides stronger performance than the often-used off-policy methods. Extensive experiments in both simulation and real-robot settings demonstrate that WMPO (i) substantially improves sample efficiency, (ii) achieves stronger overall performance, (iii) exhibits emergent behaviors such as self-correction, and (iv) demonstrates robust generalization and lifelong learning capabilities.",
    "translation": "标题：WMPO：基于世界模型的视觉-语言-动作策略优化方法\n\n摘要：视觉-语言-动作模型在通用机器人操作任务中展现出巨大潜力，但其对专家示范数据的依赖限制了从失败中学习及执行自我修正的能力。强化学习通过与物理环境的自主交互实现自我改进，但在真实机器人上面临样本复杂度高的问题。本文提出基于世界模型的策略优化方法WMPO，该理论框架支持在线VLA强化学习而无需与真实环境交互。与广泛使用的潜空间世界模型不同，WMPO专注于基于像素的预测，使\"想象\"轨迹与通过网络规模图像预训练的VLA特征对齐。关键的是，WMPO使策略能够执行在线GRPO，其性能优于常用的离线方法。在仿真和真实机器人环境中的大量实验表明，WMPO具有以下优势：（i）显著提升样本效率；（ii）获得更优的整体性能；（iii）展现出自我修正等涌现行为；（iv）表现出强大的泛化能力和持续学习特性。",
    "url": "https://huggingface.co/papers/2511.09515",
    "arxiv_url": "https://arxiv.org/abs/2511.09515"
  },
  {
    "title": "MathSE: Improving Multimodal Mathematical Reasoning via Self-Evolving Iterative Reflection and Reward-Guided Fine-Tuning",
    "summary": "Multimodal large language models (MLLMs) have demonstrated remarkable capabilities in vision-language answering tasks. Despite their strengths, these models often encounter challenges in achieving complex reasoning tasks such as mathematical problem-solving. Previous works have focused on fine-tuning on specialized mathematical datasets. However, these datasets are typically distilled directly from teacher models, which capture only static reasoning patterns and leaving substantial gaps compared to student models. This reliance on fixed teacher-derived datasets not only restricts the model's ability to adapt to novel or more intricate questions that extend beyond the confines of the training data, but also lacks the iterative depth needed for robust generalization. To overcome these limitations, we propose \\method, a Mathematical Self-Evolving framework for MLLMs. In contrast to traditional one-shot fine-tuning paradigms, \\method iteratively refines the model through cycles of inference, reflection, and reward-based feedback. Specifically, we leverage iterative fine-tuning by incorporating correct reasoning paths derived from previous-stage inference and integrating reflections from a specialized Outcome Reward Model (ORM). To verify the effectiveness of \\method, we evaluate it on a suite of challenging benchmarks, demonstrating significant performance gains over backbone models. Notably, our experimental results on MathVL-test surpass the leading open-source multimodal mathematical reasoning model QVQ. Our code and models are available at https://zheny2751\\allowbreak-dotcom.github.io/\\allowbreak MathSE.github.io/.",
    "translation": "标题：MathSE：通过自演进迭代反思与奖励导向微调提升多模态数学推理能力\n\n摘要：多模态大语言模型在视觉语言问答任务中展现出卓越能力，然而在处理数学问题求解等复杂推理任务时仍面临挑战。现有研究主要集中于对专业数学数据集进行微调，但这些数据集通常直接源自教师模型的蒸馏输出，仅能捕捉静态推理模式，且与学生模型存在显著差距。这种依赖固定教师衍生数据集的方法不仅限制了模型适应训练数据范围之外的新颖或复杂问题的能力，更缺乏实现强泛化能力所需的迭代深度。为突破这些局限，我们提出MathSE——面向多模态大语言模型的数学自演进框架。与传统单次微调范式不同，MathSE通过推理、反思和基于奖励的反馈循环对模型进行迭代优化。具体而言，我们通过整合前阶段推理产生的正确解题路径，并融合专业结果奖励模型的反思信号，实现迭代式微调。为验证MathSE的有效性，我们在系列挑战性基准测试上进行评估，结果表明其相较基线模型取得显著性能提升。值得注意的是，在MathVL-test数据集上的实验结果超越了当前领先的开源多模态数学推理模型QVQ。相关代码与模型已发布于https://zheny2751-dotcom.github.io/MathSE.github.io/。",
    "url": "https://huggingface.co/papers/2511.06805",
    "arxiv_url": "https://arxiv.org/abs/2511.06805"
  },
  {
    "title": "WebVIA: A Web-based Vision-Language Agentic Framework for Interactive and Verifiable UI-to-Code Generation",
    "summary": "User interface (UI) development requires translating design mockups into functional code, a process that remains repetitive and labor-intensive. While recent Vision-Language Models (VLMs) automate UI-to-Code generation, they generate only static HTML/CSS/JavaScript layouts lacking interactivity. To address this, we propose WebVIA, the first agentic framework for interactive UI-to-Code generation and validation. The framework comprises three components: 1) an exploration agent to capture multi-state UI screenshots; 2) a UI2Code model that generates executable interactive code; 3) a validation module that verifies the interactivity. Experiments demonstrate that WebVIA-Agent achieves more stable and accurate UI exploration than general-purpose agents (e.g., Gemini-2.5-Pro). In addition, our fine-tuned WebVIA-UI2Code models exhibit substantial improvements in generating executable and interactive HTML/CSS/JavaScript code, outperforming their base counterparts across both interactive and static UI2Code benchmarks. Our code and models are available at https://zheny2751-dotcom.github.io/webvia.github.io/{https://webvia.github.io}.",
    "translation": "标题：WebVIA：基于Web的视觉语言智能体框架——面向交互式可验证的UI到代码生成\n\n摘要：用户界面（UI）开发需要将设计稿转换为功能代码，这一过程仍存在重复性强、劳动密集的问题。尽管当前视觉语言模型（VLMs）能自动实现UI到代码的生成，但其仅能生成缺乏交互性的静态HTML/CSS/JavaScript布局。为此，我们提出WebVIA——首个支持交互式UI到代码生成与验证的智能体框架。该框架包含三个核心组件：1）用于捕获多状态UI截图的探索智能体；2）生成可执行交互代码的UI2Code模型；3）验证交互功能的检测模块。实验表明，WebVIA智能体相较于通用智能体（如Gemini-2.5-Pro）能实现更稳定精准的UI探索。此外，我们微调后的WebVIA-UI2Code模型在生成可执行交互的HTML/CSS/JavaScript代码方面取得显著提升，在交互式和静态UI2Code基准测试中均优于其基础模型。代码与模型已开源：https://zheny2751-dotcom.github.io/webvia.github.io/{https://webvia.github.io}",
    "url": "https://huggingface.co/papers/2511.06251",
    "arxiv_url": "https://arxiv.org/abs/2511.06251"
  },
  {
    "title": "Toward the Frontiers of Reliable Diffusion Sampling via Adversarial Sinkhorn Attention Guidance",
    "summary": "Diffusion models have demonstrated strong generative performance when using guidance methods such as classifier-free guidance (CFG), which enhance output quality by modifying the sampling trajectory. These methods typically improve a target output by intentionally degrading another, often the unconditional output, using heuristic perturbation functions such as identity mixing or blurred conditions. However, these approaches lack a principled foundation and rely on manually designed distortions. In this work, we propose Adversarial Sinkhorn Attention Guidance (ASAG), a novel method that reinterprets attention scores in diffusion models through the lens of optimal transport and intentionally disrupt the transport cost via Sinkhorn algorithm. Instead of naively corrupting the attention mechanism, ASAG injects an adversarial cost within self-attention layers to reduce pixel-wise similarity between queries and keys. This deliberate degradation weakens misleading attention alignments and leads to improved conditional and unconditional sample quality. ASAG shows consistent improvements in text-to-image diffusion, and enhances controllability and fidelity in downstream applications such as IP-Adapter and ControlNet. The method is lightweight, plug-and-play, and improves reliability without requiring any model retraining.",
    "translation": "标题：迈向可靠扩散采样的前沿：基于对抗性Sinkhorn注意力引导的方法\n\n摘要：扩散模型在使用分类器无引导等指导方法时展现出强大的生成性能，这类方法通过修改采样轨迹来提升输出质量。现有方法通常采用启发式扰动函数（如恒等混合或模糊条件）刻意劣化无条件输出来增强目标输出，但缺乏理论依据且依赖人工设计的畸变。本研究提出对抗性Sinkhorn注意力引导（ASAG），该方法通过最优传输理论重新阐释扩散模型中的注意力机制，并利用Sinkhorn算法主动干扰传输成本。ASAG并非简单破坏注意力机制，而是在自注意力层中注入对抗性成本以降低查询向量与键向量的像素级相似度。这种刻意劣化有效削弱了误导性注意力对齐，从而提升条件与非条件样本的生成质量。ASAG在文本到图像生成任务中表现出稳定的性能提升，并在IP-Adapter、ControlNet等下游应用中增强了可控性与保真度。该方法具有轻量化、即插即用特性，无需重新训练模型即可提升生成可靠性。",
    "url": "https://huggingface.co/papers/2511.07499",
    "arxiv_url": "https://arxiv.org/abs/2511.07499"
  },
  {
    "title": "Adapting Web Agents with Synthetic Supervision",
    "summary": "Web agents struggle to adapt to new websites due to the scarcity of environment specific tasks and demonstrations. Recent works have explored synthetic data generation to address this challenge, however, they suffer from data quality issues where synthesized tasks contain hallucinations that cannot be executed, and collected trajectories are noisy with redundant or misaligned actions. In this paper, we propose SynthAgent, a fully synthetic supervision framework that aims at improving synthetic data quality via dual refinement of both tasks and trajectories. Our approach begins by synthesizing diverse tasks through categorized exploration of web elements, ensuring efficient coverage of the target environment. During trajectory collection, we refine tasks when conflicts with actual observations are detected, mitigating hallucinations while maintaining task consistency. After collection, we conduct trajectory refinement with a global context to mitigate potential noise or misalignments. Finally, we fine-tune open-source web agents on the refined synthetic data to adapt them to the target environment. Experimental results demonstrate that SynthAgent outperforms existing synthetic data methods, validating the importance of high-quality synthetic supervision. The code will be publicly available at https://github.com/aiming-lab/SynthAgent.",
    "translation": "标题：基于合成监督的网页智能体自适应方法\n\n摘要：由于特定环境任务及示范数据的稀缺，网页智能体在新网站上的适应能力面临挑战。现有研究尝试通过合成数据生成解决该问题，但存在数据质量缺陷：合成任务常包含无法执行的幻象内容，而采集的行为轨迹则存在冗余或错位等噪声。本文提出SynthAgent——一个通过任务与轨迹双重优化的全合成监督框架。该方法首先通过网页元素分类探索来合成多样化任务，确保对目标环境的高效覆盖。在轨迹采集过程中，当检测到任务与实际观察存在冲突时，系统会对任务进行实时优化，在保持任务一致性的同时消除幻象。采集完成后，我们基于全局上下文进行轨迹优化以消除潜在噪声与错位。最终，利用优化后的合成数据对开源网页智能体进行微调，使其适应目标环境。实验结果表明，SynthAgent在性能上超越现有合成数据方法，验证了高质量合成监督的重要性。代码将在https://github.com/aiming-lab/SynthAgent 公开。",
    "url": "https://huggingface.co/papers/2511.06101",
    "arxiv_url": "https://arxiv.org/abs/2511.06101"
  },
  {
    "title": "Agentic Refactoring: An Empirical Study of AI Coding Agents",
    "summary": "Agentic coding tools, such as OpenAI Codex, Claude Code, and Cursor, are transforming the software engineering landscape. These AI-powered systems function as autonomous teammates capable of planning and executing complex development tasks. Agents have become active participants in refactoring, a cornerstone of sustainable software development aimed at improving internal code quality without altering observable behavior. Despite their increasing adoption, there is a critical lack of empirical understanding regarding how agentic refactoring is utilized in practice, how it compares to human-driven refactoring, and what impact it has on code quality. To address this empirical gap, we present a large-scale study of AI agent-generated refactorings in real-world open-source Java projects, analyzing 15,451 refactoring instances across 12,256 pull requests and 14,988 commits derived from the AIDev dataset. Our empirical analysis shows that refactoring is a common and intentional activity in this development paradigm, with agents explicitly targeting refactoring in 26.1% of commits. Analysis of refactoring types reveals that agentic efforts are dominated by low-level, consistency-oriented edits, such as Change Variable Type (11.8%), Rename Parameter (10.4%), and Rename Variable (8.5%), reflecting a preference for localized improvements over the high-level design changes common in human refactoring. Additionally, the motivations behind agentic refactoring focus overwhelmingly on internal quality concerns, with maintainability (52.5%) and readability (28.1%). Furthermore, quantitative evaluation of code quality metrics shows that agentic refactoring yields small but statistically significant improvements in structural metrics, particularly for medium-level changes, reducing class size and complexity (e.g., Class LOC median Δ = -15.25).",
    "translation": "标题：智能体重构实践：AI编程智能体的实证研究\n\n摘要：以OpenAI Codex、Claude Code和Cursor为代表的智能编程工具正在重塑软件工程领域。这些AI驱动的系统作为自主协作伙伴，能够规划并执行复杂的开发任务。在重构这一旨在提升代码内在质量而不改变外部行为的可持续软件开发核心实践中，智能体已成为积极参与者。尽管应用日益广泛，但关于智能体重构的实际应用模式、与人工重构的差异及其对代码质量的影响仍缺乏实证研究。为填补这一空白，我们基于AIDev数据集开展了大规模实证研究，分析了真实开源Java项目中12,256个拉取请求和14,988次提交产生的15,451个重构实例。实证分析表明：重构已成为该开发范式中的常规 intentional 活动，26.1%的提交明确以重构为目标；重构类型分析显示智能体主导低层次、一致性导向的修改，其中变更变量类型（11.8%）、重命名参数（10.4%）和重命名变量（8.5%）最为突出，反映出其更倾向于局部优化而非人类重构常见的高层设计变更；重构动机高度集中于内在质量诉求，可维护性（52.5%）与可读性（28.1%）占比最高；代码质量指标的定量评估表明，智能体重构虽改善幅度有限但具有统计显著性，特别是在中等规模变更中有效降低了类的规模与复杂度（如类代码行数中位数变化量Δ=-15.25）。",
    "url": "https://huggingface.co/papers/2511.04824",
    "arxiv_url": "https://arxiv.org/abs/2511.04824"
  },
  {
    "title": "Motif 2 12.7B technical report",
    "summary": "We introduce Motif-2-12.7B, a new open-weight foundation model that pushes the efficiency frontier of large language models by combining architectural innovation with system-level optimization. Designed for scalable language understanding and robust instruction generalization under constrained compute budgets, Motif-2-12.7B builds upon Motif-2.6B with the integration of Grouped Differential Attention (GDA), which improves representational efficiency by disentangling signal and noise-control attention pathways. The model is pre-trained on 5.5 trillion tokens spanning diverse linguistic, mathematical, scientific, and programming domains using a curriculum-driven data scheduler that gradually changes the data composition ratio. The training system leverages the MuonClip optimizer alongside custom high-performance kernels, including fused PolyNorm activations and the Parallel Muon algorithm, yielding significant throughput and memory efficiency gains in large-scale distributed environments. Post-training employs a three-stage supervised fine-tuning pipeline that successively enhances general instruction adherence, compositional understanding, and linguistic precision. Motif-2-12.7B demonstrates competitive performance across diverse benchmarks, showing that thoughtful architectural scaling and optimized training design can rival the capabilities of much larger models.",
    "translation": "标题：Motif-2-12.7B技术报告\n\n摘要：本文介绍Motif-2-12.7B——一种新型开放权重基础模型，该模型通过架构创新与系统级优化的结合，突破了大型语言模型的效率边界。该模型专为在有限计算预算下实现可扩展的语言理解和稳健的指令泛化能力而设计，在Motif-2.6B架构基础上集成差分分组注意力机制（GDA），通过解耦信号通路与噪声控制注意力路径来提升表征效率。模型基于课程驱动型数据调度策略，在涵盖多语言、数学、科学及编程领域的5.5万亿标记上进行了预训练，该策略会动态调整数据组成比例。训练系统采用MuonClip优化器及定制高性能内核，包括融合PolyNorm激活函数与并行Muon算法，在大规模分布式环境中实现了显著的吞吐量与内存效率提升。后训练阶段采用三阶段监督微调流程，依次增强通用指令遵循能力、组合理解能力与语言精确性。实验表明，Motif-2-12.7B在多项基准测试中展现出竞争优势，证明经过精密设计的架构扩展与优化训练方案能够媲美规模更大的模型性能。",
    "url": "https://huggingface.co/papers/2511.07464",
    "arxiv_url": "https://arxiv.org/abs/2511.07464"
  },
  {
    "title": "Stemming Hallucination in Language Models Using a Licensing Oracle",
    "summary": "Language models exhibit remarkable natural language generation capabilities\nbut remain prone to hallucinations, generating factually incorrect information\ndespite producing syntactically coherent responses. This study introduces the\nLicensing Oracle, an architectural solution designed to stem hallucinations in\nLMs by enforcing truth constraints through formal validation against structured\nknowledge graphs. Unlike statistical approaches that rely on data scaling or\nfine-tuning, the Licensing Oracle embeds a deterministic validation step into\nthe model's generative process, ensuring that only factually accurate claims\nare made. We evaluated the effectiveness of the Licensing Oracle through\nexperiments comparing it with several state-of-the-art methods, including\nbaseline language model generation, fine-tuning for factual recall, fine-tuning\nfor abstention behavior, and retrieval-augmented generation (RAG). Our results\ndemonstrate that although RAG and fine-tuning improve performance, they fail to\neliminate hallucinations. In contrast, the Licensing Oracle achieved perfect\nabstention precision (AP = 1.0) and zero false answers (FAR-NE = 0.0), ensuring\nthat only valid claims were generated with 89.1% accuracy in factual responses.\nThis work shows that architectural innovations, such as the Licensing Oracle,\noffer a necessary and sufficient solution for hallucinations in domains with\nstructured knowledge representations, offering guarantees that statistical\nmethods cannot match. Although the Licensing Oracle is specifically designed to\naddress hallucinations in fact-based domains, its framework lays the groundwork\nfor truth-constrained generation in future AI systems, providing a new path\ntoward reliable, epistemically grounded models.",
    "translation": "标题：基于授权验证机制抑制语言模型幻觉现象的研究\n\n摘要：语言模型展现出卓越的自然语言生成能力，但仍存在幻觉现象——尽管能生成语法连贯的响应，却时常产生事实性错误信息。本研究提出授权验证机制这一架构解决方案，通过基于结构化知识图谱的形式化验证来实施真实性约束，从而抑制语言模型中的幻觉生成。与依赖数据扩展或微调的统计方法不同，该机制在模型生成过程中嵌入确定性验证步骤，确保仅产生事实准确的论断。我们通过对比实验评估该机制的有效性，参与对比的包括基线语言模型生成、事实召回微调、弃答行为微调以及检索增强生成等方法。实验结果表明：尽管检索增强生成与微调能提升性能，但均无法完全消除幻觉；而授权验证机制实现了完美的弃答精度（AP=1.0）和零错误答案率（FAR-NE=0.0），在事实性响应中确保以89.1%的准确率生成有效论断。本研究表明，对于具有结构化知识表示的领域，授权验证机制这类架构创新为消除幻觉提供了充分必要条件，其保障效果是统计方法无法实现的。尽管该机制专门针对事实性领域的幻觉问题设计，但其框架为未来人工智能系统中实现真实性约束生成奠定了基础，为构建可靠且具有认知依据的模型开辟了新路径。",
    "url": "https://huggingface.co/papers/2511.06073",
    "arxiv_url": "https://arxiv.org/abs/2511.06073"
  }
]