[
  {
    "title": "Kandinsky 5.0: A Family of Foundation Models for Image and Video Generation",
    "summary": "This report introduces Kandinsky 5.0, a family of state-of-the-art foundation models for high-resolution image and 10-second video synthesis. The framework comprises three core line-up of models: Kandinsky 5.0 Image Lite - a line-up of 6B parameter image generation models, Kandinsky 5.0 Video Lite - a fast and lightweight 2B parameter text-to-video and image-to-video models, and Kandinsky 5.0 Video Pro - 19B parameter models that achieves superior video generation quality. We provide a comprehensive review of the data curation lifecycle - including collection, processing, filtering and clustering - for the multi-stage training pipeline that involves extensive pre-training and incorporates quality-enhancement techniques such as self-supervised fine-tuning (SFT) and reinforcement learning (RL)-based post-training. We also present novel architectural, training, and inference optimizations that enable Kandinsky 5.0 to achieve high generation speeds and state-of-the-art performance across various tasks, as demonstrated by human evaluation. As a large-scale, publicly available generative framework, Kandinsky 5.0 leverages the full potential of its pre-training and subsequent stages to be adapted for a wide range of generative applications. We hope that this report, together with the release of our open-source code and training checkpoints, will substantially advance the development and accessibility of high-quality generative models for the research community.",
    "translation": "标题：Kandinsky 5.0：面向图像与视频生成的基座模型系列\n\n摘要：本报告介绍Kandinsky 5.0——一系列面向高分辨率图像与10秒视频合成的尖端基座模型。该框架包含三大核心模型组合：Kandinsky 5.0图像轻量版——包含60亿参数的图像生成模型系列；Kandinsky 5.0视频轻量版——快速轻量的20亿参数文生视频与图生视频模型；以及Kandinsky 5.0视频专业版——190亿参数的高品质视频生成模型。我们系统阐述了多阶段训练流程中的数据治理生命周期（涵盖收集、处理、筛选与聚类），该流程包含大规模预训练阶段，并融合了自监督微调（SFT）与基于强化学习（RL）的后训练等质量增强技术。同时，我们提出了创新的架构设计、训练方法与推理优化方案，使Kandinsky 5.0在人类评估中展现出高速生成能力与多任务的顶尖性能。作为大规模开源生成框架，Kandinsky 5.0充分发挥预训练及后续阶段的潜力，可适配多样化的生成应用场景。我们期待通过本报告及同步开源的核心代码与训练检查点，显著推动高质量生成模型在科研领域的发展与普及。",
    "url": "https://huggingface.co/papers/2511.14993",
    "arxiv_url": "https://arxiv.org/abs/2511.14993"
  },
  {
    "title": "Reasoning via Video: The First Evaluation of Video Models' Reasoning Abilities through Maze-Solving Tasks",
    "summary": "Video Models have achieved remarkable success in high-fidelity video generation with coherent motion dynamics. Analogous to the development from text generation to text-based reasoning in language modeling, the development of video models motivates us to ask: Can video models reason via video generation? Compared with the discrete text corpus, video grounds reasoning in explicit spatial layouts and temporal continuity, which serves as an ideal substrate for spatial reasoning. In this work, we explore the reasoning via video paradigm and introduce VR-Bench -- a comprehensive benchmark designed to systematically evaluate video models' reasoning capabilities. Grounded in maze-solving tasks that inherently require spatial planning and multi-step reasoning, VR-Bench contains 7,920 procedurally generated videos across five maze types and diverse visual styles. Our empirical analysis demonstrates that SFT can efficiently elicit the reasoning ability of video model. Video models exhibit stronger spatial perception during reasoning, outperforming leading VLMs and generalizing well across diverse scenarios, tasks, and levels of complexity. We further discover a test-time scaling effect, where diverse sampling during inference improves reasoning reliability by 10--20%. These findings highlight the unique potential and scalability of reasoning via video for spatial reasoning tasks.",
    "translation": "标题：基于视频的推理：通过迷宫求解任务首次评估视频模型的推理能力\n\n摘要：视频模型在具有连贯运动动态的高保真视频生成领域已取得显著成功。类似于语言建模从文本生成到基于文本推理的发展历程，视频模型的进步促使我们思考：视频模型能否通过视频生成进行推理？与离散的文本语料相比，视频将推理锚定在明确的空间布局和时间连续性中，这使其成为空间推理的理想载体。本研究探索基于视频的推理范式，并推出VR-Bench——一个系统评估视频模型推理能力的综合基准。该基准以迷宫求解任务为基础，此类任务本质要求空间规划与多步推理能力，涵盖5种迷宫类型和多样化视觉风格，共包含7,920个程序化生成的视频。实证分析表明，监督微调能有效激发视频模型的推理能力。视频模型在推理过程中展现出更强的空间感知能力，其表现超越主流视觉语言模型，并在多样化场景、任务及复杂度层级中均具有良好的泛化性能。我们还发现测试时规模效应——推理阶段采用多样化采样可使推理可靠性提升10-20%。这些发现彰显了基于视频的推理方法在空间推理任务中独特的潜力和可扩展性。",
    "url": "https://huggingface.co/papers/2511.15065",
    "arxiv_url": "https://arxiv.org/abs/2511.15065"
  },
  {
    "title": "What Does It Take to Be a Good AI Research Agent? Studying the Role of Ideation Diversity",
    "summary": "AI research agents offer the promise to accelerate scientific progress by automating the design, implementation, and training of machine learning models. However, the field is still in its infancy, and the key factors driving the success or failure of agent trajectories are not fully understood. We examine the role that ideation diversity plays in agent performance. First, we analyse agent trajectories on MLE-bench, a well-known benchmark to evaluate AI research agents, across different models and agent scaffolds. Our analysis reveals that different models and agent scaffolds yield varying degrees of ideation diversity, and that higher-performing agents tend to have increased ideation diversity. Further, we run a controlled experiment where we modify the degree of ideation diversity, demonstrating that higher ideation diversity results in stronger performance. Finally, we strengthen our results by examining additional evaluation metrics beyond the standard medal-based scoring of MLE-bench, showing that our findings still hold across other agent performance metrics.",
    "translation": "标题：优秀AI研究智能体需具备何种特质？探讨构思多样性的作用\n\n摘要：AI研究智能体通过自动化机器学习模型的设计、实现与训练，为加速科学进步提供了可能。然而该领域仍处于发展初期，驱动智能体轨迹成败的关键因素尚未被完全理解。本文研究了构思多样性在智能体性能中发挥的作用。首先，我们分析了不同模型与智能体框架在MLE-bench（评估AI研究智能体的知名基准）上的运行轨迹。分析表明：不同模型与智能体框架会产生不同程度的构思多样性，且性能越优异的智能体往往展现出更高的构思多样性。进而，我们通过控制实验调节构思多样性程度，证明提高构思多样性可显著增强智能体性能。最后，我们突破MLE-bench基于奖牌评分的标准指标体系，通过附加评估指标验证了研究结论的稳健性——在不同智能体性能度量标准下，我们的发现依然成立。",
    "url": "https://huggingface.co/papers/2511.15593",
    "arxiv_url": "https://arxiv.org/abs/2511.15593"
  },
  {
    "title": "VisPlay: Self-Evolving Vision-Language Models from Images",
    "summary": "Reinforcement learning (RL) provides a principled framework for improving Vision-Language Models (VLMs) on complex reasoning tasks. However, existing RL approaches often rely on human-annotated labels or task-specific heuristics to define verifiable rewards, both of which are costly and difficult to scale. We introduce VisPlay, a self-evolving RL framework that enables VLMs to autonomously improve their reasoning abilities using large amounts of unlabeled image data. Starting from a single base VLM, VisPlay assigns the model into two interacting roles: an Image-Conditioned Questioner that formulates challenging yet answerable visual questions, and a Multimodal Reasoner that generates silver responses. These roles are jointly trained with Group Relative Policy Optimization (GRPO), which incorporates diversity and difficulty rewards to balance the complexity of generated questions with the quality of the silver answers. VisPlay scales efficiently across two model families. When trained on Qwen2.5-VL and MiMo-VL, VisPlay achieves consistent improvements in visual reasoning, compositional generalization, and hallucination reduction across eight benchmarks, including MM-Vet and MMMU, demonstrating a scalable path toward self-evolving multimodal intelligence. The project page is available at https://bruno686.github.io/VisPlay/",
    "translation": "标题：VisPlay：基于图像自演进的多模态视觉语言模型\n\n摘要：强化学习为提升视觉语言模型在复杂推理任务中的性能提供了理论框架。然而现有强化学习方法通常依赖人工标注标签或任务特定启发式规则来定义可验证奖励，这两种方式均成本高昂且难以扩展。我们提出VisPlay——一种自演进强化学习框架，使视觉语言模型能够利用大量未标注图像数据自主提升推理能力。该框架以单一基础视觉语言模型为起点，将其分配至两个交互角色：图像条件提问器负责构建具有挑战性但可回答的视觉问题，多模态推理器则生成银标答案。通过群体相对策略优化算法对这两个角色进行联合训练，该算法融合多样性与难度奖励机制，以平衡生成问题的复杂程度与银标答案的质量。VisPlay在Qwen2.5-VL和MiMo-VL两大模型系列中均展现出良好的扩展性。在MM-Vet和MMMU等八个基准测试中，该框架在视觉推理、组合泛化及幻觉抑制方面均取得持续提升，为自演进多模态智能的发展提供了可扩展路径。项目页面详见：https://bruno686.github.io/VisPlay/",
    "url": "https://huggingface.co/papers/2511.15661",
    "arxiv_url": "https://arxiv.org/abs/2511.15661"
  },
  {
    "title": "Instruction-Guided Lesion Segmentation for Chest X-rays with Automatically Generated Large-Scale Dataset",
    "summary": "The applicability of current lesion segmentation models for chest X-rays (CXRs) has been limited both by a small number of target labels and the reliance on long, detailed expert-level text inputs, creating a barrier to practical use. To address these limitations, we introduce a new paradigm: instruction-guided lesion segmentation (ILS), which is designed to segment diverse lesion types based on simple, user-friendly instructions. Under this paradigm, we construct MIMIC-ILS, the first large-scale instruction-answer dataset for CXR lesion segmentation, using our fully automated multimodal pipeline that generates annotations from chest X-ray images and their corresponding reports. MIMIC-ILS contains 1.1M instruction-answer pairs derived from 192K images and 91K unique segmentation masks, covering seven major lesion types. To empirically demonstrate its utility, we introduce ROSALIA, a vision-language model fine-tuned on MIMIC-ILS. ROSALIA can segment diverse lesions and provide textual explanations in response to user instructions. The model achieves high segmentation and textual accuracy in our newly proposed task, highlighting the effectiveness of our pipeline and the value of MIMIC-ILS as a foundational resource for pixel-level CXR lesion grounding.",
    "translation": "标题：基于自动生成大规模数据集的指令引导胸部X光病灶分割  \n\n摘要：当前胸部X光病灶分割模型的应用受限于目标标签数量稀少及依赖冗长的专家级文本输入，这为实际应用带来了障碍。为解决这些局限性，我们提出了一种新范式：指令引导病灶分割，该范式旨在基于简单易用的用户指令分割多种病灶类型。在此范式下，我们通过全自动多模态流程构建了首个面向胸部X光病灶分割的大规模指令-答案数据集MIMIC-ILS，该流程能够根据胸部X光图像及其对应报告自动生成标注。MIMIC-ILS包含源自19.2万张图像和9.1万个独立分割掩码的110万条指令-答案对，涵盖七种主要病灶类型。为实证验证其效用，我们提出了基于MIMIC-ILS微调的视觉语言模型ROSALIA。该模型能够根据用户指令分割多种病灶并提供文本解释。在我们新提出的任务中，该模型实现了较高的分割精度与文本描述准确性，充分证明了我们流程的有效性，以及MIMIC-ILS作为像素级胸部X光病灶定位基础资源的重要价值。",
    "url": "https://huggingface.co/papers/2511.15186",
    "arxiv_url": "https://arxiv.org/abs/2511.15186"
  },
  {
    "title": "ARC-Chapter: Structuring Hour-Long Videos into Navigable Chapters and Hierarchical Summaries",
    "summary": "The proliferation of hour-long videos (e.g., lectures, podcasts, documentaries) has intensified demand for efficient content structuring. However, existing approaches are constrained by small-scale training with annotations that are typical short and coarse, restricting generalization to nuanced transitions in long videos. We introduce ARC-Chapter, the first large-scale video chaptering model trained on over million-level long video chapters, featuring bilingual, temporally grounded, and hierarchical chapter annotations. To achieve this goal, we curated a bilingual English-Chinese chapter dataset via a structured pipeline that unifies ASR transcripts, scene texts, visual captions into multi-level annotations, from short title to long summaries. We demonstrate clear performance improvements with data scaling, both in data volume and label intensity. Moreover, we design a new evaluation metric termed GRACE, which incorporates many-to-one segment overlaps and semantic similarity, better reflecting real-world chaptering flexibility. Extensive experiments demonstrate that ARC-Chapter establishes a new state-of-the-art by a significant margin, outperforming the previous best by 14.0% in F1 score and 11.3% in SODA score. Moreover, ARC-Chapter shows excellent transferability, improving the state-of-the-art on downstream tasks like dense video captioning on YouCook2.",
    "translation": "标题：ARC-Chapter：将小时级视频结构化构建为可导航章节与层级化摘要\n\n摘要：小时级长视频（如讲座、播客、纪录片）的激增强化了对高效内容结构化的需求。然而现有方法受限于小规模标注训练，其标注通常简短粗糙，难以泛化至长视频中的细微内容转换。我们提出ARC-Chapter——首个基于百万级长视频章节训练的大规模视频分章模型，其特点在于包含双语、时序锚定和层次化的章节标注。为实现这一目标，我们通过结构化流程构建了英汉双语章节数据集，将语音转录文本、场景文字和视觉描述统一整合为从简短标题到详细摘要的多层级标注。我们通过数据规模（数据量与标注强度）的扩展证明了明显的性能提升。此外，我们设计了名为GRACE的新型评估指标，该指标综合考量多对一的片段重叠度与语义相似性，能更好反映实际分章任务的灵活性。大量实验表明，ARC-Chapter以显著优势创下新性能纪录，F1分数较先前最佳方法提升14.0%，SODA分数提升11.3%。值得注意的是，ARC-Chapter展现出卓越的迁移学习能力，在YouCook2密集视频描述等下游任务中同样提升了现有最佳性能。",
    "url": "https://huggingface.co/papers/2511.14349",
    "arxiv_url": "https://arxiv.org/abs/2511.14349"
  },
  {
    "title": "MHR: Momentum Human Rig",
    "summary": "We present MHR, a parametric human body model that combines the decoupled skeleton/shape paradigm of ATLAS with a flexible, modern rig and pose corrective system inspired by the Momentum library. Our model enables expressive, anatomically plausible human animation, supporting non-linear pose correctives, and is designed for robust integration in AR/VR and graphics pipelines.",
    "translation": "标题：MHR：动量人体骨骼系统\n\n摘要：本文提出MHR参数化人体模型，该模型融合了ATLAS架构的解耦骨骼/形态范式，并借鉴Momentum函数库构建了具有灵活性的现代骨骼绑定与姿态校正系统。该模型能够生成表现力丰富且符合解剖学原理的人体动画，支持非线性姿态校正，其设计可稳健集成于增强现实/虚拟现实及图形处理流程中。",
    "url": "https://huggingface.co/papers/2511.15586",
    "arxiv_url": "https://arxiv.org/abs/2511.15586"
  },
  {
    "title": "FreeAskWorld: An Interactive and Closed-Loop Simulator for Human-Centric Embodied AI",
    "summary": "As embodied intelligence emerges as a core frontier in artificial intelligence research, simulation platforms must evolve beyond low-level physical interactions to capture complex, human-centered social behaviors. We introduce FreeAskWorld, an interactive simulation framework that integrates large language models (LLMs) for high-level behavior planning and semantically grounded interaction, informed by theories of intention and social cognition. Our framework supports scalable, realistic human-agent simulations and includes a modular data generation pipeline tailored for diverse embodied tasks.To validate the framework, we extend the classic Vision-and-Language Navigation (VLN) task into a interaction enriched Direction Inquiry setting, wherein agents can actively seek and interpret navigational guidance. We present and publicly release FreeAskWorld, a large-scale benchmark dataset comprising reconstructed environments, six diverse task types, 16 core object categories, 63,429 annotated sample frames, and more than 17 hours of interaction data to support training and evaluation of embodied AI systems. We benchmark VLN models, and human participants under both open-loop and closed-loop settings. Experimental results demonstrate that models fine-tuned on FreeAskWorld outperform their original counterparts, achieving enhanced semantic understanding and interaction competency. These findings underscore the efficacy of socially grounded simulation frameworks in advancing embodied AI systems toward sophisticated high-level planning and more naturalistic human-agent interaction. Importantly, our work underscores that interaction itself serves as an additional information modality.",
    "translation": "标题：FreeAskWorld：一种以人为中心的具身AI交互式闭环仿真平台\n\n摘要：随着具身智能成为人工智能研究的核心前沿，仿真平台必须超越低层次物理交互，以捕捉复杂的人类中心社会行为。我们提出FreeAskWorld——一个融合大语言模型进行高层次行为规划与语义接地交互的仿真框架，其设计基于意图理论与社会认知理论。该框架支持可扩展、逼真的人机交互仿真，并包含专为多样化具身任务设计的模块化数据生成流程。为验证框架性能，我们将经典视觉语言导航任务扩展为交互增强的方位问询场景，使智能体能够主动寻求并解析导航指引。我们公开发布FreeAskWorld大规模基准数据集，包含重构环境、六类任务形态、16个核心物体类别、63,429帧标注样本及超过17小时的交互数据，为具身AI系统的训练与评估提供支持。通过对视觉语言导航模型和人类参与者进行开环与闭环设置的基准测试，实验结果表明：基于FreeAskWorld微调的模型显著优于原模型，在语义理解与交互能力方面均获得提升。这些发现印证了社会情境化仿真框架在推动具身AI系统实现高级规划与自然人机交互方面的有效性。特别需要指出的是，本研究揭示了交互本身可作为独立的信息模态这一重要特性。",
    "url": "https://huggingface.co/papers/2511.13524",
    "arxiv_url": "https://arxiv.org/abs/2511.13524"
  },
  {
    "title": "RoMa v2: Harder Better Faster Denser Feature Matching",
    "summary": "Dense feature matching aims to estimate all correspondences between two images of a 3D scene and has recently been established as the gold-standard due to its high accuracy and robustness. However, existing dense matchers still fail or perform poorly for many hard real-world scenarios, and high-precision models are often slow, limiting their applicability. In this paper, we attack these weaknesses on a wide front through a series of systematic improvements that together yield a significantly better model. In particular, we construct a novel matching architecture and loss, which, combined with a curated diverse training distribution, enables our model to solve many complex matching tasks. We further make training faster through a decoupled two-stage matching-then-refinement pipeline, and at the same time, significantly reduce refinement memory usage through a custom CUDA kernel. Finally, we leverage the recent DINOv3 foundation model along with multiple other insights to make the model more robust and unbiased. In our extensive set of experiments we show that the resulting novel matcher sets a new state-of-the-art, being significantly more accurate than its predecessors. Code is available at https://github.com/Parskatt/romav2",
    "translation": "标题：RoMa v2：更强大、更优质、更快速、更密集的特征匹配\n\n摘要：密集特征匹配旨在估计三维场景两幅图像间的所有对应关系，因其高精度与强鲁棒性已成为当前黄金标准。然而现有密集匹配器在诸多复杂现实场景中仍存在失效或性能不佳的问题，且高精度模型往往速度迟缓，限制了实际应用。本文通过一系列系统性改进多管齐下攻克这些缺陷，最终形成显著优化的模型。我们构建了新颖的匹配架构与损失函数，结合精心策划的多样化训练数据分布，使模型能够解决众多复杂匹配任务。通过解耦的“先匹配后优化”两阶段流程，我们进一步加速训练过程，同时借助定制CUDA内核显著降低优化阶段内存占用。此外，我们利用近期提出的DINOv3基础模型并融合多项创新洞见，有效提升模型鲁棒性与无偏性。在大量实验验证中，新型匹配器创造了最新技术标杆，其精度显著超越前代模型。代码已发布于https://github.com/Parskatt/romav2",
    "url": "https://huggingface.co/papers/2511.15706",
    "arxiv_url": "https://arxiv.org/abs/2511.15706"
  },
  {
    "title": "Aligning Generative Music AI with Human Preferences: Methods and Challenges",
    "summary": "Recent advances in generative AI for music have achieved remarkable fidelity and stylistic diversity, yet these systems often fail to align with nuanced human preferences due to the specific loss functions they use. This paper advocates for the systematic application of preference alignment techniques to music generation, addressing the fundamental gap between computational optimization and human musical appreciation. Drawing on recent breakthroughs including MusicRL's large-scale preference learning, multi-preference alignment frameworks like diffusion-based preference optimization in DiffRhythm+, and inference-time optimization techniques like Text2midi-InferAlign, we discuss how these techniques can address music's unique challenges: temporal coherence, harmonic consistency, and subjective quality assessment. We identify key research challenges including scalability to long-form compositions, reliability amongst others in preference modelling. Looking forward, we envision preference-aligned music generation enabling transformative applications in interactive composition tools and personalized music services. This work calls for sustained interdisciplinary research combining advances in machine learning, music-theory to create music AI systems that truly serve human creative and experiential needs.",
    "translation": "标题：生成式音乐人工智能与人类偏好的对齐：方法与挑战\n\n摘要：尽管音乐生成式人工智能近期在保真度与风格多样性方面取得了显著进展，但由于其所采用的特定损失函数，这些系统往往难以契合人类细腻的审美偏好。本文主张将偏好对齐技术系统化应用于音乐生成领域，以弥合计算优化与人类音乐审美之间的根本鸿沟。基于包括MusicRL的大规模偏好学习、DiffRhythm+中基于扩散偏好多目标对齐框架、以及Text2midi-InferAlign等推理时优化技术在内的最新突破，我们深入探讨这些技术如何应对音乐特有的挑战：时序连贯性、和声一致性以及主观质量评估。我们指出了关键研究挑战，包括长篇幅作品的可扩展性、偏好建模可靠性等核心问题。展望未来，我们预见偏好对齐的音乐生成技术将在交互式作曲工具和个性化音乐服务中催生变革性应用。本研究呼吁持续开展跨学科合作，结合机器学习与音乐理论的前沿进展，共同构建真正服务于人类创作与体验需求的音乐人工智能系统。",
    "url": "https://huggingface.co/papers/2511.15038",
    "arxiv_url": "https://arxiv.org/abs/2511.15038"
  },
  {
    "title": "Mixture of States: Routing Token-Level Dynamics for Multimodal Generation",
    "summary": "We introduce MoS (Mixture of States), a novel fusion paradigm for multimodal diffusion models that merges modalities using flexible, state-based interactions. The core of MoS is a learnable, token-wise router that creates denoising timestep- and input-dependent interactions between modalities' hidden states, precisely aligning token-level features with the diffusion trajectory. This router sparsely selects the top-k hidden states and is trained with an ε-greedy strategy, efficiently selecting contextual features with minimal learnable parameters and negligible computational overhead. We validate our design with text-to-image generation (MoS-Image) and editing (MoS-Editing), which achieve state-of-the-art results. With only 3B to 5B parameters, our models match or surpass counterparts up to 4times larger. These findings establish MoS as a flexible and compute-efficient paradigm for scaling multimodal diffusion models.",
    "translation": "标题：状态混合：面向多模态生成的令牌级动态路由机制\n\n摘要：本文提出状态混合（MoS）——一种创新的多模态扩散模型融合范式，通过基于状态的灵活交互实现模态融合。该机制的核心在于可学习的令牌级路由模块，该模块能够根据去噪时间步长与输入内容，在多模态隐状态间建立动态交互关系，从而精确对齐令牌级特征与扩散轨迹。该路由模块采用稀疏化策略选取前k个隐状态，并配合ε-贪婪训练策略，以最小可学习参数和可忽略的计算开销实现上下文特征的高效选择。我们通过文本到图像生成（MoS-Image）与编辑任务（MoS-Editing）验证了该设计的有效性，相关成果达到业界最优水平。仅需30亿至50亿参数的模型即可匹配甚至超越参数量达4倍的大型对比模型。这些发现确立了MoS作为可扩展多模态扩散模型的灵活高效计算范式。",
    "url": "https://huggingface.co/papers/2511.12207",
    "arxiv_url": "https://arxiv.org/abs/2511.12207"
  },
  {
    "title": "Medal S: Spatio-Textual Prompt Model for Medical Segmentation",
    "summary": "We introduce Medal S, a medical segmentation foundation model that supports native-resolution spatial and textual prompts within an end-to-end trainable framework. Unlike text-only methods lacking spatial awareness, Medal S achieves channel-wise alignment between volumetric prompts and text embeddings, mitigating inaccuracies from resolution mismatches. By preserving full 3D context, it efficiently processes multiple native-resolution masks in parallel, enhancing multi-class segmentation performance. A lightweight 3D convolutional module enables precise voxel-space refinement guided by both prompt types, supporting up to 243 classes across CT, MRI, PET, ultrasound, and microscopy modalities in the BiomedSegFM dataset. Medal S offers two prompting modes: a text-only mode, where model predictions serve as spatial prompts for self-refinement without human input, and a hybrid mode, incorporating manual annotations for enhanced flexibility. For 24-class segmentation, parallel spatial prompting reduces inference time by more than 90% compared to sequential prompting. We propose dynamic resampling to address target-patch ratio imbalance, extending SAT and nnU-Net for data augmentation. Furthermore, we develop optimized text preprocessing, a two-stage inference strategy, and post-processing techniques to improve memory efficiency, precision, and inference speed. On the five-modality average on the validation set, Medal S outperforms SAT with a DSC of 75.44 (vs. 69.83), NSD of 77.34 (vs. 71.06), F1 of 38.24 (vs. 24.88), and DSC TP of 65.46 (vs. 46.97). Medal S achieves excellent performance by harmonizing spatial precision with semantic textual guidance, demonstrating superior efficiency and accuracy in multi-class medical segmentation tasks compared to sequential prompt-based approaches. Medal S will be publicly available at https://github.com/yinghemedical/Medal-S.",
    "translation": "标题：Medal S：面向医学分割的空间-文本提示模型\n\n摘要：本文提出Medal S医学分割基础模型，该模型在端到端可训练框架内支持原生分辨率空间提示与文本提示。相较于缺乏空间感知的纯文本方法，Medal S实现了体数据提示与文本嵌入的通道级对齐，有效缓解分辨率失配导致的定位偏差。通过保留完整三维上下文信息，该模型能并行处理多个原生分辨率掩码，显著提升多类别分割性能。轻量化三维卷积模块在双提示类型引导下实现精确体素空间优化，支持BiomedSegFM数据集中CT、MRI、PET、超声及显微影像等五种模态的243类分割任务。Medal S提供两种提示模式：纯文本模式下模型预测结果作为空间提示进行自主优化，无需人工干预；混合模式则融合人工标注以实现更高灵活性。在24类分割任务中，并行空间提示较顺序提示减少90%以上推理时间。针对目标-图像块比例失衡问题，我们提出动态重采样技术，扩展了SAT与nnU-Net的数据增强方法。此外，通过优化文本预处理流程、设计两阶段推理策略及后处理技术，显著提升了内存效率、精度与推理速度。在验证集五模态平均指标上，Medal S以DSC 75.44（对比69.83）、NSD 77.34（对比71.06）、F1 38.24（对比24.88）及DSC TP 65.46（对比46.97）全面超越SAT。该模型通过协调空间精度与语义文本指导，在多项医学分割任务中展现出较顺序提示方法更优异的效率与准确性。Medal S代码已开源：https://github.com/yinghemedical/Medal-S。",
    "url": "https://huggingface.co/papers/2511.13001",
    "arxiv_url": "https://arxiv.org/abs/2511.13001"
  }
]