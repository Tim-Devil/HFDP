[
  {
    "title": "Native Parallel Reasoner: Reasoning in Parallelism via Self-Distilled Reinforcement Learning",
    "summary": "We introduce Native Parallel Reasoner (NPR), a teacher-free framework that enables Large Language Models (LLMs) to self-evolve genuine parallel reasoning capabilities. NPR transforms the model from sequential emulation to native parallel cognition through three key innovations: 1) a self-distilled progressive training paradigm that transitions from ``cold-start'' format discovery to strict topological constraints without external supervision; 2) a novel Parallel-Aware Policy Optimization (PAPO) algorithm that optimizes branching policies directly within the execution graph, allowing the model to learn adaptive decomposition via trial and error; and 3) a robust NPR Engine that refactors memory management and flow control of SGLang to enable stable, large-scale parallel RL training. Across eight reasoning benchmarks, NPR trained on Qwen3-4B achieves performance gains of up to 24.5% and inference speedups up to 4.6x. Unlike prior baselines that often fall back to autoregressive decoding, NPR demonstrates 100% genuine parallel execution, establishing a new standard for self-evolving, efficient, and scalable agentic reasoning.",
    "translation": "标题：原生并行推理器：通过自蒸馏强化学习实现并行推理\n\n摘要：本文提出原生并行推理器（NPR），一种无需教师模型的框架，使大语言模型（LLM）能够自我演化出真正的并行推理能力。NPR通过三项关键创新将模型从序列化模仿转变为原生并行认知：1）自蒸馏渐进式训练范式，在无外部监督的情况下，从“冷启动”格式发现过渡到严格的拓扑约束；2）新颖的并行感知策略优化（PAPO）算法，直接在执行图中优化分支策略，使模型能够通过试错学习自适应分解；3）稳健的NPR引擎，重构SGLang的内存管理与流程控制，实现稳定的大规模并行强化学习训练。在八个推理基准测试中，基于Qwen3-4B训练的NPR实现了高达24.5%的性能提升和最高4.6倍的推理加速。与先前常退化为自回归解码的基线方法不同，NPR实现了100%的真正并行执行，为自我演化、高效且可扩展的智能体推理设立了新标准。",
    "url": "https://huggingface.co/papers/2512.07461",
    "arxiv_url": "https://arxiv.org/abs/2512.07461"
  },
  {
    "title": "Beyond Real: Imaginary Extension of Rotary Position Embeddings for Long-Context LLMs",
    "summary": "Rotary Position Embeddings (RoPE) have become a standard for encoding sequence order in Large Language Models (LLMs) by applying rotations to query and key vectors in the complex plane. Standard implementations, however, utilize only the real component of the complex-valued dot product for attention score calculation. This simplification discards the imaginary component, which contains valuable phase information, leading to a potential loss of relational details crucial for modeling long-context dependencies. In this paper, we propose an extension that re-incorporates this discarded imaginary component. Our method leverages the full complex-valued representation to create a dual-component attention score. We theoretically and empirically demonstrate that this approach enhances the modeling of long-context dependencies by preserving more positional information. Furthermore, evaluations on a suite of long-context language modeling benchmarks show that our method consistently improves performance over the standard RoPE, with the benefits becoming more significant as context length increases. The code is available at https://github.com/OpenMOSS/rope_pp.",
    "translation": "标题：超越实数：长上下文大语言模型中旋转位置编码的虚部扩展\n\n摘要：旋转位置编码通过将查询向量和键向量映射到复数平面进行旋转，已成为大语言模型中编码序列顺序的标准方法。然而，标准实现仅使用复数点积的实部计算注意力分数。这种简化舍弃了包含重要相位信息的虚部，可能导致对长上下文依赖建模至关重要的关系细节丢失。本文提出一种扩展方法，重新整合被舍弃的虚部信息。该方法利用完整的复数表示构建双组分注意力分数，从理论和实验上证明该方案能通过保留更多位置信息来增强长上下文依赖建模能力。在一系列长上下文语言建模基准测试中的评估表明，相较于标准旋转位置编码，本方法能持续提升模型性能，且随着上下文长度增加，改进效果愈加显著。代码已开源：https://github.com/OpenMOSS/rope_pp。",
    "url": "https://huggingface.co/papers/2512.07525",
    "arxiv_url": "https://arxiv.org/abs/2512.07525"
  },
  {
    "title": "Unified Video Editing with Temporal Reasoner",
    "summary": "Existing video editing methods face a critical trade-off: expert models offer precision but rely on task-specific priors like masks, hindering unification; conversely, unified temporal in-context learning models are mask-free but lack explicit spatial cues, leading to weak instruction-to-region mapping and imprecise localization. To resolve this conflict, we propose VideoCoF, a novel Chain-of-Frames approach inspired by Chain-of-Thought reasoning. VideoCoF enforces a ``see, reason, then edit\" procedure by compelling the video diffusion model to first predict reasoning tokens (edit-region latents) before generating the target video tokens. This explicit reasoning step removes the need for user-provided masks while achieving precise instruction-to-region alignment and fine-grained video editing. Furthermore, we introduce a RoPE alignment strategy that leverages these reasoning tokens to ensure motion alignment and enable length extrapolation beyond the training duration. We demonstrate that with a minimal data cost of only 50k video pairs, VideoCoF achieves state-of-the-art performance on VideoCoF-Bench, validating the efficiency and effectiveness of our approach. Our code, weight, data are available at https://github.com/knightyxp/VideoCoF.",
    "translation": "标题：基于时序推理器的统一视频编辑方法\n\n摘要：现有视频编辑方法面临关键权衡：专家模型虽能提供精确编辑效果，但依赖掩码等任务特定先验知识，阻碍了方法统一化；反之，基于时序上下文学习的统一模型虽无需掩码，但缺乏显式空间线索，导致指令-区域映射能力薄弱与定位精度不足。为解决这一矛盾，我们受思维链推理启发，提出一种新颖的帧链式方法VideoCoF。该方法通过强制视频扩散模型在生成目标视频标记前先预测推理标记（编辑区域潜在表示），构建“观察-推理-编辑”的流程。这种显式推理步骤在无需用户提供掩码的前提下，实现了精准的指令-区域对齐与细粒度视频编辑。此外，我们提出一种RoPE对齐策略，利用推理标记确保运动连贯性，并实现超越训练时长的序列长度外推能力。实验表明，仅使用5万对视频数据的极低训练成本，VideoCoF即在VideoCoF-Bench评测中达到最先进性能，验证了本方法的高效性与有效性。相关代码、权重及数据已开源：https://github.com/knightyxp/VideoCoF。",
    "url": "https://huggingface.co/papers/2512.07469",
    "arxiv_url": "https://arxiv.org/abs/2512.07469"
  },
  {
    "title": "Voxify3D: Pixel Art Meets Volumetric Rendering",
    "summary": "Voxel art is a distinctive stylization widely used in games and digital media, yet automated generation from 3D meshes remains challenging due to conflicting requirements of geometric abstraction, semantic preservation, and discrete color coherence. Existing methods either over-simplify geometry or fail to achieve the pixel-precise, palette-constrained aesthetics of voxel art. We introduce Voxify3D, a differentiable two-stage framework bridging 3D mesh optimization with 2D pixel art supervision. Our core innovation lies in the synergistic integration of three components: (1) orthographic pixel art supervision that eliminates perspective distortion for precise voxel-pixel alignment; (2) patch-based CLIP alignment that preserves semantics across discretization levels; (3) palette-constrained Gumbel-Softmax quantization enabling differentiable optimization over discrete color spaces with controllable palette strategies. This integration addresses fundamental challenges: semantic preservation under extreme discretization, pixel-art aesthetics through volumetric rendering, and end-to-end discrete optimization. Experiments show superior performance (37.12 CLIP-IQA, 77.90\\% user preference) across diverse characters and controllable abstraction (2-8 colors, 20x-50x resolutions). Project page: https://yichuanh.github.io/Voxify-3D/",
    "translation": "标题：Voxify3D：像素艺术与体素渲染的交汇\n\n摘要：体素艺术是一种广泛应用于游戏和数字媒体的独特风格化形式，然而，由于几何抽象、语义保持和离散色彩一致性之间的相互冲突要求，从三维网格自动生成体素艺术仍具挑战性。现有方法要么过度简化几何结构，要么无法实现体素艺术所要求的像素级精确、调色板约束的美学效果。本文提出Voxify3D，一个可微分的两阶段框架，将三维网格优化与二维像素艺术监督相结合。我们的核心创新在于协同整合了三个组件：(1) 正交像素艺术监督，消除透视畸变以实现精确的体素-像素对齐；(2) 基于分块的CLIP对齐，在离散化过程中保持语义一致性；(3) 调色板约束的Gumbel-Softmax量化，支持在离散色彩空间上进行可微分优化，并提供可控的调色板策略。该整合解决了关键挑战：极端离散化下的语义保持、通过体素渲染实现像素艺术美学，以及端到端的离散优化。实验表明，该方法在多样化角色模型和可控抽象程度（2-8种颜色，20倍至50倍分辨率）上均表现出优越性能（CLIP-IQA得分37.12，用户偏好率77.90%）。项目页面：https://yichuanh.github.io/Voxify-3D/",
    "url": "https://huggingface.co/papers/2512.07834",
    "arxiv_url": "https://arxiv.org/abs/2512.07834"
  },
  {
    "title": "Scaling Zero-Shot Reference-to-Video Generation",
    "summary": "Reference-to-video (R2V) generation aims to synthesize videos that align with a text prompt while preserving the subject identity from reference images. However, current R2V methods are hindered by the reliance on explicit reference image-video-text triplets, whose construction is highly expensive and difficult to scale. We bypass this bottleneck by introducing Saber, a scalable zero-shot framework that requires no explicit R2V data. Trained exclusively on video-text pairs, Saber employs a masked training strategy and a tailored attention-based model design to learn identity-consistent and reference-aware representations. Mask augmentation techniques are further integrated to mitigate copy-paste artifacts common in reference-to-video generation. Moreover, Saber demonstrates remarkable generalization capabilities across a varying number of references and achieves superior performance on the OpenS2V-Eval benchmark compared to methods trained with R2V data.",
    "translation": "标题：规模化零样本参考视频生成\n\n摘要：参考视频生成旨在合成符合文本提示的视频，同时保持参考图像中的主体身份一致性。然而，现有方法受限于对显式参考图像-视频-文本三元组的依赖，此类数据的构建成本高昂且难以规模化。本研究通过引入Saber框架突破这一瓶颈，该可扩展的零样本框架无需显式的参考视频生成数据。Saber仅使用视频-文本对进行训练，通过掩码训练策略与定制的基于注意力的模型设计，学习身份一致且具有参考感知能力的表征。为进一步缓解参考视频生成中常见的复制粘贴伪影，本研究整合了掩码增强技术。实验表明，Saber在不同数量参考图像条件下均展现出卓越的泛化能力，并在OpenS2V-Eval基准测试中超越了依赖参考视频生成数据训练的方法，实现了更优的性能。",
    "url": "https://huggingface.co/papers/2512.06905",
    "arxiv_url": "https://arxiv.org/abs/2512.06905"
  },
  {
    "title": "DoVer: Intervention-Driven Auto Debugging for LLM Multi-Agent Systems",
    "summary": "Large language model (LLM)-based multi-agent systems are challenging to debug because failures often arise from long, branching interaction traces. The prevailing practice is to leverage LLMs for log-based failure localization, attributing errors to a specific agent and step. However, this paradigm has two key limitations: (i) log-only debugging lacks validation, producing untested hypotheses, and (ii) single-step or single-agent attribution is often ill-posed, as we find that multiple distinct interventions can independently repair the failed task. To address the first limitation, we introduce DoVer, an intervention-driven debugging framework, which augments hypothesis generation with active verification through targeted interventions (e.g., editing messages, altering plans). For the second limitation, rather than evaluating on attribution accuracy, we focus on measuring whether the system resolves the failure or makes quantifiable progress toward task success, reflecting a more outcome-oriented view of debugging. Within the Magnetic-One agent framework, on the datasets derived from GAIA and AssistantBench, DoVer flips 18-28% of failed trials into successes, achieves up to 16% milestone progress, and validates or refutes 30-60% of failure hypotheses. DoVer also performs effectively on a different dataset (GSMPlus) and agent framework (AG2), where it recovers 49% of failed trials. These results highlight intervention as a practical mechanism for improving reliability in agentic systems and open opportunities for more robust, scalable debugging methods for LLM-based multi-agent systems. Project website and code will be available at https://aka.ms/DoVer.",
    "translation": "标题：DoVer：面向大语言模型多智能体系统的干预驱动自动调试方法\n\n摘要：基于大语言模型的多智能体系统调试难度较高，其故障往往源于冗长且分支复杂的交互轨迹。当前主流方法依赖大语言模型进行基于日志的故障定位，将错误归因于特定智能体或执行步骤。然而，该范式存在两个关键局限：（1）纯日志调试缺乏验证环节，仅生成未经检验的假设；（2）单步骤或单智能体归因常与实际情况不符，本研究发现多种不同的干预措施均可独立修复任务故障。针对第一个局限，我们提出DoVer——一种干预驱动的调试框架，通过定向干预（如编辑消息、调整计划）进行主动验证，从而增强假设生成的可信度。对于第二个局限，我们不再以归因准确性为评估核心，转而关注系统能否解决故障或在任务成功方向上取得可量化的进展，这体现了更注重实际结果的调试视角。在Magnetic-One智能体框架中，基于GAIA与AssistantBench衍生的数据集进行实验，DoVer成功将18%-28%的失败案例转化为成功案例，实现最高16%的里程碑进度突破，并能验证或推翻30%-60%的故障假设。在另一数据集（GSMPlus）与智能体框架（AG2）的测试中，DoVer同样表现优异，成功修复了49%的失败案例。这些结果凸显了干预机制在提升智能体系统可靠性方面的实用价值，并为开发更鲁棒、可扩展的大语言模型多智能体系统调试方法开辟了新路径。项目网站与代码将在 https://aka.ms/DoVer 发布。",
    "url": "https://huggingface.co/papers/2512.06749",
    "arxiv_url": "https://arxiv.org/abs/2512.06749"
  },
  {
    "title": "EgoEdit: Dataset, Real-Time Streaming Model, and Benchmark for Egocentric Video Editing",
    "summary": "We study instruction-guided editing of egocentric videos for interactive AR applications. While recent AI video editors perform well on third-person footage, egocentric views present unique challenges - including rapid egomotion and frequent hand-object interactions - that create a significant domain gap. Moreover, existing offline editing pipelines suffer from high latency, limiting real-time interaction. To address these issues, we present a complete ecosystem for egocentric video editing. First, we construct EgoEditData, a carefully designed and manually curated dataset specifically designed for egocentric editing scenarios, featuring rich hand-object interactions, while explicitly preserving hands. Second, we develop EgoEdit, an instruction-following egocentric video editor that supports real-time streaming inference on a single GPU. Finally, we introduce EgoEditBench, an evaluation suite targeting instruction faithfulness, hand and interaction preservation, and temporal stability under egomotion. Across both egocentric and general editing tasks, EgoEdit produces temporally stable, instruction-faithful results with interactive latency. It achieves clear gains on egocentric editing benchmarks-where existing methods struggle-while maintaining performance comparable to the strongest baselines on general editing tasks. EgoEditData and EgoEditBench will be made public for the research community. See our website at https://snap-research.github.io/EgoEdit",
    "translation": "标题：EgoEdit：面向第一人称视频编辑的数据集、实时流式处理模型与基准框架\n\n摘要：本研究针对交互式增强现实应用，探索基于指令引导的第一人称视频编辑技术。当前AI视频编辑工具虽在第三人称视角素材上表现良好，但第一人称视角存在独特挑战——包括快速的自我运动与频繁的手-物交互——这导致了显著的领域差异。此外，现有离线编辑流程存在高延迟问题，限制了实时交互体验。为解决这些难题，我们构建了一套完整的第一人称视频编辑生态系统。首先，我们创建了EgoEditData数据集，该数据集专为第一人称编辑场景设计，通过精细构建与人工标注，突出丰富的手-物交互特征并明确保持手部完整性。其次，我们开发了EgoEdit模型，这是一个遵循指令的第一人称视频编辑器，支持在单GPU上实现实时流式推理。最后，我们提出EgoEditBench评估体系，针对指令遵循度、手部与交互保持能力，以及自我运动下的时序稳定性进行系统评测。实验表明，在第一人称与通用编辑任务中，EgoEdit均能生成时序稳定、忠实于指令的编辑结果，并保持交互级延迟。在现有方法表现不佳的第一人称编辑基准测试中，本方法取得显著优势，同时在通用编辑任务上保持与最强基线模型相当的性能。EgoEditData数据集与EgoEditBench评估套件将向研究社区公开。详见项目网站：https://snap-research.github.io/EgoEdit",
    "url": "https://huggingface.co/papers/2512.06065",
    "arxiv_url": "https://arxiv.org/abs/2512.06065"
  },
  {
    "title": "Distribution Matching Variational AutoEncoder",
    "summary": "Most visual generative models compress images into a latent space before applying diffusion or autoregressive modelling. Yet, existing approaches such as VAEs and foundation model aligned encoders implicitly constrain the latent space without explicitly shaping its distribution, making it unclear which types of distributions are optimal for modeling. We introduce Distribution-Matching VAE (DMVAE), which explicitly aligns the encoder's latent distribution with an arbitrary reference distribution via a distribution matching constraint. This generalizes beyond the Gaussian prior of conventional VAEs, enabling alignment with distributions derived from self-supervised features, diffusion noise, or other prior distributions. With DMVAE, we can systematically investigate which latent distributions are more conducive to modeling, and we find that SSL-derived distributions provide an excellent balance between reconstruction fidelity and modeling efficiency, reaching gFID equals 3.2 on ImageNet with only 64 training epochs. Our results suggest that choosing a suitable latent distribution structure (achieved via distribution-level alignment), rather than relying on fixed priors, is key to bridging the gap between easy-to-model latents and high-fidelity image synthesis. Code is avaliable at https://github.com/sen-ye/dmvae.",
    "translation": "标题：分布匹配变分自编码器\n\n摘要：大多数视觉生成模型在应用扩散或自回归建模之前，会将图像压缩至潜在空间。然而，现有方法（如变分自编码器及与基础模型对齐的编码器）仅隐式约束潜在空间，而未显式塑造其分布，导致何种分布最适合建模尚不明确。本文提出分布匹配变分自编码器，通过分布匹配约束显式地将编码器的潜在分布与任意参考分布对齐。该方法突破了传统变分自编码器高斯先验的局限，可实现与自监督特征、扩散噪声或其他先验分布所导出分布的对齐。借助分布匹配变分自编码器，我们能够系统探究何种潜在分布更有利于建模，并发现自监督学习导出的分布能在重建保真度与建模效率间取得优异平衡——仅经过64轮训练即在ImageNet数据集上达到gFID=3.2。实验结果表明：选择适宜的潜在分布结构（通过分布层级对齐实现），而非依赖固定先验，是弥合易建模潜在空间与高保真图像合成之间差距的关键。代码已发布于https://github.com/sen-ye/dmvae。",
    "url": "https://huggingface.co/papers/2512.07778",
    "arxiv_url": "https://arxiv.org/abs/2512.07778"
  },
  {
    "title": "Relational Visual Similarity",
    "summary": "Humans do not just see attribute similarity -- we also see relational similarity. An apple is like a peach because both are reddish fruit, but the Earth is also like a peach: its crust, mantle, and core correspond to the peach's skin, flesh, and pit. This ability to perceive and recognize relational similarity, is arguable by cognitive scientist to be what distinguishes humans from other species. Yet, all widely used visual similarity metrics today (e.g., LPIPS, CLIP, DINO) focus solely on perceptual attribute similarity and fail to capture the rich, often surprising relational similarities that humans perceive. How can we go beyond the visible content of an image to capture its relational properties? How can we bring images with the same relational logic closer together in representation space? To answer these questions, we first formulate relational image similarity as a measurable problem: two images are relationally similar when their internal relations or functions among visual elements correspond, even if their visual attributes differ. We then curate 114k image-caption dataset in which the captions are anonymized -- describing the underlying relational logic of the scene rather than its surface content. Using this dataset, we finetune a Vision-Language model to measure the relational similarity between images. This model serves as the first step toward connecting images by their underlying relational structure rather than their visible appearance. Our study shows that while relational similarity has a lot of real-world applications, existing image similarity models fail to capture it -- revealing a critical gap in visual computing.",
    "translation": "标题：关系视觉相似性\n\n摘要：人类不仅能识别属性相似性——还能识别关系相似性。苹果与桃子相似是因为两者都是偏红色的水果，但地球也与桃子相似：其地壳、地幔和地核分别对应桃子的表皮、果肉和果核。认知科学家认为，这种感知和识别关系相似性的能力正是人类区别于其他物种的关键特征。然而，当前广泛使用的视觉相似性度量方法（如LPIPS、CLIP、DINO）仅关注感知属性相似性，未能捕捉人类所感知的丰富且常令人惊奇的关系相似性。我们该如何超越图像的可见内容以捕捉其关系特性？又该如何在表征空间中使具有相同关系逻辑的图像彼此靠近？为回答这些问题，我们首先将关系图像相似性形式化为可度量问题：当两幅图像的视觉元素之间的内在关系或功能相互对应时，即使其视觉属性不同，它们也具有关系相似性。随后，我们构建了一个包含11.4万条图像-文本对的数据集，其中文本描述经过匿名化处理——着重描述场景底层的关系逻辑而非表面内容。基于该数据集，我们对视觉-语言模型进行微调，以度量图像间的关系相似性。该模型标志着我们朝着依据底层关系结构（而非可见外观）连接图像迈出了第一步。研究表明，尽管关系相似性具有广泛的实际应用价值，现有图像相似性模型却未能有效捕捉这一特性——这揭示了视觉计算领域存在的重要空白。",
    "url": "https://huggingface.co/papers/2512.07833",
    "arxiv_url": "https://arxiv.org/abs/2512.07833"
  },
  {
    "title": "Multi-view Pyramid Transformer: Look Coarser to See Broader",
    "summary": "We propose Multi-view Pyramid Transformer (MVP), a scalable multi-view transformer architecture that directly reconstructs large 3D scenes from tens to hundreds of images in a single forward pass. Drawing on the idea of ``looking broader to see the whole, looking finer to see the details,\" MVP is built on two core design principles: 1) a local-to-global inter-view hierarchy that gradually broadens the model's perspective from local views to groups and ultimately the full scene, and 2) a fine-to-coarse intra-view hierarchy that starts from detailed spatial representations and progressively aggregates them into compact, information-dense tokens. This dual hierarchy achieves both computational efficiency and representational richness, enabling fast reconstruction of large and complex scenes. We validate MVP on diverse datasets and show that, when coupled with 3D Gaussian Splatting as the underlying 3D representation, it achieves state-of-the-art generalizable reconstruction quality while maintaining high efficiency and scalability across a wide range of view configurations.",
    "translation": "标题：多视角金字塔变换器：观其大略以见全局\n\n摘要：本文提出多视角金字塔变换器（MVP），一种可扩展的多视角变换器架构，能够在前向单次推理中直接根据数十至数百张图像重建大规模三维场景。受“观全局以见整体，察细微以知局部”思想启发，MVP建立在两个核心设计原则之上：1）局部到全局的视角间层次结构，使模型视角从局部视图逐步扩展至视图组，最终覆盖完整场景；2）精细到粗略的视角内层次结构，从详细的空间表征出发，逐步聚合为紧凑且信息密集的令牌。这种双重层次结构兼顾计算效率与表征丰富性，实现了对大规模复杂场景的快速重建。我们在多个数据集上验证了MVP的性能，结果表明：当以三维高斯溅射作为底层三维表征方法时，该架构在保持高效性与可扩展性的同时，能够在多种视角配置下达到当前最优的泛化重建质量。\n\n摘要：[中文摘要]",
    "url": "https://huggingface.co/papers/2512.07806",
    "arxiv_url": "https://arxiv.org/abs/2512.07806"
  },
  {
    "title": "On the Interplay of Pre-Training, Mid-Training, and RL on Reasoning Language Models",
    "summary": "Recent reinforcement learning (RL) techniques have yielded impressive reasoning improvements in language models, yet it remains unclear whether post-training truly extends a model's reasoning ability beyond what it acquires during pre-training. A central challenge is the lack of control in modern training pipelines: large-scale pre-training corpora are opaque, mid-training is often underexamined, and RL objectives interact with unknown prior knowledge in complex ways. To resolve this ambiguity, we develop a fully controlled experimental framework that isolates the causal contributions of pre-training, mid-training, and RL-based post-training. Our approach employs synthetic reasoning tasks with explicit atomic operations, parseable step-by-step reasoning traces, and systematic manipulation of training distributions. We evaluate models along two axes: extrapolative generalization to more complex compositions and contextual generalization across surface contexts. Using this framework, we reconcile competing views on RL's effectiveness. We show that: 1) RL produces true capability gains (pass@128) only when pre-training leaves sufficient headroom and when RL data target the model's edge of competence, tasks at the boundary that are difficult but not yet out of reach. 2) Contextual generalization requires minimal yet sufficient pre-training exposure, after which RL can reliably transfer. 3) Mid-training significantly enhances performance under fixed compute compared with RL only, demonstrating its central but underexplored role in training pipelines. 4) Process-level rewards reduce reward hacking and improve reasoning fidelity. Together, these results clarify the interplay between pre-training, mid-training, and RL, offering a foundation for understanding and improving reasoning LM training strategies.",
    "translation": "标题：论预训练、中期训练与强化学习在推理语言模型中的相互作用\n\n摘要：近期的强化学习技术显著提升了语言模型的推理能力，但后训练是否真正扩展了模型超越预训练阶段所获能力仍不明确。核心挑战在于现代训练流程缺乏可控性：大规模预训练语料不透明，中期训练常被忽视，且强化学习目标与未知的先验知识以复杂方式相互作用。为厘清这一问题，我们构建了一个完全可控的实验框架，以分离预训练、中期训练和基于强化学习的后训练的因果贡献。该方法采用具有明确原子操作、可解析逐步推理轨迹及训练分布系统性操控的合成推理任务。我们从两个维度评估模型：面向更复杂组合的外推泛化能力，以及跨表面语境的上下文泛化能力。通过该框架，我们调和了关于强化学习有效性的对立观点。研究表明：1）仅当预训练留有足够提升空间、且强化学习数据针对模型能力边界（即困难但尚未完全无法掌握的任务）时，强化学习才能带来真实能力提升（pass@128）；2）上下文泛化需要最低限度但充分的预训练基础，此后强化学习可稳定实现能力迁移；3）在固定计算量下，中期训练较单纯强化学习能显著提升性能，揭示了其在训练流程中关键却未被充分探索的作用；4）过程级奖励能减少奖励破解现象并提升推理保真度。这些结果共同阐明了预训练、中期训练与强化学习之间的相互作用机制，为理解和改进推理语言模型的训练策略提供了理论基础。",
    "url": "https://huggingface.co/papers/2512.07783",
    "arxiv_url": "https://arxiv.org/abs/2512.07783"
  },
  {
    "title": "LongCat-Image Technical Report",
    "summary": "We introduce LongCat-Image, a pioneering open-source and bilingual (Chinese-English) foundation model for image generation, designed to address core challenges in multilingual text rendering, photorealism, deployment efficiency, and developer accessibility prevalent in current leading models. 1) We achieve this through rigorous data curation strategies across the pre-training, mid-training, and SFT stages, complemented by the coordinated use of curated reward models during the RL phase. This strategy establishes the model as a new state-of-the-art (SOTA), delivering superior text-rendering capabilities and remarkable photorealism, and significantly enhancing aesthetic quality. 2) Notably, it sets a new industry standard for Chinese character rendering. By supporting even complex and rare characters, it outperforms both major open-source and commercial solutions in coverage, while also achieving superior accuracy. 3) The model achieves remarkable efficiency through its compact design. With a core diffusion model of only 6B parameters, it is significantly smaller than the nearly 20B or larger Mixture-of-Experts (MoE) architectures common in the field. This ensures minimal VRAM usage and rapid inference, significantly reducing deployment costs. Beyond generation, LongCat-Image also excels in image editing, achieving SOTA results on standard benchmarks with superior editing consistency compared to other open-source works. 4) To fully empower the community, we have established the most comprehensive open-source ecosystem to date. We are releasing not only multiple model versions for text-to-image and image editing, including checkpoints after mid-training and post-training stages, but also the entire toolchain of training procedure. We believe that the openness of LongCat-Image will provide robust support for developers and researchers, pushing the frontiers of visual content creation.",
    "translation": "标题：LongCat-Image 技术报告\n\n摘要：我们推出 LongCat-Image，这是一个开创性的开源双语（中英）图像生成基础模型，旨在解决当前主流模型在多语言文本渲染、照片真实感、部署效率和开发者可访问性方面的核心挑战。1）我们通过在预训练、中期训练和 SFT（监督微调）阶段实施严格的数据策展策略，并在强化学习阶段协调使用精选的奖励模型来实现这一目标。这一策略使模型达到了新的最优性能（SOTA），提供了卓越的文本渲染能力和出色的照片真实感，并显著提升了美学质量。2）值得注意的是，该模型为汉字渲染树立了新的行业标准。通过支持复杂和罕见字符，其在覆盖范围上超越了主流开源和商业解决方案，同时实现了更高的准确性。3）该模型凭借其紧凑设计实现了卓越的效率。其核心扩散模型仅包含 60 亿参数，远小于该领域常见的近 200 亿或更大规模的专家混合模型架构。这确保了最小的显存占用和快速的推理速度，显著降低了部署成本。除了生成能力，LongCat-Image 在图像编辑方面也表现出色，在标准基准测试中取得了 SOTA 结果，与其他开源作品相比具有更优的编辑一致性。4）为了全面赋能社区，我们建立了迄今为止最全面的开源生态系统。我们不仅发布了用于文生图和图像编辑的多个模型版本（包括中期训练和后训练阶段的检查点），还开放了整个训练流程的工具链。我们相信，LongCat-Image 的开放性将为开发者和研究人员提供强有力的支持，推动视觉内容创作的前沿发展。",
    "url": "https://huggingface.co/papers/2512.07584",
    "arxiv_url": "https://arxiv.org/abs/2512.07584"
  },
  {
    "title": "UnityVideo: Unified Multi-Modal Multi-Task Learning for Enhancing World-Aware Video Generation",
    "summary": "Recent video generation models demonstrate impressive synthesis capabilities but remain limited by single-modality conditioning, constraining their holistic world understanding. This stems from insufficient cross-modal interaction and limited modal diversity for comprehensive world knowledge representation. To address these limitations, we introduce UnityVideo, a unified framework for world-aware video generation that jointly learns across multiple modalities (segmentation masks, human skeletons, DensePose, optical flow, and depth maps) and training paradigms. Our approach features two core components: (1) dynamic noising to unify heterogeneous training paradigms, and (2) a modality switcher with an in-context learner that enables unified processing via modular parameters and contextual learning. We contribute a large-scale unified dataset with 1.3M samples. Through joint optimization, UnityVideo accelerates convergence and significantly enhances zero-shot generalization to unseen data. We demonstrate that UnityVideo achieves superior video quality, consistency, and improved alignment with physical world constraints. Code and data can be found at: https://github.com/dvlab-research/UnityVideo",
    "translation": "标题：UnityVideo：面向增强世界感知视频生成的统一多模态多任务学习框架\n\n摘要：当前视频生成模型虽展现出卓越的合成能力，但仍受限于单模态条件约束，制约了其对世界的整体理解能力。这一局限源于跨模态交互不足以及模态多样性有限，难以支撑全面的世界知识表征。为突破这些限制，我们提出UnityVideo——一个面向世界感知视频生成的统一框架，能够跨多模态（分割掩码、人体骨架、DensePose、光流与深度图）及多训练范式进行联合学习。本框架包含两大核心组件：（1）动态噪声注入机制，用于统一异构训练范式；（2）搭载上下文学习器的模态切换器，通过模块化参数与上下文学习实现统一处理。我们构建了包含130万样本的大规模统一数据集。经联合优化，UnityVideo显著加速模型收敛，并大幅提升对未见数据的零样本泛化能力。实验表明，UnityVideo在视频质量、时序一致性及物理世界约束对齐方面均达到更优性能。代码与数据详见：https://github.com/dvlab-research/UnityVideo",
    "url": "https://huggingface.co/papers/2512.07831",
    "arxiv_url": "https://arxiv.org/abs/2512.07831"
  },
  {
    "title": "Visual Generation Tuning",
    "summary": "Large Vision Language Models (VLMs) effectively bridge the modality gap through extensive pretraining, acquiring sophisticated visual representations aligned with language. However, it remains underexplored whether these representations, optimized for multimodal understanding tasks, harbor an inherent potential for visual generation. In this paper, we propose VGT, Visual Generation Tuning, a novel paradigm designed to stimulate the underlying capabilities of visual generation within any vision language models. By performing efficient visual generation tuning on well-pretrained VLMs, we significantly mitigate the alignment costs and accelerate the convergence of autoregressive modeling in the continuous space (20x speedup). Specifically, we dismiss the entangled pixel-level VAEs designed for diffusion transformers and formulate VGT-AE through aligning the semantic encoders from pretrained VLMs with the latent representations of pixel decoders. In image reconstruction tasks, we achieve 26.67 PSNR and 0.50 rFID at a 28x compression ratio, outperforming specialized VAEs; in visual generation tasks, we achieve state-of-the-art outcomes among autoregressive models, 0.77 on GenEval and 78.73 on DPG-Bench. Furthermore, our proposed VGT showcases significant scaling promise and is versatile for endowing any VLMs trained for multimodal understanding with the capabilities of visual generation, which paves the new avenue to explore next-generation unified multimodal foundation models. Models and codes are available at https://github.com/hustvl/VGT.",
    "translation": "标题：视觉生成调优\n\n摘要：大规模视觉语言模型通过广泛的预训练有效弥合了模态鸿沟，获得了与语言对齐的复杂视觉表征。然而，这些针对多模态理解任务优化的表征是否蕴含视觉生成的内在潜力，目前仍未得到充分探索。本文提出视觉生成调优这一新范式，旨在激发任意视觉语言模型中潜在的视觉生成能力。通过对预训练良好的视觉语言模型进行高效的视觉生成调优，我们显著降低了对齐成本，并加速了连续空间中自回归建模的收敛速度（提速20倍）。具体而言，我们摒弃了为扩散变换器设计的纠缠像素级变分自编码器，通过将预训练视觉语言模型的语义编码器与像素解码器的潜在表征对齐，构建了视觉生成调优自编码器。在图像重建任务中，我们在28倍压缩比下实现了26.67的峰值信噪比和0.50的相对弗雷歇距离，性能优于专用变分自编码器；在视觉生成任务中，我们在自回归模型中取得了最先进的成果——GenEval评测达到0.77分，DPG-Bench评测达到78.73分。此外，所提出的视觉生成调优展现出显著的扩展潜力，能够灵活赋能任何为多模态理解训练的视觉语言模型，使其具备视觉生成能力，这为探索下一代统一多模态基础模型开辟了新路径。模型与代码已发布于https://github.com/hustvl/VGT。",
    "url": "https://huggingface.co/papers/2511.23469",
    "arxiv_url": "https://arxiv.org/abs/2511.23469"
  },
  {
    "title": "SPARK: Stepwise Process-Aware Rewards for Reference-Free Reinforcement Learning",
    "summary": "Process reward models (PRMs) that provide dense, step-level feedback have shown promise for reinforcement learning, yet their adoption remains limited by the need for expensive step-level annotations or ground truth references. We propose SPARK: a three-stage framework where in the first stage a generator model produces diverse solutions and a verifier model evaluates them using parallel scaling (self-consistency) and sequential scaling (meta-critique). In the second stage, we use these verification outputs as synthetic training data to fine-tune generative process reward models, which subsequently serve as reward signals during training. We show that aggregating multiple independent verifications at the step level produces training data for process reward models that surpass ground-truth outcome supervision, achieving 67.5 F1 on ProcessBench (a benchmark for identifying erroneous steps in mathematical reasoning) compared to 66.4 for reference-guided training and 61.9 for GPT-4o. In the final stage, we apply our generative PRM with chain-of-thought verification (PRM-CoT) as the reward model in RL experiments on mathematical reasoning, and introduce format constraints to prevent reward hacking. Using Qwen2.5-Math-7B, we achieve 47.4% average accuracy across six mathematical reasoning benchmarks, outperforming ground-truth-based RLVR (43.9%). Our work enables reference-free RL training that exceeds ground-truth methods, opening new possibilities for domains lacking verifiable answers or accessible ground truth.",
    "translation": "标题：SPARK：面向无参考强化学习的渐进式过程感知奖励机制\n\n摘要：提供密集步骤级反馈的过程奖励模型在强化学习中展现出潜力，但其应用仍受限于昂贵的步骤级标注或真实参考答案的需求。本文提出SPARK三阶段框架：第一阶段通过生成模型产生多样化解决方案，并利用并行扩展（自洽性验证）与序列扩展（元批判）机制驱动验证模型进行评估；第二阶段将验证输出作为合成训练数据，对生成式过程奖励模型进行微调，使其在训练过程中提供奖励信号。研究表明，在步骤层面聚合多个独立验证结果所生成的过程奖励模型训练数据，其效果优于真实结果监督方法——在数学推理错误步骤识别基准ProcessBench上达到67.5的F1分数，显著超越参考指导训练的66.4分与GPT-4o的61.9分。第三阶段将融合思维链验证的生成式过程奖励模型应用于数学推理强化学习实验，并通过格式约束机制防范奖励攻击。基于Qwen2.5-Math-7B模型，我们在六项数学推理基准测试中取得47.4%的平均准确率，优于基于真实结果的RLVR方法（43.9%）。本工作实现了超越真实参考方法的无参考强化学习训练，为缺乏可验证答案或难以获取真实参考的领域开辟了新路径。",
    "url": "https://huggingface.co/papers/2512.03244",
    "arxiv_url": "https://arxiv.org/abs/2512.03244"
  },
  {
    "title": "VG-Refiner: Towards Tool-Refined Referring Grounded Reasoning via Agentic Reinforcement Learning",
    "summary": "Tool-integrated visual reasoning (TiVR) has demonstrated great potential in enhancing multimodal problem-solving. However, existing TiVR paradigms mainly focus on integrating various visual tools through reinforcement learning, while neglecting to design effective response mechanisms for handling unreliable or erroneous tool outputs. This limitation is particularly pronounced in referring and grounding tasks, where inaccurate detection tool predictions often mislead TiVR models into generating hallucinated reasoning. To address this issue, we propose the VG-Refiner, the first framework aiming at the tool-refined referring grounded reasoning. Technically, we introduce a two-stage think-rethink mechanism that enables the model to explicitly analyze and respond to tool feedback, along with a refinement reward that encourages effective correction in response to poor tool results. In addition, we propose two new metrics and establish fair evaluation protocols to systematically measure the refinement ability of current models. We adopt a small amount of task-specific data to enhance the refinement capability of VG-Refiner, achieving a significant improvement in accuracy and correction ability on referring and reasoning grounding benchmarks while preserving the general capabilities of the pretrained model.",
    "translation": "标题：VG-Refiner：基于智能体强化学习的工具精炼指称接地推理\n\n摘要：工具集成视觉推理（TiVR）在增强多模态问题解决能力方面展现出巨大潜力。然而，现有TiVR范式主要关注通过强化学习整合各类视觉工具，却未能设计有效的响应机制来处理不可靠或错误的工具输出。这一局限在指称与接地任务中尤为突出，其中不准确的检测工具预测常误导TiVR模型产生幻觉推理。为解决此问题，我们提出VG-Refiner——首个面向工具精炼指称接地推理的框架。技术上，我们引入两阶段“思考-再思考”机制，使模型能够显式分析并响应工具反馈，同时设计精炼奖励机制以激励模型针对低质量工具结果进行有效修正。此外，我们提出两项新评估指标并建立公平评测协议，以系统衡量现有模型的精炼能力。通过采用少量任务特定数据增强VG-Refiner的精炼能力，该框架在指称与推理接地基准测试中实现了准确率与修正能力的显著提升，同时保持了预训练模型的通用能力。",
    "url": "https://huggingface.co/papers/2512.06373",
    "arxiv_url": "https://arxiv.org/abs/2512.06373"
  },
  {
    "title": "ReCamDriving: LiDAR-Free Camera-Controlled Novel Trajectory Video Generation",
    "summary": "We propose ReCamDriving, a purely vision-based, camera-controlled novel-trajectory video generation framework. While repair-based methods fail to restore complex artifacts and LiDAR-based approaches rely on sparse and incomplete cues, ReCamDriving leverages dense and scene-complete 3DGS renderings for explicit geometric guidance, achieving precise camera-controllable generation. To mitigate overfitting to restoration behaviors when conditioned on 3DGS renderings, ReCamDriving adopts a two-stage training paradigm: the first stage uses camera poses for coarse control, while the second stage incorporates 3DGS renderings for fine-grained viewpoint and geometric guidance. Furthermore, we present a 3DGS-based cross-trajectory data curation strategy to eliminate the train-test gap in camera transformation patterns, enabling scalable multi-trajectory supervision from monocular videos. Based on this strategy, we construct the ParaDrive dataset, containing over 110K parallel-trajectory video pairs. Extensive experiments demonstrate that ReCamDriving achieves state-of-the-art camera controllability and structural consistency.",
    "translation": "标题：ReCamDriving：一种无激光雷达的相机控制新轨迹视频生成方法\n\n摘要：本文提出ReCamDriving，一种纯视觉、相机控制的新轨迹视频生成框架。基于修复的方法难以恢复复杂的伪影，而基于激光雷达的方法依赖于稀疏且不完整的线索，与此不同，ReCamDriving利用密集且场景完整的3D高斯溅射渲染提供显式几何引导，实现了精确的相机可控生成。为缓解在3DGS渲染条件下对修复行为的过拟合问题，ReCamDriving采用两阶段训练范式：第一阶段使用相机位姿进行粗略控制，第二阶段则引入3DGS渲染以实现细粒度的视点与几何引导。此外，我们提出一种基于3DGS的跨轨迹数据构建策略，以消除相机变换模式在训练与测试间的差异，从而能够从单目视频中实现可扩展的多轨迹监督。基于此策略，我们构建了ParaDrive数据集，包含超过11万组平行轨迹视频对。大量实验表明，ReCamDriving在相机可控性与结构一致性方面达到了最先进的性能水平。",
    "url": "https://huggingface.co/papers/2512.03621",
    "arxiv_url": "https://arxiv.org/abs/2512.03621"
  },
  {
    "title": "OmniSafeBench-MM: A Unified Benchmark and Toolbox for Multimodal Jailbreak Attack-Defense Evaluation",
    "summary": "Recent advances in multi-modal large language models (MLLMs) have enabled unified perception-reasoning capabilities, yet these systems remain highly vulnerable to jailbreak attacks that bypass safety alignment and induce harmful behaviors. Existing benchmarks such as JailBreakV-28K, MM-SafetyBench, and HADES provide valuable insights into multi-modal vulnerabilities, but they typically focus on limited attack scenarios, lack standardized defense evaluation, and offer no unified, reproducible toolbox. To address these gaps, we introduce OmniSafeBench-MM, which is a comprehensive toolbox for multi-modal jailbreak attack-defense evaluation. OmniSafeBench-MM integrates 13 representative attack methods, 15 defense strategies, and a diverse dataset spanning 9 major risk domains and 50 fine-grained categories, structured across consultative, imperative, and declarative inquiry types to reflect realistic user intentions. Beyond data coverage, it establishes a three-dimensional evaluation protocol measuring (1) harmfulness, distinguished by a granular, multi-level scale ranging from low-impact individual harm to catastrophic societal threats, (2) intent alignment between responses and queries, and (3) response detail level, enabling nuanced safety-utility analysis. We conduct extensive experiments on 10 open-source and 8 closed-source MLLMs to reveal their vulnerability to multi-modal jailbreak. By unifying data, methodology, and evaluation into an open-source, reproducible platform, OmniSafeBench-MM provides a standardized foundation for future research. The code is released at https://github.com/jiaxiaojunQAQ/OmniSafeBench-MM.",
    "translation": "标题：OmniSafeBench-MM：多模态越狱攻击-防御评估的统一基准与工具箱\n\n摘要：多模态大语言模型（MLLMs）的最新进展实现了统一的感知-推理能力，但这些系统仍极易受到越狱攻击，从而绕过安全对齐机制并诱发有害行为。现有基准如JailBreakV-28K、MM-SafetyBench和HADES为多模态漏洞提供了有价值的洞见，但它们通常局限于有限的攻击场景，缺乏标准化的防御评估，且未提供统一、可复现的工具箱。为弥补这些不足，我们提出了OmniSafeBench-MM，这是一个用于多模态越狱攻击-防御评估的综合工具箱。OmniSafeBench-MM整合了13种代表性攻击方法、15种防御策略，以及一个涵盖9个主要风险领域和50个细粒度类别的多样化数据集，并按照咨询式、命令式和陈述式查询类型进行结构化设计，以反映真实的用户意图。除了数据覆盖范围，该基准还建立了一个三维评估协议，用于衡量：（1）危害性，采用从低影响个体伤害到灾难性社会威胁的细粒度多级尺度进行区分；（2）响应与查询之间的意图对齐度；（3）响应详细程度，从而支持细致的安全-效用分析。我们对10个开源和8个闭源MLLMs进行了广泛实验，揭示了它们对多模态越狱攻击的脆弱性。通过将数据、方法与评估统一到一个开源、可复现的平台中，OmniSafeBench-MM为未来研究提供了标准化基础。代码已发布于https://github.com/jiaxiaojunQAQ/OmniSafeBench-MM。",
    "url": "https://huggingface.co/papers/2512.06589",
    "arxiv_url": "https://arxiv.org/abs/2512.06589"
  },
  {
    "title": "Beyond Token-level Supervision: Unlocking the Potential of Decoding-based Regression via Reinforcement Learning",
    "summary": "Decoding-based regression, which reformulates regression as a sequence generation task, has emerged as a promising paradigm of applying large language models for numerical prediction. However, its progress is hindered by the misalignment between discrete token-level objectives (e.g., cross-entropy) and continuous numerical values. Existing approaches relying on token-level constraints often fail to capture the global magnitude of the target value, limiting their precision and generalization. In this paper, we propose to unlock the potential of decoding-based regression via Reinforcement Learning (RL). We formulate the generation process as a Markov Decision Process, utilizing sequence-level rewards to enforce global numerical coherence. Extensive experiments on tabular regression and code metric regression demonstrate that our method (specifically with ReMax and GRPO) consistently outperforms both state-of-the-art token-level baselines and traditional regression heads, showing the superiority of introducing sequence-level signals. Our analysis further reveals that RL significantly enhances sampling efficiency and predictive precision, establishing decoding-based regression as a robust and accurate paradigm for general-purpose numerical prediction.",
    "translation": "标题：超越词元级监督：通过强化学习释放基于解码的回归潜力\n\n摘要：基于解码的回归将回归任务重构为序列生成问题，已成为应用大语言模型进行数值预测的一种前景广阔的范式。然而，其发展受到离散词元级目标（如交叉熵）与连续数值之间错位的制约。现有依赖词元级约束的方法往往难以捕捉目标值的整体量级，限制了预测精度与泛化能力。本文提出通过强化学习释放基于解码的回归潜力。我们将生成过程建模为马尔可夫决策过程，利用序列级奖励来保证全局数值一致性。在表格回归与代码度量回归任务上的大量实验表明，本方法（特别是采用ReMax与GRPO时）在性能上持续优于当前最先进的词元级基线方法与传统回归头，证明了引入序列级信号的优势。进一步分析揭示，强化学习显著提升了采样效率与预测精度，从而确立了基于解码的回归作为一种鲁棒且精确的通用数值预测范式。",
    "url": "https://huggingface.co/papers/2512.06533",
    "arxiv_url": "https://arxiv.org/abs/2512.06533"
  },
  {
    "title": "OpenSubject: Leveraging Video-Derived Identity and Diversity Priors for Subject-driven Image Generation and Manipulation",
    "summary": "Despite the promising progress in subject-driven image generation, current models often deviate from the reference identities and struggle in complex scenes with multiple subjects. To address this challenge, we introduce OpenSubject, a video-derived large-scale corpus with 2.5M samples and 4.35M images for subject-driven generation and manipulation. The dataset is built with a four-stage pipeline that exploits cross-frame identity priors. (i) Video Curation. We apply resolution and aesthetic filtering to obtain high-quality clips. (ii) Cross-Frame Subject Mining and Pairing. We utilize vision-language model (VLM)-based category consensus, local grounding, and diversity-aware pairing to select image pairs. (iii) Identity-Preserving Reference Image Synthesis. We introduce segmentation map-guided outpainting to synthesize the input images for subject-driven generation and box-guided inpainting to generate input images for subject-driven manipulation, together with geometry-aware augmentations and irregular boundary erosion. (iv) Verification and Captioning. We utilize a VLM to validate synthesized samples, re-synthesize failed samples based on stage (iii), and then construct short and long captions. In addition, we introduce a benchmark covering subject-driven generation and manipulation, and then evaluate identity fidelity, prompt adherence, manipulation consistency, and background consistency with a VLM judge. Extensive experiments show that training with OpenSubject improves generation and manipulation performance, particularly in complex scenes.",
    "translation": "标题：OpenSubject：利用视频衍生的身份与多样性先验实现主体驱动的图像生成与编辑\n\n摘要：尽管主体驱动的图像生成已取得显著进展，但现有模型常偏离参考身份特征，且在包含多主体的复杂场景中表现欠佳。为解决这一挑战，我们提出了OpenSubject——一个基于视频构建的大规模数据集，包含250万个样本和435万张图像，专用于主体驱动的生成与编辑任务。该数据集通过利用跨帧身份先验的四阶段流程构建：（一）视频筛选。通过分辨率与美学过滤获取高质量视频片段。（二）跨帧主体挖掘与配对。采用基于视觉语言模型的类别共识、局部定位及多样性感知配对策略筛选图像对。（三）身份保持的参考图像合成。通过分割图引导的外延绘制合成主体驱动生成的输入图像，结合边界框引导的内绘修复生成主体驱动编辑的输入图像，并辅以几何感知增强与不规则边界侵蚀技术。（四）验证与标注。使用视觉语言模型验证合成样本，对失败样本基于第三阶段流程重新合成，最终构建简短与详细描述文本。此外，我们建立了涵盖主体驱动生成与编辑任务的基准测试体系，通过视觉语言模型评估身份保真度、提示符遵从性、编辑一致性与背景一致性。大量实验表明，使用OpenSubject进行训练能显著提升生成与编辑性能，尤其在复杂场景中效果突出。",
    "url": "https://huggingface.co/papers/2512.08294",
    "arxiv_url": "https://arxiv.org/abs/2512.08294"
  },
  {
    "title": "One Layer Is Enough: Adapting Pretrained Visual Encoders for Image Generation",
    "summary": "Visual generative models (e.g., diffusion models) typically operate in compressed latent spaces to balance training efficiency and sample quality. In parallel, there has been growing interest in leveraging high-quality pre-trained visual representations, either by aligning them inside VAEs or directly within the generative model. However, adapting such representations remains challenging due to fundamental mismatches between understanding-oriented features and generation-friendly latent spaces. Representation encoders benefit from high-dimensional latents that capture diverse hypotheses for masked regions, whereas generative models favor low-dimensional latents that must faithfully preserve injected noise. This discrepancy has led prior work to rely on complex objectives and architectures. In this work, we propose FAE (Feature Auto-Encoder), a simple yet effective framework that adapts pre-trained visual representations into low-dimensional latents suitable for generation using as little as a single attention layer, while retaining sufficient information for both reconstruction and understanding. The key is to couple two separate deep decoders: one trained to reconstruct the original feature space, and a second that takes the reconstructed features as input for image generation. FAE is generic; it can be instantiated with a variety of self-supervised encoders (e.g., DINO, SigLIP) and plugged into two distinct generative families: diffusion models and normalizing flows. Across class-conditional and text-to-image benchmarks, FAE achieves strong performance. For example, on ImageNet 256x256, our diffusion model with CFG attains a near state-of-the-art FID of 1.29 (800 epochs) and 1.70 (80 epochs). Without CFG, FAE reaches the state-of-the-art FID of 1.48 (800 epochs) and 2.08 (80 epochs), demonstrating both high quality and fast learning.",
    "translation": "标题：一层足矣：面向图像生成任务的自适应预训练视觉编码器\n\n摘要：视觉生成模型（如扩散模型）通常在压缩的潜在空间中运行，以平衡训练效率与生成样本质量。与此同时，利用高质量预训练视觉表征的研究日益受到关注，常见方法包括将其与变分自编码器（VAEs）对齐或直接整合至生成模型中。然而，由于理解导向的特征与生成友好的潜在空间之间存在本质性不匹配，此类表征的适配仍具挑战性。表征编码器受益于高维潜在空间以捕捉掩码区域的多样化假设，而生成模型则倾向于低维潜在空间以忠实保留注入的噪声。这种差异导致先前研究依赖复杂的优化目标与架构设计。本文提出特征自编码器（FAE），这是一个简洁而高效的框架，能够将预训练的视觉表征适配至适用于生成任务的低维潜在空间，仅需使用单个注意力层即可实现，同时保留足够的重建与理解信息。其关键在于耦合两个独立的深度解码器：一个训练用于重建原始特征空间，另一个则以重建特征作为输入进行图像生成。FAE具有通用性：可与多种自监督编码器（如DINO、SigLIP）结合实例化，并嵌入到扩散模型与标准化流这两类不同的生成框架中。在类别条件生成与文生图基准测试中，FAE均表现出优异性能。例如，在ImageNet 256×256数据集上，结合分类器无引导（CFG）的扩散模型实现了接近最优的FID分数1.29（800训练周期）与1.70（80训练周期）；未使用CFG时，FAE达到当前最优的FID分数1.48（800训练周期）与2.08（80训练周期），展现出高质量生成与快速学习的双重优势。",
    "url": "https://huggingface.co/papers/2512.07829",
    "arxiv_url": "https://arxiv.org/abs/2512.07829"
  },
  {
    "title": "Group Representational Position Encoding",
    "summary": "We present GRAPE (Group RepresentAtional Position Encoding), a unified framework for positional encoding based on group actions. GRAPE brings together two families of mechanisms: (i) multiplicative rotations (Multiplicative GRAPE) in SO(d) and (ii) additive logit biases (Additive GRAPE) arising from unipotent actions in the general linear group GL. In Multiplicative GRAPE, a position n in Z (or t in R) acts as G(n)=exp(n,ω,L) with a rank-2 skew generator L in R^{d times d}, yielding a relative, compositional, norm-preserving map with a closed-form matrix exponential. RoPE is recovered exactly when the d/2 planes are the canonical coordinate pairs with log-uniform spectrum. Learned commuting subspaces and compact non-commuting mixtures strictly extend this geometry to capture cross-subspace feature coupling at O(d) and O(r d) cost per head, respectively. In Additive GRAPE, additive logits arise as rank-1 (or low-rank) unipotent actions, recovering ALiBi and the Forgetting Transformer (FoX) as exact special cases while preserving an exact relative law and streaming cacheability. Altogether, GRAPE supplies a principled design space for positional geometry in long-context models, subsuming RoPE and ALiBi as special cases. Project Page: https://github.com/model-architectures/GRAPE.",
    "translation": "标题：群表示位置编码\n\n摘要：本文提出GRAPE（群表示位置编码），这是一个基于群作用的统一位置编码框架。GRAPE整合了两类机制：（1）SO(d)群中的乘法旋转（乘法GRAPE），以及（2）源于一般线性群GL中单能作用的加法对数偏置（加法GRAPE）。在乘法GRAPE中，Z中的位置n（或R中的t）通过G(n)=exp(n,ω,L)作用，其中L为R^{d×d}中的二阶斜生成元，产生具有闭式矩阵指数的相对性、复合性、保范映射。当d/2平面为具有对数均匀谱的标准坐标对时，可精确恢复RoPE。通过学习可交换子空间和紧致非交换混合，该几何结构被严格扩展至分别以每头O(d)和O(r d)代价捕获跨子空间特征耦合。在加法GRAPE中，加法对数以秩1（或低秩）单能作用形式出现，将ALiBi与遗忘变换器（FoX）作为精确特例恢复，同时保持精确的相对律与流式缓存能力。总体而言，GRAPE为长上下文模型中的位置几何提供了原则性设计空间，将RoPE与ALiBi纳入为特例。项目页面：https://github.com/model-architectures/GRAPE。",
    "url": "https://huggingface.co/papers/2512.07805",
    "arxiv_url": "https://arxiv.org/abs/2512.07805"
  },
  {
    "title": "Decouple to Generalize: Context-First Self-Evolving Learning for Data-Scarce Vision-Language Reasoning",
    "summary": "Recent vision-language models (VLMs) achieve remarkable reasoning through reinforcement learning (RL), which provides a feasible solution for realizing continuous self-evolving large vision-language models (LVLMs) in the era of experience. However, RL for VLMs requires abundant high-quality multimodal data, especially challenging in specialized domains like chemistry, earth sciences, and multimodal mathematics. Existing strategies such as synthetic data and self-rewarding mechanisms suffer from limited distributions and alignment difficulties, ultimately causing reward hacking: models exploit high-reward patterns, collapsing policy entropy and destabilizing training. We propose DoGe (Decouple to Generalize), a dual-decoupling framework that guides models to first learn from context rather than problem solving by refocusing on the problem context scenarios overlooked by synthetic data methods. By decoupling learning process into dual components (Thinker and Solver), we reasonably quantify the reward signals of this process and propose a two-stage RL post-training approach from freely exploring context to practically solving tasks. Second, to increase the diversity of training data, DoGe constructs an evolving curriculum learning pipeline: an expanded native domain knowledge corpus and an iteratively evolving seed problems pool. Experiments show that our method consistently outperforms the baseline across various benchmarks, providing a scalable pathway for realizing self-evolving LVLMs.",
    "translation": "标题：解耦以泛化：面向数据稀缺视觉语言推理的上下文优先自演化学习\n\n摘要：当前视觉语言模型通过强化学习实现了卓越的推理能力，为在经验时代实现持续自我演化的大型视觉语言模型提供了可行路径。然而，视觉语言模型的强化学习需要大量高质量多模态数据，在化学、地球科学及多模态数学等专业领域尤为困难。现有策略如合成数据与自奖励机制存在分布局限和对齐困难，最终导致奖励破解现象：模型利用高奖励模式，致使策略熵崩溃并破坏训练稳定性。我们提出DoGe（解耦以泛化）——一种双重解耦框架，通过引导模型首先从上下文而非问题求解中学习，重新聚焦于合成数据方法所忽视的问题情境场景。通过将学习过程解耦为双组件（思考器与求解器），我们合理量化该过程的奖励信号，并提出从自由探索上下文到实际解决问题的两阶段强化学习后训练方法。其次，为提升训练数据多样性，DoGe构建了动态演化的课程学习流程：扩展的本体领域知识库与迭代演化的种子问题池。实验表明，我们的方法在多种基准测试中持续超越基线模型，为实现自演化大型视觉语言模型提供了可扩展路径。",
    "url": "https://huggingface.co/papers/2512.06835",
    "arxiv_url": "https://arxiv.org/abs/2512.06835"
  },
  {
    "title": "VideoVLA: Video Generators Can Be Generalizable Robot Manipulators",
    "summary": "Generalization in robot manipulation is essential for deploying robots in open-world environments and advancing toward artificial general intelligence. While recent Vision-Language-Action (VLA) models leverage large pre-trained understanding models for perception and instruction following, their ability to generalize to novel tasks, objects, and settings remains limited. In this work, we present VideoVLA, a simple approach that explores the potential of transforming large video generation models into robotic VLA manipulators. Given a language instruction and an image, VideoVLA predicts an action sequence as well as the future visual outcomes. Built on a multi-modal Diffusion Transformer, VideoVLA jointly models video, language, and action modalities, using pre-trained video generative models for joint visual and action forecasting. Our experiments show that high-quality imagined futures correlate with reliable action predictions and task success, highlighting the importance of visual imagination in manipulation. VideoVLA demonstrates strong generalization, including imitating other embodiments' skills and handling novel objects. This dual-prediction strategy - forecasting both actions and their visual consequences - explores a paradigm shift in robot learning and unlocks generalization capabilities in manipulation systems.",
    "translation": "标题：VideoVLA：视频生成器可作为通用机器人操作器\n\n摘要：机器人操作的泛化能力对于在开放世界环境中部署机器人以及迈向通用人工智能至关重要。尽管近期的视觉-语言-动作模型利用大规模预训练理解模型进行感知和指令跟随，但其在新任务、新物体和新环境中的泛化能力仍然有限。本研究提出VideoVLA，这是一种探索将大型视频生成模型转化为机器人视觉-语言-动作操作器的简单方法。给定语言指令和图像，VideoVLA可预测动作序列及未来视觉结果。基于多模态扩散变换器架构，VideoVLA通过预训练视频生成模型实现视觉与动作的联合建模，同步处理视频、语言和动作模态。实验表明，高质量的视觉想象与可靠的动作预测及任务成功率呈正相关，凸显了视觉想象力在操作任务中的重要性。VideoVLA展现出强大的泛化能力，包括模仿其他实体技能和处理新异物体。这种同时预测动作及其视觉结果的双重预测策略，探索了机器人学习范式的转变，并释放了操作系统的泛化潜能。",
    "url": "https://huggingface.co/papers/2512.06963",
    "arxiv_url": "https://arxiv.org/abs/2512.06963"
  },
  {
    "title": "Rethinking Training Dynamics in Scale-wise Autoregressive Generation",
    "summary": "Recent advances in autoregressive (AR) generative models have produced increasingly powerful systems for media synthesis. Among them, next-scale prediction has emerged as a popular paradigm, where models generate images in a coarse-to-fine manner. However, scale-wise AR models suffer from exposure bias, which undermines generation quality. We identify two primary causes of this issue: (1) train-test mismatch, where the model must rely on its own imperfect predictions during inference, and (2) imbalance in scale-wise learning difficulty, where certain scales exhibit disproportionately higher optimization complexity. Through a comprehensive analysis of training dynamics, we propose Self-Autoregressive Refinement (SAR) to address these limitations. SAR introduces a Stagger-Scale Rollout (SSR) mechanism that performs lightweight autoregressive rollouts to expose the model to its own intermediate predictions, thereby aligning train-test patterns, and a complementary Contrastive Student-Forcing Loss (CSFL) that provides adequate supervision for self-generated contexts to ensure stable training. Experimental results show that applying SAR to pretrained AR models consistently improves generation quality with minimal computational overhead. For instance, SAR yields a 5.2% FID reduction on FlexVAR-d16 trained on ImageNet 256 within 10 epochs (5 hours on 32xA100 GPUs). Given its efficiency, scalability, and effectiveness, we expect SAR to serve as a reliable post-training method for visual autoregressive generation.",
    "translation": "标题：尺度自回归生成中的训练动态机制再思考\n\n摘要：自回归生成模型的最新进展催生了日益强大的媒体合成系统。其中，尺度递进预测已成为主流范式，模型通过从粗到细的方式生成图像。然而，尺度自回归模型普遍存在曝光偏差问题，严重影响生成质量。我们识别出该问题的两个核心成因：（1）训练-测试失配——推理阶段模型必须依赖自身不完美的预测结果；（2）尺度学习难度失衡——特定尺度表现出不成比例的高优化复杂度。通过对训练动态的全面分析，我们提出自回归精炼框架以解决这些局限。该框架包含交错尺度展开机制——通过轻量级自回归展开使模型接触其中间预测结果，从而实现训练-测试模式对齐；以及互补的对比性强制学习损失函数——为自生成语境提供充分监督以确保训练稳定性。实验结果表明，将自回归精炼框架应用于预训练的自回归模型后，能以极小计算开销持续提升生成质量。例如在ImageNet 256数据集上训练的FlexVAR-d16模型中，自回归精炼框架仅需10个训练周期（32xA100 GPU运行5小时）即可实现FID指标5.2%的优化。鉴于其高效性、可扩展性与有效性，我们预期自回归精炼框架将成为视觉自回归生成领域可靠的训练后优化方法。",
    "url": "https://huggingface.co/papers/2512.06421",
    "arxiv_url": "https://arxiv.org/abs/2512.06421"
  },
  {
    "title": "Small-Gain Nash: Certified Contraction to Nash Equilibria in Differentiable Games",
    "summary": "Classical convergence guarantees for gradient-based learning in games require the pseudo-gradient to be (strongly) monotone in Euclidean geometry as shown by rosen(1965), a condition that often fails even in simple games with strong cross-player couplings. We introduce Small-Gain Nash (SGN), a block small-gain condition in a custom block-weighted geometry. SGN converts local curvature and cross-player Lipschitz coupling bounds into a tractable certificate of contraction. It constructs a weighted block metric in which the pseudo-gradient becomes strongly monotone on any region where these bounds hold, even when it is non-monotone in the Euclidean sense. The continuous flow is exponentially contracting in this designed geometry, and projected Euler and RK4 discretizations converge under explicit step-size bounds derived from the SGN margin and a local Lipschitz constant. Our analysis reveals a certified ``timescale band'', a non-asymptotic, metric-based certificate that plays a TTUR-like role: rather than forcing asymptotic timescale separation via vanishing, unequal step sizes, SGN identifies a finite band of relative metric weights for which a single-step-size dynamics is provably contractive. We validate the framework on quadratic games where Euclidean monotonicity analysis fails to predict convergence, but SGN successfully certifies it, and extend the construction to mirror/Fisher geometries for entropy-regularized policy gradient in Markov games. The result is an offline certification pipeline that estimates curvature, coupling, and Lipschitz parameters on compact regions, optimizes block weights to enlarge the SGN margin, and returns a structural, computable convergence certificate consisting of a metric, contraction rate, and safe step-sizes for non-monotone games.",
    "translation": "标题：小增益纳什：可微博弈中经认证的纳什均衡收缩方法\n\n摘要：经典博弈梯度学习收敛性证明要求伪梯度在欧几里得几何中满足（强）单调性条件（如Rosen于1965年所示），但该条件即使在具有强跨参与者耦合的简单博弈中也常不成立。本文提出小增益纳什方法，这是一种在定制块加权几何中的块小增益条件。该方法将局部曲率与跨参与者Lipschitz耦合边界转化为可处理的收缩认证条件，通过构建加权块度量，使得伪梯度在满足这些边界的任意区域呈现强单调性——即使其在欧几里得意义下非单调。连续流在此设计几何中呈指数收缩，且投影欧拉法与RK4离散化在基于SGN裕度与局部Lipschitz常数导出的显式步长范围内收敛。分析揭示了一种经认证的“时间尺度带”，这是一种非渐近的、基于度量的认证机制，其作用类似于TTUR方法：不同于通过趋近零的不等步长强制渐近时间尺度分离，SGN可识别相对度量权重的有限带域，使得单一步长动力学可证收缩。我们在二次博弈中验证了该框架的有效性——传统欧几里得单调性分析无法预测收敛的情形，SGN却能成功认证收敛性，并将该构造扩展至马尔可夫博弈中熵正则化策略梯度的镜像/费希尔几何。最终形成离线认证流程：在紧致区域估计曲率、耦合与Lipschitz参数，优化块权重以扩大SGN裕度，并为非单调博弈返回包含度量、收缩率与安全步长的结构化可计算收敛认证。",
    "url": "https://huggingface.co/papers/2512.06791",
    "arxiv_url": "https://arxiv.org/abs/2512.06791"
  },
  {
    "title": "Vector Quantization using Gaussian Variational Autoencoder",
    "summary": "Vector quantized variational autoencoder (VQ-VAE) is a discrete auto-encoder that compresses images into discrete tokens. It is difficult to train due to discretization. In this paper, we propose a simple yet effective technique, dubbed Gaussian Quant (GQ), that converts a Gaussian VAE with certain constraint into a VQ-VAE without training. GQ generates random Gaussian noise as a codebook and finds the closest noise to the posterior mean. Theoretically, we prove that when the logarithm of the codebook size exceeds the bits-back coding rate of the Gaussian VAE, a small quantization error is guaranteed. Practically, we propose a heuristic to train Gaussian VAE for effective GQ, named target divergence constraint (TDC). Empirically, we show that GQ outperforms previous VQ-VAEs, such as VQGAN, FSQ, LFQ, and BSQ, on both UNet and ViT architectures. Furthermore, TDC also improves upon previous Gaussian VAE discretization methods, such as TokenBridge. The source code is provided in https://github.com/tongdaxu/VQ-VAE-from-Gaussian-VAE.",
    "translation": "标题：基于高斯变分自编码器的向量量化方法\n\n摘要：向量量化变分自编码器（VQ-VAE）是一种将图像压缩为离散标记的离散自编码器，其离散化特性导致模型训练困难。本文提出一种简单而有效的技术——高斯量化（GQ），该方法通过特定约束将高斯变分自编码器直接转换为VQ-VAE而无需额外训练。GQ通过生成随机高斯噪声构建码本，并寻找与后验均值最接近的噪声向量。理论上，我们证明当码本对数值超过高斯变分自编码器的比特回传编码率时，该方法可保证较小的量化误差。实践中，我们提出一种启发式训练策略——目标散度约束（TDC），用于优化高斯变分自编码器以提升GQ效果。实验表明，在UNet和ViT架构上，GQ的性能优于VQGAN、FSQ、LFQ和BSQ等现有VQ-VAE方法。此外，TDC策略也改进了TokenBridge等传统高斯变分自编码器离散化方法。源代码已发布于https://github.com/tongdaxu/VQ-VAE-from-Gaussian-VAE。",
    "url": "https://huggingface.co/papers/2512.06609",
    "arxiv_url": "https://arxiv.org/abs/2512.06609"
  },
  {
    "title": "Embodied Referring Expression Comprehension in Human-Robot Interaction",
    "summary": "As robots enter human workspaces, there is a crucial need for them to comprehend embodied human instructions, enabling intuitive and fluent human-robot interaction (HRI). However, accurate comprehension is challenging due to a lack of large-scale datasets that capture natural embodied interactions in diverse HRI settings. Existing datasets suffer from perspective bias, single-view collection, inadequate coverage of nonverbal gestures, and a predominant focus on indoor environments. To address these issues, we present the Refer360 dataset, a large-scale dataset of embodied verbal and nonverbal interactions collected across diverse viewpoints in both indoor and outdoor settings. Additionally, we introduce MuRes, a multimodal guided residual module designed to improve embodied referring expression comprehension. MuRes acts as an information bottleneck, extracting salient modality-specific signals and reinforcing them into pre-trained representations to form complementary features for downstream tasks. We conduct extensive experiments on four HRI datasets, including the Refer360 dataset, and demonstrate that current multimodal models fail to capture embodied interactions comprehensively; however, augmenting them with MuRes consistently improves performance. These findings establish Refer360 as a valuable benchmark and exhibit the potential of guided residual learning to advance embodied referring expression comprehension in robots operating within human environments.",
    "translation": "标题：人机交互中的具身指代表达理解\n\n摘要：随着机器人进入人类工作空间，其理解具身化人类指令以实现直观流畅的人机交互（HRI）的需求日益迫切。然而，由于缺乏能够捕捉多样化HRI场景中自然具身交互的大规模数据集，准确理解面临挑战。现有数据集普遍存在视角偏差、单视角采集、非语言手势覆盖不足以及过度聚焦室内环境等问题。为应对这些挑战，我们提出了Refer360数据集——一个在室内外多视角环境下采集的大规模具身化语言与非语言交互数据集。同时，我们设计了多模态引导残差模块MuRes，以提升具身指代表达理解能力。该模块通过构建信息瓶颈，提取显著模态特异性信号并将其强化至预训练表征中，从而为下游任务构建互补特征。我们在四个HRI数据集（包括Refer360）上进行了广泛实验，结果表明当前多模态模型未能全面捕捉具身交互特征，而引入MuRes模块能持续提升模型性能。这些发现确立了Refer360作为重要基准数据集的价值，并展现了引导残差学习在推动人类环境中机器人具身指代表达理解方面的潜力。",
    "url": "https://huggingface.co/papers/2512.06558",
    "arxiv_url": "https://arxiv.org/abs/2512.06558"
  },
  {
    "title": "Structured Document Translation via Format Reinforcement Learning",
    "summary": "Recent works on structured text translation remain limited to the sentence level, as they struggle to effectively handle the complex document-level XML or HTML structures. To address this, we propose Format Reinforcement Learning (FormatRL), which employs Group Relative Policy Optimization on top of a supervised fine-tuning model to directly optimize novel structure-aware rewards: 1) TreeSim, which measures structural similarity between predicted and reference XML trees and 2) Node-chrF, which measures translation quality at the level of XML nodes. Additionally, we apply StrucAUC, a fine-grained metric distinguishing between minor errors and major structural failures. Experiments on the SAP software-documentation benchmark demonstrate improvements across six metrics and an analysis further shows how different reward functions contribute to improvements in both structural and translation quality.",
    "translation": "标题：基于格式强化学习的结构化文档翻译\n\n摘要：当前结构化文本翻译研究仍局限于句子层面，难以有效处理复杂的文档级XML或HTML结构。为此，我们提出格式强化学习方法，该方法在监督微调模型基础上采用组相对策略优化，直接优化两种新型结构感知奖励函数：1）TreeSim——衡量预测XML树与参考XML树之间的结构相似性；2）Node-chrF——在XML节点层面评估翻译质量。此外，我们引入StrucAUC细粒度评估指标，以区分细微错误与重大结构缺陷。在SAP软件文档基准测试上的实验表明，该方法在六项指标上均取得提升；进一步分析揭示了不同奖励函数如何共同促进结构完整性与翻译质量的改进。",
    "url": "https://huggingface.co/papers/2512.05100",
    "arxiv_url": "https://arxiv.org/abs/2512.05100"
  },
  {
    "title": "DZ-TDPO: Non-Destructive Temporal Alignment for Mutable State Tracking in Long-Context Dialogue",
    "summary": "Long-context dialogue systems suffer from State Inertia, where static constraints prevent models from resolving conflicts between evolving user intents and established historical context. To address this, we propose DZ-TDPO, a non-destructive alignment framework that synergizes conflict-aware dynamic KL constraints with a calibrated temporal attention bias. Experiments on the Multi-Session Chat (MSC) dataset demonstrate that DZ-TDPO achieves state-of-the-art win rates (55.4% on Phi-3.5) while maintaining robust zero-shot generalization. Our scaling analysis reveals a \"Capacity-Stability Trade-off\": while smaller models incur an \"alignment tax\" (perplexity surge) to overcome historical inertia, the larger Qwen2.5-7B model achieves 50.8% win rate with negligible perplexity overhead. This confirms that TAI can be alleviated via precise attention regulation rather than destructive weight updates, preserving general capabilities (MMLU) across model scales. Code and data are available: https://github.com/lyj20071013/DZ-TDPO",
    "translation": "标题：DZ-TDPO：面向长对话中可变状态追踪的非破坏性时序对齐方法\n\n摘要：长上下文对话系统普遍存在状态惯性问题，即静态约束阻碍模型在动态演变的用户意图与既定历史语境间有效化解冲突。为解决此问题，我们提出DZ-TDPO——一种融合冲突感知动态KL约束与校准时序注意力偏置的非破坏性对齐框架。在Multi-Session Chat（MSC）数据集上的实验表明，DZ-TDPO实现了最先进的胜率（Phi-3.5模型达55.4%），同时保持稳健的零样本泛化能力。我们的扩展分析揭示了“容量-稳定性权衡”现象：较小模型需付出“对齐代价”（困惑度激增）以克服历史惯性，而更大的Qwen2.5-7B模型以可忽略的困惑度开销实现了50.8%的胜率。这证实了时序注意力惯性可通过精确的注意力调控（而非破坏性权重更新）来缓解，从而在不同规模模型上保持通用能力（MMLU指标）。代码与数据已开源：https://github.com/lyj20071013/DZ-TDPO",
    "url": "https://huggingface.co/papers/2512.03704",
    "arxiv_url": "https://arxiv.org/abs/2512.03704"
  },
  {
    "title": "JEPA as a Neural Tokenizer: Learning Robust Speech Representations with Density Adaptive Attention",
    "summary": "We introduce a two-stage self-supervised framework that combines the Joint-Embedding Predictive Architecture (JEPA) with a Density Adaptive Attention Mechanism (DAAM) for learning robust speech representations. Stage~1 uses JEPA with DAAM to learn semantic audio features via masked prediction in latent space, fully decoupled from waveform reconstruction. Stage~2 leverages these representations for efficient tokenization using Finite Scalar Quantization (FSQ) and a mixed-radix packing scheme, followed by high-fidelity waveform reconstruction with a HiFi-GAN decoder. By integrating Gaussian mixture-based density-adaptive gating into the JEPA encoder, the model performs adaptive temporal feature selection and discovers hierarchical speech structure at a low frame rate of 2.5~Hz. The resulting tokens (47.5 tokens/sec) provide a reversible, highly compressed, and language-model-friendly representation that is competitive with, and often more efficient than, existing neural audio codecs.",
    "translation": "标题：JEPA作为神经分词器：基于密度自适应注意力机制的鲁棒语音表征学习\n\n摘要：本文提出一种两阶段自监督学习框架，将联合嵌入预测架构（JEPA）与密度自适应注意力机制（DAAM）相结合，用于学习鲁棒的语音表征。第一阶段采用集成DAAM的JEPA架构，通过在隐空间进行掩码预测来学习语义音频特征，该过程完全与波形重构解耦。第二阶段利用有限标量化（FSQ）和混合基数打包方案，将学到的表征高效转换为分词表示，再通过HiFi-GAN解码器实现高保真波形重建。通过将基于高斯混合模型的密度自适应门控机制集成到JEPA编码器中，该模型能以2.5Hz的低帧率实现自适应时序特征选择，并发现语音的层次化结构。最终生成的分词（47.5词元/秒）具有可逆性、高压缩性和语言模型友好性，其性能与现有神经音频编解码器相当，且通常具有更高的效率。",
    "url": "https://huggingface.co/papers/2512.07168",
    "arxiv_url": "https://arxiv.org/abs/2512.07168"
  },
  {
    "title": "The SAM2-to-SAM3 Gap in the Segment Anything Model Family: Why Prompt-Based Expertise Fails in Concept-Driven Image Segmentation",
    "summary": "This paper investigates the fundamental discontinuity between the latest two Segment Anything Models: SAM2 and SAM3. We explain why the expertise in prompt-based segmentation of SAM2 does not transfer to the multimodal concept-driven paradigm of SAM3. SAM2 operates through spatial prompts points, boxes, and masks yielding purely geometric and temporal segmentation. In contrast, SAM3 introduces a unified vision-language architecture capable of open-vocabulary reasoning, semantic grounding, contrastive alignment, and exemplar-based concept understanding. We structure this analysis through five core components: (1) a Conceptual Break Between Prompt-Based and Concept-Based Segmentation, contrasting spatial prompt semantics of SAM2 with multimodal fusion and text-conditioned mask generation of SAM3; (2) Architectural Divergence, detailing pure vision-temporal design of SAM2 versus integration of vision-language encoders, geometry and exemplar encoders, fusion modules, DETR-style decoders, object queries, and ambiguity-handling via Mixture-of-Experts in SAM3; (3) Dataset and Annotation Differences, contrasting SA-V video masks with multimodal concept-annotated corpora of SAM3; (4) Training and Hyperparameter Distinctions, showing why SAM2 optimization knowledge does not apply to SAM3; and (5) Evaluation, Metrics, and Failure Modes, outlining the transition from geometric IoU metrics to semantic, open-vocabulary evaluation. Together, these analyses establish SAM3 as a new class of segmentation foundation model and chart future directions for the emerging concept-driven segmentation era.",
    "translation": "标题：SAM系列模型中的SAM2至SAM3断层：基于提示的专长为何在概念驱动的图像分割中失效\n\n摘要：本文探究了最新两代Segment Anything模型（SAM2与SAM3）之间的根本性断层。我们阐释了为何SAM2在基于提示的分割任务中的专业能力无法迁移至SAM3的多模态概念驱动范式。SAM2通过空间提示（点、框、掩码）进行操作，产生纯几何与时间维度的分割结果；而SAM3引入了统一的视觉-语言架构，具备开放词汇推理、语义 grounding、对比对齐以及基于范例的概念理解能力。本研究通过五个核心组成部分展开分析：（1）基于提示与基于概念的分割模式间的理念断裂，对比SAM2的空间提示语义与SAM3的多模态融合及文本条件掩码生成机制；（2）架构差异，详述SAM2的纯视觉-时序设计与SAM3中视觉-语言编码器、几何与范例编码器、融合模块、DETR风格解码器、对象查询机制以及通过专家混合模型处理模糊性的整合架构；（3）数据集与标注差异，对比SAM2的SA-V视频掩码数据与SAM3的多模态概念标注语料库；（4）训练与超参数区别，说明SAM2的优化经验为何不适用于SAM3；（5）评估体系、指标与失效模式，梳理从几何IoU指标向语义化、开放词汇评估范式的转变。这些分析共同确立了SAM3作为新一代分割基础模型的地位，并为新兴的概念驱动分割时代指明了未来发展方向。",
    "url": "https://huggingface.co/papers/2512.06032",
    "arxiv_url": "https://arxiv.org/abs/2512.06032"
  }
]