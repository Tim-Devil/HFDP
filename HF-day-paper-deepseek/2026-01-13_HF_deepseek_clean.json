[
  {
    "title": "Watching, Reasoning, and Searching: A Video Deep Research Benchmark on Open Web for Agentic Video Reasoning",
    "summary": "In real-world video question answering scenarios, videos often provide only localized visual cues, while verifiable answers are distributed across the open web; models therefore need to jointly perform cross-frame clue extraction, iterative retrieval, and multi-hop reasoning-based verification. To bridge this gap, we construct the first video deep research benchmark, VideoDR. VideoDR centers on video-conditioned open-domain video question answering, requiring cross-frame visual anchor extraction, interactive web retrieval, and multi-hop reasoning over joint video-web evidence; through rigorous human annotation and quality control, we obtain high-quality video deep research samples spanning six semantic domains. We evaluate multiple closed-source and open-source multimodal large language models under both the Workflow and Agentic paradigms, and the results show that Agentic is not consistently superior to Workflow: its gains depend on a model's ability to maintain the initial video anchors over long retrieval chains. Further analysis indicates that goal drift and long-horizon consistency are the core bottlenecks. In sum, VideoDR provides a systematic benchmark for studying video agents in open-web settings and reveals the key challenges for next-generation video deep research agents.",
    "translation": "标题：观看、推理与检索：面向开放网络的智能体视频推理深度研究基准\n\n摘要：在现实世界的视频问答场景中，视频通常仅提供局部视觉线索，而可验证的答案广泛分布于开放网络中；因此模型需要联合执行跨帧线索提取、迭代检索以及基于多跳推理的验证。为填补这一空白，我们构建了首个视频深度研究基准VideoDR。VideoDR以视频条件化的开放域视频问答为核心，要求进行跨帧视觉锚点提取、交互式网络检索以及对视频-网络联合证据的多跳推理；通过严格的人工标注与质量控制，我们获得了涵盖六个语义领域的高质量视频深度研究样本。我们在工作流范式与智能体范式下评估了多个闭源与开源多模态大语言模型，结果表明智能体范式并非始终优于工作流范式：其优势取决于模型在长检索链中保持初始视频锚点的能力。进一步分析指出，目标漂移与长程一致性是核心瓶颈。综上所述，VideoDR为研究开放网络环境下的视频智能体提供了系统性基准，并揭示了下一代视频深度研究智能体面临的关键挑战。",
    "url": "https://huggingface.co/papers/2601.06943",
    "arxiv_url": "https://arxiv.org/abs/2601.06943"
  },
  {
    "title": "BabyVision: Visual Reasoning Beyond Language",
    "summary": "While humans develop core visual skills long before acquiring language, contemporary Multimodal LLMs (MLLMs) still rely heavily on linguistic priors to compensate for their fragile visual understanding. We uncovered a crucial fact: state-of-the-art MLLMs consistently fail on basic visual tasks that humans, even 3-year-olds, can solve effortlessly. To systematically investigate this gap, we introduce BabyVision, a benchmark designed to assess core visual abilities independent of linguistic knowledge for MLLMs. BabyVision spans a wide range of tasks, with 388 items divided into 22 subclasses across four key categories. Empirical results and human evaluation reveal that leading MLLMs perform significantly below human baselines. Gemini3-Pro-Preview scores 49.7, lagging behind 6-year-old humans and falling well behind the average adult score of 94.1. These results show despite excelling in knowledge-heavy evaluations, current MLLMs still lack fundamental visual primitives. Progress in BabyVision represents a step toward human-level visual perception and reasoning capabilities. We also explore solving visual reasoning with generation models by proposing BabyVision-Gen and automatic evaluation toolkit. Our code and benchmark data are released at https://github.com/UniPat-AI/BabyVision for reproduction.",
    "translation": "标题：BabyVision：超越语言的视觉推理能力研究\n\n摘要：人类在掌握语言能力之前便已发展出核心视觉技能，而当前的多模态大语言模型（MLLMs）仍严重依赖语言先验知识以弥补其薄弱的视觉理解能力。我们发现一个重要事实：即使面对三岁幼儿也能轻松解决的基础视觉任务，最先进的多模态大语言模型仍持续表现失败。为系统探究这一差距，我们提出BabyVision基准测试，旨在评估多模态大语言模型独立于语言知识的核心视觉能力。该基准涵盖四大关键类别，包含22个子类共388项任务。实证结果与人工评估表明，主流多模态大语言模型的表现显著低于人类基线水平。其中Gemini3-Pro-Preview得分仅为49.7，落后于六岁儿童水平，与成人平均分94.1存在巨大差距。这些结果揭示，尽管当前多模态大语言模型在知识密集型评估中表现优异，其仍缺乏基础视觉原语能力。BabyVision研究的进展标志着向人类水平视觉感知与推理能力迈出的重要一步。我们同时通过提出BabyVision-Gen生成模型与自动评估工具包探索视觉推理任务的解决方案。相关代码与基准数据已发布于https://github.com/UniPat-AI/BabyVision以供复现研究。",
    "url": "https://huggingface.co/papers/2601.06521",
    "arxiv_url": "https://arxiv.org/abs/2601.06521"
  },
  {
    "title": "PaCoRe: Learning to Scale Test-Time Compute with Parallel Coordinated Reasoning",
    "summary": "We introduce Parallel Coordinated Reasoning (PaCoRe), a training-and-inference framework designed to overcome a central limitation of contemporary language models: their inability to scale test-time compute (TTC) far beyond sequential reasoning under a fixed context window. PaCoRe departs from the traditional sequential paradigm by driving TTC through massive parallel exploration coordinated via a message-passing architecture in multiple rounds. Each round launches many parallel reasoning trajectories, compacts their findings into context-bounded messages, and synthesizes these messages to guide the next round and ultimately produce the final answer. Trained end-to-end with large-scale, outcome-based reinforcement learning, the model masters the synthesis abilities required by PaCoRe and scales to multi-million-token effective TTC without exceeding context limits. The approach yields strong improvements across diverse domains, and notably pushes reasoning beyond frontier systems in mathematics: an 8B model reaches 94.5% on HMMT 2025, surpassing GPT-5's 93.2% by scaling effective TTC to roughly two million tokens. We open-source model checkpoints, training data, and the full inference pipeline to accelerate follow-up work.",
    "translation": "标题：PaCoRe：通过并行协同推理学习扩展测试时计算\n\n摘要：本文提出并行协同推理（PaCoRe），一种旨在克服当代语言模型核心局限的训练与推理框架：即模型无法在固定上下文窗口下，将测试时计算（TTC）规模显著扩展至超越顺序推理的范畴。PaCoRe摒弃了传统的顺序推理范式，通过在多轮消息传递架构协调下的大规模并行探索来驱动测试时计算。每一轮并行启动多个推理轨迹，将其发现压缩为上下文受限的消息，并综合这些消息以指导下一轮推理，最终生成答案。通过基于结果的大规模端到端强化学习训练，模型掌握了PaCoRe所需的信息综合能力，并能够将有效测试时计算扩展至数百万令牌量级，同时不突破上下文限制。该方法在多个领域均带来显著性能提升，尤其在数学推理方面超越了前沿系统：一个80亿参数的模型在HMMT 2025数据集上达到94.5%的准确率，通过将有效测试时计算扩展至约两百万令牌，超越了GPT-5的93.2%表现。我们开源了模型检查点、训练数据及完整推理流程，以加速后续研究。",
    "url": "https://huggingface.co/papers/2601.05593",
    "arxiv_url": "https://arxiv.org/abs/2601.05593"
  },
  {
    "title": "MHLA: Restoring Expressivity of Linear Attention via Token-Level Multi-Head",
    "summary": "While the Transformer architecture dominates many fields, its quadratic self-attention complexity hinders its use in large-scale applications. Linear attention offers an efficient alternative, but its direct application often degrades performance, with existing fixes typically re-introducing computational overhead through extra modules (e.g., depthwise separable convolution) that defeat the original purpose. In this work, we identify a key failure mode in these methods: global context collapse, where the model loses representational diversity. To address this, we propose Multi-Head Linear Attention (MHLA), which preserves this diversity by computing attention within divided heads along the token dimension. We prove that MHLA maintains linear complexity while recovering much of the expressive power of softmax attention, and verify its effectiveness across multiple domains, achieving a 3.6\\% improvement on ImageNet classification, a 6.3\\% gain on NLP, a 12.6\\% improvement on image generation, and a 41\\% enhancement on video generation under the same time complexity.",
    "translation": "标题：MHLA：通过令牌级多头机制恢复线性注意力的表达能力\n\n摘要：尽管Transformer架构在许多领域占据主导地位，但其二次复杂度的自注意力机制限制了其在大规模应用中的使用。线性注意力提供了一种高效的替代方案，但其直接应用往往会导致性能下降。现有改进方法通常通过引入额外模块（如深度可分离卷积）重新引入计算开销，违背了提升效率的初衷。本文指出这些方法存在一个关键缺陷：全局上下文坍缩，即模型丧失表征多样性。为解决这一问题，我们提出了多头线性注意力机制（MHLA），该机制通过在令牌维度上划分多头并分别计算注意力，有效保持了表征多样性。我们证明MHLA在保持线性复杂度的同时，能够恢复softmax注意力的大部分表达能力，并在多个领域验证了其有效性：在相同时间复杂度下，ImageNet分类任务提升3.6%，自然语言处理任务提升6.3%，图像生成任务提升12.6%，视频生成任务提升41%。",
    "url": "https://huggingface.co/papers/2601.07832",
    "arxiv_url": "https://arxiv.org/abs/2601.07832"
  },
  {
    "title": "X-Coder: Advancing Competitive Programming with Fully Synthetic Tasks, Solutions, and Tests",
    "summary": "Competitive programming presents great challenges for Code LLMs due to its intensive reasoning demands and high logical complexity. However, current Code LLMs still rely heavily on real-world data, which limits their scalability. In this paper, we explore a fully synthetic approach: training Code LLMs with entirely generated tasks, solutions, and test cases, to empower code reasoning models without relying on real-world data. To support this, we leverage feature-based synthesis to propose a novel data synthesis pipeline called SynthSmith. SynthSmith shows strong potential in producing diverse and challenging tasks, along with verified solutions and tests, supporting both supervised fine-tuning and reinforcement learning. Based on the proposed synthetic SFT and RL datasets, we introduce the X-Coder model series, which achieves a notable pass rate of 62.9 avg@8 on LiveCodeBench v5 and 55.8 on v6, outperforming DeepCoder-14B-Preview and AReal-boba2-14B despite having only 7B parameters. In-depth analysis reveals that scaling laws hold on our synthetic dataset, and we explore which dimensions are more effective to scale. We further provide insights into code-centric reinforcement learning and highlight the key factors that shape performance through detailed ablations and analysis. Our findings demonstrate that scaling high-quality synthetic data and adopting staged training can greatly advance code reasoning, while mitigating reliance on real-world coding data.",
    "translation": "标题：X-Coder：通过全合成任务、解决方案与测试推进竞技编程能力\n\n摘要：竞技编程因其密集的推理需求与高逻辑复杂性，对代码大语言模型提出了巨大挑战。然而，当前代码大语言模型仍严重依赖现实世界数据，这限制了其可扩展性。本文探索了一种全合成方法：使用完全生成的任务、解决方案与测试用例训练代码大语言模型，从而在不依赖现实数据的情况下增强代码推理能力。为此，我们基于特征合成技术提出了一种名为SynthSmith的新型数据合成流程。SynthSmith展现出生成多样化、高难度任务以及已验证解决方案与测试的强大潜力，同时支持监督微调与强化学习。基于所提出的合成监督微调与强化学习数据集，我们推出了X-Coder模型系列，该系列在LiveCodeBench v5上达到62.9 avg@8的显著通过率，在v6上达到55.8，尽管仅拥有70亿参数，其表现仍优于DeepCoder-14B-Preview与AReal-boba2-14B。深入分析表明，缩放定律在我们的合成数据集上依然成立，并进一步探讨了哪些维度对扩展更有效。我们通过详细的消融实验与分析，深入揭示了以代码为中心的强化学习机制，并阐明了影响性能的关键因素。研究结果表明，扩展高质量合成数据并采用分阶段训练策略能显著推进代码推理能力，同时减少对现实世界编程数据的依赖。",
    "url": "https://huggingface.co/papers/2601.06953",
    "arxiv_url": "https://arxiv.org/abs/2601.06953"
  },
  {
    "title": "GlimpRouter: Efficient Collaborative Inference by Glimpsing One Token of Thoughts",
    "summary": "Large Reasoning Models (LRMs) achieve remarkable performance by explicitly generating multi-step chains of thought, but this capability incurs substantial inference latency and computational cost. Collaborative inference offers a promising solution by selectively allocating work between lightweight and large models, yet a fundamental challenge remains: determining when a reasoning step requires the capacity of a large model or the efficiency of a small model. Existing routing strategies either rely on local token probabilities or post-hoc verification, introducing significant inference overhead. In this work, we propose a novel perspective on step-wise collaboration: the difficulty of a reasoning step can be inferred from its very first token. Inspired by the \"Aha Moment\" phenomenon in LRMs, we show that the entropy of the initial token serves as a strong predictor of step difficulty. Building on this insight, we introduce GlimpRouter, a training-free step-wise collaboration framework. GlimpRouter employs a lightweight model to generate only the first token of each reasoning step and routes the step to a larger model only when the initial token entropy exceeds a threshold. Experiments on multiple benchmarks demonstrate that our approach significantly reduces inference latency while preserving accuracy. For instance, GlimpRouter attains a substantial 10.7% improvement in accuracy while reducing inference latency by 25.9% compared to a standalone large model on AIME25. These results suggest a simple yet effective mechanism for reasoning: allocating computation based on a glimpse of thought rather than full-step evaluation.",
    "translation": "标题：GlimpRouter：基于思维单词元瞥见的协同推理高效框架\n\n摘要：大型推理模型通过显式生成多步思维链获得了卓越性能，但该能力会带来显著的推理延迟与计算成本。协同推理通过将计算任务在轻量模型与大型模型间进行选择性分配，提供了具有前景的解决方案，但核心挑战依然存在：如何判断推理步骤何时需要大型模型的强大能力，何时可交由轻量模型高效处理。现有路由策略或依赖局部词元概率，或采用事后验证机制，均会引入显著的推理开销。本研究提出一种新颖的步进式协同视角：推理步骤的难度可通过其首个词元进行推断。受大型推理模型中“顿悟时刻”现象的启发，我们发现初始词元的熵值可作为步骤难度的有效预测指标。基于此洞见，我们提出了GlimpRouter——一种无需训练的步进式协同推理框架。该框架使用轻量模型仅生成每个推理步骤的首个词元，仅当初始词元熵值超过阈值时才将当前步骤路由至大型模型。在多基准测试上的实验表明，该方法在保持准确性的同时显著降低了推理延迟。例如在AIME25基准上，GlimpRouter相较于独立大型模型在准确率提升10.7%的同时，推理延迟降低了25.9%。这些结果表明了一种简洁而有效的推理机制：基于思维片段的瞥见而非完整步骤评估来分配计算资源。",
    "url": "https://huggingface.co/papers/2601.05110",
    "arxiv_url": "https://arxiv.org/abs/2601.05110"
  },
  {
    "title": "Lost in the Noise: How Reasoning Models Fail with Contextual Distractors",
    "summary": "Recent advances in reasoning models and agentic AI systems have led to an increased reliance on diverse external information. However, this shift introduces input contexts that are inherently noisy, a reality that current sanitized benchmarks fail to capture. We introduce NoisyBench, a comprehensive benchmark that systematically evaluates model robustness across 11 datasets in RAG, reasoning, alignment, and tool-use tasks against diverse noise types, including random documents, irrelevant chat histories, and hard negative distractors. Our evaluation reveals a catastrophic performance drop of up to 80% in state-of-the-art models when faced with contextual distractors. Crucially, we find that agentic workflows often amplify these errors by over-trusting noisy tool outputs, and distractors can trigger emergent misalignment even without adversarial intent. We find that prompting, context engineering, SFT, and outcome-reward only RL fail to ensure robustness; in contrast, our proposed Rationale-Aware Reward (RARE) significantly strengthens resilience by incentivizing the identification of helpful information within noise. Finally, we uncover an inverse scaling trend where increased test-time computation leads to worse performance in noisy settings and demonstrate via attention visualization that models disproportionately focus on distractor tokens, providing vital insights for building the next generation of robust, reasoning-capable agents.",
    "translation": "标题：迷失于噪声：推理模型如何受上下文干扰项影响而失效\n\n摘要：推理模型与智能体人工智能系统的最新进展，使得对多样化外部信息的依赖日益增强。然而，这种转变引入了本质上充满噪声的输入上下文，而当前经过净化的基准测试未能捕捉这一现实。我们提出了NoisyBench，这是一个综合性基准，系统性地评估了模型在RAG、推理、对齐和工具使用等11个数据集上，面对随机文档、无关对话历史和困难负样本干扰等多种噪声类型时的鲁棒性。我们的评估显示，在面对上下文干扰项时，最先进的模型性能会出现高达80%的灾难性下降。关键的是，我们发现智能体工作流常因过度信任带噪声的工具输出而放大这些错误，并且干扰项即使在没有对抗意图的情况下也可能引发突发性失准。我们发现，提示工程、上下文工程、监督微调以及仅基于结果奖励的强化学习均无法确保鲁棒性；相比之下，我们提出的**理性感知奖励机制**通过激励模型识别噪声中有用信息，显著增强了其抗干扰能力。最后，我们揭示了一种逆向缩放趋势：在噪声环境中，增加测试时计算量反而导致性能下降，并通过注意力可视化证明模型会过度关注干扰标记。这些发现为构建下一代鲁棒且具备强推理能力的智能体提供了重要洞见。",
    "url": "https://huggingface.co/papers/2601.07226",
    "arxiv_url": "https://arxiv.org/abs/2601.07226"
  },
  {
    "title": "OS-Symphony: A Holistic Framework for Robust and Generalist Computer-Using Agent",
    "summary": "While Vision-Language Models (VLMs) have significantly advanced Computer-Using Agents (CUAs), current frameworks struggle with robustness in long-horizon workflows and generalization in novel domains. These limitations stem from a lack of granular control over historical visual context curation and the absence of visual-aware tutorial retrieval. To bridge these gaps, we introduce OS-Symphony, a holistic framework that comprises an Orchestrator coordinating two key innovations for robust automation: (1) a Reflection-Memory Agent that utilizes milestone-driven long-term memory to enable trajectory-level self-correction, effectively mitigating visual context loss in long-horizon tasks; (2) Versatile Tool Agents featuring a Multimodal Searcher that adopts a SeeAct paradigm to navigate a browser-based sandbox to synthesize live, visually aligned tutorials, thereby resolving fidelity issues in unseen scenarios. Experimental results demonstrate that OS-Symphony delivers substantial performance gains across varying model scales, establishing new state-of-the-art results on three online benchmarks, notably achieving 65.84% on OSWorld.",
    "translation": "标题：OS-Symphony：一种面向鲁棒通用计算机使用智能体的整体框架\n\n摘要：尽管视觉语言模型（VLMs）显著推动了计算机使用智能体（CUAs）的发展，但现有框架在长流程工作流的鲁棒性和新领域泛化能力方面仍面临挑战。这些局限主要源于对历史视觉上下文管理缺乏细粒度控制，以及缺少视觉感知的教程检索机制。为弥补上述不足，本文提出OS-Symphony整体框架，其核心协调器整合了两项关键创新以实现鲁棒自动化：（1）反思记忆智能体——采用里程碑驱动的长期记忆机制实现轨迹级自我修正，有效缓解长流程任务中的视觉上下文丢失问题；（2）多功能工具智能体——配备多模态搜索器，通过SeeAct范式在基于浏览器的沙箱环境中合成实时视觉对齐教程，从而解决未知场景中的保真度问题。实验结果表明，OS-Symphony在不同规模模型上均实现显著性能提升，在三大在线基准测试中创下最新最优记录，其中在OSWorld基准上达到65.84%的卓越表现。",
    "url": "https://huggingface.co/papers/2601.07779",
    "arxiv_url": "https://arxiv.org/abs/2601.07779"
  },
  {
    "title": "Beyond Hard Masks: Progressive Token Evolution for Diffusion Language Models",
    "summary": "Diffusion Language Models (DLMs) offer a promising alternative for language modeling by enabling parallel decoding through iterative refinement. However, most DLMs rely on hard binary masking and discrete token assignments, which hinder the revision of early decisions and underutilize intermediate probabilistic representations. In this paper, we propose EvoToken-DLM, a novel diffusion-based language modeling approach that replaces hard binary masks with evolving soft token distributions. EvoToken-DLM enables a progressive transition from masked states to discrete outputs, supporting revisable decoding. To effectively support this evolution, we introduce continuous trajectory supervision, which aligns training objectives with iterative probabilistic updates. Extensive experiments across multiple benchmarks show that EvoToken-DLM consistently achieves superior performance, outperforming strong diffusion-based and masked DLM baselines. Project webpage: https://aim-uofa.github.io/EvoTokenDLM.",
    "translation": "标题：超越硬掩码：扩散语言模型的渐进式词元演化\n\n摘要：扩散语言模型通过迭代优化实现并行解码，为语言建模提供了一种前景广阔的替代方案。然而，现有扩散语言模型大多依赖硬二值掩码和离散词元分配机制，这限制了早期决策的修正能力，且未能充分利用中间概率表示。本文提出EvoToken-DLM——一种创新的基于扩散的语言建模方法，通过演化的软词元分布替代传统硬二值掩码。该模型实现了从掩码状态到离散输出的渐进式过渡，支持可修正的解码过程。为有效支撑这种演化机制，我们引入连续轨迹监督方法，使训练目标与迭代概率更新过程保持一致。在多个基准测试上的实验表明，EvoToken-DLM始终取得卓越性能，显著优于现有基于扩散和掩码的扩散语言模型基线。项目主页：https://aim-uofa.github.io/EvoTokenDLM。",
    "url": "https://huggingface.co/papers/2601.07351",
    "arxiv_url": "https://arxiv.org/abs/2601.07351"
  },
  {
    "title": "Controllable Memory Usage: Balancing Anchoring and Innovation in Long-Term Human-Agent Interaction",
    "summary": "As LLM-based agents are increasingly used in long-term interactions, cumulative memory is critical for enabling personalization and maintaining stylistic consistency. However, most existing systems adopt an ``all-or-nothing'' approach to memory usage: incorporating all relevant past information can lead to Memory Anchoring, where the agent is trapped by past interactions, while excluding memory entirely results in under-utilization and the loss of important interaction history. We show that an agent's reliance on memory can be modeled as an explicit and user-controllable dimension. We first introduce a behavioral metric of memory dependence to quantify the influence of past interactions on current outputs. We then propose Steerable Memory Agent, SteeM, a framework that allows users to dynamically regulate memory reliance, ranging from a fresh-start mode that promotes innovation to a high-fidelity mode that closely follows interaction history. Experiments across different scenarios demonstrate that our approach consistently outperforms conventional prompting and rigid memory masking strategies, yielding a more nuanced and effective control for personalized human-agent collaboration.",
    "translation": "标题：可控记忆使用：长期人机交互中的锚定与创新平衡\n\n摘要：随着基于大语言模型的智能体日益广泛地应用于长期交互场景，累积记忆对于实现个性化服务和保持风格一致性至关重要。然而，现有系统大多采用“全有或全无”的记忆使用策略：若完全纳入历史相关信息可能导致“记忆锚定”现象，使智能体受困于过往交互模式；而完全排除记忆则会造成信息利用不足与重要交互历史的丢失。本研究证明，智能体对记忆的依赖程度可被建模为显式且用户可控的维度。我们首先提出记忆依赖性的行为度量指标，用以量化历史交互对当前输出的影响程度。继而提出可调控记忆智能体框架SteeM，该框架允许用户动态调节记忆依赖强度——从促进创新的“全新启动”模式到严格遵循交互历史的“高保真”模式。跨场景实验表明，相较于传统提示方法与刚性记忆掩码策略，本方法能持续生成更精细、更有效的控制机制，为个性化人机协作提供优化解决方案。",
    "url": "https://huggingface.co/papers/2601.05107",
    "arxiv_url": "https://arxiv.org/abs/2601.05107"
  },
  {
    "title": "DrivingGen: A Comprehensive Benchmark for Generative Video World Models in Autonomous Driving",
    "summary": "Video generation models, as one form of world models, have emerged as one of the most exciting frontiers in AI, promising agents the ability to imagine the future by modeling the temporal evolution of complex scenes. In autonomous driving, this vision gives rise to driving world models: generative simulators that imagine ego and agent futures, enabling scalable simulation, safe testing of corner cases, and rich synthetic data generation. Yet, despite fast-growing research activity, the field lacks a rigorous benchmark to measure progress and guide priorities. Existing evaluations remain limited: generic video metrics overlook safety-critical imaging factors; trajectory plausibility is rarely quantified; temporal and agent-level consistency is neglected; and controllability with respect to ego conditioning is ignored. Moreover, current datasets fail to cover the diversity of conditions required for real-world deployment. To address these gaps, we present DrivingGen, the first comprehensive benchmark for generative driving world models. DrivingGen combines a diverse evaluation dataset curated from both driving datasets and internet-scale video sources, spanning varied weather, time of day, geographic regions, and complex maneuvers, with a suite of new metrics that jointly assess visual realism, trajectory plausibility, temporal coherence, and controllability. Benchmarking 14 state-of-the-art models reveals clear trade-offs: general models look better but break physics, while driving-specific ones capture motion realistically but lag in visual quality. DrivingGen offers a unified evaluation framework to foster reliable, controllable, and deployable driving world models, enabling scalable simulation, planning, and data-driven decision-making.",
    "translation": "标题：DrivingGen：自动驾驶生成式视频世界模型综合基准\n\n摘要：作为世界模型的一种形式，视频生成模型已成为人工智能领域最令人兴奋的前沿方向之一，其通过建模复杂场景的时间演化，使智能体具备预测未来的能力。在自动驾驶领域，这一愿景催生了驾驶世界模型：这类生成式模拟器能够推演自车与其他交通参与者的未来状态，从而实现可扩展的仿真、极端场景的安全测试以及丰富的合成数据生成。然而，尽管相关研究快速增长，该领域仍缺乏严谨的基准来衡量进展并指导研究方向。现有评估方法存在明显局限：通用视频指标忽略了安全关键的成像因素；轨迹合理性鲜少被量化；时间一致性与智能体层级一致性未被充分考量；基于自车条件的可控性亦遭忽视。此外，当前数据集未能覆盖现实部署所需的多变场景条件。为弥补这些不足，我们提出了DrivingGen——首个面向生成式驾驶世界模型的综合基准。DrivingGen整合了从驾驶数据集和互联网规模视频源中精选的多样化评估数据集，涵盖不同天气、昼夜时段、地理区域及复杂驾驶场景，并配套一套全新评估指标，从视觉真实感、轨迹合理性、时间连贯性和可控性四个维度进行联合评估。通过对14个前沿模型的基准测试，我们发现了明显的性能权衡：通用模型视觉表现更优但违背物理规律，而驾驶专用模型能真实捕捉运动模式却在视觉质量上存在不足。DrivingGen提供了一个统一的评估框架，旨在推动可靠、可控、可部署的驾驶世界模型发展，为可扩展仿真、路径规划及数据驱动决策提供支持。",
    "url": "https://huggingface.co/papers/2601.01528",
    "arxiv_url": "https://arxiv.org/abs/2601.01528"
  },
  {
    "title": "MegaFlow: Large-Scale Distributed Orchestration System for the Agentic Era",
    "summary": "The rapid development of interactive and autonomous AI systems signals our entry into the agentic era. Training and evaluating agents on complex agentic tasks such as software engineering and computer use requires not only efficient model computation but also sophisticated infrastructure capable of coordinating vast agent-environment interactions. However, no open-source infrastructure can effectively support large-scale training and evaluation on such complex agentic tasks. To address this challenge, we present MegaFlow, a large-scale distributed orchestration system that enables efficient scheduling, resource allocation, and fine-grained task management for agent-environment workloads. MegaFlow abstracts agent training infrastructure into three independent services (Model Service, Agent Service, and Environment Service) that interact through unified interfaces, enabling independent scaling and flexible resource allocation across diverse agent-environment configurations. In our agent training deployments, MegaFlow successfully orchestrates tens of thousands of concurrent agent tasks while maintaining high system stability and achieving efficient resource utilization. By enabling such large-scale agent training, MegaFlow addresses a critical infrastructure gap in the emerging agentic AI landscape.",
    "translation": "标题：MegaFlow：面向智能体时代的大规模分布式编排系统\n\n摘要：交互式与自主人工智能系统的快速发展标志着我们正迈入智能体时代。在软件工程、计算机操作等复杂智能体任务上训练与评估智能体，不仅需要高效的模型计算能力，更依赖于能够协调海量智能体-环境交互的复杂基础设施。然而，当前尚无开源基础设施能有效支持此类复杂智能体任务的大规模训练与评估。为应对这一挑战，本文提出MegaFlow——一个面向智能体-环境工作负载的大规模分布式编排系统，可实现高效的任务调度、资源分配与细粒度任务管理。MegaFlow将智能体训练基础设施抽象为三个通过统一接口交互的独立服务（模型服务、智能体服务与环境服务），支持在不同智能体-环境配置中实现独立扩展与灵活的资源分配。在实际部署的智能体训练场景中，MegaFlow成功协调了数万个并发智能体任务，在保持系统高稳定性的同时实现了高效的资源利用率。通过支持如此大规模的智能体训练，MegaFlow填补了新兴智能体人工智能领域的关键基础设施空白。",
    "url": "https://huggingface.co/papers/2601.07526",
    "arxiv_url": "https://arxiv.org/abs/2601.07526"
  },
  {
    "title": "Boosting Latent Diffusion Models via Disentangled Representation Alignment",
    "summary": "Latent Diffusion Models (LDMs) generate high-quality images by operating in a compressed latent space, typically obtained through image tokenizers such as Variational Autoencoders (VAEs). In pursuit of a generation-friendly VAE, recent studies have explored leveraging Vision Foundation Models (VFMs) as representation alignment targets for VAEs, mirroring the approach commonly adopted for LDMs. Although this yields certain performance gains, using the same alignment target for both VAEs and LDMs overlooks their fundamentally different representational requirements. We advocate that while LDMs benefit from latents retaining high-level semantic concepts, VAEs should excel in semantic disentanglement, enabling encoding of attribute-level information in a structured way. To address this, we propose the Semantic disentangled VAE (Send-VAE), explicitly optimized for disentangled representation learning through aligning its latent space with the semantic hierarchy of pre-trained VFMs. Our approach employs a non-linear mapper network to transform VAE latents, aligning them with VFMs to bridge the gap between attribute-level disentanglement and high-level semantics, facilitating effective guidance for VAE learning. We evaluate semantic disentanglement via linear probing on attribute prediction tasks, showing strong correlation with improved generation performance. Finally, using Send-VAE, we train flow-based transformers SiTs; experiments show Send-VAE significantly speeds up training and achieves a state-of-the-art FID of 1.21 and 1.75 with and without classifier-free guidance on ImageNet 256x256.",
    "translation": "标题：通过解耦表征对齐增强潜在扩散模型\n\n摘要：潜在扩散模型通过在压缩的潜在空间中操作生成高质量图像，该空间通常通过变分自编码器等图像标记器获得。为构建更利于生成的变分自编码器，近期研究探索利用视觉基础模型作为变分自编码器的表征对齐目标，这与潜在扩散模型常用的方法相呼应。尽管这种方法带来了一定的性能提升，但对变分自编码器和潜在扩散模型使用相同的对齐目标，忽略了两者根本不同的表征需求。我们认为，潜在扩散模型受益于保留高层语义概念的潜在表示，而变分自编码器则应擅长语义解耦，能够以结构化方式编码属性级信息。为此，我们提出语义解耦变分自编码器，通过将其潜在空间与预训练视觉基础模型的语义层次对齐，显式优化解耦表征学习。该方法采用非线性映射网络转换变分自编码器潜在表示，使其与视觉基础模型对齐，从而弥合属性级解耦与高层语义之间的鸿沟，为变分自编码器学习提供有效指导。我们通过在属性预测任务上的线性探针评估语义解耦效果，证明其与生成性能提升存在强相关性。最终，基于语义解耦变分自编码器训练的流式变换器实验表明：该编码器显著加速训练过程，在ImageNet 256×256数据集上使用/不使用无分类器引导时，分别达到1.21和1.75的最新FID指标。",
    "url": "https://huggingface.co/papers/2601.05823",
    "arxiv_url": "https://arxiv.org/abs/2601.05823"
  },
  {
    "title": "What Users Leave Unsaid: Under-Specified Queries Limit Vision-Language Models",
    "summary": "Current vision-language benchmarks predominantly feature well-structured questions with clear, explicit prompts. However, real user queries are often informal and underspecified. Users naturally leave much unsaid, relying on images to convey context. We introduce HAERAE-Vision, a benchmark of 653 real-world visual questions from Korean online communities (0.76% survival from 86K candidates), each paired with an explicit rewrite, yielding 1,306 query variants in total. Evaluating 39 VLMs, we find that even state-of-the-art models (GPT-5, Gemini 2.5 Pro) achieve under 50% on the original queries. Crucially, query explicitation alone yields 8 to 22 point improvements, with smaller models benefiting most. We further show that even with web search, under-specified queries underperform explicit queries without search, revealing that current retrieval cannot compensate for what users leave unsaid. Our findings demonstrate that a substantial portion of VLM difficulty stem from natural query under-specification instead of model capability, highlighting a critical gap between benchmark evaluation and real-world deployment.",
    "translation": "标题：用户未尽之言：欠明确查询对视觉语言模型的限制\n\n摘要：当前的视觉语言基准测试主要采用结构清晰、提示明确的规范化问题。然而，真实用户查询往往具有非正式性和欠明确性。用户通常会省略大量背景信息，依赖图像传递语境。本文提出HAERAE-Vision基准数据集，该数据集收集自韩国网络社区的653个真实视觉问题（从8.6万候选问题中筛选保留0.76%），每个问题均配有明确改写版本，共形成1,306个查询变体。通过对39个视觉语言模型的评估发现，即使是前沿模型（GPT-5、Gemini 2.5 Pro）在原始查询上的准确率也不足50%。关键发现表明，仅通过查询明确化处理即可带来8至22个百分点的性能提升，其中较小模型获益最为显著。进一步研究显示，即使结合网络搜索，欠明确查询的表现仍逊于未使用搜索的明确查询，这揭示出现有检索技术无法弥补用户省略的语境信息。我们的研究证明，视觉语言模型面临的主要困难很大程度上源于自然查询的欠明确特性，而非模型能力本身，这凸显出基准测试评估与实际应用部署之间存在关键差距。",
    "url": "https://huggingface.co/papers/2601.06165",
    "arxiv_url": "https://arxiv.org/abs/2601.06165"
  },
  {
    "title": "ET-Agent: Incentivizing Effective Tool-Integrated Reasoning Agent via Behavior Calibration",
    "summary": "Large Language Models (LLMs) can extend their parameter knowledge limits by adopting the Tool-Integrated Reasoning (TIR) paradigm. However, existing LLM-based agent training framework often focuses on answers' accuracy, overlooking specific alignment for behavior patterns. Consequently, agent often exhibits ineffective actions during TIR tasks, such as redundant and insufficient tool calls. How to calibrate erroneous behavioral patterns when executing TIR tasks, thereby exploring effective trajectories, remains an open-ended problem. In this paper, we propose ET-Agent, a training framework for calibrating agent's tool-use behavior through two synergistic perspectives: Self-evolving Data Flywheel and Behavior Calibration Training. Specifically, we introduce a self-evolutionary data flywheel to generate enhanced data, used to fine-tune LLM to improve its exploration ability. Based on this, we implement an two-phases behavior-calibration training framework. It is designed to progressively calibrate erroneous behavioral patterns to optimal behaviors. Further in-depth experiments confirm the superiority of  across multiple dimensions, including correctness, efficiency, reasoning conciseness, and tool execution accuracy. Our ET-Agent framework provides practical insights for research in the TIR field. Codes can be found in https://github.com/asilverlight/ET-Agent",
    "translation": "标题：ET-Agent：通过行为校准激励高效工具集成推理智能体\n\n摘要：大型语言模型（LLMs）可通过采用工具集成推理范式突破其参数知识限制。然而，现有基于LLM的智能体训练框架通常侧重于答案准确性，而忽视了对行为模式的针对性对齐，导致智能体在TIR任务中常出现低效行为，例如冗余或不足的工具调用。如何在校准TIR任务执行过程中的错误行为模式、进而探索高效轨迹，仍是一个开放性问题。本文提出ET-Agent训练框架，通过自我演进数据飞轮与行为校准训练两个协同视角校准智能体的工具使用行为。具体而言，我们引入自演进数据飞轮机制生成增强数据，用于微调LLM以提升其探索能力。在此基础上，我们构建了双阶段行为校准训练框架，旨在逐步将错误行为模式校准至最优行为。深入的实验验证了该框架在正确性、效率、推理简洁性和工具执行准确性等多维度的优越性。ET-Agent框架为TIR领域研究提供了实践性见解，代码开源地址：https://github.com/asilverlight/ET-Agent",
    "url": "https://huggingface.co/papers/2601.06860",
    "arxiv_url": "https://arxiv.org/abs/2601.06860"
  },
  {
    "title": "Dr. Zero: Self-Evolving Search Agents without Training Data",
    "summary": "As high-quality data becomes increasingly difficult to obtain, data-free self-evolution has emerged as a promising paradigm. This approach allows large language models (LLMs) to autonomously generate and solve complex problems, thereby improving their reasoning capabilities. However, multi-turn search agents struggle in data-free self-evolution due to the limited question diversity and the substantial compute required for multi-step reasoning and tool using. In this work, we introduce Dr. Zero, a framework enabling search agents to effectively self-evolve without any training data. In particular, we design a self-evolution feedback loop where a proposer generates diverse questions to train a solver initialized from the same base model. As the solver evolves, it incentivizes the proposer to produce increasingly difficult yet solvable tasks, thus establishing an automated curriculum to refine both agents. To enhance training efficiency, we also introduce hop-grouped relative policy optimization (HRPO). This method clusters structurally similar questions to construct group-level baselines, effectively minimizing the sampling overhead in evaluating each query's individual difficulty and solvability. Consequently, HRPO significantly reduces the compute requirements for solver training without compromising performance or stability. Extensive experiment results demonstrate that the data-free Dr. Zero matches or surpasses fully supervised search agents, proving that complex reasoning and search capabilities can emerge solely through self-evolution.",
    "translation": "标题：Dr. Zero：无需训练数据的自进化搜索智能体\n\n摘要：随着高质量数据日益难以获取，无数据自进化已成为一种前景广阔的研究范式。该方法使大语言模型能够自主生成并解决复杂问题，从而提升其推理能力。然而，多轮搜索智能体在无数据自进化中面临挑战，主要受限于问题多样性不足以及多步推理与工具调用所需的大量计算资源。本研究提出Dr. Zero框架，使搜索智能体能够在完全无需训练数据的情况下实现高效自进化。具体而言，我们设计了一种自进化反馈循环：提案器生成多样化问题用于训练基于同一基础模型初始化的求解器，而求解器的进化反过来激励提案器产生难度递增且可解决的任务，从而形成自动化课程以同步优化两个智能体。为提升训练效率，我们进一步提出跳数分组相对策略优化方法。该方法通过聚类结构相似的问题构建组级基线，有效减少评估单个查询难度与可解性时的采样开销，进而不牺牲性能或稳定性的前提下显著降低求解器训练的计算需求。大量实验结果表明，无数据训练的Dr. Zero框架在性能上达到甚至超越全监督搜索智能体，证明复杂推理与搜索能力可仅通过自进化机制实现。",
    "url": "https://huggingface.co/papers/2601.07055",
    "arxiv_url": "https://arxiv.org/abs/2601.07055"
  },
  {
    "title": "Forest Before Trees: Latent Superposition for Efficient Visual Reasoning",
    "summary": "While Chain-of-Thought empowers Large Vision-Language Models with multi-step reasoning, explicit textual rationales suffer from an information bandwidth bottleneck, where continuous visual details are discarded during discrete tokenization. Recent latent reasoning methods attempt to address this challenge, but often fall prey to premature semantic collapse due to rigid autoregressive objectives. In this paper, we propose Laser, a novel paradigm that reformulates visual deduction via Dynamic Windowed Alignment Learning (DWAL). Instead of forcing a point-wise prediction, Laser aligns the latent state with a dynamic validity window of future semantics. This mechanism enforces a \"Forest-before-Trees\" cognitive hierarchy, enabling the model to maintain a probabilistic superposition of global features before narrowing down to local details. Crucially, Laser maintains interpretability via decodable trajectories while stabilizing unconstrained learning via Self-Refined Superposition. Extensive experiments on 6 benchmarks demonstrate that Laser achieves state-of-the-art performance among latent reasoning methods, surpassing the strong baseline Monet by 5.03% on average. Notably, it achieves these gains with extreme efficiency, reducing inference tokens by more than 97%, while demonstrating robust generalization to out-of-distribution domains.",
    "translation": "标题：先见森林后见树：基于潜在叠加的高效视觉推理方法\n\n摘要：尽管思维链技术赋予大型视觉语言模型多步推理能力，但显式文本推理过程存在信息带宽瓶颈，连续视觉细节在离散化标记过程中易被丢失。近期潜在推理方法试图解决这一挑战，却常因僵化的自回归目标而陷入过早的语义坍缩。本文提出Laser新范式，通过动态窗口对齐学习重构视觉推理过程。该方法摒弃逐点预测的强制约束，使潜在状态与动态变化的未来语义有效窗口对齐。这种机制构建了\"先森林后树木\"的认知层级，使模型能够在聚焦局部细节前保持全局特征的概率叠加态。关键创新在于，Laser通过可解码轨迹保持可解释性，同时借助自优化叠加机制稳定无约束学习过程。在6个基准测试上的大量实验表明，Laser在潜在推理方法中达到最先进性能，较强势基线Monet平均提升5.03%。值得注意的是，该方法在实现性能增益的同时具备极高效率，推理标记量减少97%以上，并展现出对分布外领域强大的泛化能力。",
    "url": "https://huggingface.co/papers/2601.06803",
    "arxiv_url": "https://arxiv.org/abs/2601.06803"
  },
  {
    "title": "TourPlanner: A Competitive Consensus Framework with Constraint-Gated Reinforcement Learning for Travel Planning",
    "summary": "Travel planning is a sophisticated decision-making process that requires synthesizing multifaceted information to construct itineraries. However, existing travel planning approaches face several challenges: (1) Pruning candidate points of interest (POIs) while maintaining a high recall rate; (2) A single reasoning path restricts the exploration capability within the feasible solution space for travel planning; (3) Simultaneously optimizing hard constraints and soft constraints remains a significant difficulty. To address these challenges, we propose TourPlanner, a comprehensive framework featuring multi-path reasoning and constraint-gated reinforcement learning. Specifically, we first introduce a Personalized Recall and Spatial Optimization (PReSO) workflow to construct spatially-aware candidate POIs' set. Subsequently, we propose Competitive consensus Chain-of-Thought (CCoT), a multi-path reasoning paradigm that improves the ability of exploring the feasible solution space. To further refine the plan, we integrate a sigmoid-based gating mechanism into the reinforcement learning stage, which dynamically prioritizes soft-constraint satisfaction only after hard constraints are met. Experimental results on travel planning benchmarks demonstrate that TourPlanner achieves state-of-the-art performance, significantly surpassing existing methods in both feasibility and user-preference alignment.",
    "translation": "标题：TourPlanner：一种融合约束门控强化学习的竞争性共识框架用于旅行规划\n\n摘要：旅行规划是一个复杂的决策过程，需要综合多维度信息以构建行程方案。然而，现有旅行规划方法面临若干挑战：（1）在保持高召回率的同时筛选候选兴趣点；（2）单一推理路径限制了旅行规划在可行解空间内的探索能力；（3）同时优化硬约束与软约束仍是重大难题。为应对这些挑战，我们提出TourPlanner——一个融合多路径推理与约束门控强化学习的综合框架。具体而言，我们首先引入个性化召回与空间优化流程，构建具有空间感知的候选兴趣点集合。随后，我们提出竞争性共识思维链这一多路径推理范式，以增强对可行解空间的探索能力。为进一步优化规划方案，我们在强化学习阶段集成基于Sigmoid函数的门控机制，该机制仅在硬约束满足后动态优先处理软约束的达成。在旅行规划基准测试上的实验结果表明，TourPlanner实现了最先进的性能，在方案可行性与用户偏好契合度方面均显著超越现有方法。",
    "url": "https://huggingface.co/papers/2601.04698",
    "arxiv_url": "https://arxiv.org/abs/2601.04698"
  },
  {
    "title": "OpenTinker: Separating Concerns in Agentic Reinforcement Learning",
    "summary": "We introduce OpenTinker, an infrastructure for reinforcement learning (RL) of large language model (LLM) agents built around a separation of concerns across algorithm design, execution, and agent-environment interaction. Rather than relying on monolithic, end-to-end RL pipelines, OpenTinker decomposes agentic learning systems into lightweight, composable components with clearly defined abstraction boundaries. Users specify agents, environments, and interaction protocols, while inference and training are delegated to a managed execution runtime. OpenTinker introduces a centralized scheduler for managing training and inference workloads, including LoRA-based and full-parameter RL, supervised fine-tuning, and inference, over shared resources. We further discuss design principles for extending OpenTinker to multi-agent training. Finally, we present a set of RL use cases that demonstrate the effectiveness of the framework in practical agentic learning scenarios.",
    "translation": "标题：OpenTinker：智能体强化学习中的关注点分离\n\n摘要：本文介绍OpenTinker，这是一个围绕算法设计、执行以及智能体-环境交互的关注点分离而构建的大型语言模型（LLM）智能体强化学习（RL）基础设施。OpenTinker摒弃了单一、端到端的强化学习流程，将智能体学习系统分解为具有明确定义抽象边界的轻量级、可组合组件。用户负责定义智能体、环境及交互协议，而推理与训练任务则交由托管执行运行时处理。OpenTinker引入了一个集中式调度器，用于在共享资源上管理训练与推理工作负载，包括基于LoRA和全参数的强化学习、监督微调以及推理任务。我们进一步探讨了将OpenTinker扩展至多智能体训练的设计原则。最后，我们通过一系列强化学习应用案例，展示了该框架在实际智能体学习场景中的有效性。",
    "url": "https://huggingface.co/papers/2601.07376",
    "arxiv_url": "https://arxiv.org/abs/2601.07376"
  },
  {
    "title": "Are LLM Decisions Faithful to Verbal Confidence?",
    "summary": "Large Language Models (LLMs) can produce surprisingly sophisticated estimates of their own uncertainty. However, it remains unclear to what extent this expressed confidence is tied to the reasoning, knowledge, or decision making of the model. To test this, we introduce RiskEval: a framework designed to evaluate whether models adjust their abstention policies in response to varying error penalties. Our evaluation of several frontier models reveals a critical dissociation: models are neither cost-aware when articulating their verbal confidence, nor strategically responsive when deciding whether to engage or abstain under high-penalty conditions. Even when extreme penalties render frequent abstention the mathematically optimal strategy, models almost never abstain, resulting in utility collapse. This indicates that calibrated verbal confidence scores may not be sufficient to create trustworthy and interpretable AI systems, as current models lack the strategic agency to convert uncertainty signals into optimal and risk-sensitive decisions.",
    "translation": "标题：大语言模型的决策是否忠实于其口头置信度？\n\n摘要：大语言模型（LLMs）能够生成令人惊讶的、关于其自身不确定性的复杂估计。然而，这种表达出的置信度在多大程度上与模型的推理、知识或决策过程相关联，目前尚不明确。为探究此问题，我们引入了RiskEval框架：该框架旨在评估模型是否会根据不同的错误惩罚调整其弃权策略。我们对多个前沿模型的评估揭示了一个关键分离现象：模型在表达口头置信度时并不具备成本意识，在高惩罚条件下决定是否参与或弃权时也缺乏策略性响应。即使极端惩罚使得频繁弃权成为数学上的最优策略，模型也几乎从不弃权，从而导致效用崩溃。这表明，经过校准的口头置信度分数可能不足以构建可信且可解释的人工智能系统，因为当前模型缺乏将不确定性信号转化为最优且风险敏感决策的策略性能力。",
    "url": "https://huggingface.co/papers/2601.07767",
    "arxiv_url": "https://arxiv.org/abs/2601.07767"
  },
  {
    "title": "Structured Episodic Event Memory",
    "summary": "Current approaches to memory in Large Language Models (LLMs) predominantly rely on static Retrieval-Augmented Generation (RAG), which often results in scattered retrieval and fails to capture the structural dependencies required for complex reasoning. For autonomous agents, these passive and flat architectures lack the cognitive organization necessary to model the dynamic and associative nature of long-term interaction. To address this, we propose Structured Episodic Event Memory (SEEM), a hierarchical framework that synergizes a graph memory layer for relational facts with a dynamic episodic memory layer for narrative progression. Grounded in cognitive frame theory, SEEM transforms interaction streams into structured Episodic Event Frames (EEFs) anchored by precise provenance pointers. Furthermore, we introduce an agentic associative fusion and Reverse Provenance Expansion (RPE) mechanism to reconstruct coherent narrative contexts from fragmented evidence. Experimental results on the LoCoMo and LongMemEval benchmarks demonstrate that SEEM significantly outperforms baselines, enabling agents to maintain superior narrative coherence and logical consistency.",
    "translation": "标题：结构化情景事件记忆\n\n摘要：当前大型语言模型中的记忆方法主要依赖静态检索增强生成技术，该方法往往导致检索结果分散，难以捕捉复杂推理所需的结构化依赖关系。对于自主智能体而言，这种被动扁平的架构缺乏对长期交互动态关联特性进行建模所需的认知组织能力。为此，我们提出结构化情景事件记忆框架，该分层架构通过图记忆层处理关系性事实，并与动态情景记忆层协同实现叙事演进。基于认知框架理论，本框架将交互流转化为以前溯指针锚定的结构化情景事件框架。此外，我们引入智能体关联融合机制与反向溯源扩展方法，从碎片化证据中重构连贯的叙事语境。在LoCoMo与LongMemEval基准测试中的实验结果表明，该框架显著超越基线模型，使智能体能够保持卓越的叙事连贯性与逻辑一致性。",
    "url": "https://huggingface.co/papers/2601.06411",
    "arxiv_url": "https://arxiv.org/abs/2601.06411"
  },
  {
    "title": "e5-omni: Explicit Cross-modal Alignment for Omni-modal Embeddings",
    "summary": "Modern information systems often involve different types of items, e.g., a text query, an image, a video clip, or an audio segment. This motivates omni-modal embedding models that map heterogeneous modalities into a shared space for direct comparison. However, most recent omni-modal embeddings still rely heavily on implicit alignment inherited from pretrained vision-language model (VLM) backbones. In practice, this causes three common issues: (i) similarity logits have modality-dependent sharpness, so scores are not on a consistent scale; (ii) in-batch negatives become less effective over time because mixed-modality batches create an imbalanced hardness distribution; as a result, many negatives quickly become trivial and contribute little gradient; and (iii) embeddings across modalities show mismatched first- and second-order statistics, which makes rankings less stable. To tackle these problems, we propose e5-omni, a lightweight explicit alignment recipe that adapts off-the-shelf VLMs into robust omni-modal embedding models. e5-omni combines three simple components: (1) modality-aware temperature calibration to align similarity scales, (2) a controllable negative curriculum with debiasing to focus on confusing negatives while reducing the impact of false negatives, and (3) batch whitening with covariance regularization to better match cross-modal geometry in the shared embedding space. Experiments on MMEB-V2 and AudioCaps show consistent gains over strong bi-modal and omni-modal baselines, and the same recipe also transfers well to other VLM backbones. We release our model checkpoint at https://huggingface.co/Haon-Chen/e5-omni-7B.",
    "translation": "标题：e5-omni：面向全模态嵌入的显式跨模态对齐方法\n\n摘要：现代信息系统通常涉及多种类型的项目，例如文本查询、图像、视频片段或音频片段。这推动了全模态嵌入模型的发展，旨在将异构模态映射到共享空间中以进行直接比较。然而，当前大多数全模态嵌入方法仍严重依赖于从预训练视觉-语言模型（VLM）主干中继承的隐式对齐机制。在实践中，这引发了三个常见问题：（1）相似度对数具有模态依赖的锐度，导致评分尺度不一致；（2）混合模态批次造成困难度分布不均衡，使得批内负样本随时间推移效果减弱，许多负样本迅速变得无关紧要，对梯度贡献甚微；（3）跨模态嵌入呈现不匹配的一阶和二阶统计特征，导致排序稳定性下降。为解决这些问题，我们提出e5-omni——一种轻量级的显式对齐方案，可将现成的VLM适配为鲁棒的全模态嵌入模型。e5-omni包含三个简单组件：（1）模态感知温度校准，用于对齐相似度尺度；（2）具有去偏控制的可控负样本课程学习，专注于混淆性负样本同时减少假负样本的影响；（3）协方差正则化的批白化处理，以更好地匹配共享嵌入空间中的跨模态几何结构。在MMEB-V2和AudioCaps数据集上的实验表明，该方法相较于强双模态与全模态基线模型均取得稳定提升，且该方案能良好迁移至其他VLM主干网络。模型检查点已发布于https://huggingface.co/Haon-Chen/e5-omni-7B。",
    "url": "https://huggingface.co/papers/2601.03666",
    "arxiv_url": "https://arxiv.org/abs/2601.03666"
  },
  {
    "title": "\"TODO: Fix the Mess Gemini Created\": Towards Understanding GenAI-Induced Self-Admitted Technical Debt",
    "summary": "As large language models (LLMs) such as ChatGPT, Copilot, Claude, and Gemini become integrated into software development workflows, developers increasingly leave traces of AI involvement in their code comments. Among these, some comments explicitly acknowledge both the use of generative AI and the presence of technical shortcomings. Analyzing 6,540 LLM-referencing code comments from public Python and JavaScript-based GitHub repositories (November 2022-July 2025), we identified 81 that also self-admit technical debt(SATD). Developers most often describe postponed testing, incomplete adaptation, and limited understanding of AI-generated code, suggesting that AI assistance affects both when and why technical debt emerges. We term GenAI-Induced Self-admitted Technical debt (GIST) as a proposed conceptual lens to describe recurring cases where developers incorporate AI-generated code while explicitly expressing uncertainty about its behavior or correctness.",
    "translation": "标题：“TODO: 修复Gemini造成的混乱”：理解生成式人工智能引发的自承技术债务\n\n摘要：随着ChatGPT、Copilot、Claude和Gemini等大型语言模型（LLMs）逐渐融入软件开发工作流，开发者在代码注释中越来越多地留下人工智能参与的痕迹。其中，部分注释明确承认了生成式人工智能的使用以及技术缺陷的存在。通过分析来自公开Python和JavaScript的GitHub仓库（2022年11月至2025年7月）的6,540条涉及LLM的代码注释，我们识别出81条同时自承技术债务（SATD）的案例。开发者最常描述推迟测试、不完整适配以及对AI生成代码的理解有限，这表明人工智能辅助既影响了技术债务出现的时间，也影响了其产生的原因。我们提出“生成式人工智能引发的自承技术债务”（GIST）这一概念框架，用以描述开发者在使用AI生成代码时，同时明确表达对其行为或正确性存在不确定性的重复性案例。",
    "url": "https://huggingface.co/papers/2601.07786",
    "arxiv_url": "https://arxiv.org/abs/2601.07786"
  },
  {
    "title": "ShowUI-Aloha: Human-Taught GUI Agent",
    "summary": "Graphical User Interfaces (GUIs) are central to human-computer interaction, yet automating complex GUI tasks remains a major challenge for autonomous agents, largely due to a lack of scalable, high-quality training data. While recordings of human demonstrations offer a rich data source, they are typically long, unstructured, and lack annotations, making them difficult for agents to learn from.To address this, we introduce ShowUI-Aloha, a comprehensive pipeline that transforms unstructured, in-the-wild human screen recordings from desktop environments into structured, actionable tasks. Our framework includes four key components: A recorder that captures screen video along with precise user interactions like mouse clicks, keystrokes, and scrolls. A learner that semantically interprets these raw interactions and the surrounding visual context, translating them into descriptive natural language captions. A planner that reads the parsed demonstrations, maintains task states, and dynamically formulates the next high-level action plan based on contextual reasoning. An executor that faithfully carries out these action plans at the OS level, performing precise clicks, drags, text inputs, and window operations with safety checks and real-time feedback. Together, these components provide a scalable solution for collecting and parsing real-world human data, demonstrating a viable path toward building general-purpose GUI agents that can learn effectively from simply observing humans.",
    "translation": "标题：ShowUI-Aloha：人类示范驱动的图形用户界面智能体\n\n摘要：图形用户界面（GUI）是人机交互的核心，然而自动化复杂的GUI任务仍是自主智能体面临的主要挑战，这很大程度上源于缺乏可扩展的高质量训练数据。虽然人类操作录屏提供了丰富的数据源，但这些记录通常冗长、非结构化且缺乏标注，使得智能体难以从中有效学习。为此，我们提出了ShowUI-Aloha——一个完整的处理流程，能够将桌面环境中非结构化的真实人类屏幕录像转化为结构化、可执行的任务序列。该框架包含四个核心组件：记录器负责捕获屏幕视频及精确的用户交互（如鼠标点击、键盘输入和滚动操作）；学习器通过语义理解原始交互行为与视觉上下文，将其转化为描述性自然语言标注；规划器通过解析示范记录、维护任务状态，并基于上下文推理动态生成下一步高层动作计划；执行器则在操作系统层面忠实执行这些动作计划，通过安全检查与实时反馈机制实现精确点击、拖拽、文本输入及窗口操作。这些组件共同构成了一个可扩展的真实人类数据采集与解析方案，为构建能够通过观察人类行为进行高效学习的通用GUI智能体提供了可行路径。",
    "url": "https://huggingface.co/papers/2601.07181",
    "arxiv_url": "https://arxiv.org/abs/2601.07181"
  },
  {
    "title": "Codified Foreshadowing-Payoff Text Generation",
    "summary": "Foreshadowing and payoff are ubiquitous narrative devices through which authors introduce commitments early in a story and resolve them through concrete, observable outcomes. However, despite advances in story generation, large language models (LLMs) frequently fail to bridge these long-range narrative dependencies, often leaving \"Chekhov's guns\" unfired even when the necessary context is present. Existing evaluations largely overlook this structural failure, focusing on surface-level coherence rather than the logical fulfillment of narrative setups. In this paper, we introduce Codified Foreshadowing-Payoff Generation (CFPG), a novel framework that reframes narrative quality through the lens of payoff realization. Recognizing that LLMs struggle to intuitively grasp the \"triggering mechanism\" of a foreshadowed event, CFPG transforms narrative continuity into a set of executable causal predicates. By mining and encoding Foreshadow-Trigger-Payoff triples from the BookSum corpus, we provide structured supervision that ensures foreshadowed commitments are not only mentioned but also temporally and logically fulfilled. Experiments demonstrate that CFPG significantly outperforms standard prompting baselines in payoff accuracy and narrative alignment. Our findings suggest that explicitly codifying narrative mechanics is essential for moving LLMs from surface-level fluency to genuine narrative competence.",
    "translation": "标题：编码化伏笔-照应文本生成\n\n摘要：伏笔与照应是普遍存在的叙事手法，作者通过其在故事早期引入承诺，并通过具体可观察的结果予以解决。然而，尽管故事生成技术取得了进展，大语言模型在连接此类长程叙事依赖关系时仍常显乏力，即便在必要语境存在的情况下，也往往让“契科夫的枪”未能击发。现有评估方法大多忽视了这种结构性缺陷，侧重于表层连贯性而非叙事铺垫的逻辑实现。本文提出编码化伏笔-照应生成框架，该创新框架通过照应实现的视角重构叙事质量评估体系。针对大语言模型难以直观把握伏笔事件“触发机制”的现状，CFPG将叙事连续性转化为一系列可执行的因果谓词。通过从BookSum语料库中挖掘并编码“伏笔-触发-照应”三元组，我们提供了结构化监督机制，确保伏笔承诺不仅被提及，更能在时间与逻辑层面得到兑现。实验表明，CFPG在照应准确度与叙事一致性方面显著优于标准提示基线方法。我们的研究结果表明，对叙事机制进行显式编码对于推动大语言模型从表层流畅性迈向真正的叙事能力至关重要。",
    "url": "https://huggingface.co/papers/2601.07033",
    "arxiv_url": "https://arxiv.org/abs/2601.07033"
  },
  {
    "title": "Sci-Reasoning: A Dataset Decoding AI Innovation Patterns",
    "summary": "While AI innovation accelerates rapidly, the intellectual process behind breakthroughs -- how researchers identify gaps, synthesize prior work, and generate insights -- remains poorly understood. The lack of structured data on scientific reasoning hinders systematic analysis and development of AI research agents. We introduce Sci-Reasoning, the first dataset capturing the intellectual synthesis behind high-quality AI research. Using community-validated quality signals and an LLM-accelerated, human-verified pipeline, we trace Oral and Spotlight papers across NeurIPS, ICML, and ICLR (2023-2025) to its key predecessors, articulating specific reasoning links in a structured format. Our analysis identifies 15 distinct thinking patterns, with three dominant strategies accounting for 52.7%: Gap-Driven Reframing (24.2%), Cross-Domain Synthesis (18.0%), and Representation Shift (10.5%). The most powerful innovation recipes combine multiple patterns: Gap-Driven Reframing + Representation Shift, Cross-Domain Synthesis + Representation Shift, and Gap-Driven Reframing + Cross-Domain Synthesis. This dataset enables quantitative studies of scientific progress and provides structured reasoning trajectories for training the next generation AI research agents.",
    "translation": "标题：Sci-Reasoning：解码人工智能创新模式的数据集\n\n摘要：尽管人工智能创新正加速发展，但突破背后的智力过程——研究者如何识别研究空白、整合先前工作并产生洞见——仍鲜为人知。科学推理结构化数据的缺乏，阻碍了对人工智能研究智能体的系统性分析与开发。本文介绍Sci-Reasoning，这是首个捕捉高质量人工智能研究背后智力综合过程的数据集。通过采用社区验证的质量信号以及基于大语言模型加速、人工验证的流程，我们追踪了NeurIPS、ICML和ICLR（2023-2025年）中Oral与Spotlight论文的关键前驱工作，并以结构化形式阐明了具体的推理关联。我们的分析识别出15种不同的思维模式，其中三种主导策略占比达52.7%：空白驱动重构（24.2%）、跨领域综合（18.0%）与表征转换（10.5%）。最具影响力的创新路径往往融合多种模式：空白驱动重构+表征转换、跨领域综合+表征转换，以及空白驱动重构+跨领域综合。本数据集支持对科学进展的量化研究，并为训练下一代人工智能研究智能体提供了结构化推理轨迹。",
    "url": "https://huggingface.co/papers/2601.04577",
    "arxiv_url": "https://arxiv.org/abs/2601.04577"
  },
  {
    "title": "How Do Large Language Models Learn Concepts During Continual Pre-Training?",
    "summary": "Human beings primarily understand the world through concepts (e.g., dog), abstract mental representations that structure perception, reasoning, and learning. However, how large language models (LLMs) acquire, retain, and forget such concepts during continual pretraining remains poorly understood. In this work, we study how individual concepts are acquired and forgotten, as well as how multiple concepts interact through interference and synergy. We link these behavioral dynamics to LLMs' internal Concept Circuits, computational subgraphs associated with specific concepts, and incorporate Graph Metrics to characterize circuit structure. Our analysis reveals: (1) LLMs concept circuits provide a non-trivial, statistically significant signal of concept learning and forgetting; (2) Concept circuits exhibit a stage-wise temporal pattern during continual pretraining, with an early increase followed by gradual decrease and stabilization; (3) concepts with larger learning gains tend to exhibit greater forgetting under subsequent training; (4) semantically similar concepts induce stronger interference than weakly related ones; (5) conceptual knowledge differs in their transferability, with some significantly facilitating the learning of others. Together, our findings offer a circuit-level view of concept learning dynamics and inform the design of more interpretable and robust concept-aware training strategies for LLMs.",
    "translation": "标题：大语言模型在持续预训练中如何学习概念？\n\n摘要：人类主要通过概念（如“狗”）来理解世界，这些抽象的心理表征构建了感知、推理和学习的基础。然而，大语言模型在持续预训练过程中如何获取、保留和遗忘此类概念，目前仍缺乏深入理解。本研究探讨了单个概念的获取与遗忘机制，以及多个概念之间如何通过干扰和协同产生交互。我们将这些行为动态与模型内部的“概念回路”（即与特定概念相关的计算子图）联系起来，并引入图度量指标来刻画回路结构。分析结果表明：（1）大语言模型的概念回路能够提供具有统计显著性的概念学习与遗忘信号；（2）在持续预训练过程中，概念回路呈现阶段性时序模式，即早期增强、随后逐渐减弱并趋于稳定；（3）学习增益较大的概念在后续训练中往往表现出更强的遗忘现象；（4）语义相似的概念比弱相关概念产生更显著的干扰效应；（5）不同概念知识的可迁移性存在差异，部分概念能显著促进其他概念的学习。综上，本研究从回路层面揭示了概念学习的动态特征，为设计更具可解释性与鲁棒性的概念感知训练策略提供了理论依据。",
    "url": "https://huggingface.co/papers/2601.03570",
    "arxiv_url": "https://arxiv.org/abs/2601.03570"
  },
  {
    "title": "On the Non-decoupling of Supervised Fine-tuning and Reinforcement Learning in Post-training",
    "summary": "Post-training of large language models routinely interleaves supervised fine-tuning (SFT) with reinforcement learning (RL). These two methods have different objectives: SFT minimizes the cross-entropy loss between model outputs and expert responses, while RL maximizes reward signals derived from human preferences or rule-based verifiers. Modern reasoning models have widely adopted the practice of alternating SFT and RL training. However, there is no theoretical account of whether they can be decoupled. We prove that decoupling is impossible in either order: (1) SFT-then-RL coupling: RL increases SFT loss under SFT optimality and (2) RL-then-SFT coupling: SFT lowers the reward achieved by RL. Experiments on Qwen3-0.6B confirm the predicted degradation, verifying that SFT and RL cannot be separated without loss of prior performance in the post-training",
    "translation": "标题：论后训练中监督微调与强化学习的不可解耦性\n\n摘要：大语言模型的后训练通常交替进行监督微调（SFT）与强化学习（RL）。这两种方法具有不同目标：SFT旨在最小化模型输出与专家响应之间的交叉熵损失，而RL则致力于最大化来自人类偏好或基于规则的验证器所衍生的奖励信号。现代推理模型已广泛采用交替进行SFT与RL训练的做法。然而，二者能否解耦尚未有理论阐释。我们证明无论以何种顺序均无法实现解耦：（1）先SFT后RL的耦合：在SFT最优性条件下，RL会增加SFT损失；（2）先RL后SFT的耦合：SFT会降低RL已获得的奖励。基于Qwen3-0.6B的实验证实了预测的性能退化现象，验证了在后训练中若分离SFT与RL，则无法保持先前已达到的性能水平。",
    "url": "https://huggingface.co/papers/2601.07389",
    "arxiv_url": "https://arxiv.org/abs/2601.07389"
  },
  {
    "title": "Can Textual Reasoning Improve the Performance of MLLMs on Fine-grained Visual Classification?",
    "summary": "Multi-modal large language models (MLLMs) exhibit strong general-purpose capabilities, yet still struggle on Fine-Grained Visual Classification (FGVC), a core perception task that requires subtle visual discrimination and is crucial for many real-world applications. A widely adopted strategy for boosting performance on challenging tasks such as math and coding is Chain-of-Thought (CoT) reasoning. However, several prior works have reported that CoT can actually harm performance on visual perception tasks. These studies, though, examine the issue from relatively narrow angles and leave open why CoT degrades perception-heavy performance. We systematically re-examine the role of CoT in FGVC through the lenses of zero-shot evaluation and multiple training paradigms. Across these settings, we uncover a central paradox: the degradation induced by CoT is largely driven by the reasoning length, in which longer textual reasoning consistently lowers classification accuracy. We term this phenomenon the ``Cost of Thinking''. Building on this finding, we make two key contributions: (1) \\alg, a simple and general plug-and-play normalization method for multi-reward optimization that balances heterogeneous reward signals, and (2) ReFine-RFT, a framework that combines ensemble rewards with \\alg to constrain reasoning length while providing dense accuracy-oriented feedback. Extensive experiments demonstrate the effectiveness of our findings and the proposed ReFine-RFT, achieving state-of-the-art performance across FGVC benchmarks. Code and models are available at https://github.com/jiezhu23/ReFine-RFT{Project Link}.",
    "translation": "标题：文本推理能否提升多模态大语言模型在细粒度视觉分类中的性能？\n\n摘要：多模态大语言模型（MLLMs）展现出强大的通用能力，但在细粒度视觉分类（FGVC）这一核心感知任务上仍面临挑战。FGVC需要细微的视觉辨别能力，对众多现实应用至关重要。在数学和代码等复杂任务中，提升性能的常用策略是思维链（CoT）推理。然而，先前多项研究表明，CoT实际上可能损害视觉感知任务的性能。这些研究虽从相对局限的视角探讨了该问题，但尚未揭示CoT降低感知密集型任务性能的根本原因。本文通过零样本评估与多种训练范式的视角，系统性地重新审视了CoT在FGVC中的作用。在不同实验设置中，我们发现一个核心悖论：CoT导致的性能下降主要受推理长度驱动，即更长的文本推理会持续降低分类准确率。我们将此现象称为“思维成本”。基于这一发现，我们做出两项关键贡献：（1）提出\\alg方法——一种简单通用的即插即用式多奖励优化归一化方法，可平衡异构奖励信号；（2）提出ReFine-RFT框架，该框架结合集成奖励与\\alg方法，在约束推理长度的同时提供密集的以准确率为导向的反馈。大量实验验证了我们发现的普适性及所提ReFine-RFT框架的有效性，该框架在多项FGVC基准测试中达到了最先进的性能。代码与模型已开源：https://github.com/jiezhu23/ReFine-RFT{项目链接}。",
    "url": "https://huggingface.co/papers/2601.06993",
    "arxiv_url": "https://arxiv.org/abs/2601.06993"
  },
  {
    "title": "RealMem: Benchmarking LLMs in Real-World Memory-Driven Interaction",
    "summary": "As Large Language Models (LLMs) evolve from static dialogue interfaces to autonomous general agents, effective memory is paramount to ensuring long-term consistency. However, existing benchmarks primarily focus on casual conversation or task-oriented dialogue, failing to capture **\"long-term project-oriented\"** interactions where agents must track evolving goals.\n  To bridge this gap, we introduce **RealMem**, the first benchmark grounded in realistic project scenarios. RealMem comprises over 2,000 cross-session dialogues across eleven scenarios, utilizing natural user queries for evaluation.\n  We propose a synthesis pipeline that integrates Project Foundation Construction, Multi-Agent Dialogue Generation, and Memory and Schedule Management to simulate the dynamic evolution of memory. Experiments reveal that current memory systems face significant challenges in managing the long-term project states and dynamic context dependencies inherent in real-world projects.\n  Our code and datasets are available at [https://github.com/AvatarMemory/RealMemBench](https://github.com/AvatarMemory/RealMemBench).",
    "translation": "标题：RealMem：面向现实世界记忆驱动交互的大语言模型评测基准\n\n摘要：随着大语言模型从静态对话接口演变为自主通用智能体，有效的记忆机制对于保障长期行为一致性至关重要。然而，现有评测基准主要集中于日常对话或任务导向型对话，未能涵盖智能体必须追踪动态目标的**\"长期项目导向型\"**交互场景。为填补这一空白，我们提出了首个基于真实项目场景构建的评测基准**RealMem**。该基准涵盖十一个场景下的两千余组跨会话对话，采用自然用户查询进行性能评估。我们设计了一套融合项目基础构建、多智能体对话生成以及记忆与进度管理的综合流程，以模拟记忆的动态演化过程。实验表明，现有记忆系统在管理现实项目固有的长期状态与动态上下文依赖方面面临显著挑战。相关代码与数据集已发布于[https://github.com/AvatarMemory/RealMemBench](https://github.com/AvatarMemory/RealMemBench)。",
    "url": "https://huggingface.co/papers/2601.06966",
    "arxiv_url": "https://arxiv.org/abs/2601.06966"
  },
  {
    "title": "SketchJudge: A Diagnostic Benchmark for Grading Hand-drawn Diagrams with Multimodal Large Language Models",
    "summary": "While Multimodal Large Language Models (MLLMs) have achieved remarkable progress in visual understanding, they often struggle when faced with the unstructured and ambiguous nature of human-generated sketches. This limitation is particularly pronounced in the underexplored task of visual grading, where models should not only solve a problem but also diagnose errors in hand-drawn diagrams. Such diagnostic capabilities depend on complex structural, semantic, and metacognitive reasoning. To bridge this gap, we introduce SketchJudge, a novel benchmark tailored for evaluating MLLMs as graders of hand-drawn STEM diagrams. SketchJudge encompasses 1,015 hand-drawn student responses across four domains: geometry, physics, charts, and flowcharts, featuring diverse stylistic variations and distinct error types. Evaluations on SketchJudge demonstrate that even advanced MLLMs lag significantly behind humans, validating the benchmark's effectiveness in exposing the fragility of current vision-language alignment in symbolic and noisy contexts. All data, code, and evaluation scripts are publicly available at https://github.com/yuhangsu82/SketchJudge.",
    "translation": "标题：SketchJudge：基于多模态大语言模型的手绘图表评分诊断基准\n\n摘要：尽管多模态大语言模型（MLLMs）在视觉理解方面取得了显著进展，但在处理人类手绘草图的无结构性和模糊性时仍面临挑战。这一局限在视觉评分这一尚未充分探索的任务中尤为突出——模型不仅需要解决问题，还需诊断手绘图表中的错误。此类诊断能力依赖于复杂的结构、语义及元认知推理。为弥补这一不足，我们提出了SketchJudge，这是一个专为评估MLLMs作为手绘STEM（科学、技术、工程、数学）图表评分者而设计的新型基准。SketchJudge涵盖几何、物理、图表和流程图四个领域的1,015份手绘学生作答样本，包含多样化的风格差异和明确的错误类型。基于SketchJudge的评估表明，即使先进的MLLMs仍显著落后于人类水平，验证了该基准在揭示符号与噪声语境下当前视觉-语言对齐机制脆弱性方面的有效性。所有数据、代码及评估脚本均已公开，访问地址为：https://github.com/yuhangsu82/SketchJudge。",
    "url": "https://huggingface.co/papers/2601.06944",
    "arxiv_url": "https://arxiv.org/abs/2601.06944"
  },
  {
    "title": "Artificial Entanglement in the Fine-Tuning of Large Language Models",
    "summary": "Large language models (LLMs) can be adapted to new tasks using parameter-efficient fine-tuning (PEFT) methods that modify only a small number of trainable parameters, often through low-rank updates. In this work, we adopt a quantum-information-inspired perspective to understand their effectiveness. From this perspective, low-rank parameterizations naturally correspond to low-dimensional Matrix Product States (MPS) representations, which enable entanglement-based characterizations of parameter structure. Thereby, we term and measure \"Artificial Entanglement\", defined as the entanglement entropy of the parameters in artificial neural networks (in particular the LLMs). We first study the representative low-rank adaptation (LoRA) PEFT method, alongside full fine-tuning (FFT), using LLaMA models at the 1B and 8B scales trained on the Tulu3 and OpenThoughts3 datasets, and uncover: (i) Internal artificial entanglement in the updates of query and value projection matrices in LoRA follows a volume law with a central suppression (termed as the \"Entanglement Valley\"), which is sensitive to hyper-parameters and is distinct from that in FFT; (ii) External artificial entanglement in attention matrices, corresponding to token-token correlations in representation space, follows an area law with logarithmic corrections and remains robust to LoRA hyper-parameters and training steps. Drawing a parallel to the No-Hair Theorem in black hole physics, we propose that although LoRA and FFT induce distinct internal entanglement signatures, such differences do not manifest in the attention outputs, suggesting a \"no-hair\" property that results in the effectiveness of low rank updates. We further provide theoretical support based on random matrix theory, and extend our analysis to an MPS Adaptation PEFT method, which exhibits qualitatively similar behaviors.",
    "translation": "标题：大语言模型微调中的人工纠缠现象研究\n\n摘要：大语言模型（LLMs）可通过仅修改少量可训练参数的参数高效微调（PEFT）方法适配新任务，其中低秩更新是常用策略。本研究采用量子信息视角解析其有效性机制：低秩参数化天然对应低维矩阵乘积态（MPS）表示，从而可通过纠缠熵量化参数结构特征。据此我们提出并度量“人工纠缠”——即人工神经网络（特指大语言模型）参数体系的纠缠熵。我们以1B和8B规模的LLaMA模型在Tulu3与OpenThoughts3数据集上的训练为实验基础，对比研究了典型低秩适应（LoRA）PEFT方法与全参数微调（FFT），发现：（1）LoRA中查询与值投影矩阵更新呈现具有中心抑制特征的体积律内部人工纠缠（称为“纠缠谷”），其对超参数敏感且与FFT模式显著不同；（2）注意力矩阵中表征空间内词元关联的外部人工纠缠遵循带对数修正的面积律，对LoRA超参数及训练步数保持稳健。借鉴黑洞物理中的无毛定理，我们提出：虽然LoRA与FFT产生相异的内部纠缠特征，但此类差异未在注意力输出中显现，这种“无毛”特性可能是低秩更新有效的内在原因。我们进一步基于随机矩阵理论提供理论支撑，并将分析拓展至MPS适应PEFT方法，发现其具有定性相似的行为特征。",
    "url": "https://huggingface.co/papers/2601.06788",
    "arxiv_url": "https://arxiv.org/abs/2601.06788"
  },
  {
    "title": "FinForge: Semi-Synthetic Financial Benchmark Generation",
    "summary": "Evaluating Language Models (LMs) in specialized, high-stakes domains such as finance remains a significant challenge due to the scarcity of open, high-quality, and domain-specific datasets. Existing general-purpose benchmarks provide broad coverage but lack the depth and domain fidelity needed to assess LMs' capabilities for real-world financial reasoning, which requires both conceptual understanding and quantitative rigor. To address this gap, we introduce FinForge, a scalable, semi-synthetic pipeline for constructing finance-specific evaluation benchmarks through a hybrid of expert-guided data curation and controlled LM-based synthesis. FinForge combines manual and programmatic corpus construction from authoritative financial sources with structured question generation and validation using Gemini 2.5 Flash. To demonstrate the pipeline's efficacy, we produce FinForge-5k, a snapshot benchmark comprising over 5,000 human-validated question-answer pairs across 11 finance subdomains, derived from a curated corpus of 100,000 verified documents totaling 143M tokens. Evaluation of state-of-the-art open-source and closed-source models on FinForge-5k reveals significant differences in financial reasoning, with leading models achieving accuracy levels near 80%. These findings underscore the framework's utility for diagnosing current model limitations and guiding future improvements in financial domain competence. All code and data are available at https://github.com/gtfintechlab/FinForge.",
    "translation": "标题：FinForge：半合成金融基准生成框架\n\n摘要：在金融等专业且高风险的领域中评估语言模型仍面临重大挑战，主要原因是缺乏公开、高质量且领域特定的数据集。现有的通用基准虽覆盖广泛，但缺乏评估语言模型在实际金融推理能力（既需概念理解又需定量严谨性）所需的深度与领域保真度。为填补这一空白，我们提出了FinForge——一个可扩展的半合成流程，通过专家指导的数据策展与基于语言模型的可控合成相结合，构建金融专用评估基准。FinForge整合了来自权威金融源的手动与程序化语料构建，并利用Gemini 2.5 Flash进行结构化问题生成与验证。为验证该流程的有效性，我们发布了FinForge-5k基准快照，包含超过5,000个人工验证的问答对，涵盖11个金融子领域；其源自从10万份已验证文档（总计1.43亿词元）中精选的语料库。基于FinForge-5k对前沿开源与闭源模型的评估显示，模型在金融推理能力上存在显著差异，领先模型的准确率接近80%。这些发现凸显了该框架在诊断当前模型局限、指导未来金融领域能力改进方面的实用价值。所有代码与数据已公开于https://github.com/gtfintechlab/FinForge。",
    "url": "https://huggingface.co/papers/2601.06747",
    "arxiv_url": "https://arxiv.org/abs/2601.06747"
  },
  {
    "title": "Gecko: An Efficient Neural Architecture Inherently Processing Sequences with Arbitrary Lengths",
    "summary": "Designing a unified neural network to efficiently and inherently process sequential data with arbitrary lengths is a central and challenging problem in sequence modeling. The design choices in Transformer, including quadratic complexity and weak length extrapolation, have limited their ability to scale to long sequences. In this work, we propose Gecko, a neural architecture that inherits the design of Mega and Megalodon (exponential moving average with gated attention), and further introduces multiple technical components to improve its capability to capture long range dependencies, including timestep decay normalization, sliding chunk attention mechanism, and adaptive working memory. In a controlled pretraining comparison with Llama2 and Megalodon in the scale of 7 billion parameters and 2 trillion training tokens, Gecko achieves better efficiency and long-context scalability. Gecko reaches a training loss of 1.68, significantly outperforming Llama2-7B (1.75) and Megalodon-7B (1.70), and landing close to Llama2-13B (1.67). Notably, without relying on any context-extension techniques, Gecko exhibits inherent long-context processing and retrieval capabilities, stably handling sequences of up to 4 million tokens and retrieving information from contexts up to 4times longer than its attention window. Code: https://github.com/XuezheMax/gecko-llm",
    "translation": "标题：Gecko：一种高效处理任意长度序列的固有神经架构\n\n摘要：设计一种能够高效且固有地处理任意长度序列数据的统一神经网络，是序列建模领域一个核心且具有挑战性的问题。Transformer架构中的设计选择，包括二次复杂度与较弱的长度外推能力，限制了其向长序列扩展的能力。本研究提出Gecko神经架构，该架构继承了Mega和Megalodon（采用门控注意力的指数移动平均）的设计思路，并进一步引入多项技术组件以增强其长程依赖捕获能力，包括时间步衰减归一化、滑动分块注意力机制以及自适应工作记忆。在70亿参数规模、2万亿训练标记量的控制性预训练实验中，与Llama2和Megalodon进行对比，Gecko展现出更优的效率和长上下文扩展性。Gecko取得了1.68的训练损失，显著优于Llama2-7B（1.75）和Megalodon-7B（1.70），并接近Llama2-13B（1.67）的水平。值得注意的是，在不依赖任何上下文扩展技术的情况下，Gecko展现出固有的长上下文处理与检索能力，能够稳定处理长达400万标记的序列，并从超出其注意力窗口4倍长度的上下文中检索信息。代码地址：https://github.com/XuezheMax/gecko-llm",
    "url": "https://huggingface.co/papers/2601.06463",
    "arxiv_url": "https://arxiv.org/abs/2601.06463"
  },
  {
    "title": "Does Inference Scaling Improve Reasoning Faithfulness? A Multi-Model Analysis of Self-Consistency Tradeoffs",
    "summary": "Self-consistency has emerged as a popular technique for improving large language model accuracy on reasoning tasks. The approach is straightforward: generate multiple reasoning paths and select the most common answer through majority voting. While this reliably boosts accuracy, it remains unclear whether these gains reflect genuine improvements in reasoning quality. We investigate a fundamental question that has not been studied before: does inference scaling improve reasoning faithfulness?\n  We conduct a comprehensive empirical study across four frontier models (GPT-5.2, Claude Opus 4.5, Gemini-3-flash-preview, and DeepSeek-v3.2) on 100 GSM8K mathematical reasoning problems. Our analysis employs bootstrap confidence intervals, McNemar's tests for paired comparisons, and Cohen's d effect sizes to quantify the effects rigorously. The results reveal striking differences across models that challenge common assumptions about self-consistency.\n  GPT-5.2 shows the expected pattern: accuracy improves from 78% to 90% at N=5, with faithfulness remaining relatively stable (0.540 to 0.510). Claude Opus 4.5 tells a completely different story. Its accuracy actually drops from 78% to 74.3% while faithfulness jumps dramatically from 0.270 to 0.891 at N=5. DeepSeek-v3.2, already at 98% accuracy, shows ceiling effects with modest faithfulness gains (0.440 to 0.541). Gemini-3-flash improves from 81% to 86% accuracy with a slight faithfulness decrease (0.260 to 0.212).\n  Problem difficulty analysis reveals that GPT-5.2 solves 82% of hard problems while breaking only 13% of easy ones. Claude, in contrast, breaks 23% of easy problems, explaining its accuracy decrease. These findings matter for practitioners: self-consistency is not universally beneficial, and teams should test their specific models before deployment. We release our code and provide practical recommendations for navigating these tradeoffs.",
    "translation": "标题：推理扩展能否提升推理忠实度？——关于自洽性权衡的多模型分析\n\n摘要：自洽性已成为提升大语言模型在推理任务上准确性的常用技术。该方法思路简明：生成多条推理路径，通过多数投票选择最常见答案。尽管这种方法能稳定提升准确率，但其增益是否真正反映推理质量的改善仍不明确。本文探究了一个此前未被研究的基础问题：推理扩展能否提升推理忠实度？\n\n我们在100道GSM8K数学推理问题上，对四种前沿模型（GPT-5.2、Claude Opus 4.5、Gemini-3-flash-preview和DeepSeek-v3.2）展开了全面实证研究。通过自助置信区间、配对比较的麦克尼马尔检验及科恩d效应值等量化分析方法，我们严谨评估了扩展推理的影响。研究结果揭示了模型间的显著差异，对关于自洽性的普遍假设提出了挑战。\n\nGPT-5.2呈现预期模式：当N=5时准确率从78%提升至90%，忠实度保持相对稳定（0.540至0.510）。Claude Opus 4.5则展现出完全不同的情况：其准确率从78%下降至74.3%，而忠实度在N=5时从0.270大幅跃升至0.891。DeepSeek-v3.2初始准确率已达98%，呈现天花板效应，仅获得有限的忠实度提升（0.440至0.541）。Gemini-3-flash准确率从81%提升至86%，但忠实度轻微下降（0.260至0.212）。\n\n问题难度分析表明：GPT-5.2能解决82%的难题，而仅在13%的简单问题上出现推理断裂。相比之下，Claude在23%的简单问题上出现断裂，这解释了其准确率下降的原因。这些发现对实践者具有重要意义：自洽性并非普遍有益，团队在部署前应针对具体模型进行测试。我们公开了研究代码，并为如何权衡这些特性提供了实用建议。",
    "url": "https://huggingface.co/papers/2601.06423",
    "arxiv_url": "https://arxiv.org/abs/2601.06423"
  },
  {
    "title": "FlyPose: Towards Robust Human Pose Estimation From Aerial Views",
    "summary": "Unmanned Aerial Vehicles (UAVs) are increasingly deployed in close proximity to humans for applications such as parcel delivery, traffic monitoring, disaster response and infrastructure inspections. Ensuring safe and reliable operation in these human-populated environments demands accurate perception of human poses and actions from an aerial viewpoint. This perspective challenges existing methods with low resolution, steep viewing angles and (self-)occlusion, especially if the application demands realtime feasibile models. We train and deploy FlyPose, a lightweight top-down human pose estimation pipeline for aerial imagery. Through multi-dataset training, we achieve an average improvement of 6.8 mAP in person detection across the test-sets of Manipal-UAV, VisDrone, HIT-UAV as well as our custom dataset. For 2D human pose estimation we report an improvement of 16.3 mAP on the challenging UAV-Human dataset. FlyPose runs with an inference latency of ~20 milliseconds including preprocessing on a Jetson Orin AGX Developer Kit and is deployed onboard a quadrotor UAV during flight experiments. We also publish FlyPose-104, a small but challenging aerial human pose estimation dataset, that includes manual annotations from difficult aerial perspectives: https://github.com/farooqhassaan/FlyPose.",
    "translation": "标题：FlyPose：面向无人机视角下鲁棒人体姿态估计的研究\n\n摘要：无人机正日益频繁地部署于人类活动密集的场景中，例如包裹配送、交通监控、灾害响应和基础设施巡检。为确保在此类人机共融环境中的安全可靠运行，需要从空中视角准确感知人体姿态与行为。这种视角因图像分辨率低、拍摄角度陡峭以及（自）遮挡等问题，对现有方法提出了严峻挑战，尤其在需要实时可行模型的应用场景中。本研究训练并部署了FlyPose——一种面向航拍图像的轻量级自上而下人体姿态估计流程。通过多数据集联合训练，我们在Manipal-UAV、VisDrone、HIT-UAV及自建数据集的测试集上实现了人体检测平均精度6.8 mAP的提升。针对二维人体姿态估计任务，我们在极具挑战性的UAV-Human数据集上取得了16.3 mAP的精度提升。FlyPose在Jetson Orin AGX开发套件上的推理延迟（含预处理）约为20毫秒，并在飞行实验中成功部署于四旋翼无人机平台。同时，我们开源了FlyPose-104数据集——一个规模较小但极具挑战性的航拍人体姿态估计数据集，包含从困难航拍视角手工标注的样本：https://github.com/farooqhassaan/FlyPose。",
    "url": "https://huggingface.co/papers/2601.05747",
    "arxiv_url": "https://arxiv.org/abs/2601.05747"
  },
  {
    "title": "Benchmarking Small Language Models and Small Reasoning Language Models on System Log Severity Classification",
    "summary": "System logs are crucial for monitoring and diagnosing modern computing infrastructure, but their scale and complexity require reliable and efficient automated interpretation. Since severity levels are predefined metadata in system log messages, having a model merely classify them offers limited standalone practical value, revealing little about its underlying ability to interpret system logs. We argue that severity classification is more informative when treated as a benchmark for probing runtime log comprehension rather than as an end task. Using real-world journalctl data from Linux production servers, we evaluate nine small language models (SLMs) and small reasoning language models (SRLMs) under zero-shot, few-shot, and retrieval-augmented generation (RAG) prompting. The results reveal strong stratification. Qwen3-4B achieves the highest accuracy at 95.64% with RAG, while Gemma3-1B improves from 20.25% under few-shot prompting to 85.28% with RAG. Notably, the tiny Qwen3-0.6B reaches 88.12% accuracy despite weak performance without retrieval. In contrast, several SRLMs, including Qwen3-1.7B and DeepSeek-R1-Distill-Qwen-1.5B, degrade substantially when paired with RAG. Efficiency measurements further separate models: most Gemma and Llama variants complete inference in under 1.2 seconds per log, whereas Phi-4-Mini-Reasoning exceeds 228 seconds per log while achieving <10% accuracy. These findings suggest that (1) architectural design, (2) training objectives, and (3) the ability to integrate retrieved context under strict output constraints jointly determine performance. By emphasizing small, deployable models, this benchmark aligns with real-time requirements of digital twin (DT) systems and shows that severity classification serves as a lens for evaluating model competence and real-time deployability, with implications for root cause analysis (RCA) and broader DT integration.",
    "translation": "标题：系统日志严重性分类任务中轻量级语言模型与轻量级推理语言模型的基准测试\n\n摘要：系统日志对于监控和诊断现代计算基础设施至关重要，但其规模与复杂性要求可靠高效的自动化解析。由于严重性级别是系统日志消息中预定义的元数据，仅让模型对其进行分类的独立实用价值有限，难以揭示模型理解系统日志的深层能力。我们认为，将严重性分类视为探究运行时日志理解能力的基准测试，而非最终任务，能提供更多信息。基于Linux生产服务器的真实journalctl数据，我们在零样本、少样本及检索增强生成（RAG）提示下评估了九种轻量级语言模型（SLM）与轻量级推理语言模型（SRLM）。结果显示出明显的性能分层：Qwen3-4B在RAG设置下达到最高准确率95.64%，而Gemma3-1B在少样本提示下准确率仅为20.25%，结合RAG后提升至85.28%。值得注意的是，微型的Qwen3-0.6B在无检索时表现较弱，但借助RAG准确率达到88.12%。相比之下，包括Qwen3-1.7B和DeepSeek-R1-Distill-Qwen-1.5B在内的多款SRLM在与RAG结合时性能显著下降。效率测试进一步区分了模型性能：多数Gemma和Llama变体能在每条日志1.2秒内完成推理，而Phi-4-Mini-Reasoning的单条日志推理时间超过228秒，准确率却低于10%。这些发现表明：（1）架构设计，（2）训练目标，以及（3）在严格输出约束下整合检索上下文的能力共同决定了模型表现。通过聚焦轻量可部署模型，本基准测试契合数字孪生（DT）系统的实时性需求，并证明严重性分类可作为评估模型能力与实时部署可行性的观察窗口，对根本原因分析（RCA）及更广泛的DT集成具有启示意义。",
    "url": "https://huggingface.co/papers/2601.07790",
    "arxiv_url": "https://arxiv.org/abs/2601.07790"
  },
  {
    "title": "Stochastic CHAOS: Why Deterministic Inference Kills, and Distributional Variability Is the Heartbeat of Artifical Cognition",
    "summary": "Deterministic inference is a comforting ideal in classical software: the same program on the same input should always produce the same output. As large language models move into real-world deployment, this ideal has been imported wholesale into inference stacks. Recent work from the Thinking Machines Lab has presented a detailed analysis of nondeterminism in LLM inference, showing how batch-invariant kernels and deterministic attention can enforce bitwise-identical outputs, positioning deterministic inference as a prerequisite for reproducibility and enterprise reliability.\n  In this paper, we take the opposite stance. We argue that, for LLMs, deterministic inference kills. It kills the ability to model uncertainty, suppresses emergent abilities, collapses reasoning into a single brittle path, and weakens safety alignment by hiding tail risks. LLMs implement conditional distributions over outputs, not fixed functions. Collapsing these distributions to a single canonical completion may appear reassuring, but it systematically conceals properties central to artificial cognition. We instead advocate Stochastic CHAOS, treating distributional variability as a signal to be measured and controlled.\n  Empirically, we show that deterministic inference is systematically misleading. Single-sample deterministic evaluation underestimates both capability and fragility, masking failure probability under paraphrases and noise. Phase-like transitions associated with emergent abilities disappear under greedy decoding. Multi-path reasoning degrades when forced onto deterministic backbones, reducing accuracy and diagnostic insight. Finally, deterministic evaluation underestimates safety risk by hiding rare but dangerous behaviors that appear only under multi-sample evaluation.",
    "translation": "标题：随机混沌：为何确定性推断扼杀人工智能认知，而分布变异性是其生命之源\n\n摘要：确定性推断是经典软件中一种令人安心的理想状态：相同程序在相同输入下应始终产生相同输出。随着大语言模型进入实际部署阶段，这一理念被全盘引入推断架构。思维机器实验室近期研究详细分析了LLM推断中的非确定性，展示了批不变内核与确定性注意力机制如何强制实现比特级完全一致的输出，并将确定性推断定位为可复现性与企业级可靠性的前提。\n\n本文持相反立场。我们认为，对于大语言模型而言，确定性推断具有扼杀性：它扼杀不确定性建模能力，抑制涌现能力，将推理压缩为单一脆弱路径，并通过隐藏尾部风险削弱安全对齐效果。大语言模型实现的是输出上的条件概率分布，而非固定函数。将这些分布坍缩为单一标准补全看似可靠，却系统性掩盖了人工认知的核心特性。我们主张采用“随机混沌”范式，将分布变异性视为需测量与控制的重要信号。\n\n实证研究表明，确定性推断具有系统性误导。单样本确定性评估会同时低估模型能力与脆弱性，掩盖在语义改写与噪声干扰下的失败概率。与涌现能力相关的类相变现象在贪婪解码下消失。强制采用确定性主干网络会降低多路径推理能力，损害准确性与诊断洞察力。最后，确定性评估通过隐藏仅在多样本评估中出现的罕见危险行为，导致安全风险被系统性低估。",
    "url": "https://huggingface.co/papers/2601.07239",
    "arxiv_url": "https://arxiv.org/abs/2601.07239"
  },
  {
    "title": "3D CoCa v2: Contrastive Learners with Test-Time Search for Generalizable Spatial Intelligence",
    "summary": "Spatial intelligence refers to the ability to perceive, reason about, and describe objects and their relationships within three-dimensional environments, forming a foundation for embodied perception and scene understanding. 3D captioning aims to describe 3D scenes in natural language; however, it remains challenging due to the sparsity and irregularity of point clouds and, more critically, the weak grounding and limited out-of-distribution (OOD) generalization of existing captioners across drastically different environments, including indoor and outdoor 3D scenes. To address this challenge, we propose 3D CoCa v2, a generalizable 3D captioning framework that unifies contrastive vision-language learning with 3D caption generation and further improves robustness via test-time search (TTS) without updating the captioner parameters. 3D CoCa v2 builds on a frozen CLIP-based semantic prior, a spatially-aware 3D scene encoder for geometry, and a multimodal decoder jointly optimized with contrastive and captioning objectives, avoiding external detectors or handcrafted proposals. At inference, TTS produces diverse caption candidates and performs reward-guided selection using a compact scene summary. Experiments show improvements over 3D CoCa of +1.50 CIDEr@0.5IoU on ScanRefer and +1.61 CIDEr@0.5IoU on Nr3D, and +3.8 CIDEr@0.25 in zero-shot OOD evaluation on TOD3Cap. Code will be released at https://github.com/AIGeeksGroup/3DCoCav2.",
    "translation": "标题：3D CoCa v2：结合测试时搜索的可泛化空间智能对比学习框架\n\n摘要：空间智能指在三维环境中感知、推理并描述物体及其关系的能力，是具身感知与场景理解的基础。三维场景描述旨在用自然语言描述三维场景，但由于点云的稀疏性与不规则性，以及现有描述模型在室内外等差异显著场景中存在的弱 grounding 问题和有限分布外泛化能力，该任务仍面临挑战。为解决这一问题，我们提出 3D CoCa v2——一种可泛化的三维场景描述框架，该框架将对比式视觉-语言学习与三维描述生成相统一，并通过无需更新模型参数的测试时搜索进一步提升鲁棒性。3D CoCa v2 基于冻结的 CLIP 语义先验、感知几何信息的空间感知三维场景编码器，以及通过对比学习与描述生成目标联合优化的多模态解码器构建，无需依赖外部检测器或人工提案。在推理阶段，测试时搜索生成多样化的描述候选，并基于紧凑场景摘要进行奖励引导的选择。实验表明，本方法在 ScanRefer 数据集上 CIDEr@0.5IoU 指标提升 1.50，在 Nr3D 数据集上提升 1.61，在 TOD3Cap 的零样本分布外评估中 CIDEr@0.25 指标提升 3.8。代码将发布于 https://github.com/AIGeeksGroup/3DCoCav2。",
    "url": "https://huggingface.co/papers/2601.06496",
    "arxiv_url": "https://arxiv.org/abs/2601.06496"
  },
  {
    "title": "On the Fallacy of Global Token Perplexity in Spoken Language Model Evaluation",
    "summary": "Generative spoken language models pretrained on large-scale raw audio can continue a speech prompt with appropriate content while preserving attributes like speaker and emotion, serving as foundation models for spoken dialogue. In prior literature, these models are often evaluated using ``global token perplexity'', which directly applies the text perplexity formulation to speech tokens. However, this practice overlooks fundamental differences between speech and text modalities, possibly leading to an underestimation of the speech characteristics. In this work, we propose a variety of likelihood- and generative-based evaluation methods that serve in place of naive global token perplexity. We demonstrate that the proposed evaluations more faithfully reflect perceived generation quality, as evidenced by stronger correlations with human-rated mean opinion scores (MOS). When assessed under the new metrics, the relative performance landscape of spoken language models is reshaped, revealing a significantly reduced gap between the best-performing model and the human topline. Together, these results suggest that appropriate evaluation is critical for accurately assessing progress in spoken language modeling.",
    "translation": "标题：论口语语言模型评估中全局词元困惑度的谬误\n\n摘要：基于大规模原始音频预训练的生成式口语语言模型能够在保持说话者身份与情感特征等属性的同时，生成符合语境的语音延续内容，已成为口语对话系统的基石模型。在现有文献中，这类模型常采用\"全局词元困惑度\"进行评估——该方法直接将文本困惑度公式应用于语音词元。然而，这种评估方式忽视了语音与文本模态间的本质差异，可能导致对语音特性的低估。本研究提出一系列基于似然估计与生成质量的评估方法，以替代简单的全局词元困惑度指标。实验证明，所提出的评估方法能更真实地反映感知生成质量，其与人工评定的平均意见得分（MOS）之间展现出更强的相关性。在新指标评估体系下，口语语言模型的性能对比格局发生重构：最佳模型与人类表现上限之间的差距显著缩小。这些结果表明，采用恰当的评估方法对于准确衡量口语语言建模的发展进程具有至关重要的意义。",
    "url": "https://huggingface.co/papers/2601.06329",
    "arxiv_url": "https://arxiv.org/abs/2601.06329"
  },
  {
    "title": "A Rising Tide Lifts All Boats: MTQE Rewards for Idioms Improve General Translation Quality",
    "summary": "Non-compositional expressions (e.g., idioms, proverbs, and metaphors) pose significant challenges for neural machine translation systems because their meanings cannot be derived from individual words alone. These expressions encode rich, cultural meaning, and have both figurative and literal meanings, making accurate translation difficult. Because models are fairly good at translating compositional text, we investigate GRPO-style fine-tuning using Machine Translation Quality Estimation (MTQE) models as reward functions to train models to better translate idioms. Using Chinese and Hindi idiom datasets, we find that idiom translation abilities improve by ~14 points, general, non-idiomatic translation implicitly improves by ~8 points, and cross-lingual translation abilities (trained on one language, evaluated on another) improves by ~6 points. Overall, our work quantifies the non-compositional translation gap and offers insights for developing LLMs with stronger cross-cultural and figurative language understanding.",
    "translation": "标题：水涨船高：基于机器翻译质量估计的习语奖励机制提升整体翻译质量\n\n摘要：非组合性表达（如习语、谚语和隐喻）对神经机器翻译系统构成显著挑战，因其含义无法仅通过单个词汇推导得出。这类表达承载着丰富的文化内涵，兼具比喻义与字面义，导致准确翻译难度较高。鉴于现有模型在组合性文本翻译中表现良好，本研究探索采用机器翻译质量估计模型作为奖励函数，通过GRPO式微调训练模型以提升习语翻译能力。基于汉语和印地语习语数据集的实验表明：模型习语翻译能力提升约14个百分点，通用非习语翻译能力隐性提升约8个百分点，跨语言翻译能力（单语言训练、跨语言评估）提升约6个百分点。本研究量化了非组合性表达的翻译差距，为开发具有更强跨文化及比喻语言理解能力的大语言模型提供了理论依据。",
    "url": "https://huggingface.co/papers/2601.06307",
    "arxiv_url": "https://arxiv.org/abs/2601.06307"
  },
  {
    "title": "SPINAL -- Scaling-law and Preference Integration in Neural Alignment Layers",
    "summary": "Direct Preference Optimization (DPO) is a principled, scalable alternative to RLHF for aligning large language models from pairwise preferences, but its internal geometric footprint remains undercharacterized, limiting audits, checkpoint comparisons, and failure prediction. We introduce SPINAL (Scaling-law and Preference Integration in Neural Alignment Layers), a diagnostic that measures how alignment reshapes representations across depth by tracing localized structural change layer by layer. Across model families, DPO produces a layerwise calibration effect concentrated in the final decoder blocks (often layers 21-30), where preference gradients most directly affect the next-token distribution. SPINAL encodes each checkpoint as a depth trace over (layer index, contraction score, transport score). The contraction score summarizes how quickly the tail of a layer's spectrum decays (how fast small modes vanish); higher values indicate stronger contraction into fewer effective directions. The transport score summarizes how much the token distribution shifts between adjacent layers using a bounded overlap measure; lower values indicate shorter, smoother steps through representation space. Aligned checkpoints show a late-layer ramp-up in contraction and a smooth reduction in transport, consistent with tightened and stabilized policy mass, while unaligned models trace higher-curvature, more entropic, and geometrically incoherent depth paths. Overall, alignment is geometrically localized: the final layers encode the dominant preference-induced corrections. SPINAL turns this localization into a practical audit signal, quantifying where alignment concentrates, how strongly it manifests, and when it begins to destabilize during training.",
    "translation": "标题：SPINAL——神经对齐层中的缩放律与偏好整合\n\n摘要：直接偏好优化（DPO）是一种基于原则且可扩展的方法，可作为RLHF的替代方案，用于根据成对偏好对齐大语言模型。然而，其内部几何特征尚未得到充分刻画，这限制了对模型的审计、检查点比较以及故障预测。本文提出SPINAL（神经对齐层中的缩放律与偏好整合），这是一种通过逐层追踪局部结构变化来度量对齐过程如何重塑各层表征的诊断方法。在不同模型系列中，DPO会产生一种集中于最后解码器块（通常为第21至30层）的逐层校准效应，偏好梯度在此处对下一词元分布产生最直接影响。SPINAL将每个检查点编码为基于（层索引、收缩分数、传输分数）的深度轨迹。收缩分数概括了层谱尾部的衰减速度（小模态消失的快慢），数值越高表明表征向更少有效方向的收缩越强；传输分数则通过有界重叠度量概括相邻层间词元分布的偏移程度，数值越低表明表征空间的步进更短、更平滑。对齐后的检查点显示出收缩分数在深层骤升、传输分数平稳下降的特征，与策略分布的收紧和稳定相一致；而未对齐模型则呈现出更高曲率、更高熵值及几何不连贯的深度轨迹。总体而言，对齐过程具有几何局部性：最终层编码了最主要的偏好诱导修正。SPINAL将这种局部性转化为实用的审计信号，可量化对齐集中发生的层位、其表现强度以及在训练过程中开始失稳的时机。",
    "url": "https://huggingface.co/papers/2601.06238",
    "arxiv_url": "https://arxiv.org/abs/2601.06238"
  }
]