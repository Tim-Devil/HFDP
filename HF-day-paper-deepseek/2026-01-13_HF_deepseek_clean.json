[
  {
    "title": "Watching, Reasoning, and Searching: A Video Deep Research Benchmark on Open Web for Agentic Video Reasoning",
    "summary": "In real-world video question answering scenarios, videos often provide only localized visual cues, while verifiable answers are distributed across the open web; models therefore need to jointly perform cross-frame clue extraction, iterative retrieval, and multi-hop reasoning-based verification. To bridge this gap, we construct the first video deep research benchmark, VideoDR. VideoDR centers on video-conditioned open-domain video question answering, requiring cross-frame visual anchor extraction, interactive web retrieval, and multi-hop reasoning over joint video-web evidence; through rigorous human annotation and quality control, we obtain high-quality video deep research samples spanning six semantic domains. We evaluate multiple closed-source and open-source multimodal large language models under both the Workflow and Agentic paradigms, and the results show that Agentic is not consistently superior to Workflow: its gains depend on a model's ability to maintain the initial video anchors over long retrieval chains. Further analysis indicates that goal drift and long-horizon consistency are the core bottlenecks. In sum, VideoDR provides a systematic benchmark for studying video agents in open-web settings and reveals the key challenges for next-generation video deep research agents.",
    "translation": "标题：观看、推理与搜索：面向智能体视频推理的开放网络视频深度研究基准\n\n摘要：在真实世界视频问答场景中，视频通常仅提供局部视觉线索，而可验证答案广泛分布于开放网络；模型因此需要联合执行跨帧线索提取、迭代检索以及基于多跳推理的验证。为弥合这一差距，我们构建了首个视频深度研究基准VideoDR。该基准以视频条件开放域视频问答为核心，要求进行跨帧视觉锚点提取、交互式网络检索，以及对视频-网络联合证据的多跳推理；通过严格的人工标注与质量控制，我们获得了涵盖六个语义领域的高质量视频深度研究样本。我们分别在流程式与智能体式两种范式下评估了多个闭源与开源多模态大语言模型，结果表明智能体范式并非始终优于流程式范式：其性能提升取决于模型在长检索链中保持初始视频锚点的能力。进一步分析指出，目标漂移与长程一致性是核心瓶颈。综上所述，VideoDR为开放网络环境下的视频智能体研究提供了系统性基准，并揭示了新一代视频深度研究智能体面临的关键挑战。",
    "url": "https://huggingface.co/papers/2601.06943",
    "arxiv_url": "https://arxiv.org/abs/2601.06943"
  },
  {
    "title": "BabyVision: Visual Reasoning Beyond Language",
    "summary": "While humans develop core visual skills long before acquiring language, contemporary Multimodal LLMs (MLLMs) still rely heavily on linguistic priors to compensate for their fragile visual understanding. We uncovered a crucial fact: state-of-the-art MLLMs consistently fail on basic visual tasks that humans, even 3-year-olds, can solve effortlessly. To systematically investigate this gap, we introduce BabyVision, a benchmark designed to assess core visual abilities independent of linguistic knowledge for MLLMs. BabyVision spans a wide range of tasks, with 388 items divided into 22 subclasses across four key categories. Empirical results and human evaluation reveal that leading MLLMs perform significantly below human baselines. Gemini3-Pro-Preview scores 49.7, lagging behind 6-year-old humans and falling well behind the average adult score of 94.1. These results show despite excelling in knowledge-heavy evaluations, current MLLMs still lack fundamental visual primitives. Progress in BabyVision represents a step toward human-level visual perception and reasoning capabilities. We also explore solving visual reasoning with generation models by proposing BabyVision-Gen and automatic evaluation toolkit. Our code and benchmark data are released at https://github.com/UniPat-AI/BabyVision for reproduction.",
    "translation": "标题：BabyVision：超越语言的视觉推理能力研究\n\n摘要：人类在掌握语言之前便已发展出核心视觉能力，而当代多模态大语言模型（MLLMs）仍严重依赖语言先验知识来弥补其薄弱的视觉理解能力。我们发现一个重要事实：当前最先进的MLLMs在人类（甚至三岁儿童）可轻松解决的基础视觉任务上持续表现不佳。为系统探究这一差距，我们提出了BabyVision基准测试，旨在独立于语言知识评估MLLMs的核心视觉能力。BabyVision涵盖广泛任务类型，包含四大关键类别下的22个子类共388项测试项目。实证结果与人工评估表明，主流MLLMs的表现显著低于人类基线水平。其中Gemini3-Pro-Preview仅获49.7分，落后于六岁儿童水平，与成人平均94.1分存在巨大差距。这些结果揭示，尽管当前MLLMs在知识密集型评估中表现优异，但仍缺乏基础视觉认知能力。BabyVision的进展标志着向人类水平视觉感知与推理能力迈出了重要一步。我们还通过提出BabyVision-Gen与自动评估工具包，探索利用生成模型解决视觉推理问题。相关代码与基准数据已发布于https://github.com/UniPat-AI/BabyVision以供复现研究。",
    "url": "https://huggingface.co/papers/2601.06521",
    "arxiv_url": "https://arxiv.org/abs/2601.06521"
  },
  {
    "title": "PaCoRe: Learning to Scale Test-Time Compute with Parallel Coordinated Reasoning",
    "summary": "We introduce Parallel Coordinated Reasoning (PaCoRe), a training-and-inference framework designed to overcome a central limitation of contemporary language models: their inability to scale test-time compute (TTC) far beyond sequential reasoning under a fixed context window. PaCoRe departs from the traditional sequential paradigm by driving TTC through massive parallel exploration coordinated via a message-passing architecture in multiple rounds. Each round launches many parallel reasoning trajectories, compacts their findings into context-bounded messages, and synthesizes these messages to guide the next round and ultimately produce the final answer. Trained end-to-end with large-scale, outcome-based reinforcement learning, the model masters the synthesis abilities required by PaCoRe and scales to multi-million-token effective TTC without exceeding context limits. The approach yields strong improvements across diverse domains, and notably pushes reasoning beyond frontier systems in mathematics: an 8B model reaches 94.5% on HMMT 2025, surpassing GPT-5's 93.2% by scaling effective TTC to roughly two million tokens. We open-source model checkpoints, training data, and the full inference pipeline to accelerate follow-up work.",
    "translation": "标题：PaCoRe：通过并行协同推理实现测试时计算规模化的学习方法\n\n摘要：本文提出并行协同推理（PaCoRe），一种旨在解决当代语言模型核心局限的训练与推理框架：模型无法在固定上下文窗口限制下，将测试时计算（TTC）规模显著扩展至超越序列推理的范畴。PaCoRe摒弃传统序列化范式，通过基于消息传递架构的多轮协调机制，驱动测试时计算实现大规模并行探索。每一轮并行启动多条推理轨迹，将其发现压缩至上下文受限的消息中，并综合这些消息以指导下一轮推理，最终生成答案。通过基于结果的大规模端到端强化学习训练，模型掌握了PaCoRe所需的信息综合能力，能够将有效测试时计算扩展至数百万token量级，同时不突破上下文长度限制。该方法在多个领域均带来显著性能提升，尤其在数学推理领域超越了前沿系统：一个80亿参数的模型在HMMT 2025测试中达到94.5%的准确率，通过将有效测试时计算扩展至约两百万token，超越了GPT-5的93.2%表现。我们开源了模型检查点、训练数据及完整推理流程，以加速后续研究。",
    "url": "https://huggingface.co/papers/2601.05593",
    "arxiv_url": "https://arxiv.org/abs/2601.05593"
  },
  {
    "title": "MHLA: Restoring Expressivity of Linear Attention via Token-Level Multi-Head",
    "summary": "While the Transformer architecture dominates many fields, its quadratic self-attention complexity hinders its use in large-scale applications. Linear attention offers an efficient alternative, but its direct application often degrades performance, with existing fixes typically re-introducing computational overhead through extra modules (e.g., depthwise separable convolution) that defeat the original purpose. In this work, we identify a key failure mode in these methods: global context collapse, where the model loses representational diversity. To address this, we propose Multi-Head Linear Attention (MHLA), which preserves this diversity by computing attention within divided heads along the token dimension. We prove that MHLA maintains linear complexity while recovering much of the expressive power of softmax attention, and verify its effectiveness across multiple domains, achieving a 3.6\\% improvement on ImageNet classification, a 6.3\\% gain on NLP, a 12.6\\% improvement on image generation, and a 41\\% enhancement on video generation under the same time complexity.",
    "translation": "标题：MHLA：通过令牌级多头机制恢复线性注意力的表达能力\n\n摘要：尽管Transformer架构在众多领域占据主导地位，但其二次复杂度的自注意力机制限制了其在大规模场景中的应用。线性注意力提供了一种高效的替代方案，但其直接应用往往导致性能下降。现有改进方法通常通过引入额外模块（如深度可分离卷积）重新引入计算开销，这违背了设计初衷。本文指出这些方法存在一个关键缺陷：全局上下文坍缩，即模型丧失表征多样性。为解决此问题，我们提出多头线性注意力机制（MHLA），通过在令牌维度划分的注意力头内计算注意力以保持多样性。我们证明MHLA在维持线性复杂度的同时，能够恢复softmax注意力的大部分表达能力，并在多个领域验证其有效性：在相同时间复杂度下，ImageNet分类任务提升3.6%，自然语言处理任务提升6.3%，图像生成任务提升12.6%，视频生成任务提升41%。",
    "url": "https://huggingface.co/papers/2601.07832",
    "arxiv_url": "https://arxiv.org/abs/2601.07832"
  },
  {
    "title": "X-Coder: Advancing Competitive Programming with Fully Synthetic Tasks, Solutions, and Tests",
    "summary": "Competitive programming presents great challenges for Code LLMs due to its intensive reasoning demands and high logical complexity. However, current Code LLMs still rely heavily on real-world data, which limits their scalability. In this paper, we explore a fully synthetic approach: training Code LLMs with entirely generated tasks, solutions, and test cases, to empower code reasoning models without relying on real-world data. To support this, we leverage feature-based synthesis to propose a novel data synthesis pipeline called SynthSmith. SynthSmith shows strong potential in producing diverse and challenging tasks, along with verified solutions and tests, supporting both supervised fine-tuning and reinforcement learning. Based on the proposed synthetic SFT and RL datasets, we introduce the X-Coder model series, which achieves a notable pass rate of 62.9 avg@8 on LiveCodeBench v5 and 55.8 on v6, outperforming DeepCoder-14B-Preview and AReal-boba2-14B despite having only 7B parameters. In-depth analysis reveals that scaling laws hold on our synthetic dataset, and we explore which dimensions are more effective to scale. We further provide insights into code-centric reinforcement learning and highlight the key factors that shape performance through detailed ablations and analysis. Our findings demonstrate that scaling high-quality synthetic data and adopting staged training can greatly advance code reasoning, while mitigating reliance on real-world coding data.",
    "translation": "标题：X-Coder：基于全合成任务、解决方案与测试的竞争性编程推进研究\n\n摘要：竞争性编程因其密集的推理需求与高逻辑复杂度，对代码大语言模型提出了巨大挑战。然而，当前代码大语言模型仍严重依赖现实世界数据，这限制了其可扩展性。本文探索了一种全合成方法：通过完全生成的任务、解决方案与测试用例训练代码大语言模型，从而在不依赖现实数据的情况下增强代码推理能力。为此，我们基于特征合成技术提出了一种名为SynthSmith的新型数据合成流程。SynthSmith展现出生成多样化、高难度任务以及已验证解决方案与测试用例的强大潜力，同时支持监督微调与强化学习。基于所提出的合成监督微调与强化学习数据集，我们推出了X-Coder模型系列。该系列模型在LiveCodeBench v5上达到62.9 avg@8的显著通过率，在v6上达到55.8，仅以70亿参数即超越了DeepCoder-14B-Preview与AReal-boba2-14B模型。深入分析表明，缩放定律在我们的合成数据集上依然成立，并进一步探索了哪些维度对扩展更有效。我们通过详细的消融实验与分析，深入探讨了以代码为中心的强化学习机制，并揭示了影响性能的关键因素。研究结果表明，扩展高质量合成数据并采用分阶段训练策略能显著推进代码推理能力的发展，同时减少对现实世界编程数据的依赖。",
    "url": "https://huggingface.co/papers/2601.06953",
    "arxiv_url": "https://arxiv.org/abs/2601.06953"
  },
  {
    "title": "GlimpRouter: Efficient Collaborative Inference by Glimpsing One Token of Thoughts",
    "summary": "Large Reasoning Models (LRMs) achieve remarkable performance by explicitly generating multi-step chains of thought, but this capability incurs substantial inference latency and computational cost. Collaborative inference offers a promising solution by selectively allocating work between lightweight and large models, yet a fundamental challenge remains: determining when a reasoning step requires the capacity of a large model or the efficiency of a small model. Existing routing strategies either rely on local token probabilities or post-hoc verification, introducing significant inference overhead. In this work, we propose a novel perspective on step-wise collaboration: the difficulty of a reasoning step can be inferred from its very first token. Inspired by the \"Aha Moment\" phenomenon in LRMs, we show that the entropy of the initial token serves as a strong predictor of step difficulty. Building on this insight, we introduce GlimpRouter, a training-free step-wise collaboration framework. GlimpRouter employs a lightweight model to generate only the first token of each reasoning step and routes the step to a larger model only when the initial token entropy exceeds a threshold. Experiments on multiple benchmarks demonstrate that our approach significantly reduces inference latency while preserving accuracy. For instance, GlimpRouter attains a substantial 10.7% improvement in accuracy while reducing inference latency by 25.9% compared to a standalone large model on AIME25. These results suggest a simple yet effective mechanism for reasoning: allocating computation based on a glimpse of thought rather than full-step evaluation.",
    "translation": "标题：GlimpRouter：通过思维一瞥实现高效协同推理\n\n摘要：大型推理模型通过显式生成多步思维链取得了卓越性能，但这种能力会带来显著的推理延迟和计算成本。协同推理通过在轻量级模型与大型模型之间选择性分配工作提供了有前景的解决方案，然而一个根本性挑战仍然存在：如何判断推理步骤何时需要大型模型的能力，何时可交由小型模型高效处理。现有路由策略要么依赖局部词元概率，要么采用事后验证机制，均会引入显著的推理开销。本研究提出一种新颖的逐步骤协同视角：推理步骤的难度可通过其首个词元进行推断。受大型推理模型中“顿悟时刻”现象的启发，我们发现初始词元的熵值可作为步骤难度的强预测指标。基于此洞见，我们提出了GlimpRouter——一种无需训练的逐步骤协同推理框架。该框架使用轻量级模型仅生成每个推理步骤的首个词元，仅当初始词元熵值超过阈值时才将步骤路由至大型模型。在多个基准测试上的实验表明，该方法在保持准确性的同时显著降低了推理延迟。例如在AIME25基准上，相较于独立大型模型，GlimpRouter在降低25.9%推理延迟的同时实现了10.7%的准确率提升。这些结果表明了一种简单而有效的推理机制：基于思维片段的“一瞥”而非完整步骤评估来分配计算资源。",
    "url": "https://huggingface.co/papers/2601.05110",
    "arxiv_url": "https://arxiv.org/abs/2601.05110"
  },
  {
    "title": "Lost in the Noise: How Reasoning Models Fail with Contextual Distractors",
    "summary": "Recent advances in reasoning models and agentic AI systems have led to an increased reliance on diverse external information. However, this shift introduces input contexts that are inherently noisy, a reality that current sanitized benchmarks fail to capture. We introduce NoisyBench, a comprehensive benchmark that systematically evaluates model robustness across 11 datasets in RAG, reasoning, alignment, and tool-use tasks against diverse noise types, including random documents, irrelevant chat histories, and hard negative distractors. Our evaluation reveals a catastrophic performance drop of up to 80% in state-of-the-art models when faced with contextual distractors. Crucially, we find that agentic workflows often amplify these errors by over-trusting noisy tool outputs, and distractors can trigger emergent misalignment even without adversarial intent. We find that prompting, context engineering, SFT, and outcome-reward only RL fail to ensure robustness; in contrast, our proposed Rationale-Aware Reward (RARE) significantly strengthens resilience by incentivizing the identification of helpful information within noise. Finally, we uncover an inverse scaling trend where increased test-time computation leads to worse performance in noisy settings and demonstrate via attention visualization that models disproportionately focus on distractor tokens, providing vital insights for building the next generation of robust, reasoning-capable agents.",
    "translation": "标题：迷失于噪声之中：推理模型如何在上下文干扰项下失效\n\n摘要：推理模型与智能体人工智能系统的最新进展，导致了对多样化外部信息的日益依赖。然而，这种转变引入了本质上充满噪声的输入上下文，而当前经过净化的基准测试未能捕捉这一现实。我们提出了NoisyBench，这是一个综合性基准，系统性地评估了模型在RAG、推理、对齐和工具使用任务中，针对包括随机文档、无关聊天历史和困难负例干扰项在内的多种噪声类型，在11个数据集上的鲁棒性。我们的评估显示，在面对上下文干扰项时，最先进的模型性能会出现高达80%的灾难性下降。关键的是，我们发现智能体工作流常常因过度信任有噪声的工具输出而放大这些错误，并且干扰项即使没有对抗意图也可能引发突发的错位行为。我们发现，提示工程、上下文工程、监督微调以及仅基于结果奖励的强化学习均无法确保鲁棒性；相比之下，我们提出的**理性感知奖励**通过激励模型识别噪声中有用的信息，显著增强了其抗干扰能力。最后，我们揭示了一种逆向缩放趋势，即增加测试时的计算量反而导致在噪声环境下的性能更差，并通过注意力可视化证明模型会不成比例地关注干扰项标记，这为构建下一代鲁棒且具备推理能力的智能体提供了至关重要的见解。",
    "url": "https://huggingface.co/papers/2601.07226",
    "arxiv_url": "https://arxiv.org/abs/2601.07226"
  },
  {
    "title": "OS-Symphony: A Holistic Framework for Robust and Generalist Computer-Using Agent",
    "summary": "While Vision-Language Models (VLMs) have significantly advanced Computer-Using Agents (CUAs), current frameworks struggle with robustness in long-horizon workflows and generalization in novel domains. These limitations stem from a lack of granular control over historical visual context curation and the absence of visual-aware tutorial retrieval. To bridge these gaps, we introduce OS-Symphony, a holistic framework that comprises an Orchestrator coordinating two key innovations for robust automation: (1) a Reflection-Memory Agent that utilizes milestone-driven long-term memory to enable trajectory-level self-correction, effectively mitigating visual context loss in long-horizon tasks; (2) Versatile Tool Agents featuring a Multimodal Searcher that adopts a SeeAct paradigm to navigate a browser-based sandbox to synthesize live, visually aligned tutorials, thereby resolving fidelity issues in unseen scenarios. Experimental results demonstrate that OS-Symphony delivers substantial performance gains across varying model scales, establishing new state-of-the-art results on three online benchmarks, notably achieving 65.84% on OSWorld.",
    "translation": "标题：OS-Symphony：一种面向鲁棒通用计算机使用智能体的整体框架\n\n摘要：尽管视觉语言模型（VLMs）显著推动了计算机使用智能体（CUAs）的发展，但现有框架在长流程工作流的鲁棒性和新领域泛化能力方面仍面临挑战。这些局限主要源于对历史视觉上下文管理缺乏细粒度控制，以及缺少视觉感知的教程检索机制。为弥补这些不足，本文提出OS-Symphony整体框架，其核心协调器整合了两项关键创新以实现鲁棒自动化：（1）反思记忆智能体，利用里程碑驱动的长期记忆实现轨迹级自我修正，有效缓解长流程任务中的视觉上下文丢失问题；（2）多功能工具智能体，配备采用SeeAct范式的多模态检索器，可在基于浏览器的沙箱环境中导航并合成实时视觉对齐教程，从而解决未知场景中的保真度问题。实验结果表明，OS-Symphony在不同规模模型上均实现显著性能提升，在三个在线基准测试中创下最新最优记录，其中在OSWorld基准上达到65.84%的显著成效。",
    "url": "https://huggingface.co/papers/2601.07779",
    "arxiv_url": "https://arxiv.org/abs/2601.07779"
  },
  {
    "title": "Beyond Hard Masks: Progressive Token Evolution for Diffusion Language Models",
    "summary": "Diffusion Language Models (DLMs) offer a promising alternative for language modeling by enabling parallel decoding through iterative refinement. However, most DLMs rely on hard binary masking and discrete token assignments, which hinder the revision of early decisions and underutilize intermediate probabilistic representations. In this paper, we propose EvoToken-DLM, a novel diffusion-based language modeling approach that replaces hard binary masks with evolving soft token distributions. EvoToken-DLM enables a progressive transition from masked states to discrete outputs, supporting revisable decoding. To effectively support this evolution, we introduce continuous trajectory supervision, which aligns training objectives with iterative probabilistic updates. Extensive experiments across multiple benchmarks show that EvoToken-DLM consistently achieves superior performance, outperforming strong diffusion-based and masked DLM baselines. Project webpage: https://aim-uofa.github.io/EvoTokenDLM.",
    "translation": "标题：超越硬掩码：扩散语言模型的渐进式词元演化\n\n摘要：扩散语言模型通过迭代优化实现并行解码，为语言建模提供了一种前景广阔的替代方案。然而，现有扩散语言模型大多依赖硬二元掩码和离散词元分配机制，这限制了早期决策的可修正性，且未能充分利用中间概率表示。本文提出EvoToken-DLM——一种基于扩散机制的新型语言建模方法，该方法通过演化的软词元分布替代硬二元掩码。EvoToken-DLM实现了从掩码状态到离散输出的渐进式过渡，支持可修正的解码过程。为有效支撑这种演化机制，我们引入连续轨迹监督方法，使训练目标与迭代概率更新过程保持对齐。在多个基准测试上的广泛实验表明，EvoToken-DLM始终取得卓越性能，显著优于现有基于扩散和掩码的扩散语言模型基线。项目主页：https://aim-uofa.github.io/EvoTokenDLM。",
    "url": "https://huggingface.co/papers/2601.07351",
    "arxiv_url": "https://arxiv.org/abs/2601.07351"
  },
  {
    "title": "Controllable Memory Usage: Balancing Anchoring and Innovation in Long-Term Human-Agent Interaction",
    "summary": "As LLM-based agents are increasingly used in long-term interactions, cumulative memory is critical for enabling personalization and maintaining stylistic consistency. However, most existing systems adopt an ``all-or-nothing'' approach to memory usage: incorporating all relevant past information can lead to Memory Anchoring, where the agent is trapped by past interactions, while excluding memory entirely results in under-utilization and the loss of important interaction history. We show that an agent's reliance on memory can be modeled as an explicit and user-controllable dimension. We first introduce a behavioral metric of memory dependence to quantify the influence of past interactions on current outputs. We then propose Steerable Memory Agent, SteeM, a framework that allows users to dynamically regulate memory reliance, ranging from a fresh-start mode that promotes innovation to a high-fidelity mode that closely follows interaction history. Experiments across different scenarios demonstrate that our approach consistently outperforms conventional prompting and rigid memory masking strategies, yielding a more nuanced and effective control for personalized human-agent collaboration.",
    "translation": "标题：可控记忆使用：长期人机交互中的锚定与创新平衡\n\n摘要：随着基于大语言模型的智能体在长期交互中的应用日益广泛，累积记忆对于实现个性化服务和保持风格一致性至关重要。然而，现有系统大多采用“全有或全无”的记忆使用策略：若完全纳入过往相关信息，可能导致“记忆锚定”现象，使智能体受困于历史交互模式；而完全排除记忆则会造成信息利用不足及重要交互历史的丢失。本文提出可将智能体对记忆的依赖程度建模为显式且用户可控的维度。我们首先引入记忆依赖性的行为度量指标，用以量化历史交互对当前输出的影响程度。进而提出可调控记忆智能体框架SteeM，该框架允许用户动态调节记忆依赖强度——从促进创新的“全新启动”模式到严格遵循交互历史的“高保真”模式。多场景实验表明，该方法在个性化人机协作中始终优于传统提示策略与刚性记忆屏蔽方案，能够实现更精细有效的交互控制。",
    "url": "https://huggingface.co/papers/2601.05107",
    "arxiv_url": "https://arxiv.org/abs/2601.05107"
  },
  {
    "title": "DrivingGen: A Comprehensive Benchmark for Generative Video World Models in Autonomous Driving",
    "summary": "Video generation models, as one form of world models, have emerged as one of the most exciting frontiers in AI, promising agents the ability to imagine the future by modeling the temporal evolution of complex scenes. In autonomous driving, this vision gives rise to driving world models: generative simulators that imagine ego and agent futures, enabling scalable simulation, safe testing of corner cases, and rich synthetic data generation. Yet, despite fast-growing research activity, the field lacks a rigorous benchmark to measure progress and guide priorities. Existing evaluations remain limited: generic video metrics overlook safety-critical imaging factors; trajectory plausibility is rarely quantified; temporal and agent-level consistency is neglected; and controllability with respect to ego conditioning is ignored. Moreover, current datasets fail to cover the diversity of conditions required for real-world deployment. To address these gaps, we present DrivingGen, the first comprehensive benchmark for generative driving world models. DrivingGen combines a diverse evaluation dataset curated from both driving datasets and internet-scale video sources, spanning varied weather, time of day, geographic regions, and complex maneuvers, with a suite of new metrics that jointly assess visual realism, trajectory plausibility, temporal coherence, and controllability. Benchmarking 14 state-of-the-art models reveals clear trade-offs: general models look better but break physics, while driving-specific ones capture motion realistically but lag in visual quality. DrivingGen offers a unified evaluation framework to foster reliable, controllable, and deployable driving world models, enabling scalable simulation, planning, and data-driven decision-making.",
    "translation": "标题：DrivingGen：自动驾驶生成式视频世界模型的综合基准测试\n\n摘要：作为世界模型的一种形式，视频生成模型已成为人工智能领域最激动人心的前沿方向之一，其通过建模复杂场景的时间演化，使智能体具备预测未来的能力。在自动驾驶领域，这一愿景催生了驾驶世界模型：这类生成式模拟器能够推演自车与其他交通参与者的未来状态，从而实现可扩展的仿真、极端场景的安全测试以及丰富的合成数据生成。然而，尽管相关研究快速增长，该领域仍缺乏严谨的基准测试来衡量进展并指引重点方向。现有评估方法存在明显局限：通用视频指标忽略了安全关键的成像因素；轨迹合理性鲜少被量化；时间一致性与智能体层级一致性未被重视；基于自车条件的可控性亦遭忽视。此外，当前数据集未能覆盖现实部署所需的多变场景条件。为弥补这些不足，我们提出了DrivingGen——首个面向生成式驾驶世界模型的综合基准测试。DrivingGen整合了从驾驶数据集和互联网规模视频源中精选的多样化评估数据集，涵盖不同天气、昼夜时段、地理区域及复杂驾驶场景，并配套一套全新评估指标，从视觉真实感、轨迹合理性、时间连贯性和可控性四个维度进行联合评估。通过对14个前沿模型的基准测试，我们揭示了明确的权衡关系：通用模型视觉表现更佳但违背物理规律，而驾驶专用模型能真实捕捉运动模式却在视觉质量上存在不足。DrivingGen提供了一个统一的评估框架，旨在推动可靠、可控、可部署的驾驶世界模型发展，为可扩展仿真、路径规划与数据驱动决策提供支撑。",
    "url": "https://huggingface.co/papers/2601.01528",
    "arxiv_url": "https://arxiv.org/abs/2601.01528"
  },
  {
    "title": "MegaFlow: Large-Scale Distributed Orchestration System for the Agentic Era",
    "summary": "The rapid development of interactive and autonomous AI systems signals our entry into the agentic era. Training and evaluating agents on complex agentic tasks such as software engineering and computer use requires not only efficient model computation but also sophisticated infrastructure capable of coordinating vast agent-environment interactions. However, no open-source infrastructure can effectively support large-scale training and evaluation on such complex agentic tasks. To address this challenge, we present MegaFlow, a large-scale distributed orchestration system that enables efficient scheduling, resource allocation, and fine-grained task management for agent-environment workloads. MegaFlow abstracts agent training infrastructure into three independent services (Model Service, Agent Service, and Environment Service) that interact through unified interfaces, enabling independent scaling and flexible resource allocation across diverse agent-environment configurations. In our agent training deployments, MegaFlow successfully orchestrates tens of thousands of concurrent agent tasks while maintaining high system stability and achieving efficient resource utilization. By enabling such large-scale agent training, MegaFlow addresses a critical infrastructure gap in the emerging agentic AI landscape.",
    "translation": "标题：MegaFlow：面向智能体时代的大规模分布式编排系统\n\n摘要：交互式与自主人工智能系统的快速发展标志着我们正迈入智能体时代。在软件工程、计算机操作等复杂智能体任务上训练与评估智能体，不仅需要高效的模型计算能力，更依赖于能够协调海量智能体-环境交互的复杂基础设施。然而，当前尚无开源基础设施能有效支持此类复杂智能体任务的大规模训练与评估。为应对这一挑战，本文提出MegaFlow——一个面向智能体-环境工作负载的大规模分布式编排系统，可实现高效的任务调度、资源分配与细粒度任务管理。MegaFlow将智能体训练基础设施抽象为三个通过统一接口交互的独立服务（模型服务、智能体服务与环境服务），支持在不同智能体-环境配置中实现独立扩展与灵活资源分配。在实际部署的智能体训练场景中，MegaFlow成功协调了数万个并发智能体任务，在保持系统高稳定性的同时实现了高效的资源利用率。通过赋能如此大规模的智能体训练，MegaFlow填补了新兴智能体人工智能领域的关键基础设施空白。",
    "url": "https://huggingface.co/papers/2601.07526",
    "arxiv_url": "https://arxiv.org/abs/2601.07526"
  },
  {
    "title": "Boosting Latent Diffusion Models via Disentangled Representation Alignment",
    "summary": "Latent Diffusion Models (LDMs) generate high-quality images by operating in a compressed latent space, typically obtained through image tokenizers such as Variational Autoencoders (VAEs). In pursuit of a generation-friendly VAE, recent studies have explored leveraging Vision Foundation Models (VFMs) as representation alignment targets for VAEs, mirroring the approach commonly adopted for LDMs. Although this yields certain performance gains, using the same alignment target for both VAEs and LDMs overlooks their fundamentally different representational requirements. We advocate that while LDMs benefit from latents retaining high-level semantic concepts, VAEs should excel in semantic disentanglement, enabling encoding of attribute-level information in a structured way. To address this, we propose the Semantic disentangled VAE (Send-VAE), explicitly optimized for disentangled representation learning through aligning its latent space with the semantic hierarchy of pre-trained VFMs. Our approach employs a non-linear mapper network to transform VAE latents, aligning them with VFMs to bridge the gap between attribute-level disentanglement and high-level semantics, facilitating effective guidance for VAE learning. We evaluate semantic disentanglement via linear probing on attribute prediction tasks, showing strong correlation with improved generation performance. Finally, using Send-VAE, we train flow-based transformers SiTs; experiments show Send-VAE significantly speeds up training and achieves a state-of-the-art FID of 1.21 and 1.75 with and without classifier-free guidance on ImageNet 256x256.",
    "translation": "标题：通过解耦表征对齐增强潜在扩散模型\n\n摘要：潜在扩散模型通过在压缩的潜在空间中操作来生成高质量图像，该空间通常通过变分自编码器等图像标记器获得。为构建更利于生成的VAE，近期研究探索利用视觉基础模型作为VAE的表征对齐目标，这与LDM常用的对齐策略相呼应。尽管这种方法带来了一定的性能提升，但为VAE和LDM使用相同的对齐目标忽略了两者根本不同的表征需求。我们认为，LDM受益于保留高层语义概念的潜在表示，而VAE应擅长语义解耦，能够以结构化方式编码属性级信息。为此，我们提出语义解耦VAE，通过将其潜在空间与预训练VFM的语义层次对齐，显式优化解耦表征学习。该方法采用非线性映射网络转换VAE潜在表示，使其与VFM对齐，从而弥合属性级解耦与高层语义之间的鸿沟，为VAE学习提供有效指导。我们通过属性预测任务的线性探针评估语义解耦效果，证明其与生成性能提升存在强相关性。最终，基于Send-VAE训练了基于流的Transformer模型SiT；实验表明Send-VAE显著加速训练过程，在ImageNet 256×256数据集上使用/不使用无分类器引导时分别达到1.21和1.75的当前最优FID指标。",
    "url": "https://huggingface.co/papers/2601.05823",
    "arxiv_url": "https://arxiv.org/abs/2601.05823"
  },
  {
    "title": "What Users Leave Unsaid: Under-Specified Queries Limit Vision-Language Models",
    "summary": "Current vision-language benchmarks predominantly feature well-structured questions with clear, explicit prompts. However, real user queries are often informal and underspecified. Users naturally leave much unsaid, relying on images to convey context. We introduce HAERAE-Vision, a benchmark of 653 real-world visual questions from Korean online communities (0.76% survival from 86K candidates), each paired with an explicit rewrite, yielding 1,306 query variants in total. Evaluating 39 VLMs, we find that even state-of-the-art models (GPT-5, Gemini 2.5 Pro) achieve under 50% on the original queries. Crucially, query explicitation alone yields 8 to 22 point improvements, with smaller models benefiting most. We further show that even with web search, under-specified queries underperform explicit queries without search, revealing that current retrieval cannot compensate for what users leave unsaid. Our findings demonstrate that a substantial portion of VLM difficulty stem from natural query under-specification instead of model capability, highlighting a critical gap between benchmark evaluation and real-world deployment.",
    "translation": "标题：用户未尽之言：欠明确查询对视觉语言模型的限制\n\n摘要：当前的视觉语言基准测试主要包含结构清晰、提示明确的问题。然而，真实用户的查询往往是非正式且欠明确的。用户通常会省略大量信息，依赖图像来传递上下文。我们提出了HAERAE-Vision基准，该基准包含来自韩国在线社区的653个真实视觉问题（从8.6万个候选问题中筛选保留0.76%），每个问题均配有一个明确的重写版本，共生成1,306个查询变体。通过对39个视觉语言模型进行评估，我们发现即使是当前最先进的模型（如GPT-5、Gemini 2.5 Pro）在原始查询上的准确率也不足50%。关键的是，仅通过查询明确化处理，模型性能即可提升8至22个百分点，且较小模型受益最为显著。我们进一步证明，即使结合网络搜索，欠明确查询的表现仍不及未经搜索的明确查询，这表明当前检索技术无法弥补用户省略的信息。我们的研究结果表明，视觉语言模型面临的大部分困难源于自然查询的欠明确性，而非模型能力本身，这凸显了基准评估与实际应用之间存在的重要差距。",
    "url": "https://huggingface.co/papers/2601.06165",
    "arxiv_url": "https://arxiv.org/abs/2601.06165"
  },
  {
    "title": "ET-Agent: Incentivizing Effective Tool-Integrated Reasoning Agent via Behavior Calibration",
    "summary": "Large Language Models (LLMs) can extend their parameter knowledge limits by adopting the Tool-Integrated Reasoning (TIR) paradigm. However, existing LLM-based agent training framework often focuses on answers' accuracy, overlooking specific alignment for behavior patterns. Consequently, agent often exhibits ineffective actions during TIR tasks, such as redundant and insufficient tool calls. How to calibrate erroneous behavioral patterns when executing TIR tasks, thereby exploring effective trajectories, remains an open-ended problem. In this paper, we propose ET-Agent, a training framework for calibrating agent's tool-use behavior through two synergistic perspectives: Self-evolving Data Flywheel and Behavior Calibration Training. Specifically, we introduce a self-evolutionary data flywheel to generate enhanced data, used to fine-tune LLM to improve its exploration ability. Based on this, we implement an two-phases behavior-calibration training framework. It is designed to progressively calibrate erroneous behavioral patterns to optimal behaviors. Further in-depth experiments confirm the superiority of  across multiple dimensions, including correctness, efficiency, reasoning conciseness, and tool execution accuracy. Our ET-Agent framework provides practical insights for research in the TIR field. Codes can be found in https://github.com/asilverlight/ET-Agent",
    "translation": "标题：ET-Agent：通过行为校准激励高效工具集成推理智能体\n\n摘要：大型语言模型（LLM）能够通过采用工具集成推理（TIR）范式来扩展其参数知识边界。然而，现有基于LLM的智能体训练框架通常侧重于答案的准确性，而忽视了对行为模式的针对性对齐。因此，智能体在执行TIR任务时常常表现出低效行为，例如冗余或不足的工具调用。如何在校准TIR任务执行过程中的错误行为模式，从而探索有效轨迹，仍是一个开放性问题。本文提出ET-Agent，这是一个通过两个协同视角——自演进数据飞轮与行为校准训练——来校准智能体工具使用行为的训练框架。具体而言，我们引入自演进数据飞轮以生成增强数据，用于微调LLM以提升其探索能力。在此基础上，我们实现了一个两阶段的行为校准训练框架，旨在逐步将错误行为模式校准至最优行为。进一步的深入实验证实了该方法在多个维度上的优越性，包括正确性、效率、推理简洁性和工具执行准确性。我们的ET-Agent框架为TIR领域的研究提供了实践启示。代码可在https://github.com/asilverlight/ET-Agent获取。",
    "url": "https://huggingface.co/papers/2601.06860",
    "arxiv_url": "https://arxiv.org/abs/2601.06860"
  },
  {
    "title": "Dr. Zero: Self-Evolving Search Agents without Training Data",
    "summary": "As high-quality data becomes increasingly difficult to obtain, data-free self-evolution has emerged as a promising paradigm. This approach allows large language models (LLMs) to autonomously generate and solve complex problems, thereby improving their reasoning capabilities. However, multi-turn search agents struggle in data-free self-evolution due to the limited question diversity and the substantial compute required for multi-step reasoning and tool using. In this work, we introduce Dr. Zero, a framework enabling search agents to effectively self-evolve without any training data. In particular, we design a self-evolution feedback loop where a proposer generates diverse questions to train a solver initialized from the same base model. As the solver evolves, it incentivizes the proposer to produce increasingly difficult yet solvable tasks, thus establishing an automated curriculum to refine both agents. To enhance training efficiency, we also introduce hop-grouped relative policy optimization (HRPO). This method clusters structurally similar questions to construct group-level baselines, effectively minimizing the sampling overhead in evaluating each query's individual difficulty and solvability. Consequently, HRPO significantly reduces the compute requirements for solver training without compromising performance or stability. Extensive experiment results demonstrate that the data-free Dr. Zero matches or surpasses fully supervised search agents, proving that complex reasoning and search capabilities can emerge solely through self-evolution.",
    "translation": "标题：Dr. Zero：无需训练数据的自进化搜索智能体\n\n摘要：随着高质量数据日益难以获取，无数据自进化已成为一种前景广阔的研究范式。该方法使大型语言模型能够自主生成并解决复杂问题，从而提升其推理能力。然而，多轮搜索智能体在无数据自进化中面临挑战，主要受限于问题多样性不足以及多步推理与工具调用所需的大量计算资源。本研究提出Dr. Zero框架，使搜索智能体能够在完全无需训练数据的情况下实现高效自进化。具体而言，我们设计了自进化反馈循环机制：由提议者生成多样化问题用于训练基于同一基础模型初始化的求解器。随着求解器能力的进化，它会激励提议者产生难度递增且可解决的任务，从而形成自动化课程学习体系以同步优化两个智能体。为提升训练效率，我们进一步提出跳步分组相对策略优化方法。该方法通过聚类结构相似的问题构建组级基线，有效降低了评估单个查询难度与可解性所需的采样开销。因此，HRPO在保持性能与稳定性的同时，显著减少了求解器训练的计算需求。大量实验结果表明，无数据训练的Dr. Zero在性能上达到甚至超越了全监督训练的搜索智能体，证明了复杂推理与搜索能力可仅通过自进化机制实现涌现。",
    "url": "https://huggingface.co/papers/2601.07055",
    "arxiv_url": "https://arxiv.org/abs/2601.07055"
  },
  {
    "title": "Forest Before Trees: Latent Superposition for Efficient Visual Reasoning",
    "summary": "While Chain-of-Thought empowers Large Vision-Language Models with multi-step reasoning, explicit textual rationales suffer from an information bandwidth bottleneck, where continuous visual details are discarded during discrete tokenization. Recent latent reasoning methods attempt to address this challenge, but often fall prey to premature semantic collapse due to rigid autoregressive objectives. In this paper, we propose Laser, a novel paradigm that reformulates visual deduction via Dynamic Windowed Alignment Learning (DWAL). Instead of forcing a point-wise prediction, Laser aligns the latent state with a dynamic validity window of future semantics. This mechanism enforces a \"Forest-before-Trees\" cognitive hierarchy, enabling the model to maintain a probabilistic superposition of global features before narrowing down to local details. Crucially, Laser maintains interpretability via decodable trajectories while stabilizing unconstrained learning via Self-Refined Superposition. Extensive experiments on 6 benchmarks demonstrate that Laser achieves state-of-the-art performance among latent reasoning methods, surpassing the strong baseline Monet by 5.03% on average. Notably, it achieves these gains with extreme efficiency, reducing inference tokens by more than 97%, while demonstrating robust generalization to out-of-distribution domains.",
    "translation": "标题：先见森林后见树：基于潜在叠加的高效视觉推理方法\n\n摘要：尽管思维链技术赋予大型视觉语言模型多步推理能力，但显式文本推理过程存在信息带宽瓶颈——连续的视觉细节在离散化标记处理过程中被丢失。近期潜在推理方法试图解决这一挑战，却常因僵化的自回归目标而陷入过早的语义坍缩。本文提出Laser新范式，通过动态窗口对齐学习重构视觉推理过程。该方法摒弃逐点预测的强制约束，使潜在状态与未来语义的动态有效窗口对齐。这种机制构建了“先森林后树木”的认知层级，使模型在聚焦局部细节前能够保持全局特征的概率叠加态。关键创新在于，Laser通过可解码轨迹保持可解释性，同时借助自优化叠加机制稳定无约束学习过程。在6个基准测试上的实验表明，Laser在潜在推理方法中达到最先进性能，较强势基线Monet平均提升5.03%。值得注意的是，该方法在实现性能增益的同时具备极高效率，推理标记量减少97%以上，并展现出对分布外领域强大的泛化能力。",
    "url": "https://huggingface.co/papers/2601.06803",
    "arxiv_url": "https://arxiv.org/abs/2601.06803"
  },
  {
    "title": "TourPlanner: A Competitive Consensus Framework with Constraint-Gated Reinforcement Learning for Travel Planning",
    "summary": "Travel planning is a sophisticated decision-making process that requires synthesizing multifaceted information to construct itineraries. However, existing travel planning approaches face several challenges: (1) Pruning candidate points of interest (POIs) while maintaining a high recall rate; (2) A single reasoning path restricts the exploration capability within the feasible solution space for travel planning; (3) Simultaneously optimizing hard constraints and soft constraints remains a significant difficulty. To address these challenges, we propose TourPlanner, a comprehensive framework featuring multi-path reasoning and constraint-gated reinforcement learning. Specifically, we first introduce a Personalized Recall and Spatial Optimization (PReSO) workflow to construct spatially-aware candidate POIs' set. Subsequently, we propose Competitive consensus Chain-of-Thought (CCoT), a multi-path reasoning paradigm that improves the ability of exploring the feasible solution space. To further refine the plan, we integrate a sigmoid-based gating mechanism into the reinforcement learning stage, which dynamically prioritizes soft-constraint satisfaction only after hard constraints are met. Experimental results on travel planning benchmarks demonstrate that TourPlanner achieves state-of-the-art performance, significantly surpassing existing methods in both feasibility and user-preference alignment.",
    "translation": "标题：TourPlanner：一种基于约束门控强化学习的竞争共识式旅行规划框架\n\n摘要：旅行规划是一个复杂的决策过程，需要综合多维度信息以构建行程方案。然而，现有旅行规划方法面临多重挑战：（1）在保持高召回率的同时筛选候选兴趣点；（2）单一推理路径限制了旅行规划在可行解空间中的探索能力；（3）同时优化硬约束与软约束仍是重大难题。为应对这些挑战，我们提出TourPlanner——一个融合多路径推理与约束门控强化学习的综合框架。具体而言，我们首先设计个性化召回与空间优化流程，构建具有空间感知的候选兴趣点集合。随后提出竞争共识思维链多路径推理范式，增强对可行解空间的探索能力。为进一步优化规划方案，我们在强化学习阶段引入基于Sigmoid函数的门控机制，该机制仅在硬约束满足后动态调整对软约束的优先满足程度。在旅行规划基准测试上的实验结果表明，TourPlanner实现了最先进的性能，在方案可行性与用户偏好契合度方面均显著超越现有方法。",
    "url": "https://huggingface.co/papers/2601.04698",
    "arxiv_url": "https://arxiv.org/abs/2601.04698"
  },
  {
    "title": "OpenTinker: Separating Concerns in Agentic Reinforcement Learning",
    "summary": "We introduce OpenTinker, an infrastructure for reinforcement learning (RL) of large language model (LLM) agents built around a separation of concerns across algorithm design, execution, and agent-environment interaction. Rather than relying on monolithic, end-to-end RL pipelines, OpenTinker decomposes agentic learning systems into lightweight, composable components with clearly defined abstraction boundaries. Users specify agents, environments, and interaction protocols, while inference and training are delegated to a managed execution runtime. OpenTinker introduces a centralized scheduler for managing training and inference workloads, including LoRA-based and full-parameter RL, supervised fine-tuning, and inference, over shared resources. We further discuss design principles for extending OpenTinker to multi-agent training. Finally, we present a set of RL use cases that demonstrate the effectiveness of the framework in practical agentic learning scenarios.",
    "translation": "标题：OpenTinker：智能体强化学习中的关注点分离\n\n摘要：本文介绍OpenTinker，这是一个围绕算法设计、执行以及智能体-环境交互的关注点分离而构建的大型语言模型（LLM）智能体强化学习（RL）基础设施。OpenTinker摒弃了单一、端到端的强化学习流程，将智能体学习系统分解为具有明确定义抽象边界的轻量级可组合组件。用户负责定义智能体、环境及交互协议，而推理与训练任务则交由托管执行运行时处理。OpenTinker引入了一个集中式调度器，用于在共享资源上管理训练与推理工作负载，包括基于LoRA和全参数的强化学习、监督微调以及推理任务。我们进一步探讨了将OpenTinker扩展至多智能体训练的设计原则。最后，我们通过一系列强化学习应用案例，展示了该框架在实际智能体学习场景中的有效性。",
    "url": "https://huggingface.co/papers/2601.07376",
    "arxiv_url": "https://arxiv.org/abs/2601.07376"
  },
  {
    "title": "Are LLM Decisions Faithful to Verbal Confidence?",
    "summary": "Large Language Models (LLMs) can produce surprisingly sophisticated estimates of their own uncertainty. However, it remains unclear to what extent this expressed confidence is tied to the reasoning, knowledge, or decision making of the model. To test this, we introduce RiskEval: a framework designed to evaluate whether models adjust their abstention policies in response to varying error penalties. Our evaluation of several frontier models reveals a critical dissociation: models are neither cost-aware when articulating their verbal confidence, nor strategically responsive when deciding whether to engage or abstain under high-penalty conditions. Even when extreme penalties render frequent abstention the mathematically optimal strategy, models almost never abstain, resulting in utility collapse. This indicates that calibrated verbal confidence scores may not be sufficient to create trustworthy and interpretable AI systems, as current models lack the strategic agency to convert uncertainty signals into optimal and risk-sensitive decisions.",
    "translation": "标题：大语言模型的决策是否忠实于其口头置信度？\n\n摘要：大语言模型（LLMs）能够生成令人惊讶的、关于其自身不确定性的复杂估计。然而，这种表达出的置信度在多大程度上与模型的推理、知识或决策过程相关联，目前尚不清楚。为了检验这一点，我们引入了RiskEval框架：该框架旨在评估模型是否会根据不同的错误惩罚调整其弃权策略。我们对多个前沿模型的评估揭示了一个关键的分裂现象：模型在表达其口头置信度时并不具备成本意识，在高惩罚条件下决定是否参与或弃权时也缺乏策略性响应。即使极端惩罚使得频繁弃权成为数学上的最优策略，模型也几乎从不选择弃权，从而导致效用崩溃。这表明，经过校准的口头置信度分数可能不足以构建可信且可解释的人工智能系统，因为当前模型缺乏将不确定性信号转化为最优且风险敏感决策的策略能动性。",
    "url": "https://huggingface.co/papers/2601.07767",
    "arxiv_url": "https://arxiv.org/abs/2601.07767"
  },
  {
    "title": "Structured Episodic Event Memory",
    "summary": "Current approaches to memory in Large Language Models (LLMs) predominantly rely on static Retrieval-Augmented Generation (RAG), which often results in scattered retrieval and fails to capture the structural dependencies required for complex reasoning. For autonomous agents, these passive and flat architectures lack the cognitive organization necessary to model the dynamic and associative nature of long-term interaction. To address this, we propose Structured Episodic Event Memory (SEEM), a hierarchical framework that synergizes a graph memory layer for relational facts with a dynamic episodic memory layer for narrative progression. Grounded in cognitive frame theory, SEEM transforms interaction streams into structured Episodic Event Frames (EEFs) anchored by precise provenance pointers. Furthermore, we introduce an agentic associative fusion and Reverse Provenance Expansion (RPE) mechanism to reconstruct coherent narrative contexts from fragmented evidence. Experimental results on the LoCoMo and LongMemEval benchmarks demonstrate that SEEM significantly outperforms baselines, enabling agents to maintain superior narrative coherence and logical consistency.",
    "translation": "标题：结构化情景事件记忆\n\n摘要：当前大型语言模型（LLM）中的记忆方法主要依赖于静态检索增强生成（RAG），这种方式常导致检索结果分散，难以捕捉复杂推理所需的结构化依赖关系。对于自主智能体而言，这种被动且扁平化的架构缺乏必要的认知组织能力，无法有效建模长期交互中动态且关联的特性。为此，我们提出结构化情景事件记忆（SEEM）——一种分层框架，通过将关系性事实的图记忆层与叙事推进的动态情景记忆层相协同，构建起融合的记忆体系。该框架基于认知框架理论，将交互流转化为以前溯指针为锚点的结构化情景事件框架（EEF）。此外，我们引入了自主关联融合机制与反向溯源扩展（RPE）方法，以从碎片化证据中重建连贯的叙事语境。在LoCoMo与LongMemEval基准测试上的实验结果表明，SEEM显著优于现有基线方法，能够使智能体保持更优的叙事连贯性与逻辑一致性。",
    "url": "https://huggingface.co/papers/2601.06411",
    "arxiv_url": "https://arxiv.org/abs/2601.06411"
  },
  {
    "title": "e5-omni: Explicit Cross-modal Alignment for Omni-modal Embeddings",
    "summary": "Modern information systems often involve different types of items, e.g., a text query, an image, a video clip, or an audio segment. This motivates omni-modal embedding models that map heterogeneous modalities into a shared space for direct comparison. However, most recent omni-modal embeddings still rely heavily on implicit alignment inherited from pretrained vision-language model (VLM) backbones. In practice, this causes three common issues: (i) similarity logits have modality-dependent sharpness, so scores are not on a consistent scale; (ii) in-batch negatives become less effective over time because mixed-modality batches create an imbalanced hardness distribution; as a result, many negatives quickly become trivial and contribute little gradient; and (iii) embeddings across modalities show mismatched first- and second-order statistics, which makes rankings less stable. To tackle these problems, we propose e5-omni, a lightweight explicit alignment recipe that adapts off-the-shelf VLMs into robust omni-modal embedding models. e5-omni combines three simple components: (1) modality-aware temperature calibration to align similarity scales, (2) a controllable negative curriculum with debiasing to focus on confusing negatives while reducing the impact of false negatives, and (3) batch whitening with covariance regularization to better match cross-modal geometry in the shared embedding space. Experiments on MMEB-V2 and AudioCaps show consistent gains over strong bi-modal and omni-modal baselines, and the same recipe also transfers well to other VLM backbones. We release our model checkpoint at https://huggingface.co/Haon-Chen/e5-omni-7B.",
    "translation": "标题：e5-omni：面向全模态嵌入的显式跨模态对齐方法\n\n摘要：现代信息系统常涉及多种类型的项目，例如文本查询、图像、视频片段或音频片段。这推动了全模态嵌入模型的发展，旨在将异构模态映射到共享空间中以进行直接比较。然而，当前大多数全模态嵌入方法仍严重依赖于从预训练视觉语言模型（VLM）骨干中继承的隐式对齐机制。实践中，这导致三个常见问题：（i）相似度对数具有模态依赖的锐度，使得评分缺乏一致尺度；（ii）混合模态批次造成不平衡的困难度分布，导致批内负样本随时间推移效果减弱，许多负样本迅速变得无关紧要，对梯度贡献甚微；（iii）跨模态嵌入呈现不匹配的一阶与二阶统计量，降低了排序稳定性。为解决这些问题，我们提出e5-omni——一种轻量级的显式对齐方案，可将现成的VLM适配为鲁棒的全模态嵌入模型。e5-omni整合了三个简单组件：（1）通过模态感知温度校准对齐相似度尺度；（2）采用带去偏控制的可控负样本课程学习，聚焦于混淆性负样本同时降低假负样本的影响；（3）结合协方差正则化的批白化处理，以更好地匹配共享嵌入空间中的跨模态几何结构。在MMEB-V2和AudioCaps数据集上的实验表明，该方法相较于强双模态与全模态基线模型均取得稳定提升，且该方案能良好迁移至其他VLM骨干网络。模型检查点已发布于https://huggingface.co/Haon-Chen/e5-omni-7B。",
    "url": "https://huggingface.co/papers/2601.03666",
    "arxiv_url": "https://arxiv.org/abs/2601.03666"
  },
  {
    "title": "\"TODO: Fix the Mess Gemini Created\": Towards Understanding GenAI-Induced Self-Admitted Technical Debt",
    "summary": "As large language models (LLMs) such as ChatGPT, Copilot, Claude, and Gemini become integrated into software development workflows, developers increasingly leave traces of AI involvement in their code comments. Among these, some comments explicitly acknowledge both the use of generative AI and the presence of technical shortcomings. Analyzing 6,540 LLM-referencing code comments from public Python and JavaScript-based GitHub repositories (November 2022-July 2025), we identified 81 that also self-admit technical debt(SATD). Developers most often describe postponed testing, incomplete adaptation, and limited understanding of AI-generated code, suggesting that AI assistance affects both when and why technical debt emerges. We term GenAI-Induced Self-admitted Technical debt (GIST) as a proposed conceptual lens to describe recurring cases where developers incorporate AI-generated code while explicitly expressing uncertainty about its behavior or correctness.",
    "translation": "标题：\"TODO：修复Gemini制造的混乱\"：理解生成式人工智能引发的自认技术债务\n\n摘要：随着ChatGPT、Copilot、Claude和Gemini等大型语言模型（LLMs）被集成到软件开发工作流程中，开发者越来越多地在代码注释中留下人工智能参与的痕迹。其中，部分注释明确承认了生成式人工智能的使用以及技术缺陷的存在。通过分析来自公共Python和JavaScript的GitHub仓库（2022年11月至2025年7月）的6,540条涉及LLM的代码注释，我们识别出其中81条同时自认了技术债务（SATD）。开发者最常描述的是推迟测试、不完整的适配以及对AI生成代码的有限理解，这表明人工智能辅助既影响了技术债务出现的时间，也影响了其产生的原因。我们提出\"生成式人工智能引发的自认技术债务（GIST）\"这一概念框架，用以描述开发者在使用AI生成代码时，同时明确表达对其行为或正确性存在不确定性的重复性案例。",
    "url": "https://huggingface.co/papers/2601.07786",
    "arxiv_url": "https://arxiv.org/abs/2601.07786"
  },
  {
    "title": "ShowUI-Aloha: Human-Taught GUI Agent",
    "summary": "Graphical User Interfaces (GUIs) are central to human-computer interaction, yet automating complex GUI tasks remains a major challenge for autonomous agents, largely due to a lack of scalable, high-quality training data. While recordings of human demonstrations offer a rich data source, they are typically long, unstructured, and lack annotations, making them difficult for agents to learn from.To address this, we introduce ShowUI-Aloha, a comprehensive pipeline that transforms unstructured, in-the-wild human screen recordings from desktop environments into structured, actionable tasks. Our framework includes four key components: A recorder that captures screen video along with precise user interactions like mouse clicks, keystrokes, and scrolls. A learner that semantically interprets these raw interactions and the surrounding visual context, translating them into descriptive natural language captions. A planner that reads the parsed demonstrations, maintains task states, and dynamically formulates the next high-level action plan based on contextual reasoning. An executor that faithfully carries out these action plans at the OS level, performing precise clicks, drags, text inputs, and window operations with safety checks and real-time feedback. Together, these components provide a scalable solution for collecting and parsing real-world human data, demonstrating a viable path toward building general-purpose GUI agents that can learn effectively from simply observing humans.",
    "translation": "标题：ShowUI-Aloha：人类示范驱动的图形用户界面智能体\n\n摘要：图形用户界面（GUI）是人机交互的核心，但自动化复杂GUI任务仍是自主智能体面临的主要挑战，其根本原因在于缺乏可扩展的高质量训练数据。尽管人类操作录像是丰富的数据来源，但这些记录通常冗长、非结构化且缺乏标注，导致智能体难以从中有效学习。为解决这一问题，我们提出了ShowUI-Aloha——一个完整的处理流程，能够将桌面环境中非结构化、真实场景的人类屏幕录像转化为结构化、可执行的任务序列。该框架包含四个核心组件：记录器负责捕获屏幕视频及精确的用户交互（如鼠标点击、键盘输入和滚动操作）；学习器通过语义解析原始交互行为与视觉上下文，将其转化为描述性自然语言标注；规划器读取解析后的示范数据，维护任务状态，并基于上下文推理动态生成下一步高层动作计划；执行器则在操作系统层面忠实执行这些动作计划，通过安全检查与实时反馈机制实现精确点击、拖拽、文本输入及窗口操作。这些组件共同构成了采集与解析真实人类操作数据的可扩展解决方案，为构建能够通过观察人类行为进行高效学习的通用GUI智能体提供了可行路径。",
    "url": "https://huggingface.co/papers/2601.07181",
    "arxiv_url": "https://arxiv.org/abs/2601.07181"
  },
  {
    "title": "Codified Foreshadowing-Payoff Text Generation",
    "summary": "Foreshadowing and payoff are ubiquitous narrative devices through which authors introduce commitments early in a story and resolve them through concrete, observable outcomes. However, despite advances in story generation, large language models (LLMs) frequently fail to bridge these long-range narrative dependencies, often leaving \"Chekhov's guns\" unfired even when the necessary context is present. Existing evaluations largely overlook this structural failure, focusing on surface-level coherence rather than the logical fulfillment of narrative setups. In this paper, we introduce Codified Foreshadowing-Payoff Generation (CFPG), a novel framework that reframes narrative quality through the lens of payoff realization. Recognizing that LLMs struggle to intuitively grasp the \"triggering mechanism\" of a foreshadowed event, CFPG transforms narrative continuity into a set of executable causal predicates. By mining and encoding Foreshadow-Trigger-Payoff triples from the BookSum corpus, we provide structured supervision that ensures foreshadowed commitments are not only mentioned but also temporally and logically fulfilled. Experiments demonstrate that CFPG significantly outperforms standard prompting baselines in payoff accuracy and narrative alignment. Our findings suggest that explicitly codifying narrative mechanics is essential for moving LLMs from surface-level fluency to genuine narrative competence.",
    "translation": "标题：编码化伏笔-照应文本生成\n\n摘要：伏笔与照应是普遍存在的叙事手法，作者通过其在故事早期埋设承诺，并通过具体可观测的结果予以解决。然而，尽管故事生成技术已取得进展，大语言模型在连接此类长程叙事依赖时仍常显不足，即便在必要语境存在的情况下，也往往让“契科夫的枪”未能击发。现有评估方法大多忽视了这种结构性缺陷，侧重于表层连贯性而非叙事铺垫的逻辑实现。本文提出编码化伏笔-照应生成框架，该创新框架通过照应实现的视角重构叙事质量评估体系。针对大语言模型难以直观把握伏笔事件“触发机制”的问题，本框架将叙事连续性转化为一组可执行的因果谓词。通过从BookSum语料库中挖掘并编码“伏笔-触发-照应”三元组，我们提供了结构化监督机制，确保伏笔承诺不仅被提及，更能在时间与逻辑层面得到兑现。实验表明，该框架在照应准确度与叙事一致性方面显著优于标准提示基线方法。我们的研究结果表明，对叙事机制进行显式编码对于推动大语言模型从表层流畅性迈向真正的叙事能力至关重要。",
    "url": "https://huggingface.co/papers/2601.07033",
    "arxiv_url": "https://arxiv.org/abs/2601.07033"
  },
  {
    "title": "Sci-Reasoning: A Dataset Decoding AI Innovation Patterns",
    "summary": "While AI innovation accelerates rapidly, the intellectual process behind breakthroughs -- how researchers identify gaps, synthesize prior work, and generate insights -- remains poorly understood. The lack of structured data on scientific reasoning hinders systematic analysis and development of AI research agents. We introduce Sci-Reasoning, the first dataset capturing the intellectual synthesis behind high-quality AI research. Using community-validated quality signals and an LLM-accelerated, human-verified pipeline, we trace Oral and Spotlight papers across NeurIPS, ICML, and ICLR (2023-2025) to its key predecessors, articulating specific reasoning links in a structured format. Our analysis identifies 15 distinct thinking patterns, with three dominant strategies accounting for 52.7%: Gap-Driven Reframing (24.2%), Cross-Domain Synthesis (18.0%), and Representation Shift (10.5%). The most powerful innovation recipes combine multiple patterns: Gap-Driven Reframing + Representation Shift, Cross-Domain Synthesis + Representation Shift, and Gap-Driven Reframing + Cross-Domain Synthesis. This dataset enables quantitative studies of scientific progress and provides structured reasoning trajectories for training the next generation AI research agents.",
    "translation": "标题：Sci-Reasoning：解码人工智能创新模式的数据集\n\n摘要：尽管人工智能创新加速发展，但突破背后的智力过程——研究者如何识别研究空白、综合前人工作并产生洞见——仍鲜为人知。科学推理结构化数据的缺乏，阻碍了对人工智能研究智能体的系统性分析与开发。本文介绍 Sci-Reasoning，这是首个捕捉高质量人工智能研究背后智力综合过程的数据集。通过采用社区验证的质量信号以及基于大语言模型加速、人工验证的流程，我们追踪了 NeurIPS、ICML 与 ICLR（2023–2025）中 Oral 与 Spotlight 论文的关键前驱工作，并以结构化形式阐明其具体的推理关联。我们的分析识别出 15 种不同的思维模式，其中三种主导策略占比达 52.7%：空白驱动重构（24.2%）、跨领域综合（18.0%）与表征转换（10.5%）。最具影响力的创新路径往往融合多种模式：空白驱动重构 + 表征转换、跨领域综合 + 表征转换，以及空白驱动重构 + 跨领域综合。本数据集支持对科学进展的量化研究，并为训练下一代人工智能研究智能体提供了结构化的推理轨迹。",
    "url": "https://huggingface.co/papers/2601.04577",
    "arxiv_url": "https://arxiv.org/abs/2601.04577"
  },
  {
    "title": "How Do Large Language Models Learn Concepts During Continual Pre-Training?",
    "summary": "Human beings primarily understand the world through concepts (e.g., dog), abstract mental representations that structure perception, reasoning, and learning. However, how large language models (LLMs) acquire, retain, and forget such concepts during continual pretraining remains poorly understood. In this work, we study how individual concepts are acquired and forgotten, as well as how multiple concepts interact through interference and synergy. We link these behavioral dynamics to LLMs' internal Concept Circuits, computational subgraphs associated with specific concepts, and incorporate Graph Metrics to characterize circuit structure. Our analysis reveals: (1) LLMs concept circuits provide a non-trivial, statistically significant signal of concept learning and forgetting; (2) Concept circuits exhibit a stage-wise temporal pattern during continual pretraining, with an early increase followed by gradual decrease and stabilization; (3) concepts with larger learning gains tend to exhibit greater forgetting under subsequent training; (4) semantically similar concepts induce stronger interference than weakly related ones; (5) conceptual knowledge differs in their transferability, with some significantly facilitating the learning of others. Together, our findings offer a circuit-level view of concept learning dynamics and inform the design of more interpretable and robust concept-aware training strategies for LLMs.",
    "translation": "标题：大规模语言模型在持续预训练中如何学习概念？\n\n摘要：人类主要通过概念（例如“狗”）来理解世界，这些抽象的心理表征构建了感知、推理和学习的基础。然而，大规模语言模型（LLMs）在持续预训练过程中如何获取、保留和遗忘此类概念，目前仍缺乏深入理解。本研究探讨了单个概念如何被习得与遗忘，以及多个概念之间如何通过干扰和协同作用相互影响。我们将这些行为动态与LLMs内部的“概念回路”（即与特定概念相关的计算子图）联系起来，并引入图度量指标来刻画回路结构。分析结果表明：（1）LLMs的概念回路能够提供具有统计显著性的概念学习与遗忘信号；（2）在持续预训练过程中，概念回路呈现阶段性时序模式，表现为早期增强、随后逐渐减弱并趋于稳定；（3）学习增益较大的概念在后续训练中往往表现出更强的遗忘效应；（4）语义相近的概念比弱相关概念引发更显著的干扰；（5）不同概念知识的可迁移性存在差异，部分概念能显著促进其他概念的学习。综合而言，本研究从回路层面揭示了概念学习的动态机制，为设计更具可解释性和鲁棒性的概念感知训练策略提供了理论依据。",
    "url": "https://huggingface.co/papers/2601.03570",
    "arxiv_url": "https://arxiv.org/abs/2601.03570"
  },
  {
    "title": "On the Non-decoupling of Supervised Fine-tuning and Reinforcement Learning in Post-training",
    "summary": "Post-training of large language models routinely interleaves supervised fine-tuning (SFT) with reinforcement learning (RL). These two methods have different objectives: SFT minimizes the cross-entropy loss between model outputs and expert responses, while RL maximizes reward signals derived from human preferences or rule-based verifiers. Modern reasoning models have widely adopted the practice of alternating SFT and RL training. However, there is no theoretical account of whether they can be decoupled. We prove that decoupling is impossible in either order: (1) SFT-then-RL coupling: RL increases SFT loss under SFT optimality and (2) RL-then-SFT coupling: SFT lowers the reward achieved by RL. Experiments on Qwen3-0.6B confirm the predicted degradation, verifying that SFT and RL cannot be separated without loss of prior performance in the post-training",
    "translation": "标题：论后训练中监督微调与强化学习的不可解耦性\n\n摘要：大语言模型的后训练通常交替进行监督微调与强化学习。这两种方法具有不同目标：监督微调旨在最小化模型输出与专家响应之间的交叉熵损失，而强化学习则致力于最大化基于人类偏好或规则验证器生成的奖励信号。现代推理模型已广泛采用交替进行监督微调与强化学习的训练范式。然而，关于二者能否解耦的理论研究尚属空白。我们证明两种顺序的解耦均不可行：（1）先监督微调后强化学习的耦合：在监督微调最优性条件下，强化学习会增大监督微调损失；（2）先强化学习后监督微调的耦合：监督微调会降低强化学习已获得的奖励。基于Qwen3-0.6B的实验证实了理论预测的性能退化现象，验证了在后训练过程中若分离监督微调与强化学习，则无法保持先前已达成的性能水平。",
    "url": "https://huggingface.co/papers/2601.07389",
    "arxiv_url": "https://arxiv.org/abs/2601.07389"
  },
  {
    "title": "Can Textual Reasoning Improve the Performance of MLLMs on Fine-grained Visual Classification?",
    "summary": "Multi-modal large language models (MLLMs) exhibit strong general-purpose capabilities, yet still struggle on Fine-Grained Visual Classification (FGVC), a core perception task that requires subtle visual discrimination and is crucial for many real-world applications. A widely adopted strategy for boosting performance on challenging tasks such as math and coding is Chain-of-Thought (CoT) reasoning. However, several prior works have reported that CoT can actually harm performance on visual perception tasks. These studies, though, examine the issue from relatively narrow angles and leave open why CoT degrades perception-heavy performance. We systematically re-examine the role of CoT in FGVC through the lenses of zero-shot evaluation and multiple training paradigms. Across these settings, we uncover a central paradox: the degradation induced by CoT is largely driven by the reasoning length, in which longer textual reasoning consistently lowers classification accuracy. We term this phenomenon the ``Cost of Thinking''. Building on this finding, we make two key contributions: (1) \\alg, a simple and general plug-and-play normalization method for multi-reward optimization that balances heterogeneous reward signals, and (2) ReFine-RFT, a framework that combines ensemble rewards with \\alg to constrain reasoning length while providing dense accuracy-oriented feedback. Extensive experiments demonstrate the effectiveness of our findings and the proposed ReFine-RFT, achieving state-of-the-art performance across FGVC benchmarks. Code and models are available at https://github.com/jiezhu23/ReFine-RFT{Project Link}.",
    "translation": "标题：文本推理能否提升多模态大语言模型在细粒度视觉分类中的性能？\n\n摘要：多模态大语言模型（MLLMs）展现出强大的通用能力，但在细粒度视觉分类（FGVC）这一核心感知任务上仍面临挑战。FGVC要求细微的视觉辨别能力，对众多现实应用至关重要。在数学、编程等复杂任务中，提升性能的常用策略是思维链（CoT）推理。然而，先前多项研究表明，CoT实际上可能损害视觉感知任务的性能。这些研究虽从相对局限的视角探讨了该问题，但未阐明CoT为何会降低以感知为主导的任务性能。本文通过零样本评估与多种训练范式的系统性再检验，深入探究了CoT在FGVC中的作用。在不同实验设置中，我们揭示了一个核心矛盾：CoT导致的性能下降主要受推理长度驱动，即更长的文本推理会持续降低分类准确率。我们将此现象称为“思考代价”。基于这一发现，我们做出两项关键贡献：（1）提出\\alg方法——一种简单通用的即插即用式多奖励优化归一化方法，可平衡异构奖励信号；（2）提出ReFine-RFT框架，该框架结合集成奖励与\\alg方法，在约束推理长度的同时提供密集的、以准确率为导向的反馈。大量实验验证了我们发现的可靠性及所提ReFine-RFT框架的有效性，该框架在多个FGVC基准测试中取得了最先进的性能。代码与模型已开源：https://github.com/jiezhu23/ReFine-RFT{项目链接}。",
    "url": "https://huggingface.co/papers/2601.06993",
    "arxiv_url": "https://arxiv.org/abs/2601.06993"
  },
  {
    "title": "RealMem: Benchmarking LLMs in Real-World Memory-Driven Interaction",
    "summary": "As Large Language Models (LLMs) evolve from static dialogue interfaces to autonomous general agents, effective memory is paramount to ensuring long-term consistency. However, existing benchmarks primarily focus on casual conversation or task-oriented dialogue, failing to capture **\"long-term project-oriented\"** interactions where agents must track evolving goals.\n  To bridge this gap, we introduce **RealMem**, the first benchmark grounded in realistic project scenarios. RealMem comprises over 2,000 cross-session dialogues across eleven scenarios, utilizing natural user queries for evaluation.\n  We propose a synthesis pipeline that integrates Project Foundation Construction, Multi-Agent Dialogue Generation, and Memory and Schedule Management to simulate the dynamic evolution of memory. Experiments reveal that current memory systems face significant challenges in managing the long-term project states and dynamic context dependencies inherent in real-world projects.\n  Our code and datasets are available at [https://github.com/AvatarMemory/RealMemBench](https://github.com/AvatarMemory/RealMemBench).",
    "translation": "标题：RealMem：面向现实世界记忆驱动交互的大语言模型基准测试\n\n摘要：随着大语言模型从静态对话接口演变为自主通用智能体，有效的记忆机制对于保障长期行为一致性至关重要。然而，现有基准测试主要集中于日常对话或任务导向型对话，未能涵盖智能体必须追踪动态目标的**\"长期项目导向型\"**交互场景。为填补这一空白，我们提出了首个基于真实项目场景的基准测试框架**RealMem**。该框架涵盖十一种项目场景下的2000余组跨会话对话，采用自然用户查询进行评估。我们设计了一套融合项目基础构建、多智能体对话生成以及记忆与进度管理的综合流程，以模拟记忆的动态演化过程。实验表明，现有记忆系统在处理现实项目固有的长期状态维护与动态上下文依赖方面面临显著挑战。相关代码与数据集已在[https://github.com/AvatarMemory/RealMemBench](https://github.com/AvatarMemory/RealMemBench)开源。",
    "url": "https://huggingface.co/papers/2601.06966",
    "arxiv_url": "https://arxiv.org/abs/2601.06966"
  },
  {
    "title": "SketchJudge: A Diagnostic Benchmark for Grading Hand-drawn Diagrams with Multimodal Large Language Models",
    "summary": "While Multimodal Large Language Models (MLLMs) have achieved remarkable progress in visual understanding, they often struggle when faced with the unstructured and ambiguous nature of human-generated sketches. This limitation is particularly pronounced in the underexplored task of visual grading, where models should not only solve a problem but also diagnose errors in hand-drawn diagrams. Such diagnostic capabilities depend on complex structural, semantic, and metacognitive reasoning. To bridge this gap, we introduce SketchJudge, a novel benchmark tailored for evaluating MLLMs as graders of hand-drawn STEM diagrams. SketchJudge encompasses 1,015 hand-drawn student responses across four domains: geometry, physics, charts, and flowcharts, featuring diverse stylistic variations and distinct error types. Evaluations on SketchJudge demonstrate that even advanced MLLMs lag significantly behind humans, validating the benchmark's effectiveness in exposing the fragility of current vision-language alignment in symbolic and noisy contexts. All data, code, and evaluation scripts are publicly available at https://github.com/yuhangsu82/SketchJudge.",
    "translation": "标题：SketchJudge：基于多模态大语言模型的手绘图表分级诊断基准\n\n摘要：尽管多模态大语言模型在视觉理解方面取得了显著进展，但在处理人类手绘草图中非结构化与模糊性特征时仍面临挑战。这一局限在视觉分级这一尚未充分探索的任务中尤为突出，该任务要求模型不仅解决问题，还需诊断手绘图表中的错误。此类诊断能力依赖于复杂的结构、语义及元认知推理。为填补这一空白，我们提出了SketchJudge——一个专门用于评估多模态大语言模型作为手绘STEM图表分级器的新型基准。SketchJudge涵盖几何、物理、图表和流程图四大领域共1,015份手绘学生作答样本，包含多样化的风格变体与典型错误类型。基于SketchJudge的评估表明，即使先进的多模态大语言模型仍显著落后于人类水平，验证了该基准在揭示符号化及噪声环境下当前视觉-语言对齐机制脆弱性方面的有效性。所有数据、代码与评估脚本已公开于https://github.com/yuhangsu82/SketchJudge。",
    "url": "https://huggingface.co/papers/2601.06944",
    "arxiv_url": "https://arxiv.org/abs/2601.06944"
  },
  {
    "title": "Artificial Entanglement in the Fine-Tuning of Large Language Models",
    "summary": "Large language models (LLMs) can be adapted to new tasks using parameter-efficient fine-tuning (PEFT) methods that modify only a small number of trainable parameters, often through low-rank updates. In this work, we adopt a quantum-information-inspired perspective to understand their effectiveness. From this perspective, low-rank parameterizations naturally correspond to low-dimensional Matrix Product States (MPS) representations, which enable entanglement-based characterizations of parameter structure. Thereby, we term and measure \"Artificial Entanglement\", defined as the entanglement entropy of the parameters in artificial neural networks (in particular the LLMs). We first study the representative low-rank adaptation (LoRA) PEFT method, alongside full fine-tuning (FFT), using LLaMA models at the 1B and 8B scales trained on the Tulu3 and OpenThoughts3 datasets, and uncover: (i) Internal artificial entanglement in the updates of query and value projection matrices in LoRA follows a volume law with a central suppression (termed as the \"Entanglement Valley\"), which is sensitive to hyper-parameters and is distinct from that in FFT; (ii) External artificial entanglement in attention matrices, corresponding to token-token correlations in representation space, follows an area law with logarithmic corrections and remains robust to LoRA hyper-parameters and training steps. Drawing a parallel to the No-Hair Theorem in black hole physics, we propose that although LoRA and FFT induce distinct internal entanglement signatures, such differences do not manifest in the attention outputs, suggesting a \"no-hair\" property that results in the effectiveness of low rank updates. We further provide theoretical support based on random matrix theory, and extend our analysis to an MPS Adaptation PEFT method, which exhibits qualitatively similar behaviors.",
    "translation": "标题：大语言模型微调中的人工纠缠现象研究\n\n摘要：大语言模型（LLMs）可通过仅修改少量可训练参数的参数高效微调（PEFT）方法适配新任务，这类方法通常采用低秩更新策略。本研究从量子信息视角出发解析其有效性机制。在此视角下，低秩参数化天然对应于低维矩阵乘积态（MPS）表示，从而支持基于纠缠的参数结构表征。据此，我们提出并度量了“人工纠缠”——定义为人工神经网络（特指大语言模型）参数体系的纠缠熵。我们首先以在Tulu3和OpenThoughts3数据集上训练的1B和8B规模LLaMA模型为对象，研究了代表性低秩适应（LoRA）PEFT方法与全参数微调（FFT）方法，发现：（i）LoRA中查询与值投影矩阵更新过程的内部人工纠缠遵循具有中心抑制特征的体积律（称为“纠缠谷”），该现象对超参数敏感且与FFT模式存在显著差异；（ii）注意力矩阵中表征语义单元相关性的外部人工纠缠遵循带对数修正的面积律，对LoRA超参数及训练步数保持稳健。借鉴黑洞物理中的无毛定理，我们提出：虽然LoRA与FFT会引发不同的内部纠缠特征，但这些差异并未体现在注意力输出中，这种“无毛”特性可能是低秩更新有效的内在原因。我们进一步基于随机矩阵理论提供了理论支撑，并将分析拓展至MPS适应PEFT方法，发现其展现出定性相似的行为规律。",
    "url": "https://huggingface.co/papers/2601.06788",
    "arxiv_url": "https://arxiv.org/abs/2601.06788"
  },
  {
    "title": "FinForge: Semi-Synthetic Financial Benchmark Generation",
    "summary": "Evaluating Language Models (LMs) in specialized, high-stakes domains such as finance remains a significant challenge due to the scarcity of open, high-quality, and domain-specific datasets. Existing general-purpose benchmarks provide broad coverage but lack the depth and domain fidelity needed to assess LMs' capabilities for real-world financial reasoning, which requires both conceptual understanding and quantitative rigor. To address this gap, we introduce FinForge, a scalable, semi-synthetic pipeline for constructing finance-specific evaluation benchmarks through a hybrid of expert-guided data curation and controlled LM-based synthesis. FinForge combines manual and programmatic corpus construction from authoritative financial sources with structured question generation and validation using Gemini 2.5 Flash. To demonstrate the pipeline's efficacy, we produce FinForge-5k, a snapshot benchmark comprising over 5,000 human-validated question-answer pairs across 11 finance subdomains, derived from a curated corpus of 100,000 verified documents totaling 143M tokens. Evaluation of state-of-the-art open-source and closed-source models on FinForge-5k reveals significant differences in financial reasoning, with leading models achieving accuracy levels near 80%. These findings underscore the framework's utility for diagnosing current model limitations and guiding future improvements in financial domain competence. All code and data are available at https://github.com/gtfintechlab/FinForge.",
    "translation": "标题：FinForge：半合成金融基准生成框架\n\n摘要：在金融等专业性强、风险高的领域中评估语言模型仍面临重大挑战，主要原因在于缺乏公开、高质量且领域特定的数据集。现有的通用基准虽覆盖广泛，但缺乏评估语言模型在实际金融推理能力（既需概念理解又需定量严谨性）所需的深度与领域保真度。为填补这一空白，我们提出了FinForge——一个可扩展的半合成流程，通过专家引导的数据策展与基于语言模型的受控合成相结合，构建金融专用评估基准。FinForge融合了来自权威金融资料的手动与程序化语料构建，并利用Gemini 2.5 Flash进行结构化问题生成与验证。为验证该流程的有效性，我们构建了FinForge-5k基准快照，包含来自10万份经审核文档（总计1.43亿词元）的11个金融子领域超过5000个人工验证问答对。基于FinForge-5k对当前领先的开源与闭源模型进行评估，结果显示各模型在金融推理能力上存在显著差异，最优模型的准确率接近80%。这些发现凸显了该框架在诊断现有模型局限、指导未来金融领域能力改进方面的实用价值。所有代码与数据已公开于https://github.com/gtfintechlab/FinForge。",
    "url": "https://huggingface.co/papers/2601.06747",
    "arxiv_url": "https://arxiv.org/abs/2601.06747"
  },
  {
    "title": "Gecko: An Efficient Neural Architecture Inherently Processing Sequences with Arbitrary Lengths",
    "summary": "Designing a unified neural network to efficiently and inherently process sequential data with arbitrary lengths is a central and challenging problem in sequence modeling. The design choices in Transformer, including quadratic complexity and weak length extrapolation, have limited their ability to scale to long sequences. In this work, we propose Gecko, a neural architecture that inherits the design of Mega and Megalodon (exponential moving average with gated attention), and further introduces multiple technical components to improve its capability to capture long range dependencies, including timestep decay normalization, sliding chunk attention mechanism, and adaptive working memory. In a controlled pretraining comparison with Llama2 and Megalodon in the scale of 7 billion parameters and 2 trillion training tokens, Gecko achieves better efficiency and long-context scalability. Gecko reaches a training loss of 1.68, significantly outperforming Llama2-7B (1.75) and Megalodon-7B (1.70), and landing close to Llama2-13B (1.67). Notably, without relying on any context-extension techniques, Gecko exhibits inherent long-context processing and retrieval capabilities, stably handling sequences of up to 4 million tokens and retrieving information from contexts up to 4times longer than its attention window. Code: https://github.com/XuezheMax/gecko-llm",
    "translation": "标题：Gecko：一种能高效处理任意长度序列的神经架构\n\n摘要：设计一种能够高效且本质地处理任意长度序列数据的统一神经网络，是序列建模领域一个核心且具有挑战性的问题。Transformer架构中的设计选择，包括二次计算复杂度和较弱的长序列外推能力，限制了其向长序列扩展的能力。本研究提出Gecko神经架构，它继承了Mega和Megalodon的设计思想（采用带门控注意力的指数移动平均机制），并进一步引入了多项技术组件以增强其捕获长程依赖的能力，包括时间步衰减归一化、滑动分块注意力机制和自适应工作记忆。在与Llama2和Megalodon进行的70亿参数、2万亿训练标记规模的受控预训练对比中，Gecko展现出更优的效率和长上下文扩展能力。Gecko取得了1.68的训练损失，显著优于Llama2-7B（1.75）和Megalodon-7B（1.70），并接近Llama2-13B（1.67）的水平。值得注意的是，在不依赖任何上下文扩展技术的情况下，Gecko展现出固有的长上下文处理与检索能力，能够稳定处理长达400万标记的序列，并能从超出其注意力窗口长度4倍的上下文中检索信息。代码地址：https://github.com/XuezheMax/gecko-llm",
    "url": "https://huggingface.co/papers/2601.06463",
    "arxiv_url": "https://arxiv.org/abs/2601.06463"
  },
  {
    "title": "Does Inference Scaling Improve Reasoning Faithfulness? A Multi-Model Analysis of Self-Consistency Tradeoffs",
    "summary": "Self-consistency has emerged as a popular technique for improving large language model accuracy on reasoning tasks. The approach is straightforward: generate multiple reasoning paths and select the most common answer through majority voting. While this reliably boosts accuracy, it remains unclear whether these gains reflect genuine improvements in reasoning quality. We investigate a fundamental question that has not been studied before: does inference scaling improve reasoning faithfulness?\n  We conduct a comprehensive empirical study across four frontier models (GPT-5.2, Claude Opus 4.5, Gemini-3-flash-preview, and DeepSeek-v3.2) on 100 GSM8K mathematical reasoning problems. Our analysis employs bootstrap confidence intervals, McNemar's tests for paired comparisons, and Cohen's d effect sizes to quantify the effects rigorously. The results reveal striking differences across models that challenge common assumptions about self-consistency.\n  GPT-5.2 shows the expected pattern: accuracy improves from 78% to 90% at N=5, with faithfulness remaining relatively stable (0.540 to 0.510). Claude Opus 4.5 tells a completely different story. Its accuracy actually drops from 78% to 74.3% while faithfulness jumps dramatically from 0.270 to 0.891 at N=5. DeepSeek-v3.2, already at 98% accuracy, shows ceiling effects with modest faithfulness gains (0.440 to 0.541). Gemini-3-flash improves from 81% to 86% accuracy with a slight faithfulness decrease (0.260 to 0.212).\n  Problem difficulty analysis reveals that GPT-5.2 solves 82% of hard problems while breaking only 13% of easy ones. Claude, in contrast, breaks 23% of easy problems, explaining its accuracy decrease. These findings matter for practitioners: self-consistency is not universally beneficial, and teams should test their specific models before deployment. We release our code and provide practical recommendations for navigating these tradeoffs.",
    "translation": "标题：推理规模扩展能否提升推理忠实度？关于自洽性权衡的多模型分析\n\n摘要：自洽性已成为提升大语言模型在推理任务上准确性的常用技术。该方法思路直接：生成多条推理路径并通过多数投票选择最常见答案。虽然这能可靠地提升准确率，但尚不清楚这种提升是否反映了推理质量的真实改进。我们探究了一个此前未被研究的基础性问题：推理规模扩展能否提升推理忠实度？\n\n我们在100道GSM8K数学推理问题上对四种前沿模型（GPT-5.2、Claude Opus 4.5、Gemini-3-flash-preview和DeepSeek-v3.2）进行了全面实证研究。通过自助置信区间、配对比较的麦克尼马尔检验及科恩d效应值等量化分析方法，我们严谨评估了扩展效应。研究结果揭示了模型间的显著差异，对关于自洽性的普遍假设提出了挑战。\n\nGPT-5.2呈现预期模式：当N=5时准确率从78%提升至90%，忠实度保持相对稳定（0.540至0.510）。Claude Opus 4.5则呈现完全不同的情况：其准确率从78%下降至74.3%，而忠实度在N=5时从0.270大幅跃升至0.891。DeepSeek-v3.2已达98%准确率，呈现天花板效应，忠实度仅小幅提升（0.440至0.541）。Gemini-3-flash准确率从81%提升至86%，但忠实度轻微下降（0.260至0.212）。\n\n问题难度分析表明：GPT-5.2能解决82%的难题，而仅破坏13%的简单题。相比之下，Claude模型破坏了23%的简单题，这解释了其准确率下降的原因。这些发现对实践者具有重要意义：自洽性并非普遍有益，团队在部署前应测试特定模型。我们公开了研究代码，并为权衡这些特性提供了实用建议。",
    "url": "https://huggingface.co/papers/2601.06423",
    "arxiv_url": "https://arxiv.org/abs/2601.06423"
  },
  {
    "title": "FlyPose: Towards Robust Human Pose Estimation From Aerial Views",
    "summary": "Unmanned Aerial Vehicles (UAVs) are increasingly deployed in close proximity to humans for applications such as parcel delivery, traffic monitoring, disaster response and infrastructure inspections. Ensuring safe and reliable operation in these human-populated environments demands accurate perception of human poses and actions from an aerial viewpoint. This perspective challenges existing methods with low resolution, steep viewing angles and (self-)occlusion, especially if the application demands realtime feasibile models. We train and deploy FlyPose, a lightweight top-down human pose estimation pipeline for aerial imagery. Through multi-dataset training, we achieve an average improvement of 6.8 mAP in person detection across the test-sets of Manipal-UAV, VisDrone, HIT-UAV as well as our custom dataset. For 2D human pose estimation we report an improvement of 16.3 mAP on the challenging UAV-Human dataset. FlyPose runs with an inference latency of ~20 milliseconds including preprocessing on a Jetson Orin AGX Developer Kit and is deployed onboard a quadrotor UAV during flight experiments. We also publish FlyPose-104, a small but challenging aerial human pose estimation dataset, that includes manual annotations from difficult aerial perspectives: https://github.com/farooqhassaan/FlyPose.",
    "translation": "标题：FlyPose：面向航拍视角下鲁棒人体姿态估计的研究\n\n摘要：无人机正日益频繁地部署于人类活动密集的场景中，例如包裹配送、交通监控、灾害响应和基础设施巡检。为确保此类人机共存环境下的安全可靠运行，需要从航拍视角精确感知人体姿态与行为。这一视角因图像分辨率低、拍摄角度陡峭及（自）遮挡等问题，对现有方法构成显著挑战，尤其在需要实时可行模型的应用场景中。本文训练并部署了FlyPose——一种面向航拍图像的轻量级自上而下人体姿态估计框架。通过多数据集联合训练，我们在Manipal-UAV、VisDrone、HIT-UAV及自建数据集的测试集上实现了人体检测平均精度提升6.8 mAP。针对二维人体姿态估计任务，在具有挑战性的UAV-Human数据集上取得了16.3 mAP的性能提升。FlyPose在Jetson Orin AGX开发套件上的推理延迟（含预处理）约为20毫秒，并已在四旋翼无人机飞行实验中完成机载部署。同时，我们公开了FlyPose-104数据集——一个规模较小但极具挑战性的航拍人体姿态估计数据集，包含从困难航拍视角手动标注的样本：https://github.com/farooqhassaan/FlyPose。",
    "url": "https://huggingface.co/papers/2601.05747",
    "arxiv_url": "https://arxiv.org/abs/2601.05747"
  },
  {
    "title": "Benchmarking Small Language Models and Small Reasoning Language Models on System Log Severity Classification",
    "summary": "System logs are crucial for monitoring and diagnosing modern computing infrastructure, but their scale and complexity require reliable and efficient automated interpretation. Since severity levels are predefined metadata in system log messages, having a model merely classify them offers limited standalone practical value, revealing little about its underlying ability to interpret system logs. We argue that severity classification is more informative when treated as a benchmark for probing runtime log comprehension rather than as an end task. Using real-world journalctl data from Linux production servers, we evaluate nine small language models (SLMs) and small reasoning language models (SRLMs) under zero-shot, few-shot, and retrieval-augmented generation (RAG) prompting. The results reveal strong stratification. Qwen3-4B achieves the highest accuracy at 95.64% with RAG, while Gemma3-1B improves from 20.25% under few-shot prompting to 85.28% with RAG. Notably, the tiny Qwen3-0.6B reaches 88.12% accuracy despite weak performance without retrieval. In contrast, several SRLMs, including Qwen3-1.7B and DeepSeek-R1-Distill-Qwen-1.5B, degrade substantially when paired with RAG. Efficiency measurements further separate models: most Gemma and Llama variants complete inference in under 1.2 seconds per log, whereas Phi-4-Mini-Reasoning exceeds 228 seconds per log while achieving <10% accuracy. These findings suggest that (1) architectural design, (2) training objectives, and (3) the ability to integrate retrieved context under strict output constraints jointly determine performance. By emphasizing small, deployable models, this benchmark aligns with real-time requirements of digital twin (DT) systems and shows that severity classification serves as a lens for evaluating model competence and real-time deployability, with implications for root cause analysis (RCA) and broader DT integration.",
    "translation": "标题：系统日志严重性分类任务中轻量级语言模型与轻量级推理语言模型的基准测试\n\n摘要：系统日志对于监控与诊断现代计算基础设施至关重要，但其规模与复杂性要求可靠且高效的自动化解析。由于严重性级别是系统日志消息中预定义的元数据，仅让模型对其进行分类的独立实用价值有限，难以反映模型理解系统日志的深层能力。我们认为，将严重性分类视为探究运行时日志理解能力的基准测试，而非最终任务，能提供更丰富的信息。基于从Linux生产服务器采集的真实journalctl数据，我们在零样本、少样本及检索增强生成提示策略下评估了九种轻量级语言模型与轻量级推理语言模型。结果显示明显的性能分层：Qwen3-4B在检索增强生成条件下达到最高准确率95.64%，而Gemma3-1B在少样本提示下准确率仅为20.25%，通过检索增强生成提升至85.28%。值得注意的是，微型模型Qwen3-0.6B在无检索时表现较弱，但仍实现了88.12%的准确率。相反，包括Qwen3-1.7B与DeepSeek-R1-Distill-Qwen-1.5B在内的多款轻量级推理语言模型在结合检索增强生成后性能显著下降。效率测量进一步区分了模型性能：多数Gemma与Llama变体可在单条日志1.2秒内完成推理，而Phi-4-Mini-Reasoning的单条日志推理时间超过228秒，准确率却不足10%。这些发现表明：（1）架构设计，（2）训练目标，以及（3）在严格输出约束下整合检索上下文的能力共同决定了模型表现。通过聚焦轻量化可部署模型，本基准测试契合数字孪生系统的实时性需求，并证明严重性分类可作为评估模型能力与实时部署可行性的观察窗口，对根本原因分析与更广泛的数字孪生集成具有重要参考价值。",
    "url": "https://huggingface.co/papers/2601.07790",
    "arxiv_url": "https://arxiv.org/abs/2601.07790"
  },
  {
    "title": "Stochastic CHAOS: Why Deterministic Inference Kills, and Distributional Variability Is the Heartbeat of Artifical Cognition",
    "summary": "Deterministic inference is a comforting ideal in classical software: the same program on the same input should always produce the same output. As large language models move into real-world deployment, this ideal has been imported wholesale into inference stacks. Recent work from the Thinking Machines Lab has presented a detailed analysis of nondeterminism in LLM inference, showing how batch-invariant kernels and deterministic attention can enforce bitwise-identical outputs, positioning deterministic inference as a prerequisite for reproducibility and enterprise reliability.\n  In this paper, we take the opposite stance. We argue that, for LLMs, deterministic inference kills. It kills the ability to model uncertainty, suppresses emergent abilities, collapses reasoning into a single brittle path, and weakens safety alignment by hiding tail risks. LLMs implement conditional distributions over outputs, not fixed functions. Collapsing these distributions to a single canonical completion may appear reassuring, but it systematically conceals properties central to artificial cognition. We instead advocate Stochastic CHAOS, treating distributional variability as a signal to be measured and controlled.\n  Empirically, we show that deterministic inference is systematically misleading. Single-sample deterministic evaluation underestimates both capability and fragility, masking failure probability under paraphrases and noise. Phase-like transitions associated with emergent abilities disappear under greedy decoding. Multi-path reasoning degrades when forced onto deterministic backbones, reducing accuracy and diagnostic insight. Finally, deterministic evaluation underestimates safety risk by hiding rare but dangerous behaviors that appear only under multi-sample evaluation.",
    "translation": "标题：随机混沌：为何确定性推断扼杀人工智能认知，而分布变异性是其生命之源\n\n摘要：确定性推断是经典软件中一种令人安心的理想状态：同一程序在相同输入下应始终产生相同输出。随着大语言模型进入实际部署阶段，这一理念被全盘引入推断技术栈。思维机器实验室近期研究详细分析了LLM推断中的非确定性，展示了批次不变内核与确定性注意力机制如何强制实现比特级完全相同的输出，并将确定性推断定位为可复现性和企业可靠性的前提。\n\n本文持相反立场。我们认为，对于LLM而言，确定性推断具有致命危害：它扼杀模型对不确定性的表征能力，抑制涌现能力的出现，将推理过程压缩为单一脆弱路径，并通过掩盖尾部风险削弱安全对齐效果。LLM本质上是输出空间的条件概率分布实现，而非固定函数。将这些分布坍缩为单一标准输出看似可靠，实则系统性遮蔽了人工认知的核心特性。我们主张采用“随机混沌”范式，将分布变异性视为可测量、可控制的信号。\n\n实证研究表明，确定性推断具有系统性误导效应：单样本确定性评估会同时低估模型能力与脆弱性，掩盖其在语义改写和噪声干扰下的失败概率；与涌现能力相关的类相变现象在贪心解码策略下消失；多路径推理能力在强制适配确定性框架时发生退化，导致准确率下降与诊断洞察力减弱；确定性评估还会通过隐藏仅在多样本评估中出现的罕见危险行为，从而低估安全风险。",
    "url": "https://huggingface.co/papers/2601.07239",
    "arxiv_url": "https://arxiv.org/abs/2601.07239"
  },
  {
    "title": "3D CoCa v2: Contrastive Learners with Test-Time Search for Generalizable Spatial Intelligence",
    "summary": "Spatial intelligence refers to the ability to perceive, reason about, and describe objects and their relationships within three-dimensional environments, forming a foundation for embodied perception and scene understanding. 3D captioning aims to describe 3D scenes in natural language; however, it remains challenging due to the sparsity and irregularity of point clouds and, more critically, the weak grounding and limited out-of-distribution (OOD) generalization of existing captioners across drastically different environments, including indoor and outdoor 3D scenes. To address this challenge, we propose 3D CoCa v2, a generalizable 3D captioning framework that unifies contrastive vision-language learning with 3D caption generation and further improves robustness via test-time search (TTS) without updating the captioner parameters. 3D CoCa v2 builds on a frozen CLIP-based semantic prior, a spatially-aware 3D scene encoder for geometry, and a multimodal decoder jointly optimized with contrastive and captioning objectives, avoiding external detectors or handcrafted proposals. At inference, TTS produces diverse caption candidates and performs reward-guided selection using a compact scene summary. Experiments show improvements over 3D CoCa of +1.50 CIDEr@0.5IoU on ScanRefer and +1.61 CIDEr@0.5IoU on Nr3D, and +3.8 CIDEr@0.25 in zero-shot OOD evaluation on TOD3Cap. Code will be released at https://github.com/AIGeeksGroup/3DCoCav2.",
    "translation": "标题：3D CoCa v2：结合测试时搜索的可泛化空间智能对比学习框架\n\n摘要：空间智能指在三维环境中感知、推理并描述物体及其相互关系的能力，是具身感知与场景理解的基础。三维描述任务旨在用自然语言描述三维场景，但由于点云的稀疏性与不规则性，以及现有描述模型在室内外等差异显著场景中存在弱 grounding 问题与有限的分布外泛化能力，该任务仍面临挑战。为解决这一问题，我们提出 3D CoCa v2——一种可泛化的三维描述框架，该框架将对比式视觉-语言学习与三维描述生成相统一，并通过无需更新模型参数的测试时搜索进一步提升鲁棒性。3D CoCa v2 基于冻结的 CLIP 语义先验、感知几何的空间化三维场景编码器，以及通过对比与描述目标联合优化的多模态解码器构建，无需依赖外部检测器或人工设计提案。在推理阶段，测试时搜索生成多样化的描述候选句，并基于紧凑场景摘要进行奖励引导的选择。实验表明，本方法在 ScanRefer 数据集上 CIDEr@0.5IoU 指标提升 1.50，在 Nr3D 数据集上提升 1.61，在 TOD3Cap 的零样本分布外评估中 CIDEr@0.25 指标提升 3.8。代码将在 https://github.com/AIGeeksGroup/3DCoCav2 发布。",
    "url": "https://huggingface.co/papers/2601.06496",
    "arxiv_url": "https://arxiv.org/abs/2601.06496"
  },
  {
    "title": "On the Fallacy of Global Token Perplexity in Spoken Language Model Evaluation",
    "summary": "Generative spoken language models pretrained on large-scale raw audio can continue a speech prompt with appropriate content while preserving attributes like speaker and emotion, serving as foundation models for spoken dialogue. In prior literature, these models are often evaluated using ``global token perplexity'', which directly applies the text perplexity formulation to speech tokens. However, this practice overlooks fundamental differences between speech and text modalities, possibly leading to an underestimation of the speech characteristics. In this work, we propose a variety of likelihood- and generative-based evaluation methods that serve in place of naive global token perplexity. We demonstrate that the proposed evaluations more faithfully reflect perceived generation quality, as evidenced by stronger correlations with human-rated mean opinion scores (MOS). When assessed under the new metrics, the relative performance landscape of spoken language models is reshaped, revealing a significantly reduced gap between the best-performing model and the human topline. Together, these results suggest that appropriate evaluation is critical for accurately assessing progress in spoken language modeling.",
    "translation": "标题：论口语模型评估中全局词元困惑度的谬误\n\n摘要：基于大规模原始音频预训练的生成式口语模型能够延续语音提示并生成合适内容，同时保持说话者身份、情感等属性，可作为口语对话的基础模型。在现有文献中，这些模型常采用\"全局词元困惑度\"进行评估，该方法直接将文本困惑度公式应用于语音词元。然而，这种做法忽视了语音与文本模态间的本质差异，可能导致对语音特性的低估。本研究提出一系列基于似然估计与生成能力的评估方法，以替代朴素的全局词元困惑度指标。实验证明，所提出的评估方式能更真实地反映感知生成质量，其与人工评定的平均意见得分（MOS）呈现更强的相关性。在新指标评估下，口语模型的性能对比格局发生重构：最佳模型与人类表现上限之间的差距显著缩小。这些结果表明，采用恰当的评估方法对于准确衡量口语建模研究进展具有关键意义。",
    "url": "https://huggingface.co/papers/2601.06329",
    "arxiv_url": "https://arxiv.org/abs/2601.06329"
  },
  {
    "title": "A Rising Tide Lifts All Boats: MTQE Rewards for Idioms Improve General Translation Quality",
    "summary": "Non-compositional expressions (e.g., idioms, proverbs, and metaphors) pose significant challenges for neural machine translation systems because their meanings cannot be derived from individual words alone. These expressions encode rich, cultural meaning, and have both figurative and literal meanings, making accurate translation difficult. Because models are fairly good at translating compositional text, we investigate GRPO-style fine-tuning using Machine Translation Quality Estimation (MTQE) models as reward functions to train models to better translate idioms. Using Chinese and Hindi idiom datasets, we find that idiom translation abilities improve by ~14 points, general, non-idiomatic translation implicitly improves by ~8 points, and cross-lingual translation abilities (trained on one language, evaluated on another) improves by ~6 points. Overall, our work quantifies the non-compositional translation gap and offers insights for developing LLMs with stronger cross-cultural and figurative language understanding.",
    "translation": "标题：水涨船高：基于机器翻译质量评估的成语奖励机制提升整体翻译质量\n\n摘要：非组合性表达（如成语、谚语和隐喻）对神经机器翻译系统构成显著挑战，因为其含义无法仅通过单个词汇推导得出。这类表达承载着丰富的文化内涵，兼具比喻义与字面义，导致准确翻译尤为困难。鉴于现有模型在组合性文本翻译上表现良好，本研究探索采用基于机器翻译质量评估模型的GRPO式微调方法，将其作为奖励函数来训练模型以提升成语翻译能力。通过使用汉语和印地语成语数据集进行实验，我们发现模型的成语翻译能力提升约14个百分点，非成语类通用文本的翻译能力隐性提升约8个百分点，跨语言翻译能力（单语言训练，多语言评估）提升约6个百分点。本研究量化了非组合性文本的翻译差距，并为开发具有更强跨文化及比喻语言理解能力的大语言模型提供了理论参考。",
    "url": "https://huggingface.co/papers/2601.06307",
    "arxiv_url": "https://arxiv.org/abs/2601.06307"
  },
  {
    "title": "SPINAL -- Scaling-law and Preference Integration in Neural Alignment Layers",
    "summary": "Direct Preference Optimization (DPO) is a principled, scalable alternative to RLHF for aligning large language models from pairwise preferences, but its internal geometric footprint remains undercharacterized, limiting audits, checkpoint comparisons, and failure prediction. We introduce SPINAL (Scaling-law and Preference Integration in Neural Alignment Layers), a diagnostic that measures how alignment reshapes representations across depth by tracing localized structural change layer by layer. Across model families, DPO produces a layerwise calibration effect concentrated in the final decoder blocks (often layers 21-30), where preference gradients most directly affect the next-token distribution. SPINAL encodes each checkpoint as a depth trace over (layer index, contraction score, transport score). The contraction score summarizes how quickly the tail of a layer's spectrum decays (how fast small modes vanish); higher values indicate stronger contraction into fewer effective directions. The transport score summarizes how much the token distribution shifts between adjacent layers using a bounded overlap measure; lower values indicate shorter, smoother steps through representation space. Aligned checkpoints show a late-layer ramp-up in contraction and a smooth reduction in transport, consistent with tightened and stabilized policy mass, while unaligned models trace higher-curvature, more entropic, and geometrically incoherent depth paths. Overall, alignment is geometrically localized: the final layers encode the dominant preference-induced corrections. SPINAL turns this localization into a practical audit signal, quantifying where alignment concentrates, how strongly it manifests, and when it begins to destabilize during training.",
    "translation": "标题：SPINAL——神经对齐层中的缩放律与偏好整合\n\n摘要：直接偏好优化（DPO）是一种基于原则、可扩展的替代方法，用于通过成对偏好对齐大型语言模型，以替代基于人类反馈的强化学习（RLHF）。然而，其内部几何特征仍未被充分刻画，这限制了对模型的审计、检查点比较以及故障预测。我们提出SPINAL（神经对齐层中的缩放律与偏好整合），这是一种通过逐层追踪局部结构变化来度量对齐过程如何重塑各层表征的诊断方法。在不同模型系列中，DPO会产生一种集中于最后解码器块（通常为第21至30层）的逐层校准效应，偏好梯度在此处对接续词元分布产生最直接的影响。SPINAL将每个检查点编码为基于（层索引、收缩分数、传输分数）的深度轨迹。收缩分数概括了层谱尾部的衰减速度（小模态消失的快慢）；更高的值表示向更少有效方向的收缩更强。传输分数通过有界重叠度量概括了相邻层间词元分布的偏移程度；更低的值表示在表征空间中步长更短、更平滑。对齐后的检查点显示出收缩性在深层急剧上升，同时传输性平滑下降，这与策略质量的收紧和稳定相一致；而未对齐模型则呈现出更高曲率、更高熵值以及几何不一致的深度路径。总体而言，对齐在几何上是局部化的：最终层编码了主要的偏好诱导修正。SPINAL将这种局部化转化为实用的审计信号，能够量化对齐集中何处、其表现强度如何，以及在训练过程中何时开始失稳。",
    "url": "https://huggingface.co/papers/2601.06238",
    "arxiv_url": "https://arxiv.org/abs/2601.06238"
  }
]