[
  {
    "title": "EgoX: Egocentric Video Generation from a Single Exocentric Video",
    "summary": "Egocentric perception enables humans to experience and understand the world directly from their own point of view. Translating exocentric (third-person) videos into egocentric (first-person) videos opens up new possibilities for immersive understanding but remains highly challenging due to extreme camera pose variations and minimal view overlap. This task requires faithfully preserving visible content while synthesizing unseen regions in a geometrically consistent manner. To achieve this, we present EgoX, a novel framework for generating egocentric videos from a single exocentric input. EgoX leverages the pretrained spatio temporal knowledge of large-scale video diffusion models through lightweight LoRA adaptation and introduces a unified conditioning strategy that combines exocentric and egocentric priors via width and channel wise concatenation. Additionally, a geometry-guided self-attention mechanism selectively attends to spatially relevant regions, ensuring geometric coherence and high visual fidelity. Our approach achieves coherent and realistic egocentric video generation while demonstrating strong scalability and robustness across unseen and in-the-wild videos.",
    "translation": "标题：EgoX：基于单视角第三人称视频的第一人称视频生成\n\n摘要：第一人称感知使人类能够直接从自身视角体验和理解世界。将第三人称视频转换为第一人称视频为沉浸式理解开辟了新途径，但由于相机位姿的极端变化和视角重叠度极低，该任务仍极具挑战性。这需要在保持可见内容真实性的同时，以几何一致的方式合成未观测区域。为此，我们提出EgoX——一种从单段第三人称视频生成第一人称视频的创新框架。EgoX通过轻量级LoRA适配器利用大规模视频扩散模型预训练的时空知识，并引入统一的条件控制策略，通过宽度与通道维度的拼接融合第三人称与第一人称先验信息。此外，我们设计了几何引导的自注意力机制，该机制能选择性关注空间相关区域，确保几何连贯性与高视觉保真度。我们的方法实现了连贯逼真的第一人称视频生成，并在未见过的真实场景视频中展现出强大的可扩展性与鲁棒性。",
    "url": "https://huggingface.co/papers/2512.08269",
    "arxiv_url": "https://arxiv.org/abs/2512.08269"
  },
  {
    "title": "DentalGPT: Incentivizing Multimodal Complex Reasoning in Dentistry",
    "summary": "Reliable interpretation of multimodal data in dentistry is essential for automated oral healthcare, yet current multimodal large language models (MLLMs) struggle to capture fine-grained dental visual details and lack sufficient reasoning ability for precise diagnosis. To address these limitations, we present DentalGPT, a specialized dental MLLM developed through high-quality domain knowledge injection and reinforcement learning. Specifically, the largest annotated multimodal dataset for dentistry to date was constructed by aggregating over 120k dental images paired with detailed descriptions that highlight diagnostically relevant visual features, making it the multimodal dataset with the most extensive collection of dental images to date. Training on this dataset significantly enhances the MLLM's visual understanding of dental conditions, while the subsequent reinforcement learning stage further strengthens its capability for multimodal complex reasoning. Comprehensive evaluations on intraoral and panoramic benchmarks, along with dental subsets of medical VQA benchmarks, show that DentalGPT achieves superior performance in disease classification and dental VQA tasks, outperforming many state-of-the-art MLLMs despite having only 7B parameters. These results demonstrate that high-quality dental data combined with staged adaptation provides an effective pathway for building capable and domain-specialized dental MLLMs.",
    "translation": "标题：DentalGPT：激励牙科多模态复杂推理能力的发展\n\n摘要：牙科多模态数据的可靠解读对于自动化口腔医疗保健至关重要，然而当前的多模态大语言模型（MLLMs）难以捕捉细粒度的牙科视觉细节，且缺乏进行精确诊断所需的充分推理能力。为应对这些局限，我们提出了DentalGPT，这是一个通过高质量领域知识注入和强化学习开发的专用牙科MLLM。具体而言，我们通过整合超过12万张牙科图像及其突出诊断相关视觉特征的详细描述，构建了迄今为止规模最大的注释多模态牙科数据集，这也是目前涵盖牙科图像最广泛的多模态数据集。基于该数据集的训练显著增强了MLLM对牙科病况的视觉理解能力，而后续的强化学习阶段则进一步强化了其多模态复杂推理能力。在口内及全景X光片基准测试以及医学视觉问答（VQA）基准的牙科子集上的综合评估表明，DentalGPT在疾病分类和牙科VQA任务中均取得了卓越性能，尽管仅拥有70亿参数，但其表现优于许多先进的多模态大语言模型。这些结果证明，高质量牙科数据结合分阶段适应策略，为构建能力强且领域专用的牙科MLLMs提供了一条有效途径。",
    "url": "https://huggingface.co/papers/2512.11558",
    "arxiv_url": "https://arxiv.org/abs/2512.11558"
  },
  {
    "title": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder",
    "summary": "Visual generation grounded in Visual Foundation Model (VFM) representations offers a highly promising unified pathway for integrating visual understanding, perception, and generation. Despite this potential, training large-scale text-to-image diffusion models entirely within the VFM representation space remains largely unexplored. To bridge this gap, we scale the SVG (Self-supervised representations for Visual Generation) framework, proposing SVG-T2I to support high-quality text-to-image synthesis directly in the VFM feature domain. By leveraging a standard text-to-image diffusion pipeline, SVG-T2I achieves competitive performance, reaching 0.75 on GenEval and 85.78 on DPG-Bench. This performance validates the intrinsic representational power of VFMs for generative tasks. We fully open-source the project, including the autoencoder and generation model, together with their training, inference, evaluation pipelines, and pre-trained weights, to facilitate further research in representation-driven visual generation.",
    "translation": "标题：SVG-T2I：无需变分自编码器即可扩展文本到图像的潜在扩散模型\n\n摘要：基于视觉基础模型表示的视觉生成为整合视觉理解、感知与生成提供了一条极具前景的统一路径。尽管潜力巨大，但在VFM表示空间内完全训练大规模文本到图像扩散模型的研究仍较为有限。为填补这一空白，本文对SVG框架进行了扩展，提出SVG-T2I模型，以支持直接在VFM特征域中进行高质量的文本到图像合成。通过采用标准文本到图像扩散流程，SVG-T2I实现了具有竞争力的性能，在GenEval评估中达到0.75分，在DPG-Bench评估中达到85.78分。这一结果验证了VFM表示在生成任务中的内在表征能力。我们已全面开源该项目，包括自编码器与生成模型，以及相应的训练、推理、评估流程和预训练权重，以促进表征驱动视觉生成领域的进一步研究。",
    "url": "https://huggingface.co/papers/2512.11749",
    "arxiv_url": "https://arxiv.org/abs/2512.11749"
  },
  {
    "title": "V-RGBX: Video Editing with Accurate Controls over Intrinsic Properties",
    "summary": "Large-scale video generation models have shown remarkable potential in modeling photorealistic appearance and lighting interactions in real-world scenes. However, a closed-loop framework that jointly understands intrinsic scene properties (e.g., albedo, normal, material, and irradiance), leverages them for video synthesis, and supports editable intrinsic representations remains unexplored. We present V-RGBX, the first end-to-end framework for intrinsic-aware video editing. V-RGBX unifies three key capabilities: (1) video inverse rendering into intrinsic channels, (2) photorealistic video synthesis from these intrinsic representations, and (3) keyframe-based video editing conditioned on intrinsic channels. At the core of V-RGBX is an interleaved conditioning mechanism that enables intuitive, physically grounded video editing through user-selected keyframes, supporting flexible manipulation of any intrinsic modality. Extensive qualitative and quantitative results show that V-RGBX produces temporally consistent, photorealistic videos while propagating keyframe edits across sequences in a physically plausible manner. We demonstrate its effectiveness in diverse applications, including object appearance editing and scene-level relighting, surpassing the performance of prior methods.",
    "translation": "标题：V-RGBX：基于内在属性精确控制的视频编辑\n\n摘要：大规模视频生成模型在模拟真实场景中的逼真外观与光照交互方面展现出显著潜力。然而，一个能够联合理解场景内在属性（如反照率、法线、材质和辐照度）、利用这些属性进行视频合成并支持可编辑内在表征的闭环框架尚未得到探索。本文提出V-RGBX，这是首个面向内在属性感知的端到端视频编辑框架。V-RGBX整合了三大核心功能：（1）将视频逆向渲染分解为内在属性通道；（2）基于这些内在表征生成逼真视频；（3）通过内在属性通道约束实现基于关键帧的视频编辑。该框架的核心是交错条件机制，允许用户通过选定关键帧进行直观且符合物理规律的视频编辑，并支持对任意内在模态的灵活操控。大量定性与定量实验表明，V-RGBX能够生成时序一致、视觉逼真的视频，同时以符合物理规律的方式将关键帧编辑效果在序列中传播。我们在物体外观编辑与场景级重照明等多种应用中验证了其有效性，其性能超越了现有方法。",
    "url": "https://huggingface.co/papers/2512.11799",
    "arxiv_url": "https://arxiv.org/abs/2512.11799"
  },
  {
    "title": "Sliding Window Attention Adaptation",
    "summary": "The self-attention mechanism in Transformer-based Large Language Models (LLMs) scales quadratically with input length, making long-context inference expensive. Sliding window attention (SWA) reduces this cost to linear complexity, but naively enabling complete SWA at inference-time for models pretrained with full attention (FA) causes severe long-context performance degradation due to training-inference mismatch. This makes us wonder: Can FA-pretrained LLMs be well adapted to SWA without pretraining? We investigate this by proposing Sliding Window Attention Adaptation (SWAA), a set of practical recipes that combine five methods for better adaptation: (1) applying SWA only during prefilling; (2) preserving \"sink\" tokens; (3) interleaving FA/SWA layers; (4) chain-of-thought (CoT); and (5) fine-tuning. Our experiments show that SWA adaptation is feasible while non-trivial: no single method suffices, yet specific synergistic combinations effectively recover the original long-context performance. We further analyze the performance-efficiency trade-offs of different SWAA configurations and provide recommended recipes for diverse scenarios. Our code is available at https://github.com/yuyijiong/sliding-window-attention-adaptation",
    "translation": "标题：滑动窗口注意力适应方法\n\n摘要：基于Transformer架构的大语言模型（LLM）中的自注意力机制计算复杂度随输入长度呈二次方增长，导致长上下文推理成本高昂。滑动窗口注意力（SWA）可将计算复杂度降低至线性级别，但若对采用全注意力（FA）预训练的模型在推理阶段直接完全切换为SWA，会因训练与推理模式失配而导致长上下文性能严重下降。这引发我们思考：能否在不重新预训练的情况下，使FA预训练的LLM有效适应SWA？为此，我们提出滑动窗口注意力适应（SWAA）方法，通过整合五种策略实现更好的适应效果：（1）仅在预填充阶段应用SWA；（2）保留“锚点”标记；（3）交错排列FA/SWA层；（4）思维链（CoT）推理；（5）微调技术。实验表明，SWA适应具有可行性但非易事：单一方法均不足够，而特定的协同组合能有效恢复原始长上下文性能。我们进一步分析了不同SWAA配置在性能与效率间的权衡关系，并为多样化场景提供推荐方案。代码已开源：https://github.com/yuyijiong/sliding-window-attention-adaptation",
    "url": "https://huggingface.co/papers/2512.10411",
    "arxiv_url": "https://arxiv.org/abs/2512.10411"
  },
  {
    "title": "PersonaLive! Expressive Portrait Image Animation for Live Streaming",
    "summary": "Current diffusion-based portrait animation models predominantly focus on enhancing visual quality and expression realism, while overlooking generation latency and real-time performance, which restricts their application range in the live streaming scenario. We propose PersonaLive, a novel diffusion-based framework towards streaming real-time portrait animation with multi-stage training recipes. Specifically, we first adopt hybrid implicit signals, namely implicit facial representations and 3D implicit keypoints, to achieve expressive image-level motion control. Then, a fewer-step appearance distillation strategy is proposed to eliminate appearance redundancy in the denoising process, greatly improving inference efficiency. Finally, we introduce an autoregressive micro-chunk streaming generation paradigm equipped with a sliding training strategy and a historical keyframe mechanism to enable low-latency and stable long-term video generation. Extensive experiments demonstrate that PersonaLive achieves state-of-the-art performance with up to 7-22x speedup over prior diffusion-based portrait animation models.",
    "translation": "标题：PersonaLive！面向直播场景的富有表现力人像图像动画生成\n\n摘要：当前基于扩散模型的人像动画生成方法主要关注提升视觉质量与表情真实感，而忽视了生成延迟与实时性能，这限制了其在直播场景中的应用范围。本文提出PersonaLive——一种基于扩散模型的新型框架，通过多阶段训练方案实现流式实时人像动画生成。具体而言，我们首先采用混合隐式信号（即隐式面部表征与三维隐式关键点）来实现富有表现力的图像级运动控制。随后，提出一种少步数外观蒸馏策略，以消除去噪过程中的外观冗余，显著提升推理效率。最后，我们引入一种配备滑动训练策略与历史关键帧机制的自回归微片段流式生成范式，从而实现低延迟且稳定的长时序视频生成。大量实验表明，PersonaLive在达到最先进性能的同时，相比现有基于扩散模型的人像动画方法实现了最高7至22倍的加速效果。",
    "url": "https://huggingface.co/papers/2512.11253",
    "arxiv_url": "https://arxiv.org/abs/2512.11253"
  },
  {
    "title": "Exploring MLLM-Diffusion Information Transfer with MetaCanvas",
    "summary": "Multimodal learning has rapidly advanced visual understanding, largely via multimodal large language models (MLLMs) that use powerful LLMs as cognitive cores. In visual generation, however, these powerful core models are typically reduced to global text encoders for diffusion models, leaving most of their reasoning and planning ability unused. This creates a gap: current multimodal LLMs can parse complex layouts, attributes, and knowledge-intensive scenes, yet struggle to generate images or videos with equally precise and structured control. We propose MetaCanvas, a lightweight framework that lets MLLMs reason and plan directly in spatial and spatiotemporal latent spaces and interface tightly with diffusion generators. We empirically implement MetaCanvas on three different diffusion backbones and evaluate it across six tasks, including text-to-image generation, text/image-to-video generation, image/video editing, and in-context video generation, each requiring precise layouts, robust attribute binding, and reasoning-intensive control. MetaCanvas consistently outperforms global-conditioning baselines, suggesting that treating MLLMs as latent-space planners is a promising direction for narrowing the gap between multimodal understanding and generation.",
    "translation": "标题：基于MetaCanvas探索多模态大语言模型与扩散模型的信息传递\n\n摘要：多模态学习通过采用强大大型语言模型作为认知核心的多模态大语言模型，显著推动了视觉理解领域的发展。然而在视觉生成任务中，这些核心模型通常仅被简化为扩散模型的全局文本编码器，其大部分推理与规划能力未能得到充分利用。这导致了一个显著差距：当前多模态大语言模型虽能解析复杂布局、属性及知识密集型场景，却难以生成具有同等精确度与结构化控制能力的图像或视频。本文提出MetaCanvas——一个轻量级框架，使多模态大语言模型能够在空间与时空潜空间中进行直接推理与规划，并与扩散生成器实现紧密交互。我们在三种不同扩散模型骨干上实证实现了MetaCanvas，并在六类任务中开展评估，包括文本到图像生成、文本/图像到视频生成、图像/视频编辑以及上下文视频生成，每类任务均需精确布局控制、鲁棒属性绑定及推理密集型调控。实验表明，MetaCanvas在各项任务中持续优于全局条件基准方法，这证明将多模态大语言模型作为潜空间规划器，是弥合多模态理解与生成之间差距的有效途径。",
    "url": "https://huggingface.co/papers/2512.11464",
    "arxiv_url": "https://arxiv.org/abs/2512.11464"
  },
  {
    "title": "MeshSplatting: Differentiable Rendering with Opaque Meshes",
    "summary": "Primitive-based splatting methods like 3D Gaussian Splatting have revolutionized novel view synthesis with real-time rendering. However, their point-based representations remain incompatible with mesh-based pipelines that power AR/VR and game engines. We present MeshSplatting, a mesh-based reconstruction approach that jointly optimizes geometry and appearance through differentiable rendering. By enforcing connectivity via restricted Delaunay triangulation and refining surface consistency, MeshSplatting creates end-to-end smooth, visually high-quality meshes that render efficiently in real-time 3D engines. On Mip-NeRF360, it boosts PSNR by +0.69 dB over the current state-of-the-art MiLo for mesh-based novel view synthesis, while training 2x faster and using 2x less memory, bridging neural rendering and interactive 3D graphics for seamless real-time scene interaction. The project page is available at https://meshsplatting.github.io/.",
    "translation": "标题：MeshSplatting：基于不透明网格的可微分渲染\n\n摘要：以基元为基础的点云渲染方法，如3D高斯点云渲染，已通过实时渲染技术革新了新视角合成领域。然而，其基于点的表示方式仍无法兼容驱动AR/VR及游戏引擎的网格化流程。本文提出MeshSplatting——一种基于网格的重建方法，通过可微分渲染联合优化几何结构与外观属性。该方法通过受限Delaunay三角剖分强制保持网格连通性，并优化表面一致性，从而构建出端到端平滑、视觉质量高的网格模型，可在实时3D引擎中高效渲染。在Mip-NeRF360数据集上，本方法将基于网格的新视角合成性能提升至PSNR +0.69 dB，超越当前最优的MiLo方法，同时训练速度提升2倍，内存占用减少50%，成功弥合了神经渲染与交互式3D图形学之间的技术鸿沟，为实现无缝实时场景交互提供了新途径。项目页面详见：https://meshsplatting.github.io/。",
    "url": "https://huggingface.co/papers/2512.06818",
    "arxiv_url": "https://arxiv.org/abs/2512.06818"
  },
  {
    "title": "Structure From Tracking: Distilling Structure-Preserving Motion for Video Generation",
    "summary": "Reality is a dance between rigid constraints and deformable structures. For video models, that means generating motion that preserves fidelity as well as structure. Despite progress in diffusion models, producing realistic structure-preserving motion remains challenging, especially for articulated and deformable objects such as humans and animals. Scaling training data alone, so far, has failed to resolve physically implausible transitions. Existing approaches rely on conditioning with noisy motion representations, such as optical flow or skeletons extracted using an external imperfect model. To address these challenges, we introduce an algorithm to distill structure-preserving motion priors from an autoregressive video tracking model (SAM2) into a bidirectional video diffusion model (CogVideoX). With our method, we train SAM2VideoX, which contains two innovations: (1) a bidirectional feature fusion module that extracts global structure-preserving motion priors from a recurrent model like SAM2; (2) a Local Gram Flow loss that aligns how local features move together. Experiments on VBench and in human studies show that SAM2VideoX delivers consistent gains (+2.60\\% on VBench, 21-22\\% lower FVD, and 71.4\\% human preference) over prior baselines. Specifically, on VBench, we achieve 95.51\\%, surpassing REPA (92.91\\%) by 2.60\\%, and reduce FVD to 360.57, a 21.20\\% and 22.46\\% improvement over REPA- and LoRA-finetuning, respectively. The project website can be found at https://sam2videox.github.io/ .",
    "translation": "标题：基于跟踪的结构生成：为视频生成提炼结构保持性运动\n\n摘要：现实世界是刚性约束与可变形结构之间的动态平衡。对于视频模型而言，这意味着需要生成既保持保真度又维持结构一致性的运动。尽管扩散模型取得了进展，但生成真实且保持结构的运动仍然具有挑战性，尤其对于人体和动物等关节化与可变形物体。迄今为止，仅靠扩大训练数据未能解决物理上不合理的运动过渡问题。现有方法依赖于使用噪声运动表示（如光流或通过外部不完美模型提取的骨架）作为条件输入。为应对这些挑战，我们提出一种算法，将自回归视频跟踪模型（SAM2）中的结构保持性运动先验知识提炼到双向视频扩散模型（CogVideoX）中。基于该方法，我们训练了SAM2VideoX模型，其包含两项创新：（1）双向特征融合模块，可从SAM2等循环模型中提取全局结构保持性运动先验；（2）局部格拉姆流损失函数，用于对齐局部特征的协同运动模式。在VBench基准测试和人类评估实验中，SAM2VideoX相比现有基线模型取得显著提升（VBench得分提升2.60%，FVD降低21-22%，人类偏好率达71.4%）。具体而言，在VBench上我们获得95.51%的得分，较REPA（92.91%）提升2.60%；同时将FVD降至360.57，较REPA微调和LoRA微调分别改善21.20%和22.46%。项目网站详见 https://sam2videox.github.io/ 。",
    "url": "https://huggingface.co/papers/2512.11792",
    "arxiv_url": "https://arxiv.org/abs/2512.11792"
  },
  {
    "title": "LEO-RobotAgent: A General-purpose Robotic Agent for Language-driven Embodied Operator",
    "summary": "We propose LEO-RobotAgent, a general-purpose language-driven intelligent agent framework for robots. Under this framework, LLMs can operate different types of robots to complete unpredictable complex tasks across various scenarios. This framework features strong generalization, robustness, and efficiency. The application-level system built around it can fully enhance bidirectional human-robot intent understanding and lower the threshold for human-robot interaction. Regarding robot task planning, the vast majority of existing studies focus on the application of large models in single-task scenarios and for single robot types. These algorithms often have complex structures and lack generalizability. Thus, the proposed LEO-RobotAgent framework is designed with a streamlined structure as much as possible, enabling large models to independently think, plan, and act within this clear framework. We provide a modular and easily registrable toolset, allowing large models to flexibly call various tools to meet different requirements. Meanwhile, the framework incorporates a human-robot interaction mechanism, enabling the algorithm to collaborate with humans like a partner. Experiments have verified that this framework can be easily adapted to mainstream robot platforms including unmanned aerial vehicles (UAVs), robotic arms, and wheeled robot, and efficiently execute a variety of carefully designed tasks with different complexity levels. Our code is available at https://github.com/LegendLeoChen/LEO-RobotAgent.",
    "translation": "标题：LEO-RobotAgent：面向语言驱动具身操作器的通用机器人智能体\n\n摘要：本文提出LEO-RobotAgent，一种面向机器人的通用语言驱动智能体框架。在该框架下，大语言模型能够操作不同类型机器人，完成跨场景、不可预知的复杂任务。该框架具备强泛化性、鲁棒性与高效性，围绕其构建的应用级系统可全面增强人机双向意图理解，降低人机交互门槛。在机器人任务规划方面，现有研究大多聚焦于大模型在单一任务场景、单一机器人类型中的应用，其算法往往结构复杂且缺乏普适性。因此，所提出的LEO-RobotAgent框架尽可能采用精简结构设计，使大模型能够在清晰的框架内自主思考、规划与执行。我们提供了模块化且易于注册的工具集，使大模型能够灵活调用各类工具以满足多样化需求。同时，框架融合了人机协作机制，使算法能够像合作伙伴一样与人类协同工作。实验验证表明，该框架可轻松适配包括无人机、机械臂与轮式机器人在内的主流机器人平台，并能高效执行多种精心设计的、不同复杂度的任务。我们的代码已开源：https://github.com/LegendLeoChen/LEO-RobotAgent。",
    "url": "https://huggingface.co/papers/2512.10605",
    "arxiv_url": "https://arxiv.org/abs/2512.10605"
  },
  {
    "title": "Causal Judge Evaluation: Calibrated Surrogate Metrics for LLM Systems",
    "summary": "LLM-as-judge evaluation has become the de facto standard for scaling model assessment, but the practice is statistically unsound: uncalibrated scores can invert preferences, naive confidence intervals on uncalibrated scores achieve near-0% coverage, and importance-weighted estimators collapse under limited overlap despite high effective sample size (ESS). We introduce Causal Judge Evaluation (CJE), a framework that fixes all three failures. On n=4,961 Chatbot Arena prompts (after filtering from 5k), CJE achieves 99% pairwise ranking accuracy at full sample size (94% averaged across configurations), matching oracle quality, at 14x lower cost (for ranking 5 policies) by calibrating a 16x cheaper judge on just 5% oracle labels (~250 labels). CJE combines three components: (i) AutoCal-R, reward calibration via mean-preserving isotonic regression; (ii) SIMCal-W, weight stabilization via stacking of S-monotone candidates; and (iii) Oracle-Uncertainty Aware (OUA) inference that propagates calibration uncertainty into confidence intervals. We formalize the Coverage-Limited Efficiency (CLE) diagnostic, which explains why IPS-style estimators fail even when ESS exceeds 90%: the logger rarely visits regions where target policies concentrate. Key findings: SNIPS inverts rankings even with reward calibration (38% pairwise, negative Kendall's tau) due to weight instability; calibrated IPS remains near-random (47%) despite weight stabilization, consistent with CLE; OUA improves coverage from near-0% to ~86% (Direct) and ~96% (stacked-DR), where naive intervals severely under-cover.",
    "translation": "标题：因果评判评估：面向大语言模型系统的校准替代指标\n\n摘要：以大语言模型作为评判者的评估方法已成为扩展模型评估的事实标准，但其统计基础存在缺陷：未经校准的分数可能导致偏好逆转，基于未校准分数的朴素置信区间覆盖率趋近于零，而重要性加权估计量在有限重叠条件下即便有效样本量较高仍会失效。本文提出因果评判评估框架，该框架通过三项核心组件系统解决了上述问题：通过均值保持保序回归实现奖励校准；通过S单调候选模型的堆叠实现权重稳定；通过融入标注者不确定性的推断机制将校准不确定性传递至置信区间。在经筛选的4,961条Chatbot Arena提示数据上，该框架仅需约250条标注数据对成本降低16倍的评判模型进行校准，即以降低14倍的成本实现了与全样本标注相当的排序性能。本研究形式化提出的覆盖率受限效率诊断揭示了重要性采样类估计量失效的根本原因：日志策略极少访问目标策略集中分布的区域。关键发现表明：权重失稳会导致重要性采样在奖励校准后仍出现排序逆转；校准后的重要性采样估计量受覆盖率受限效率影响仍接近随机排序；而融入不确定性的推断机制将置信区间覆盖率从趋近于零提升至约86%（直接估计）和约96%（堆叠双重稳健估计），显著改善了朴素区间严重低估覆盖率的缺陷。",
    "url": "https://huggingface.co/papers/2512.11150",
    "arxiv_url": "https://arxiv.org/abs/2512.11150"
  },
  {
    "title": "Fairy2i: Training Complex LLMs from Real LLMs with All Parameters in {pm 1, pm i}",
    "summary": "Large language models (LLMs) have revolutionized artificial intelligence, yet their massive memory and computational demands necessitate aggressive quantization, increasingly pushing representations toward the theoretical limit of a single bit. While complex-valued LLMs, such as iFairy, offer a superior chance for low-bit representation compared to real-valued counterparts, they require training from scratch, preventing the utilization of the vast ecosystem of pre-trained real-valued foundation models. Here we present Fairy2i, a universal framework that transforms pre-trained real-valued layers into an equivalent widely-linear complex form, enabling extremely low-bit quantization while reusing existing checkpoints. By proving a lossless mathematical equivalence between real and widely-linear maps, we convert standard Transformers into the complex domain and employ a phase-aware quantization scheme with a highly efficient codebook of fourth roots of unity. Furthermore, we introduce a recursive residual quantization mechanism that iteratively minimizes quantization error, allowing inference to proceed via efficient multiplication-free accumulation. We demonstrate that Fairy2i restores the performance of LLaMA-2 7B at an effective 2-bit precision to levels nearly comparable with full-precision baselines, significantly outperforming state-of-the-art real-valued binary and ternary quantization methods. This work bridges the gap between the representational efficiency of complex-valued arithmetic and the practical utility of pre-trained models, paving a new way for efficient inference on commodity hardware.",
    "translation": "标题：Fairy2i：在{±1, ±i}参数空间下从实数大语言模型训练复数大语言模型\n\n摘要：大语言模型（LLMs）已彻底改变人工智能领域，但其巨大的内存与计算需求迫使人们采用激进的量化技术，逐渐将表示推向单比特的理论极限。尽管如iFairy等复数值大语言模型相较于实数值模型在低比特表示上具有更优潜力，但它们需从头开始训练，无法利用庞大的预训练实数值基础模型生态。本文提出Fairy2i——一种通用框架，可将预训练的实数值层转换为等效的广义线性复数形式，从而在复用现有模型参数的同时实现极低比特量化。通过证明实数映射与广义线性复数映射间的无损数学等价性，我们将标准Transformer转换至复数域，并采用基于四次单位根高效码本的相位感知量化方案。此外，我们引入递归残差量化机制，通过迭代最小化量化误差，实现无需乘法的高效累积推理。实验表明，Fairy2i能以等效2比特精度将LLaMA-2 7B模型的性能恢复至接近全精度基准的水平，显著优于当前最先进的实数值二值与三值量化方法。这项工作弥合了复数值算术的表征效率与预训练模型实用价值之间的鸿沟，为在通用硬件上实现高效推理开辟了新路径。",
    "url": "https://huggingface.co/papers/2512.02901",
    "arxiv_url": "https://arxiv.org/abs/2512.02901"
  },
  {
    "title": "CLINIC: Evaluating Multilingual Trustworthiness in Language Models for Healthcare",
    "summary": "Integrating language models (LMs) in healthcare systems holds great promise for improving medical workflows and decision-making. However, a critical barrier to their real-world adoption is the lack of reliable evaluation of their trustworthiness, especially in multilingual healthcare settings. Existing LMs are predominantly trained in high-resource languages, making them ill-equipped to handle the complexity and diversity of healthcare queries in mid- and low-resource languages, posing significant challenges for deploying them in global healthcare contexts where linguistic diversity is key. In this work, we present CLINIC, a Comprehensive Multilingual Benchmark to evaluate the trustworthiness of language models in healthcare. CLINIC systematically benchmarks LMs across five key dimensions of trustworthiness: truthfulness, fairness, safety, robustness, and privacy, operationalized through 18 diverse tasks, spanning 15 languages (covering all the major continents), and encompassing a wide array of critical healthcare topics like disease conditions, preventive actions, diagnostic tests, treatments, surgeries, and medications. Our extensive evaluation reveals that LMs struggle with factual correctness, demonstrate bias across demographic and linguistic groups, and are susceptible to privacy breaches and adversarial attacks. By highlighting these shortcomings, CLINIC lays the foundation for enhancing the global reach and safety of LMs in healthcare across diverse languages.",
    "translation": "标题：CLINIC：面向医疗健康领域的语言模型多语言可信度评估框架\n\n摘要：将语言模型整合到医疗健康系统中，对改善医疗工作流程和临床决策具有巨大潜力。然而，其在实际应用中的关键障碍在于缺乏对其可信度的可靠评估，尤其是在多语言医疗场景下。现有语言模型主要基于高资源语言进行训练，难以有效处理中低资源语言中医疗查询的复杂性与多样性，这在以语言多样性为特征的全球医疗健康部署场景中构成了重大挑战。本研究提出CLINIC——一个用于评估医疗领域语言模型可信度的综合性多语言基准测试框架。该框架系统化地从五个可信度核心维度（真实性、公平性、安全性、鲁棒性与隐私性）对语言模型进行测评，通过涵盖15种语言（覆盖全球主要大洲）的18项差异化任务实现可操作化评估，并包含疾病状况、预防措施、诊断检测、治疗方案、外科手术及药物使用等关键医疗主题。大规模评估结果表明，现有语言模型在事实准确性方面存在不足，在不同人口统计学群体和语言群体间表现出偏见，且易受隐私泄露与对抗性攻击的影响。通过系统揭示这些缺陷，CLINIC为提升语言模型在全球多语言医疗健康应用中的普及度与安全性奠定了重要基础。",
    "url": "https://huggingface.co/papers/2512.11437",
    "arxiv_url": "https://arxiv.org/abs/2512.11437"
  },
  {
    "title": "Scaling Behavior of Discrete Diffusion Language Models",
    "summary": "Modern LLM pre-training consumes vast amounts of compute and training data, making the scaling behavior, or scaling laws, of different models a key distinguishing factor. Discrete diffusion language models (DLMs) have been proposed as an alternative to autoregressive language models (ALMs). However, their scaling behavior has not yet been fully explored, with prior work suggesting that they require more data and compute to match the performance of ALMs.\n  We study the scaling behavior of DLMs on different noise types by smoothly interpolating between masked and uniform diffusion while paying close attention to crucial hyperparameters such as batch size and learning rate. Our experiments reveal that the scaling behavior of DLMs strongly depends on the noise type and is considerably different from ALMs. While all noise types converge to similar loss values in compute-bound scaling, we find that uniform diffusion requires more parameters and less data for compute-efficient training compared to masked diffusion, making them a promising candidate in data-bound settings. We scale our uniform diffusion model up to 10B parameters trained for 10^{22} FLOPs, confirming the predicted scaling behavior and making it the largest publicly known uniform diffusion model to date.",
    "translation": "标题：离散扩散语言模型的缩放行为研究\n\n摘要：现代大语言模型预训练消耗大量计算资源和训练数据，使得不同模型的缩放行为（即缩放定律）成为关键区分因素。离散扩散语言模型作为自回归语言模型的替代方案被提出，但其缩放行为尚未得到充分探索。先前研究表明，离散扩散模型需要更多数据和计算资源才能达到与自回归模型相当的性能。\n\n本研究通过平滑插值掩码扩散与均匀扩散策略，系统探究了不同噪声类型下离散扩散模型的缩放行为，并重点关注批量大小与学习率等关键超参数的影响。实验表明：离散扩散模型的缩放行为高度依赖于噪声类型，且与自回归模型存在显著差异。在计算资源受限的缩放场景中，所有噪声类型最终收敛至相近的损失值；但相较于掩码扩散，均匀扩散在计算效率优化训练中需要更多参数和更少数据，使其在数据受限场景中展现出独特优势。我们将均匀扩散模型扩展至100亿参数规模，训练计算量达10^{22} FLOPs，验证了预测的缩放规律，该模型也成为目前公开已知的最大规模均匀扩散语言模型。",
    "url": "https://huggingface.co/papers/2512.10858",
    "arxiv_url": "https://arxiv.org/abs/2512.10858"
  },
  {
    "title": "Task adaptation of Vision-Language-Action model: 1st Place Solution for the 2025 BEHAVIOR Challenge",
    "summary": "We present a vision-action policy that won 1st place in the 2025 BEHAVIOR Challenge - a large-scale benchmark featuring 50 diverse long-horizon household tasks in photo-realistic simulation, requiring bimanual manipulation, navigation, and context-aware decision making.\n  Building on the Pi0.5 architecture, we introduce several innovations. Our primary contribution is correlated noise for flow matching, which improves training efficiency and enables correlation-aware inpainting for smooth action sequences. We also apply learnable mixed-layer attention and System 2 stage tracking for ambiguity resolution. Training employs multi-sample flow matching to reduce variance, while inference uses action compression and challenge-specific correction rules.\n  Our approach achieves 26% q-score across all 50 tasks on both public and private leaderboards.",
    "translation": "标题：视觉-语言-行动模型的任务适应性：2025年BEHAVIOR挑战赛冠军解决方案\n\n摘要：本文提出一种视觉-行动策略模型，该模型在2025年BEHAVIOR挑战赛中荣获冠军。该挑战赛采用大规模基准测试，包含50项多样化的长周期家庭任务，在逼真仿真环境中要求执行双手操作、导航及情境感知决策。基于Pi0.5架构，我们引入多项创新：核心贡献是提出用于流匹配的相关噪声机制，该机制提升了训练效率，并能通过相关性感知修复技术生成平滑的动作序列。我们还采用可学习的混合层注意力机制与系统二阶段追踪方法以解决任务歧义。训练阶段使用多样本流匹配降低方差，推理阶段则采用动作压缩与挑战赛专用修正规则。该方法在公开与私有排行榜的50项任务中均实现26%的综合q分数。",
    "url": "https://huggingface.co/papers/2512.06951",
    "arxiv_url": "https://arxiv.org/abs/2512.06951"
  },
  {
    "title": "Fast-FoundationStereo: Real-Time Zero-Shot Stereo Matching",
    "summary": "Stereo foundation models achieve strong zero-shot generalization but remain computationally prohibitive for real-time applications. Efficient stereo architectures, on the other hand, sacrifice robustness for speed and require costly per-domain fine-tuning. To bridge this gap, we present Fast-FoundationStereo, a family of architectures that achieve, for the first time, strong zero-shot generalization at real-time frame rate. We employ a divide-and-conquer acceleration strategy with three components: (1) knowledge distillation to compress the hybrid backbone into a single efficient student; (2) blockwise neural architecture search for automatically discovering optimal cost filtering designs under latency budgets, reducing search complexity exponentially; and (3) structured pruning for eliminating redundancy in the iterative refinement module. Furthermore, we introduce an automatic pseudo-labeling pipeline used to curate 1.4M in-the-wild stereo pairs to supplement synthetic training data and facilitate knowledge distillation. The resulting model can run over 10x faster than FoundationStereo while closely matching its zero-shot accuracy, thus establishing a new state-of-the-art among real-time methods. Project page: https://nvlabs.github.io/Fast-FoundationStereo/",
    "translation": "标题：Fast-FoundationStereo：实时零样本立体匹配\n\n摘要：立体基础模型虽能实现强大的零样本泛化能力，但其计算成本过高，难以满足实时应用需求。另一方面，高效的立体架构往往以牺牲鲁棒性为代价来提升速度，且需要针对不同领域进行成本高昂的微调。为弥合这一差距，本文提出了Fast-FoundationStereo系列架构，首次在实时帧率下实现了强大的零样本泛化性能。我们采用分治加速策略，该策略包含三个核心组件：（1）通过知识蒸馏将混合骨干网络压缩为单一高效的学生模型；（2）采用分块神经架构搜索，在延迟预算约束下自动发现最优代价滤波设计，将搜索复杂度指数级降低；（3）通过结构化剪枝消除迭代优化模块中的冗余。此外，我们引入了一种自动伪标注流程，用于构建包含140万张真实场景立体图像对的数据集，以补充合成训练数据并促进知识蒸馏。最终模型运行速度比FoundationStereo提升10倍以上，同时零样本精度与之高度接近，从而在实时方法中确立了新的技术标杆。项目页面：https://nvlabs.github.io/Fast-FoundationStereo/",
    "url": "https://huggingface.co/papers/2512.11130",
    "arxiv_url": "https://arxiv.org/abs/2512.11130"
  },
  {
    "title": "CheXmask-U: Quantifying uncertainty in landmark-based anatomical segmentation for X-ray images",
    "summary": "Uncertainty estimation is essential for the safe clinical deployment of medical image segmentation systems, enabling the identification of unreliable predictions and supporting human oversight. While prior work has largely focused on pixel-level uncertainty, landmark-based segmentation offers inherent topological guarantees yet remains underexplored from an uncertainty perspective. In this work, we study uncertainty estimation for anatomical landmark-based segmentation on chest X-rays. Inspired by hybrid neural network architectures that combine standard image convolutional encoders with graph-based generative decoders, and leveraging their variational latent space, we derive two complementary measures: (i) latent uncertainty, captured directly from the learned distribution parameters, and (ii) predictive uncertainty, obtained by generating multiple stochastic output predictions from latent samples. Through controlled corruption experiments we show that both uncertainty measures increase with perturbation severity, reflecting both global and local degradation. We demonstrate that these uncertainty signals can identify unreliable predictions by comparing with manual ground-truth, and support out-of-distribution detection on the CheXmask dataset. More importantly, we release CheXmask-U (huggingface.co/datasets/mcosarinsky/CheXmask-U), a large scale dataset of 657,566 chest X-ray landmark segmentations with per-node uncertainty estimates, enabling researchers to account for spatial variations in segmentation quality when using these anatomical masks. Our findings establish uncertainty estimation as a promising direction to enhance robustness and safe deployment of landmark-based anatomical segmentation methods in chest X-ray. A fully working interactive demo of the method is available at huggingface.co/spaces/matiasky/CheXmask-U and the source code at github.com/mcosarinsky/CheXmask-U.",
    "translation": "标题：CheXmask-U：基于解剖标志点的X光图像分割不确定性量化研究\n\n摘要：不确定性估计对于医学图像分割系统的安全临床部署至关重要，它能够识别不可靠的预测结果并支持人工监督。尽管先前的研究主要集中于像素级不确定性，但基于解剖标志点的分割方法虽具备固有的拓扑结构保证，却从未在不确定性视角下得到充分探索。本研究针对胸部X光图像中基于解剖标志点的分割任务，系统探讨了不确定性估计方法。受启发于结合标准图像卷积编码器与基于图的生成式解码器的混合神经网络架构，并利用其变分隐空间特性，我们推导出两种互补的度量指标：（1）隐空间不确定性——直接从学习得到的分布参数中捕获；（2）预测不确定性——通过对隐空间样本进行多次随机输出预测获得。通过受控数据退化实验，我们证明这两种不确定性度量均随扰动强度增加而上升，能够同时反映全局与局部退化特征。通过与人工标注金标准对比，我们验证了这些不确定性信号可有效识别不可靠预测，并在CheXmask数据集上实现了分布外检测。更重要的是，我们发布了CheXmask-U数据集（huggingface.co/datasets/mcosarinsky/CheXmask-U），该大规模数据集包含657,566例胸部X光解剖标志点分割结果及每个节点的不确定性估计，使研究者在运用这些解剖掩模时能够充分考虑分割质量的空间异质性。本研究证实了不确定性估计可作为提升胸部X光解剖标志点分割方法鲁棒性与安全部署前景的重要方向。该方法的完整交互演示可通过huggingface.co/spaces/matiasky/CheXmask-U访问，源代码发布于github.com/mcosarinsky/CheXmask-U。",
    "url": "https://huggingface.co/papers/2512.10715",
    "arxiv_url": "https://arxiv.org/abs/2512.10715"
  },
  {
    "title": "The N-Body Problem: Parallel Execution from Single-Person Egocentric Video",
    "summary": "Humans can intuitively parallelise complex activities, but can a model learn this from observing a single person? Given one egocentric video, we introduce the N-Body Problem: how N individuals, can hypothetically perform the same set of tasks observed in this video. The goal is to maximise speed-up, but naive assignment of video segments to individuals often violates real-world constraints, leading to physically impossible scenarios like two people using the same object or occupying the same space. To address this, we formalise the N-Body Problem and propose a suite of metrics to evaluate both performance (speed-up, task coverage) and feasibility (spatial collisions, object conflicts and causal constraints). We then introduce a structured prompting strategy that guides a Vision-Language Model (VLM) to reason about the 3D environment, object usage, and temporal dependencies to produce a viable parallel execution. On 100 videos from EPIC-Kitchens and HD-EPIC, our method for N = 2 boosts action coverage by 45% over a baseline prompt for Gemini 2.5 Pro, while simultaneously slashing collision rates, object and causal conflicts by 55%, 45% and 55% respectively.",
    "translation": "标题：N体问题：基于单人第一人称视频的并行执行\n\n摘要：人类能够直观地对复杂活动进行并行化处理，但模型能否通过观察单个人的行为学会这种能力？给定一段第一人称视频，我们提出了N体问题：如何假设N个个体能够并行执行该视频中观察到的同一组任务。目标在于最大化加速比，但将视频片段简单分配给不同个体常会违反现实约束，导致物理上不可行的场景（如两人使用同一物体或占据同一空间）。为解决此问题，我们形式化定义了N体问题，并提出一套兼顾性能（加速比、任务覆盖率）与可行性（空间碰撞、物体冲突及因果约束）的评估指标。进而，我们设计了一种结构化提示策略，引导视觉语言模型通过三维环境推理、物体使用分析和时序依赖关系解析，生成可行的并行执行方案。在EPIC-Kitchens和HD-EPIC数据集的100段视频测试中，当N=2时，我们的方法相较于Gemini 2.5 Pro的基线提示方案，将动作覆盖率提升45%，同时将碰撞率、物体冲突和因果冲突分别降低55%、45%和55%。",
    "url": "https://huggingface.co/papers/2512.11393",
    "arxiv_url": "https://arxiv.org/abs/2512.11393"
  },
  {
    "title": "Sharp Monocular View Synthesis in Less Than a Second",
    "summary": "We present SHARP, an approach to photorealistic view synthesis from a single image. Given a single photograph, SHARP regresses the parameters of a 3D Gaussian representation of the depicted scene. This is done in less than a second on a standard GPU via a single feedforward pass through a neural network. The 3D Gaussian representation produced by SHARP can then be rendered in real time, yielding high-resolution photorealistic images for nearby views. The representation is metric, with absolute scale, supporting metric camera movements. Experimental results demonstrate that SHARP delivers robust zero-shot generalization across datasets. It sets a new state of the art on multiple datasets, reducing LPIPS by 25-34% and DISTS by 21-43% versus the best prior model, while lowering the synthesis time by three orders of magnitude. Code and weights are provided at https://github.com/apple/ml-sharp",
    "translation": "标题：一秒内实现锐利的单目视图合成\n\n摘要：本文提出SHARP方法，通过单张图像实现逼真的视图合成。给定单张照片，SHARP通过神经网络单次前向传播，在标准GPU上以不足一秒的时间回归出场景的三维高斯表示参数。该方法生成的三维高斯表示可实时渲染，为邻近视角生成高分辨率逼真图像。该表示具有绝对尺度的度量特性，支持度量级相机运动。实验结果表明，SHARP在不同数据集上均展现出强大的零样本泛化能力。在多个数据集上创造了新的技术标杆，与现有最佳模型相比，LPIPS指标降低25-34%，DISTS指标降低21-43%，同时将合成时间缩短三个数量级。代码与权重已发布于https://github.com/apple/ml-sharp",
    "url": "https://huggingface.co/papers/2512.10685",
    "arxiv_url": "https://arxiv.org/abs/2512.10685"
  },
  {
    "title": "Interpretable Embeddings with Sparse Autoencoders: A Data Analysis Toolkit",
    "summary": "Analyzing large-scale text corpora is a core challenge in machine learning, crucial for tasks like identifying undesirable model behaviors or biases in training data. Current methods often rely on costly LLM-based techniques (e.g. annotating dataset differences) or dense embedding models (e.g. for clustering), which lack control over the properties of interest. We propose using sparse autoencoders (SAEs) to create SAE embeddings: representations whose dimensions map to interpretable concepts. Through four data analysis tasks, we show that SAE embeddings are more cost-effective and reliable than LLMs and more controllable than dense embeddings. Using the large hypothesis space of SAEs, we can uncover insights such as (1) semantic differences between datasets and (2) unexpected concept correlations in documents. For instance, by comparing model responses, we find that Grok-4 clarifies ambiguities more often than nine other frontier models. Relative to LLMs, SAE embeddings uncover bigger differences at 2-8x lower cost and identify biases more reliably. Additionally, SAE embeddings are controllable: by filtering concepts, we can (3) cluster documents along axes of interest and (4) outperform dense embeddings on property-based retrieval. Using SAE embeddings, we study model behavior with two case studies: investigating how OpenAI model behavior has changed over time and finding \"trigger\" phrases learned by Tulu-3 (Lambert et al., 2024) from its training data. These results position SAEs as a versatile tool for unstructured data analysis and highlight the neglected importance of interpreting models through their data.",
    "translation": "标题：可解释稀疏自编码器嵌入：一种数据分析工具包\n\n摘要：分析大规模文本语料库是机器学习领域的核心挑战，对于识别训练数据中不良模型行为或偏见等任务至关重要。现有方法通常依赖成本高昂的基于大语言模型的技术（例如标注数据集差异）或稠密嵌入模型（例如用于聚类），这些方法难以针对特定关注属性进行有效控制。我们提出使用稀疏自编码器构建SAE嵌入：该表示方法的维度可直接映射到可解释的概念。通过四项数据分析任务，我们证明SAE嵌入相较于大语言模型更具成本效益和可靠性，同时比稠密嵌入更具可控性。借助SAE庞大的假设空间，我们能够揭示以下洞察：（1）数据集间的语义差异；（2）文档中意外的概念关联。例如，通过比较模型响应，我们发现Grok-4模型比其他九种前沿模型更频繁地澄清歧义。相较于大语言模型，SAE嵌入能以降低2-8倍的成本发现更显著的差异，并能更可靠地识别偏见。此外，SAE嵌入具有可控性：通过概念筛选，我们能够（3）沿关注维度对文档进行聚类，并（4）在基于属性的检索任务中超越稠密嵌入方法。基于SAE嵌入，我们通过两个案例研究模型行为：探究OpenAI模型随时间的行为演变，以及识别Tulu-3模型（Lambert等，2024）从其训练数据中学到的“触发”短语。这些研究成果确立了SAE作为非结构化数据分析通用工具的地位，并凸显了通过数据视角解释模型这一被忽视的重要维度。",
    "url": "https://huggingface.co/papers/2512.10092",
    "arxiv_url": "https://arxiv.org/abs/2512.10092"
  },
  {
    "title": "Particulate: Feed-Forward 3D Object Articulation",
    "summary": "We present Particulate, a feed-forward approach that, given a single static 3D mesh of an everyday object, directly infers all attributes of the underlying articulated structure, including its 3D parts, kinematic structure, and motion constraints. At its core is a transformer network, Part Articulation Transformer, which processes a point cloud of the input mesh using a flexible and scalable architecture to predict all the aforementioned attributes with native multi-joint support. We train the network end-to-end on a diverse collection of articulated 3D assets from public datasets. During inference, Particulate lifts the network's feed-forward prediction to the input mesh, yielding a fully articulated 3D model in seconds, much faster than prior approaches that require per-object optimization. Particulate can also accurately infer the articulated structure of AI-generated 3D assets, enabling full-fledged extraction of articulated 3D objects from a single (real or synthetic) image when combined with an off-the-shelf image-to-3D generator. We further introduce a new challenging benchmark for 3D articulation estimation curated from high-quality public 3D assets, and redesign the evaluation protocol to be more consistent with human preferences. Quantitative and qualitative results show that Particulate significantly outperforms state-of-the-art approaches.",
    "translation": "标题：Particulate：一种用于三维物体关节结构预测的前馈方法\n\n摘要：本文提出Particulate，一种前馈式方法，能够基于日常物体的单个静态三维网格，直接推断底层关节结构的所有属性，包括三维部件、运动学结构及运动约束。其核心是一个基于Transformer架构的网络——部件关节变换器（Part Articulation Transformer），该网络通过灵活可扩展的架构处理输入网格的点云数据，以原生多关节支持的方式预测上述所有属性。我们使用来自公共数据集的多样化三维关节资产对网络进行端到端训练。在推理阶段，Particulate将网络的前馈预测结果映射至输入网格，可在数秒内生成完全关节化的三维模型，其速度远快于以往需要逐对象优化的方法。结合现成的图像到三维生成器，Particulate还能准确推断人工智能生成的三维资产的关节结构，从而支持从单张（真实或合成）图像中完整提取关节化三维物体。此外，我们基于高质量公共三维资产构建了一个具有挑战性的三维关节估计新基准，并重新设计了更符合人类偏好的评估流程。定量与定性实验结果表明，Particulate在性能上显著优于现有先进方法。",
    "url": "https://huggingface.co/papers/2512.11798",
    "arxiv_url": "https://arxiv.org/abs/2512.11798"
  }
]