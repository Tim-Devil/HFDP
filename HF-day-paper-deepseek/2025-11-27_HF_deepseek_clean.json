[
  {
    "title": "Multimodal Evaluation of Russian-language Architectures",
    "summary": "Multimodal large language models (MLLMs) are currently at the center of research attention, showing rapid progress in scale and capabilities, yet their intelligence, limitations, and risks remain insufficiently understood. To address these issues, particularly in the context of the Russian language, where no multimodal benchmarks currently exist, we introduce Mera Multi, an open multimodal evaluation framework for Russian-spoken architectures. The benchmark is instruction-based and encompasses default text, image, audio, and video modalities, comprising 18 newly constructed evaluation tasks for both general-purpose models and modality-specific architectures (image-to-text, video-to-text, and audio-to-text). Our contributions include: (i) a universal taxonomy of multimodal abilities; (ii) 18 datasets created entirely from scratch with attention to Russian cultural and linguistic specificity, unified prompts, and metrics; (iii) baseline results for both closed-source and open-source models; (iv) a methodology for preventing benchmark leakage, including watermarking and licenses for private sets. While our current focus is on Russian, the proposed benchmark provides a replicable methodology for constructing multimodal benchmarks in typologically diverse languages, particularly within the Slavic language family.",
    "translation": "标题：俄语架构的多模态评估框架\n\n摘要：多模态大语言模型目前已成为研究焦点，其在规模与能力层面呈现快速发展，但其智能水平、局限性及潜在风险仍未得到充分认知。针对这一现状，特别是在俄语语境下尚无多模态基准的背景下，我们推出Mera Multi——一个面向俄语架构的开放式多模态评估框架。该基准采用指令驱动模式，涵盖默认的文本、图像、音频与视频模态，包含18项全新构建的评估任务，既适用于通用模型，也针对特定模态架构（图像到文本、视频到文本及音频到文本）。我们的贡献包括：（i）建立多模态能力的统一分类体系；（ii）完全从零构建的18个数据集，重点关注俄罗斯文化及语言特性，并统一提示词与评估指标；（iii）闭源与开源模型的基线结果；（iv）包含水印技术与私有数据集许可的基准泄露防范方法论。虽然当前研究聚焦俄语，但本基准为在类型学多样的语言（特别是斯拉夫语系）中构建多模态评估体系提供了可复现的方法论路径。",
    "url": "https://huggingface.co/papers/2511.15552",
    "arxiv_url": "https://arxiv.org/abs/2511.15552"
  },
  {
    "title": "Latent Collaboration in Multi-Agent Systems",
    "summary": "Multi-agent systems (MAS) extend large language models (LLMs) from independent single-model reasoning to coordinative system-level intelligence. While existing LLM agents depend on text-based mediation for reasoning and communication, we take a step forward by enabling models to collaborate directly within the continuous latent space. We introduce LatentMAS, an end-to-end training-free framework that enables pure latent collaboration among LLM agents. In LatentMAS, each agent first performs auto-regressive latent thoughts generation through last-layer hidden embeddings. A shared latent working memory then preserves and transfers each agent's internal representations, ensuring lossless information exchange. We provide theoretical analyses establishing that LatentMAS attains higher expressiveness and lossless information preservation with substantially lower complexity than vanilla text-based MAS. In addition, empirical evaluations across 9 comprehensive benchmarks spanning math and science reasoning, commonsense understanding, and code generation show that LatentMAS consistently outperforms strong single-model and text-based MAS baselines, achieving up to 14.6% higher accuracy, reducing output token usage by 70.8%-83.7%, and providing 4x-4.3x faster end-to-end inference. These results demonstrate that our new latent collaboration framework enhances system-level reasoning quality while offering substantial efficiency gains without any additional training. Code and data are fully open-sourced at https://github.com/Gen-Verse/LatentMAS.",
    "translation": "标题：多智能体系统中的潜在协作\n\n摘要：多智能体系统（MAS）将大语言模型（LLMs）从独立的单模型推理扩展至协同的系统级智能。现有LLM智能体依赖基于文本的中介进行推理与通信，本研究通过实现模型在连续潜在空间内的直接协作推进了一步。我们提出LatentMAS——一种支持LLM智能体间纯潜在协作的端到端免训练框架。在该框架中，每个智能体首先通过末层隐藏嵌入进行自回归潜在思维生成，随后通过共享潜在工作记忆保存并传递各智能体的内部表征，确保无损信息交换。理论分析表明，相较于传统基于文本的MAS，LatentMAS在显著降低复杂度的同时实现了更强的表达能力与无损信息保存。在涵盖数学科学推理、常识理解和代码生成的9个综合基准测试中，实证评估表明LatentMAS持续优于强单模型及基于文本的MAS基线，准确率最高提升14.6%，输出令牌使用量减少70.8%-83.7%，端到端推理速度提升4-4.3倍。这些结果证明，我们的新型潜在协作框架在无需额外训练的情况下，既能提升系统级推理质量，又可实现显著的效率增益。代码与数据已通过https://github.com/Gen-Verse/LatentMAS 完全开源。",
    "url": "https://huggingface.co/papers/2511.20639",
    "arxiv_url": "https://arxiv.org/abs/2511.20639"
  },
  {
    "title": "Inferix: A Block-Diffusion based Next-Generation Inference Engine for World Simulation",
    "summary": "World models serve as core simulators for fields such as agentic AI, embodied AI, and gaming, capable of generating long, physically realistic, and interactive high-quality videos. Moreover, scaling these models could unlock emergent capabilities in visual perception, understanding, and reasoning, paving the way for a new paradigm that moves beyond current LLM-centric vision foundation models. A key breakthrough empowering them is the semi-autoregressive (block-diffusion) decoding paradigm, which merges the strengths of diffusion and autoregressive methods by generating video tokens in block-applying diffusion within each block while conditioning on previous ones, resulting in more coherent and stable video sequences. Crucially, it overcomes limitations of standard video diffusion by reintroducing LLM-style KV Cache management, enabling efficient, variable-length, and high-quality generation.\n  Therefore, Inferix is specifically designed as a next-generation inference engine to enable immersive world synthesis through optimized semi-autoregressive decoding processes. This dedicated focus on world simulation distinctly sets it apart from systems engineered for high-concurrency scenarios (like vLLM or SGLang) and from classic video diffusion models (such as xDiTs). Inferix further enhances its offering with interactive video streaming and profiling, enabling real-time interaction and realistic simulation to accurately model world dynamics. Additionally, it supports efficient benchmarking through seamless integration of LV-Bench, a new fine-grained evaluation benchmark tailored for minute-long video generation scenarios. We hope the community will work together to advance Inferix and foster world model exploration.",
    "translation": "标题：Inferix：基于块扩散的新一代世界模拟推理引擎\n\n摘要：世界模型作为智能体AI、具身AI和游戏等领域的核心模拟器，能够生成长时序、物理真实且交互式的高质量视频。此外，扩展这些模型有望释放视觉感知、理解与推理的涌现能力，为超越当前以LLM为中心的视觉基础模型开辟新范式。实现这一突破的关键在于半自回归（块扩散）解码范式，该范式融合了扩散方法与自回归方法的优势，通过分块生成视频令牌——在每个块内应用扩散过程的同时以前序块为条件，从而产生更连贯稳定的视频序列。尤为关键的是，该范式通过重新引入LLM风格的KV缓存管理机制，突破了标准视频扩散模型的局限，实现了高效、可变长度的高质量生成。\n因此，Inferix被专门设计为新一代推理引擎，通过优化的半自回归解码过程实现沉浸式世界合成。这种对世界模拟的专注定位使其明显区别于面向高并发场景的系统（如vLLM或SGLang）及经典视频扩散模型（如xDiTs）。Inferix进一步通过交互式视频流传输与性能分析功能增强系统能力，支持实时交互与逼真模拟以精确建模世界动态。此外，系统通过无缝集成LV-Bench——专为分钟级视频生成场景定制的新型细粒度评估基准，实现了高效性能评测。我们期待学界携手推进Inferix发展，共同推动世界模型的前沿探索。",
    "url": "https://huggingface.co/papers/2511.20714",
    "arxiv_url": "https://arxiv.org/abs/2511.20714"
  },
  {
    "title": "Harmony: Harmonizing Audio and Video Generation through Cross-Task Synergy",
    "summary": "The synthesis of synchronized audio-visual content is a key challenge in generative AI, with open-source models facing challenges in robust audio-video alignment. Our analysis reveals that this issue is rooted in three fundamental challenges of the joint diffusion process: (1) Correspondence Drift, where concurrently evolving noisy latents impede stable learning of alignment; (2) inefficient global attention mechanisms that fail to capture fine-grained temporal cues; and (3) the intra-modal bias of conventional Classifier-Free Guidance (CFG), which enhances conditionality but not cross-modal synchronization. To overcome these challenges, we introduce Harmony, a novel framework that mechanistically enforces audio-visual synchronization. We first propose a Cross-Task Synergy training paradigm to mitigate drift by leveraging strong supervisory signals from audio-driven video and video-driven audio generation tasks. Then, we design a Global-Local Decoupled Interaction Module for efficient and precise temporal-style alignment. Finally, we present a novel Synchronization-Enhanced CFG (SyncCFG) that explicitly isolates and amplifies the alignment signal during inference. Extensive experiments demonstrate that Harmony establishes a new state-of-the-art, significantly outperforming existing methods in both generation fidelity and, critically, in achieving fine-grained audio-visual synchronization.",
    "translation": "标题：Harmony：通过跨任务协同实现音视频生成的和谐统一\n\n摘要：同步音视频内容的合成是生成式人工智能面临的关键挑战，开源模型在实现鲁棒的音视频对齐方面存在诸多困难。我们的分析表明，这一问题根源于联合扩散过程的三个基本挑战：(1) 对应漂移问题——并发演化的噪声潜在空间阻碍了对齐关系的稳定学习；(2) 低效的全局注意力机制难以捕捉细粒度时序特征；(3) 传统无分类器引导机制存在的模态内偏差，虽能增强条件可控性却无益于跨模态同步。为攻克这些难题，我们提出Harmony这一创新框架，通过机制化设计强化音视频同步。首先，我们建立跨任务协同训练范式，利用音频驱动视频生成与视频驱动音频生成任务中的强监督信号来抑制对应漂移。继而，设计全局-局部解耦交互模块，实现高效精准的时序风格对齐。最后，提出同步增强型无分类器引导机制，在推理阶段显式分离并强化对齐信号。大量实验表明，Harmony开创了全新性能标杆，在生成保真度及关键的细粒度音视频同步指标上均显著超越现有方法。",
    "url": "https://huggingface.co/papers/2511.21579",
    "arxiv_url": "https://arxiv.org/abs/2511.21579"
  },
  {
    "title": "Revisiting Generalization Across Difficulty Levels: It's Not So Easy",
    "summary": "We investigate how well large language models (LLMs) generalize across different task difficulties, a key question for effective data curation and evaluation. Existing research is mixed regarding whether training on easier or harder data leads to better results, and whether those gains come on easier or harder test data. We address this question by conducting a systematic evaluation of LLMs' generalization across models, datasets, and fine-grained groups of example difficulty. We rank examples in six datasets using the outputs of thousands of different LLMs and Item Response Theory (IRT), a well-established difficulty metric in educational testing. Unlike prior work, our difficulty ratings are therefore determined solely by the abilities of many different LLMs, excluding human opinions of difficulty. With a more objective, larger-scale, and finer-grained analysis, we show that cross-difficulty generalization is often limited; training on either easy or hard data cannot achieve consistent improvements across the full range of difficulties. These results show the importance of having a range of difficulties in both training and evaluation data for LLMs, and that taking shortcuts with respect to difficulty is risky.",
    "translation": "标题：重新审视跨难度水平的泛化能力：并非易事\n\n摘要：本文系统研究大型语言模型在不同任务难度间的泛化能力，这是影响数据筛选与评估效果的核心问题。现有研究对\"使用简易或困难训练数据能否获得更优结果\"及\"性能提升体现在何种难度测试数据上\"存在分歧。我们通过构建跨模型、跨数据集、跨细粒度难度分组的系统评估框架，采用数千种大型语言模型的输出结果结合教育测试领域的成熟难度度量指标——项目反应理论（IRT），对六个数据集中的样本进行难度分级。与既往研究不同，本研究的难度评级完全基于多样本LLM的自身能力，排除了人类主观难度判断。通过更客观、大规模、细粒度的分析，我们发现：跨难度泛化能力普遍受限；仅使用简易或困难数据训练均无法在全难度范围内实现持续性能提升。这些结果表明，在LLM的训练与评估数据中保持难度多样性至关重要，任何在难度维度上的捷径策略都存在风险。",
    "url": "https://huggingface.co/papers/2511.21692",
    "arxiv_url": "https://arxiv.org/abs/2511.21692"
  },
  {
    "title": "NVIDIA Nemotron Parse 1.1",
    "summary": "We introduce Nemotron-Parse-1.1, a lightweight document parsing and OCR model that advances the capabilities of its predecessor, Nemoretriever-Parse-1.0. Nemotron-Parse-1.1 delivers improved capabilities across general OCR, markdown formatting, structured table parsing, and text extraction from pictures, charts, and diagrams. It also supports a longer output sequence length for visually dense documents. As with its predecessor, it extracts bounding boxes of text segments, as well as corresponding semantic classes. Nemotron-Parse-1.1 follows an encoder-decoder architecture with 885M parameters, including a compact 256M-parameter language decoder. It achieves competitive accuracy on public benchmarks making it a strong lightweight OCR solution. We release the model weights publicly on Huggingface, as well as an optimized NIM container, along with a subset of the training data as part of the broader Nemotron-VLM-v2 dataset. Additionally, we release Nemotron-Parse-1.1-TC which operates on a reduced vision token length, offering a 20% speed improvement with minimal quality degradation.",
    "translation": "标题：NVIDIA Nemotron解析模型1.1版\n\n摘要：本文介绍Nemotron-Parse-1.1——一个轻量级文档解析与OCR模型，该模型在前代Nemoretriever-Parse-1.0的基础上实现了全面升级。本模型在通用OCR识别、Markdown格式解析、结构化表格处理以及图像/图表/示意图文本提取等任务中展现出显著增强的性能，同时支持更长的输出序列以处理视觉密集文档。与前一版本相同，该模型能够提取文本段的边界框及对应语义类别。Nemotron-Parse-1.1采用编码器-解码器架构，参数量达8.85亿（其中语言解码器为紧凑的2.56亿参数），在公开基准测试中达到具有竞争力的准确率，堪称优秀的轻量级OCR解决方案。我们已在Huggingface平台公开发布模型权重、优化的NIM推理容器，并作为Nemotron-VLM-v2数据集组成部分开放部分训练数据。此外，同步推出Nemotron-Parse-1.1-TC版本，该版本通过缩减视觉令牌长度实现推理速度提升20%，且质量损失控制在最小范围。",
    "url": "https://huggingface.co/papers/2511.20478",
    "arxiv_url": "https://arxiv.org/abs/2511.20478"
  },
  {
    "title": "Monet: Reasoning in Latent Visual Space Beyond Images and Language",
    "summary": "\"Thinking with images\" has emerged as an effective paradigm for advancing visual reasoning, extending beyond text-only chains of thought by injecting visual evidence into intermediate reasoning steps. However, existing methods fall short of human-like abstract visual thinking, as their flexibility is fundamentally limited by external tools. In this work, we introduce Monet, a training framework that enables multimodal large language models (MLLMs) to reason directly within the latent visual space by generating continuous embeddings that function as intermediate visual thoughts. We identify two core challenges in training MLLMs for latent visual reasoning: high computational cost in latent-vision alignment and insufficient supervision over latent embeddings, and address them with a three-stage distillation-based supervised fine-tuning (SFT) pipeline. We further reveal a limitation of applying GRPO to latent reasoning: it primarily enhances text-based reasoning rather than latent reasoning. To overcome this, we propose VLPO (Visual-latent Policy Optimization), a reinforcement learning method that explicitly incorporates latent embeddings into policy gradient updates. To support SFT, we construct Monet-SFT-125K, a high-quality text-image interleaved CoT dataset containing 125K real-world, chart, OCR, and geometry CoTs. Our model, Monet-7B, shows consistent gains across real-world perception and reasoning benchmarks and exhibits strong out-of-distribution generalization on challenging abstract visual reasoning tasks. We also empirically analyze the role of each training component and discuss our early unsuccessful attempts, providing insights for future developments in visual latent reasoning. Our model, data, and code are available at https://github.com/NOVAglow646/Monet.",
    "translation": "标题：Monet：超越图像与语言的潜在视觉空间推理\n\n摘要：\"图像思维\"已成为推进视觉推理的有效范式，通过将视觉证据注入中间推理步骤，超越了纯文本的思维链条。然而，现有方法因受限于外部工具，其灵活性远未达到类人的抽象视觉思维水平。本研究提出Monet训练框架，使多模态大语言模型能够通过生成作为中间视觉思维的连续嵌入表示，直接在潜在视觉空间中进行推理。我们识别出训练潜在视觉推理MLLMs的两个核心挑战：潜在视觉对齐的高计算成本与对潜在嵌入监督不足，并通过三阶段基于蒸馏的监督微调流程予以解决。进一步发现GRPO在潜在推理中的局限：其主要增强文本推理而非潜在推理。为此提出VLPO（视觉潜在策略优化），一种将潜在嵌入显式纳入策略梯度更新的强化学习方法。为支持监督微调，我们构建了包含12.5万条真实世界图表、OCR和几何思维链的高质量图文交错数据集Monet-SFT-125K。我们的Monet-7B模型在真实世界感知与推理基准测试中表现持续提升，并在具有挑战性的抽象视觉推理任务上展现出强大的分布外泛化能力。我们通过实证分析各训练组件的作用，并讨论早期失败尝试，为视觉潜在推理的未来发展提供见解。模型、数据与代码已开源：https://github.com/NOVAglow646/Monet。",
    "url": "https://huggingface.co/papers/2511.21395",
    "arxiv_url": "https://arxiv.org/abs/2511.21395"
  },
  {
    "title": "Terminal Velocity Matching",
    "summary": "We propose Terminal Velocity Matching (TVM), a generalization of flow matching that enables high-fidelity one- and few-step generative modeling. TVM models the transition between any two diffusion timesteps and regularizes its behavior at its terminal time rather than at the initial time. We prove that TVM provides an upper bound on the 2-Wasserstein distance between data and model distributions when the model is Lipschitz continuous. However, since Diffusion Transformers lack this property, we introduce minimal architectural changes that achieve stable, single-stage training. To make TVM efficient in practice, we develop a fused attention kernel that supports backward passes on Jacobian-Vector Products, which scale well with transformer architectures. On ImageNet-256x256, TVM achieves 3.29 FID with a single function evaluation (NFE) and 1.99 FID with 4 NFEs. It similarly achieves 4.32 1-NFE FID and 2.94 4-NFE FID on ImageNet-512x512, representing state-of-the-art performance for one/few-step models from scratch.",
    "translation": "标题：终端速度匹配\n\n摘要：本文提出终端速度匹配（TVM）方法，作为流匹配的泛化形式，能够实现高保真度的单步及少步生成建模。TVM模拟任意两个扩散时间步之间的过渡过程，并在终止时刻而非初始时刻对其行为进行正则化处理。我们证明当模型满足Lipschitz连续性时，TVM可为数据分布与模型分布之间的2-Wasserstein距离提供上界。鉴于扩散变换器不具备该性质，我们引入最小化架构调整以实现稳定的单阶段训练。为提升TVM实际效率，我们开发了融合注意力核函数，支持在雅可比-向量积上进行反向传播，该机制与变换器架构具有良好的扩展适配性。在ImageNet-256x256数据集上，TVM以单次函数评估（NFE）取得3.29 FID，4次NFE取得1.99 FID；在ImageNet-512x512数据集上分别实现4.32（1-NFE）和2.94（4-NFE）的FID成绩，这代表了从零开始训练的单步/少步模型达到的最先进性能。",
    "url": "https://huggingface.co/papers/2511.19797",
    "arxiv_url": "https://arxiv.org/abs/2511.19797"
  },
  {
    "title": "UniGame: Turning a Unified Multimodal Model Into Its Own Adversary",
    "summary": "Unified Multimodal Models (UMMs) have shown impressive performance in both understanding and generation with a single architecture. However, UMMs still exhibit a fundamental inconsistency: understanding favors compact embeddings, whereas generation favors reconstruction-rich representations. This structural trade-off produces misaligned decision boundaries, degraded cross-modal coherence, and heightened vulnerability under distributional and adversarial shifts. In this paper, we present UniGame, a self-adversarial post-training framework that directly targets the inconsistencies. By applying a lightweight perturber at the shared token interface, UniGame enables the generation branch to actively seek and challenge fragile understanding, turning the model itself into its own adversary. Experiments demonstrate that UniGame significantly improves the consistency (+4.6%). Moreover, it also achieves substantial improvements in understanding (+3.6%), generation (+0.02), out-of-distribution and adversarial robustness (+4.8% and +6.2% on NaturalBench and AdVQA). The framework is architecture-agnostic, introduces less than 1% additional parameters, and is complementary to existing post-training methods. These results position adversarial self-play as a general and effective principle for enhancing the coherence, stability, and unified competence of future multimodal foundation models. The official code is available at: https://github.com/AIFrontierLab/UniGame",
    "translation": "标题：UniGame：将统一多模态模型转化为自身对抗者\n\n摘要：统一多模态模型（UMMs）通过单一架构在理解与生成任务中均展现出卓越性能。然而这类模型仍存在根本性不一致问题：理解任务偏好紧凑嵌入表示，而生成任务需要重建丰富的表征。这种结构性权衡导致决策边界失准、跨模态连贯性下降，以及在分布偏移和对抗攻击下的脆弱性加剧。本文提出UniGame——一种针对不一致性问题的自对抗后训练框架。通过在共享令牌接口施加轻量化扰动器，该框架使生成分支能够主动探寻并挑战脆弱理解环节，将模型自身转化为其对抗者。实验表明，UniGame显著提升模型一致性（+4.6%），同时在理解任务（+3.6%）、生成质量（+0.02）以及分布外鲁棒性（NaturalBench +4.8%）与对抗鲁棒性（AdVQA +6.2%）方面取得显著提升。该框架具备架构无关性，仅增加不足1%的参数量，且可与现有后训练方法形成互补。这些成果确立了对抗自博弈作为增强未来多模态基础模型的连贯性、稳定性与统一能力的普适性原则。项目代码已开源：https://github.com/AIFrontierLab/UniGame",
    "url": "https://huggingface.co/papers/2511.19413",
    "arxiv_url": "https://arxiv.org/abs/2511.19413"
  },
  {
    "title": "G^2VLM: Geometry Grounded Vision Language Model with Unified 3D Reconstruction and Spatial Reasoning",
    "summary": "Vision-Language Models (VLMs) still lack robustness in spatial intelligence, demonstrating poor performance on spatial understanding and reasoning tasks. We attribute this gap to the absence of a visual geometry learning process capable of reconstructing 3D space from 2D images. We present G^2VLM, a geometry grounded vision-language model that bridges two fundamental aspects of spatial intelligence: spatial 3D reconstruction and spatial understanding. G^2VLM natively leverages learned 3D visual geometry features to directly predict 3D attributes and enhance spatial reasoning tasks via in-context learning and interleaved reasoning. Our unified design is highly scalable for spatial understanding: it trains on abundant multi-view image and video data, while simultaneously leveraging the benefits of 3D visual priors that are typically only derived from hard-to-collect annotations. Experimental results demonstrate G^2VLM is proficient in both tasks, achieving comparable results to state-of-the-art feed-forward 3D reconstruction models and achieving better or competitive results across spatial understanding and reasoning tasks. By unifying a semantically strong VLM with low-level 3D vision tasks, we hope G^2VLM can serve as a strong baseline for the community and unlock more future applications, such as 3D scene editing.",
    "translation": "标题：G^2VLM：基于几何基础的视觉语言模型——统一三维重建与空间推理的新范式\n\n摘要：当前视觉语言模型在空间智能方面仍存在明显不足，其在空间理解与推理任务中的表现亟待提升。我们认为这一缺陷源于缺乏能够从二维图像重建三维空间的视觉几何学习过程。本文提出G^2VLM这一基于几何基础的视觉语言模型，成功融合空间智能的两个核心维度：三维空间重建与空间理解。该模型通过原生利用学习得到的三维视觉几何特征，直接预测三维属性，并借助上下文学习与交叉推理机制增强空间推理任务。我们的统一架构在空间理解方面具有显著扩展性：既能利用海量多视角图像和视频数据进行训练，又可充分获取通常仅能通过难以采集的标注数据才能得到的三维视觉先验优势。实验结果表明，G^2VLM在两项任务中均表现优异，其三维重建效果与前沿的前馈式模型相当，在空间理解与推理任务中取得领先或具有竞争力的成绩。通过将语义强大的视觉语言模型与底层三维视觉任务相融合，我们期待G^2VLM能成为该领域的强基准，并为三维场景编辑等未来应用开启新的可能。",
    "url": "https://huggingface.co/papers/2511.21688",
    "arxiv_url": "https://arxiv.org/abs/2511.21688"
  },
  {
    "title": "Block Cascading: Training Free Acceleration of Block-Causal Video Models",
    "summary": "Block-causal video generation faces a stark speed-quality trade-off: small 1.3B models manage only 16 FPS while large 14B models crawl at 4.5 FPS, forcing users to choose between responsiveness and quality. Block Cascading significantly mitigates this trade-off through training-free parallelization. Our key insight: future video blocks do not need fully denoised current blocks to begin generation. By starting block generation with partially denoised context from predecessors, we transform sequential pipelines into parallel cascades where multiple blocks denoise simultaneously. With 5 GPUs exploiting temporal parallelism, we achieve ~2x acceleration across all model scales: 1.3B models accelerate from 16 to 30 FPS, 14B models from 4.5 to 12.5 FPS. Beyond inference speed, Block Cascading eliminates overhead from KV-recaching (of ~200ms) during context switches for interactive generation. Extensive evaluations validated against multiple block-causal pipelines demonstrate no significant loss in generation quality when switching from block-causal to Block Cascading pipelines for inference. Project Page: https://hmrishavbandy.github.io/block_cascading_page/",
    "translation": "标题：块级联：无需训练的块因果视频模型加速方法\n\n摘要：块因果视频生成面临显著的速度-质量权衡困境：13亿参数的小型模型仅能达到16 FPS，而140亿参数的大型模型更是低至4.5 FPS，迫使用户在响应速度与生成质量之间做出取舍。块级联技术通过免训练的并行化方案显著缓解了这一矛盾。我们的核心发现是：后续视频块的生成无需等待当前块完全去噪完成。通过基于前驱块部分去噪的上下文信息启动新块生成，我们将串行处理流程转变为多块同时去噪的并行级联系统。借助5张GPU实现时序并行计算，所有模型规模均实现约2倍加速：13亿模型从16 FPS提升至30 FPS，140亿模型从4.5 FPS提升至12.5 FPS。除推理速度提升外，块级联还消除了交互式生成中上下文切换时KV重缓存带来的约200毫秒开销。针对多种块因果流程的广泛评估表明，从传统块因果推理切换到块级联推理时，生成质量未出现显著下降。项目页面：https://hmrishavbandy.github.io/block_cascading_page/",
    "url": "https://huggingface.co/papers/2511.20426",
    "arxiv_url": "https://arxiv.org/abs/2511.20426"
  },
  {
    "title": "MobileVLA-R1: Reinforcing Vision-Language-Action for Mobile Robots",
    "summary": "Grounding natural-language instructions into continuous control for quadruped robots remains a fundamental challenge in vision language action. Existing methods struggle to bridge high-level semantic reasoning and low-level actuation, leading to unstable grounding and weak generalization in the real world. To address these issues, we present MobileVLA-R1, a unified vision-language-action framework that enables explicit reasoning and continuous control for quadruped robots. We construct MobileVLA-CoT, a large-scale dataset of multi-granularity chain-of-thought (CoT) for embodied trajectories, providing structured reasoning supervision for alignment. Built upon this foundation, we introduce a two-stage training paradigm that combines supervised CoT alignment with GRPO reinforcement learning to enhance reasoning consistency, control stability, and long-horizon execution. Extensive evaluations on VLN and VLA tasks demonstrate superior performance over strong baselines, with approximately a 5% improvement. Real-world deployment on a quadruped robot validates robust performance in complex environments. Code: https://github.com/AIGeeksGroup/MobileVLA-R1. Website: https://aigeeksgroup.github.io/MobileVLA-R1.",
    "translation": "标题：MobileVLA-R1：强化移动机器人的视觉-语言-动作协同框架\n\n摘要：将自然语言指令落地实现为四足机器人的连续控制，始终是视觉-语言-动作领域的核心挑战。现有方法难以衔接高层语义推理与底层驱动控制，导致现实场景中的指令落地不稳定且泛化能力薄弱。为解决这些问题，我们提出MobileVLA-R1——一个支持四足机器人显式推理与连续控制的统一视觉-语言-动作框架。通过构建MobileVLA-CoT这一包含具身轨迹多粒度思维链的大规模数据集，为语义对齐提供结构化推理监督。在此基础上，我们引入结合监督式思维链对齐与GRPO强化学习的双阶段训练范式，显著提升推理一致性、控制稳定性和长周期任务执行能力。在VLN和VLA任务上的系统评估表明，本方法较现有基线模型实现约5%的性能提升。在四足机器人实体上的部署实验验证了其在复杂环境中的鲁棒表现。代码库：https://github.com/AIGeeksGroup/MobileVLA-R1 项目网站：https://aigeeksgroup.github.io/MobileVLA-R1",
    "url": "https://huggingface.co/papers/2511.17889",
    "arxiv_url": "https://arxiv.org/abs/2511.17889"
  },
  {
    "title": "Reinforcing Action Policies by Prophesying",
    "summary": "Vision-Language-Action (VLA) policies excel in aligning language, perception, and robot control. However, most VLAs are trained purely by imitation, which overfits to demonstrations, and is brittle under distribution shift. Reinforcement learning (RL) directly optimizes task reward and thus addresses this misalignment, but real-robot interaction is expensive and conventional simulators are hard to engineer and transfer. We address both data efficiency and optimization stability in VLA post-training via a learned world model and an RL procedure tailored to flow-based action heads. Specifically, we introduce Prophet, a unified action-to-video robot actuation pretrained across large-scale, heterogeneous robot data to learn reusable action-outcome dynamics. It is able to few-shot adapt to new robots, objects, and environments, yielding a rollout-ready simulator. Upon Prophet, we reinforce action policies with Flow-action-GRPO (FA-GRPO), which adapts Flow-GRPO to operate on VLA actions, and with FlowScale, a stepwise reweighting that rescales per-step gradients in the flow head. Together, Prophet, FA-GRPO, and FlowScale constitute ProphRL, a practical, data- and compute-efficient path to VLA post-training. Experiments show 5-17% success gains on public benchmarks and 24-30% gains on real robots across different VLA variants.",
    "translation": "标题：通过预测强化动作策略\n\n摘要：视觉-语言-动作策略在语言、感知与机器人控制的协同方面表现卓越。然而大多数VLA策略仅通过模仿学习进行训练，这种方法会对示范数据产生过拟合，且在分布偏移时表现脆弱。强化学习通过直接优化任务奖励来解决此类错位问题，但真实机器人交互成本高昂，传统模拟器又难以开发与迁移。我们通过构建学习型世界模型和针对流式动作头优化的强化学习流程，同步解决VLA后训练阶段的数据效率与优化稳定性问题。具体而言，我们提出Prophet模型——一种基于大规模异构机器人数据预训练的统一动作到视频机器人执行框架，可学习可重用的动作-结果动态关系。该框架能够快速适应新的机器人、物体及环境，形成可直接部署的模拟系统。基于Prophet，我们通过适配VLA动作的流式动作GRPO算法，以及通过FlowScale梯度重缩放技术对流式动作头进行逐步梯度调整，共同强化动作策略。Prophet模型、FA-GRPO算法与FlowScale技术共同构成ProphRL框架，为VLA后训练提供了实用且节约数据计算资源的解决方案。实验表明，该方案在公开基准测试中实现5-17%的成功率提升，在不同VLA变体的真实机器人测试中取得24-30%的性能增益。",
    "url": "https://huggingface.co/papers/2511.20633",
    "arxiv_url": "https://arxiv.org/abs/2511.20633"
  },
  {
    "title": "Image-Free Timestep Distillation via Continuous-Time Consistency with Trajectory-Sampled Pairs",
    "summary": "Timestep distillation is an effective approach for improving the generation efficiency of diffusion models. The Consistency Model (CM), as a trajectory-based framework, demonstrates significant potential due to its strong theoretical foundation and high-quality few-step generation. Nevertheless, current continuous-time consistency distillation methods still rely heavily on training data and computational resources, hindering their deployment in resource-constrained scenarios and limiting their scalability to diverse domains. To address this issue, we propose Trajectory-Backward Consistency Model (TBCM), which eliminates the dependence on external training data by extracting latent representations directly from the teacher model's generation trajectory. Unlike conventional methods that require VAE encoding and large-scale datasets, our self-contained distillation paradigm significantly improves both efficiency and simplicity. Moreover, the trajectory-extracted samples naturally bridge the distribution gap between training and inference, thereby enabling more effective knowledge transfer. Empirically, TBCM achieves 6.52 FID and 28.08 CLIP scores on MJHQ-30k under one-step generation, while reducing training time by approximately 40% compared to Sana-Sprint and saving a substantial amount of GPU memory, demonstrating superior efficiency without sacrificing quality. We further reveal the diffusion-generation space discrepancy in continuous-time consistency distillation and analyze how sampling strategies affect distillation performance, offering insights for future distillation research. GitHub Link: https://github.com/hustvl/TBCM.",
    "translation": "标题：基于轨迹采样对连续时间一致性的免图像时间步蒸馏方法\n\n摘要：时间步蒸馏是提升扩散模型生成效率的有效途径。一致性模型作为基于轨迹的框架，凭借其坚实的理论基础和高质量少步生成能力展现出巨大潜力。然而，当前连续时间一致性蒸馏方法仍严重依赖训练数据和计算资源，制约了其在资源受限场景的部署及跨领域扩展能力。针对此问题，我们提出轨迹反向一致性模型（TBCM），通过直接从教师模型生成轨迹中提取潜在表征，消除对外部训练数据的依赖。与需要VAE编码和大规模数据集的传统方法不同，我们的自包含蒸馏范式显著提升了效率与简洁性。此外，轨迹提取样本天然弥合了训练与推理间的分布差异，从而实现更有效的知识迁移。实验表明，在单步生成条件下，TBCM在MJHQ-30k数据集上达到6.52 FID和28.08 CLIP得分，相较Sana-Sprint缩短约40%训练时间并显著节省GPU显存，在保持质量的同时展现出卓越效率。我们进一步揭示了连续时间一致性蒸馏中的扩散-生成空间差异，分析了采样策略对蒸馏性能的影响，为未来蒸馏研究提供重要参考。项目地址：https://github.com/hustvl/TBCM。",
    "url": "https://huggingface.co/papers/2511.20410",
    "arxiv_url": "https://arxiv.org/abs/2511.20410"
  },
  {
    "title": "SPHINX: A Synthetic Environment for Visual Perception and Reasoning",
    "summary": "We present Sphinx, a synthetic environment for visual perception and reasoning that targets core cognitive primitives. Sphinx procedurally generates puzzles using motifs, tiles, charts, icons, and geometric primitives, each paired with verifiable ground-truth solutions, enabling both precise evaluation and large-scale dataset construction. The benchmark covers 25 task types spanning symmetry detection, geometric transformations, spatial reasoning, chart interpretation, and sequence prediction. Evaluating recent large vision-language models (LVLMs) shows that even state-of-the-art GPT-5 attains only 51.1% accuracy, well below human performance. Finally, we demonstrate that reinforcement learning with verifiable rewards (RLVR) substantially improves model accuracy on these tasks and yields gains on external visual reasoning benchmarks, highlighting its promise for advancing multimodal reasoning.",
    "translation": "标题：SPHINX：面向视觉感知与推理的合成环境\n\n摘要：本文提出Sphinx——一个针对核心认知基元的视觉感知与推理合成环境。该系统通过程序化生成包含纹样、图块、图表、图标及几何基元的谜题，每个谜题均配有可验证的基准解，既能实现精准评估又可支持大规模数据集构建。该基准测试涵盖对称检测、几何变换、空间推理、图表解读与序列预测等25类任务。对近期大尺度视觉语言模型的评估表明，即便最先进的GPT-5模型准确率也仅为51.1%，远低于人类表现。最后我们验证了基于可验证奖励的强化学习能显著提升模型在这些任务上的准确率，并在外部视觉推理基准测试中产生增益，凸显了该方法对推进多模态推理发展的潜力。",
    "url": "https://huggingface.co/papers/2511.20814",
    "arxiv_url": "https://arxiv.org/abs/2511.20814"
  },
  {
    "title": "Position: The Complexity of Perfect AI Alignment -- Formalizing the RLHF Trilemma",
    "summary": "Reinforcement Learning from Human Feedback (RLHF) is widely used for aligning large language models, yet practitioners face a persistent puzzle: improving safety often reduces fairness, scaling to diverse populations becomes computationally intractable, and making systems robust often amplifies majority biases. We formalize this tension as the Alignment Trilemma: no RLHF system can simultaneously achieve (i) epsilon-representativeness across diverse human values, (ii) polynomial tractability in sample and compute complexity, and (iii) delta-robustness against adversarial perturbations and distribution shift. Through a complexity-theoretic analysis integrating statistical learning theory and robust optimization, we prove that achieving both representativeness (epsilon <= 0.01) and robustness (delta <= 0.001) for global-scale populations requires Omega(2^{d_context}) operations, which is super-polynomial in the context dimensionality. We show that current RLHF implementations resolve this trilemma by sacrificing representativeness: they collect only 10^3--10^4 samples from homogeneous annotator pools while 10^7--10^8 samples are needed for true global representation. Our framework provides a unified explanation for documented RLHF pathologies including preference collapse, sycophancy, and systematic bias amplification. We conclude with concrete directions for navigating these fundamental trade-offs through strategic relaxations of alignment requirements.",
    "translation": "标题：立场声明：完美AI对齐的复杂性——形式化RLHF三难困境\n\n摘要：基于人类反馈的强化学习（RLHF）被广泛用于对齐大型语言模型，但实践者始终面临一个难题：提升安全性往往损害公平性，扩展到多样化群体时计算复杂度难以处理，增强系统鲁棒性又会放大主流偏见。我们将其形式化为对齐三难困境：任何RLHF系统都无法同时实现（i）跨多元人类价值观的ε代表性，（ii）样本与计算复杂度的多项式可处理性，以及（iii）对抗性扰动与分布偏移的δ鲁棒性。通过融合统计学习理论与鲁棒优化的计算复杂性分析，我们证明要实现全球规模群体的代表性（ε≤0.01）与鲁棒性（δ≤0.001）需要Ω(2^{d_context})量级运算，这在上下文维度上呈超多项式增长。研究表明当前RLHF实施方案通过牺牲代表性来解决该困境：仅从同质化标注群体收集10^3-10^4样本，而真实全球代表性需要10^7-10^8样本。我们的框架为已记录的RLHF缺陷（包括偏好坍缩、谄媚效应和系统性偏见放大）提供了统一解释。最后提出通过策略性放宽对齐要求来应对这些根本性权衡的具体方向。",
    "url": "https://huggingface.co/papers/2511.19504",
    "arxiv_url": "https://arxiv.org/abs/2511.19504"
  },
  {
    "title": "NAF: Zero-Shot Feature Upsampling via Neighborhood Attention Filtering",
    "summary": "Vision Foundation Models (VFMs) extract spatially downsampled representations, posing challenges for pixel-level tasks. Existing upsampling approaches face a fundamental trade-off: classical filters are fast and broadly applicable but rely on fixed forms, while modern upsamplers achieve superior accuracy through learnable, VFM-specific forms at the cost of retraining for each VFM. We introduce Neighborhood Attention Filtering (NAF), which bridges this gap by learning adaptive spatial-and-content weights through Cross-Scale Neighborhood Attention and Rotary Position Embeddings (RoPE), guided solely by the high-resolution input image. NAF operates zero-shot: it upsamples features from any VFM without retraining, making it the first VFM-agnostic architecture to outperform VFM-specific upsamplers and achieve state-of-the-art performance across multiple downstream tasks. It maintains high efficiency, scaling to 2K feature maps and reconstructing intermediate-resolution maps at 18 FPS. Beyond feature upsampling, NAF demonstrates strong performance on image restoration, highlighting its versatility. Code and checkpoints are available at https://github.com/valeoai/NAF.",
    "translation": "标题：NAF：基于邻域注意力滤波的零样本特征上采样方法\n\n摘要：视觉基础模型提取的空间降采样表征为像素级任务带来挑战。现有上采样方法面临根本性权衡：经典滤波器虽快速通用但依赖固定形式，而现代上采样器通过可学习的VFM专用形式实现更高精度，却需为每个VFM重新训练。我们提出邻域注意力滤波方法，通过跨尺度邻域注意力与旋转位置编码，仅以高分辨率输入图像为指导学习自适应空间-内容权重，成功弥合这一差距。NAF具备零样本特性：无需重新训练即可对任意VFM的特征进行上采样，成为首个在多个下游任务中超越VFM专用上采样器并达到最先进性能的VFM无关架构。该方法保持高效性，可扩展至2K特征图并以18FPS速率重建中间分辨率图。除特征上采样外，NAF在图像复原任务中也展现出卓越性能，体现了其多功能的特性。代码与检查点已发布于https://github.com/valeoai/NAF。",
    "url": "https://huggingface.co/papers/2511.18452",
    "arxiv_url": "https://arxiv.org/abs/2511.18452"
  },
  {
    "title": "RAISECity: A Multimodal Agent Framework for Reality-Aligned 3D World Generation at City-Scale",
    "summary": "City-scale 3D generation is of great importance for the development of embodied intelligence and world models. Existing methods, however, face significant challenges regarding quality, fidelity, and scalability in 3D world generation. Thus, we propose RAISECity, a Reality-Aligned Intelligent Synthesis Engine that creates detailed, City-scale 3D worlds. We introduce an agentic framework that leverages diverse multimodal foundation tools to acquire real-world knowledge, maintain robust intermediate representations, and construct complex 3D scenes. This agentic design, featuring dynamic data processing, iterative self-reflection and refinement, and the invocation of advanced multimodal tools, minimizes cumulative errors and enhances overall performance. Extensive quantitative experiments and qualitative analyses validate the superior performance of RAISECity in real-world alignment, shape precision, texture fidelity, and aesthetics level, achieving over a 90% win-rate against existing baselines for overall perceptual quality. This combination of 3D quality, reality alignment, scalability, and seamless compatibility with computer graphics pipelines makes RAISECity a promising foundation for applications in immersive media, embodied intelligence, and world models.",
    "translation": "标题：RAISECity：面向城市级现实对齐三维世界生成的多模态智能体框架\n\n摘要：城市级三维生成对于具身智能与世界模型的发展具有重要意义。然而现有方法在三维世界生成的质量、保真度与可扩展性方面面临重大挑战。为此，我们提出RAISECity——一个能够创建精细化城市级三维世界的现实对齐智能合成引擎。我们引入了一种智能体框架，通过利用多样化多模态基础工具获取现实世界知识，维护鲁棒的中间表征，并构建复杂三维场景。该智能体设计具备动态数据处理、迭代式自反思优化及先进多模态工具调用等特性，能有效减少累积误差并提升整体性能。大量定量实验与定性分析表明，RAISECity在现实对齐度、几何精度、纹理保真度与美学水准方面均表现优异，在整体感知质量评估中以超过90%的胜率领先现有基线方法。这种集三维质量、现实对齐性、可扩展性及与计算机图形管线无缝兼容于一体的特性，使RAISECity成为沉浸式媒体、具身智能与世界模型应用的理想基础平台。",
    "url": "https://huggingface.co/papers/2511.18005",
    "arxiv_url": "https://arxiv.org/abs/2511.18005"
  },
  {
    "title": "I-GLIDE: Input Groups for Latent Health Indicators in Degradation Estimation",
    "summary": "Accurate remaining useful life (RUL) prediction hinges on the quality of health indicators (HIs), yet existing methods often fail to disentangle complex degradation mechanisms in multi-sensor systems or quantify uncertainty in HI reliability. This paper introduces a novel framework for HI construction, advancing three key contributions. First, we adapt Reconstruction along Projected Pathways (RaPP) as a health indicator (HI) for RUL prediction for the first time, showing that it outperforms traditional reconstruction error metrics. Second, we show that augmenting RaPP-derived HIs with aleatoric and epistemic uncertainty quantification (UQ) via Monte Carlo dropout and probabilistic latent spaces- significantly improves RUL-prediction robustness. Third, and most critically, we propose indicator groups, a paradigm that isolates sensor subsets to model system-specific degradations, giving rise to our novel method, I-GLIDE which enables interpretable, mechanism-specific diagnostics. Evaluated on data sourced from aerospace and manufacturing systems, our approach achieves marked improvements in accuracy and generalizability compared to state-of-the-art HI methods while providing actionable insights into system failure pathways. This work bridges the gap between anomaly detection and prognostics, offering a principled framework for uncertainty-aware degradation modeling in complex systems.",
    "translation": "标题：I-GLIDE：退化估计中潜在健康指标的输入分组方法\n\n摘要：准确的剩余使用寿命预测取决于健康指标的质量，然而现有方法往往难以解耦多传感器系统中的复杂退化机制，也无法量化健康指标可靠性的不确定性。本文提出了一种创新的健康指标构建框架，具有三个关键贡献：首先，我们首次将投影路径重构方法改造为适用于剩余使用寿命预测的健康指标，证明其性能优于传统重构误差度量；其次，通过蒙特卡洛丢弃法和概率潜在空间实现认知与随机不确定性量化，显著增强了基于投影路径重构的健康指标在剩余使用寿命预测中的鲁棒性；最后，我们提出指标分组这一创新范式，通过分离传感器子集来建模系统特定退化机制，由此形成名为I-GLIDE的新方法，可实现可解释的机制特异性诊断。在航空航天与制造系统数据上的实验表明，相较于最先进的健康指标方法，本方案在预测精度与泛化能力方面均取得显著提升，同时为系统失效路径提供了可操作的洞见。该研究填补了异常检测与预测性维护之间的空白，为复杂系统中的不确定性感知退化建模提供了理论框架。",
    "url": "https://huggingface.co/papers/2511.21208",
    "arxiv_url": "https://arxiv.org/abs/2511.21208"
  },
  {
    "title": "Frequency-Adaptive Sharpness Regularization for Improving 3D Gaussian Splatting Generalization",
    "summary": "Despite 3D Gaussian Splatting (3DGS) excelling in most configurations, it lacks generalization across novel viewpoints in a few-shot scenario because it overfits to the sparse observations. We revisit 3DGS optimization from a machine learning perspective, framing novel view synthesis as a generalization problem to unseen viewpoints-an underexplored direction. We propose Frequency-Adaptive Sharpness Regularization (FASR), which reformulates the 3DGS training objective, thereby guiding 3DGS to converge toward a better generalization solution. Although Sharpness-Aware Minimization (SAM) similarly reduces the sharpness of the loss landscape to improve generalization of classification models, directly employing it to 3DGS is suboptimal due to the discrepancy between the tasks. Specifically, it hinders reconstructing high-frequency details due to excessive regularization, while reducing its strength leads to under-penalizing sharpness. To address this, we reflect the local frequency of images to set the regularization weight and the neighborhood radius when estimating the local sharpness. It prevents floater artifacts in novel viewpoints and reconstructs fine details that SAM tends to oversmooth. Across datasets with various configurations, our method consistently improves a wide range of baselines. Code will be available at https://bbangsik13.github.io/FASR.",
    "translation": "标题：基于频率自适应锐度正则化的三维高斯溅射泛化能力提升方法\n\n摘要：尽管三维高斯溅射（3DGS）在多数配置中表现卓越，但在稀疏观测场景下由于对有限样本的过拟合，其在新视角下的泛化能力存在不足。本文从机器学习视角重新审视3DGS优化过程，将新视角合成问题定义为对未观测视角的泛化问题——这一研究方向尚未得到充分探索。我们提出频率自适应锐度正则化方法（FASR），通过重构3DGS训练目标函数，引导3DGS收敛至具有更优泛化能力的解。虽然锐度感知最小化（SAM）方法同样通过降低损失景观的锐度来提升分类模型的泛化能力，但由于任务差异，直接将其应用于3DGS会导致次优结果。具体而言，过强的正则化会阻碍高频细节重建，而减弱正则化强度又会导致锐度惩罚不足。为解决该问题，我们根据图像局部频率特性动态设置正则化权重及局部锐度估计的邻域半径。该方法既能有效抑制新视角下的浮游伪影，又能重建SAM方法容易过度平滑的精细细节。在多种配置的数据集测试中，本方法持续提升了各类基线的性能表现。代码将在https://bbangsik13.github.io/FASR 发布。",
    "url": "https://huggingface.co/papers/2511.17918",
    "arxiv_url": "https://arxiv.org/abs/2511.17918"
  }
]