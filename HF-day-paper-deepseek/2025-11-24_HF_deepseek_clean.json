[
  {
    "title": "OpenMMReasoner: Pushing the Frontiers for Multimodal Reasoning with an Open and General Recipe",
    "summary": "Recent advancements in large reasoning models have fueled growing interest in extending such capabilities to multimodal domains. However, despite notable progress in visual reasoning, the lack of transparent and reproducible data curation and training strategies remains a major barrier to scalable research. In this work, we introduce OpenMMReasoner, a fully transparent two-stage recipe for multimodal reasoning spanning supervised fine-tuning (SFT) and reinforcement learning (RL). In the SFT stage, we construct an 874K-sample cold-start dataset with rigorous step-by-step validation, providing a strong foundation for reasoning capabilities. The subsequent RL stage leverages a 74K-sample dataset across diverse domains to further sharpen and stabilize these abilities, resulting in a more robust and efficient learning process. Extensive evaluations demonstrate that our training recipe not only surpasses strong baselines but also highlights the critical role of data quality and training design in shaping multimodal reasoning performance. Notably, our method achieves a 11.6% improvement over the Qwen2.5-VL-7B-Instruct baseline across nine multimodal reasoning benchmarks, establishing a solid empirical foundation for future large-scale multimodal reasoning research. We open-sourced all our codes, pipeline, and data at https://github.com/EvolvingLMMs-Lab/OpenMMReasoner.",
    "translation": "标题：OpenMMReasoner：以开放通用方案推动多模态推理前沿研究\n\n摘要：大型推理模型的最新进展推动着研究者将此类能力扩展至多模态领域的兴趣日益增长。然而，尽管视觉推理领域取得了显著进步，但缺乏透明可复现的数据构建与训练策略仍是制约规模化研究的主要障碍。本研究提出OpenMMReasoner——一个完全透明的双阶段多模态推理方案，涵盖监督微调（SFT）与强化学习（RL）两个阶段。在SFT阶段，我们构建了包含87.4万样本的冷启动数据集，并通过严格的逐步骤验证，为推理能力奠定坚实基础。随后的RL阶段利用跨多个领域的7.4万样本数据集进一步强化和稳定这些能力，从而实现更鲁棒高效的学习过程。大量实验评估表明，我们的训练方案不仅超越了强基线模型，更凸显了数据质量和训练设计对塑造多模态推理性能的关键作用。值得注意的是，在九大多模态推理基准测试中，我们的方法相较Qwen2.5-VL-7B-Instruct基线实现了11.6%的性能提升，为未来大规模多模态推理研究奠定了坚实的实证基础。我们已在https://github.com/EvolvingLMMs-Lab/OpenMMReasoner 开源全部代码、训练流程及数据。",
    "url": "https://huggingface.co/papers/2511.16334",
    "arxiv_url": "https://arxiv.org/abs/2511.16334"
  },
  {
    "title": "Unveiling Intrinsic Dimension of Texts: from Academic Abstract to Creative Story",
    "summary": "Intrinsic dimension (ID) is an important tool in modern LLM analysis, informing studies of training dynamics, scaling behavior, and dataset structure, yet its textual determinants remain underexplored. We provide the first comprehensive study grounding ID in interpretable text properties through cross-encoder analysis, linguistic features, and sparse autoencoders (SAEs). In this work, we establish three key findings. First, ID is complementary to entropy-based metrics: after controlling for length, the two are uncorrelated, with ID capturing geometric complexity orthogonal to prediction quality. Second, ID exhibits robust genre stratification: scientific prose shows low ID (~8), encyclopedic content medium ID (~9), and creative/opinion writing high ID (~10.5) across all models tested. This reveals that contemporary LLMs find scientific text \"representationally simple\" while fiction requires additional degrees of freedom. Third, using SAEs, we identify causal features: scientific signals (formal tone, report templates, statistics) reduce ID; humanized signals (personalization, emotion, narrative) increase it. Steering experiments confirm these effects are causal. Thus, for contemporary models, scientific writing appears comparatively \"easy\", whereas fiction, opinion, and affect add representational degrees of freedom. Our multi-faceted analysis provides practical guidance for the proper use of ID and the sound interpretation of ID-based results.",
    "translation": "标题：揭示文本本征维度：从学术摘要到创意故事\n\n摘要：本征维度作为现代大语言模型分析的重要工具，为训练动态、缩放规律及数据集结构的研究提供了关键洞见，但其文本决定因素尚未得到充分探索。我们通过交叉编码器分析、语言特征提取和稀疏自编码器，首次建立了本征维度与可解释文本特性之间的系统关联。本研究获得三项核心发现：首先，本征维度与基于熵的指标形成互补——在控制文本长度后，二者无显著相关性，表明本征维度捕捉的是独立于预测质量的几何复杂性。其次，本征维度呈现稳健的体裁分层：在所有测试模型中，科学论述呈现低维度（约8），百科类内容居中（约9），而创意/观点性写作则显示高维度（约10.5）。这表明当代大语言模型将科学文本视为\"表征简单\"的范畴，而虚构文学则需要更多表征自由度。第三，通过稀疏自编码器识别出因果特征：科学信号（正式语体、报告模板、统计数据）降低维度，人性化信号（个性化表达、情感元素、叙事结构）则提升维度。定向调控实验证实了这些影响的因果性。因此对当代模型而言，科学写作相对\"简单\"，而虚构文学、观点表达及情感内容则增加了表征自由度。我们的多维度分析为正确运用本征维度及合理解读基于本征维度的研究结果提供了实践指导。",
    "url": "https://huggingface.co/papers/2511.15210",
    "arxiv_url": "https://arxiv.org/abs/2511.15210"
  },
  {
    "title": "GeoVista: Web-Augmented Agentic Visual Reasoning for Geolocalization",
    "summary": "Current research on agentic visual reasoning enables deep multimodal understanding but primarily focuses on image manipulation tools, leaving a gap toward more general-purpose agentic models. In this work, we revisit the geolocalization task, which requires not only nuanced visual grounding but also web search to confirm or refine hypotheses during reasoning. Since existing geolocalization benchmarks fail to meet the need for high-resolution imagery and the localization challenge for deep agentic reasoning, we curate GeoBench, a benchmark that includes photos and panoramas from around the world, along with a subset of satellite images of different cities to rigorously evaluate the geolocalization ability of agentic models. We also propose GeoVista, an agentic model that seamlessly integrates tool invocation within the reasoning loop, including an image-zoom-in tool to magnify regions of interest and a web-search tool to retrieve related web information. We develop a complete training pipeline for it, including a cold-start supervised fine-tuning (SFT) stage to learn reasoning patterns and tool-use priors, followed by a reinforcement learning (RL) stage to further enhance reasoning ability. We adopt a hierarchical reward to leverage multi-level geographical information and improve overall geolocalization performance. Experimental results show that GeoVista surpasses other open-source agentic models on the geolocalization task greatly and achieves performance comparable to closed-source models such as Gemini-2.5-flash and GPT-5 on most metrics.",
    "translation": "标题：GeoVista：面向地理定位的增强型网络代理视觉推理系统\n\n摘要：当前代理视觉推理研究虽能实现深度多模态理解，但主要聚焦于图像处理工具，在通用型代理模型开发方面仍存不足。本研究重新审视地理定位任务，该任务不仅需要精细的视觉定位能力，还需借助网络搜索在推理过程中验证或修正假设。针对现有地理定位基准数据集无法满足高分辨率图像需求及深度代理推理定位挑战的问题，我们构建了GeoBench基准数据集，包含全球范围的静态照片与全景图像，以及不同城市的卫星图像子集，用以系统评估代理模型的地理定位能力。我们提出GeoVista模型，该代理系统将工具调用无缝集成至推理循环，包含用于放大感兴趣区域的图像缩放工具和获取关联网络信息的搜索工具。我们开发了完整的训练流程：首先通过冷启动监督微调阶段学习推理模式与工具使用先验，继而通过强化学习阶段进一步提升推理能力。采用分层奖励机制以利用多层级地理信息，显著提升整体定位性能。实验结果表明，GeoVista在地理定位任务上大幅超越其他开源代理模型，在多数指标上达到与Gemini-2.5-flash、GPT-5等闭源模型相当的性能水平。",
    "url": "https://huggingface.co/papers/2511.15705",
    "arxiv_url": "https://arxiv.org/abs/2511.15705"
  },
  {
    "title": "SAM 3: Segment Anything with Concepts",
    "summary": "We present Segment Anything Model (SAM) 3, a unified model that detects, segments, and tracks objects in images and videos based on concept prompts, which we define as either short noun phrases (e.g., \"yellow school bus\"), image exemplars, or a combination of both. Promptable Concept Segmentation (PCS) takes such prompts and returns segmentation masks and unique identities for all matching object instances. To advance PCS, we build a scalable data engine that produces a high-quality dataset with 4M unique concept labels, including hard negatives, across images and videos. Our model consists of an image-level detector and a memory-based video tracker that share a single backbone. Recognition and localization are decoupled with a presence head, which boosts detection accuracy. SAM 3 doubles the accuracy of existing systems in both image and video PCS, and improves previous SAM capabilities on visual segmentation tasks. We open source SAM 3 along with our new Segment Anything with Concepts (SA-Co) benchmark for promptable concept segmentation.",
    "translation": "标题：SAM 3：基于概念的可提示分割模型\n\n摘要：本文提出可提示分割模型（SAM）第三代，这是一个基于概念提示实现图像与视频中目标检测、分割与跟踪的统一模型。概念提示定义为简短名词短语（如“黄色校车”）、示例图像或二者组合。可提示概念分割（PCS）接收此类提示后，可为所有匹配目标实例返回分割掩码与唯一标识。为推进PCS技术发展，我们构建了可扩展数据引擎，生成包含400万独特概念标签的高质量数据集，涵盖图像与视频场景中的困难负样本。该模型由共享主干网络的图像级检测器与基于记忆机制的视频跟踪器构成，通过解耦识别与定位的存现度检测头提升检测精度。实验表明，SAM 3在图像与视频PCS任务中的准确率较现有系统提升一倍，并改进了前代SAM在视觉分割任务中的性能。我们同步开源SAM 3模型及配套构建的全新基准数据集SA-Co，用于可提示概念分割研究。",
    "url": "https://huggingface.co/papers/2511.16719",
    "arxiv_url": "https://arxiv.org/abs/2511.16719"
  },
  {
    "title": "O-Mem: Omni Memory System for Personalized, Long Horizon, Self-Evolving Agents",
    "summary": "Recent advancements in LLM-powered agents have demonstrated significant potential in generating human-like responses; however, they continue to face challenges in maintaining long-term interactions within complex environments, primarily due to limitations in contextual consistency and dynamic personalization. Existing memory systems often depend on semantic grouping prior to retrieval, which can overlook semantically irrelevant yet critical user information and introduce retrieval noise. In this report, we propose the initial design of O-Mem, a novel memory framework based on active user profiling that dynamically extracts and updates user characteristics and event records from their proactive interactions with agents. O-Mem supports hierarchical retrieval of persona attributes and topic-related context, enabling more adaptive and coherent personalized responses. O-Mem achieves 51.67% on the public LoCoMo benchmark, a nearly 3% improvement upon LangMem,the previous state-of-the-art, and it achieves 62.99% on PERSONAMEM, a 3.5% improvement upon A-Mem,the previous state-of-the-art. O-Mem also boosts token and interaction response time efficiency compared to previous memory frameworks. Our work opens up promising directions for developing efficient and human-like personalized AI assistants in the future.",
    "translation": "标题：O-Mem：面向个性化长周期自演进智能体的全域记忆系统\n\n摘要：基于大语言模型的智能体近期取得的进展在生成类人响应方面展现出巨大潜力，然而在复杂环境中维持长期交互时仍面临挑战，主要源于上下文一致性与动态个性化能力的局限。现有记忆系统通常在检索前依赖语义分组，这种方法可能遗漏语义无关但关键的用户信息，并引入检索噪声。本报告提出O-Mem的初步设计框架，该创新记忆系统基于动态用户画像，通过智能体与用户的主动交互实时提取并更新用户特征与事件记录。O-Mem支持人物属性与主题相关语境的分层检索，从而实现更具适应性与连贯性的个性化响应。在公开基准测试中，O-Mem在LoCoMo上达到51.67%的准确率，较先前最优系统LangMem提升近3%；在PERSONAMEM上取得62.99%的准确率，较前最优系统A-Mem提升3.5%。与既有记忆框架相比，O-Mem还显著提升了令牌处理与交互响应的时间效率。本研究为开发高效且具人类特质的个性化人工智能助手开辟了新的研究方向。",
    "url": "https://huggingface.co/papers/2511.13593",
    "arxiv_url": "https://arxiv.org/abs/2511.13593"
  },
  {
    "title": "RynnVLA-002: A Unified Vision-Language-Action and World Model",
    "summary": "We introduce RynnVLA-002, a unified Vision-Language-Action (VLA) and world model. The world model leverages action and visual inputs to predict future image states, learning the underlying physics of the environment to refine action generation. Conversely, the VLA model produces subsequent actions from image observations, enhancing visual understanding and supporting the world model's image generation. The unified framework of RynnVLA-002 enables joint learning of environmental dynamics and action planning. Our experiments show that RynnVLA-002 surpasses individual VLA and world models, demonstrating their mutual enhancement. We evaluate RynnVLA-002 in both simulation and real-world robot tasks. RynnVLA-002 achieves 97.4% success rate on the LIBERO simulation benchmark without pretraining, while in real-world LeRobot experiments, its integrated world model boosts the overall success rate by 50%.",
    "translation": "标题：RynnVLA-002：统一视觉-语言-行动与世界模型\n\n摘要：本文提出RynnVLA-002，一种统一的视觉-语言-行动（VLA）与世界模型。该世界模型通过行动与视觉输入预测未来图像状态，学习环境底层物理规律以优化行动生成。相应地，VLA模型通过图像观测生成后续行动，增强视觉理解能力并支持世界模型的图像生成。RynnVLA-002的统一框架实现了环境动态特性与行动规划的联合学习。实验表明，RynnVLA-002超越了独立的VLA和世界模型，展现出二者的协同增强效应。我们在仿真与真实机器人任务中对该模型进行评估：在LIBERO仿真基准测试中，RynnVLA-002在未经预训练的情况下取得97.4%的成功率；在LeRobot真实场景实验中，其集成世界模型使整体成功率提升50%。",
    "url": "https://huggingface.co/papers/2511.17502",
    "arxiv_url": "https://arxiv.org/abs/2511.17502"
  },
  {
    "title": "Parrot: Persuasion and Agreement Robustness Rating of Output Truth -- A Sycophancy Robustness Benchmark for LLMs",
    "summary": "This study presents PARROT (Persuasion and Agreement Robustness Rating of Output Truth), a robustness focused framework designed to measure the degradation in accuracy that occurs under social pressure exerted on users through authority and persuasion in large language models (LLMs) the phenomenon of sycophancy (excessive conformity). PARROT (i) isolates causal effects by comparing the neutral version of the same question with an authoritatively false version using a double-blind evaluation, (ii) quantifies confidence shifts toward the correct and imposed false responses using log-likelihood-based calibration tracking, and (iii) systematically classifies failure modes (e.g., robust correct, sycophantic agreement, reinforced error, stubborn error, self-correction, etc.) using an eight-state behavioral taxonomy. We evaluated 22 models using 1,302 MMLU-style multiple-choice questions across 13 domains and domain-specific authority templates. Findings show marked heterogeneity: advanced models (e.g., GPT-5, GPT-4.1, Claude Sonnet 4.5) exhibit low \"follow rates\" (leq 11%, GPT-5: 4\\%) and minimal accuracy loss, while older/smaller models show severe epistemic collapse (GPT-4: 80\\%, Qwen 2.5-1.5B: 94\\%). The danger is not limited to response changes; weak models reduce confidence in the correct response while increasing confidence in the imposed incorrect response. While international law and global knowledge at the domain level exhibit high fragility, elementary mathematics is relatively resilient. Consequently, we argue that the goal of \"resistance to overfitting pressure\" should be addressed as a primary objective alongside accuracy, harm avoidance, and privacy for safe deployment in the real world.",
    "translation": "标题：PARROT：输出真值的说服力与一致性鲁棒性评级——面向大语言模型的谄媚鲁棒性基准\n\n摘要：本研究提出PARROT（输出真值的说服力与一致性鲁棒性评级）框架，该框架以鲁棒性为核心，旨在衡量大语言模型在权威说服等社会压力下产生的谄媚现象（过度顺从）导致的准确性退化。PARROT通过三重机制实现评估：(i) 采用双盲评估对比同一问题的中立版本与权威错误版本，以隔离因果效应；(ii) 基于对数似然的校准追踪量化模型对正确答案与强加错误答案的置信度偏移；(iii) 通过八态行为分类法系统化识别失效模式（如鲁棒正确、谄媚认同、错误强化、顽固错误、自我修正等）。我们在13个学科领域使用1,302道MMLU式选择题及领域专属权威模板评估了22个模型。结果显示显著异质性：先进模型（如GPT-5、GPT-4.1、Claude Sonnet 4.5）表现出较低的\"顺从率\"（≤11%，GPT-5：4%）和极小的准确率损失，而早期/小规模模型则出现严重的认知坍塌（GPT-4：80%，Qwen 2.5-1.5B：94%）。风险不仅限于答案改变：弱势模型会降低对正确答案的置信度，同时提升对强加错误答案的置信度。尽管国际法与领域级全球知识表现出高度脆弱性，基础数学则相对稳健。据此我们主张，为实现现实世界的安全部署，\"抵抗过度顺从压力\"应作为与准确性、伤害规避和隐私保护并列的核心优化目标。",
    "url": "https://huggingface.co/papers/2511.17220",
    "arxiv_url": "https://arxiv.org/abs/2511.17220"
  },
  {
    "title": "Loomis Painter: Reconstructing the Painting Process",
    "summary": "Step-by-step painting tutorials are vital for learning artistic techniques, but existing video resources (e.g., YouTube) lack interactivity and personalization. While recent generative models have advanced artistic image synthesis, they struggle to generalize across media and often show temporal or structural inconsistencies, hindering faithful reproduction of human creative workflows. To address this, we propose a unified framework for multi-media painting process generation with a semantics-driven style control mechanism that embeds multiple media into a diffusion models conditional space and uses cross-medium style augmentation. This enables consistent texture evolution and process transfer across styles. A reverse-painting training strategy further ensures smooth, human-aligned generation. We also build a large-scale dataset of real painting processes and evaluate cross-media consistency, temporal coherence, and final-image fidelity, achieving strong results on LPIPS, DINO, and CLIP metrics. Finally, our Perceptual Distance Profile (PDP) curve quantitatively models the creative sequence, i.e., composition, color blocking, and detail refinement, mirroring human artistic progression.",
    "translation": "标题：Loomis Painter：绘画过程重建技术研究\n\n摘要：分步绘画教程对学习艺术技法至关重要，但现有视频资源（如YouTube）缺乏交互性与个性化。尽管近期生成模型在艺术图像合成领域取得进展，但其在跨媒介泛化方面存在局限，常出现时序或结构不一致问题，难以忠实复现人类创作流程。为此，我们提出一个融合多媒介绘画过程生成的统一框架，采用语义驱动的风格控制机制，将多种媒介嵌入扩散模型的条件空间并实施跨媒介风格增强。该方案能实现跨风格的一致纹理演化与过程迁移。通过逆向绘制训练策略进一步确保生成结果符合人类绘画的流畅性。我们还构建了大规模真实绘画过程数据集，从跨媒介一致性、时序连贯性和最终图像保真度三个维度进行评估，在LPIPS、DINO和CLIP指标上取得优异表现。最后提出的感知距离分布（PDP）曲线可量化建模创作序列——构图、色块铺陈与细节精修，精准对应人类艺术创作进程。",
    "url": "https://huggingface.co/papers/2511.17344",
    "arxiv_url": "https://arxiv.org/abs/2511.17344"
  },
  {
    "title": "WorldGen: From Text to Traversable and Interactive 3D Worlds",
    "summary": "We introduce WorldGen, a system that enables the automatic creation of large-scale, interactive 3D worlds directly from text prompts. Our approach transforms natural language descriptions into traversable, fully textured environments that can be immediately explored or edited within standard game engines. By combining LLM-driven scene layout reasoning, procedural generation, diffusion-based 3D generation, and object-aware scene decomposition, WorldGen bridges the gap between creative intent and functional virtual spaces, allowing creators to design coherent, navigable worlds without manual modeling or specialized 3D expertise. The system is fully modular and supports fine-grained control over layout, scale, and style, producing worlds that are geometrically consistent, visually rich, and efficient to render in real time. This work represents a step towards accessible, generative world-building at scale, advancing the frontier of 3D generative AI for applications in gaming, simulation, and immersive social environments.",
    "translation": "标题：WorldGen：从文本到可遍历交互式三维世界的生成系统\n\n摘要：本文提出WorldGen系统，能够直接根据文本提示自动创建大规模交互式三维世界。该方法将自然语言描述转化为可遍历、全贴图的虚拟环境，支持在标准游戏引擎中即时探索或编辑。通过融合大语言模型驱动的场景布局推理、程序化生成、基于扩散模型的三维生成及对象感知的场景解构技术，WorldGen有效弥合了创意构想与功能化虚拟空间之间的鸿沟，使创作者无需手动建模或具备专业三维技能即可设计出连贯可导航的虚拟世界。该系统采用全模块化架构，支持对布局、尺度和风格的细粒度控制，所生成的世界兼具几何一致性、视觉丰富性与实时渲染效率。本研究成果为推动规模化、低门槛的生成式虚拟世界构建迈出关键一步，拓展了三维生成人工智能在游戏开发、仿真模拟及沉浸式社交等领域的应用前沿。",
    "url": "https://huggingface.co/papers/2511.16825",
    "arxiv_url": "https://arxiv.org/abs/2511.16825"
  },
  {
    "title": "Mantis: A Versatile Vision-Language-Action Model with Disentangled Visual Foresight",
    "summary": "Recent advances in Vision-Language-Action (VLA) models demonstrate that visual signals can effectively complement sparse action supervisions. However, letting VLA directly predict high-dimensional visual states can distribute model capacity and incur prohibitive training cost, while compressing visual states into more compact supervisory signals inevitably incurs information bottlenecks. Moreover, existing methods often suffer from poor comprehension and reasoning capabilities due to the neglect of language supervision. This paper introduces Mantis, a novel framework featuring a Disentangled Visual Foresight (DVF) to tackle these issues. Specifically, Mantis decouples visual foresight prediction from the backbone with the combination of meta queries and a diffusion Transformer (DiT) head. With the current visual state provided to the DiT via a residual connection, a simple next-state prediction objective enables the meta queries to automatically capture the latent actions that delineate the visual trajectory, and hence boost the learning of explicit actions. The disentanglement reduces the burden of the VLA backbone, enabling it to maintain comprehension and reasoning capabilities through language supervision. Empirically, pretrained on human manipulation videos, robot demonstrations, and image-text pairs, Mantis achieves a 96.7% success rate on LIBERO benchmark after fine-tuning, surpassing powerful baselines while exhibiting high convergence speed. Real-world evaluations show that Mantis outperforms π_{0.5}, a leading open-source VLA model, particularly in instruction-following capability, generalization to unseen instructions, and reasoning ability. Code and weights are released to support the open-source community.",
    "translation": "标题：螳螂模型：具备解耦视觉预测能力的多模态视觉-语言-动作框架\n\n摘要：视觉-语言-动作模型的最新进展表明，视觉信号能有效补充稀疏动作监督。然而，直接让VLA模型预测高维视觉状态会分散模型容量并产生高昂训练成本，而将视觉状态压缩为紧凑监督信号则不可避免地引发信息瓶颈。此外，现有方法因忽视语言监督常导致理解与推理能力不足。本文提出螳螂模型，该创新框架通过解耦视觉预测机制解决上述问题。具体而言，该模型结合元查询与扩散Transformer头，将视觉预测从主干网络解耦。通过残差连接向DiT提供当前视觉状态，简单的下一状态预测目标使元查询能自动捕捉描述视觉轨迹的潜在动作，从而增强显式动作的学习。这种解耦设计减轻了VLA主干网络负担，使其能通过语言监督保持理解与推理能力。经人类操作视频、机器人示范及图文对预训练后，螳螂模型在LIBERO基准微调后达到96.7%的成功率，在超越强基线的同时展现出高收敛速度。真实场景评估表明，该模型在指令遵循能力、未知指令泛化性和推理能力方面均优于主流开源VLA模型π_{0.5}。相关代码与权重已开源以支持社区研究。",
    "url": "https://huggingface.co/papers/2511.16175",
    "arxiv_url": "https://arxiv.org/abs/2511.16175"
  },
  {
    "title": "VisMem: Latent Vision Memory Unlocks Potential of Vision-Language Models",
    "summary": "Despite the remarkable success of Vision-Language Models (VLMs), their performance on a range of complex visual tasks is often hindered by a \"visual processing bottleneck\": a propensity to lose grounding in visual evidence and exhibit a deficit in contextualized visual experience during prolonged generation. Drawing inspiration from human cognitive memory theory, which distinguishes short-term visually-dominant memory and long-term semantically-dominant memory, we propose VisMem, a cognitively-aligned framework that equips VLMs with dynamic latent vision memories, a short-term module for fine-grained perceptual retention and a long-term module for abstract semantic consolidation. These memories are seamlessly invoked during inference, allowing VLMs to maintain both perceptual fidelity and semantic consistency across thinking and generation. Extensive experiments across diverse visual benchmarks for understanding, reasoning, and generation reveal that VisMem delivers a significant average performance boost of 11.8% relative to the vanilla model and outperforms all counterparts, establishing a new paradigm for latent-space memory enhancement. The code will be available: https://github.com/YU-deep/VisMem.git.",
    "translation": "标题：VisMem：潜在视觉记忆解锁视觉语言模型的潜力\n\n摘要：尽管视觉语言模型取得了显著成功，但其在复杂视觉任务上的表现常受限于\"视觉处理瓶颈\"：在长序列生成过程中容易丧失视觉证据的锚定，并表现出情境化视觉经验的缺失。受人类认知记忆理论中短期视觉主导记忆与长期语义主导记忆区分的启发，我们提出VisMem——一个认知对齐框架，通过动态潜在视觉记忆为VLMs赋能，包含用于细粒度感知保持的短期模块和用于抽象语义巩固的长期模块。这些记忆在推理过程中被无缝调用，使VLMs能够在思维与生成过程中同时保持感知保真度与语义一致性。在涵盖理解、推理与生成的多样化视觉基准测试中，实验结果表明VisMem相较于原始模型实现了11.8%的平均性能提升，且优于所有对比模型，确立了潜在空间记忆增强的新范式。代码将发布于：https://github.com/YU-deep/VisMem.git。",
    "url": "https://huggingface.co/papers/2511.11007",
    "arxiv_url": "https://arxiv.org/abs/2511.11007"
  },
  {
    "title": "InstructMix2Mix: Consistent Sparse-View Editing Through Multi-View Model Personalization",
    "summary": "We address the task of multi-view image editing from sparse input views, where the inputs can be seen as a mix of images capturing the scene from different viewpoints. The goal is to modify the scene according to a textual instruction while preserving consistency across all views. Existing methods, based on per-scene neural fields or temporal attention mechanisms, struggle in this setting, often producing artifacts and incoherent edits. We propose InstructMix2Mix (I-Mix2Mix), a framework that distills the editing capabilities of a 2D diffusion model into a pretrained multi-view diffusion model, leveraging its data-driven 3D prior for cross-view consistency. A key contribution is replacing the conventional neural field consolidator in Score Distillation Sampling (SDS) with a multi-view diffusion student, which requires novel adaptations: incremental student updates across timesteps, a specialized teacher noise scheduler to prevent degeneration, and an attention modification that enhances cross-view coherence without additional cost. Experiments demonstrate that I-Mix2Mix significantly improves multi-view consistency while maintaining high per-frame edit quality.",
    "translation": "标题：InstructMix2Mix：通过多视角模型个性化实现一致的稀疏视角编辑\n\n摘要：本文研究从稀疏输入视角进行多视角图像编辑的任务，其中输入可视为从不同视角捕捉场景的图像混合体。目标是根据文本指令修改场景，同时保持所有视角间的一致性。现有基于逐场景神经场或时序注意力机制的方法在此设定下表现不佳，常产生伪影和不连贯的编辑效果。我们提出InstructMix2Mix框架，通过将二维扩散模型的编辑能力蒸馏至预训练的多视角扩散模型，利用其数据驱动的三维先验实现跨视角一致性。核心创新在于用多视角扩散学生模型取代分数蒸馏采样中的传统神经场整合器，这需要三项新颖适配：跨时间步的渐进式学生模型更新、防止性能退化的专用教师噪声调度器，以及无需额外成本即可增强跨视图一致性的注意力机制改进。实验表明，I-Mix2Mix在保持单帧高质量编辑的同时，显著提升了多视角一致性。",
    "url": "https://huggingface.co/papers/2511.14899",
    "arxiv_url": "https://arxiv.org/abs/2511.14899"
  },
  {
    "title": "MergeDNA: Context-aware Genome Modeling with Dynamic Tokenization through Token Merging",
    "summary": "Modeling genomic sequences faces two unsolved challenges: the information density varies widely across different regions, while there is no clearly defined minimum vocabulary unit. Relying on either four primitive bases or independently designed DNA tokenizers, existing approaches with naive masked language modeling pre-training often fail to adapt to the varying complexities of genomic sequences. Leveraging Token Merging techniques, this paper introduces a hierarchical architecture that jointly optimizes a dynamic genomic tokenizer and latent Transformers with context-aware pre-training tasks. As for network structures, the tokenization module automatically chunks adjacent bases into words by stacking multiple layers of the differentiable token merging blocks with local-window constraints, then a Latent Encoder captures the global context of these merged words by full-attention blocks. Symmetrically employing a Latent Decoder and a Local Decoder, MergeDNA learns with two pre-training tasks: Merged Token Reconstruction simultaneously trains the dynamic tokenization module and adaptively filters important tokens, while Adaptive Masked Token Modeling learns to predict these filtered tokens to capture informative contents. Extensive experiments show that MergeDNA achieves superior performance on three popular DNA benchmarks and several multi-omics tasks with fine-tuning or zero-shot evaluation, outperforming typical tokenization methods and large-scale DNA foundation models.",
    "translation": "标题：MergeDNA：基于动态分词与令牌融合的上下文感知基因组建模方法\n\n摘要：基因组序列建模面临两大未解难题：不同区域的信息密度差异显著，且缺乏明确定义的最小词汇单元。现有方法依赖四种碱基或独立设计的DNA分词器，结合简单的掩码语言建模预训练，往往难以适应基因组序列的复杂度变化。本文利用令牌融合技术，提出一种通过上下文感知预训练任务联合优化动态基因组分词器与潜在Transformer的层次化架构。在网络结构方面，分词模块通过堆叠多层具有局部窗口约束的可微分令牌融合块，将相邻碱基自动分块为词汇单元，随后潜在编码器通过全局注意力块捕获这些融合词汇的上下文信息。通过对称部署潜在解码器与局部解码器，MergeDNA采用两项预训练任务：融合令牌重构任务同步训练动态分词模块并自适应筛选重要令牌，而自适应掩码令牌建模任务则学习预测这些被筛选令牌以捕获信息内容。大量实验表明，MergeDNA在三个主流DNA基准测试和若干多组学任务中，无论是通过微调还是零样本评估，均取得了优于典型分词方法及大规模DNA基础模型的性能表现。",
    "url": "https://huggingface.co/papers/2511.14806",
    "arxiv_url": "https://arxiv.org/abs/2511.14806"
  },
  {
    "title": "OmniScientist: Toward a Co-evolving Ecosystem of Human and AI Scientists",
    "summary": "With the rapid development of Large Language Models (LLMs), AI agents have demonstrated increasing proficiency in scientific tasks, ranging from hypothesis generation and experimental design to manuscript writing. Such agent systems are commonly referred to as \"AI Scientists.\" However, existing AI Scientists predominantly formulate scientific discovery as a standalone search or optimization problem, overlooking the fact that scientific research is inherently a social and collaborative endeavor. Real-world science relies on a complex scientific infrastructure composed of collaborative mechanisms, contribution attribution, peer review, and structured scientific knowledge networks. Due to the lack of modeling for these critical dimensions, current systems struggle to establish a genuine research ecosystem or interact deeply with the human scientific community. To bridge this gap, we introduce OmniScientist, a framework that explicitly encodes the underlying mechanisms of human research into the AI scientific workflow. OmniScientist not only achieves end-to-end automation across data foundation, literature review, research ideation, experiment automation, scientific writing, and peer review, but also provides comprehensive infrastructural support by simulating the human scientific system, comprising: (1) a structured knowledge system built upon citation networks and conceptual correlations; (2) a collaborative research protocol (OSP), which enables seamless multi-agent collaboration and human researcher participation; and (3) an open evaluation platform (ScienceArena) based on blind pairwise user voting and Elo rankings. This infrastructure empowers agents to not only comprehend and leverage human knowledge systems but also to collaborate and co-evolve, fostering a sustainable and scalable innovation ecosystem.",
    "translation": "标题：OmniScientist：构建人类与AI科学家协同演化的科研生态系统\n\n摘要：随着大语言模型（LLM）的快速发展，AI智能体在科学任务中展现出日益精进的能力，涵盖假设生成、实验设计乃至论文撰写等环节。此类智能体系统通常被称为\"AI科学家\"。然而，现有AI科学家主要将科学发现建模为独立搜索或优化问题，忽视了科学研究本质上是社会性协作活动这一根本特征。现实科学体系依赖于由协作机制、贡献归属、同行评议和结构化科学知识网络构成的复杂科研基础设施。由于缺乏对这些关键维度的建模，现有系统难以建立真正的研究生态系统，也无法与人类科学界实现深度互动。为弥补这一缺陷，我们提出OmniScientist框架，将人类科研的内在机制显式编码至AI科学工作流中。该框架不仅实现从数据基础、文献综述、研究构思、实验自动化、科学写作到同行评议的端到端自动化，还通过模拟人类科学体系提供完整的基础设施支持，包括：（1）基于引文网络与概念关联的结构化知识体系；（2）支持多智能体无缝协作及人类研究者参与的开放式科研协议（OSP）；（3）基于双盲用户投票与Elo排序的开放评估平台（ScienceArena）。这套基础设施使智能体既能理解并利用人类知识体系，又能通过协作实现共同演化，最终培育出可持续、可扩展的创新生态系统。",
    "url": "https://huggingface.co/papers/2511.16931",
    "arxiv_url": "https://arxiv.org/abs/2511.16931"
  },
  {
    "title": "Downscaling Intelligence: Exploring Perception and Reasoning Bottlenecks in Small Multimodal Models",
    "summary": "Scaling up multimodal models has enabled remarkable advances in visual understanding and reasoning, but practical demands call for smaller, efficient systems. In this work, we conduct a principled analysis of downscaling intelligence in multimodal models, examining how reduced large language model (LLM) capacity affects multimodal capabilities. Our initial findings reveal an interesting trend: LLM downscaling disproportionately affects visual capabilities, rather than abilities inherited from the LLM. We then examine whether this drop mainly reflects the expected decline in visual reasoning or a more fundamental loss of perceptual abilities. Isolating the effect of LLM downscaling on perception, we find performance still drops sharply, often matching or exceeding the impact on reasoning. To address this bottleneck, we introduce visual extraction tuning, which explicitly trains the model to extract instruction-relevant visual details consistently across tasks. With these extracted visual details, we then apply step-by-step reasoning to generate answers. Together, these components form our Extract+Think approach, setting a new standard for efficiency and performance in this space.",
    "translation": "标题：缩小智能规模：探索小型多模态模型中的感知与推理瓶颈\n\n摘要：多模态模型的规模扩展显著提升了视觉理解与推理能力，但实际应用需求呼唤更精简高效的系统。本研究对多模态模型智能规模缩减现象展开系统性分析，探究大型语言模型容量缩减如何影响多模态能力。初步发现揭示了一个有趣趋势：语言模型规模缩减对视觉能力的影响远超其对语言模型固有能力的继承。我们进一步探究这种性能下降究竟源于预期的视觉推理能力衰减，还是更根本的感知能力丧失。通过分离语言模型规模缩减对感知能力的影响，发现性能仍会急剧下降，其降幅往往与推理能力下降相当甚至更甚。为突破此瓶颈，我们提出视觉提取调优方法，通过显式训练使模型在不同任务中持续提取与指令相关的视觉细节。基于这些提取的视觉信息，我们采用分步推理机制生成答案。这些组件共同构成了\"提取+思考\"方法论，为该领域的效率与性能设立了新基准。",
    "url": "https://huggingface.co/papers/2511.17487",
    "arxiv_url": "https://arxiv.org/abs/2511.17487"
  },
  {
    "title": "Diversity Has Always Been There in Your Visual Autoregressive Models",
    "summary": "Visual Autoregressive (VAR) models have recently garnered significant attention for their innovative next-scale prediction paradigm, offering notable advantages in both inference efficiency and image quality compared to traditional multi-step autoregressive (AR) and diffusion models. However, despite their efficiency, VAR models often suffer from the diversity collapse i.e., a reduction in output variability, analogous to that observed in few-step distilled diffusion models. In this paper, we introduce DiverseVAR, a simple yet effective approach that restores the generative diversity of VAR models without requiring any additional training. Our analysis reveals the pivotal component of the feature map as a key factor governing diversity formation at early scales. By suppressing the pivotal component in the model input and amplifying it in the model output, DiverseVAR effectively unlocks the inherent generative potential of VAR models while preserving high-fidelity synthesis. Empirical results demonstrate that our approach substantially enhances generative diversity with only neglectable performance influences. Our code will be publicly released at https://github.com/wangtong627/DiverseVAR.",
    "translation": "标题：视觉自回归模型中的多样性本真溯源\n\n摘要：视觉自回归模型凭借其创新的尺度递进预测范式，相较传统多步自回归与扩散模型在推理效率与图像质量方面展现出显著优势，近来备受关注。然而尽管效率卓越，该类模型仍常面临多样性坍缩问题——即输出变异性的减弱，这种现象与少步蒸馏扩散模型中的观测结果具有相似性。本文提出DiverseVAR这一无需额外训练即可恢复视觉自回归模型生成多样性的简易有效方案。通过理论分析，我们发现特征图中的关键成分是早期尺度多样性形成的主导因素。通过抑制模型输入中的关键成分并增强其输出表达，DiverseVAR在保持高保真合成能力的同时，有效释放了视觉自回归模型内蕴的生成潜力。实证研究表明，该方法仅以可忽略的性能影响为代价，即可显著提升生成多样性。相关代码将发布于https://github.com/wangtong627/DiverseVAR。",
    "url": "https://huggingface.co/papers/2511.17074",
    "arxiv_url": "https://arxiv.org/abs/2511.17074"
  },
  {
    "title": "Video-R4: Reinforcing Text-Rich Video Reasoning with Visual Rumination",
    "summary": "Understanding text-rich videos requires reading small, transient textual cues that often demand repeated inspection. Yet most video QA models rely on single-pass perception over fixed frames, leading to hallucinations and failures on fine-grained evidence. Inspired by how humans pause, zoom, and re-read critical regions, we introduce Video-R4 (Reinforcing Text-Rich Video Reasoning with Visual Rumination), a video reasoning LMM that performs visual rumination: iteratively selecting frames, zooming into informative regions, re-encoding retrieved pixels, and updating its reasoning state. We construct two datasets with executable rumination trajectories: Video-R4-CoT-17k for supervised practice and Video-R4-RL-30k for reinforcement learning. We propose a multi-stage rumination learning framework that progressively finetunes a 7B LMM to learn atomic and mixing visual operations via SFT and GRPO-based RL. Video-R4-7B achieves state-of-the-art results on M4-ViteVQA and further generalizes to multi-page document QA, slides QA, and generic video QA, demonstrating that iterative rumination is an effective paradigm for pixel-grounded multimodal reasoning.",
    "translation": "标题：Video-R4：通过视觉反刍增强文本富集视频推理能力\n\n摘要：理解文本富集视频需要捕捉短暂出现的小尺寸文本线索，这往往需要反复观察。然而现有视频问答模型大多基于固定帧的单次感知，导致存在幻觉现象且在细粒度证据识别上表现不佳。受人类暂停播放、放大关键区域及重复阅读行为的启发，我们提出Video-R4（基于视觉反刍的文本富集视频推理增强模型），该视频推理大语言模型能够执行视觉反刍操作：迭代选择帧序列、放大信息密集区域、重新编码检索像素并持续更新推理状态。我们构建了两个包含可执行反刍轨迹的数据集：用于监督训练的Video-R4-CoT-17k和用于强化学习的Video-R4-RL-30k。提出多阶段反刍学习框架，通过指令微调和基于GRPO的强化学习，逐步训练70亿参数模型掌握原子视觉操作与混合视觉操作。Video-R4-7B在M4-ViteVQA基准测试中达到最先进水平，并进一步泛化至多页文档问答、幻灯片问答及通用视频问答任务，证明迭代式反刍是实现像素级多模态推理的有效范式。",
    "url": "https://huggingface.co/papers/2511.17490",
    "arxiv_url": "https://arxiv.org/abs/2511.17490"
  },
  {
    "title": "Insights from the ICLR Peer Review and Rebuttal Process",
    "summary": "Peer review is a cornerstone of scientific publishing, including at premier machine learning conferences such as ICLR. As submission volumes increase, understanding the nature and dynamics of the review process is crucial for improving its efficiency, effectiveness, and the quality of published papers. We present a large-scale analysis of the ICLR 2024 and 2025 peer review processes, focusing on before- and after-rebuttal scores and reviewer-author interactions. We examine review scores, author-reviewer engagement, temporal patterns in review submissions, and co-reviewer influence effects. Combining quantitative analyses with LLM-based categorization of review texts and rebuttal discussions, we identify common strengths and weaknesses for each rating group, as well as trends in rebuttal strategies that are most strongly associated with score changes. Our findings show that initial scores and the ratings of co-reviewers are the strongest predictors of score changes during the rebuttal, pointing to a degree of reviewer influence. Rebuttals play a valuable role in improving outcomes for borderline papers, where thoughtful author responses can meaningfully shift reviewer perspectives. More broadly, our study offers evidence-based insights to improve the peer review process, guiding authors on effective rebuttal strategies and helping the community design fairer and more efficient review processes. Our code and score changes data are available at https://github.com/papercopilot/iclr-insights.",
    "translation": "标题：ICLR同行评审与作者反馈机制的研究启示\n\n摘要：同行评审是科学出版体系的基石，在ICLR等顶级机器学习会议中亦是如此。随着投稿量持续增长，深入理解评审机制的特性与动态对于提升流程效率、评审效能及论文质量至关重要。本研究对ICLR 2024与2025年的同行评审过程开展大规模分析，重点关注反驳环节前后的评分变化及审稿人与作者的互动关系。我们系统考察了评审分数分布、作者-审稿人互动强度、评审提交的时间规律以及共同审稿人的影响效应。通过量化分析与基于大语言模型的评论文本及反驳讨论分类，我们揭示了不同评分区间论文的共性优势与不足，并识别出与分数变化关联最显著的反驳策略趋势。研究发现：初始评分与共同审稿人的评级是反驳过程中分数变化的最强预测因子，这表明审稿人之间存在相互影响；对于临界论文，反驳环节能显著改善评审结果，深思熟虑的作者回应可有效转变审稿人观点。本研究从宏观层面为改进同行评审机制提供了实证依据，既可指导作者制定有效反驳策略，也有助于学术社区设计更公平高效的评审流程。相关代码与评分变化数据已公开于https://github.com/papercopilot/iclr-insights。",
    "url": "https://huggingface.co/papers/2511.15462",
    "arxiv_url": "https://arxiv.org/abs/2511.15462"
  },
  {
    "title": "Planning with Sketch-Guided Verification for Physics-Aware Video Generation",
    "summary": "Recent video generation approaches increasingly rely on planning intermediate control signals such as object trajectories to improve temporal coherence and motion fidelity. However, these methods mostly employ single-shot plans that are typically limited to simple motions, or iterative refinement which requires multiple calls to the video generator, incuring high computational cost. To overcome these limitations, we propose SketchVerify, a training-free, sketch-verification-based planning framework that improves motion planning quality with more dynamically coherent trajectories (i.e., physically plausible and instruction-consistent motions) prior to full video generation by introducing a test-time sampling and verification loop. Given a prompt and a reference image, our method predicts multiple candidate motion plans and ranks them using a vision-language verifier that jointly evaluates semantic alignment with the instruction and physical plausibility. To efficiently score candidate motion plans, we render each trajectory as a lightweight video sketch by compositing objects over a static background, which bypasses the need for expensive, repeated diffusion-based synthesis while achieving comparable performance. We iteratively refine the motion plan until a satisfactory one is identified, which is then passed to the trajectory-conditioned generator for final synthesis. Experiments on WorldModelBench and PhyWorldBench demonstrate that our method significantly improves motion quality, physical realism, and long-term consistency compared to competitive baselines while being substantially more efficient. Our ablation study further shows that scaling up the number of trajectory candidates consistently enhances overall performance.",
    "translation": "标题：基于草图引导验证的物理感知视频生成规划方法\n\n摘要：当前视频生成方法日益依赖规划中间控制信号（如物体轨迹）来提升时序连贯性与运动保真度。然而这些方法多采用单次规划方案，通常仅能处理简单运动，或需通过多次调用视频生成器进行迭代优化，导致计算成本高昂。为突破这些局限，我们提出SketchVerify——一种免训练的草图验证规划框架，通过在生成完整视频前引入测试时采样与验证循环，以更具动态连贯性的轨迹（即物理合理且符合指令要求的运动）提升运动规划质量。给定提示词与参考图像，本方法首先生成多个候选运动规划方案，随后采用视觉语言验证器从语义指令对齐度和物理合理性两个维度进行综合评估与排序。为高效评分候选运动方案，我们将每条轨迹合成为静态背景上的物体组合轻量视频草图，在保持性能相当的同时规避了昂贵的重复扩散合成过程。通过迭代优化运动规划直至获得满意方案，最终将其输入轨迹条件生成器完成视频合成。在WorldModelBench与PhyWorldBench数据集上的实验表明：相较于基线模型，本方法在运动质量、物理真实感与长程一致性方面均有显著提升，同时具备更高运算效率。消融实验进一步证明，增加轨迹候选方案数量能持续提升整体性能。",
    "url": "https://huggingface.co/papers/2511.17450",
    "arxiv_url": "https://arxiv.org/abs/2511.17450"
  },
  {
    "title": "VLA-4D: Embedding 4D Awareness into Vision-Language-Action Models for SpatioTemporally Coherent Robotic Manipulation",
    "summary": "Vision-language-action (VLA) models show potential for general robotic tasks, but remain challenging in spatiotemporally coherent manipulation, which requires fine-grained representations. Typically, existing methods embed 3D positions into visual representations to enhance the spatial precision of actions. However, these methods struggle to achieve temporally coherent control over action execution. In this work, we propose VLA-4D, a general VLA model with 4D awareness for spatiotemporally coherent robotic manipulation. Our model is guided by two key designs: 1) 4D-aware visual representation. We extract visual features, embed 1D time into 3D positions for 4D embeddings, and fuse them into a unified visual representation via a cross-attention mechanism. 2) Spatiotemporal action representation. We extend conventional spatial action representations with temporal information to enable the spatiotemporal planning, and align the multimodal representations into the LLM for spatiotemporal action prediction. Within this unified framework, the designed visual and action representations jointly make robotic manipulation spatially-smooth and temporally-coherent. In addition, we extend the VLA dataset with temporal action annotations for fine-tuning our model. Extensive experiments have been conducted to verify the superiority of our method across different tasks of robotic manipulation.",
    "translation": "标题：VLA-4D：将四维感知嵌入视觉-语言-动作模型以实现时空连贯的机器人操控\n\n摘要：视觉-语言-动作模型在通用机器人任务中展现出潜力，但在需要细粒度表征的时空连贯操控任务中仍面临挑战。现有方法通常将三维位置嵌入视觉表征以提升动作的空间精度，但难以实现动作执行的时序连贯控制。本研究提出VLA-4D——一种具备四维感知的通用VLA模型，用于实现时空连贯的机器人操控。我们的模型基于两项核心设计：1）四维感知视觉表征：通过提取视觉特征，将一维时间嵌入三维位置形成四维嵌入，并借助交叉注意力机制将其融合为统一视觉表征；2）时空动作表征：在传统空间动作表征基础上引入时序信息以实现时空规划，并将多模态表征对齐至大语言模型中完成时空动作预测。在此统一框架下，所设计的视觉与动作表征共同促使机器人操控实现空间平滑性与时序连贯性。此外，我们通过扩展带有时序动作标注的VLA数据集对模型进行微调。大量实验验证了本方法在多种机器人操控任务中的优越性。",
    "url": "https://huggingface.co/papers/2511.17199",
    "arxiv_url": "https://arxiv.org/abs/2511.17199"
  },
  {
    "title": "Multi-Faceted Attack: Exposing Cross-Model Vulnerabilities in Defense-Equipped Vision-Language Models",
    "summary": "The growing misuse of Vision-Language Models (VLMs) has led providers to deploy multiple safeguards, including alignment tuning, system prompts, and content moderation. However, the real-world robustness of these defenses against adversarial attacks remains underexplored. We introduce Multi-Faceted Attack (MFA), a framework that systematically exposes general safety vulnerabilities in leading defense-equipped VLMs such as GPT-4o, Gemini-Pro, and Llama-4. The core component of MFA is the Attention-Transfer Attack (ATA), which hides harmful instructions inside a meta task with competing objectives. We provide a theoretical perspective based on reward hacking to explain why this attack succeeds. To improve cross-model transferability, we further introduce a lightweight transfer-enhancement algorithm combined with a simple repetition strategy that jointly bypasses both input-level and output-level filters without model-specific fine-tuning. Empirically, we show that adversarial images optimized for one vision encoder transfer broadly to unseen VLMs, indicating that shared visual representations create a cross-model safety vulnerability. Overall, MFA achieves a 58.5% success rate and consistently outperforms existing methods. On state-of-the-art commercial models, MFA reaches a 52.8% success rate, surpassing the second-best attack by 34%. These results challenge the perceived robustness of current defense mechanisms and highlight persistent safety weaknesses in modern VLMs. Code: https://github.com/cure-lab/MultiFacetedAttack",
    "translation": "标题：多维度攻击：揭示具备防御机制的视觉语言模型中的跨模型脆弱性  \n\n摘要：视觉语言模型（VLM）的滥用日益增多，促使服务商部署多重防护机制，包括对齐调优、系统提示和内容审核。然而，这些防御措施在面对对抗性攻击时的实际鲁棒性仍待深入探究。本文提出多维度攻击框架，系统性地揭示了主流防御型VLM（如GPT-4o、Gemini-Pro与Llama-4）中存在的通用安全漏洞。该框架的核心组件是注意力转移攻击，通过将有害指令隐藏于具有竞争目标的元任务中实现攻击。基于奖励破解理论，我们从理论层面解释了该攻击的成功机制。为提升跨模型迁移性，我们进一步提出结合轻量级迁移增强算法与简单重复策略的方法，无需模型特定微调即可联合绕过输入级与输出级过滤器。实证研究表明，针对某一视觉编码器优化的对抗图像可广泛迁移至未接触过的VLM，表明共享视觉表征导致了跨模型安全漏洞。总体而言，MFA实现了58.5%的成功率，持续优于现有方法。在最新商用模型上，MFA达到52.8%的成功率，较次优攻击方法提升34%。这些结果对当前防御机制的感知鲁棒性提出质疑，揭示了现代VLM中持续存在的安全缺陷。代码地址：https://github.com/cure-lab/MultiFacetedAttack",
    "url": "https://huggingface.co/papers/2511.16110",
    "arxiv_url": "https://arxiv.org/abs/2511.16110"
  },
  {
    "title": "Taming Generative Synthetic Data for X-ray Prohibited Item Detection",
    "summary": "Training prohibited item detection models requires a large amount of X-ray security images, but collecting and annotating these images is time-consuming and laborious. To address data insufficiency, X-ray security image synthesis methods composite images to scale up datasets. However, previous methods primarily follow a two-stage pipeline, where they implement labor-intensive foreground extraction in the first stage and then composite images in the second stage. Such a pipeline introduces inevitable extra labor cost and is not efficient. In this paper, we propose a one-stage X-ray security image synthesis pipeline (Xsyn) based on text-to-image generation, which incorporates two effective strategies to improve the usability of synthetic images. The Cross-Attention Refinement (CAR) strategy leverages the cross-attention map from the diffusion model to refine the bounding box annotation. The Background Occlusion Modeling (BOM) strategy explicitly models background occlusion in the latent space to enhance imaging complexity. To the best of our knowledge, compared with previous methods, Xsyn is the first to achieve high-quality X-ray security image synthesis without extra labor cost. Experiments demonstrate that our method outperforms all previous methods with 1.2% mAP improvement, and the synthetic images generated by our method are beneficial to improve prohibited item detection performance across various X-ray security datasets and detectors. Code is available at https://github.com/pILLOW-1/Xsyn/.",
    "translation": "标题：基于生成式合成数据的X射线违禁品检测优化方法\n\n摘要：训练违禁品检测模型需要大量X射线安检图像，但此类图像的采集与标注过程耗时费力。为解决数据不足问题，现有研究多采用图像合成技术来扩展数据集。然而传统方法主要遵循两阶段流程：第一阶段需进行劳动密集型的前景目标提取，第二阶段执行图像合成。这种流程不仅引入额外人力成本，且效率较低。本文提出基于文本到图像生成的单阶段X射线安检图像合成框架Xsyn，通过两种创新策略提升合成图像的可用性：交叉注意力优化策略利用扩散模型的交叉注意力图优化边界框标注；背景遮挡建模策略在潜在空间中显式建模背景遮挡以增强成像复杂度。据我们所知，与现有方法相比，Xsyn是首个无需额外人力成本即可实现高质量X射线安检图像合成的方案。实验表明，本方法以1.2% mAP提升超越所有现有方法，且生成的合成图像能有效提升多种X射线安检数据集和检测器中的违禁品识别性能。代码已开源：https://github.com/pILLOW-1/Xsyn/。",
    "url": "https://huggingface.co/papers/2511.15299",
    "arxiv_url": "https://arxiv.org/abs/2511.15299"
  },
  {
    "title": "Rethinking Saliency Maps: A Cognitive Human Aligned Taxonomy and Evaluation Framework for Explanations",
    "summary": "Saliency maps are widely used for visual explanations in deep learning, but a fundamental lack of consensus persists regarding their intended purpose and alignment with diverse user queries. This ambiguity hinders the effective evaluation and practical utility of explanation methods. We address this gap by introducing the Reference-Frame times Granularity (RFxG) taxonomy, a principled conceptual framework that organizes saliency explanations along two essential axes:Reference-Frame: Distinguishing between pointwise (\"Why this prediction?\") and contrastive (\"Why this and not an alternative?\") explanations. Granularity: Ranging from fine-grained class-level (e.g., \"Why Husky?\") to coarse-grained group-level (e.g., \"Why Dog?\") interpretations. Using the RFxG lens, we demonstrate critical limitations in existing evaluation metrics, which overwhelmingly prioritize pointwise faithfulness while neglecting contrastive reasoning and semantic granularity. To systematically assess explanation quality across both RFxG dimensions, we propose four novel faithfulness metrics. Our comprehensive evaluation framework applies these metrics to ten state-of-the-art saliency methods, four model architectures, and three datasets. By advocating a shift toward user-intent-driven evaluation, our work provides both the conceptual foundation and the practical tools necessary to develop visual explanations that are not only faithful to the underlying model behavior but are also meaningfully aligned with the complexity of human understanding and inquiry.",
    "translation": "标题：重新审视显著图：一种认知对齐的解释方法分类体系与评估框架\n\n摘要：显著图在深度学习视觉解释中应用广泛，但其预期目标与多样化用户需求之间的对应关系仍缺乏共识。这种模糊性阻碍了解释方法的有效评估与实际应用。为解决这一局限，我们提出参考框架×粒度（RFxG）分类体系——一个基于双维度的原则性概念框架：参考框架维度区分逐点解释（“为何有此预测？”）与对比解释（“为何此结果而非替代结果？”）；粒度维度涵盖从细粒度类别层面（如“为何是哈士奇？”）到粗粒度组群层面（如“为何是犬类？”）的解释谱系。通过RFxG视角，我们揭示了现有评估指标的关键局限：这些指标过度侧重逐点保真度，却忽视了对比推理与语义粒度。为系统评估RFxG双维度的解释质量，我们提出四项新颖的保真度指标。该综合评估框架将指标应用于十种前沿显著方法、四种模型架构与三个数据集。通过推动用户意图驱动的评估范式转型，本研究不仅为开发契合模型行为的视觉解释奠定了概念基础，更为实现与人类认知复杂性相匹配的解释效果提供了实用工具。\n\n摘要：[显著图在深度学习视觉解释中应用广泛，但其预期目标与多样化用户需求之间的对应关系仍缺乏共识。这种模糊性阻碍了解释方法的有效评估与实际应用。为解决这一局限，我们提出参考框架×粒度（RFxG）分类体系——一个基于双维度的原则性概念框架：参考框架维度区分逐点解释（“为何有此预测？”）与对比解释（“为何此结果而非替代结果？”）；粒度维度涵盖从细粒度类别层面（如“为何是哈士奇？”）到粗粒度组群层面（如“为何是犬类？”）的解释谱系。通过RFxG视角，我们揭示了现有评估指标的关键局限：这些指标过度侧重逐点保真度，却忽视了对比推理与语义粒度。为系统评估RFxG双维度的解释质量，我们提出四项新颖的保真度指标。该综合评估框架将指标应用于十种前沿显著方法、四种模型架构与三个数据集。通过推动用户意图驱动的评估范式转型，本研究不仅为开发契合模型行为的视觉解释奠定了概念基础，更为实现与人类认知复杂性相匹配的解释效果提供了实用工具。]",
    "url": "https://huggingface.co/papers/2511.13081",
    "arxiv_url": "https://arxiv.org/abs/2511.13081"
  },
  {
    "title": "Training Foundation Models on a Full-Stack AMD Platform: Compute, Networking, and System Design",
    "summary": "We report on the first large-scale mixture-of-experts (MoE) pretraining study on pure AMD hardware, utilizing both MI300X GPUs with Pollara interconnect. We distill practical guidance for both systems and model design. On the systems side, we deliver a comprehensive cluster and networking characterization: microbenchmarks for all core collectives (all-reduce, reduce-scatter, all-gather, broadcast) across message sizes and GPU counts on Pollara. To our knowledge, this is the first at this scale. We further provide MI300X microbenchmarks on kernel sizing and memory bandwidth to inform model design. On the modeling side, we introduce and apply MI300X-aware transformer sizing rules for attention and MLP blocks and justify MoE widths that jointly optimize training throughput and inference latency. We describe our training stack in depth, including often-ignored utilities such as fault-tolerance and checkpoint-reshaping, as well as detailed information on our training recipe. We also provide a preview of our model architecture and base model - ZAYA1 (760M active, 8.3B total parameters MoE) - which will be further improved upon in forthcoming papers. ZAYA1-base achieves performance comparable to leading base models such as Qwen3-4B and Gemma3-12B at its scale and larger, and outperforms models including Llama-3-8B and OLMoE across reasoning, mathematics, and coding benchmarks. Together, these results demonstrate that the AMD hardware, network, and software stack are mature and optimized enough for competitive large-scale pretraining.",
    "translation": "标题：基于全栈AMD平台的基础模型训练：计算、网络与系统设计\n\n摘要：我们首次在纯AMD硬件平台上开展了大规模专家混合模型预训练研究，同时采用了配备Pollara互联技术的MI300X GPU。本研究为系统设计与模型架构提供了实用指导。在系统层面，我们实现了对集群与网络特性的全面表征：通过微基准测试分析了Pollara网络上不同消息规模和GPU数量下所有核心集合通信操作（全归约、规约散射、全收集、广播）的性能。据我们所知，这是该领域的首例大规模研究。我们进一步提供了MI300X在核心规模与内存带宽方面的微基准测试数据，为模型设计提供参考。在模型层面，我们引入并应用了针对MI300X优化的Transformer规模配置规则，涵盖注意力机制与多层感知机模块，同时论证了能协同优化训练吞吐量与推理延迟的MoE宽度配置。我们深入阐述了训练技术栈，包括常被忽视的容错机制与检查点重塑等实用工具，并提供了训练方案的详细信息。此外，我们首次披露了ZAYA1基础模型架构（760M激活参数，83亿总参数的MoE模型），该模型将在后续研究中持续优化。ZAYA1基础模型在同等及更大规模模型中，其性能可比肩Qwen3-4B与Gemma3-12B等领先基础模型，并在推理、数学和代码基准测试中超越Llama-3-8B与OLMoE等模型。这些成果共同证明，AMD的硬件、网络和软件栈已发展成熟并完成优化，足以支撑具有竞争力的大规模预训练任务。",
    "url": "https://huggingface.co/papers/2511.17127",
    "arxiv_url": "https://arxiv.org/abs/2511.17127"
  }
]