[
  {
    "title": "DoPE: Denoising Rotary Position Embedding",
    "summary": "Rotary Position Embedding (RoPE) in Transformer models has inherent limits that weaken length extrapolation. We reinterpret the attention map with positional encoding as a noisy feature map, and propose Denoising Positional Encoding (DoPE), a training-free method based on truncated matrix entropy to detect outlier frequency bands in the feature map. Leveraging the noise characteristics of the feature map, we further reparameterize it with a parameter-free Gaussian distribution to achieve robust extrapolation. Our method theoretically reveals the underlying cause of the attention sink phenomenon and its connection to truncated matrix entropy. Experiments on needle-in-a-haystack and many-shot in-context learning tasks demonstrate that DoPE significantly improves retrieval accuracy and reasoning stability across extended contexts (up to 64K tokens). The results show that the denoising strategy for positional embeddings effectively mitigates attention sinks and restores balanced attention patterns, providing a simple yet powerful solution for improving length generalization. Our project page is Project: https://The-physical-picture-of-LLMs.github.io",
    "translation": "标题：DoPE：旋转位置编码去噪方法\n\n摘要：Transformer模型中的旋转位置编码存在固有局限，会削弱长度外推能力。我们将带位置编码的注意力图重新解读为含噪声的特征图，并提出基于截断矩阵熵的无训练去噪位置编码方法DoPE，用于检测特征图中的异常频带。利用特征图的噪声特性，我们进一步通过无参数高斯分布对其进行重参数化，以实现稳健的外推。该方法从理论上揭示了注意力汇聚现象的成因及其与截断矩阵熵的关联。在\"大海捞针\"和多样本上下文学习任务上的实验表明，DoPE在扩展上下文场景下显著提升了检索精度和推理稳定性。研究结果证明，针对位置嵌入的去噪策略能有效缓解注意力汇聚问题，恢复均衡的注意力模式，为改进长度泛化能力提供了简洁而有效的解决方案。项目页面详见：https://The-physical-picture-of-LLMs.github.io",
    "url": "https://huggingface.co/papers/2511.09146",
    "arxiv_url": "https://arxiv.org/abs/2511.09146"
  },
  {
    "title": "WEAVE: Unleashing and Benchmarking the In-context Interleaved Comprehension and Generation",
    "summary": "Recent advances in unified multimodal models (UMMs) have enabled impressive progress in visual comprehension and generation. However, existing datasets and benchmarks focus primarily on single-turn interactions, failing to capture the multi-turn, context-dependent nature of real-world image creation and editing. To address this gap, we present WEAVE, the first suite for in-context interleaved cross-modality comprehension and generation. Our suite consists of two complementary parts. WEAVE-100k is a large-scale dataset of 100K interleaved samples spanning over 370K dialogue turns and 500K images, covering comprehension, editing, and generation tasks that require reasoning over historical context. WEAVEBench is a human-annotated benchmark with 100 tasks based on 480 images, featuring a hybrid VLM judger evaluation framework based on both the reference image and the combination of the original image with editing instructions that assesses models' abilities in multi-turn generation, visual memory, and world-knowledge reasoning across diverse domains. Experiments demonstrate that training on WEAVE-100k enables vision comprehension, image editing, and comprehension-generation collaboration capabilities. Furthermore, it facilitates UMMs to develop emergent visual-memory capabilities, while extensive evaluations on WEAVEBench expose the persistent limitations and challenges of current approaches in multi-turn, context-aware image generation and editing. We believe WEAVE provides a view and foundation for studying in-context interleaved comprehension and generation for multi-modal community.",
    "translation": "标题：WEAVE：解锁与评测上下文交织式多模态理解与生成能力\n\n摘要：统一多模态模型的最新进展显著推动了视觉理解与生成领域的发展。然而现有数据集和基准主要关注单轮交互，未能体现真实世界图像创作与编辑过程中多轮次、上下文依赖的特性。为弥补这一空白，我们提出WEAVE——首个面向上下文交织式跨模态理解与生成的综合套件。该套件包含两个互补部分：WEAVE-100k作为大规模数据集包含10万个交织样本，覆盖37万次对话轮转和50万张图像，涵盖需要历史上下文推理的理解、编辑与生成任务；WEAVEBench则是基于480张图像构建的含100个任务的人工标注基准，采用融合参考图像及原图与编辑指令的混合式视觉语言模型评判框架，评估模型在多轮生成、视觉记忆和跨领域世界知识推理等方面的能力。实验表明，基于WEAVE-100k的训练能有效提升视觉理解、图像编辑及理解-生成协作能力，更有助于统一多模态模型涌现视觉记忆能力。同时，在WEAVEBench上的广泛评测揭示了当前方法在多轮上下文感知图像生成与编辑方面存在的持续局限与挑战。我们相信WEAVE为多模态社区研究上下文交织式理解与生成提供了新视角与基础平台。",
    "url": "https://huggingface.co/papers/2511.11434",
    "arxiv_url": "https://arxiv.org/abs/2511.11434"
  },
  {
    "title": "GGBench: A Geometric Generative Reasoning Benchmark for Unified Multimodal Models",
    "summary": "The advent of Unified Multimodal Models (UMMs) signals a paradigm shift in artificial intelligence, moving from passive perception to active, cross-modal generation. Despite their unprecedented ability to synthesize information, a critical gap persists in evaluation: existing benchmarks primarily assess discriminative understanding or unconstrained image generation separately, failing to measure the integrated cognitive process of generative reasoning. To bridge this gap, we propose that geometric construction provides an ideal testbed as it inherently demands a fusion of language comprehension and precise visual generation. We introduce GGBench, a benchmark designed specifically to evaluate geometric generative reasoning. It provides a comprehensive framework for systematically diagnosing a model's ability to not only understand and reason but to actively construct a solution, thereby setting a more rigorous standard for the next generation of intelligent systems. Project website: https://opendatalab-raiser.github.io/GGBench/.",
    "translation": "标题：GGBench：面向统一多模态模型的几何生成推理基准\n\n摘要：统一多模态模型（UMMs）的出现标志着人工智能领域的范式转变，从被动感知转向主动的跨模态生成。尽管这些模型具有前所未有的信息融合能力，但其评估体系仍存在关键缺陷：现有基准主要分别评估判别式理解或无约束图像生成能力，未能有效衡量生成推理这一整合性认知过程。为弥补这一空白，我们提出几何构造任务可作为理想测试平台，因其本质上要求语言理解与精确视觉生成的深度融合。我们开发的GGBench基准专门用于评估几何生成推理能力，该框架通过系统化诊断模型在理解、推理及主动构建解决方案等方面的综合能力，为新一代智能系统设立了更严谨的评估标准。项目主页：https://opendatalab-raiser.github.io/GGBench/。",
    "url": "https://huggingface.co/papers/2511.11134",
    "arxiv_url": "https://arxiv.org/abs/2511.11134"
  },
  {
    "title": "UI2Code^N: A Visual Language Model for Test-Time Scalable Interactive UI-to-Code Generation",
    "summary": "User interface (UI) programming is a core yet highly complex part of modern software development. Recent advances in visual language models (VLMs) highlight the potential of automatic UI coding, but current approaches face two key limitations: multimodal coding capabilities remain underdeveloped, and single-turn paradigms make little use of iterative visual feedback. We address these challenges with an interactive UI-to-code paradigm that better reflects real-world workflows and raises the upper bound of achievable performance. Under this paradigm, we present UI2Code^N, a visual language model trained through staged pretraining, fine-tuning, and reinforcement learning to achieve foundational improvements in multimodal coding. The model unifies three key capabilities: UI-to-code generation, UI editing, and UI polishing. We further explore test-time scaling for interactive generation, enabling systematic use of multi-turn feedback. Experiments on UI-to-code and UI polishing benchmarks show that UI2Code^N establishes a new state of the art among open-source models and achieves performance comparable to leading closed-source models such as Claude-4-Sonnet and GPT-5. Our code and models are available at https://github.com/zai-org/UI2Code_N.",
    "translation": "标题：UI2Code^N：支持测试时扩展交互的界面到代码生成视觉语言模型\n\n摘要：用户界面编程是现代软件开发中核心且高度复杂的环节。视觉语言模型的最新进展凸显了自动界面编程的潜力，但现有方法存在两个关键局限：多模态编程能力尚未充分发展，单轮生成范式难以利用迭代的视觉反馈。我们提出交互式界面到代码生成范式来解决这些挑战，该范式更贴合实际工作流程并提升了性能上限。在此范式下，我们推出UI2Code^N视觉语言模型，通过分阶段预训练、微调与强化学习实现多模态编程能力的根本性提升。该模型统一了三大核心能力：界面到代码生成、界面编辑与界面优化。我们进一步探索测试时扩展的交互生成机制，实现多轮反馈的系统化利用。在界面到代码生成与界面优化基准测试中，UI2Code^N在开源模型中确立了最新技术标杆，其性能可与Claude-4-Sonnet、GPT-5等领先闭源模型相媲美。代码与模型已发布于https://github.com/zai-org/UI2Code_N。",
    "url": "https://huggingface.co/papers/2511.08195",
    "arxiv_url": "https://arxiv.org/abs/2511.08195"
  },
  {
    "title": "AIonopedia: an LLM agent orchestrating multimodal learning for ionic liquid discovery",
    "summary": "The discovery of novel Ionic Liquids (ILs) is hindered by critical challenges in property prediction, including limited data, poor model accuracy, and fragmented workflows. Leveraging the power of Large Language Models (LLMs), we introduce AIonopedia, to the best of our knowledge, the first LLM agent for IL discovery. Powered by an LLM-augmented multimodal domain foundation model for ILs, AIonopedia enables accurate property predictions and incorporates a hierarchical search architecture for molecular screening and design. Trained and evaluated on a newly curated and comprehensive IL dataset, our model delivers superior performance. Complementing these results, evaluations on literature-reported systems indicate that the agent can perform effective IL modification. Moving beyond offline tests, the practical efficacy was further confirmed through real-world wet-lab validation, in which the agent demonstrated exceptional generalization capabilities on challenging out-of-distribution tasks, underscoring its ability to accelerate real-world IL discovery.",
    "translation": "标题：AIonopedia：基于大语言模型的多模态学习协同框架用于离子液体发现\n\n摘要：新型离子液体的开发面临物性预测领域的多重挑战，包括数据稀缺、模型精度不足及工作流程碎片化。本研究创新性地运用大语言模型技术，首次提出名为AIonopedia的离子液体发现智能体。该智能体通过融合大语言模型增强的多模态领域基础模型，不仅能实现精准物性预测，更构建了分层搜索架构以支持分子筛选与设计。基于新构建的综合性离子液体数据集进行训练与评估，本模型展现出卓越性能。对文献报道体系的补充评估表明，该智能体可有效执行离子液体结构优化。突破离线测试局限，我们通过真实湿实验证进一步确认其实际效能：该智能体在具有挑战性的分布外任务中展现出卓越的泛化能力，彰显其加速实际离子液体发现进程的潜力。",
    "url": "https://huggingface.co/papers/2511.11257",
    "arxiv_url": "https://arxiv.org/abs/2511.11257"
  },
  {
    "title": "Virtual Width Networks",
    "summary": "We introduce Virtual Width Networks (VWN), a framework that delivers the benefits of wider representations without incurring the quadratic cost of increasing the hidden size. VWN decouples representational width from backbone width, expanding the embedding space while keeping backbone compute nearly constant. In our large-scale experiment, an 8-times expansion accelerates optimization by over 2 times for next-token and 3 times for next-2-token prediction. The advantage amplifies over training as both the loss gap grows and the convergence-speedup ratio increases, showing that VWN is not only token-efficient but also increasingly effective with scale. Moreover, we identify an approximately log-linear scaling relation between virtual width and loss reduction, offering an initial empirical basis and motivation for exploring virtual-width scaling as a new dimension of large-model efficiency.",
    "translation": "标题：虚拟宽度网络\n\n摘要：本文提出虚拟宽度网络（VWN），该框架能够在避免隐藏层维度增加带来二次计算成本的前提下，实现宽表征的优势。VWN将表征宽度与主干网络宽度解耦，在保持主干计算量基本不变的同时扩展嵌入空间。我们的大规模实验表明：在8倍扩展配置下，下一词元预测的优化速度提升2倍以上，下两词元预测速度提升3倍。随着训练进程推进，损失差距持续扩大且收敛加速比不断增长，证明VWN不仅具有词元效率优势，更随规模扩大持续增强效果。此外，我们发现了虚拟宽度与损失降低之间近似对数线性的缩放规律，这为探索虚拟宽度缩放作为大模型效率新维度提供了初步实证依据和研究动机。",
    "url": "https://huggingface.co/papers/2511.11238",
    "arxiv_url": "https://arxiv.org/abs/2511.11238"
  },
  {
    "title": "LiteAttention: A Temporal Sparse Attention for Diffusion Transformers",
    "summary": "Diffusion Transformers, particularly for video generation, achieve remarkable quality but suffer from quadratic attention complexity, leading to prohibitive latency. Existing acceleration methods face a fundamental trade-off: dynamically estimating sparse attention patterns at each denoising step incurs high computational overhead and estimation errors, while static sparsity patterns remain fixed and often suboptimal throughout denoising. We identify a key structural property of diffusion attention, namely, its sparsity patterns exhibit strong temporal coherence across denoising steps. Tiles deemed non-essential at step t typically remain so at step t+δ. Leveraging this observation, we introduce LiteAttention, a method that exploits temporal coherence to enable evolutionary computation skips across the denoising sequence. By marking non-essential tiles early and propagating skip decisions forward, LiteAttention eliminates redundant attention computations without repeated profiling overheads, combining the adaptivity of dynamic methods with the efficiency of static ones. We implement a highly optimized LiteAttention kernel on top of FlashAttention and demonstrate substantial speedups on production video diffusion models, with no degradation in quality. The code and implementation details will be publicly released.",
    "translation": "标题：LiteAttention：面向扩散变换器的时间稀疏注意力机制\n\n摘要：扩散变换器在视频生成等领域展现出卓越的质量，但其二次方注意力复杂度导致难以承受的计算延迟。现有加速方法面临根本性权衡：在去噪过程的每一步动态估计稀疏注意力模式会产生高昂计算开销和估计误差，而静态稀疏模式在整个去噪过程中保持固定且往往非最优。我们发现扩散注意力具有关键的结构特性——其稀疏模式在连续去噪步骤间呈现显著的时间连贯性。在步骤t中被判定为非关键的图块，通常在步骤t+δ中仍保持非关键状态。基于这一发现，我们提出LiteAttention方法，利用时间连贯性实现跨去噪序列的进化计算跳跃。通过早期标记非关键图块并向前传播跳跃决策，LiteAttention在无需重复性能分析开销的前提下消除冗余注意力计算，兼具动态方法的自适应性与静态方法的高效性。我们在FlashAttention基础上实现了高度优化的LiteAttention内核，在商用视频扩散模型上实现了显著加速，且未造成质量损失。代码与实现细节将公开发布。",
    "url": "https://huggingface.co/papers/2511.11062",
    "arxiv_url": "https://arxiv.org/abs/2511.11062"
  },
  {
    "title": "Simulating the Visual World with Artificial Intelligence: A Roadmap",
    "summary": "The landscape of video generation is shifting, from a focus on generating visually appealing clips to building virtual environments that support interaction and maintain physical plausibility. These developments point toward the emergence of video foundation models that function not only as visual generators but also as implicit world models, models that simulate the physical dynamics, agent-environment interactions, and task planning that govern real or imagined worlds. This survey provides a systematic overview of this evolution, conceptualizing modern video foundation models as the combination of two core components: an implicit world model and a video renderer. The world model encodes structured knowledge about the world, including physical laws, interaction dynamics, and agent behavior. It serves as a latent simulation engine that enables coherent visual reasoning, long-term temporal consistency, and goal-driven planning. The video renderer transforms this latent simulation into realistic visual observations, effectively producing videos as a \"window\" into the simulated world. We trace the progression of video generation through four generations, in which the core capabilities advance step by step, ultimately culminating in a world model, built upon a video generation model, that embodies intrinsic physical plausibility, real-time multimodal interaction, and planning capabilities spanning multiple spatiotemporal scales. For each generation, we define its core characteristics, highlight representative works, and examine their application domains such as robotics, autonomous driving, and interactive gaming. Finally, we discuss open challenges and design principles for next-generation world models, including the role of agent intelligence in shaping and evaluating these systems. An up-to-date list of related works is maintained at this link.",
    "translation": "标题：用人工智能模拟视觉世界：发展路线图\n\n摘要：视频生成领域正在经历重大转型——从关注生成视觉吸引力强的片段，转向构建支持交互并保持物理合理性的虚拟环境。这些进展预示着视频基础模型的出现，它们不仅是视觉生成器，更是隐式世界模型：能够模拟物理动力学、智能体-环境交互以及支配现实或想象世界的任务规划系统。本综述系统梳理了这一演进历程，将现代视频基础模型概念化为两个核心组件的结合：隐式世界模型与视频渲染器。世界模型编码关于世界的结构化知识，包括物理定律、交互动力学和智能体行为，作为潜在模拟引擎实现连贯的视觉推理、长期时间一致性和目标驱动规划；视频渲染器则将这种潜在模拟转化为逼真的视觉观测，使生成的视频成为窥视模拟世界的“窗口”。我们通过四代演进追溯视频生成的发展脉络：核心能力逐步提升，最终形成基于视频生成模型的世界模型，其具备内在物理合理性、实时多模态交互以及跨时空尺度的规划能力。针对每一代，我们界定其核心特征，重点介绍代表性工作，并考察其在机器人、自动驾驶、交互式游戏等领域的应用。最后，我们探讨下一代世界模型的开放挑战与设计原则，包括智能体智能在塑造和评估这些系统中的作用。相关文献动态列表请访问此链接。",
    "url": "https://huggingface.co/papers/2511.08585",
    "arxiv_url": "https://arxiv.org/abs/2511.08585"
  },
  {
    "title": "SpatialThinker: Reinforcing 3D Reasoning in Multimodal LLMs via Spatial Rewards",
    "summary": "Multimodal large language models (MLLMs) have achieved remarkable progress in vision-language tasks, but they continue to struggle with spatial understanding. Existing spatial MLLMs often rely on explicit 3D inputs or architecture-specific modifications, and remain constrained by large-scale datasets or sparse supervision. To address these limitations, we introduce SpatialThinker, a 3D-aware MLLM trained with RL to integrate structured spatial grounding with multi-step reasoning. The model simulates human-like spatial perception by constructing a scene graph of task-relevant objects and spatial relations, and reasoning towards an answer via dense spatial rewards. SpatialThinker consists of two key contributions: (1) a data synthesis pipeline that generates STVQA-7K, a high-quality spatial VQA dataset, and (2) online RL with a multi-objective dense spatial reward enforcing spatial grounding. SpatialThinker-7B outperforms supervised fine-tuning and the sparse RL baseline on spatial understanding and real-world VQA benchmarks, nearly doubling the base-model gain compared to sparse RL, and surpassing GPT-4o. These results showcase the effectiveness of combining spatial supervision with reward-aligned reasoning in enabling robust 3D spatial understanding with limited data and advancing MLLMs towards human-level visual reasoning.",
    "translation": "标题：SpatialThinker：通过空间奖励增强多模态大语言模型的三维推理能力\n\n摘要：多模态大语言模型在视觉语言任务中取得了显著进展，但在空间理解方面仍存在明显不足。现有空间模型往往依赖显式三维输入或特定架构修改，且受限于大规模数据集或稀疏监督。为突破这些限制，我们提出SpatialThinker——一种通过强化学习训练的3D感知多模态大语言模型，将结构化空间定位与多步推理相融合。该模型通过构建任务相关对象与空间关系的场景图，并借助密集空间奖励进行推理推演，模拟类人空间认知能力。本研究的核心贡献包括：（1）开发数据合成流程，生成包含7K样本的高质量空间视觉问答数据集STVQA-7K；（2）采用多目标密集空间奖励的在线强化学习机制以强化空间定位。实验表明，SpatialThinker-7B在空间理解和真实场景视觉问答基准测试中均优于监督微调与稀疏强化学习基线，其基础模型增益较稀疏强化学习提升近一倍，并超越GPT-4o。这些结果验证了将空间监督与奖励对齐推理相结合的有效性，既能以有限数据实现稳健的三维空间理解，又推动多模态大语言模型向人类水平的视觉推理迈进。",
    "url": "https://huggingface.co/papers/2511.07403",
    "arxiv_url": "https://arxiv.org/abs/2511.07403"
  },
  {
    "title": "HI-TransPA: Hearing Impairments Translation Personal Assistant",
    "summary": "To provide a unified and flexible solution for daily communication among hearing-impaired individuals, we introduce the Omni-Model paradigm into assistive technology and present HI-TransPA, an instruction-driven audio-visual personal assistant. The model fuses indistinct speech with high-frame-rate lip dynamics, enabling both translation and dialogue within a single multimodal framework. To tackle the challenges of noisy and heterogeneous raw data and the limited adaptability of existing Omni-Models to hearing-impaired speech, we construct a comprehensive preprocessing and curation pipeline that detects facial landmarks, isolates and stabilizes the lip region, and quantitatively assesses multimodal sample quality. These quality scores guide a curriculum learning strategy that first trains on clean, high-confidence samples and progressively incorporates harder cases to strengthen model robustness. We further adopt a SigLIP encoder combined with a Unified 3D-Resampler to efficiently encode high-frame-rate lip motion. Experiments on our purpose-built HI-Dialogue dataset show that HI-TransPA achieves state-of-the-art performance in both literal accuracy and semantic fidelity. This work establishes a foundation for applying Omni-Models to assistive communication technology, providing an end-to-end modeling framework and essential processing tools for future research.",
    "translation": "标题：HI-TransPA：听力障碍翻译个人助手\n\n摘要：为给听障人士日常交流提供统一灵活的解决方案，我们将全模态范式引入辅助技术领域，提出指令驱动的视听个人助手HI-TransPA。该模型通过融合模糊语音与高帧率唇部动态，在统一多模态框架内实现翻译与对话双重功能。针对原始数据噪声干扰强、异质性明显，以及现有全模态对听障语音适应性不足的挑战，我们构建了完整的预处理流程：检测面部关键点，分离并稳定唇部区域，定量评估多模态样本质量。基于质量评分设计的课程学习策略，优先训练清晰高置信度样本，逐步引入复杂案例以增强模型鲁棒性。采用SigLIP编码器结合统一3D重采样器，实现对高帧率唇部运动的高效编码。在自建的HI-Dialogue数据集上的实验表明，HI-TransPA在字面准确度与语义保真度方面均达到最优性能。本研究为全模态在辅助通信技术的应用奠定基础，为后续研究提供了端到端建模框架与核心处理工具。",
    "url": "https://huggingface.co/papers/2511.09915",
    "arxiv_url": "https://arxiv.org/abs/2511.09915"
  },
  {
    "title": "MarsRL: Advancing Multi-Agent Reasoning System via Reinforcement Learning with Agentic Pipeline Parallelism",
    "summary": "Recent progress in large language models (LLMs) has been propelled by reinforcement learning with verifiable rewards (RLVR) and test-time scaling. However, the limited output length of LLMs constrains the depth of reasoning attainable in a single inference process. Multi-agent reasoning systems offer a promising alternative by employing multiple agents including Solver, Verifier, and Corrector, to iteratively refine solutions. While effective in closed-source models like Gemini 2.5 Pro, they struggle to generalize to open-source models due to insufficient critic and correction capabilities. To address this, we propose MarsRL, a novel reinforcement learning framework with agentic pipeline parallelism, designed to jointly optimize all agents in the system. MarsRL introduces agent-specific reward mechanisms to mitigate reward noise and employs pipeline-inspired training to enhance efficiency in handling long trajectories. Applied to Qwen3-30B-A3B-Thinking-2507, MarsRL improves AIME2025 accuracy from 86.5% to 93.3% and BeyondAIME from 64.9% to 73.8%, even surpassing Qwen3-235B-A22B-Thinking-2507. These findings highlight the potential of MarsRL to advance multi-agent reasoning systems and broaden their applicability across diverse reasoning tasks.",
    "translation": "标题：MarsRL：基于智能体流水线并行强化学习的多智能体推理系统进阶研究\n\n摘要：当前大语言模型的发展主要得益于可验证奖励的强化学习与测试时缩放技术。然而，大语言模型有限的输出长度制约了单次推理过程所能达到的思维深度。多智能体推理系统通过部署求解器、验证器和校正器等多元智能体进行迭代优化，为此提供了颇具前景的替代方案。尽管该体系在Gemini 2.5 Pro等闭源模型中表现优异，但由于开源模型普遍缺乏足够的评判与修正能力，其泛化性能受到限制。为此，我们提出MarsRL——一种融合智能体流水线并行的新型强化学习框架，旨在实现系统中所有智能体的协同优化。该框架通过引入智能体专属奖励机制以降低奖励噪声，并采用流水线式训练策略提升长轨迹处理效率。在Qwen3-30B-A3B-Thinking-2507模型上的实验表明，MarsRL将AIME2025准确率从86.5%提升至93.3%，BeyondAIME准确率从64.9%提升至73.8%，其表现甚至超越Qwen3-235B-A22B-Thinking-2507。这些发现充分证明MarsRL在推动多智能体推理系统发展、拓展其在多样化推理任务中应用边界方面具有重要潜力。",
    "url": "https://huggingface.co/papers/2511.11373",
    "arxiv_url": "https://arxiv.org/abs/2511.11373"
  },
  {
    "title": "RF-DETR: Neural Architecture Search for Real-Time Detection Transformers",
    "summary": "Open-vocabulary detectors achieve impressive performance on COCO, but often fail to generalize to real-world datasets with out-of-distribution classes not typically found in their pre-training. Rather than simply fine-tuning a heavy-weight vision-language model (VLM) for new domains, we introduce RF-DETR, a light-weight specialist detection transformer that discovers accuracy-latency Pareto curves for any target dataset with weight-sharing neural architecture search (NAS). Our approach fine-tunes a pre-trained base network on a target dataset and evaluates thousands of network configurations with different accuracy-latency tradeoffs without re-training. Further, we revisit the \"tunable knobs\" for NAS to improve the transferability of DETRs to diverse target domains. Notably, RF-DETR significantly improves on prior state-of-the-art real-time methods on COCO and Roboflow100-VL. RF-DETR (nano) achieves 48.0 AP on COCO, beating D-FINE (nano) by 5.3 AP at similar latency, and RF-DETR (2x-large) outperforms GroundingDINO (tiny) by 1.2 AP on Roboflow100-VL while running 20x as fast. To the best of our knowledge, RF-DETR (2x-large) is the first real-time detector to surpass 60 AP on COCO. Our code is at https://github.com/roboflow/rf-detr",
    "translation": "标题：RF-DETR：基于神经架构搜索的实时检测Transformer\n\n摘要：开放词汇检测器在COCO数据集上表现出色，但在面对包含预训练阶段未见的分布外类别的真实数据集时往往泛化能力不足。不同于直接对大型视觉语言模型进行新领域的微调，我们提出RF-DETR——一种轻量级专业检测Transformer，通过权重共享神经架构搜索为任意目标数据集构建精度-延迟帕累托曲线。该方法在目标数据集上微调预训练基础网络，无需重新训练即可评估数千种具有不同精度-延迟权衡的网络配置。此外，我们重新审视神经架构搜索的\"可调参数\"，以提升DETR架构在多样化目标领域的迁移能力。值得注意的是，RF-DETR在COCO和Roboflow100-VL数据集上显著超越了现有最先进的实时检测方法。RF-DETR（纳米版）在COCO上达到48.0 AP，在相同延迟条件下较D-FINE（纳米版）提升5.3 AP；RF-DETR（2倍大型版）在Roboflow100-VL上以1.2 AP优势超越GroundingDINO（微型版），且推理速度提升20倍。据我们所知，RF-DETR（2倍大型版）是首个在COCO上突破60 AP的实时检测器。代码已开源：https://github.com/roboflow/rf-detr",
    "url": "https://huggingface.co/papers/2511.09554",
    "arxiv_url": "https://arxiv.org/abs/2511.09554"
  },
  {
    "title": "Experience-Guided Adaptation of Inference-Time Reasoning Strategies",
    "summary": "Enabling agentic AI systems to adapt their problem-solving approaches based on post-training interactions remains a fundamental challenge. While systems that update and maintain a memory at inference time have been proposed, existing designs only steer the system by modifying textual input to a language model or agent, which means that they cannot change sampling parameters, remove tools, modify system prompts, or switch between agentic and workflow paradigms. On the other hand, systems that adapt more flexibly require offline optimization and remain static once deployed. We present Experience-Guided Reasoner (EGuR), which generates tailored strategies -- complete computational procedures involving LLM calls, tools, sampling parameters, and control logic -- dynamically at inference time based on accumulated experience. We achieve this using an LLM-based meta-strategy -- a strategy that outputs strategies -- enabling adaptation of all strategy components (prompts, sampling parameters, tool configurations, and control logic). EGuR operates through two components: a Guide generates multiple candidate strategies conditioned on the current problem and structured memory of past experiences, while a Consolidator integrates execution feedback to improve future strategy generation. This produces complete, ready-to-run strategies optimized for each problem, which can be cached, retrieved, and executed as needed without wasting resources. Across five challenging benchmarks (AIME 2025, 3-SAT, and three Big Bench Extra Hard tasks), EGuR achieves up to 14% accuracy improvements over the strongest baselines while reducing computational costs by up to 111x, with both metrics improving as the system gains experience.",
    "translation": "标题：经验引导的推理时策略自适应方法\n\n摘要：使智能体AI系统能够基于训练后交互自适应调整问题解决方式仍是一项基础性挑战。现有支持推理时更新维护记忆的系统仅通过修改语言模型或智能体的文本输入进行引导，这意味着无法调整采样参数、移除工具、修改系统提示或在智能体与工作流范式间切换。另一方面，具备更强自适应能力的系统需要离线优化且部署后保持静态。我们提出经验引导推理器（EGuR），该系统基于累积经验在推理时动态生成定制化策略——包含LLM调用、工具使用、采样参数与控制逻辑的完整计算流程。这一突破通过基于LLM的元策略（即输出策略的策略）实现，支持对所有策略组件（提示词、采样参数、工具配置与控制逻辑）进行自适应调整。EGuR通过双组件运作：引导器基于当前问题与结构化经验记忆生成多个候选策略，整合器则通过执行反馈优化后续策略生成。该系统能生成针对特定问题优化的完整可执行策略，支持缓存、检索与按需执行，避免资源浪费。在五个高难度基准测试（AIME 2025、3-SAT及三项Big Bench Extra Hard任务）中，EGuR相较最强基线实现最高14%的准确率提升，同时将计算成本降低达111倍，且两项指标均随系统经验积累持续优化。",
    "url": "https://huggingface.co/papers/2511.11519",
    "arxiv_url": "https://arxiv.org/abs/2511.11519"
  },
  {
    "title": "DiscoX: Benchmarking Discourse-Level Translation task in Expert Domains",
    "summary": "The evaluation of discourse-level translation in expert domains remains inadequate, despite its centrality to knowledge dissemination and cross-lingual scholarly communication. While these translations demand discourse-level coherence and strict terminological precision, current evaluation methods predominantly focus on segment-level accuracy and fluency. To address this limitation, we introduce DiscoX, a new benchmark for discourse-level and expert-level Chinese-English translation. It comprises 200 professionally-curated texts from 7 domains, with an average length exceeding 1700 tokens. To evaluate performance on DiscoX, we also develop Metric-S, a reference-free system that provides fine-grained automatic assessments across accuracy, fluency, and appropriateness. Metric-S demonstrates strong consistency with human judgments, significantly outperforming existing metrics. Our experiments reveal a remarkable performance gap: even the most advanced LLMs still trail human experts on these tasks. This finding validates the difficulty of DiscoX and underscores the challenges that remain in achieving professional-grade machine translation. The proposed benchmark and evaluation system provide a robust framework for more rigorous evaluation, facilitating future advancements in LLM-based translation.",
    "translation": "标题：DiscoX：专业领域篇章级翻译任务的基准评测体系\n\n摘要：专业领域的篇章级翻译评估体系尚不完善，尽管其对知识传播与跨语言学术交流具有核心意义。这类翻译任务既要求篇章层面的连贯性，又需要严格的术语准确性，而现有评估方法主要聚焦于句段层面的精确度与流畅性。为突破此局限，我们提出DiscoX——一个面向中英双语的专业级篇章翻译新型基准体系。该体系包含7个专业领域的200篇经专家审校文本，平均长度超过1700词符。为评估系统在DiscoX上的表现，我们同步开发了Metric-S无参考评估系统，可从准确性、流畅度及适配性三个维度提供细粒度自动评测。Metric-S与人工评判呈现高度一致性，显著优于现有评估指标。实验结果显示显著性能差距：即使最先进的大语言模型在这些任务上仍落后于人类专家。该发现验证了DiscoX的挑战难度，揭示了实现专业级机器翻译仍面临的困境。本研究提出的基准体系与评估系统为更严谨的翻译质量评估提供了可靠框架，将推动基于大语言模型的翻译技术持续发展。",
    "url": "https://huggingface.co/papers/2511.10984",
    "arxiv_url": "https://arxiv.org/abs/2511.10984"
  },
  {
    "title": "EmoVid: A Multimodal Emotion Video Dataset for Emotion-Centric Video Understanding and Generation",
    "summary": "Emotion plays a pivotal role in video-based expression, but existing video generation systems predominantly focus on low-level visual metrics while neglecting affective dimensions. Although emotion analysis has made progress in the visual domain, the video community lacks dedicated resources to bridge emotion understanding with generative tasks, particularly for stylized and non-realistic contexts. To address this gap, we introduce EmoVid, the first multimodal, emotion-annotated video dataset specifically designed for creative media, which includes cartoon animations, movie clips, and animated stickers. Each video is annotated with emotion labels, visual attributes (brightness, colorfulness, hue), and text captions. Through systematic analysis, we uncover spatial and temporal patterns linking visual features to emotional perceptions across diverse video forms. Building on these insights, we develop an emotion-conditioned video generation technique by fine-tuning the Wan2.1 model. The results show a significant improvement in both quantitative metrics and the visual quality of generated videos for text-to-video and image-to-video tasks. EmoVid establishes a new benchmark for affective video computing. Our work not only offers valuable insights into visual emotion analysis in artistically styled videos, but also provides practical methods for enhancing emotional expression in video generation.",
    "translation": "标题：EmoVid：面向情感中心视频理解与生成的多模态情感视频数据集\n\n摘要：情感在视频表达中具有关键作用，但现有视频生成系统主要关注低层次视觉指标而忽视情感维度。尽管情感分析在视觉领域已取得进展，视频学界仍缺乏专门资源来衔接情感理解与生成任务，尤其在风格化非写实场景中。为填补这一空白，我们推出EmoVid——首个专为创意媒体设计的多模态情感标注视频数据集，包含卡通动画、影视片段和动态表情包。每个视频均标注有情感标签、视觉属性（亮度、色彩饱和度、色调）及文本描述。通过系统分析，我们揭示了不同视频形式中视觉特征与情感感知关联的时空模式。基于这些发现，我们通过微调Wan2.1模型开发出情感条件视频生成技术。实验结果表明，在文本到视频和图像到视频任务中，生成视频的量化指标与视觉质量均获得显著提升。EmoVid为情感视频计算建立了新基准。本研究不仅为艺术风格化视频的视觉情感分析提供了重要见解，更为增强视频生成中的情感表达提供了实用方法。",
    "url": "https://huggingface.co/papers/2511.11002",
    "arxiv_url": "https://arxiv.org/abs/2511.11002"
  },
  {
    "title": "Don't Waste It: Guiding Generative Recommenders with Structured Human Priors via Multi-head Decoding",
    "summary": "Optimizing recommender systems for objectives beyond accuracy, such as diversity, novelty, and personalization, is crucial for long-term user satisfaction. To this end, industrial practitioners have accumulated vast amounts of structured domain knowledge, which we term human priors (e.g., item taxonomies, temporal patterns). This knowledge is typically applied through post-hoc adjustments during ranking or post-ranking. However, this approach remains decoupled from the core model learning, which is particularly undesirable as the industry shifts to end-to-end generative recommendation foundation models. On the other hand, many methods targeting these beyond-accuracy objectives often require architecture-specific modifications and discard these valuable human priors by learning user intent in a fully unsupervised manner.\n  Instead of discarding the human priors accumulated over years of practice, we introduce a backbone-agnostic framework that seamlessly integrates these human priors directly into the end-to-end training of generative recommenders. With lightweight, prior-conditioned adapter heads inspired by efficient LLM decoding strategies, our approach guides the model to disentangle user intent along human-understandable axes (e.g., interaction types, long- vs. short-term interests). We also introduce a hierarchical composition strategy for modeling complex interactions across different prior types. Extensive experiments on three large-scale datasets demonstrate that our method significantly enhances both accuracy and beyond-accuracy objectives. We also show that human priors allow the backbone model to more effectively leverage longer context lengths and larger model sizes.",
    "translation": "标题：物尽其用：通过多头解码以结构化人类先验引导生成式推荐系统\n\n摘要：为推荐系统优化除准确性之外的目标（如多样性、新颖性和个性化）对实现长期用户满意度至关重要。为此，工业实践者已积累了海量结构化领域知识，我们称之为人类先验（如物品分类体系、时序模式）。这类知识通常通过排序或后排序阶段的后期调整方式应用，但该方法始终与核心模型学习相分离——这在行业向端到端生成式推荐基础模型转型的背景下尤显不足。另一方面，许多针对超准确性目标的方法往往需要特定架构修改，并以完全无监督的方式学习用户意图，从而丢弃了这些宝贵的人类先验。\n\n我们提出了一种与主干模型无关的框架，将人类先验直接无缝集成到生成式推荐器的端到端训练中，而非摒弃多年实践积累的先验知识。通过借鉴高效大语言模型解码策略设计的轻量级先验条件适配头，我们的方法引导模型沿人类可理解的维度（如交互类型、长短期兴趣）解耦用户意图。同时，我们引入了分层组合策略来建模不同先验类型间的复杂交互。在三个大规模数据集上的实验表明，本方法显著提升了准确性及超准确性目标的表现。我们还证实人类先验能使主干模型更有效地利用更长上下文窗口和更大模型容量。",
    "url": "https://huggingface.co/papers/2511.10492",
    "arxiv_url": "https://arxiv.org/abs/2511.10492"
  },
  {
    "title": "Workload Schedulers -- Genesis, Algorithms and Differences",
    "summary": "This paper presents a novel approach to categorization of modern workload schedulers. We provide descriptions of three classes of schedulers: Operating Systems Process Schedulers, Cluster Systems Jobs Schedulers and Big Data Schedulers. We describe their evolution from early adoptions to modern implementations, considering both the use and features of algorithms. In summary, we discuss differences between all presented classes of schedulers and discuss their chronological development. In conclusion we highlight similarities in the focus of scheduling strategies design, applicable to both local and distributed systems.",
    "translation": "标题：工作负载调度器——起源、算法与差异\n\n摘要：本文提出一种现代工作负载调度器的新型分类方法。我们详细描述了三类调度器：操作系统进程调度器、集群系统作业调度器与大数据调度器。通过考察算法应用与功能特性，系统阐述其从早期雏形到现代实现的演进过程。总结部分通过对比分析各类调度器的差异，探讨其历时性发展规律。最后我们着重指出，适用于本地与分布式系统的调度策略设计在核心关注点上呈现出显著共性。",
    "url": "https://huggingface.co/papers/2511.10258",
    "arxiv_url": "https://arxiv.org/abs/2511.10258"
  },
  {
    "title": "Large Language Models for Scientific Idea Generation: A Creativity-Centered Survey",
    "summary": "Scientific idea generation lies at the heart of scientific discovery and has driven human progress-whether by solving unsolved problems or proposing novel hypotheses to explain unknown phenomena. Unlike standard scientific reasoning or general creative generation, idea generation in science is a multi-objective and open-ended task, where the novelty of a contribution is as essential as its empirical soundness. Large language models (LLMs) have recently emerged as promising generators of scientific ideas, capable of producing coherent and factual outputs with surprising intuition and acceptable reasoning, yet their creative capacity remains inconsistent and poorly understood. This survey provides a structured synthesis of methods for LLM-driven scientific ideation, examining how different approaches balance creativity with scientific soundness. We categorize existing methods into five complementary families: External knowledge augmentation, Prompt-based distributional steering, Inference-time scaling, Multi-agent collaboration, and Parameter-level adaptation. To interpret their contributions, we employ two complementary frameworks: Boden's taxonomy of Combinatorial, Exploratory and Transformational creativity to characterize the level of ideas each family expected to generate, and Rhodes' 4Ps framework-Person, Process, Press, and Product-to locate the aspect or source of creativity that each method emphasizes. By aligning methodological advances with creativity frameworks, this survey clarifies the state of the field and outlines key directions toward reliable, systematic, and transformative applications of LLMs in scientific discovery.",
    "translation": "标题：面向科学创意生成的大语言模型：以创造力为核心的综述研究\n\n摘要：科学创意生成是科学发现的核心驱动力，无论是通过解决未解难题还是提出解释未知现象的新颖假说，这一过程始终推动着人类进步。与标准科学推理或通用创意生成不同，科学领域的创意生成具有多目标性和开放性的特点，其贡献的新颖性与实证严谨性同等重要。大语言模型近期展现出作为科学创意生成器的潜力，能够输出兼具连贯性、事实准确性、惊人直觉与合理推理的成果，但其创造能力仍存在不稳定性且缺乏深入理解。本综述对驱动大语言模型科学创意生成的方法进行结构化梳理，重点考察不同方法如何平衡创造力与科学严谨性。我们将现有方法归纳为五个互补体系：外部知识增强、基于提示的分布导向、推理时参数调控、多智能体协作以及参数级适应。为解析其贡献，我们采用两个互补框架：运用博登提出的组合型、探索型与变革型创造力分类法来界定各方法体系预期生成创意的层级，借助罗兹的4P框架（创作者、创作过程、创作环境、创作成果）定位不同方法强调的创造力维度或来源。通过将方法论进展与创造力理论框架相映射，本综述厘清了该领域的发展现状，并为实现大语言模型在科学发现中可靠、系统化且具有变革性应用的关键方向提供了路线指引。",
    "url": "https://huggingface.co/papers/2511.07448",
    "arxiv_url": "https://arxiv.org/abs/2511.07448"
  },
  {
    "title": "Building the Web for Agents: A Declarative Framework for Agent-Web Interaction",
    "summary": "The increasing deployment of autonomous AI agents on the web is hampered by a fundamental misalignment: agents must infer affordances from human-oriented user interfaces, leading to brittle, inefficient, and insecure interactions. To address this, we introduce VOIX, a web-native framework that enables websites to expose reliable, auditable, and privacy-preserving capabilities for AI agents through simple, declarative HTML elements. VOIX introduces <tool> and <context> tags, allowing developers to explicitly define available actions and relevant state, thereby creating a clear, machine-readable contract for agent behavior. This approach shifts control to the website developer while preserving user privacy by disconnecting the conversational interactions from the website. We evaluated the framework's practicality, learnability, and expressiveness in a three-day hackathon study with 16 developers. The results demonstrate that participants, regardless of prior experience, were able to rapidly build diverse and functional agent-enabled web applications. Ultimately, this work provides a foundational mechanism for realizing the Agentic Web, enabling a future of seamless and secure human-AI collaboration on the web.",
    "translation": "标题：构建面向智能体的网络：一种声明式的智能体-网络交互框架\n\n摘要：当前自主AI智能体在网络上的部署受到一个根本性错位的阻碍：智能体必须从面向人类的用户界面推断功能可见性，导致交互过程脆弱、低效且不安全。为解决这一问题，我们提出VOIX——一个原生网络框架，通过简单的声明式HTML元素使网站能够为AI智能体提供可靠、可审计且保护隐私的能力。VOIX引入<tool>和<context>标签，允许开发者明确定义可用操作及相关状态，从而为智能体行为建立清晰的机器可读契约。该方法将控制权转移至网站开发者，同时通过将会话交互与网站分离来保护用户隐私。我们通过为期三天的黑客松研究（16名开发者参与）评估了该框架的实用性、易学性和表达能力。结果表明，无论参与者先前经验如何，均能快速构建多样化且功能完整的智能体赋能网络应用。最终，本研究为实现智能体网络提供了基础机制，为未来网络环境中无缝安全的人机协作铺平道路。",
    "url": "https://huggingface.co/papers/2511.11287",
    "arxiv_url": "https://arxiv.org/abs/2511.11287"
  },
  {
    "title": "CATS-V2V: A Real-World Vehicle-to-Vehicle Cooperative Perception Dataset with Complex Adverse Traffic Scenarios",
    "summary": "Vehicle-to-Vehicle (V2V) cooperative perception has great potential to enhance autonomous driving performance by overcoming perception limitations in complex adverse traffic scenarios (CATS). Meanwhile, data serves as the fundamental infrastructure for modern autonomous driving AI. However, due to stringent data collection requirements, existing datasets focus primarily on ordinary traffic scenarios, constraining the benefits of cooperative perception. To address this challenge, we introduce CATS-V2V, the first-of-its-kind real-world dataset for V2V cooperative perception under complex adverse traffic scenarios. The dataset was collected by two hardware time-synchronized vehicles, covering 10 weather and lighting conditions across 10 diverse locations. The 100-clip dataset includes 60K frames of 10 Hz LiDAR point clouds and 1.26M multi-view 30 Hz camera images, along with 750K anonymized yet high-precision RTK-fixed GNSS and IMU records. Correspondingly, we provide time-consistent 3D bounding box annotations for objects, as well as static scenes to construct a 4D BEV representation. On this basis, we propose a target-based temporal alignment method, ensuring that all objects are precisely aligned across all sensor modalities. We hope that CATS-V2V, the largest-scale, most supportive, and highest-quality dataset of its kind to date, will benefit the autonomous driving community in related tasks.",
    "translation": "标题：CATS-V2V：面向复杂恶劣交通场景的真实世界车车协同感知数据集\n\n摘要：车车协同感知技术通过克服复杂恶劣交通场景下的感知局限，在提升自动驾驶性能方面具有巨大潜力。与此同时，数据作为现代自动驾驶人工智能的基础设施至关重要。然而受限于严苛的数据采集要求，现有数据集主要聚焦于常规交通场景，制约了协同感知效益的充分发挥。为应对这一挑战，我们推出CATS-V2V——全球首个面向复杂恶劣交通场景的真实世界车车协同感知数据集。该数据集通过两辆硬件时间同步的车辆采集完成，涵盖10个不同地理位置的10类天气与光照条件。包含100段数据序列的该数据集提供了6万帧10Hz激光雷达点云与126万张多视角30Hz相机图像，并附有75万条经过匿名化处理的高精度RTK固定GNSS/IMU记录。我们同步提供了时序一致的物体三维边界框标注及静态场景数据，用以构建四维鸟瞰图表征。基于此，我们提出基于目标的时序对齐方法，确保所有物体在全传感器模态间实现精准对齐。我们期待CATS-V2V这一迄今同类数据集中规模最大、支持性最强、质量最高的数据集能够推动自动驾驶领域相关研究的发展。",
    "url": "https://huggingface.co/papers/2511.11168",
    "arxiv_url": "https://arxiv.org/abs/2511.11168"
  },
  {
    "title": "From Proof to Program: Characterizing Tool-Induced Reasoning Hallucinations in Large Language Models",
    "summary": "Tool-augmented Language Models (TaLMs) can invoke external tools to solve problems beyond their parametric capacity. However, it remains unclear whether these tool-enabled gains reflect trustworthy reasoning. Focusing on the Code Interpreter tool, we show that even when tools are selected and executed correctly, TaLMs treat tool outputs as substitutes for reasoning, producing solutions that appear correct but lack coherent justification. We term this failure mode Tool-Induced Myopia (TIM), and study it using PYMATH, a benchmark of 1,679 competition-level mathematical problems for which Python code is helpful but not sufficient. We further develop a multi-dimensional evaluation suite to quantify reasoning degradation in TaLMs relative to their non-tool counterparts. Our findings reveal that while TaLMs achieve up to a 19.3 percentage point gain in final-answer accuracy, their reasoning behavior consistently deteriorates (e.g., non-tool LLMs win up to 41.5% more often in pairwise comparisons of the reasoning process). This degradation intensifies with tool use; the more frequently a model invokes tools, the less coherent its reasoning becomes. Moreover, tool use shifts errors from arithmetic mistakes toward global reasoning failures (logic, assumption, creativity); with TIM present in ~55% of high-risk cases. Finally, we propose a preference-optimization-based framework that realigns TaLMs to use tools as assistive evidence, improving both final-answer accuracy and reasoning depth under tool use. Codes and data are available at: https://github.com/megagonlabs/TIM.",
    "translation": "标题：从证明到程序：大语言模型中工具诱发推理幻觉的特征分析\n\n摘要：工具增强语言模型能够调用外部工具来解决超越其参数能力的问题。然而，这些工具带来的性能提升是否反映可信推理仍不明确。本研究聚焦代码解释器工具，发现即使工具被正确选择和执行，工具增强语言模型仍会将工具输出视为推理替代品，生成看似正确但缺乏连贯论证的解决方案。我们将这种失效模式定义为工具诱发短视，并基于PYMATH基准（包含1,679个竞赛级数学问题，其中Python代码具有辅助作用但非充分解）展开研究。我们进一步开发了多维评估体系，量化工具增强语言模型相较于无工具模型的推理退化现象。研究结果表明：虽然工具增强语言模型的最终答案准确率最高提升19.3个百分点，但其推理行为持续恶化（例如在推理过程的两两比较中，无工具大语言模型胜出率最高提升41.5%）。这种退化随工具使用频次加剧：模型调用工具越频繁，其推理连贯性越差。此外，工具使用使错误类型从算术失误转向全局推理失败（逻辑、假设、创造性错误），约55%的高风险案例中存在工具诱发短视现象。最后，我们提出基于偏好优化的对齐框架，使工具增强语言模型将工具作为辅助证据，在工具使用场景下同步提升最终答案准确率与推理深度。代码与数据详见：https://github.com/megagonlabs/TIM。",
    "url": "https://huggingface.co/papers/2511.10899",
    "arxiv_url": "https://arxiv.org/abs/2511.10899"
  },
  {
    "title": "A Meta-Heuristic Load Balancer for Cloud Computing Systems",
    "summary": "This paper presents a strategy to allocate services on a Cloud system without overloading nodes and maintaining the system stability with minimum cost. We specify an abstract model of cloud resources utilization, including multiple types of resources as well as considerations for the service migration costs. A prototype meta-heuristic load balancer is demonstrated and experimental results are presented and discussed. We also propose a novel genetic algorithm, where population is seeded with the outputs of other meta-heuristic algorithms.",
    "translation": "标题：一种面向云计算系统的元启发式负载均衡器\n\n摘要：本文提出一种云系统服务分配策略，旨在实现节点无过载运行状态下的系统稳定维持与最小成本运维。通过构建云资源利用的抽象模型，该研究综合考虑了多类型资源要素及服务迁移成本因素。实验部分展示了原型元启发式负载均衡器的运行效果，并对测试数据进行了系统性分析与讨论。此外，我们创新性地提出一种混合遗传算法，该算法通过引入其他元启发式算法的输出结果作为初始种群，有效提升了算法性能。",
    "url": "https://huggingface.co/papers/2511.11721",
    "arxiv_url": "https://arxiv.org/abs/2511.11721"
  },
  {
    "title": "miniF2F-Lean Revisited: Reviewing Limitations and Charting a Path Forward",
    "summary": "We perform a thorough analysis of the formal and informal statements in the miniF2F benchmark from the perspective of an AI system that is tasked to participate in a math Olympiad consisting of the problems in miniF2F. In such setting, the model has to read and comprehend the problems in natural language, formalize them in Lean language, then proceed with proving the problems, and it will get credit for each problem if the formal proof corresponds to the original informal statement presented to the model. Our evaluation results reveal that the best accuracy of such pipeline can be about 36% using the SoTA models in the literature, considerably lower than the individual SoTA accuracies, 97% and 69% reported in the autoformalization and theorem proving literature. Analyzing the failure modes, we trace back a considerable portion of this drop to discrepancies between the formal and informal statements for more than half of the problems in miniF2F. We proceed with correcting all the errors, discrepancies and simplifications in formal and informal statements, and present the miniF2F-v2 with fully verified formal and informal statements and proofs. Evaluating the full theorem proving pipeline on miniF2F-v2 leads to the best accuracy of 70%, a significant improvement from the 40% on the original miniF2F, yet indicating considerable misalignment between the autoformalization models and theorem provers. Our deep analysis suggests that a higher quality benchmark can help the community better evaluate progress in the field of formal reasoning and also better diagnose the failure and success modes of autoformalization and theorem proving models. Our dataset is available at https://github.com/roozbeh-yz/miniF2F_v2.",
    "translation": "标题：miniF2F-Lean再审视：局限分析与未来路径规划\n\n摘要：本文从参与数学奥林匹克竞赛的AI系统视角，对miniF2F基准测试中的形式化与非形式化命题展开全面分析。在此场景下，模型需要阅读理解自然语言表述的数学问题，将其形式化为Lean语言，继而完成证明过程。当形式化证明与原始非形式化命题相符时，系统方可获得相应积分。评估结果表明：采用文献中现有最优模型，该流程的最高准确率约为36%，显著低于自动形式化与定理证明领域分别报告的97%与69%的单项最优准确率。通过分析错误模式，我们发现准确率下降的主要根源在于miniF2F中超过半数问题的形式化与非形式化表述存在偏差。我们系统修正了形式化与非形式化陈述中的所有错误、差异及简化问题，最终提出包含完全验证的形式化/非形式化陈述及证明的miniF2F-v2。在新基准上的全流程定理证明测试显示，最佳准确率提升至70%（原miniF2F为40%），但仍反映出自动形式化模型与定理证明器之间存在显著失配。深度分析表明，更高质量的基准测试将有助于学界更精准评估形式推理领域进展，同时更有效诊断自动形式化与定理证明模型的失败与成功模式。本数据集已发布于：https://github.com/roozbeh-yz/miniF2F_v2",
    "url": "https://huggingface.co/papers/2511.03108",
    "arxiv_url": "https://arxiv.org/abs/2511.03108"
  }
]