[
  {
    "title": "mHC: Manifold-Constrained Hyper-Connections",
    "summary": "Recently, studies exemplified by Hyper-Connections (HC) have extended the ubiquitous residual connection paradigm established over the past decade by expanding the residual stream width and diversifying connectivity patterns. While yielding substantial performance gains, this diversification fundamentally compromises the identity mapping property intrinsic to the residual connection, which causes severe training instability and restricted scalability, and additionally incurs notable memory access overhead. To address these challenges, we propose Manifold-Constrained Hyper-Connections (mHC), a general framework that projects the residual connection space of HC onto a specific manifold to restore the identity mapping property, while incorporating rigorous infrastructure optimization to ensure efficiency. Empirical experiments demonstrate that mHC is effective for training at scale, offering tangible performance improvements and superior scalability. We anticipate that mHC, as a flexible and practical extension of HC, will contribute to a deeper understanding of topological architecture design and suggest promising directions for the evolution of foundational models.",
    "translation": "标题：mHC：流形约束超连接\n\n摘要：近年来，以超连接为代表的研究通过扩展残差流宽度并多样化连接模式，拓展了过去十年间建立的普适性残差连接范式。尽管这种多样化带来了显著的性能提升，但它从根本上破坏了残差连接固有的恒等映射特性，导致严重的训练不稳定性和受限的可扩展性，同时还产生了显著的内存访问开销。为应对这些挑战，我们提出流形约束超连接——一种将超连接的残差连接空间投影至特定流形以恢复恒等映射特性的通用框架，并通过严格的基础设施优化确保其效率。实证实验表明，mHC在大规模训练中具有显著效果，能够提供实质性的性能改进与卓越的可扩展性。我们预期mHC作为超连接的灵活实用扩展，将有助于深化对拓扑架构设计的理解，并为基础模型的演进指明具有前景的发展方向。",
    "url": "https://huggingface.co/papers/2512.24880",
    "arxiv_url": "https://arxiv.org/abs/2512.24880"
  },
  {
    "title": "Youtu-LLM: Unlocking the Native Agentic Potential for Lightweight Large Language Models",
    "summary": "We introduce Youtu-LLM, a lightweight yet powerful language model that harmonizes high computational efficiency with native agentic intelligence. Unlike typical small models that rely on distillation, Youtu-LLM (1.96B) is pre-trained from scratch to systematically cultivate reasoning and planning capabilities. The key technical advancements are as follows: (1) Compact Architecture with Long-Context Support: Built on a dense Multi-Latent Attention (MLA) architecture with a novel STEM-oriented vocabulary, Youtu-LLM supports a 128k context window. This design enables robust long-context reasoning and state tracking within a minimal memory footprint, making it ideal for long-horizon agent and reasoning tasks. (2) Principled \"Commonsense-STEM-Agent\" Curriculum: We curated a massive corpus of approximately 11T tokens and implemented a multi-stage training strategy. By progressively shifting the pre-training data distribution from general commonsense to complex STEM and agentic tasks, we ensure the model acquires deep cognitive abilities rather than superficial alignment. (3) Scalable Agentic Mid-training: Specifically for the agentic mid-training, we employ diverse data construction schemes to synthesize rich and varied trajectories across math, coding, and tool-use domains. This high-quality data enables the model to internalize planning and reflection behaviors effectively. Extensive evaluations show that Youtu-LLM sets a new state-of-the-art for sub-2B LLMs. On general benchmarks, it achieves competitive performance against larger models, while on agent-specific tasks, it significantly surpasses existing SOTA baselines, demonstrating that lightweight models can possess strong intrinsic agentic capabilities.",
    "translation": "标题：Youtu-LLM：解锁轻量级大语言模型的原生智能体潜力\n\n摘要：本文介绍Youtu-LLM——一个兼顾高计算效率与原生智能体能力的轻量级语言模型。区别于依赖知识蒸馏的典型小模型，Youtu-LLM（1.96B）通过从头预训练系统化培育推理与规划能力，其核心技术突破包括：（1）支持长上下文的紧凑架构：基于稠密多潜在注意力（MLA）架构与面向STEM任务设计的新型词表，模型支持128k上下文窗口。该设计在极小内存占用量下实现稳健的长程推理与状态追踪，使其特别适用于长周期智能体与推理任务。（2）结构化的“常识-STEM-智能体”渐进式训练框架：我们构建了约11T token的大规模语料库，实施多阶段训练策略。通过将预训练数据分布从通用常识逐步过渡至复杂STEM与智能体任务，确保模型获得深层认知能力而非表面指令对齐。（3）可扩展的智能体中期训练：针对智能体中期训练阶段，我们采用多样化数据构建方案，在数学、编程与工具调用领域合成丰富异构的任务轨迹。高质量数据使模型能有效内化规划与反思行为。大量实验表明，Youtu-LLM在20亿参数以下模型中达到最新最优水平：在通用基准测试中与更大规模模型性能相当，在智能体专项任务上显著超越现有最优基线，证明轻量级模型可具备强大的内生智能体能力。",
    "url": "https://huggingface.co/papers/2512.24618",
    "arxiv_url": "https://arxiv.org/abs/2512.24618"
  },
  {
    "title": "Let It Flow: Agentic Crafting on Rock and Roll, Building the ROME Model within an Open Agentic Learning Ecosystem",
    "summary": "Agentic crafting requires LLMs to operate in real-world environments over multiple turns by taking actions, observing outcomes, and iteratively refining artifacts. Despite its importance, the open-source community lacks a principled, end-to-end ecosystem to streamline agent development. We introduce the Agentic Learning Ecosystem (ALE), a foundational infrastructure that optimizes the production pipeline for agent LLMs. ALE consists of three components: ROLL, a post-training framework for weight optimization; ROCK, a sandbox environment manager for trajectory generation; and iFlow CLI, an agent framework for efficient context engineering. We release ROME (ROME is Obviously an Agentic Model), an open-source agent grounded by ALE and trained on over one million trajectories. Our approach includes data composition protocols for synthesizing complex behaviors and a novel policy optimization algorithm, Interaction-based Policy Alignment (IPA), which assigns credit over semantic interaction chunks rather than individual tokens to improve long-horizon training stability. Empirically, we evaluate ROME within a structured setting and introduce Terminal Bench Pro, a benchmark with improved scale and contamination control. ROME demonstrates strong performance across benchmarks like SWE-bench Verified and Terminal Bench, proving the effectiveness of the ALE infrastructure.",
    "translation": "标题：顺势而为：摇滚乐中的能动性塑造——在开放式能动学习生态系统中构建ROME模型\n\n摘要：能动性塑造要求大语言模型在多轮次真实环境中通过执行动作、观察结果并迭代优化产物来运作。尽管其重要性日益凸显，开源社区目前仍缺乏一个系统化、端到端的生态系统来简化智能体开发流程。本文提出能动学习生态系统（ALE），这是一个优化智能体大语言模型生产流程的基础设施框架。ALE包含三个核心组件：用于权重优化的后训练框架ROLL、用于轨迹生成的沙盒环境管理器ROCK，以及支持高效上下文工程的智能体框架iFlow CLI。我们发布了基于ALE构建、经过超百万条轨迹训练的开源智能体ROME（ROME显然是一个能动模型）。该方法包含合成复杂行为的数据组合协议，以及创新的策略优化算法——基于交互的策略对齐（IPA）。该算法通过在语义交互片段而非单个词元上分配学习信号，显著提升了长周期训练的稳定性。实证研究中，我们在结构化环境中评估ROME，并推出具有更高规模标准与污染控制水平的基准测试Terminal Bench Pro。ROME在SWE-bench Verified和Terminal Bench等基准测试中均表现出色，验证了ALE基础设施的有效性。",
    "url": "https://huggingface.co/papers/2512.24873",
    "arxiv_url": "https://arxiv.org/abs/2512.24873"
  },
  {
    "title": "GaMO: Geometry-aware Multi-view Diffusion Outpainting for Sparse-View 3D Reconstruction",
    "summary": "Recent advances in 3D reconstruction have achieved remarkable progress in high-quality scene capture from dense multi-view imagery, yet struggle when input views are limited. Various approaches, including regularization techniques, semantic priors, and geometric constraints, have been implemented to address this challenge. Latest diffusion-based methods have demonstrated substantial improvements by generating novel views from new camera poses to augment training data, surpassing earlier regularization and prior-based techniques. Despite this progress, we identify three critical limitations in these state-of-the-art approaches: inadequate coverage beyond known view peripheries, geometric inconsistencies across generated views, and computationally expensive pipelines. We introduce GaMO (Geometry-aware Multi-view Outpainter), a framework that reformulates sparse-view reconstruction through multi-view outpainting. Instead of generating new viewpoints, GaMO expands the field of view from existing camera poses, which inherently preserves geometric consistency while providing broader scene coverage. Our approach employs multi-view conditioning and geometry-aware denoising strategies in a zero-shot manner without training. Extensive experiments on Replica and ScanNet++ demonstrate state-of-the-art reconstruction quality across 3, 6, and 9 input views, outperforming prior methods in PSNR and LPIPS, while achieving a 25times speedup over SOTA diffusion-based methods with processing time under 10 minutes. Project page: https://yichuanh.github.io/GaMO/",
    "translation": "标题：GaMO：面向稀疏视图三维重建的几何感知多视角扩散外绘方法\n\n摘要：三维重建领域的最新进展已在密集多视角图像的高质量场景捕捉方面取得显著成果，但在输入视角有限时仍面临挑战。现有研究通过正则化技术、语义先验和几何约束等多种方法应对该问题。基于扩散模型的最新方法通过从新相机位姿生成新视角以扩充训练数据，展现出显著优势，其性能已超越早期的正则化与先验驱动方法。然而，尽管取得进展，我们发现当前先进方法存在三个关键局限：已知视角外围的覆盖不足、生成视角间的几何不一致性以及计算流程的高开销。本文提出GaMO（几何感知多视角外绘框架），该框架通过多视角外绘任务重新定义稀疏视图重建问题。GaMO不生成新视点，而是从现有相机位姿扩展视野范围，这种方法在提供更广场景覆盖的同时，本质上保持了几何一致性。我们的方法采用多视角条件化与几何感知去噪策略，以零样本方式实现而无需额外训练。在Replica和ScanNet++数据集上的大量实验表明，该方法在3、6、9个输入视角下均达到最先进的重建质量，在PSNR和LPIPS指标上超越现有方法，同时相比基于扩散的先进方法实现25倍加速，处理时间低于10分钟。项目页面：https://yichuanh.github.io/GaMO/",
    "url": "https://huggingface.co/papers/2512.25073",
    "arxiv_url": "https://arxiv.org/abs/2512.25073"
  },
  {
    "title": "A unified framework for detecting point and collective anomalies in operating system logs via collaborative transformers",
    "summary": "Log anomaly detection is crucial for preserving the security of operating systems. Depending on the source of log data collection, various information is recorded in logs that can be considered log modalities. In light of this intuition, unimodal methods often struggle by ignoring the different modalities of log data. Meanwhile, multimodal methods fail to handle the interactions between these modalities. Applying multimodal sentiment analysis to log anomaly detection, we propose CoLog, a framework that collaboratively encodes logs utilizing various modalities. CoLog utilizes collaborative transformers and multi-head impressed attention to learn interactions among several modalities, ensuring comprehensive anomaly detection. To handle the heterogeneity caused by these interactions, CoLog incorporates a modality adaptation layer, which adapts the representations from different log modalities. This methodology enables CoLog to learn nuanced patterns and dependencies within the data, enhancing its anomaly detection capabilities. Extensive experiments demonstrate CoLog's superiority over existing state-of-the-art methods. Furthermore, in detecting both point and collective anomalies, CoLog achieves a mean precision of 99.63%, a mean recall of 99.59%, and a mean F1 score of 99.61% across seven benchmark datasets for log-based anomaly detection. The comprehensive detection capabilities of CoLog make it highly suitable for cybersecurity, system monitoring, and operational efficiency. CoLog represents a significant advancement in log anomaly detection, providing a sophisticated and effective solution to point and collective anomaly detection through a unified framework and a solution to the complex challenges automatic log data analysis poses. We also provide the implementation of CoLog at https://github.com/NasirzadehMoh/CoLog.",
    "translation": "标题：基于协同Transformer的操作系统日志点异常与集体异常统一检测框架\n\n摘要：日志异常检测对维护操作系统安全至关重要。根据日志数据采集来源的不同，日志中记录着可被视为日志模态的多样化信息。基于这一认知，单模态方法常因忽略日志数据的多模态特性而存在局限，而多模态方法则未能有效处理模态间的交互关系。本文将多模态情感分析应用于日志异常检测，提出CoLog框架，该框架通过协同编码机制综合利用多种日志模态。CoLog采用协同Transformer架构与多头增强注意力机制，学习多模态间的交互关系，确保异常检测的全面性。为处理模态交互导致的异构性问题，CoLog引入模态适配层以自适应调整不同日志模态的表征。该方法使CoLog能够学习数据中细微的模式与依赖关系，从而提升异常检测能力。大量实验证明CoLog在现有先进方法中具有显著优势。在七个日志异常检测基准数据集上，CoLog对点异常与集体异常的检测平均精确率达到99.63%，平均召回率为99.59%，平均F1分数达99.61%。CoLog的全面检测能力使其高度适用于网络安全、系统监控与运行效率优化领域。该框架通过统一架构为点异常与集体异常检测提供了精密有效的解决方案，并攻克了自动日志数据分析中的复杂挑战，标志着日志异常检测领域的重大进展。我们在https://github.com/NasirzadehMoh/CoLog公开了CoLog的实现代码。",
    "url": "https://huggingface.co/papers/2512.23380",
    "arxiv_url": "https://arxiv.org/abs/2512.23380"
  },
  {
    "title": "Scaling Open-Ended Reasoning to Predict the Future",
    "summary": "High-stakes decision making involves reasoning under uncertainty about the future. In this work, we train language models to make predictions on open-ended forecasting questions. To scale up training data, we synthesize novel forecasting questions from global events reported in daily news, using a fully automated, careful curation recipe. We train the Qwen3 thinking models on our dataset, OpenForesight. To prevent leakage of future information during training and evaluation, we use an offline news corpus, both for data generation and retrieval in our forecasting system. Guided by a small validation set, we show the benefits of retrieval, and an improved reward function for reinforcement learning (RL). Once we obtain our final forecasting system, we perform held-out testing between May to August 2025. Our specialized model, OpenForecaster 8B, matches much larger proprietary models, with our training improving the accuracy, calibration, and consistency of predictions. We find calibration improvements from forecasting training generalize across popular benchmarks. We open-source all our models, code, and data to make research on language model forecasting broadly accessible.",
    "translation": "标题：扩展开放式推理以预测未来\n\n摘要：高风险决策涉及在不确定性条件下对未来进行推理。本研究训练语言模型对开放式预测问题做出预测。为扩大训练数据规模，我们基于每日新闻报道的全球事件，采用全自动、精细化的筛选方法合成新型预测问题。我们在自建数据集OpenForesight上训练Qwen3思维模型。为防止训练和评估过程中未来信息泄露，我们使用离线新闻语料库进行数据生成及预测系统的检索。通过小规模验证集的指导，我们证明了检索机制的优势以及改进的强化学习奖励函数的有效性。在获得最终预测系统后，我们对2025年5月至8月期间的数据进行留出测试。我们的专用模型OpenForecaster 8B在预测准确性、校准度和一致性方面均取得提升，其性能可媲美规模更大的专有模型。研究发现预测训练带来的校准改进可泛化至主流基准测试。我们开源所有模型、代码和数据，以促进语言模型预测研究的广泛开展。",
    "url": "https://huggingface.co/papers/2512.25070",
    "arxiv_url": "https://arxiv.org/abs/2512.25070"
  },
  {
    "title": "PhyGDPO: Physics-Aware Groupwise Direct Preference Optimization for Physically Consistent Text-to-Video Generation",
    "summary": "Recent advances in text-to-video (T2V) generation have achieved good visual quality, yet synthesizing videos that faithfully follow physical laws remains an open challenge. Existing methods mainly based on graphics or prompt extension struggle to generalize beyond simple simulated environments or learn implicit physical reasoning. The scarcity of training data with rich physics interactions and phenomena is also a problem. In this paper, we first introduce a Physics-Augmented video data construction Pipeline, PhyAugPipe, that leverages a vision-language model (VLM) with chain-of-thought reasoning to collect a large-scale training dataset, PhyVidGen-135K. Then we formulate a principled Physics-aware Groupwise Direct Preference Optimization, PhyGDPO, framework that builds upon the groupwise Plackett-Luce probabilistic model to capture holistic preferences beyond pairwise comparisons. In PhyGDPO, we design a Physics-Guided Rewarding (PGR) scheme that embeds VLM-based physics rewards to steer optimization toward physical consistency. We also propose a LoRA-Switch Reference (LoRA-SR) scheme that eliminates memory-heavy reference duplication for efficient training. Experiments show that our method significantly outperforms state-of-the-art open-source methods on PhyGenBench and VideoPhy2. Please check our project page at https://caiyuanhao1998.github.io/project/PhyGDPO for more video results. Our code, models, and data will be released at https://github.com/caiyuanhao1998/Open-PhyGDPO",
    "translation": "标题：PhyGDPO：面向物理一致性文本到视频生成的物理感知分组直接偏好优化\n\n摘要：当前文本到视频（T2V）生成技术虽在视觉质量上取得了显著进展，但合成严格遵循物理规律的真实视频仍是一个开放挑战。现有方法主要基于图形学或提示扩展，难以在简单模拟环境之外实现泛化，或难以学习隐式的物理推理。同时，缺乏包含丰富物理交互与现象的训练数据也是关键瓶颈。本文首先提出一种物理增强视频数据构建流程（PhyAugPipe），该流程利用具备思维链推理能力的视觉语言模型（VLM）收集大规模训练数据集PhyVidGen-135K。随后，我们构建了一种基于原则的物理感知分组直接偏好优化框架（PhyGDPO），该框架以分组Plackett-Luce概率模型为基础，能够捕捉超越成对比较的整体偏好关系。在PhyGDPO中，我们设计了物理引导奖励机制（PGR），通过嵌入基于VLM的物理奖励来引导优化过程朝向物理一致性目标。此外，我们还提出LoRA切换参考机制（LoRA-SR），以消除训练中繁重的参考模型复制开销，实现高效训练。实验表明，我们的方法在PhyGenBench和VideoPhy2基准上显著优于当前最先进的开源方法。更多视频结果请访问项目页面：https://caiyuanhao1998.github.io/project/PhyGDPO。代码、模型与数据将在https://github.com/caiyuanhao1998/Open-PhyGDPO开源发布。",
    "url": "https://huggingface.co/papers/2512.24551",
    "arxiv_url": "https://arxiv.org/abs/2512.24551"
  },
  {
    "title": "AI Meets Brain: Memory Systems from Cognitive Neuroscience to Autonomous Agents",
    "summary": "Memory serves as the pivotal nexus bridging past and future, providing both humans and AI systems with invaluable concepts and experience to navigate complex tasks. Recent research on autonomous agents has increasingly focused on designing efficient memory workflows by drawing on cognitive neuroscience. However, constrained by interdisciplinary barriers, existing works struggle to assimilate the essence of human memory mechanisms. To bridge this gap, we systematically synthesizes interdisciplinary knowledge of memory, connecting insights from cognitive neuroscience with LLM-driven agents. Specifically, we first elucidate the definition and function of memory along a progressive trajectory from cognitive neuroscience through LLMs to agents. We then provide a comparative analysis of memory taxonomy, storage mechanisms, and the complete management lifecycle from both biological and artificial perspectives. Subsequently, we review the mainstream benchmarks for evaluating agent memory. Additionally, we explore memory security from dual perspectives of attack and defense. Finally, we envision future research directions, with a focus on multimodal memory systems and skill acquisition.",
    "translation": "标题：AI与大脑相遇：从认知神经科学到自主智能体的记忆系统\n\n摘要：记忆作为连接过去与未来的关键枢纽，为人类与人工智能系统提供了驾驭复杂任务的宝贵概念与经验。当前自主智能体的研究日益聚焦于借鉴认知神经科学设计高效记忆工作流，但受限于学科壁垒，现有研究难以融合人类记忆机制的精髓。为弥合这一鸿沟，本研究系统整合跨学科记忆知识，将认知神经科学的洞见与基于大语言模型的智能体相联结。具体而言，我们首先沿着从认知神经科学到大语言模型再到智能体的演进路径，阐释记忆的定义与功能；继而从生物与人工双重视角，对记忆分类体系、存储机制及完整管理生命周期展开对比分析；随后梳理评估智能体记忆的主流基准框架；此外，从攻击与防御双重维度探讨记忆安全性问题；最后展望未来研究方向，重点关注多模态记忆系统与技能习得领域。",
    "url": "https://huggingface.co/papers/2512.23343",
    "arxiv_url": "https://arxiv.org/abs/2512.23343"
  },
  {
    "title": "GR-Dexter Technical Report",
    "summary": "Vision-language-action (VLA) models have enabled language-conditioned, long-horizon robot manipulation, but most existing systems are limited to grippers. Scaling VLA policies to bimanual robots with high degree-of-freedom (DoF) dexterous hands remains challenging due to the expanded action space, frequent hand-object occlusions, and the cost of collecting real-robot data. We present GR-Dexter, a holistic hardware-model-data framework for VLA-based generalist manipulation on a bimanual dexterous-hand robot. Our approach combines the design of a compact 21-DoF robotic hand, an intuitive bimanual teleoperation system for real-robot data collection, and a training recipe that leverages teleoperated robot trajectories together with large-scale vision-language and carefully curated cross-embodiment datasets. Across real-world evaluations spanning long-horizon everyday manipulation and generalizable pick-and-place, GR-Dexter achieves strong in-domain performance and improved robustness to unseen objects and unseen instructions. We hope GR-Dexter serves as a practical step toward generalist dexterous-hand robotic manipulation.",
    "translation": "标题：GR-Dexter技术报告\n\n摘要：视觉-语言-动作模型已能实现语言条件化的长时程机器人操控，但现有系统大多局限于夹爪式末端执行器。将视觉-语言-动作策略扩展到配备高自由度灵巧手的双臂机器人仍面临诸多挑战，包括动作空间维度扩张、频繁的手-物体遮挡以及真实机器人数据采集成本高昂。本文提出GR-Dexter——一个面向双臂灵巧手机器人的视觉-语言-动作通用操控硬件-模型-数据一体化框架。该框架融合了三项核心设计：紧凑型21自由度机械手结构、面向真实机器人数据采集的直观双臂遥操作系统，以及融合遥操作机器人轨迹数据、大规模视觉-语言数据集与精细化跨具身数据集的训练方案。在涵盖长时程日常操作与泛化抓放任务的真实环境评估中，GR-Dexter在领域内任务表现出色，并对未见物体与未知指令展现出更强的鲁棒性。我们期待GR-Dexter能为通用灵巧手机器人操控研究提供切实可行的技术路径。",
    "url": "https://huggingface.co/papers/2512.24210",
    "arxiv_url": "https://arxiv.org/abs/2512.24210"
  },
  {
    "title": "Guiding a Diffusion Transformer with the Internal Dynamics of Itself",
    "summary": "The diffusion model presents a powerful ability to capture the entire (conditional) data distribution. However, due to the lack of sufficient training and data to learn to cover low-probability areas, the model will be penalized for failing to generate high-quality images corresponding to these areas. To achieve better generation quality, guidance strategies such as classifier free guidance (CFG) can guide the samples to the high-probability areas during the sampling stage. However, the standard CFG often leads to over-simplified or distorted samples. On the other hand, the alternative line of guiding diffusion model with its bad version is limited by carefully designed degradation strategies, extra training and additional sampling steps. In this paper, we proposed a simple yet effective strategy Internal Guidance (IG), which introduces an auxiliary supervision on the intermediate layer during training process and extrapolates the intermediate and deep layer's outputs to obtain generative results during sampling process. This simple strategy yields significant improvements in both training efficiency and generation quality on various baselines. On ImageNet 256x256, SiT-XL/2+IG achieves FID=5.31 and FID=1.75 at 80 and 800 epochs. More impressively, LightningDiT-XL/1+IG achieves FID=1.34 which achieves a large margin between all of these methods. Combined with CFG, LightningDiT-XL/1+IG achieves the current state-of-the-art FID of 1.19.",
    "translation": "标题：基于自身内部动态引导的扩散Transformer模型\n\n摘要：扩散模型展现出捕捉完整（条件）数据分布的强大能力。然而，由于缺乏足够的训练数据来学习覆盖低概率区域，模型在生成这些区域对应的高质量图像时会受到性能制约。为提升生成质量，可采用无分类器引导等策略在采样阶段将样本导向高概率区域，但标准无分类器引导常导致样本过度简化或失真。另一方面，基于退化版本引导扩散模型的替代方案受限于精心设计的退化策略、额外训练步骤和附加采样流程。本文提出一种简洁有效的内部引导策略，通过在训练阶段对中间层引入辅助监督，并在采样阶段外推中间层与深层输出以获取生成结果。该策略在多种基线模型上显著提升了训练效率与生成质量：在ImageNet 256×256数据集上，SiT-XL/2+IG模型在80轮和800轮训练后分别达到FID=5.31和FID=1.75；更突出的是，LightningDiT-XL/1+IG模型取得FID=1.34，显著超越现有方法。结合无分类器引导后，LightningDiT-XL/1+IG模型以FID=1.19的成绩达到当前最优水平。",
    "url": "https://huggingface.co/papers/2512.24176",
    "arxiv_url": "https://arxiv.org/abs/2512.24176"
  },
  {
    "title": "Fantastic Reasoning Behaviors and Where to Find Them: Unsupervised Discovery of the Reasoning Process",
    "summary": "Despite the growing reasoning capabilities of recent large language models (LLMs), their internal mechanisms during the reasoning process remain underexplored. Prior approaches often rely on human-defined concepts (e.g., overthinking, reflection) at the word level to analyze reasoning in a supervised manner. However, such methods are limited, as it is infeasible to capture the full spectrum of potential reasoning behaviors, many of which are difficult to define in token space. In this work, we propose an unsupervised framework (namely, RISE: Reasoning behavior Interpretability via Sparse auto-Encoder) for discovering reasoning vectors, which we define as directions in the activation space that encode distinct reasoning behaviors. By segmenting chain-of-thought traces into sentence-level 'steps' and training sparse auto-encoders (SAEs) on step-level activations, we uncover disentangled features corresponding to interpretable behaviors such as reflection and backtracking. Visualization and clustering analyses show that these behaviors occupy separable regions in the decoder column space. Moreover, targeted interventions on SAE-derived vectors can controllably amplify or suppress specific reasoning behaviors, altering inference trajectories without retraining. Beyond behavior-specific disentanglement, SAEs capture structural properties such as response length, revealing clusters of long versus short reasoning traces. More interestingly, SAEs enable the discovery of novel behaviors beyond human supervision. We demonstrate the ability to control response confidence by identifying confidence-related vectors in the SAE decoder space. These findings underscore the potential of unsupervised latent discovery for both interpreting and controllably steering reasoning in LLMs.",
    "translation": "标题：非凡推理行为及其发现：推理过程的无监督探索\n\n摘要：尽管近期大型语言模型（LLM）的推理能力不断提升，但其推理过程中的内部机制仍未得到充分探索。现有方法通常依赖于词汇层面的人工定义概念（如过度思考、反思）以监督方式分析推理行为，但这类方法存在局限——难以覆盖所有潜在的推理行为谱系，且许多行为难以在词元空间中明确定义。本研究提出一种无监督框架（RISE：基于稀疏自编码器的推理行为可解释性方法），用于发现推理向量。我们将推理向量定义为激活空间中编码特定推理行为的方向。通过将思维链轨迹分割为句子级“步骤”，并在步骤级激活上训练稀疏自编码器（SAE），我们分离出对应可解释行为（如反思与回溯）的解耦特征。可视化与聚类分析表明，这些行为在解码器列空间中占据可分离区域。进一步地，通过对SAE衍生向量进行定向干预，我们能够可控地增强或抑制特定推理行为，从而在不重新训练的情况下改变推理轨迹。除行为特异性解耦外，SAE还能捕捉响应长度等结构特征，揭示长推理轨迹与短推理轨迹的聚类模式。更有趣的是，SAE能够发现超越人类监督范畴的新颖行为。我们通过识别SAE解码器空间中与置信度相关的向量，展示了调控响应置信度的能力。这些发现凸显了无监督潜在特征发现方法在解释与可控引导LLM推理方面的潜力。",
    "url": "https://huggingface.co/papers/2512.23988",
    "arxiv_url": "https://arxiv.org/abs/2512.23988"
  },
  {
    "title": "Pretraining Frame Preservation in Autoregressive Video Memory Compression",
    "summary": "We present PFP, a neural network structure to compress long videos into short contexts, with an explicit pretraining objective to preserve the high-frequency details of single frames at arbitrary temporal positions. The baseline model can compress a 20-second video into a context at about 5k length, where random frames can be retrieved with perceptually preserved appearances. Such pretrained models can be directly fine-tuned as memory encoders for autoregressive video models, enabling long history memory with low context cost and relatively low fidelity loss. We evaluate the framework with ablative settings and discuss the trade-offs of possible neural architecture designs.",
    "translation": "标题：自回归视频记忆压缩中的预训练帧保持技术\n\n摘要：本文提出PFP神经网络结构，该结构能够将长视频压缩为短上下文表示，并通过明确的预训练目标来保留任意时间位置上单帧图像的高频细节。基线模型可将20秒视频压缩至约5k长度的上下文表示，在此过程中随机帧的视觉外观特征能够得到感知层面的保留。此类预训练模型可直接微调为自回归视频模型的记忆编码器，实现以较低上下文成本存储长时历史信息，同时保持相对较低的保真度损失。我们通过消融实验评估该框架，并探讨不同神经网络架构设计在性能上的权衡关系。",
    "url": "https://huggingface.co/papers/2512.23851",
    "arxiv_url": "https://arxiv.org/abs/2512.23851"
  },
  {
    "title": "SpaceTimePilot: Generative Rendering of Dynamic Scenes Across Space and Time",
    "summary": "We present SpaceTimePilot, a video diffusion model that disentangles space and time for controllable generative rendering. Given a monocular video, SpaceTimePilot can independently alter the camera viewpoint and the motion sequence within the generative process, re-rendering the scene for continuous and arbitrary exploration across space and time. To achieve this, we introduce an effective animation time-embedding mechanism in the diffusion process, allowing explicit control of the output video's motion sequence with respect to that of the source video. As no datasets provide paired videos of the same dynamic scene with continuous temporal variations, we propose a simple yet effective temporal-warping training scheme that repurposes existing multi-view datasets to mimic temporal differences. This strategy effectively supervises the model to learn temporal control and achieve robust space-time disentanglement. To further enhance the precision of dual control, we introduce two additional components: an improved camera-conditioning mechanism that allows altering the camera from the first frame, and CamxTime, the first synthetic space-and-time full-coverage rendering dataset that provides fully free space-time video trajectories within a scene. Joint training on the temporal-warping scheme and the CamxTime dataset yields more precise temporal control. We evaluate SpaceTimePilot on both real-world and synthetic data, demonstrating clear space-time disentanglement and strong results compared to prior work. Project page: https://zheninghuang.github.io/Space-Time-Pilot/ Code: https://github.com/ZheningHuang/spacetimepilot",
    "translation": "标题：SpaceTimePilot：跨时空动态场景的生成式渲染\n\n摘要：本文提出SpaceTimePilot，一种能够解耦空间与时间以实现可控生成式渲染的视频扩散模型。给定单目视频输入，SpaceTimePilot可在生成过程中独立调整摄像机视点与运动序列，实现对场景在连续时空维度上的任意探索与重渲染。为实现这一目标，我们在扩散过程中引入了一种高效的动画时间嵌入机制，使模型能够显式控制输出视频相对于源视频的运动序列。由于现有数据集缺乏具有连续时序变化的同一动态场景配对视频，我们提出了一种简单而有效的时序扭曲训练方案，通过改造现有多视角数据集来模拟时序差异。该策略有效监督模型学习时序控制，实现稳健的时空解耦。为进一步提升双控精度，我们引入两个关键组件：改进的摄像机条件机制（支持从首帧开始调整摄像机参数）以及CamxTime数据集——首个提供场景内完全自由时空视频轨迹的合成时空全覆盖渲染数据集。通过时序扭曲方案与CamxTime数据集的联合训练，模型实现了更精确的时序控制。我们在真实数据与合成数据上评估SpaceTimePilot，实验结果显示出清晰的时空解耦特性，且相较于现有方法具有显著优势。项目主页：https://zheninghuang.github.io/Space-Time-Pilot/ 代码仓库：https://github.com/ZheningHuang/spacetimepilot",
    "url": "https://huggingface.co/papers/2512.25075",
    "arxiv_url": "https://arxiv.org/abs/2512.25075"
  },
  {
    "title": "Forging Spatial Intelligence: A Roadmap of Multi-Modal Data Pre-Training for Autonomous Systems",
    "summary": "The rapid advancement of autonomous systems, including self-driving vehicles and drones, has intensified the need to forge true Spatial Intelligence from multi-modal onboard sensor data. While foundation models excel in single-modal contexts, integrating their capabilities across diverse sensors like cameras and LiDAR to create a unified understanding remains a formidable challenge. This paper presents a comprehensive framework for multi-modal pre-training, identifying the core set of techniques driving progress toward this goal. We dissect the interplay between foundational sensor characteristics and learning strategies, evaluating the role of platform-specific datasets in enabling these advancements. Our central contribution is the formulation of a unified taxonomy for pre-training paradigms: ranging from single-modality baselines to sophisticated unified frameworks that learn holistic representations for advanced tasks like 3D object detection and semantic occupancy prediction. Furthermore, we investigate the integration of textual inputs and occupancy representations to facilitate open-world perception and planning. Finally, we identify critical bottlenecks, such as computational efficiency and model scalability, and propose a roadmap toward general-purpose multi-modal foundation models capable of achieving robust Spatial Intelligence for real-world deployment.",
    "translation": "标题：锻造空间智能：面向自主系统的多模态数据预训练路线图\n\n摘要：自动驾驶汽车与无人机等自主系统的快速发展，日益凸显了从多模态车载传感器数据中锻造真正空间智能的迫切需求。尽管基础模型在单一模态场景中表现出色，但如何整合其在相机、激光雷达等异构传感器上的能力，以形成统一的环境理解，仍是一项艰巨挑战。本文提出了一个全面的多模态预训练框架，系统梳理了推动该领域进展的核心技术体系。我们深入剖析了基础传感器特性与学习策略之间的相互作用，评估了特定平台数据集对技术发展的支撑作用。本研究的核心贡献在于构建了预训练范式的统一分类体系：从单模态基线方法，到能够为三维目标检测、语义占据预测等高级任务学习整体表征的复杂统一框架。此外，我们探讨了文本输入与占据表征的融合机制，以促进开放世界感知与规划能力。最后，我们指出了计算效率与模型可扩展性等关键瓶颈，并规划了通向通用多模态基础模型的发展路线，旨在为实现实际部署所需的鲁棒空间智能提供技术路径。",
    "url": "https://huggingface.co/papers/2512.24385",
    "arxiv_url": "https://arxiv.org/abs/2512.24385"
  },
  {
    "title": "Figure It Out: Improving the Frontier of Reasoning with Active Visual Thinking",
    "summary": "Complex reasoning problems often involve implicit spatial, geometric, and structural relationships that are not explicitly encoded in text. While recent reasoning models have achieved strong performance across many domains, purely text-based reasoning struggles to represent global structural constraints in complex settings. In this paper, we introduce FIGR, which integrates active visual thinking into multi-turn reasoning via end-to-end reinforcement learning. FIGR externalizes intermediate structural hypotheses by constructing visual representations during problem solving. By adaptively regulating when and how visual reasoning should be invoked, FIGR enables more stable and coherent reasoning over global structural properties that are difficult to capture from text alone. Experiments on challenging mathematical reasoning benchmarks demonstrate that FIGR outperforms strong text-only chain-of-thought baselines. In particular, FIGR improves the base model by 13.12% on AIME 2025 and 11.00% on BeyondAIME, highlighting the effectiveness of figure-guided multimodal reasoning in enhancing the stability and reliability of complex reasoning.",
    "translation": "标题：图解求解：通过主动视觉思维提升推理前沿性能\n\n摘要：复杂推理问题通常涉及隐含的空间、几何与结构关系，这些关系无法通过文本直接编码。尽管当前推理模型已在多领域取得显著成果，但纯文本推理在复杂场景中难以表征全局结构约束。本文提出FIGR模型，通过端到端强化学习将主动视觉思维融入多轮推理过程。该模型在问题求解过程中通过构建视觉表征，将中间结构假设外部化。通过自适应调控视觉推理的触发时机与方式，FIGR能够对纯文本难以捕捉的全局结构特性进行更稳定、连贯的推理。在具有挑战性的数学推理基准测试中，实验表明FIGR显著优于纯文本思维链基线模型。具体而言，FIGR在AIME 2025数据集上将基础模型性能提升13.12%，在BeyondAIME数据集上提升11.00%，这凸显了图示引导的多模态推理在增强复杂推理稳定性与可靠性方面的有效性。",
    "url": "https://huggingface.co/papers/2512.24297",
    "arxiv_url": "https://arxiv.org/abs/2512.24297"
  },
  {
    "title": "JavisGPT: A Unified Multi-modal LLM for Sounding-Video Comprehension and Generation",
    "summary": "This paper presents JavisGPT, the first unified multimodal large language model (MLLM) for Joint Audio-Video (JAV) comprehension and generation. JavisGPT adopts a concise encoder-LLM-decoder architecture, featuring a SyncFusion module for spatio-temporal audio-video fusion and synchrony-aware learnable queries to bridge a pretrained JAV-DiT generator. This design enables temporally coherent video-audio understanding and generation from multimodal instructions. We design an effective three-stage training pipeline consisting of multimodal pretraining, audio-video fine-tuning, and large-scale instruction-tuning, to progressively build multimodal comprehension and generation from existing vision-language models. To support this, we further construct JavisInst-Omni, a high-quality instruction dataset with over 200K GPT-4o-curated audio-video-text dialogues that span diverse and multi-level comprehension and generation scenarios. Extensive experiments on JAV comprehension and generation benchmarks show that JavisGPT outperforms existing MLLMs, particularly in complex and temporally synchronized settings.",
    "translation": "标题：JavisGPT：面向音视频理解与生成的统一多模态大语言模型\n\n摘要：本文提出JavisGPT，这是首个面向音视频联合理解与生成的统一多模态大语言模型。JavisGPT采用简洁的编码器-大语言模型-解码器架构，其核心是通过时空音视频融合模块与同步感知可学习查询机制，桥接预训练的音视频扩散变换生成器。该设计实现了基于多模态指令的时序一致性音视频理解与生成。我们设计了一套高效的三阶段训练流程，包含多模态预训练、音视频微调和大规模指令调优，逐步在现有视觉语言模型基础上构建多模态理解与生成能力。为支持训练，我们进一步构建了JavisInst-Omni高质量指令数据集，包含超过20万条由GPT-4o生成的音视频文本对话，涵盖多样化、多层次的理解与生成场景。在音视频理解与生成基准测试上的大量实验表明，JavisGPT性能优于现有多模态大语言模型，尤其在复杂时序同步任务中表现突出。",
    "url": "https://huggingface.co/papers/2512.22905",
    "arxiv_url": "https://arxiv.org/abs/2512.22905"
  },
  {
    "title": "Geometry-Aware Optimization for Respiratory Sound Classification: Enhancing Sensitivity with SAM-Optimized Audio Spectrogram Transformers",
    "summary": "Respiratory sound classification is hindered by the limited size, high noise levels, and severe class imbalance of benchmark datasets like ICBHI 2017. While Transformer-based models offer powerful feature extraction capabilities, they are prone to overfitting and often converge to sharp minima in the loss landscape when trained on such constrained medical data. To address this, we introduce a framework that enhances the Audio Spectrogram Transformer (AST) using Sharpness-Aware Minimization (SAM). Instead of merely minimizing the training loss, our approach optimizes the geometry of the loss surface, guiding the model toward flatter minima that generalize better to unseen patients. We also implement a weighted sampling strategy to handle class imbalance effectively. Our method achieves a state-of-the-art score of 68.10% on the ICBHI 2017 dataset, outperforming existing CNN and hybrid baselines. More importantly, it reaches a sensitivity of 68.31%, a crucial improvement for reliable clinical screening. Further analysis using t-SNE and attention maps confirms that the model learns robust, discriminative features rather than memorizing background noise.",
    "translation": "标题：面向呼吸音分类的几何感知优化：基于SAM优化的音频谱图Transformer提升灵敏度\n\n摘要：呼吸音分类任务受到ICBHI 2017等基准数据集规模有限、噪声水平高以及类别严重不平衡的制约。尽管基于Transformer的模型具备强大的特征提取能力，但在处理此类受限医学数据时容易过拟合，且往往收敛至损失曲面的尖锐极小值。为解决这一问题，我们提出一种利用锐度感知最小化（SAM）增强音频谱图Transformer（AST）的框架。该方法不仅最小化训练损失，更通过优化损失曲面的几何形态，引导模型朝向更平坦的极小值收敛，从而提升对未见患者的泛化能力。同时，我们采用加权采样策略以有效处理类别不平衡问题。在ICBHI 2017数据集上，本方法取得了68.10%的当前最优性能，超越了现有CNN及混合基线模型。更重要的是，其灵敏度达到68.31%，这对可靠的临床筛查具有重要意义。通过t-SNE降维可视化与注意力图谱的进一步分析证实，该模型能够学习具有判别力的鲁棒特征，而非简单记忆背景噪声。",
    "url": "https://huggingface.co/papers/2512.22564",
    "arxiv_url": "https://arxiv.org/abs/2512.22564"
  },
  {
    "title": "BEDA: Belief Estimation as Probabilistic Constraints for Performing Strategic Dialogue Acts",
    "summary": "Strategic dialogue requires agents to execute distinct dialogue acts, for which belief estimation is essential. While prior work often estimates beliefs accurately, it lacks a principled mechanism to use those beliefs during generation. We bridge this gap by first formalizing two core acts Adversarial and Alignment, and by operationalizing them via probabilistic constraints on what an agent may generate. We instantiate this idea in BEDA, a framework that consists of the world set, the belief estimator for belief estimation, and the conditional generator that selects acts and realizes utterances consistent with the inferred beliefs. Across three settings, Conditional Keeper Burglar (CKBG, adversarial), Mutual Friends (MF, cooperative), and CaSiNo (negotiation), BEDA consistently outperforms strong baselines: on CKBG it improves success rate by at least 5.0 points across backbones and by 20.6 points with GPT-4.1-nano; on Mutual Friends it achieves an average improvement of 9.3 points; and on CaSiNo it achieves the optimal deal relative to all baselines. These results indicate that casting belief estimation as constraints provides a simple, general mechanism for reliable strategic dialogue.",
    "translation": "标题：BEDA：信念估计作为执行策略性对话行为的概率约束\n\n摘要：策略性对话要求智能体执行特定的对话行为，信念估计对此至关重要。现有研究虽能准确估计信念，但缺乏在生成过程中运用这些信念的机制化方法。我们通过以下方式填补这一空白：首先形式化两种核心行为——对抗与协调，进而通过智能体生成内容的概率约束将其操作化。我们在BEDA框架中实现了这一构想，该框架包含世界状态集合、用于信念估计的信念估计器，以及根据推断信念选择行为并生成对应话语的条件生成器。在条件型守护者-窃贼（CKBG，对抗性）、共同好友（MF，合作性）和CaSiNo（协商性）三种实验场景中，BEDA均持续优于强基线模型：在CKBG任务中，不同骨干模型上的成功率提升至少5.0个百分点，使用GPT-4.1-nano时提升达20.6个百分点；在共同好友任务中平均提升9.3个百分点；在CaSiNo任务中达成了相较于所有基线模型的最优协议。这些结果表明，将信念估计转化为约束条件为可靠策略性对话提供了简洁通用的机制。",
    "url": "https://huggingface.co/papers/2512.24885",
    "arxiv_url": "https://arxiv.org/abs/2512.24885"
  },
  {
    "title": "Factorized Learning for Temporally Grounded Video-Language Models",
    "summary": "Recent video-language models have shown great potential for video understanding, but still struggle with accurate temporal grounding for event-level perception. We observe that two main factors in video understanding (i.e., temporal grounding and textual response) form a logical hierarchy: accurate temporal evidence grounding lays the foundation for reliable textual response. However, existing works typically handle these two tasks in a coupled manner without a clear logical structure, leading to sub-optimal objectives. We address this from a factorized learning perspective. We first propose D^2VLM, a framework that decouples the learning of these two tasks while also emphasizing their inherent dependency. We adopt a \"grounding then answering with evidence referencing\" paradigm and introduce evidence tokens for evidence grounding, which emphasize event-level visual semantic capture beyond the focus on timestamp representation in existing works. To further facilitate the learning of these two tasks, we introduce a novel factorized preference optimization (FPO) algorithm. Unlike standard preference optimization, FPO explicitly incorporates probabilistic temporal grounding modeling into the optimization objective, enabling preference learning for both temporal grounding and textual response. We also construct a synthetic dataset to address the lack of suitable datasets for factorized preference learning with explicit temporal grounding. Experiments on various tasks demonstrate the clear advantage of our approach. Our source code is available at https://github.com/nusnlp/d2vlm.",
    "translation": "标题：面向时序定位视频语言模型的因子化学习\n\n摘要：现有视频语言模型在视频理解方面展现出巨大潜力，但在事件级感知的精确时序定位方面仍面临挑战。我们发现视频理解中的两个核心要素（即时序定位与文本响应）构成逻辑层次：准确的时序证据定位是可靠文本响应的基础。然而，现有研究通常以耦合方式处理这两项任务，缺乏清晰的逻辑结构，导致优化目标未能达到最优。本文从因子化学习视角对此进行改进：首先提出D²VLM框架，在解耦两项任务学习的同时强调其内在依赖关系。该框架采用“先定位后证据引用作答”范式，引入证据标记实现证据定位，其设计重点超越现有工作对时间戳表示的关注，更强调事件级视觉语义的捕捉。为促进两项任务的协同学习，我们进一步提出新型因子化偏好优化算法。与标准偏好优化不同，该算法将概率化时序定位建模显式纳入优化目标，实现对时序定位与文本响应的联合偏好学习。针对现有数据集缺乏显式时序标注的问题，我们还构建了适用于因子化偏好学习的合成数据集。在多任务实验中的结果表明，本方法具有显著优势。源代码已公开于https://github.com/nusnlp/d2vlm。",
    "url": "https://huggingface.co/papers/2512.24097",
    "arxiv_url": "https://arxiv.org/abs/2512.24097"
  },
  {
    "title": "Valori: A Deterministic Memory Substrate for AI Systems",
    "summary": "Modern AI systems rely on vector embeddings stored and searched using floating-point arithmetic. While effective for approximate similarity search, this design introduces fundamental non-determinism: identical models, inputs, and code can produce different memory states and retrieval results across hardware architectures (e.g., x86 vs. ARM). This prevents replayability and safe deployment, leading to silent data divergence that prevents post-hoc verification and compromises audit trails in regulated sectors. We present Valori, a deterministic AI memory substrate that replaces floating-point memory operations with fixed-point arithmetic (Q16.16) and models memory as a replayable state machine. Valori guarantees bit-identical memory states, snapshots, and search results across platforms. We demonstrate that non-determinism arises before indexing or retrieval and show how Valori enforces determinism at the memory boundary. Our results suggest that deterministic memory is a necessary primitive for trustworthy AI systems. The reference implementation is open-source and available at https://github.com/varshith-Git/Valori-Kernel (archived at https://zenodo.org/records/18022660).",
    "translation": "标题：Valori：面向人工智能系统的确定性内存基板\n\n摘要：现代人工智能系统依赖于通过浮点运算存储和检索的向量嵌入。尽管这种设计在近似相似性搜索中表现有效，但其引入了根本性的非确定性：相同的模型、输入和代码在不同硬件架构（如x86与ARM）上可能产生不同的内存状态与检索结果。这导致系统无法实现状态复现与安全部署，引发难以追溯的数据静默分歧，从而阻碍事后验证并危及受监管领域的审计追溯。本文提出Valori——一种确定性AI内存基板，其采用定点运算（Q16.16）替代浮点内存操作，并将内存建模为可复现的状态机。Valori确保跨平台实现比特级一致的内存状态、快照及搜索结果。我们论证了非确定性在索引与检索操作之前即已产生，并阐明Valori如何在内存边界实施确定性保障。实验结果表明，确定性内存是构建可信人工智能系统的必要基础组件。该系统的参考实现已开源，访问地址：https://github.com/varshith-Git/Valori-Kernel（存档于https://zenodo.org/records/18022660）。",
    "url": "https://huggingface.co/papers/2512.22280",
    "arxiv_url": "https://arxiv.org/abs/2512.22280"
  }
]