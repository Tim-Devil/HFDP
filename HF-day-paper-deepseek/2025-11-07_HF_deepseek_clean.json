[
  {
    "title": "Thinking with Video: Video Generation as a Promising Multimodal\n  Reasoning Paradigm",
    "summary": "\"Thinking with Text\" and \"Thinking with Images\" paradigm significantly\nimprove the reasoning ability of large language models (LLMs) and Vision\nLanguage Models (VLMs). However, these paradigms have inherent limitations. (1)\nImages capture only single moments and fail to represent dynamic processes or\ncontinuous changes, and (2) The separation of text and vision as distinct\nmodalities, hindering unified multimodal understanding and generation. To\novercome these limitations, we introduce \"Thinking with Video\", a new paradigm\nthat leverages video generation models, such as Sora-2, to bridge visual and\ntextual reasoning in a unified temporal framework. To support this exploration,\nwe developed the Video Thinking Benchmark (VideoThinkBench). VideoThinkBench\nencompasses two task categories: (1) vision-centric tasks (e.g., Eyeballing\nPuzzles), and (2) text-centric tasks (e.g., subsets of GSM8K, MMMU). Our\nevaluation establishes Sora-2 as a capable reasoner. On vision-centric tasks,\nSora-2 is generally comparable to state-of-the-art (SOTA) VLMs, and even\nsurpasses VLMs on several tasks, such as Eyeballing Games. On text-centric\ntasks, Sora-2 achieves 92% accuracy on MATH, and 75.53% accuracy on MMMU.\nFurthermore, we systematically analyse the source of these abilities. We also\nfind that self-consistency and in-context learning can improve Sora-2's\nperformance. In summary, our findings demonstrate that the video generation\nmodel is the potential unified multimodal understanding and generation model,\npositions \"thinking with video\" as a unified multimodal reasoning paradigm.",
    "translation": "标题：以视频思考：视频生成作为一种前景广阔的多模态推理范式\n\n摘要：“以文本思考”和“以图像思考”范式显著提升了大型语言模型（LLMs）与视觉语言模型（VLMs）的推理能力。然而这些范式存在固有局限：（1）图像仅能捕捉瞬时状态而无法表征动态过程或连续变化；（2）文本与视觉作为独立模态的割裂阻碍了统一的多模态理解与生成。为突破这些限制，我们提出“以视频思考”新范式，通过视频生成模型（如Sora-2）在统一时序框架中桥接视觉与文本推理。为支持该研究，我们开发了视频思维基准（VideoThinkBench），涵盖两大任务类别：（1）视觉中心任务（如目测谜题）；（2）文本中心任务（如GSM8K、MMMU子集）。评估结果表明Sora-2具备卓越推理能力：在视觉中心任务中整体媲美最先进VLMs，且在目测游戏等任务中表现更优；在文本中心任务中，MATH准确率达92%，MMMU准确率达75.53%。我们系统分析了能力来源，发现自洽性与上下文学习能提升Sora-2性能。本研究证实视频生成模型有望成为统一的多模态理解与生成载体，使“以视频思考”成为统一的多模态推理范式。",
    "url": "https://huggingface.co/papers/2511.04570",
    "arxiv_url": "https://arxiv.org/abs/2511.04570"
  },
  {
    "title": "V-Thinker: Interactive Thinking with Images",
    "summary": "Empowering Large Multimodal Models (LMMs) to deeply integrate image\ninteraction with long-horizon reasoning capabilities remains a long-standing\nchallenge in this field. Recent advances in vision-centric reasoning explore a\npromising \"Thinking with Images\" paradigm for LMMs, marking a shift from\nimage-assisted reasoning to image-interactive thinking. While this milestone\nenables models to focus on fine-grained image regions, progress remains\nconstrained by limited visual tool spaces and task-specific workflow designs.\nTo bridge this gap, we present V-Thinker, a general-purpose multimodal\nreasoning assistant that enables interactive, vision-centric thinking through\nend-to-end reinforcement learning. V-Thinker comprises two key components: (1)\na Data Evolution Flywheel that automatically synthesizes, evolves, and verifies\ninteractive reasoning datasets across three dimensions-diversity, quality, and\ndifficulty; and (2) a Visual Progressive Training Curriculum that first aligns\nperception via point-level supervision, then integrates interactive reasoning\nthrough a two-stage reinforcement learning framework. Furthermore, we introduce\nVTBench, an expert-verified benchmark targeting vision-centric interactive\nreasoning tasks. Extensive experiments demonstrate that V-Thinker consistently\noutperforms strong LMM-based baselines in both general and interactive\nreasoning scenarios, providing valuable insights for advancing\nimage-interactive reasoning applications.",
    "translation": "标题：V-Thinker：基于图像交互的思维框架\n\n摘要：如何使大型多模态模型深度整合图像交互与长周期推理能力，始终是该领域长期存在的挑战。近期以视觉为中心的推理研究探索了\"基于图像的思维\"这一新兴范式，标志着从图像辅助推理到图像交互思维的范式转变。尽管这一里程碑进展使模型能够聚焦细粒度图像区域，但现有研究仍受限于狭窄的视觉工具空间和特定任务的工作流设计。为突破这些限制，我们提出V-Thinker——一个通过端到端强化学习实现交互式视觉思维的通用的多模态推理助手。该框架包含两大核心组件：（1）数据进化飞轮机制，通过多样性、质量与难度三维度自动合成、演进并验证交互式推理数据集；（2）视觉渐进训练课程，首先通过点级监督实现感知对齐，继而通过两阶段强化学习框架融合交互推理。此外，我们构建了VTBench专家验证基准，专门针对视觉中心交互推理任务进行评估。大量实验表明，V-Thinker在通用推理和交互推理场景中均持续优于基于大型多模态模型的基线方法，为推进图像交互推理应用提供了重要洞见。",
    "url": "https://huggingface.co/papers/2511.04460",
    "arxiv_url": "https://arxiv.org/abs/2511.04460"
  },
  {
    "title": "Scaling Agent Learning via Experience Synthesis",
    "summary": "While reinforcement learning (RL) can empower large language model (LLM)\nagents by enabling self-improvement through interaction, its practical adoption\nremains challenging due to costly rollouts, limited task diversity, unreliable\nreward signals, and infrastructure complexity, all of which obstruct the\ncollection of scalable experience data. To address these challenges, we\nintroduce DreamGym, the first unified framework designed to synthesize diverse\nexperiences with scalability in mind to enable effective online RL training for\nautonomous agents. Rather than relying on expensive real-environment rollouts,\nDreamGym distills environment dynamics into a reasoning-based experience model\nthat derives consistent state transitions and feedback signals through\nstep-by-step reasoning, enabling scalable agent rollout collection for RL. To\nimprove the stability and quality of transitions, DreamGym leverages an\nexperience replay buffer initialized with offline real-world data and\ncontinuously enriched with fresh interactions to actively support agent\ntraining. To improve knowledge acquisition, DreamGym adaptively generates new\ntasks that challenge the current agent policy, enabling more effective online\ncurriculum learning. Experiments across diverse environments and agent\nbackbones demonstrate that DreamGym substantially improves RL training, both in\nfully synthetic settings and in sim-to-real transfer scenarios. On non-RL-ready\ntasks like WebArena, DreamGym outperforms all baselines by over 30%. And in\nRL-ready but costly settings, it matches GRPO and PPO performance using only\nsynthetic interactions. When transferring a policy trained purely on synthetic\nexperiences to real-environment RL, DreamGym yields significant additional\nperformance gains while requiring far fewer real-world interactions, providing\na scalable warm-start strategy for general-purpose RL.",
    "translation": "标题：基于经验合成的智能体规模化学习\n\n摘要：尽管强化学习能够通过交互式自我优化赋能大语言模型智能体，但其实际应用仍面临诸多挑战：昂贵的环境交互成本、有限的任务多样性、不可靠的奖励信号以及复杂的基础设施要求，这些因素共同阻碍了可扩展经验数据的收集。为应对这些挑战，我们提出DreamGym——首个以可扩展性为核心目标、通过合成多样化经验来实现自主智能体高效在线强化学习的统一框架。该框架摒弃成本高昂的真实环境交互，将环境动态蒸馏为基于推理的经验模型，通过逐步推导生成连贯的状态转移与反馈信号，从而为强化学习提供可扩展的智能体交互数据收集方案。为提升状态转移的稳定性和质量，DreamGym采用由离线真实数据初始化、并通过持续注入新型交互不断扩展的经验回放缓冲区，以动态支撑智能体训练。在知识获取方面，DreamGym自适应生成挑战当前策略的新任务，实现更高效的在线课程学习。跨环境与智能体架构的实验表明，DreamGym在完全合成环境与仿真到现实迁移场景中均能显著提升强化学习效果。在WebArena等非强化学习适配任务中，DreamGym以超过30%的优势超越所有基线方法；在强化学习适配但成本高昂的场景中，仅通过合成交互即可达到GRPO和PPO的性能水平。当将纯合成经验训练的策略迁移至真实环境时，DreamGym在显著减少现实交互次数的同时带来额外性能提升，为通用强化学习提供了可扩展的预热启动策略。",
    "url": "https://huggingface.co/papers/2511.03773",
    "arxiv_url": "https://arxiv.org/abs/2511.03773"
  },
  {
    "title": "Cambrian-S: Towards Spatial Supersensing in Video",
    "summary": "We argue that progress in true multimodal intelligence calls for a shift from\nreactive, task-driven systems and brute-force long context towards a broader\nparadigm of supersensing. We frame spatial supersensing as four stages beyond\nlinguistic-only understanding: semantic perception (naming what is seen),\nstreaming event cognition (maintaining memory across continuous experiences),\nimplicit 3D spatial cognition (inferring the world behind pixels), and\npredictive world modeling (creating internal models that filter and organize\ninformation). Current benchmarks largely test only the early stages, offering\nnarrow coverage of spatial cognition and rarely challenging models in ways that\nrequire true world modeling. To drive progress in spatial supersensing, we\npresent VSI-SUPER, a two-part benchmark: VSR (long-horizon visual spatial\nrecall) and VSC (continual visual spatial counting). These tasks require\narbitrarily long video inputs yet are resistant to brute-force context\nexpansion. We then test data scaling limits by curating VSI-590K and training\nCambrian-S, achieving +30% absolute improvement on VSI-Bench without\nsacrificing general capabilities. Yet performance on VSI-SUPER remains limited,\nindicating that scale alone is insufficient for spatial supersensing. We\npropose predictive sensing as a path forward, presenting a proof-of-concept in\nwhich a self-supervised next-latent-frame predictor leverages surprise\n(prediction error) to drive memory and event segmentation. On VSI-SUPER, this\napproach substantially outperforms leading proprietary baselines, showing that\nspatial supersensing requires models that not only see but also anticipate,\nselect, and organize experience.",
    "translation": "标题：寒武纪-S：迈向视频空间超感知之路\n\n摘要：我们认为，实现真正多模态智能的进展需要从被动式任务驱动系统和暴力长上下文处理转向更广阔的超感知范式。我们将空间超感知定义为超越纯语言理解的四个阶段：语义感知（识别所见事物的名称）、流式事件认知（在连续体验中维持记忆）、隐式三维空间认知（推断像素背后的世界结构）以及预测性世界建模（建立筛选和组织信息的内部模型）。现有基准大多仅测试初级阶段，对空间认知的覆盖范围有限，且鲜少以需要真实世界建模的方式挑战模型。为推进空间超感知研究，我们提出VSI-SUPER双模块基准：VSR（长程视觉空间回溯）与VSC（持续视觉空间计数）。这些任务需要处理任意长度的视频输入，且能有效抵御暴力上下文扩展。我们通过构建VSI-590K数据集并训练寒武纪-S模型测试数据扩展极限，在VSI基准上实现30%的绝对性能提升且未牺牲通用能力。然而模型在VSI-SUPER上的表现仍存在局限，表明仅靠规模扩展不足以实现空间超感知。我们提出预测性感知作为发展路径，并通过概念验证展示自监督的潜在帧预测器如何利用预测误差驱动记忆与事件分割。在VSI-SUPER基准上，该方法显著超越主流闭源基线模型，证明空间超感知需要模型不仅能够观察，更要具备对经验的预测、筛选和组织能力。",
    "url": "https://huggingface.co/papers/2511.04670",
    "arxiv_url": "https://arxiv.org/abs/2511.04670"
  },
  {
    "title": "GUI-360: A Comprehensive Dataset and Benchmark for Computer-Using Agents",
    "summary": "We introduce GUI-360^circ, a large-scale, comprehensive dataset and\nbenchmark suite designed to advance computer-using agents (CUAs). CUAs present\nunique challenges and is constrained by three persistent gaps: a scarcity of\nreal-world CUA tasks, the lack of automated collection-and-annotation pipelines\nfor multi-modal trajectories, and the absence of a unified benchmark that\njointly evaluates GUI grounding, screen parsing, and action prediction.\n  GUI-360^circ addresses these gaps with an LLM-augmented, largely automated\npipeline for query sourcing, environment-template construction, task\ninstantiation, batched execution, and LLM-driven quality filtering. The\nreleased corpus contains over 1.2M executed action steps across thousands of\ntrajectories in popular Windows office applications, and includes\nfull-resolution screenshots, accessibility metadata when available,\ninstantiated goals, intermediate reasoning traces, and both successful and\nfailed action trajectories. The dataset supports three canonical tasks, GUI\ngrounding, screen parsing, and action prediction, and a hybrid GUI+API action\nspace that reflects modern agent designs. Benchmarking state-of-the-art\nvision--language models on GUI-360^circ reveals substantial out-of-the-box\nshortcomings in grounding and action prediction; supervised fine-tuning and\nreinforcement learning yield significant gains but do not close the gap to\nhuman-level reliability. We release GUI-360^circ and accompanying code to\nfacilitate reproducible research and accelerate progress on robust desktop\nCUAs.\n  The full dataset has been made public on\nhttps://huggingface.co/datasets/vyokky/GUI-360.",
    "translation": "标题：GUI-360：面向计算机使用智能体的综合数据集与基准测试框架\n\n摘要：本文提出GUI-360^circ——一个大规模综合性数据集与基准测试套件，旨在推动计算机使用智能体（CUAs）的发展。当前CUAs面临三大持续性挑战：真实场景任务稀缺、多模态轨迹自动采集与标注流程缺失、以及缺乏统一评估GUI定位、屏幕解析与行为预测的基准框架。GUI-360^circ通过基于大语言模型的增强型自动化流程（包含查询溯源、环境模板构建、任务实例化、批量执行与质量过滤）解决了这些难题。该开放语料库涵盖主流Windows办公应用的数千条轨迹，包含超过120万次执行动作步骤，集成全分辨率屏幕截图、可访问性元数据（若可用）、实例化目标、中间推理轨迹以及成功/失败行为轨迹。本数据集支持GUI定位、屏幕解析和行为预测三大核心任务，并提供反映现代智能体设计的混合GUI+API行为空间。在GUI-360^circ上对前沿视觉-语言模型的基准测试显示，现有模型在定位与行为预测方面存在显著不足；监督微调与强化学习虽能提升性能，但尚未达到人类水平可靠性。我们公开GUI-360^circ数据集及配套代码，以促进可复现研究并加速稳健桌面CUAs的发展。完整数据集已发布于：https://huggingface.co/datasets/vyokky/GUI-360。",
    "url": "https://huggingface.co/papers/2511.04307",
    "arxiv_url": "https://arxiv.org/abs/2511.04307"
  },
  {
    "title": "NVIDIA Nemotron Nano V2 VL",
    "summary": "We introduce Nemotron Nano V2 VL, the latest model of the Nemotron\nvision-language series designed for strong real-world document understanding,\nlong video comprehension, and reasoning tasks. Nemotron Nano V2 VL delivers\nsignificant improvements over our previous model,\nLlama-3.1-Nemotron-Nano-VL-8B, across all vision and text domains through major\nenhancements in model architecture, datasets, and training recipes. Nemotron\nNano V2 VL builds on Nemotron Nano V2, a hybrid Mamba-Transformer LLM, and\ninnovative token reduction techniques to achieve higher inference throughput in\nlong document and video scenarios. We are releasing model checkpoints in BF16,\nFP8, and FP4 formats and sharing large parts of our datasets, recipes and\ntraining code.",
    "translation": "标题：英伟达Nemotron Nano V2 VL模型\n\n摘要：我们推出Nemotron视觉语言系列最新模型Nemotron Nano V2 VL，该模型专为强化现实场景文档理解、长视频解析与推理任务而设计。通过对模型架构、数据集和训练方案的重大改进，Nemotron Nano V2 VL在视觉与文本全领域性能均显著超越前代模型Llama-3.1-Nemotron-Nano-VL-8B。本模型基于混合Mamba-Transformer架构的Nemotron Nano V2大型语言模型，结合创新性令牌精简技术，在长文档和长视频场景中实现了更高的推理吞吐量。我们将发布BF16、FP8和FP4三种格式的模型检查点，并公开大部分数据集、训练方案及训练代码。",
    "url": "https://huggingface.co/papers/2511.03929",
    "arxiv_url": "https://arxiv.org/abs/2511.03929"
  },
  {
    "title": "Contamination Detection for VLMs using Multi-Modal Semantic Perturbation",
    "summary": "Recent advances in Vision-Language Models (VLMs) have achieved\nstate-of-the-art performance on numerous benchmark tasks. However, the use of\ninternet-scale, often proprietary, pretraining corpora raises a critical\nconcern for both practitioners and users: inflated performance due to test-set\nleakage. While prior works have proposed mitigation strategies such as\ndecontamination of pretraining data and benchmark redesign for LLMs, the\ncomplementary direction of developing detection methods for contaminated VLMs\nremains underexplored. To address this gap, we deliberately contaminate\nopen-source VLMs on popular benchmarks and show that existing detection\napproaches either fail outright or exhibit inconsistent behavior. We then\npropose a novel simple yet effective detection method based on multi-modal\nsemantic perturbation, demonstrating that contaminated models fail to\ngeneralize under controlled perturbations. Finally, we validate our approach\nacross multiple realistic contamination strategies, confirming its robustness\nand effectiveness. The code and perturbed dataset will be released publicly.",
    "translation": "标题：基于多模态语义扰动的视觉语言模型污染检测\n\n摘要：视觉语言模型（VLM）的最新进展已在众多基准任务中实现最先进的性能。然而，使用互联网规模且通常具有专有属性的预训练语料库引发了从业者和用户共同关注的关键问题：由于测试集泄露导致的性能虚高。尽管已有研究针对大型语言模型提出了预训练数据净化与基准重构等缓解策略，但针对受污染VLM开发检测方法的互补方向仍待深入探索。为填补这一空白，我们通过故意污染开源VLM在主流基准测试上的数据，发现现有检测方法要么完全失效，要么表现出不一致的行为。进而提出一种基于多模态语义扰动的新型检测方法，该方法兼具简洁性与高效性，实验证明受污染模型在受控扰动下无法保持泛化能力。最后，我们在多种现实污染场景中验证该方法的有效性，确认其具有强鲁棒性和实用价值。相关代码与扰动数据集将公开发布。",
    "url": "https://huggingface.co/papers/2511.03774",
    "arxiv_url": "https://arxiv.org/abs/2511.03774"
  },
  {
    "title": "The Strong Lottery Ticket Hypothesis for Multi-Head Attention Mechanisms",
    "summary": "The strong lottery ticket hypothesis (SLTH) conjectures that high-performing\nsubnetworks, called strong lottery tickets (SLTs), are hidden in randomly\ninitialized neural networks. Although recent theoretical studies have\nestablished the SLTH across various neural architectures, the SLTH for\ntransformer architectures still lacks theoretical understanding. In particular,\nthe current theory of the SLTH does not yet account for the multi-head\nattention (MHA) mechanism, a core component of transformers. To address this\ngap, we introduce a theoretical analysis of the existence of SLTs within MHAs.\nWe prove that, if a randomly initialized MHA of H heads and input dimension\nd has the hidden dimension O(dlog(Hd^{3/2})) for the key and value, it\ncontains an SLT that approximates an arbitrary MHA with the same input\ndimension with high probability. Furthermore, by leveraging this theory for\nMHAs, we extend the SLTH to transformers without normalization layers. We\nempirically validate our theoretical findings, demonstrating that the\napproximation error between the SLT within a source model (MHA and transformer)\nand an approximate target counterpart decreases exponentially by increasing the\nhidden dimension of the source model.",
    "translation": "标题：多头注意力机制的强彩票假设\n\n摘要：强彩票假设认为，在随机初始化的神经网络中隐藏着高性能子网络（称为强彩票）。尽管近期理论研究已在多种神经架构中证实了该假设，但针对Transformer架构的强彩票假设仍缺乏理论支撑。特别是现有理论尚未涵盖Transformer的核心组件——多头注意力机制。为填补这一空白，我们提出了对多头注意力机制中存在强彩票的理论分析。研究证明：若具有H个头、输入维度为d的随机初始化多头注意力机制中键与值的隐藏维度为O(dlog(Hd^{3/2}))，则该机制极大概率包含可逼近任意具有相同输入维度多头注意力机制的强彩票。进一步地，通过将该理论拓展至无归一化层的Transformer架构，我们实证验证了理论发现：当增加源模型（多头注意力机制与Transformer）的隐藏维度时，其内部强彩票与近似目标模型之间的逼近误差呈指数级下降。",
    "url": "https://huggingface.co/papers/2511.04217",
    "arxiv_url": "https://arxiv.org/abs/2511.04217"
  },
  {
    "title": "Benchmark Designers Should \"Train on the Test Set\" to Expose Exploitable\n  Non-Visual Shortcuts",
    "summary": "Robust benchmarks are crucial for evaluating Multimodal Large Language Models\n(MLLMs). Yet we find that models can ace many multimodal benchmarks without\nstrong visual understanding, instead exploiting biases, linguistic priors, and\nsuperficial patterns. This is especially problematic for vision-centric\nbenchmarks that are meant to require visual inputs. We adopt a diagnostic\nprinciple for benchmark design: if a benchmark can be gamed, it will be.\nDesigners should therefore try to ``game'' their own benchmarks first, using\ndiagnostic and debiasing procedures to systematically identify and mitigate\nnon-visual biases. Effective diagnosis requires directly ``training on the test\nset'' -- probing the released test set for its intrinsic, exploitable patterns.\n  We operationalize this standard with two components. First, we diagnose\nbenchmark susceptibility using a ``Test-set Stress-Test'' (TsT) methodology.\nOur primary diagnostic tool involves fine-tuning a powerful Large Language\nModel via k-fold cross-validation on exclusively the non-visual, textual\ninputs of the test set to reveal shortcut performance and assign each sample a\nbias score s(x). We complement this with a lightweight Random Forest-based\ndiagnostic operating on hand-crafted features for fast, interpretable auditing.\nSecond, we debias benchmarks by filtering high-bias samples using an\n``Iterative Bias Pruning'' (IBP) procedure. Applying this framework to four\nbenchmarks -- VSI-Bench, CV-Bench, MMMU, and VideoMME -- we uncover pervasive\nnon-visual biases. As a case study, we apply our full framework to create\nVSI-Bench-Debiased, demonstrating reduced non-visual solvability and a wider\nvision-blind performance gap than the original.",
    "translation": "标题：基准设计者应“在测试集上训练”以揭示可被利用的非视觉捷径\n\n摘要：稳健的基准对于评估多模态大语言模型至关重要。然而我们发现，许多模型无需具备强大的视觉理解能力即可在多项多模态基准测试中取得优异成绩，这实际上是通过利用数据偏差、语言先验和表面模式实现的。对于本需依赖视觉输入的以视觉为中心的基准而言，这一问题尤为严重。我们提出一项基准设计的诊断原则：若基准存在被钻空子的可能，则必将被钻空子。因此设计者应率先尝试“破解”自身构建的基准，通过诊断与去偏差流程系统性地识别并消除非视觉偏差。有效的诊断需要直接“在测试集上训练”——通过探测已发布测试集固有的可被利用模式来实现。我们将这一标准具体化为两个组成部分：首先采用“测试集压力测试”方法诊断基准的脆弱性，主要诊断工具涉及对强大语言模型进行k折交叉验证的微调（仅使用测试集的非视觉文本输入），以揭示捷径性能并为每个样本分配偏差分数s(x)；同时辅以基于手工特征的轻量级随机森林诊断法，实现快速可解释的审计。其次通过“迭代偏差剪枝”程序过滤高偏差样本以实现基准去偏差。将该框架应用于VSI-Bench、CV-Bench、MMMU和VideoMME四个基准后，我们发现了普遍存在的非视觉偏差。作为案例研究，我们应用完整框架创建了VSI-Bench-Debiased，结果显示其非视觉可解性显著降低，且视觉盲区性能差距较原始基准更为明显。",
    "url": "https://huggingface.co/papers/2511.04655",
    "arxiv_url": "https://arxiv.org/abs/2511.04655"
  },
  {
    "title": "Learning Vision-Driven Reactive Soccer Skills for Humanoid Robots",
    "summary": "Humanoid soccer poses a representative challenge for embodied intelligence,\nrequiring robots to operate within a tightly coupled perception-action loop.\nHowever, existing systems typically rely on decoupled modules, resulting in\ndelayed responses and incoherent behaviors in dynamic environments, while\nreal-world perceptual limitations further exacerbate these issues. In this\nwork, we present a unified reinforcement learning-based controller that enables\nhumanoid robots to acquire reactive soccer skills through the direct\nintegration of visual perception and motion control. Our approach extends\nAdversarial Motion Priors to perceptual settings in real-world dynamic\nenvironments, bridging motion imitation and visually grounded dynamic control.\nWe introduce an encoder-decoder architecture combined with a virtual perception\nsystem that models real-world visual characteristics, allowing the policy to\nrecover privileged states from imperfect observations and establish active\ncoordination between perception and action. The resulting controller\ndemonstrates strong reactivity, consistently executing coherent and robust\nsoccer behaviors across various scenarios, including real RoboCup matches.",
    "translation": "标题：面向仿人机器人的视觉驱动反应式足球技能学习\n\n摘要：仿人足球运动对具身智能提出了代表性挑战，要求机器人在紧密耦合的感知-行动循环中运作。然而，现有系统通常依赖解耦模块，导致动态环境中出现响应延迟与行为失调，而现实世界的感知局限进一步加剧了这些问题。本研究提出一种基于强化学习的统一控制器，通过视觉感知与运动控制的直接集成，使仿人机器人获得反应式足球技能。我们的方法将对抗运动先验扩展至现实动态环境中的感知场景，搭建起运动模仿与视觉基础动态控制之间的桥梁。我们引入结合虚拟感知系统的编码器-解码器架构，该系统可建模真实世界的视觉特性，使策略能够从不完美观测中恢复特权状态，并建立感知与行动的主动协同。最终实现的控制器展现出强大的反应能力，在包括真实RoboCup比赛在内的多种场景中，持续执行协调一致且鲁棒的足球行为。",
    "url": "https://huggingface.co/papers/2511.03996",
    "arxiv_url": "https://arxiv.org/abs/2511.03996"
  },
  {
    "title": "How to Evaluate Speech Translation with Source-Aware Neural MT Metrics",
    "summary": "Automatic evaluation of speech-to-text translation (ST) systems is typically\nperformed by comparing translation hypotheses with one or more reference\ntranslations. While effective to some extent, this approach inherits the\nlimitation of reference-based evaluation that ignores valuable information from\nthe source input. In machine translation (MT), recent progress has shown that\nneural metrics incorporating the source text achieve stronger correlation with\nhuman judgments. Extending this idea to ST, however, is not trivial because the\nsource is audio rather than text, and reliable transcripts or alignments\nbetween source and references are often unavailable. In this work, we conduct\nthe first systematic study of source-aware metrics for ST, with a particular\nfocus on real-world operating conditions where source transcripts are not\navailable. We explore two complementary strategies for generating textual\nproxies of the input audio, automatic speech recognition (ASR) transcripts, and\nback-translations of the reference translation, and introduce a novel two-step\ncross-lingual re-segmentation algorithm to address the alignment mismatch\nbetween synthetic sources and reference translations. Our experiments, carried\nout on two ST benchmarks covering 79 language pairs and six ST systems with\ndiverse architectures and performance levels, show that ASR transcripts\nconstitute a more reliable synthetic source than back-translations when word\nerror rate is below 20%, while back-translations always represent a\ncomputationally cheaper but still effective alternative. Furthermore, our\ncross-lingual re-segmentation algorithm enables robust use of source-aware MT\nmetrics in ST evaluation, paving the way toward more accurate and principled\nevaluation methodologies for speech translation.",
    "translation": "标题：如何利用源语言感知神经机器翻译指标评估语音翻译系统\n\n摘要：语音到文本翻译系统的自动评估通常通过比较翻译假设与一个或多个参考译文来实现。尽管这种方法具有一定效果，但它继承了基于参考评估的固有局限——忽略了源语言输入中的有价值信息。在机器翻译领域，最新研究表明融入源文本的神经评估指标能获得与人工评判更强的一致性。然而将这一思路延伸至语音翻译领域面临特殊挑战：源输入是音频而非文本，且通常无法获得可靠的源语言转录文本或源语与参考译文的对齐信息。本研究首次系统探讨了语音翻译的源语言感知评估方法，特别关注源语言文本不可用的实际应用场景。我们提出了两种互补的文本代理生成策略：自动语音识别转录文本和参考译文的反向翻译，并引入一种新颖的两步跨语言重对齐算法来解决合成源文本与参考译文之间的对齐失配问题。我们在涵盖79个语言对的两个语音翻译基准测试集上展开实验，评估了六种不同架构与性能水平的语音翻译系统。实验结果表明：当词错误率低于20%时，自动语音识别转录文本比反向翻译更能提供可靠的合成源语言信息，而反向翻译始终是计算成本更低且仍具效力的替代方案。此外，我们提出的跨语言重对齐算法能够确保源语言感知机器翻译指标在语音翻译评估中的稳健应用，为建立更精准、更系统的语音翻译评估方法开辟了新途径。",
    "url": "https://huggingface.co/papers/2511.03295",
    "arxiv_url": "https://arxiv.org/abs/2511.03295"
  },
  {
    "title": "SIMS-V: Simulated Instruction-Tuning for Spatial Video Understanding",
    "summary": "Despite impressive high-level video comprehension, multimodal language models\nstruggle with spatial reasoning across time and space. While current spatial\ntraining approaches rely on real-world video data, obtaining diverse footage\nwith precise spatial annotations remains a bottleneck. To alleviate this\nbottleneck, we present SIMS-V -- a systematic data-generation framework that\nleverages the privileged information of 3D simulators to create spatially-rich\nvideo training data for multimodal language models. Using this framework, we\ninvestigate which properties of simulated data drive effective real-world\ntransfer through systematic ablations of question types, mixes, and scales. We\nidentify a minimal set of three question categories (metric measurement,\nperspective-dependent reasoning, and temporal tracking) that prove most\neffective for developing transferable spatial intelligence, outperforming\ncomprehensive coverage despite using fewer question types. These insights\nenable highly efficient training: our 7B-parameter video LLM fine-tuned on just\n25K simulated examples outperforms the larger 72B baseline and achieves\ncompetitive performance with proprietary models on rigorous real-world spatial\nreasoning benchmarks. Our approach demonstrates robust generalization,\nmaintaining performance on general video understanding while showing\nsubstantial improvements on embodied and real-world spatial tasks.",
    "translation": "标题：SIMS-V：面向空间视频理解的模拟指令调优框架\n\n摘要：尽管多模态语言模型在高层视频理解方面表现卓越，但其在跨时空空间推理方面仍存在困难。当前空间训练方法主要依赖现实世界视频数据，然而获取具有精确空间标注的多样化视频素材仍是瓶颈问题。为突破此限制，我们提出SIMS-V——一个系统化的数据生成框架，通过利用三维模拟器的特权信息，为多模态语言模型创建富含空间信息的视频训练数据。基于该框架，我们通过系统化消融实验探究问题类型、混合策略与数据规模等模拟数据特性对现实世界迁移效果的影响。研究发现，仅需三类核心问题（度量测算、视角依赖推理与时序追踪）即可最有效地发展可迁移的空间智能，其效果优于全面覆盖式的提问策略。这些发现实现了高效训练：我们基于2.5万模拟样本微调的70亿参数视频大语言模型，不仅超越720亿参数基线模型，更在严谨的现实空间推理基准测试中与专用模型性能相当。该方法展现出强大泛化能力，在保持通用视频理解性能的同时，在具身智能任务和现实空间任务上实现显著提升。",
    "url": "https://huggingface.co/papers/2511.04668",
    "arxiv_url": "https://arxiv.org/abs/2511.04668"
  },
  {
    "title": "RDMA Point-to-Point Communication for LLM Systems",
    "summary": "Emerging Large Language Model (LLM) system patterns, such as disaggregated\ninference, Mixture-of-Experts (MoE) routing, and asynchronous reinforcement\nfine-tuning, require flexible point-to-point communication beyond simple\ncollectives. Existing implementations are locked to specific Network Interface\nControllers (NICs), hindering integration into inference engines and\nportability across hardware providers. We present TransferEngine, which bridges\nthe functionality of common NICs to expose a uniform interface. TransferEngine\nexposes one-sided WriteImm operations with a ImmCounter primitive for\ncompletion notification, without ordering assumptions of network transport,\ntransparently managing multiple NICs per GPU. We demonstrate peak throughput of\n400 Gbps on both NVIDIA ConnectX-7 and AWS Elastic Fabric Adapter (EFA). We\nshowcase TransferEngine through three production systems: (1) KvCache transfer\nfor disaggregated inference with dynamic scaling, (2) RL weight updates\nachieving 1.3 seconds for trillion-parameter models, and (3) MoE\ndispatch/combine implementation exceeding DeepEP decode latency on ConnectX-7,\nwith the first viable latencies on EFA. We demonstrate that our portable\npoint-to-point communication complements collectives while avoiding lock-in.",
    "translation": "标题：面向大语言模型系统的RDMA点对点通信技术\n\n摘要：新兴的大语言模型系统范式，如分离式推理、专家混合路由及异步强化微调等，需要超越传统集合通信的灵活点对点通信支持。现有实现方案受限于特定网络接口控制器，难以融入推理引擎且缺乏跨硬件供应商的移植性。本文提出TransferEngine系统，通过桥接通用网卡功能提供统一接口。该系统采用ImmCounter原语实现单向WriteImm操作完成通知，无需依赖网络传输顺序假设，可透明管理每块GPU对应的多个网卡。实验表明在NVIDIA ConnectX-7与AWS弹性光纤适配器上均实现400 Gbps峰值吞吐量。通过三个生产系统验证其效能：(1)支持动态扩展的分离式推理KvCache传输；(2)万亿参数模型的强化学习权重更新仅需1.3秒；(3)在ConnectX-7上实现超越DeepEP解码延迟的专家混合调度/聚合方案，并在EFA上获得首批可行延迟数据。研究表明，该可移植点对点通信方案可与集合通信形成互补，同时规避供应商锁定风险。",
    "url": "https://huggingface.co/papers/2510.27656",
    "arxiv_url": "https://arxiv.org/abs/2510.27656"
  },
  {
    "title": "SAIL-RL: Guiding MLLMs in When and How to Think via Dual-Reward RL\n  Tuning",
    "summary": "We introduce SAIL-RL, a reinforcement learning (RL) post-training framework\nthat enhances the reasoning capabilities of multimodal large language models\n(MLLMs) by teaching them when and how to think. Existing approaches are limited\nby outcome-only supervision, which rewards correct answers without ensuring\nsound reasoning, and by uniform thinking strategies, which often lead to\noverthinking on simple tasks and underthinking on complex ones. SAIL-RL\naddresses these challenges with a dual reward system: the Thinking Reward,\nwhich evaluates reasoning quality through factual grounding, logical coherence,\nand answer consistency, and the Judging Reward, which adaptively determines\nwhether deep reasoning or direct answering is appropriate. Experiments on the\nstate-of-the-art SAIL-VL2 show that SAIL-RL improves reasoning and multimodal\nunderstanding benchmarks at both 4B and 8B scales, achieving competitive\nperformance against commercial closed-source models such as GPT-4o, and\nsubstantially reduces hallucinations, establishing it as a principled framework\nfor building more reliable and adaptive MLLMs. The code will be available at\nhttps://github.com/BytedanceDouyinContent/SAIL-RL.",
    "translation": "标题：SAIL-RL：基于双奖励强化学习的多模态大语言模型思考时机与方式引导框架\n\n摘要：本文提出SAIL-RL强化学习后训练框架，通过指导多模态大语言模型掌握思考时机与思考方式，显著增强其推理能力。现有方法存在双重局限：仅依赖结果监督的机制虽奖励正确答案却无法确保推理过程的合理性；采用统一思考策略常导致简单任务过度思考而复杂任务思考不足。SAIL-RL通过双奖励机制突破这些限制：思考奖励从事实依据、逻辑连贯性及答案一致性三维度评估推理质量，判断奖励则自适应决策应进行深度推理或直接作答。在最新SAIL-VL2模型上的实验表明，该框架在4B与8B参数规模下均能提升推理与多模态理解基准性能，相较于GPT-4o等商业闭源模型展现出竞争优势，并显著降低幻觉现象，为构建更可靠、自适应的多模态大语言模型建立了理论框架。代码已开源：https://github.com/BytedanceDouyinContent/SAIL-RL。",
    "url": "https://huggingface.co/papers/2511.02280",
    "arxiv_url": "https://arxiv.org/abs/2511.02280"
  },
  {
    "title": "EVTAR: End-to-End Try on with Additional Unpaired Visual Reference",
    "summary": "We propose EVTAR, an End-to-End Virtual Try-on model with Additional\nReference, that directly fits the target garment onto the person image while\nincorporating reference images to enhance try-on accuracy. Most existing\nvirtual try-on approaches rely on complex inputs such as agnostic person\nimages, human pose, densepose, or body keypoints, making them labor-intensive\nand impractical for real-world applications. In contrast, EVTAR adopts a\ntwo-stage training strategy, enabling simple inference with only the source\nimage and the target garment inputs. Our model generates try-on results without\nmasks, densepose, or segmentation maps. Moreover, EVTAR leverages additional\nreference images of different individuals wearing the same clothes to preserve\ngarment texture and fine-grained details better. This mechanism is analogous to\nhow humans consider reference models when choosing outfits, thereby simulating\na more realistic and high-quality dressing effect. We enrich the training data\nwith supplementary references and unpaired person images to support these\ncapabilities. We evaluate EVTAR on two widely used benchmarks and diverse\ntasks, and the results consistently validate the effectiveness of our approach.",
    "translation": "标题：EVTAR：基于额外非配对视觉参考的端到端虚拟试衣系统\n\n摘要：本文提出EVTAR模型——一种融合附加参考的端到端虚拟试衣系统，该模型在将目标服装直接拟合至人体图像的同时，通过引入参考图像提升试衣精度。当前主流虚拟试衣方法需依赖复杂输入，如不可知人体图像、人体姿态、密集姿态或身体关键点等，导致实施过程繁琐且难以应用于实际场景。相较之下，EVTAR采用两阶段训练策略，仅需源图像与目标服装即可完成简易推理。本模型无需掩码、密集姿态或分割图即可生成试衣效果，更通过引入其他穿着同款服装的参考者图像，显著提升服装纹理与细粒度细节的保持能力。这种机制模拟了人类挑选服装时参考模特展示的决策过程，从而实现更逼真高质量的着装效果。为支撑这些功能，我们通过补充参考图像与非配对人体图像丰富了训练数据。在两大常用基准数据集及多样化任务上的实验结果表明，本方法持续展现出卓越性能。",
    "url": "https://huggingface.co/papers/2511.00956",
    "arxiv_url": "https://arxiv.org/abs/2511.00956"
  }
]