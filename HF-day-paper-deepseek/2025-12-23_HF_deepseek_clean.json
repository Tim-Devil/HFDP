[
  {
    "title": "DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI",
    "summary": "The rapidly growing demand for high-quality data in Large Language Models (LLMs) has intensified the need for scalable, reliable, and semantically rich data preparation pipelines. However, current practices remain dominated by ad-hoc scripts and loosely specified workflows, which lack principled abstractions, hinder reproducibility, and offer limited support for model-in-the-loop data generation. To address these challenges, we present DataFlow, a unified and extensible LLM-driven data preparation framework. DataFlow is designed with system-level abstractions that enable modular, reusable, and composable data transformations, and provides a PyTorch-style pipeline construction API for building debuggable and optimizable dataflows. The framework consists of nearly 200 reusable operators and six domain-general pipelines spanning text, mathematical reasoning, code, Text-to-SQL, agentic RAG, and large-scale knowledge extraction. To further improve usability, we introduce DataFlow-Agent, which automatically translates natural-language specifications into executable pipelines via operator synthesis, pipeline planning, and iterative verification. Across six representative use cases, DataFlow consistently improves downstream LLM performance. Our math, code, and text pipelines outperform curated human datasets and specialized synthetic baselines, achieving up to +3\\% execution accuracy in Text-to-SQL over SynSQL, +7\\% average improvements on code benchmarks, and 1--3 point gains on MATH, GSM8K, and AIME. Moreover, a unified 10K-sample dataset produced by DataFlow enables base models to surpass counterparts trained on 1M Infinity-Instruct data. These results demonstrate that DataFlow provides a practical and high-performance substrate for reliable, reproducible, and scalable LLM data preparation, and establishes a system-level foundation for future data-centric AI development.",
    "translation": "标题：DataFlow：面向以数据为中心的人工智能时代的LLM驱动统一数据准备与工作流程自动化框架\n\n摘要：大型语言模型（LLM）对高质量数据需求的快速增长，使得对可扩展、可靠且语义丰富的数据准备流程的需求日益迫切。然而，当前实践仍主要依赖临时脚本和松散定义的工作流程，这些方法缺乏原则性抽象、阻碍可复现性，并且对模型在环数据生成的支持有限。为应对这些挑战，我们提出了DataFlow——一个统一且可扩展的LLM驱动数据准备框架。DataFlow采用系统级抽象设计，支持模块化、可复用和可组合的数据转换，并提供类似PyTorch风格的流程构建API，用于构建可调试和可优化的数据流。该框架包含近200个可复用算子及六类跨领域通用流程，涵盖文本、数学推理、代码、Text-to-SQL、智能体RAG和大规模知识提取。为提升易用性，我们进一步引入DataFlow-Agent，它通过算子合成、流程规划和迭代验证，能够自动将自然语言描述转换为可执行流程。在六个典型应用场景中，DataFlow持续提升了下游LLM性能：我们的数学、代码和文本流程在效果上优于人工精标数据集和专用合成基线——在Text-to-SQL任务中较SynSQL提升高达3%的执行准确率，在代码基准测试中平均提升7%，在MATH、GSM8K和AIME基准上获得1-3个百分点的增益。此外，由DataFlow生成的统一万条样本数据集，使基础模型性能超越基于百万条Infinity-Instruct数据训练的对照模型。这些结果表明，DataFlow为可靠、可复现和可扩展的LLM数据准备提供了实用高效的基础架构，并为未来以数据为中心的人工智能发展奠定了系统级基石。",
    "url": "https://huggingface.co/papers/2512.16676",
    "arxiv_url": "https://arxiv.org/abs/2512.16676"
  },
  {
    "title": "The Prism Hypothesis: Harmonizing Semantic and Pixel Representations via Unified Autoencoding",
    "summary": "Deep representations across modalities are inherently intertwined. In this paper, we systematically analyze the spectral characteristics of various semantic and pixel encoders. Interestingly, our study uncovers a highly inspiring and rarely explored correspondence between an encoder's feature spectrum and its functional role: semantic encoders primarily capture low-frequency components that encode abstract meaning, whereas pixel encoders additionally retain high-frequency information that conveys fine-grained detail. This heuristic finding offers a unifying perspective that ties encoder behavior to its underlying spectral structure. We define it as the Prism Hypothesis, where each data modality can be viewed as a projection of the natural world onto a shared feature spectrum, just like the prism. Building on this insight, we propose Unified Autoencoding (UAE), a model that harmonizes semantic structure and pixel details via an innovative frequency-band modulator, enabling their seamless coexistence. Extensive experiments on ImageNet and MS-COCO benchmarks validate that our UAE effectively unifies semantic abstraction and pixel-level fidelity into a single latent space with state-of-the-art performance.",
    "translation": "标题：棱镜假说：通过统一自编码实现语义与像素表征的和谐统一\n\n摘要：跨模态的深度表征本质上是相互交织的。本文系统分析了多种语义编码器与像素编码器的频谱特性。有趣的是，我们的研究揭示了一个极具启发性且鲜被探索的对应关系：编码器的特征频谱与其功能角色之间存在明确关联——语义编码器主要捕获编码抽象含义的低频成分，而像素编码器额外保留传达细粒度细节的高频信息。这一启发式发现为理解编码器行为与其底层频谱结构的关系提供了统一视角。我们将其定义为“棱镜假说”：每种数据模态都可视为现实世界在共享特征频谱上的投影，正如棱镜分光原理所示。基于这一洞见，我们提出统一自编码模型，该模型通过创新的频段调制器协调语义结构与像素细节，实现二者的无缝共存。在ImageNet和MS-COCO基准上的大量实验表明，我们的UAE模型以前沿性能将语义抽象与像素级保真度有效统一于单一潜在空间中。",
    "url": "https://huggingface.co/papers/2512.19693",
    "arxiv_url": "https://arxiv.org/abs/2512.19693"
  },
  {
    "title": "Region-Constraint In-Context Generation for Instructional Video Editing",
    "summary": "The In-context generation paradigm recently has demonstrated strong power in instructional image editing with both data efficiency and synthesis quality. Nevertheless, shaping such in-context learning for instruction-based video editing is not trivial. Without specifying editing regions, the results can suffer from the problem of inaccurate editing regions and the token interference between editing and non-editing areas during denoising. To address these, we present ReCo, a new instructional video editing paradigm that novelly delves into constraint modeling between editing and non-editing regions during in-context generation. Technically, ReCo width-wise concatenates source and target video for joint denoising. To calibrate video diffusion learning, ReCo capitalizes on two regularization terms, i.e., latent and attention regularization, conducting on one-step backward denoised latents and attention maps, respectively. The former increases the latent discrepancy of the editing region between source and target videos while reducing that of non-editing areas, emphasizing the modification on editing area and alleviating outside unexpected content generation. The latter suppresses the attention of tokens in the editing region to the tokens in counterpart of the source video, thereby mitigating their interference during novel object generation in target video. Furthermore, we propose a large-scale, high-quality video editing dataset, i.e., ReCo-Data, comprising 500K instruction-video pairs to benefit model training. Extensive experiments conducted on four major instruction-based video editing tasks demonstrate the superiority of our proposal.",
    "translation": "标题：面向教学视频编辑的区域约束上下文生成方法\n\n摘要：上下文生成范式近期在教学图像编辑领域展现出强大的数据效率与合成质量优势。然而，将此类上下文学习机制应用于基于指令的视频编辑并非易事。若未明确指定编辑区域，生成结果可能面临编辑区域定位不准的问题，同时在去噪过程中编辑区域与非编辑区域的语义单元会产生相互干扰。为解决上述挑战，本文提出ReCo——一种创新的教学视频编辑范式，该范式在上下文生成过程中深入探索编辑区域与非编辑区域间的约束建模机制。技术层面，ReCo采用宽度拼接策略将源视频与目标视频联合进行去噪处理。为校准视频扩散学习过程，ReCo设计了两项正则化约束：潜在空间正则化与注意力正则化，分别作用于单步反向去噪的潜在特征与注意力图谱。前者通过增大源视频与目标视频间编辑区域的潜在特征差异，同时减小非编辑区域的差异，从而强化对编辑区域的修改聚焦并抑制非编辑区域的异常内容生成；后者通过抑制目标视频编辑区域语义单元对源视频对应区域的注意力关联，有效降低目标视频新对象生成过程中的语义干扰。此外，我们构建了大规模高质量视频编辑数据集ReCo-Data，包含50万条指令-视频对以支撑模型训练。在四大主流指令式视频编辑任务上的大量实验验证了本方法的优越性。",
    "url": "https://huggingface.co/papers/2512.17650",
    "arxiv_url": "https://arxiv.org/abs/2512.17650"
  },
  {
    "title": "QuCo-RAG: Quantifying Uncertainty from the Pre-training Corpus for Dynamic Retrieval-Augmented Generation",
    "summary": "Dynamic Retrieval-Augmented Generation adaptively determines when to retrieve during generation to mitigate hallucinations in large language models (LLMs). However, existing methods rely on model-internal signals (e.g., logits, entropy), which are fundamentally unreliable because LLMs are typically ill-calibrated and often exhibit high confidence in erroneous outputs. We propose QuCo-RAG, which shifts from subjective confidence to objective statistics computed from pre-training data. Our method quantifies uncertainty through two stages: (1) before generation, we identify low-frequency entities indicating long-tail knowledge gaps; (2) during generation, we verify entity co-occurrence in the pre-training corpus, where zero co-occurrence often signals hallucination risk. Both stages leverage Infini-gram for millisecond-latency queries over 4 trillion tokens, triggering retrieval when uncertainty is high. Experiments on multi-hop QA benchmarks show QuCo-RAG achieves EM gains of 5--12 points over state-of-the-art baselines with OLMo-2 models, and transfers effectively to models with undisclosed pre-training data (Llama, Qwen, GPT), improving EM by up to 14 points. Domain generalization on biomedical QA further validates the robustness of our paradigm. These results establish corpus-grounded verification as a principled, practically model-agnostic paradigm for dynamic RAG. Our code is publicly available at https://github.com/ZhishanQ/QuCo-RAG.",
    "translation": "标题：QuCo-RAG：基于预训练语料库量化不确定性以实现动态检索增强生成\n\n摘要：动态检索增强生成技术通过自适应地决定生成过程中的检索时机，以缓解大语言模型中的幻觉问题。然而，现有方法依赖于模型内部信号（如对数概率、熵），这些信号本质上并不可靠，因为大语言模型通常校准不佳，且常在错误输出中表现出高置信度。我们提出QuCo-RAG方法，将依赖主观置信度转向基于预训练数据计算的客观统计量。该方法通过两个阶段量化不确定性：（1）在生成前，识别指示长尾知识缺口的低频实体；（2）在生成过程中，验证实体在预训练语料库中的共现情况，零共现往往意味着幻觉风险。两个阶段均利用Infini-gram引擎对超过4万亿词元的语料进行毫秒级延迟查询，当不确定性较高时触发检索。在多跳问答基准测试中，实验表明QuCo-RAG在使用OLMo-2模型时相比最先进基线实现了5-12个点的精确匹配提升，并能有效迁移至预训练数据未公开的模型（如Llama、Qwen、GPT），最高提升14个点。在生物医学问答领域的泛化实验进一步验证了该范式的鲁棒性。这些结果确立了基于语料库验证作为一种原则性、实际模型无关的动态检索增强生成范式。我们的代码已公开于https://github.com/ZhishanQ/QuCo-RAG。",
    "url": "https://huggingface.co/papers/2512.19134",
    "arxiv_url": "https://arxiv.org/abs/2512.19134"
  },
  {
    "title": "Infinite-Homography as Robust Conditioning for Camera-Controlled Video Generation",
    "summary": "Recent progress in video diffusion models has spurred growing interest in camera-controlled novel-view video generation for dynamic scenes, aiming to provide creators with cinematic camera control capabilities in post-production. A key challenge in camera-controlled video generation is ensuring fidelity to the specified camera pose, while maintaining view consistency and reasoning about occluded geometry from limited observations. To address this, existing methods either train trajectory-conditioned video generation model on trajectory-video pair dataset, or estimate depth from the input video to reproject it along a target trajectory and generate the unprojected regions. Nevertheless, existing methods struggle to generate camera-pose-faithful, high-quality videos for two main reasons: (1) reprojection-based approaches are highly susceptible to errors caused by inaccurate depth estimation; and (2) the limited diversity of camera trajectories in existing datasets restricts learned models. To address these limitations, we present InfCam, a depth-free, camera-controlled video-to-video generation framework with high pose fidelity. The framework integrates two key components: (1) infinite homography warping, which encodes 3D camera rotations directly within the 2D latent space of a video diffusion model. Conditioning on this noise-free rotational information, the residual parallax term is predicted through end-to-end training to achieve high camera-pose fidelity; and (2) a data augmentation pipeline that transforms existing synthetic multiview datasets into sequences with diverse trajectories and focal lengths. Experimental results demonstrate that InfCam outperforms baseline methods in camera-pose accuracy and visual fidelity, generalizing well from synthetic to real-world data. Link to our project page:https://emjay73.github.io/InfCam/",
    "translation": "标题：无限单应性作为相机控制视频生成的鲁棒条件约束\n\n摘要：视频扩散模型的最新进展激发了人们对动态场景相机控制新视角视频生成日益增长的兴趣，旨在为创作者提供后期制作中的电影级摄像机控制能力。相机控制视频生成的核心挑战在于确保生成内容对指定相机位姿的保真度，同时保持视角一致性，并基于有限观测推断被遮挡的几何结构。为此，现有方法要么在轨迹-视频配对数据集上训练轨迹条件视频生成模型，要么从输入视频估计深度以沿目标轨迹重投影并生成未投影区域。然而，现有方法难以生成高保真相机位姿的高质量视频，主要原因有二：（1）基于重投影的方法极易受深度估计误差影响；（2）现有数据集中相机轨迹的有限多样性制约了学习模型的泛化能力。为突破这些局限，我们提出InfCam——一种无需深度估计、具有高相机位姿保真度的相机控制视频到视频生成框架。该框架集成两个核心组件：（1）无限单应性变换，将三维相机旋转直接编码至视频扩散模型的二维隐空间。基于此无噪声旋转信息进行条件约束，通过端到端训练预测残差视差项，从而实现高精度的相机位姿保真；（2）数据增强流程，将现有合成多视角数据集转换为具有多样化轨迹和焦距的序列。实验结果表明，InfCam在相机位姿精度与视觉保真度上均优于基线方法，并能从合成数据良好泛化至真实场景数据。项目页面链接：https://emjay73.github.io/InfCam/",
    "url": "https://huggingface.co/papers/2512.17040",
    "arxiv_url": "https://arxiv.org/abs/2512.17040"
  },
  {
    "title": "Can LLMs Estimate Student Struggles? Human-AI Difficulty Alignment with Proficiency Simulation for Item Difficulty Prediction",
    "summary": "Accurate estimation of item (question or task) difficulty is critical for educational assessment but suffers from the cold start problem. While Large Language Models demonstrate superhuman problem-solving capabilities, it remains an open question whether they can perceive the cognitive struggles of human learners. In this work, we present a large-scale empirical analysis of Human-AI Difficulty Alignment for over 20 models across diverse domains such as medical knowledge and mathematical reasoning. Our findings reveal a systematic misalignment where scaling up model size is not reliably helpful; instead of aligning with humans, models converge toward a shared machine consensus. We observe that high performance often impedes accurate difficulty estimation, as models struggle to simulate the capability limitations of students even when being explicitly prompted to adopt specific proficiency levels. Furthermore, we identify a critical lack of introspection, as models fail to predict their own limitations. These results suggest that general problem-solving capability does not imply an understanding of human cognitive struggles, highlighting the challenge of using current models for automated difficulty prediction.",
    "translation": "标题：大语言模型能否评估学生困境？基于能力模拟的人机难度对齐在试题难度预测中的应用\n\n摘要：试题（问题或任务）难度的准确估计对教育评估至关重要，但常面临冷启动问题。尽管大语言模型展现出超越人类的问题解决能力，但其能否感知人类学习者的认知困境仍存疑问。本研究通过大规模实证分析，在医学知识和数学推理等多个领域对20余个模型进行人机难度对齐研究。研究发现存在系统性错位现象：扩大模型规模并不能可靠提升对齐效果；模型并未与人类认知对齐，而是趋同于机器共识。研究观察到，高性能表现往往阻碍准确的难度估计——即使被明确要求模拟特定能力水平，模型仍难以有效模拟学生的能力局限。此外，模型存在显著的内省能力缺失，无法预测自身局限性。这些结果表明，通用问题解决能力并不等同于对人类认知困境的理解，凸显出现有模型在自动化难度预测应用中的挑战。",
    "url": "https://huggingface.co/papers/2512.18880",
    "arxiv_url": "https://arxiv.org/abs/2512.18880"
  },
  {
    "title": "WorldWarp: Propagating 3D Geometry with Asynchronous Video Diffusion",
    "summary": "Generating long-range, geometrically consistent video presents a fundamental dilemma: while consistency demands strict adherence to 3D geometry in pixel space, state-of-the-art generative models operate most effectively in a camera-conditioned latent space. This disconnect causes current methods to struggle with occluded areas and complex camera trajectories. To bridge this gap, we propose WorldWarp, a framework that couples a 3D structural anchor with a 2D generative refiner. To establish geometric grounding, WorldWarp maintains an online 3D geometric cache built via Gaussian Splatting (3DGS). By explicitly warping historical content into novel views, this cache acts as a structural scaffold, ensuring each new frame respects prior geometry. However, static warping inevitably leaves holes and artifacts due to occlusions. We address this using a Spatio-Temporal Diffusion (ST-Diff) model designed for a \"fill-and-revise\" objective. Our key innovation is a spatio-temporal varying noise schedule: blank regions receive full noise to trigger generation, while warped regions receive partial noise to enable refinement. By dynamically updating the 3D cache at every step, WorldWarp maintains consistency across video chunks. Consequently, it achieves state-of-the-art fidelity by ensuring that 3D logic guides structure while diffusion logic perfects texture. Project page: https://hyokong.github.io/worldwarp-page/{https://hyokong.github.io/worldwarp-page/}.",
    "translation": "标题：WorldWarp：基于异步视频扩散的三维几何传播框架\n\n摘要：生成长时序且几何一致的视频面临一个根本性困境：几何一致性要求在像素空间中严格遵循三维结构，而当前最先进的生成模型在相机条件化的隐空间中运行效果最佳。这种脱节导致现有方法在处理遮挡区域和复杂相机轨迹时存在困难。为弥合这一差距，本文提出WorldWarp框架，该框架将三维结构锚点与二维生成优化器相结合。为实现几何基础，WorldWarp通过高斯溅射（3DGS）技术维护在线三维几何缓存。通过将历史内容显式变形至新视角，该缓存作为结构支架，确保每个新帧都遵循先验几何关系。然而，静态变形不可避免地会因遮挡产生空洞与伪影。我们通过设计面向“填充-修正”目标的时空扩散（ST-Diff）模型解决此问题。本研究的核心创新在于时空动态噪声调度机制：空白区域接受完整噪声以启动生成，而变形区域则接受部分噪声以实现精细化调整。通过每一步动态更新三维缓存，WorldWarp实现了视频片段间的跨帧一致性。实验表明，该框架通过三维逻辑引导结构生成、扩散逻辑优化纹理细节，达到了当前最优的生成保真度。项目页面：https://hyokong.github.io/worldwarp-page/。",
    "url": "https://huggingface.co/papers/2512.19678",
    "arxiv_url": "https://arxiv.org/abs/2512.19678"
  },
  {
    "title": "LoGoPlanner: Localization Grounded Navigation Policy with Metric-aware Visual Geometry",
    "summary": "Trajectory planning in unstructured environments is a fundamental and challenging capability for mobile robots. Traditional modular pipelines suffer from latency and cascading errors across perception, localization, mapping, and planning modules. Recent end-to-end learning methods map raw visual observations directly to control signals or trajectories, promising greater performance and efficiency in open-world settings. However, most prior end-to-end approaches still rely on separate localization modules that depend on accurate sensor extrinsic calibration for self-state estimation, thereby limiting generalization across embodiments and environments. We introduce LoGoPlanner, a localization-grounded, end-to-end navigation framework that addresses these limitations by: (1) finetuning a long-horizon visual-geometry backbone to ground predictions with absolute metric scale, thereby providing implicit state estimation for accurate localization; (2) reconstructing surrounding scene geometry from historical observations to supply dense, fine-grained environmental awareness for reliable obstacle avoidance; and (3) conditioning the policy on implicit geometry bootstrapped by the aforementioned auxiliary tasks, thereby reducing error propagation.We evaluate LoGoPlanner in both simulation and real-world settings, where its fully end-to-end design reduces cumulative error while metric-aware geometry memory enhances planning consistency and obstacle avoidance, leading to more than a 27.3\\% improvement over oracle-localization baselines and strong generalization across embodiments and environments. The code and models have been made publicly available on the https://steinate.github.io/logoplanner.github.io/{project page}.",
    "translation": "标题：LoGoPlanner：基于度量感知视觉几何的定位支撑导航策略\n\n摘要：在非结构化环境中进行轨迹规划是移动机器人的一项基础且具有挑战性的能力。传统的模块化流程存在延迟问题，且在感知、定位、建图和规划模块间易产生级联误差。近期端到端学习方法直接将原始视觉观测映射为控制信号或轨迹，有望在开放世界场景中实现更高的性能与效率。然而，大多数现有端到端方法仍依赖独立的定位模块，这些模块需借助精确的传感器外参标定进行自身状态估计，从而限制了其在不同机器人本体与环境间的泛化能力。本文提出LoGoPlanner，一种基于定位支撑的端到端导航框架，通过以下方式解决上述局限：（1）微调长时程视觉几何骨干网络，使其预测基于绝对度量尺度，从而为精确定位提供隐式状态估计；（2）从历史观测中重建周围场景几何结构，为可靠避障提供密集、细粒度的环境感知；（3）将策略建立在前述辅助任务引导的隐式几何信息基础上，从而减少误差传播。我们在仿真和真实场景中对LoGoPlanner进行评估，其完全端到端的设计降低了累积误差，同时度量感知的几何记忆增强了规划一致性与避障能力，相比基于理想定位的基线方法性能提升超过27.3%，并在不同机器人本体与环境中表现出强大的泛化能力。代码与模型已在项目页面https://steinate.github.io/logoplanner.github.io/公开。",
    "url": "https://huggingface.co/papers/2512.19629",
    "arxiv_url": "https://arxiv.org/abs/2512.19629"
  },
  {
    "title": "UCoder: Unsupervised Code Generation by Internal Probing of Large Language Models",
    "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in code generation tasks. However, their effectiveness heavily relies on supervised training with extensive labeled (e.g., question-answering pairs) or unlabeled datasets (e.g., code snippets), which are often expensive and difficult to obtain at scale. To address this limitation, this paper introduces a method IPC, an unsupervised framework that leverages Internal Probing of LLMs for Code generation without any external corpus, even unlabeled code snippets. We introduce the problem space probing, test understanding probing, solution space probing, and knowledge consolidation and reinforcement to probe the internal knowledge and confidence patterns existing in LLMs. Further, IPC identifies reliable code candidates through self-consistency mechanisms and representation-based quality estimation to train UCoder (coder with unsupervised learning). We validate the proposed approach across multiple code benchmarks, demonstrating that unsupervised methods can achieve competitive performance compared to supervised approaches while significantly reducing the dependency on labeled data and computational resources. Analytic experiments reveal that internal model states contain rich signals about code quality and correctness, and that properly harnessing these signals enables effective unsupervised learning for code generation tasks, opening new directions for training code LLMs in resource-constrained scenarios.",
    "translation": "标题：UCoder：基于大型语言模型内部探测的无监督代码生成方法\n\n摘要：大型语言模型在代码生成任务中展现出卓越能力，但其效果严重依赖于使用大规模标注数据（如问答对）或未标注数据集（如代码片段）进行监督训练，这些数据通常成本高昂且难以大规模获取。为突破这一限制，本文提出IPC方法——一种无需任何外部语料（包括未标注代码片段）的无监督框架，通过内部探测大型语言模型实现代码生成。我们引入问题空间探测、测试理解探测、解决方案空间探测以及知识巩固与强化机制，以挖掘大型语言模型内部存在的知识结构与置信度模式。进一步地，IPC通过自洽性机制与基于表示的质量评估来筛选可靠代码候选样本，用以训练UCoder（基于无监督学习的代码生成器）。我们在多个代码基准测试中验证了所提方法，结果表明无监督方法能够达到与监督方法相竞争的性能，同时显著降低对标注数据和计算资源的依赖。分析实验表明，模型内部状态蕴含丰富的代码质量与正确性信号，有效利用这些信号能够为代码生成任务实现高效的无监督学习，从而为资源受限场景下训练代码大型语言模型开辟了新方向。",
    "url": "https://huggingface.co/papers/2512.17385",
    "arxiv_url": "https://arxiv.org/abs/2512.17385"
  },
  {
    "title": "GenEnv: Difficulty-Aligned Co-Evolution Between LLM Agents and Environment Simulators",
    "summary": "Training capable Large Language Model (LLM) agents is critically bottlenecked by the high cost and static nature of real-world interaction data. We address this by introducing GenEnv, a framework that establishes a difficulty-aligned co-evolutionary game between an agent and a scalable, generative environment simulator. Unlike traditional methods that evolve models on static datasets, GenEnv instantiates a dataevolving: the simulator acts as a dynamic curriculum policy, continuously generating tasks specifically tailored to the agent's ``zone of proximal development''. This process is guided by a simple but effective α-Curriculum Reward, which aligns task difficulty with the agent's current capabilities. We evaluate GenEnv on five benchmarks, including API-Bank, ALFWorld, BFCL, Bamboogle, and TravelPlanner. Across these tasks, GenEnv improves agent performance by up to +40.3\\% over 7B baselines and matches or exceeds the average performance of larger models. Compared to Gemini 2.5 Pro-based offline data augmentation, GenEnv achieves better performance while using 3.3times less data. By shifting from static supervision to adaptive simulation, GenEnv provides a data-efficient pathway for scaling agent capabilities.",
    "translation": "标题：GenEnv：大语言模型智能体与环境模拟器间的难度对齐协同进化\n\n摘要：当前，训练高性能大语言模型智能体的关键瓶颈在于现实世界交互数据的高成本与静态特性。为此，我们提出GenEnv框架，通过在智能体与可扩展的生成式环境模拟器之间建立一种难度对齐的协同进化博弈来解决这一问题。与传统基于静态数据集进行模型进化的方法不同，GenEnv实现了数据的动态演化：模拟器充当动态课程策略，持续生成专门适配智能体“最近发展区”的任务。这一过程由一种简单而有效的α-课程奖励机制引导，确保任务难度与智能体当前能力相匹配。我们在五个基准测试（包括API-Bank、ALFWorld、BFCL、Bamboogle和TravelPlanner）上对GenEnv进行评估。实验表明，GenEnv在7B参数基线模型上的性能提升最高达40.3%，其平均表现达到或超越了更大规模模型的水平。与基于Gemini 2.5 Pro的离线数据增强方法相比，GenEnv在使用数据量减少3.3倍的同时实现了更优的性能。通过从静态监督转向自适应模拟，GenEnv为扩展智能体能力提供了一条数据高效的技术路径。",
    "url": "https://huggingface.co/papers/2512.19682",
    "arxiv_url": "https://arxiv.org/abs/2512.19682"
  },
  {
    "title": "StoryMem: Multi-shot Long Video Storytelling with Memory",
    "summary": "Visual storytelling requires generating multi-shot videos with cinematic quality and long-range consistency. Inspired by human memory, we propose StoryMem, a paradigm that reformulates long-form video storytelling as iterative shot synthesis conditioned on explicit visual memory, transforming pre-trained single-shot video diffusion models into multi-shot storytellers. This is achieved by a novel Memory-to-Video (M2V) design, which maintains a compact and dynamically updated memory bank of keyframes from historical generated shots. The stored memory is then injected into single-shot video diffusion models via latent concatenation and negative RoPE shifts with only LoRA fine-tuning. A semantic keyframe selection strategy, together with aesthetic preference filtering, further ensures informative and stable memory throughout generation. Moreover, the proposed framework naturally accommodates smooth shot transitions and customized story generation applications. To facilitate evaluation, we introduce ST-Bench, a diverse benchmark for multi-shot video storytelling. Extensive experiments demonstrate that StoryMem achieves superior cross-shot consistency over previous methods while preserving high aesthetic quality and prompt adherence, marking a significant step toward coherent minute-long video storytelling.",
    "translation": "标题：StoryMem：基于记忆机制的多镜头长视频叙事生成\n\n摘要：视觉叙事任务需要生成具备电影级画质与长程一致性的多镜头视频。受人类记忆机制启发，本文提出StoryMem范式，该框架将长视频叙事任务重构为基于显式视觉记忆的迭代式镜头合成过程，将预训练的单镜头视频扩散模型转化为多镜头叙事生成器。这一目标通过创新的记忆到视频（M2V）架构实现：该架构维护着由历史生成镜头关键帧构成的紧凑动态记忆库，并通过潜在空间拼接与负向RoPE偏移技术将存储记忆注入单镜头视频扩散模型，仅需LoRA微调即可实现。结合语义关键帧选择策略与美学偏好过滤机制，进一步保障了生成过程中记忆信息的高效性与稳定性。此外，所提框架天然支持平滑镜头转场与定制化叙事生成应用。为推进评估体系建设，我们构建了涵盖多场景的ST-Bench多镜头视频叙事基准测试集。大量实验表明，StoryMem在保持高美学品质与提示词遵循度的同时，相比现有方法实现了更优越的跨镜头一致性，标志着向分钟级连贯视频叙事生成迈出了重要一步。",
    "url": "https://huggingface.co/papers/2512.19539",
    "arxiv_url": "https://arxiv.org/abs/2512.19539"
  },
  {
    "title": "LoPA: Scaling dLLM Inference via Lookahead Parallel Decoding",
    "summary": "Diffusion Large Language Models (dLLMs) have demonstrated significant potential for high-speed inference. However, current confidence-driven decoding strategies are constrained by limited parallelism, typically achieving only 1--3 tokens per forward pass (TPF). In this work, we identify that the degree of parallelism during dLLM inference is highly sensitive to the Token Filling Order (TFO). Then, we introduce Lookahead PArallel Decoding LoPA, a training-free, plug-and-play algorithm, to identify a superior TFO and hence accelerate inference. LoPA concurrently explores distinct candidate TFOs via parallel branches, and selects the one with the highest potential for future parallelism based on branch confidence. We apply LoPA to the state-of-the-art D2F model and observe a substantial enhancement in decoding efficiency. Notably, LoPA increases the TPF of D2F-Dream to 10.1 on the GSM8K while maintaining performance superior to the Dream baseline. Furthermore, to facilitate this unprecedented degree of parallelism, we develop a specialized multi-device inference system featuring Branch Parallelism (BP), which achieves a single-sample throughput of 1073.9 tokens per second under multi-GPU deployment. The code is available at https://github.com/zhijie-group/LoPA.",
    "translation": "标题：LoPA：基于前瞻并行解码的大规模扩散语言模型推理加速方法\n\n摘要：扩散大语言模型（dLLMs）已展现出高速推理的重要潜力。然而，当前基于置信度的解码策略受限于并行度不足，通常每次前向传播仅能生成1-3个词元（TPF）。本研究首次发现dLLM推理的并行度对词元填充顺序（TFO）具有高度敏感性。为此，我们提出无需训练即插即用的前瞻并行解码算法LoPA，通过优化TFO实现推理加速。LoPA通过并行分支同步探索不同候选TFO，并依据分支置信度筛选具有最大未来并行潜力的路径。将LoPA应用于前沿的D2F模型后，解码效率获得显著提升：在GSM8K数据集上，D2F-Dream模型的TPF提升至10.1，同时保持优于Dream基准模型的性能表现。为适配此突破性并行规模，我们进一步开发了支持分支并行（BP）的专用多设备推理系统，在多GPU部署环境下实现单样本1073.9词元/秒的吞吐量。代码已开源：https://github.com/zhijie-group/LoPA。",
    "url": "https://huggingface.co/papers/2512.16229",
    "arxiv_url": "https://arxiv.org/abs/2512.16229"
  },
  {
    "title": "MobileWorld: Benchmarking Autonomous Mobile Agents in Agent-User Interactive, and MCP-Augmented Environments",
    "summary": "Among existing online mobile-use benchmarks, AndroidWorld has emerged as the dominant benchmark due to its reproducible environment and deterministic evaluation; however, recent agents achieving over 90% success rates indicate its saturation and motivate the need for a more challenging benchmark. In addition, its environment lacks key application categories, such as e-commerce and enterprise communication, and does not reflect realistic mobile-use scenarios characterized by vague user instructions and hybrid tool usage. To bridge this gap, we introduce MobileWorld, a substantially more challenging benchmark designed to better reflect real-world mobile usage, comprising 201 tasks across 20 applications, while maintaining the same level of reproducible evaluation as AndroidWorld. The difficulty of MobileWorld is twofold. First, it emphasizes long-horizon tasks with cross-application interactions: MobileWorld requires nearly twice as many task-completion steps on average (27.8 vs. 14.3) and includes far more multi-application tasks (62.2% vs. 9.5%) compared to AndroidWorld. Second, MobileWorld extends beyond standard GUI manipulation by introducing novel task categories, including agent-user interaction and MCP-augmented tasks. To ensure robust evaluation, we provide snapshot-based container environment and precise functional verifications, including backend database inspection and task callback APIs. We further develop a planner-executor agentic framework with extended action spaces to support user interactions and MCP calls. Our results reveal a sharp performance drop compared to AndroidWorld, with the best agentic framework and end-to-end model achieving 51.7% and 20.9% success rates, respectively. Our analysis shows that current models struggle significantly with user interaction and MCP calls, offering a strategic roadmap toward more robust, next-generation mobile intelligence.",
    "translation": "标题：MobileWorld：在智能体-用户交互与MCP增强环境中的自主移动智能体基准测试\n\n摘要：在现有的在线移动使用基准测试中，AndroidWorld因其可复现的环境与确定性评估机制已成为主流基准；然而，近期智能体成功率超过90%的表现表明该基准已趋近饱和，亟需更具挑战性的新基准。此外，该环境缺乏电子商务、企业通信等关键应用类别，且未能体现用户指令模糊、工具混合使用等真实移动使用场景特征。为弥补这一差距，我们提出了MobileWorld——一个显著更具挑战性且能更好反映真实移动使用场景的基准测试，涵盖20个应用程序中的201项任务，同时保持与AndroidWorld同等的可复现评估水平。MobileWorld的挑战性主要体现在两方面：其一，它强调跨应用交互的长周期任务——与AndroidWorld相比，MobileWorld平均需要近两倍的任务完成步骤（27.8步对14.3步），且包含更多跨应用任务（62.2%对9.5%）；其二，MobileWorld通过引入智能体-用户交互及MCP增强任务等新型任务类别，突破了传统图形用户界面操作的范畴。为确保评估的稳健性，我们提供了基于快照的容器环境与精确的功能验证机制，包括后端数据库检查与任务回调接口。我们进一步开发了具备扩展动作空间的规划-执行智能体框架，以支持用户交互与MCP调用。实验结果显示，相较于AndroidWorld，各模型性能出现显著下降：最佳智能体框架与端到端模型的成功率分别仅为51.7%和20.9%。分析表明，当前模型在用户交互与MCP调用方面存在明显不足，这为构建更鲁棒的下一代移动智能技术提供了战略发展路径。",
    "url": "https://huggingface.co/papers/2512.19432",
    "arxiv_url": "https://arxiv.org/abs/2512.19432"
  },
  {
    "title": "Reasoning Palette: Modulating Reasoning via Latent Contextualization for Controllable Exploration for (V)LMs",
    "summary": "Exploration capacity shapes both inference-time performance and reinforcement learning (RL) training for large (vision-) language models, as stochastic sampling often yields redundant reasoning paths with little high-level diversity. This paper proposes Reasoning Palette, a novel latent-modulation framework that endows the model with a stochastic latent variable for strategic contextualization, guiding its internal planning prior to token generation. This latent context is inferred from the mean-pooled embedding of a question-answer pair via a variational autoencoder (VAE), where each sampled latent potentially encodes a distinct reasoning context. During inference, a sampled latent is decoded into learnable token prefixes and prepended to the input prompt, modulating the model's internal reasoning trajectory. In this way, the model performs internal sampling over reasoning strategies prior to output generation, which shapes the style and structure of the entire response sequence. A brief supervised fine-tuning (SFT) warm-up phase allows the model to adapt to this latent conditioning. Within RL optimization, Reasoning Palette facilitates structured exploration by enabling on-demand injection for diverse reasoning modes, significantly enhancing exploration efficiency and sustained learning capability. Experiments across multiple reasoning benchmarks demonstrate that our method enables interpretable and controllable control over the (vision-) language model's strategic behavior, thereby achieving consistent performance gains over standard RL methods.",
    "translation": "标题：推理调色板：通过潜在情境化调控推理以实现（视觉）语言模型的可控探索\n\n摘要：探索能力深刻影响着大型（视觉）语言模型的推理时表现与强化学习训练效果，因为随机采样常产生冗余的推理路径且缺乏高层级多样性。本文提出“推理调色板”——一种新颖的潜在调制框架，通过引入随机潜变量实现策略性情境化，在词元生成前引导模型的内部规划。该潜在上下文通过变分自编码器从问题-答案对的平均池化嵌入中推断得出，每个采样潜变量可能编码独特的推理情境。在推理阶段，采样潜变量被解码为可学习的词元前缀并附加至输入提示前，从而调控模型的内部推理轨迹。通过这种方式，模型在输出生成前对推理策略进行内部采样，进而塑造整个响应序列的风格与结构。简短的监督微调预热阶段使模型适应这种潜在条件调节机制。在强化学习优化中，推理调色板通过按需注入多样化推理模式实现结构化探索，显著提升探索效率与持续学习能力。在多个推理基准测试上的实验表明，本方法能对（视觉）语言模型的策略行为实现可解释且可控的调控，从而相较标准强化学习方法获得持续的性能提升。",
    "url": "https://huggingface.co/papers/2512.17206",
    "arxiv_url": "https://arxiv.org/abs/2512.17206"
  },
  {
    "title": "Is There a Better Source Distribution than Gaussian? Exploring Source Distributions for Image Flow Matching",
    "summary": "Flow matching has emerged as a powerful generative modeling approach with flexible choices of source distribution. While Gaussian distributions are commonly used, the potential for better alternatives in high-dimensional data generation remains largely unexplored. In this paper, we propose a novel 2D simulation that captures high-dimensional geometric properties in an interpretable 2D setting, enabling us to analyze the learning dynamics of flow matching during training. Based on this analysis, we derive several key insights about flow matching behavior: (1) density approximation can paradoxically degrade performance due to mode discrepancy, (2) directional alignment suffers from path entanglement when overly concentrated, (3) Gaussian's omnidirectional coverage ensures robust learning, and (4) norm misalignment incurs substantial learning costs. Building on these insights, we propose a practical framework that combines norm-aligned training with directionally-pruned sampling. This approach maintains the robust omnidirectional supervision essential for stable flow learning, while eliminating initializations in data-sparse regions during inference. Importantly, our pruning strategy can be applied to any flow matching model trained with a Gaussian source, providing immediate performance gains without the need for retraining. Empirical evaluations demonstrate consistent improvements in both generation quality and sampling efficiency. Our findings provide practical insights and guidelines for source distribution design and introduce a readily applicable technique for improving existing flow matching models. Our code is available at https://github.com/kwanseokk/SourceFM.",
    "translation": "标题：是否存在优于高斯分布的源分布？探索图像流匹配中的源分布选择\n\n摘要：流匹配作为一种强大的生成建模方法，其源分布的选择具有高度灵活性。尽管高斯分布被广泛采用，但在高维数据生成任务中是否存在更优替代方案的问题尚未得到充分探索。本文提出了一种新颖的二维模拟方法，通过在可解释的二维环境中捕捉高维几何特性，使我们能够分析流匹配在训练过程中的学习动态。基于此分析，我们得出关于流匹配行为的若干关键发现：（1）密度近似可能因模态差异而降低生成性能；（2）方向对齐在过度集中时会产生路径纠缠问题；（3）高斯分布的全向覆盖特性可确保稳健学习；（4）范数失准会导致显著的学习成本。基于这些发现，我们提出了一种结合范数对齐训练与方向剪枝采样的实用框架。该方法既保持了稳定流学习所必需的全向监督机制，又在推理阶段消除了数据稀疏区域的初始化问题。值得注意的是，我们的剪枝策略可应用于任何使用高斯源分布训练的流匹配模型，无需重新训练即可获得即时性能提升。实证评估表明，该方法在生成质量和采样效率方面均能实现持续改进。我们的研究为源分布设计提供了实用见解与指导原则，并提出了一种可直接应用于改进现有流匹配模型的技术。代码已开源：https://github.com/kwanseokk/SourceFM。",
    "url": "https://huggingface.co/papers/2512.18184",
    "arxiv_url": "https://arxiv.org/abs/2512.18184"
  },
  {
    "title": "Real2Edit2Real: Generating Robotic Demonstrations via a 3D Control Interface",
    "summary": "Recent progress in robot learning has been driven by large-scale datasets and powerful visuomotor policy architectures, yet policy robustness remains limited by the substantial cost of collecting diverse demonstrations, particularly for spatial generalization in manipulation tasks. To reduce repetitive data collection, we present Real2Edit2Real, a framework that generates new demonstrations by bridging 3D editability with 2D visual data through a 3D control interface. Our approach first reconstructs scene geometry from multi-view RGB observations with a metric-scale 3D reconstruction model. Based on the reconstructed geometry, we perform depth-reliable 3D editing on point clouds to generate new manipulation trajectories while geometrically correcting the robot poses to recover physically consistent depth, which serves as a reliable condition for synthesizing new demonstrations. Finally, we propose a multi-conditional video generation model guided by depth as the primary control signal, together with action, edge, and ray maps, to synthesize spatially augmented multi-view manipulation videos. Experiments on four real-world manipulation tasks demonstrate that policies trained on data generated from only 1-5 source demonstrations can match or outperform those trained on 50 real-world demonstrations, improving data efficiency by up to 10-50x. Moreover, experimental results on height and texture editing demonstrate the framework's flexibility and extensibility, indicating its potential to serve as a unified data generation framework.",
    "translation": "标题：Real2Edit2Real：通过三维控制界面生成机器人演示数据\n\n摘要：机器人学习的最新进展得益于大规模数据集和强大的视觉运动策略架构，但策略的鲁棒性仍受限于收集多样化演示数据的高昂成本，尤其是在操作任务的空间泛化方面。为减少重复性数据采集，本文提出Real2Edit2Real框架，该框架通过三维控制界面将三维可编辑性与二维视觉数据相融合，从而生成新的演示数据。我们的方法首先利用度量尺度三维重建模型，从多视角RGB观测中重建场景几何结构。基于重建的几何结构，我们在点云上进行深度可靠的三维编辑以生成新的操作轨迹，同时通过几何校正机器人姿态以恢复物理一致的深度信息，这为合成新演示提供了可靠条件。最后，我们提出一种以深度为主要控制信号，并结合动作、边缘和射线图的多条件视频生成模型，用于合成空间增强的多视角操作视频。在四个真实世界操作任务上的实验表明，仅使用1-5个源演示生成数据训练的策略，其性能可达到甚至超越使用50个真实世界演示训练的策略，将数据效率提升高达10-50倍。此外，在高度和纹理编辑方面的实验结果证明了该框架的灵活性和可扩展性，表明其有潜力成为统一的数据生成框架。",
    "url": "https://huggingface.co/papers/2512.19402",
    "arxiv_url": "https://arxiv.org/abs/2512.19402"
  },
  {
    "title": "Does It Tie Out? Towards Autonomous Legal Agents in Venture Capital",
    "summary": "Before closing venture capital financing rounds, lawyers conduct diligence that includes tying out the capitalization table: verifying that every security (for example, shares, options, warrants) and issuance term (for example, vesting schedules, acceleration triggers, transfer restrictions) is supported by large sets of underlying legal documentation. While LLMs continue to improve on legal benchmarks, specialized legal workflows, such as capitalization tie-out, remain out of reach even for strong agentic systems. The task requires multi-document reasoning, strict evidence traceability, and deterministic outputs that current approaches fail to reliably deliver. We characterize capitalization tie-out as an instance of a real-world benchmark for legal AI, analyze and compare the performance of existing agentic systems, and propose a world model architecture toward tie-out automation-and more broadly as a foundation for applied legal intelligence.",
    "translation": "标题：能否核对无误？风险投资领域自主法律智能体的发展路径\n\n摘要：在完成风险投资融资轮次前，律师需开展包括股权结构表核验在内的尽职调查工作，即通过大量底层法律文件验证每项证券（如股份、期权、认股权证）及发行条款（如归属时间表、加速触发条件、转让限制）的合规性。尽管大语言模型在法律基准测试中持续进步，但针对股权结构核验等专业法律工作流程，即使当前先进的智能体系统仍难以胜任。该任务需要多文档推理能力、严格的证据可追溯性以及确定性输出，而现有技术方案尚无法稳定实现这些要求。本文将股权结构核验界定为法律人工智能的现实基准测试案例，系统分析与比较现有智能体系统的表现，进而提出面向自动化核验任务的世界模型架构——该架构亦可作为应用型法律智能更广泛发展的基础框架。",
    "url": "https://huggingface.co/papers/2512.18658",
    "arxiv_url": "https://arxiv.org/abs/2512.18658"
  },
  {
    "title": "CASA: Cross-Attention via Self-Attention for Efficient Vision-Language Fusion",
    "summary": "Vision-language models (VLMs) are commonly trained by inserting image tokens from a pretrained vision encoder into the textual stream of a language model. This allows text and image information to fully attend to one another within the model, but becomes extremely costly for high-resolution images, long conversations, or streaming videos, both in memory and compute. VLMs leveraging cross-attention are an efficient alternative to token insertion but exhibit a clear performance gap, in particular on tasks involving fine-grained visual details. We find that a key to improving such models is to also enable local text-to-text interaction in the dedicated cross-attention layers. Building on this, we propose CASA, Cross-Attention via Self-Attention, a simple and efficient paradigm which substantially reduces the gap with full token insertion on common image understanding benchmarks, while enjoying the same scalability as cross-attention models when applied to long-context multimodal tasks such as streaming video captioning. For samples and code, please see our project page at https://kyutai.org/casa .",
    "translation": "标题：CASA：通过自注意力实现跨模态注意力的高效视觉语言融合\n\n摘要：视觉语言模型通常通过将预训练视觉编码器生成的图像标记插入语言模型的文本流中进行训练。这种方法允许文本与图像信息在模型内部充分交互，但在处理高分辨率图像、长对话或流式视频时，会带来极高的内存与计算成本。采用跨注意力机制的视觉语言模型是标记插入方法的高效替代方案，但其性能存在明显差距，尤其在涉及细粒度视觉细节的任务中。我们发现，提升此类模型性能的关键在于在专用跨注意力层中同时实现局部文本到文本的交互。基于此，我们提出CASA（通过自注意力实现跨模态注意力），这是一种简洁高效的范式。该范式在常见图像理解基准测试中显著缩小了与全标记插入方法的性能差距，同时在处理流式视频描述等长上下文多模态任务时，保持了与跨注意力模型同等的可扩展性。示例与代码请访问项目页面：https://kyutai.org/casa。",
    "url": "https://huggingface.co/papers/2512.19535",
    "arxiv_url": "https://arxiv.org/abs/2512.19535"
  },
  {
    "title": "MatSpray: Fusing 2D Material World Knowledge on 3D Geometry",
    "summary": "Manual modeling of material parameters and 3D geometry is a time consuming yet essential task in the gaming and film industries. While recent advances in 3D reconstruction have enabled accurate approximations of scene geometry and appearance, these methods often fall short in relighting scenarios due to the lack of precise, spatially varying material parameters. At the same time, diffusion models operating on 2D images have shown strong performance in predicting physically based rendering (PBR) properties such as albedo, roughness, and metallicity. However, transferring these 2D material maps onto reconstructed 3D geometry remains a significant challenge. We propose a framework for fusing 2D material data into 3D geometry using a combination of novel learning-based and projection-based approaches. We begin by reconstructing scene geometry via Gaussian Splatting. From the input images, a diffusion model generates 2D maps for albedo, roughness, and metallic parameters. Any existing diffusion model that can convert images or videos to PBR materials can be applied. The predictions are further integrated into the 3D representation either by optimizing an image-based loss or by directly projecting the material parameters onto the Gaussians using Gaussian ray tracing. To enhance fine-scale accuracy and multi-view consistency, we further introduce a light-weight neural refinement step (Neural Merger), which takes ray-traced material features as input and produces detailed adjustments. Our results demonstrate that the proposed methods outperform existing techniques in both quantitative metrics and perceived visual realism. This enables more accurate, relightable, and photorealistic renderings from reconstructed scenes, significantly improving the realism and efficiency of asset creation workflows in content production pipelines.",
    "translation": "标题：MatSpray：将二维材料世界知识融合至三维几何结构\n\n摘要：在游戏与影视行业中，手动建模材质参数与三维几何结构是一项耗时但关键的任务。尽管三维重建技术的最新进展已能实现对场景几何结构与外观的精确近似，但由于缺乏精确且空间变化的材质参数，这些方法在重光照场景中往往表现不足。与此同时，基于二维图像的扩散模型在预测基于物理的渲染（PBR）属性（如反照率、粗糙度与金属度）方面展现出强大性能。然而，将这些二维材质贴图迁移至重建的三维几何结构上仍面临重大挑战。本文提出一种融合学习与投影的创新框架，将二维材质数据融入三维几何结构。我们首先通过高斯泼溅技术重建场景几何，并利用扩散模型从输入图像生成反照率、粗糙度与金属度的二维贴图——任何能够将图像或视频转换为PBR材质的现有扩散模型均可适用。随后，通过优化基于图像的损失函数，或借助高斯光线追踪将材质参数直接投影至高斯单元，将这些预测结果进一步整合到三维表征中。为提升细节精度与多视角一致性，我们进一步引入轻量级神经优化步骤（神经融合器），该模块以光线追踪生成的材质特征为输入，输出精细化调整结果。实验表明，所提方法在量化指标与视觉真实感方面均优于现有技术，能够从重建场景中生成更精确、可重光照且具有照片级真实感的渲染效果，显著提升了内容生产流程中资产创建工作的真实感与效率。",
    "url": "https://huggingface.co/papers/2512.18314",
    "arxiv_url": "https://arxiv.org/abs/2512.18314"
  },
  {
    "title": "Name That Part: 3D Part Segmentation and Naming",
    "summary": "We address semantic 3D part segmentation: decomposing objects into parts with meaningful names. While datasets exist with part annotations, their definitions are inconsistent across datasets, limiting robust training. Previous methods produce unlabeled decompositions or retrieve single parts without complete shape annotations. We propose ALIGN-Parts, which formulates part naming as a direct set alignment task. Our method decomposes shapes into partlets - implicit 3D part representations - matched to part descriptions via bipartite assignment. We combine geometric cues from 3D part fields, appearance from multi-view vision features, and semantic knowledge from language-model-generated affordance descriptions. Text-alignment loss ensures partlets share embedding space with text, enabling a theoretically open-vocabulary matching setup, given sufficient data. Our efficient and novel, one-shot, 3D part segmentation and naming method finds applications in several downstream tasks, including serving as a scalable annotation engine. As our model supports zero-shot matching to arbitrary descriptions and confidence-calibrated predictions for known categories, with human verification, we create a unified ontology that aligns PartNet, 3DCoMPaT++, and Find3D, consisting of 1,794 unique 3D parts. We also show examples from our newly created Tex-Parts dataset. We also introduce 2 novel metrics appropriate for the named 3D part segmentation task.",
    "translation": "标题：部件识别：三维部件分割与命名\n\n摘要：本文研究语义三维部件分割问题，即如何将物体分解为具有语义命名的部件。尽管现有数据集包含部件标注信息，但不同数据集间的定义标准不一致，限制了模型的鲁棒性训练。现有方法通常仅生成未标注的分解结果，或在缺乏完整形状标注的情况下检索单一部件。我们提出ALIGN-Parts方法，将部件命名建模为直接集合对齐任务。该方法将三维形状分解为部件单元——一种隐式三维部件表征，并通过二分图匹配与部件描述进行关联。我们融合了三维部件场的几何特征、多视角视觉特征的外观信息，以及语言模型生成的功能描述所包含的语义知识。通过文本对齐损失函数，部件单元与文本共享嵌入空间，在数据充足的条件下实现了理论上的开放词汇匹配框架。我们提出的高效、新颖的单次三维部件分割与命名方法，在多个下游任务中具有应用价值，包括作为可扩展的标注引擎。该模型支持对任意描述的零样本匹配，并对已知类别提供置信度校准预测。经人工验证，我们构建了统一的本体框架，整合了PartNet、3DCoMPaT++和Find3D数据集，涵盖1,794个独立三维部件。同时展示了新构建的Tex-Parts数据集样例，并针对命名三维部件分割任务提出了两种创新性评估指标。",
    "url": "https://huggingface.co/papers/2512.18003",
    "arxiv_url": "https://arxiv.org/abs/2512.18003"
  },
  {
    "title": "Understanding Syllogistic Reasoning in LLMs from Formal and Natural Language Perspectives",
    "summary": "We study syllogistic reasoning in LLMs from the logical and natural language perspectives. In process, we explore fundamental reasoning capabilities of the LLMs and the direction this research is moving forward. To aid in our studies, we use 14 large language models and investigate their syllogistic reasoning capabilities in terms of symbolic inferences as well as natural language understanding. Even though this reasoning mechanism is not a uniform emergent property across LLMs, the perfect symbolic performances in certain models make us wonder whether LLMs are becoming more and more formal reasoning mechanisms, rather than making explicit the nuances of human reasoning.",
    "translation": "标题：从形式与自然语言视角理解大语言模型的三段论推理能力\n\n摘要：本研究从逻辑学与自然语言双重视角探讨大语言模型的三段论推理能力。在此过程中，我们深入考察大语言模型的基础推理机制及其研究发展趋势。为支撑研究，我们选取了14个大型语言模型，分别从符号推理与自然语言理解两个维度系统检验其三段论推理性能。研究结果表明，虽然这种推理能力并非所有大语言模型普遍具备的涌现特性，但部分模型在符号推理任务中展现的完美表现促使我们思考：大语言模型是否正在演化为日益形式化的推理机制，而非真正揭示人类推理过程的微妙特征。",
    "url": "https://huggingface.co/papers/2512.12620",
    "arxiv_url": "https://arxiv.org/abs/2512.12620"
  },
  {
    "title": "Over++: Generative Video Compositing for Layer Interaction Effects",
    "summary": "In professional video compositing workflows, artists must manually create environmental interactions-such as shadows, reflections, dust, and splashes-between foreground subjects and background layers. Existing video generative models struggle to preserve the input video while adding such effects, and current video inpainting methods either require costly per-frame masks or yield implausible results. We introduce augmented compositing, a new task that synthesizes realistic, semi-transparent environmental effects conditioned on text prompts and input video layers, while preserving the original scene. To address this task, we present Over++, a video effect generation framework that makes no assumptions about camera pose, scene stationarity, or depth supervision. We construct a paired effect dataset tailored for this task and introduce an unpaired augmentation strategy that preserves text-driven editability. Our method also supports optional mask control and keyframe guidance without requiring dense annotations. Despite training on limited data, Over++ produces diverse and realistic environmental effects and outperforms existing baselines in both effect generation and scene preservation.",
    "translation": "标题：Over++：面向图层交互效果的生成式视频合成\n\n摘要：在专业视频合成工作流程中，艺术家需要手动创建前景主体与背景图层之间的环境交互效果（如阴影、反射、扬尘、飞溅等）。现有视频生成模型难以在添加此类效果的同时保持输入视频内容，而当前视频修复方法要么需要逐帧的高成本掩码标注，要么会产生不符合物理规律的结果。本文提出增强合成这一新任务，其目标是在保持原始场景的基础上，根据文本提示与输入视频图层合成逼真的半透明环境效果。为此，我们提出Over++视频特效生成框架，该框架无需对相机位姿、场景静止性或深度监督进行假设。我们构建了针对该任务的配对特效数据集，并提出保留文本驱动编辑能力的非配对增强策略。该方法还支持可选的掩码控制和关键帧引导，且无需密集标注。尽管在有限数据上训练，Over++仍能生成多样化且逼真的环境效果，在特效生成与场景保持两方面均优于现有基线方法。",
    "url": "https://huggingface.co/papers/2512.19661",
    "arxiv_url": "https://arxiv.org/abs/2512.19661"
  },
  {
    "title": "Brain-Grounded Axes for Reading and Steering LLM States",
    "summary": "Interpretability methods for large language models (LLMs) typically derive directions from textual supervision, which can lack external grounding. We propose using human brain activity not as a training signal but as a coordinate system for reading and steering LLM states. Using the SMN4Lang MEG dataset, we construct a word-level brain atlas of phase-locking value (PLV) patterns and extract latent axes via ICA. We validate axes with independent lexica and NER-based labels (POS/log-frequency used as sanity checks), then train lightweight adapters that map LLM hidden states to these brain axes without fine-tuning the LLM. Steering along the resulting brain-derived directions yields a robust lexical (frequency-linked) axis in a mid TinyLlama layer, surviving perplexity-matched controls, and a brain-vs-text probe comparison shows larger log-frequency shifts (relative to the text probe) with lower perplexity for the brain axis. A function/content axis (axis 13) shows consistent steering in TinyLlama, Qwen2-0.5B, and GPT-2, with PPL-matched text-level corroboration. Layer-4 effects in TinyLlama are large but inconsistent, so we treat them as secondary (Appendix). Axis structure is stable when the atlas is rebuilt without GPT embedding-change features or with word2vec embeddings (|r|=0.64-0.95 across matched axes), reducing circularity concerns. Exploratory fMRI anchoring suggests potential alignment for embedding change and log frequency, but effects are sensitive to hemodynamic modeling assumptions and are treated as population-level evidence only. These results support a new interface: neurophysiology-grounded axes provide interpretable and controllable handles for LLM behavior.",
    "translation": "标题：基于大脑的阅读与调控大语言模型状态的坐标轴\n\n摘要：针对大语言模型（LLMs）的可解释性方法通常依赖于文本监督来推导方向向量，但此类方法往往缺乏外部实体基础。本研究提出以人脑活动作为坐标系统（而非训练信号），用于读取与调控大语言模型的内部状态。基于SMN4Lang脑磁图数据集，我们构建了词级别的相位锁定值模式脑图谱，并通过独立成分分析提取潜在坐标轴。这些坐标轴通过独立词典和基于命名实体识别的标签进行验证（词性/对数词频作为有效性检验），随后训练轻量级适配器将大语言模型隐藏状态映射至这些大脑坐标轴，而无需对大语言模型本身进行微调。沿大脑衍生的方向进行调控后，在TinyLlama模型的中间层发现了一个稳健的词汇（与词频相关）坐标轴，该结果在困惑度匹配的对照实验中依然成立；与基于文本的探针相比，大脑坐标轴在实现更低困惑度的同时展现出更大的对数词频偏移量。此外，一个功能/内容坐标轴（第13轴）在TinyLlama、Qwen2-0.5B和GPT-2模型中均表现出一致的调控效果，并得到困惑度匹配的文本层面证据支持。TinyLlama第4层的影响虽显著但不稳定，故我们将其视为次要发现（详见附录）。当使用不含GPT嵌入变化特征的图谱或采用word2vec嵌入重建图谱时，坐标轴结构保持稳定（匹配坐标轴间|r|=0.64-0.95），降低了循环论证的担忧。探索性功能磁共振成像锚定实验提示嵌入变化与对数词频可能存在潜在关联，但该结果对血流动力学模型假设较为敏感，因此仅视为群体层面的参考证据。这些研究结果支持一种新型交互范式：基于神经生理学的坐标轴为大语言模型行为提供了可解释且可控的调控路径。",
    "url": "https://huggingface.co/papers/2512.19399",
    "arxiv_url": "https://arxiv.org/abs/2512.19399"
  },
  {
    "title": "SecureCode v2.0: A Production-Grade Dataset for Training Security-Aware Code Generation Models",
    "summary": "AI assistants produce vulnerable code in 45% of security-relevant scenarios, introducing flaws into production systems at scale. Yet existing secure coding datasets fall short. They lack incident grounding, don't provide the scale modern training requires, and miss the operational security context developers need for production deployments. We present SecureCode v2.0, a production-grade dataset of 1,215 security-focused coding examples that passed structural validation and expert security review. Every example ties to actual documented security incidents with CVE references, provides vulnerable and secure implementations, demonstrates concrete attacks, and includes defense-in-depth operational guidance. The dataset covers 11 vulnerability categories (complete OWASP Top 10:2025 plus AI/ML Security Threats) across 11 languages (Python, JavaScript, Java, Go, PHP, C#, TypeScript, Ruby, Rust, Kotlin, and YAML for infrastructure-as-code).\n  Our quality assurance framework ensures complete incident grounding. Each example includes SIEM integration strategies, infrastructure hardening recommendations (Docker, AppArmor, WAF configurations), and testing approaches using language-appropriate frameworks. The dataset uses a 4-turn conversational structure mirroring actual developer-AI interactions, escalating from basic implementations to advanced security considerations and defense-in-depth guidance.\n  Our contributions: (1) 1,215 rigorously validated examples split into 989 training, 122 validation, and 104 test sets, (2) an automated validation framework ensuring dataset consistency, (3) a 4-turn conversational structure capturing realistic security workflows, (4) comprehensive operational security guidance with SIEM integration strategies, (5) complete language-specific implementation fidelity, and (6) open-source release of data, validation tools, and benchmarking protocols.",
    "translation": "标题：SecureCode v2.0：用于训练安全感知代码生成模型的生产级数据集\n\n摘要：AI助手在45%的安全相关场景中会产生存在漏洞的代码，从而将缺陷大规模引入生产系统。然而，现有的安全编码数据集存在不足：它们缺乏真实事件依据，无法满足现代训练所需的规模，并且缺失开发者在生产部署时所需的操作安全上下文。我们提出了SecureCode v2.0，这是一个包含1,215个聚焦安全性的编码示例的生产级数据集，所有示例均通过了结构验证和专家安全审查。每个示例均关联到具有CVE编号的实际已记录安全事件，提供存在漏洞的实现与安全实现方案，展示具体攻击方式，并包含纵深防御操作指南。该数据集涵盖11个漏洞类别（完整覆盖OWASP Top 10:2025及AI/ML安全威胁）和11种编程语言（Python、JavaScript、Java、Go、PHP、C#、TypeScript、Ruby、Rust、Kotlin以及用于基础设施即代码的YAML）。\n\n我们的质量保障框架确保所有示例均基于真实事件。每个示例包含SIEM集成策略、基础设施加固建议（Docker、AppArmor、WAF配置）以及使用语言适配框架的测试方法。数据集采用4轮对话结构模拟真实开发者与AI的交互过程，从基础实现逐步升级到高级安全考量与纵深防御指导。\n\n我们的贡献包括：（1）1,215个经过严格验证的示例，划分为989个训练集、122个验证集和104个测试集；（2）确保数据集一致性的自动化验证框架；（3）捕捉真实安全工作流程的4轮对话结构；（4）包含SIEM集成策略的全面操作安全指南；（5）完整的语言特定实现保真度；（6）开源发布数据集、验证工具与基准测试协议。",
    "url": "https://huggingface.co/papers/2512.18542",
    "arxiv_url": "https://arxiv.org/abs/2512.18542"
  }
]