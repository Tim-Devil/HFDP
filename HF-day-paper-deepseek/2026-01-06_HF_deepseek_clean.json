[
  {
    "title": "Can LLMs Predict Their Own Failures? Self-Awareness via Internal Circuits",
    "summary": "Large language models (LLMs) generate fluent and complex outputs but often fail to recognize their own mistakes and hallucinations. Existing approaches typically rely on external judges, multi-sample consistency, or text-based self-critique, which incur additional compute or correlate weakly with true correctness. We ask: can LLMs predict their own failures by inspecting internal states during inference? We introduce Gnosis, a lightweight self-awareness mechanism that enables frozen LLMs to perform intrinsic self-verification by decoding signals from hidden states and attention patterns. Gnosis passively observes internal traces, compresses them into fixed-budget descriptors, and predicts correctness with negligible inference cost, adding only ~5M parameters and operating independently of sequence length. Across math reasoning, open-domain question answering, and academic knowledge benchmarks, and over frozen backbones ranging from 1.7B to 20B parameters, Gnosis consistently outperforms strong internal baselines and large external judges in both accuracy and calibration. Moreover, it generalizes zero-shot to partial generations, enabling early detection of failing trajectories and compute-aware control. These results show that reliable correctness cues are intrinsic to generation process and can be extracted efficiently without external supervision.",
    "translation": "标题：大语言模型能否预测自身失误？基于内部回路的自我感知机制\n\n摘要：大语言模型（LLMs）能够生成流畅复杂的输出，却往往无法识别自身的错误与幻觉。现有方法通常依赖外部评判器、多样本一致性检验或基于文本的自我批判，这些方式要么需要额外计算资源，要么与真实准确性的关联度较弱。本文提出核心问题：大语言模型能否通过推理过程中对内部状态的监测来预测自身失误？我们引入Gnosis——一种轻量级自我感知机制，使冻结参数的大语言模型能够通过解码隐藏状态与注意力模式的信号进行内在自我验证。Gnosis被动观测内部计算轨迹，将其压缩为固定预算的描述符，并以可忽略的推理成本预测正确性，仅增加约500万参数且运算独立于序列长度。在数学推理、开放域问答和学术知识基准测试中，针对1.7B至20B参数规模的冻结骨干模型，Gnosis在准确率与校准度方面持续优于强内部基线及大型外部评判器。此外，该机制能零样本泛化至部分生成结果，实现对错误轨迹的早期检测及计算感知控制。这些结果表明，可靠的正确性线索内生于生成过程，无需外部监督即可高效提取。\n\n请按照以下格式返回：\n标题：[大语言模型能否预测自身失误？基于内部回路的自我感知机制]\n摘要：[大语言模型（LLMs）能够生成流畅复杂的输出，却往往无法识别自身的错误与幻觉。现有方法通常依赖外部评判器、多样本一致性检验或基于文本的自我批判，这些方式要么需要额外计算资源，要么与真实准确性的关联度较弱。本文提出核心问题：大语言模型能否通过推理过程中对内部状态的监测来预测自身失误？我们引入Gnosis——一种轻量级自我感知机制，使冻结参数的大语言模型能够通过解码隐藏状态与注意力模式的信号进行内在自我验证。Gnosis被动观测内部计算轨迹，将其压缩为固定预算的描述符，并以可忽略的推理成本预测正确性，仅增加约500万参数且运算独立于序列长度。在数学推理、开放域问答和学术知识基准测试中，针对1.7B至20B参数规模的冻结骨干模型，Gnosis在准确率与校准度方面持续优于强内部基线及大型外部评判器。此外，该机制能零样本泛化至部分生成结果，实现对错误轨迹的早期检测及计算感知控制。这些结果表明，可靠的正确性线索内生于生成过程，无需外部监督即可高效提取。]",
    "url": "https://huggingface.co/papers/2512.20578",
    "arxiv_url": "https://arxiv.org/abs/2512.20578"
  },
  {
    "title": "K-EXAONE Technical Report",
    "summary": "This technical report presents K-EXAONE, a large-scale multilingual language model developed by LG AI Research. K-EXAONE is built on a Mixture-of-Experts architecture with 236B total parameters, activating 23B parameters during inference. It supports a 256K-token context window and covers six languages: Korean, English, Spanish, German, Japanese, and Vietnamese. We evaluate K-EXAONE on a comprehensive benchmark suite spanning reasoning, agentic, general, Korean, and multilingual abilities. Across these evaluations, K-EXAONE demonstrates performance comparable to open-weight models of similar size. K-EXAONE, designed to advance AI for a better life, is positioned as a powerful proprietary AI foundation model for a wide range of industrial and research applications.",
    "translation": "标题：K-EXAONE 技术报告\n\n摘要：本技术报告介绍了由 LG AI Research 开发的大规模多语言语言模型 K-EXAONE。K-EXAONE 基于混合专家架构构建，总参数量达 2360 亿，推理时激活 230 亿参数。它支持 256K 令牌的上下文窗口，并涵盖六种语言：韩语、英语、西班牙语、德语、日语和越南语。我们在涵盖推理、智能体、通用能力、韩语能力及多语言能力的综合基准测试套件上对 K-EXAONE 进行了评估。在所有评估中，K-EXAONE 展现出与同类规模开源模型相当的性能。K-EXAONE 旨在推动人工智能发展以创造更美好的生活，其定位是为广泛的工业和研究应用提供强大的专有 AI 基础模型。",
    "url": "https://huggingface.co/papers/2601.01739",
    "arxiv_url": "https://arxiv.org/abs/2601.01739"
  },
  {
    "title": "NextFlow: Unified Sequential Modeling Activates Multimodal Understanding and Generation",
    "summary": "We present NextFlow, a unified decoder-only autoregressive transformer trained on 6 trillion interleaved text-image discrete tokens. By leveraging a unified vision representation within a unified autoregressive architecture, NextFlow natively activates multimodal understanding and generation capabilities, unlocking abilities of image editing, interleaved content and video generation. Motivated by the distinct nature of modalities - where text is strictly sequential and images are inherently hierarchical - we retain next-token prediction for text but adopt next-scale prediction for visual generation. This departs from traditional raster-scan methods, enabling the generation of 1024x1024 images in just 5 seconds - orders of magnitude faster than comparable AR models. We address the instabilities of multi-scale generation through a robust training recipe. Furthermore, we introduce a prefix-tuning strategy for reinforcement learning. Experiments demonstrate that NextFlow achieves state-of-the-art performance among unified models and rivals specialized diffusion baselines in visual quality.",
    "translation": "标题：NextFlow：统一序列建模激活多模态理解与生成能力\n\n摘要：本文提出NextFlow模型，这是一种仅含解码器的统一自回归Transformer架构，在6万亿交错排列的文本-图像离散标记上进行训练。通过在统一的自回归架构中采用统一的视觉表征方法，NextFlow原生激活了多模态理解与生成能力，实现了图像编辑、交错内容生成和视频生成等功能。针对不同模态的本质特性——文本具有严格序列性而图像具有内在层次性——我们保留文本的下一标记预测机制，但对视觉生成采用下一尺度预测方法。这一设计突破了传统光栅扫描方法的局限，仅需5秒即可生成1024×1024分辨率图像，比同类自回归模型快数个数量级。我们通过稳健的训练方案解决了多尺度生成的不稳定性问题，并提出了用于强化学习的前缀调优策略。实验表明，NextFlow在统一模型中实现了最先进的性能，其视觉质量可与专业扩散基线模型相媲美。",
    "url": "https://huggingface.co/papers/2601.02204",
    "arxiv_url": "https://arxiv.org/abs/2601.02204"
  },
  {
    "title": "DreamID-V:Bridging the Image-to-Video Gap for High-Fidelity Face Swapping via Diffusion Transformer",
    "summary": "Video Face Swapping (VFS) requires seamlessly injecting a source identity into a target video while meticulously preserving the original pose, expression, lighting, background, and dynamic information. Existing methods struggle to maintain identity similarity and attribute preservation while preserving temporal consistency. To address the challenge, we propose a comprehensive framework to seamlessly transfer the superiority of Image Face Swapping (IFS) to the video domain. We first introduce a novel data pipeline SyncID-Pipe that pre-trains an Identity-Anchored Video Synthesizer and combines it with IFS models to construct bidirectional ID quadruplets for explicit supervision. Building upon paired data, we propose the first Diffusion Transformer-based framework DreamID-V, employing a core Modality-Aware Conditioning module to discriminatively inject multi-model conditions. Meanwhile, we propose a Synthetic-to-Real Curriculum mechanism and an Identity-Coherence Reinforcement Learning strategy to enhance visual realism and identity consistency under challenging scenarios. To address the issue of limited benchmarks, we introduce IDBench-V, a comprehensive benchmark encompassing diverse scenes. Extensive experiments demonstrate DreamID-V outperforms state-of-the-art methods and further exhibits exceptional versatility, which can be seamlessly adapted to various swap-related tasks.",
    "translation": "标题：DreamID-V：基于扩散Transformer的高保真人脸视频替换——弥合图像到视频的鸿沟\n\n摘要：视频人脸替换任务要求将源身份无缝注入目标视频，同时精确保持原始姿态、表情、光照、背景及动态信息。现有方法在保持时序一致性的同时，难以兼顾身份相似性与属性保留。为解决这一挑战，我们提出一个完整框架，将图像人脸替换技术的优势无缝迁移至视频领域。我们首先设计新型数据流水线SyncID-Pipe，通过预训练身份锚定视频合成器并结合图像人脸替换模型，构建双向身份四元组以实现显式监督。基于配对数据，我们提出首个基于扩散Transformer的框架DreamID-V，其核心模态感知调节模块能够 discriminatively 注入多模态条件。同时，我们提出合成到真实的课程学习机制与身份一致性强化学习策略，以增强复杂场景下的视觉真实感与身份一致性。针对现有基准数据不足的问题，我们构建了涵盖多样化场景的综合评测基准IDBench-V。大量实验表明，DreamID-V在性能上超越现有最优方法，并展现出卓越的泛化能力，可无缝适配多种替换相关任务。",
    "url": "https://huggingface.co/papers/2601.01425",
    "arxiv_url": "https://arxiv.org/abs/2601.01425"
  },
  {
    "title": "VAR RL Done Right: Tackling Asynchronous Policy Conflicts in Visual Autoregressive Generation",
    "summary": "Visual generation is dominated by three paradigms: AutoRegressive (AR), diffusion, and Visual AutoRegressive (VAR) models. Unlike AR and diffusion, VARs operate on heterogeneous input structures across their generation steps, which creates severe asynchronous policy conflicts. This issue becomes particularly acute in reinforcement learning (RL) scenarios, leading to unstable training and suboptimal alignment. To resolve this, we propose a novel framework to enhance Group Relative Policy Optimization (GRPO) by explicitly managing these conflicts. Our method integrates three synergistic components: 1) a stabilizing intermediate reward to guide early-stage generation; 2) a dynamic time-step reweighting scheme for precise credit assignment; and 3) a novel mask propagation algorithm, derived from principles of Reward Feedback Learning (ReFL), designed to isolate optimization effects both spatially and temporally. Our approach demonstrates significant improvements in sample quality and objective alignment over the vanilla GRPO baseline, enabling robust and effective optimization for VAR models.",
    "translation": "标题：VAR强化学习的正确实现：解决视觉自回归生成中的异步策略冲突\n\n摘要：视觉生成领域主要由三种范式主导：自回归模型、扩散模型以及视觉自回归模型。与自回归和扩散模型不同，视觉自回归模型在其生成步骤中处理异构输入结构，这导致了严重的异步策略冲突。该问题在强化学习场景中尤为突出，常引发训练过程不稳定与目标对齐欠佳。为解决此问题，我们提出一种新颖框架，通过显式管理这些冲突来增强分组相对策略优化方法。该框架整合了三个协同组件：1）用于引导早期生成阶段的稳定化中间奖励机制；2）实现精确贡献度分配的动态时间步重加权方案；3）一种基于奖励反馈学习原理设计的新型掩码传播算法，可在空间与时间维度同时隔离优化效应。实验表明，相较于原始分组相对策略优化基线，我们的方法在样本质量与目标对齐方面均取得显著提升，为视觉自回归模型实现了鲁棒且高效的优化。\n\n请按照以下格式返回：\n标题：VAR强化学习的正确实现：解决视觉自回归生成中的异步策略冲突\n摘要：视觉生成领域主要由三种范式主导：自回归模型、扩散模型以及视觉自回归模型。与自回归和扩散模型不同，视觉自回归模型在其生成步骤中处理异构输入结构，这导致了严重的异步策略冲突。该问题在强化学习场景中尤为突出，常引发训练过程不稳定与目标对齐欠佳。为解决此问题，我们提出一种新颖框架，通过显式管理这些冲突来增强分组相对策略优化方法。该框架整合了三个协同组件：1）用于引导早期生成阶段的稳定化中间奖励机制；2）实现精确贡献度分配的动态时间步重加权方案；3）一种基于奖励反馈学习原理设计的新型掩码传播算法，可在空间与时间维度同时隔离优化效应。实验表明，相较于原始分组相对策略优化基线，我们的方法在样本质量与目标对齐方面均取得显著提升，为视觉自回归模型实现了鲁棒且高效的优化。",
    "url": "https://huggingface.co/papers/2601.02256",
    "arxiv_url": "https://arxiv.org/abs/2601.02256"
  },
  {
    "title": "GARDO: Reinforcing Diffusion Models without Reward Hacking",
    "summary": "Fine-tuning diffusion models via online reinforcement learning (RL) has shown great potential for enhancing text-to-image alignment. However, since precisely specifying a ground-truth objective for visual tasks remains challenging, the models are often optimized using a proxy reward that only partially captures the true goal. This mismatch often leads to reward hacking, where proxy scores increase while real image quality deteriorates and generation diversity collapses. While common solutions add regularization against the reference policy to prevent reward hacking, they compromise sample efficiency and impede the exploration of novel, high-reward regions, as the reference policy is usually sub-optimal. To address the competing demands of sample efficiency, effective exploration, and mitigation of reward hacking, we propose Gated and Adaptive Regularization with Diversity-aware Optimization (GARDO), a versatile framework compatible with various RL algorithms. Our key insight is that regularization need not be applied universally; instead, it is highly effective to selectively penalize a subset of samples that exhibit high uncertainty. To address the exploration challenge, GARDO introduces an adaptive regularization mechanism wherein the reference model is periodically updated to match the capabilities of the online policy, ensuring a relevant regularization target. To address the mode collapse issue in RL, GARDO amplifies the rewards for high-quality samples that also exhibit high diversity, encouraging mode coverage without destabilizing the optimization process. Extensive experiments across diverse proxy rewards and hold-out unseen metrics consistently show that GARDO mitigates reward hacking and enhances generation diversity without sacrificing sample efficiency or exploration, highlighting its effectiveness and robustness.",
    "translation": "标题：GARDO：无需奖励破解的扩散模型强化方法\n\n摘要：通过在线强化学习对扩散模型进行微调已展现出提升文本-图像对齐能力的巨大潜力。然而，由于视觉任务中精确指定真实目标仍具挑战性，模型通常使用仅部分反映真实目标的代理奖励进行优化。这种不匹配常导致奖励破解现象，即代理分数上升的同时真实图像质量下降且生成多样性崩溃。常见解决方案通过添加针对参考策略的正则化来防止奖励破解，但由于参考策略通常并非最优，这些方法会牺牲样本效率并阻碍对新颖高奖励区域的探索。为平衡样本效率、有效探索和缓解奖励破解之间的竞争性需求，我们提出具有多样性感知优化的门控自适应正则化框架（GARDO），该通用框架可与多种强化学习算法兼容。我们的核心见解是：正则化无需普遍应用，而选择性地对高不确定性样本子集进行惩罚效果显著。针对探索挑战，GARDO引入自适应正则化机制，定期更新参考模型以匹配在线策略的能力，从而确保正则化目标的时效性。针对强化学习中的模式崩溃问题，GARDO通过放大兼具高质量与高多样性的样本奖励，在不破坏优化稳定性的前提下促进模式覆盖。在多种代理奖励和未见保留指标上的大量实验一致表明，GARDO能在不牺牲样本效率或探索能力的情况下有效缓解奖励破解并提升生成多样性，彰显了其效能与鲁棒性。",
    "url": "https://huggingface.co/papers/2512.24138",
    "arxiv_url": "https://arxiv.org/abs/2512.24138"
  },
  {
    "title": "VINO: A Unified Visual Generator with Interleaved OmniModal Context",
    "summary": "We present VINO, a unified visual generator that performs image and video generation and editing within a single framework. Instead of relying on task-specific models or independent modules for each modality, VINO uses a shared diffusion backbone that conditions on text, images and videos, enabling a broad range of visual creation and editing tasks under one model. Specifically, VINO couples a vision-language model (VLM) with a Multimodal Diffusion Transformer (MMDiT), where multimodal inputs are encoded as interleaved conditioning tokens, and then used to guide the diffusion process. This design supports multi-reference grounding, long-form instruction following, and coherent identity preservation across static and dynamic content, while avoiding modality-specific architectural components. To train such a unified system, we introduce a multi-stage training pipeline that progressively expands a video generation base model into a unified, multi-task generator capable of both image and video input and output. Across diverse generation and editing benchmarks, VINO demonstrates strong visual quality, faithful instruction following, improved reference and attribute preservation, and more controllable multi-identity edits. Our results highlight a practical path toward scalable unified visual generation, and the promise of interleaved, in-context computation as a foundation for general-purpose visual creation.",
    "translation": "标题：VINO：基于交错式全模态上下文的统一视觉生成器\n\n摘要：本文提出VINO，一个在单一框架内实现图像与视频生成及编辑的统一视觉生成器。不同于依赖针对特定任务的模型或为各模态设计独立模块的传统方案，VINO采用共享的扩散模型主干，能够同时接受文本、图像和视频作为条件输入，从而在单一模型下支持广泛的视觉创作与编辑任务。具体而言，VINO将视觉语言模型（VLM）与多模态扩散变换器（MMDiT）相结合，其中多模态输入被编码为交错排列的条件标记，进而引导扩散生成过程。该设计支持多参考对象关联、长指令序列跟随以及静态与动态内容间的连贯身份保持，同时避免了针对特定模态的专用结构组件。为训练这一统一系统，我们提出多阶段训练流程，逐步将基础视频生成模型扩展为能够同时处理图像与视频输入输出的统一多任务生成器。在多样化的生成与编辑基准测试中，VINO展现出卓越的视觉质量、精准的指令跟随能力、改进的参考对象与属性保持效果，以及更可控的多身份编辑性能。我们的研究成果为可扩展的统一视觉生成提供了可行路径，并揭示了交错式上下文计算作为通用视觉创作基础技术的潜力。",
    "url": "https://huggingface.co/papers/2601.02358",
    "arxiv_url": "https://arxiv.org/abs/2601.02358"
  },
  {
    "title": "InfiniteVGGT: Visual Geometry Grounded Transformer for Endless Streams",
    "summary": "The grand vision of enabling persistent, large-scale 3D visual geometry understanding is shackled by the irreconcilable demands of scalability and long-term stability. While offline models like VGGT achieve inspiring geometry capability, their batch-based nature renders them irrelevant for live systems. Streaming architectures, though the intended solution for live operation, have proven inadequate. Existing methods either fail to support truly infinite-horizon inputs or suffer from catastrophic drift over long sequences. We shatter this long-standing dilemma with InfiniteVGGT, a causal visual geometry transformer that operationalizes the concept of a rolling memory through a bounded yet adaptive and perpetually expressive KV cache. Capitalizing on this, we devise a training-free, attention-agnostic pruning strategy that intelligently discards obsolete information, effectively ``rolling'' the memory forward with each new frame. Fully compatible with FlashAttention, InfiniteVGGT finally alleviates the compromise, enabling infinite-horizon streaming while outperforming existing streaming methods in long-term stability. The ultimate test for such a system is its performance over a truly infinite horizon, a capability that has been impossible to rigorously validate due to the lack of extremely long-term, continuous benchmarks. To address this critical gap, we introduce the Long3D benchmark, which, for the first time, enables a rigorous evaluation of continuous 3D geometry estimation on sequences about 10,000 frames. This provides the definitive evaluation platform for future research in long-term 3D geometry understanding. Code is available at: https://github.com/AutoLab-SAI-SJTU/InfiniteVGGT",
    "translation": "标题：InfiniteVGGT：面向无限流数据的视觉几何基础Transformer\n\n摘要：实现持久、大规模三维视觉几何理解的宏伟愿景，一直受制于可扩展性与长期稳定性之间难以调和的矛盾。尽管VGGT等离线模型展现出卓越的几何理解能力，但其批处理模式使其无法适用于实时系统。而专为实时操作设计的流式架构，现有方法亦被证明存在不足：它们要么无法支持真正无限时长的输入，要么在长序列处理中遭受灾难性的性能漂移。本文提出的InfiniteVGGT彻底打破了这一长期困境。该模型是一种因果视觉几何Transformer，通过构建一个有界、自适应且持续保持表达力的KV缓存，实现了“滚动记忆”的操作化。基于此，我们设计了一种无需训练、与注意力机制无关的剪枝策略，能够智能地丢弃过时信息，随着每一帧新数据的输入有效“滚动”更新记忆。InfiniteVGGT完全兼容FlashAttention，最终消除了传统方案中的性能妥协，在支持无限时长流式处理的同时，其长期稳定性超越了现有所有流式方法。此类系统的终极考验在于其在真正无限时长上的性能，而由于极度缺乏超长时、连续的基准测试数据，该能力一直无法得到严格验证。为填补这一关键空白，我们首次提出了Long3D基准测试集，该数据集支持对约10,000帧长度的序列进行连续三维几何估计的严格评估，从而为未来长期三维几何理解研究提供了权威性的评估平台。代码已开源：https://github.com/AutoLab-SAI-SJTU/InfiniteVGGT",
    "url": "https://huggingface.co/papers/2601.02281",
    "arxiv_url": "https://arxiv.org/abs/2601.02281"
  },
  {
    "title": "Recursive Language Models",
    "summary": "We study allowing large language models (LLMs) to process arbitrarily long prompts through the lens of inference-time scaling. We propose Recursive Language Models (RLMs), a general inference strategy that treats long prompts as part of an external environment and allows the LLM to programmatically examine, decompose, and recursively call itself over snippets of the prompt. We find that RLMs successfully handle inputs up to two orders of magnitude beyond model context windows and, even for shorter prompts, dramatically outperform the quality of base LLMs and common long-context scaffolds across four diverse long-context tasks, while having comparable (or cheaper) cost per query.",
    "translation": "标题：递归语言模型\n\n摘要：本文从推理时扩展的视角出发，研究如何使大型语言模型能够处理任意长度的提示文本。我们提出递归语言模型——一种通用的推理策略，将长提示文本视为外部环境的一部分，允许大型语言模型以编程方式对提示片段进行检测、分解和递归调用。实验表明，递归语言模型能够成功处理超出模型上下文窗口两个数量级的输入；即使在较短提示任务中，其在四项不同的长上下文任务上也显著优于基础大型语言模型及常见的长上下文框架方法，同时保持可比（或更低）的单次查询成本。",
    "url": "https://huggingface.co/papers/2512.24601",
    "arxiv_url": "https://arxiv.org/abs/2512.24601"
  },
  {
    "title": "Falcon-H1R: Pushing the Reasoning Frontiers with a Hybrid Model for Efficient Test-Time Scaling",
    "summary": "This work introduces Falcon-H1R, a 7B-parameter reasoning-optimized model that establishes the feasibility of achieving competitive reasoning performance with small language models (SLMs). Falcon-H1R stands out for its parameter efficiency, consistently matching or outperforming SOTA reasoning models that are 2times to 7times larger across a variety of reasoning-intensive benchmarks. These results underscore the importance of careful data curation and targeted training strategies (via both efficient SFT and RL scaling) in delivering significant performance gains without increasing model size. Furthermore, Falcon-H1R advances the 3D limits of reasoning efficiency by combining faster inference (through its hybrid-parallel architecture design), token efficiency, and higher accuracy. This unique blend makes Falcon-H1R-7B a practical backbone for scaling advanced reasoning systems, particularly in scenarios requiring extensive chain-of-thoughts generation and parallel test-time scaling. Leveraging the recently introduced DeepConf approach, Falcon-H1R achieves state-of-the-art test-time scaling efficiency, offering substantial improvements in both accuracy and computational cost. As a result, Falcon-H1R demonstrates that compact models, through targeted model training and architectural choices, can deliver robust and scalable reasoning performance.",
    "translation": "标题：Falcon-H1R：通过混合模型推动推理前沿，实现高效测试时扩展\n\n摘要：本研究介绍了Falcon-H1R，一个拥有70亿参数、专为推理优化的模型，它证明了小型语言模型（SLMs）也能实现具有竞争力的推理性能。Falcon-H1R以其参数效率脱颖而出，在多种推理密集型基准测试中，其性能持续匹配甚至超越参数量为其2至7倍的当前最优推理模型。这些结果凸显了精细的数据筛选和针对性训练策略（通过高效的监督微调和强化学习扩展）的重要性，它们能在不增加模型规模的情况下带来显著的性能提升。此外，Falcon-H1R通过结合更快的推理速度（得益于其混合并行架构设计）、更高的令牌效率和更高的准确性，推进了推理效率的“三维”极限。这种独特的组合使Falcon-H1R-7B成为扩展高级推理系统的实用骨干模型，尤其适用于需要大量思维链生成和并行测试时扩展的场景。借助近期提出的DeepConf方法，Falcon-H1R实现了最先进的测试时扩展效率，在准确性和计算成本方面均有显著改善。因此，Falcon-H1R证明，通过针对性的模型训练和架构选择，紧凑模型能够提供强大且可扩展的推理性能。",
    "url": "https://huggingface.co/papers/2601.02346",
    "arxiv_url": "https://arxiv.org/abs/2601.02346"
  },
  {
    "title": "SimpleMem: Efficient Lifelong Memory for LLM Agents",
    "summary": "To support reliable long-term interaction in complex environments, LLM agents require memory systems that efficiently manage historical experiences. Existing approaches either retain full interaction histories via passive context extension, leading to substantial redundancy, or rely on iterative reasoning to filter noise, incurring high token costs. To address this challenge, we introduce SimpleMem, an efficient memory framework based on semantic lossless compression. We propose a three-stage pipeline designed to maximize information density and token utilization: (1) Semantic Structured Compression, which applies entropy-aware filtering to distill unstructured interactions into compact, multi-view indexed memory units; (2) Recursive Memory Consolidation, an asynchronous process that integrates related units into higher-level abstract representations to reduce redundancy; and (3) Adaptive Query-Aware Retrieval, which dynamically adjusts retrieval scope based on query complexity to construct precise context efficiently. Experiments on benchmark datasets show that our method consistently outperforms baseline approaches in accuracy, retrieval efficiency, and inference cost, achieving an average F1 improvement of 26.4% while reducing inference-time token consumption by up to 30-fold, demonstrating a superior balance between performance and efficiency. Code is available at https://github.com/aiming-lab/SimpleMem.",
    "translation": "标题：SimpleMem：面向大语言模型智能体的高效终身记忆框架\n\n摘要：为支持复杂环境中可靠的长期交互，大语言模型智能体需要能高效管理历史经验的记忆系统。现有方法要么通过被动扩展上下文保留完整交互历史，导致显著冗余；要么依赖迭代推理过滤噪声，产生高昂的令牌成本。为应对这一挑战，我们提出SimpleMem——一种基于语义无损压缩的高效记忆框架。我们设计了一个三阶段流程以最大化信息密度与令牌利用率：（1）语义结构化压缩：通过熵感知过滤将非结构化交互提炼为紧凑的多视图索引记忆单元；（2）递归记忆整合：通过异步过程将相关单元融合为更高层次的抽象表征以降低冗余；（3）自适应查询感知检索：根据查询复杂度动态调整检索范围，高效构建精准上下文。在基准数据集上的实验表明，本方法在准确性、检索效率与推理成本方面均优于基线方法，平均F1值提升26.4%，推理阶段令牌消耗最高降低至三十分之一，实现了性能与效率的卓越平衡。代码已开源：https://github.com/aiming-lab/SimpleMem。",
    "url": "https://huggingface.co/papers/2601.02553",
    "arxiv_url": "https://arxiv.org/abs/2601.02553"
  },
  {
    "title": "Talk2Move: Reinforcement Learning for Text-Instructed Object-Level Geometric Transformation in Scenes",
    "summary": "We introduce Talk2Move, a reinforcement learning (RL) based diffusion framework for text-instructed spatial transformation of objects within scenes. Spatially manipulating objects in a scene through natural language poses a challenge for multimodal generation systems. While existing text-based manipulation methods can adjust appearance or style, they struggle to perform object-level geometric transformations-such as translating, rotating, or resizing objects-due to scarce paired supervision and pixel-level optimization limits. Talk2Move employs Group Relative Policy Optimization (GRPO) to explore geometric actions through diverse rollouts generated from input images and lightweight textual variations, removing the need for costly paired data. A spatial reward guided model aligns geometric transformations with linguistic description, while off-policy step evaluation and active step sampling improve learning efficiency by focusing on informative transformation stages. Furthermore, we design object-centric spatial rewards that evaluate displacement, rotation, and scaling behaviors directly, enabling interpretable and coherent transformations. Experiments on curated benchmarks demonstrate that Talk2Move achieves precise, consistent, and semantically faithful object transformations, outperforming existing text-guided editing approaches in both spatial accuracy and scene coherence.",
    "translation": "标题：Talk2Move：基于强化学习的场景中文本指令对象级几何变换方法\n\n摘要：本文提出Talk2Move，一种基于强化学习（RL）的扩散框架，用于实现场景中对象的文本指令空间变换。通过自然语言对场景中的对象进行空间操控是多模态生成系统面临的挑战。现有基于文本的编辑方法虽能调整外观或风格，但由于缺乏成对的监督数据及像素级优化的局限性，难以实现对象级的几何变换（如平移、旋转或缩放）。Talk2Move采用组相对策略优化（GRPO），通过输入图像与轻量级文本变体生成多样化轨迹来探索几何动作，从而避免了对高成本配对数据的依赖。空间奖励引导模型将几何变换与语言描述对齐，同时离轨步长评估与主动步长采样通过聚焦于信息丰富的变换阶段提升了学习效率。此外，我们设计了以对象为中心的空间奖励机制，直接评估位移、旋转和缩放行为，实现了可解释且连贯的变换效果。在精选基准测试上的实验表明，Talk2Move能够实现精确、一致且语义保真的对象变换，在空间准确性与场景连贯性方面均优于现有文本引导编辑方法。",
    "url": "https://huggingface.co/papers/2601.02356",
    "arxiv_url": "https://arxiv.org/abs/2601.02356"
  },
  {
    "title": "Confidence Estimation for LLMs in Multi-turn Interactions",
    "summary": "While confidence estimation is a promising direction for mitigating hallucinations in Large Language Models (LLMs), current research dominantly focuses on single-turn settings. The dynamics of model confidence in multi-turn conversations, where context accumulates and ambiguity is progressively resolved, remain largely unexplored. Reliable confidence estimation in multi-turn settings is critical for many downstream applications, such as autonomous agents and human-in-the-loop systems. This work presents the first systematic study of confidence estimation in multi-turn interactions, establishing a formal evaluation framework grounded in two key desiderata: per-turn calibration and monotonicity of confidence as more information becomes available. To facilitate this, we introduce novel metrics, including a length-normalized Expected Calibration Error (InfoECE), and a new \"Hinter-Guesser\" paradigm for generating controlled evaluation datasets. Our experiments reveal that widely-used confidence techniques struggle with calibration and monotonicity in multi-turn dialogues. We propose P(Sufficient), a logit-based probe that achieves comparatively better performance, although the task remains far from solved. Our work provides a foundational methodology for developing more reliable and trustworthy conversational agents.",
    "translation": "标题：多轮交互中大语言模型的置信度估计\n\n摘要：尽管置信度估计是缓解大语言模型幻觉现象的重要方向，但现有研究主要集中于单轮交互场景。在多轮对话中，随着上下文信息累积与歧义逐步消解，模型置信度的动态变化机制尚未得到充分探索。可靠的置信度估计对自主智能体、人机协同系统等下游应用至关重要。本研究首次系统性地探讨多轮交互中的置信度估计问题，建立了基于双重核心需求的规范化评估框架：单轮校准性及信息增量下的置信度单调性。为此，我们提出了创新性评估指标（包括长度归一化的预期校准误差InfoECE），并设计了\"提示者-猜测者\"范式以构建受控评估数据集。实验表明，当前主流置信度估计技术在多轮对话中普遍存在校准失效与单调性缺失问题。我们提出的基于逻辑值的探测方法P(Sufficient)取得了相对更优的性能，但该任务仍远未完全解决。本研究为开发更可靠、可信的对话智能体奠定了方法论基础。",
    "url": "https://huggingface.co/papers/2601.02179",
    "arxiv_url": "https://arxiv.org/abs/2601.02179"
  },
  {
    "title": "KV-Embedding: Training-free Text Embedding via Internal KV Re-routing in Decoder-only LLMs",
    "summary": "While LLMs are powerful embedding backbones, their application in training-free settings faces two structural challenges: causal attention restricts early tokens from accessing subsequent context, and the next-token prediction objective biases representations toward generation rather than semantic compression. To address these limitations, we propose KV-Embedding, a framework that activates the latent representation power of frozen LLMs. Our method leverages the observation that the key-value (KV) states of the final token at each layer encode a compressed view of the sequence. By re-routing these states as a prepended prefix, we enable all tokens to access sequence-level context within a single forward pass. To ensure model-agnostic applicability, we introduce an automated layer selection strategy based on intrinsic dimensionality. Evaluations on MTEB across Qwen, Mistral, and Llama backbones show that KV-Embedding outperforms existing training-free baselines by up to 10%, while maintaining robust performance on sequences up to 4,096 tokens. These results demonstrate that internal state manipulation offers an efficient alternative to input modification, and we hope this work encourages further exploration of LLM internals for representation learning.",
    "translation": "标题：KV-Embedding：基于仅解码器大语言模型内部KV重路由的无训练文本嵌入方法\n\n摘要：尽管大语言模型（LLMs）是强大的嵌入骨干网络，但其在无训练场景下的应用面临两大结构挑战：因果注意力机制限制了早期令牌访问后续上下文的能力，而下一令牌预测目标使表示偏向生成任务而非语义压缩。为克服这些局限，本文提出KV-Embedding框架，旨在激活冻结大语言模型的潜在表示能力。该方法基于以下观察：每个网络层中最终令牌的键值（KV）状态编码了序列的压缩视图。通过将这些状态重路由为前置前缀，我们使所有令牌能在单次前向传播中访问序列级上下文。为确保模型无关的适用性，我们提出基于本征维度的自动化层级选择策略。在Qwen、Mistral和Llama骨干网络上进行的MTEB评估表明，KV-Embedding相比现有无训练基线方法性能提升最高达10%，同时在长达4,096个令牌的序列上保持稳健性能。这些结果证明，内部状态操作为输入修改提供了高效替代方案，我们期望此项工作能推动基于大语言模型内部机制的表示学习研究。",
    "url": "https://huggingface.co/papers/2601.01046",
    "arxiv_url": "https://arxiv.org/abs/2601.01046"
  },
  {
    "title": "CPPO: Contrastive Perception for Vision Language Policy Optimization",
    "summary": "We introduce CPPO, a Contrastive Perception Policy Optimization method for finetuning vision-language models (VLMs). While reinforcement learning (RL) has advanced reasoning in language models, extending it to multimodal reasoning requires improving both the perception and reasoning aspects. Prior works tackle this challenge mainly with explicit perception rewards, but disentangling perception tokens from reasoning tokens is difficult, requiring extra LLMs, ground-truth data, forced separation of perception from reasoning by policy model, or applying rewards indiscriminately to all output tokens. CPPO addresses this problem by detecting perception tokens via entropy shifts in the model outputs under perturbed input images. CPPO then extends the RL objective function with a Contrastive Perception Loss (CPL) that enforces consistency under information-preserving perturbations and sensitivity under information-removing ones. Experiments show that CPPO surpasses previous perception-rewarding methods, while avoiding extra models, making training more efficient and scalable.",
    "translation": "标题：CPPO：面向视觉语言策略优化的对比感知方法\n\n摘要：本文提出CPPO（对比感知策略优化方法），一种用于微调视觉语言模型（VLM）的新方法。尽管强化学习（RL）已显著提升了语言模型的推理能力，但将其扩展至多模态推理需要同时改进感知与推理两个维度。现有研究主要通过显式感知奖励应对这一挑战，但将感知标记与推理标记有效分离存在困难——往往需要额外的大语言模型、真实标注数据、强制策略模型分离感知与推理，或对全部输出标记 indiscriminately 施加奖励。CPPO通过分析输入图像受扰动时模型输出的熵值变化来检测感知标记，进而提出对比感知损失（CPL）以扩展RL目标函数：该方法在信息保留型扰动下强化输出一致性，在信息消除型扰动下增强敏感性。实验表明，CPPO在超越现有感知奖励方法的同时，无需引入额外模型，显著提升了训练效率与可扩展性。",
    "url": "https://huggingface.co/papers/2601.00501",
    "arxiv_url": "https://arxiv.org/abs/2601.00501"
  },
  {
    "title": "DiffProxy: Multi-View Human Mesh Recovery via Diffusion-Generated Dense Proxies",
    "summary": "Human mesh recovery from multi-view images faces a fundamental challenge: real-world datasets contain imperfect ground-truth annotations that bias the models' training, while synthetic data with precise supervision suffers from domain gap. In this paper, we propose DiffProxy, a novel framework that generates multi-view consistent human proxies for mesh recovery. Central to DiffProxy is leveraging the diffusion-based generative priors to bridge the synthetic training and real-world generalization. Its key innovations include: (1) a multi-conditional mechanism for generating multi-view consistent, pixel-aligned human proxies; (2) a hand refinement module that incorporates flexible visual prompts to enhance local details; and (3) an uncertainty-aware test-time scaling method that increases robustness to challenging cases during optimization. These designs ensure that the mesh recovery process effectively benefits from the precise synthetic ground truth and generative advantages of the diffusion-based pipeline. Trained entirely on synthetic data, DiffProxy achieves state-of-the-art performance across five real-world benchmarks, demonstrating strong zero-shot generalization particularly on challenging scenarios with occlusions and partial views. Project page: https://wrk226.github.io/DiffProxy.html",
    "translation": "标题：DiffProxy：基于扩散生成密集代理的多视角人体网格重建方法\n\n摘要：多视角图像的人体网格重建面临一个根本性挑战：现实世界数据集包含不完美的真实标注，会导致模型训练产生偏差；而具有精确标注的合成数据则存在领域差异问题。本文提出DiffProxy，一种通过生成多视角一致的人体代理进行网格重建的创新框架。该框架的核心在于利用基于扩散模型的生成先验知识，弥合合成数据训练与真实场景泛化之间的差距。其主要创新包括：（1）采用多条件生成机制，实现多视角一致且像素对齐的人体代理生成；（2）设计手部精细化模块，通过灵活的可视化提示增强局部细节；（3）提出不确定性感知的测试时缩放方法，在优化过程中提升对挑战性场景的鲁棒性。这些设计确保网格重建过程能有效利用精确的合成真实标注，并充分发挥基于扩散流程的生成优势。DiffProxy完全在合成数据上训练，在五个真实世界基准测试中均达到最先进性能，尤其在存在遮挡和局部视角的挑战性场景中展现出强大的零样本泛化能力。项目页面：https://wrk226.github.io/DiffProxy.html",
    "url": "https://huggingface.co/papers/2601.02267",
    "arxiv_url": "https://arxiv.org/abs/2601.02267"
  },
  {
    "title": "COMPASS: A Framework for Evaluating Organization-Specific Policy Alignment in LLMs",
    "summary": "As large language models are deployed in high-stakes enterprise applications, from healthcare to finance, ensuring adherence to organization-specific policies has become essential. Yet existing safety evaluations focus exclusively on universal harms. We present COMPASS (Company/Organization Policy Alignment Assessment), the first systematic framework for evaluating whether LLMs comply with organizational allowlist and denylist policies. We apply COMPASS to eight diverse industry scenarios, generating and validating 5,920 queries that test both routine compliance and adversarial robustness through strategically designed edge cases. Evaluating seven state-of-the-art models, we uncover a fundamental asymmetry: models reliably handle legitimate requests (>95% accuracy) but catastrophically fail at enforcing prohibitions, refusing only 13-40% of adversarial denylist violations. These results demonstrate that current LLMs lack the robustness required for policy-critical deployments, establishing COMPASS as an essential evaluation framework for organizational AI safety.",
    "translation": "标题：COMPASS：评估大型语言模型中组织特定政策对齐性的框架\n\n摘要：随着大型语言模型在从医疗保健到金融等高风险企业应用中的部署，确保其遵守组织特定政策已变得至关重要。然而现有的安全性评估仅聚焦于通用风险。本文提出COMPASS（公司/组织政策对齐评估）框架，这是首个系统化评估大型语言模型是否符合组织允许清单与禁止清单政策的框架。我们将COMPASS应用于八个不同行业场景，通过战略设计的边缘案例生成并验证了5,920条查询，同时测试常规合规性与对抗鲁棒性。在对七个前沿模型进行评估后，我们发现了一个根本性不对称现象：模型能可靠处理合法请求（准确率>95%），但在执行禁令时出现严重失效，仅能拒绝13-40%的对抗性禁止清单违规请求。这些结果表明当前大型语言模型缺乏政策关键型部署所需的鲁棒性，从而确立了COMPASS作为组织人工智能安全评估的核心框架地位。",
    "url": "https://huggingface.co/papers/2601.01836",
    "arxiv_url": "https://arxiv.org/abs/2601.01836"
  },
  {
    "title": "SWE-Lego: Pushing the Limits of Supervised Fine-tuning for Software Issue Resolving",
    "summary": "We present SWE-Lego, a supervised fine-tuning (SFT) recipe designed to achieve state-ofthe-art performance in software engineering (SWE) issue resolving. In contrast to prevalent methods that rely on complex training paradigms (e.g., mid-training, SFT, reinforcement learning, and their combinations), we explore how to push the limits of a lightweight SFT-only approach for SWE tasks. SWE-Lego comprises three core building blocks, with key findings summarized as follows: 1) the SWE-Lego dataset, a collection of 32k highquality task instances and 18k validated trajectories, combining real and synthetic data to complement each other in both quality and quantity; 2) a refined SFT procedure with error masking and a difficulty-based curriculum, which demonstrably improves action quality and overall performance. Empirical results show that with these two building bricks alone,the SFT can push SWE-Lego models to state-of-the-art performance among open-source models of comparable size on SWE-bench Verified: SWE-Lego-Qwen3-8B reaches 42.2%, and SWE-Lego-Qwen3-32B attains 52.6%. 3) We further evaluate and improve test-time scaling (TTS) built upon the SFT foundation. Based on a well-trained verifier, SWE-Lego models can be significantly boosted--for example, 42.2% to 49.6% and 52.6% to 58.8% under TTS@16 for the 8B and 32B models, respectively.",
    "translation": "标题：SWE-Lego：探索监督微调在软件问题解决中的性能极限\n\n摘要：本文提出SWE-Lego——一种专为软件工程问题解决任务设计的监督微调方案，旨在实现最先进的性能表现。与当前依赖复杂训练范式（如中期训练、监督微调、强化学习及其组合）的主流方法不同，本研究探索如何突破轻量级纯监督微调方法在软件工程任务中的性能极限。SWE-Lego包含三个核心构建模块，关键发现总结如下：1）SWE-Lego数据集包含3.2万个高质量任务实例与1.8万条已验证执行轨迹，融合真实数据与合成数据以在质量与数量上形成互补；2）采用错误掩码与难度分级课程的精细化监督微调流程，可显著提升动作质量与整体性能。实验结果表明，仅通过这两个构建模块，监督微调即可使SWE-Lego模型在同等规模的开源模型中达到SWE-bench Verified基准的最优性能：SWE-Lego-Qwen3-8B达到42.2%，SWE-Lego-Qwen3-32B达到52.6%。3）我们在监督微调基础上进一步评估并改进了测试时扩展方法。基于训练完备的验证器，SWE-Lego模型性能可获得显著提升——在TTS@16设置下，8B模型从42.2%提升至49.6%，32B模型从52.6%提升至58.8%。",
    "url": "https://huggingface.co/papers/2601.01426",
    "arxiv_url": "https://arxiv.org/abs/2601.01426"
  },
  {
    "title": "Toward Stable Semi-Supervised Remote Sensing Segmentation via Co-Guidance and Co-Fusion",
    "summary": "Semi-supervised remote sensing (RS) image semantic segmentation offers a promising solution to alleviate the burden of exhaustive annotation, yet it fundamentally struggles with pseudo-label drift, a phenomenon where confirmation bias leads to the accumulation of errors during training. In this work, we propose Co2S, a stable semi-supervised RS segmentation framework that synergistically fuses priors from vision-language models and self-supervised models. Specifically, we construct a heterogeneous dual-student architecture comprising two distinct ViT-based vision foundation models initialized with pretrained CLIP and DINOv3 to mitigate error accumulation and pseudo-label drift. To effectively incorporate these distinct priors, an explicit-implicit semantic co-guidance mechanism is introduced that utilizes text embeddings and learnable queries to provide explicit and implicit class-level guidance, respectively, thereby jointly enhancing semantic consistency. Furthermore, a global-local feature collaborative fusion strategy is developed to effectively fuse the global contextual information captured by CLIP with the local details produced by DINOv3, enabling the model to generate highly precise segmentation results. Extensive experiments on six popular datasets demonstrate the superiority of the proposed method, which consistently achieves leading performance across various partition protocols and diverse scenarios. Project page is available at https://xavierjiezou.github.io/Co2S/.",
    "translation": "标题：迈向稳定半监督遥感分割：基于协同引导与协同融合的方法\n\n摘要：半监督遥感图像语义分割为缓解详尽标注负担提供了有前景的解决方案，但其本质上受伪标签漂移问题困扰——这种因确认偏差导致训练过程中误差累积的现象。本研究提出Co2S框架，一种通过协同融合视觉语言模型与自监督模型先验知识的稳定半监督遥感分割方法。具体而言，我们构建了异构双学生架构，包含两个基于ViT的视觉基础模型，分别采用预训练的CLIP和DINOv3初始化，以抑制误差累积和伪标签漂移。为有效整合这些异构先验知识，我们引入显式-隐式语义协同引导机制：利用文本嵌入提供显式类别级引导，同时通过可学习查询向量实现隐式类别引导，共同增强语义一致性。此外，设计了全局-局部特征协同融合策略，将CLIP捕获的全局上下文信息与DINOv3提取的局部细节有效融合，使模型能够生成高精度分割结果。在六个主流数据集上的大量实验表明，该方法在不同数据划分协议和多样场景中均取得领先性能，验证了其优越性。项目页面详见：https://xavierjiezou.github.io/Co2S/。",
    "url": "https://huggingface.co/papers/2512.23035",
    "arxiv_url": "https://arxiv.org/abs/2512.23035"
  },
  {
    "title": "OpenNovelty: An LLM-powered Agentic System for Verifiable Scholarly Novelty Assessment",
    "summary": "Evaluating novelty is critical yet challenging in peer review, as reviewers must assess submissions against a vast, rapidly evolving literature. This report presents OpenNovelty, an LLM-powered agentic system for transparent, evidence-based novelty analysis. The system operates through four phases: (1) extracting the core task and contribution claims to generate retrieval queries; (2) retrieving relevant prior work based on extracted queries via semantic search engine; (3) constructing a hierarchical taxonomy of core-task-related work and performing contribution-level full-text comparisons against each contribution; and (4) synthesizing all analyses into a structured novelty report with explicit citations and evidence snippets. Unlike naive LLM-based approaches, OpenNovelty grounds all assessments in retrieved real papers, ensuring verifiable judgments. We deploy our system on 500+ ICLR 2026 submissions with all reports publicly available on our website, and preliminary analysis suggests it can identify relevant prior work, including closely related papers that authors may overlook. OpenNovelty aims to empower the research community with a scalable tool that promotes fair, consistent, and evidence-backed peer review.",
    "translation": "标题：OpenNovelty：一种基于大语言模型的可验证学术新颖性评估智能体系统\n\n摘要：在同行评审中，评估新颖性至关重要但也极具挑战，因为评审人必须在庞大且快速发展的文献背景下对投稿进行评判。本报告介绍了OpenNovelty，这是一个基于大语言模型的智能体系统，旨在实现透明、基于证据的新颖性分析。该系统通过四个阶段运行：（1）提取核心任务与贡献主张以生成检索查询；（2）通过语义搜索引擎基于提取的查询检索相关先前工作；（3）构建与核心任务相关工作的层次化分类体系，并在全文层面逐项对比各贡献点；（4）将所有分析整合为一份结构化的新颖性报告，其中包含明确的引用和证据片段。与简单基于大语言模型的方法不同，OpenNovelty将所有评估建立在检索到的真实论文基础上，确保判断可验证。我们在500多篇ICLR 2026投稿上部署了本系统，所有报告均公开于项目网站，初步分析表明该系统能够识别相关先前工作，包括作者可能忽略的密切关联论文。OpenNovelty旨在为研究社区提供一个可扩展的工具，以促进公平、一致且基于证据的同行评审。",
    "url": "https://huggingface.co/papers/2601.01576",
    "arxiv_url": "https://arxiv.org/abs/2601.01576"
  },
  {
    "title": "Selective Imperfection as a Generative Framework for Analysis, Creativity and Discovery",
    "summary": "We introduce materiomusic as a generative framework linking the hierarchical structures of matter with the compositional logic of music. Across proteins, spider webs and flame dynamics, vibrational and architectural principles recur as tonal hierarchies, harmonic progressions, and long-range musical form. Using reversible mappings, from molecular spectra to musical tones and from three-dimensional networks to playable instruments, we show how sound functions as a scientific probe, an epistemic inversion where listening becomes a mode of seeing and musical composition becomes a blueprint for matter. These mappings excavate deep time: patterns originating in femtosecond molecular vibrations or billion-year evolutionary histories become audible. We posit that novelty in science and art emerges when constraints cannot be satisfied within existing degrees of freedom, forcing expansion of the space of viable configurations. Selective imperfection provides the mechanism restoring balance between coherence and adaptability. Quantitative support comes from exhaustive enumeration of all 2^12 musical scales, revealing that culturally significant systems cluster in a mid-entropy, mid-defect corridor, directly paralleling the Hall-Petch optimum where intermediate defect densities maximize material strength. Iterating these mappings creates productive collisions between human creativity and physics, generating new information as musical structures encounter evolutionary constraints. We show how swarm-based AI models compose music exhibiting human-like structural signatures such as small-world connectivity, modular integration, long-range coherence, suggesting a route beyond interpolation toward invention. We show that science and art are generative acts of world-building under constraint, with vibration as a shared grammar organizing structure across scales.",
    "translation": "标题：选择性不完美：作为分析、创造与发现的生成框架\n\n摘要：本文提出“物质音乐学”这一生成框架，将物质的层级结构与音乐的创作逻辑相联结。从蛋白质、蜘蛛网到火焰动力学，振动与建筑原理以音调层级、和声进行与宏观音乐形式反复呈现。通过可逆映射（从分子光谱到乐音，从三维网络到可演奏乐器），我们揭示声音如何作为科学探针，实现认知反转——聆听成为观察模式，音乐创作转化为物质蓝图。这些映射发掘深层时间：源自飞秒分子振动或亿万年演化史的模式变得可闻。我们主张，当约束无法在现有自由度内满足时，科学与艺术的新颖性便随之涌现，这迫使可行构型空间发生扩展。选择性不完美提供了恢复连贯性与适应性平衡的机制。量化支持来自对全部2^12种音阶的穷举枚举，结果表明具有文化意义的音乐体系聚集于中等熵值与中等缺陷的通道中，这与霍尔-佩奇最优区间直接对应——中等缺陷密度使材料强度最大化。迭代这些映射在人类创造力与物理规律之间创造生产性碰撞，当音乐结构遭遇演化约束时即生成新信息。我们展示基于群体智能的AI模型如何创作出具有类人结构特征的音乐，如小世界连接性、模块化整合、长程连贯性，这为超越插值迈向发明指明路径。研究表明，科学与艺术皆是约束下的世界建构生成行为，而振动正是跨尺度组织结构的共享语法。",
    "url": "https://huggingface.co/papers/2601.00863",
    "arxiv_url": "https://arxiv.org/abs/2601.00863"
  },
  {
    "title": "IMA++: ISIC Archive Multi-Annotator Dermoscopic Skin Lesion Segmentation Dataset",
    "summary": "Multi-annotator medical image segmentation is an important research problem, but requires annotated datasets that are expensive to collect. Dermoscopic skin lesion imaging allows human experts and AI systems to observe morphological structures otherwise not discernable from regular clinical photographs. However, currently there are no large-scale publicly available multi-annotator skin lesion segmentation (SLS) datasets with annotator-labels for dermoscopic skin lesion imaging. We introduce ISIC MultiAnnot++, a large public multi-annotator skin lesion segmentation dataset for images from the ISIC Archive. The final dataset contains 17,684 segmentation masks spanning 14,967 dermoscopic images, where 2,394 dermoscopic images have 2-5 segmentations per image, making it the largest publicly available SLS dataset. Further, metadata about the segmentation, including the annotators' skill level and segmentation tool, is included, enabling research on topics such as annotator-specific preference modeling for segmentation and annotator metadata analysis. We provide an analysis on the characteristics of this dataset, curated data partitions, and consensus segmentation masks.",
    "translation": "标题：IMA++：ISIC档案多标注者皮肤镜皮肤病变分割数据集\n\n摘要：多标注者医学图像分割是一个重要的研究课题，但需要耗费高昂成本收集标注数据集。皮肤镜皮肤病变成像技术使得人类专家和人工智能系统能够观察到常规临床照片无法辨别的形态学结构。然而，目前尚无大规模公开可用的、包含标注者标签的皮肤镜皮肤病变分割多标注者数据集。本研究推出ISIC MultiAnnot++，这是一个基于ISIC档案图像的大型公开多标注者皮肤病变分割数据集。该数据集最终包含覆盖14,967张皮肤镜图像的17,684个分割掩码，其中2,394张皮肤镜图像每张包含2-5个独立分割标注，使其成为当前最大的公开皮肤病变分割数据集。此外，数据集还包含关于分割的元数据（包括标注者技能水平和分割工具），支持开展标注者特异性分割偏好建模、标注者元数据分析等方向的研究。我们对该数据集的特征进行了分析，提供了经过筛选的数据分区和共识分割掩码。",
    "url": "https://huggingface.co/papers/2512.21472",
    "arxiv_url": "https://arxiv.org/abs/2512.21472"
  },
  {
    "title": "Prithvi-Complimentary Adaptive Fusion Encoder (CAFE): unlocking full-potential for flood inundation mapping",
    "summary": "Geo-Foundation Models (GFMs), have proven effective in diverse downstream applications, including semantic segmentation, classification, and regression tasks. However, in case of flood mapping using Sen1Flood11 dataset as a downstream task, GFMs struggles to outperform the baseline U-Net, highlighting model's limitation in capturing critical local nuances. To address this, we present the Prithvi-Complementary Adaptive Fusion Encoder (CAFE), which integrate Prithvi GFM pretrained encoder with a parallel CNN residual branch enhanced by Convolutional Attention Modules (CAM). Prithvi-CAFE enables fast and efficient fine-tuning through adapters in Prithvi and performs multi-scale, multi-level fusion with CNN features, capturing critical local details while preserving long-range dependencies. We achieve state-of-the-art results on two comprehensive flood mapping datasets: Sen1Flood11 and FloodPlanet. On Sen1Flood11 test data, Prithvi-CAFE (IoU 83.41) outperforms the original Prithvi (IoU 82.50) and other major GFMs (TerraMind 82.90, DOFA 81.54, spectralGPT: 81.02). The improvement is even more pronounced on the hold-out test site, where Prithvi-CAFE achieves an IoU of 81.37 compared to the baseline U-Net (70.57) and original Prithvi (72.42). On FloodPlanet, Prithvi-CAFE also surpasses the baseline U-Net and other GFMs, achieving an IoU of 64.70 compared to U-Net (60.14), Terramind (62.33), DOFA (59.15) and Prithvi 2.0 (61.91). Our proposed simple yet effective Prithvi-CAFE demonstrates strong potential for improving segmentation tasks where multi-channel and multi-modal data provide complementary information and local details are critical. The code is released on https://github.com/Sk-2103/Prithvi-CAFE{Prithvi-CAFE Github}",
    "translation": "标题：Prithvi-互补自适应融合编码器（CAFE）：释放洪水淹没制图的全潜力\n\n摘要：地理基础模型（GFMs）已被证明在多种下游应用中具有显著效果，包括语义分割、分类和回归任务。然而，在将Sen1Flood11数据集用于洪水制图这一下游任务时，GFMs难以超越基准U-Net模型，突显了其在捕捉关键局部细节方面的局限性。为解决这一问题，本文提出了Prithvi-互补自适应融合编码器（CAFE），该模型将预训练的Prithvi GFM编码器与一个由卷积注意力模块（CAM）增强的并行CNN残差分支相结合。Prithvi-CAFE通过适配器实现Prithvi模型的快速高效微调，并与CNN特征进行多尺度、多层次融合，在保持长程依赖关系的同时捕捉关键的局部细节。我们在两个综合性洪水制图数据集（Sen1Flood11和FloodPlanet）上取得了最先进的成果。在Sen1Flood11测试数据上，Prithvi-CAFE（交并比83.41）优于原始Prithvi模型（交并比82.50）及其他主要GFMs（TerraMind 82.90、DOFA 81.54、spectralGPT 81.02）。在保留测试站点上，改进更为显著：Prithvi-CAFE的交并比达到81.37，而基准U-Net为70.57，原始Prithvi为72.42。在FloodPlanet数据集上，Prithvi-CAFE同样超越了基准U-Net及其他GFMs，交并比达到64.70，优于U-Net（60.14）、Terramind（62.33）、DOFA（59.15）和Prithvi 2.0（61.91）。我们提出的Prithvi-CAFE结构简洁而高效，在多通道与多模态数据提供互补信息且局部细节至关重要的分割任务中展现出强大潜力。代码已发布于https://github.com/Sk-2103/Prithvi-CAFE。",
    "url": "https://huggingface.co/papers/2601.02315",
    "arxiv_url": "https://arxiv.org/abs/2601.02315"
  },
  {
    "title": "Project Ariadne: A Structural Causal Framework for Auditing Faithfulness in LLM Agents",
    "summary": "As Large Language Model (LLM) agents are increasingly tasked with high-stakes autonomous decision-making, the transparency of their reasoning processes has become a critical safety concern. While Chain-of-Thought (CoT) prompting allows agents to generate human-readable reasoning traces, it remains unclear whether these traces are faithful generative drivers of the model's output or merely post-hoc rationalizations. We introduce Project Ariadne, a novel XAI framework that utilizes Structural Causal Models (SCMs) and counterfactual logic to audit the causal integrity of agentic reasoning. Unlike existing interpretability methods that rely on surface-level textual similarity, Project Ariadne performs hard interventions (do-calculus) on intermediate reasoning nodes -- systematically inverting logic, negating premises, and reversing factual claims -- to measure the Causal Sensitivity (φ) of the terminal answer. Our empirical evaluation of state-of-the-art models reveals a persistent Faithfulness Gap. We define and detect a widespread failure mode termed Causal Decoupling, where agents exhibit a violation density (ρ) of up to 0.77 in factual and scientific domains. In these instances, agents arrive at identical conclusions despite contradictory internal logic, proving that their reasoning traces function as \"Reasoning Theater\" while decision-making is governed by latent parametric priors. Our findings suggest that current agentic architectures are inherently prone to unfaithful explanation, and we propose the Ariadne Score as a new benchmark for aligning stated logic with model action.",
    "translation": "标题：阿里阿德涅项目：一种用于审计大语言模型智能体忠实度的结构因果框架\n\n摘要：随着大语言模型智能体越来越多地承担高风险自主决策任务，其推理过程的透明度已成为关键的安全关切。尽管思维链提示法使智能体能够生成人类可读的推理轨迹，但这些轨迹究竟是模型输出的真实生成驱动因素，抑或仅是事后合理化解释，目前尚不明确。我们提出“阿里阿德涅项目”——一个创新的可解释人工智能框架，该框架利用结构因果模型与反事实逻辑来审计智能体推理的因果完整性。与依赖表层文本相似性的现有可解释性方法不同，阿里阿德涅项目通过对中间推理节点实施硬干预（do-演算）——系统性地反转逻辑、否定前提并颠覆事实主张——以测量最终答案的因果敏感度。我们对前沿模型的实证评估揭示了一个持续存在的“忠实度鸿沟”。我们定义并检测到一种普遍存在的故障模式，称为“因果解耦”，该模式下智能体在事实与科学领域中的违规密度最高可达0.77。在这些案例中，尽管内部逻辑相互矛盾，智能体却得出相同结论，证明其推理轨迹实为“推理剧场”，而决策过程实则受隐式参数先验支配。我们的研究结果表明，当前智能体架构本质上易于产生不忠实的解释，为此我们提出“阿里阿德涅分数”作为评估陈述逻辑与模型行为一致性的新基准。",
    "url": "https://huggingface.co/papers/2601.02314",
    "arxiv_url": "https://arxiv.org/abs/2601.02314"
  },
  {
    "title": "M-ErasureBench: A Comprehensive Multimodal Evaluation Benchmark for Concept Erasure in Diffusion Models",
    "summary": "Text-to-image diffusion models may generate harmful or copyrighted content, motivating research on concept erasure. However, existing approaches primarily focus on erasing concepts from text prompts, overlooking other input modalities that are increasingly critical in real-world applications such as image editing and personalized generation. These modalities can become attack surfaces, where erased concepts re-emerge despite defenses. To bridge this gap, we introduce M-ErasureBench, a novel multimodal evaluation framework that systematically benchmarks concept erasure methods across three input modalities: text prompts, learned embeddings, and inverted latents. For the latter two, we evaluate both white-box and black-box access, yielding five evaluation scenarios. Our analysis shows that existing methods achieve strong erasure performance against text prompts but largely fail under learned embeddings and inverted latents, with Concept Reproduction Rate (CRR) exceeding 90% in the white-box setting. To address these vulnerabilities, we propose IRECE (Inference-time Robustness Enhancement for Concept Erasure), a plug-and-play module that localizes target concepts via cross-attention and perturbs the associated latents during denoising. Experiments demonstrate that IRECE consistently restores robustness, reducing CRR by up to 40% under the most challenging white-box latent inversion scenario, while preserving visual quality. To the best of our knowledge, M-ErasureBench provides the first comprehensive benchmark of concept erasure beyond text prompts. Together with IRECE, our benchmark offers practical safeguards for building more reliable protective generative models.",
    "translation": "标题：M-ErasureBench：扩散模型概念擦除的多模态综合评估基准\n\n摘要：文本到图像扩散模型可能生成有害或受版权保护的内容，这推动了概念擦除技术的研究。然而，现有方法主要集中于从文本提示中擦除概念，忽视了图像编辑和个性化生成等实际应用中日益重要的其他输入模态。这些模态可能成为攻击面，导致已擦除的概念绕过防御机制重新出现。为弥补这一空白，我们提出了M-ErasureBench——一个新颖的多模态评估框架，系统性地在三种输入模态（文本提示、学习嵌入和反转潜在表示）上对概念擦除方法进行基准测试。针对后两种模态，我们同时评估白盒与黑盒访问场景，共构建五种评估情境。分析表明，现有方法在应对文本提示时表现出较强的擦除性能，但在学习嵌入和反转潜在表示场景下普遍失效，其中白盒设置下的概念再现率（CRR）超过90%。为应对这些漏洞，我们提出IRECE（概念擦除的推理时鲁棒性增强模块），该即插即用模块通过交叉注意力定位目标概念，并在去噪过程中扰动相关潜在表示。实验证明，IRECE能持续恢复模型鲁棒性，在最具挑战性的白盒潜在反转场景中将CRR降低达40%，同时保持视觉质量。据我们所知，M-ErasureBench首次建立了超越文本提示的全面概念擦除评估基准。结合IRECE，本基准为构建更可靠的保护性生成模型提供了实用保障方案。",
    "url": "https://huggingface.co/papers/2512.22877",
    "arxiv_url": "https://arxiv.org/abs/2512.22877"
  }
]