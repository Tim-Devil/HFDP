[
  {
    "title": "Every Activation Boosted: Scaling General Reasoner to 1 Trillion Open\n  Language Foundation",
    "summary": "We introduce Ling 2.0, a series reasoning-oriented language foundation built\nupon the principle that every activation boosts reasoning capability. Designed\nto scale from tens of billions to one trillion parameters under a unified\nMixture-of-Experts (MoE) paradigm, Ling 2.0 emphasizes high sparsity,\ncross-scale consistency, and efficiency guided by empirical scaling laws. The\nseries includes three non-thinking (instruct) models - Ling-mini-2.0,\nLing-flash-2.0, and Ling-1T - ranging from 16B to 1T total parameters and\nachieving up to 7-fold active-compute efficiency compared with dense\ncounterparts. Ling 2.0 integrates coordinated innovations across model\narchitecture, pre-training, post-training, and infrastructure: a high-sparsity\nMoE with MTP for efficient reasoning, reasoning-oriented data and mid-training\nCoT activation, reinforcement-based fine-tuning (DFT, Evo-CoT), and full-scale\nFP8 training with fine-grained heterogeneous pipelines. At the trillion scale,\nLing-1T establishes a new Pareto frontier of reasoning accuracy versus\ncomputational efficiency, demonstrating that sparse activation, when properly\naligned with reasoning objectives, enables scalable and efficient intelligence.\nCollectively, Ling 2.0 provides a coherent, open, and efficient foundation for\nadvancing future reasoning and thinking models, including the Ring series built\nupon the same base.",
    "translation": "标题：每次激活皆提升：将通用推理模型扩展至万亿级开放语言基础\n\n摘要：我们推出Ling 2.0系列面向推理的语言基础模型，其核心设计理念是每次激活都能增强推理能力。该系列在统一的混合专家范式下，实现了从百亿到万亿参数规模的扩展，并基于经验缩放法则重点优化了高稀疏性、跨尺度一致性与运行效率。该系列包含三款非思维型（指令式）模型——Ling-mini-2.0、Ling-flash-2.0和Ling-1T，总参数量覆盖160亿至1万亿范围，与稠密模型相比最高可实现7倍激活计算效率。Ling 2.0在模型架构、预训练、后训练及基础设施层面实现了协同创新：采用配备混合张量并行的超稀疏MoE架构以提升推理效率，引入面向推理的数据与训练中程思维链激活机制，实施基于强化学习的精细调优（直接反馈训练、进化式思维链），并实现全量程FP8训练与细粒度异构流水线。在万亿参数规模上，Ling-1T确立了推理精度与计算效率的新帕累托前沿，证明当稀疏激活与推理目标精准对齐时，可实现可扩展的高效智能。整体而言，Ling 2.0为推进未来推理与思维模型（包括基于同底座的Ring系列）提供了连贯、开放且高效的基础架构。",
    "url": "https://huggingface.co/papers/2510.22115",
    "arxiv_url": "https://arxiv.org/abs/2510.22115"
  },
  {
    "title": "Generalizing Test-time Compute-optimal Scaling as an Optimizable Graph",
    "summary": "Test-Time Scaling (TTS) improves large language models (LLMs) by allocating\nadditional computation during inference, typically through parallel,\nsequential, or hybrid scaling. However, prior studies often assume fixed\ncollaboration architectures (e.g., topologies) and single-model usage,\noverlooking that optimal architectures and model combinations can vary across\ntasks. Therefore, we study the novel problem of searching for compute-optimal\nmodel combinations and architectures in TTS under a fixed budget. We formalize\nit as a multi-LLM collaboration graph, where nodes encode roles and LLM model\nassignments, and edges capture information flow. This problem is challenging\nbecause (i) the combinatorial search space is prohibitively large, and (ii)\ntask-specific requirements demand tailored designs. To address these, we\nreformulate the problem as probabilistic graph optimization and, through pilot\nexperiments, derive three empirical insights into TTS collaboration graphs.\nGuided by these insights, we propose Agent-REINFORCE, an LLM-agent-augmented\nframework that mirrors the REINFORCE pipeline by mapping\nsampling-gradient-update to sampling-feedback-update, where feedback serves as\na textual gradient to update the probabilistic graph and efficiently search for\noptimal multi-LLM collaboration graphs. Experiments show that Agent-REINFORCE\noutperforms both traditional and LLM-based baselines in sample efficiency and\nsearch performance, and effectively identifies optimal graphs under joint\nobjectives of accuracy and inference latency.",
    "translation": "标题：将测试时计算最优缩放推广为可优化图结构\n\n摘要：测试时缩放技术通过分配额外计算资源提升大语言模型性能，通常采用并行、串行或混合缩放模式。然而既有研究常预设固定的协作架构（如拓扑结构）和单一模型使用，忽视了最优架构与模型组合会随任务动态变化的特性。为此，我们研究在固定计算预算下寻找测试时缩放中最优模型组合与架构的新课题。本文将其形式化为多LLM协作图优化问题：节点编码角色与LLM模型分配，边捕捉信息流动。该问题面临双重挑战：（1）组合搜索空间呈指数级增长；（2）任务特定需求需定制化设计。我们通过将问题重构为概率图优化，并借助预实验总结出测试时协作图的三项经验性发现。基于这些发现，提出Agent-REINFORCE框架——通过将“采样-梯度-更新”映射为“采样-反馈-更新”流程，其中文本化反馈作为梯度更新概率图，以此高效搜索最优多LLM协作图。实验表明，该方法在样本效率和搜索性能上均优于传统基线与LLM基线，能有效平衡准确率与推理延迟的双重目标，精准识别最优图结构。",
    "url": "https://huggingface.co/papers/2511.00086",
    "arxiv_url": "https://arxiv.org/abs/2511.00086"
  },
  {
    "title": "The Underappreciated Power of Vision Models for Graph Structural\n  Understanding",
    "summary": "Graph Neural Networks operate through bottom-up message-passing,\nfundamentally differing from human visual perception, which intuitively\ncaptures global structures first. We investigate the underappreciated potential\nof vision models for graph understanding, finding they achieve performance\ncomparable to GNNs on established benchmarks while exhibiting distinctly\ndifferent learning patterns. These divergent behaviors, combined with\nlimitations of existing benchmarks that conflate domain features with\ntopological understanding, motivate our introduction of GraphAbstract. This\nbenchmark evaluates models' ability to perceive global graph properties as\nhumans do: recognizing organizational archetypes, detecting symmetry, sensing\nconnectivity strength, and identifying critical elements. Our results reveal\nthat vision models significantly outperform GNNs on tasks requiring holistic\nstructural understanding and maintain generalizability across varying graph\nscales, while GNNs struggle with global pattern abstraction and degrade with\nincreasing graph size. This work demonstrates that vision models possess\nremarkable yet underutilized capabilities for graph structural understanding,\nparticularly for problems requiring global topological awareness and\nscale-invariant reasoning. These findings open new avenues to leverage this\nunderappreciated potential for developing more effective graph foundation\nmodels for tasks dominated by holistic pattern recognition.",
    "translation": "标题：视觉模型在图结构理解中被低估的能力\n\n摘要：图神经网络通过自底向上的消息传递机制运作，这与人类视觉感知存在根本差异——后者能够直觉性地先捕捉全局结构。我们研究了视觉模型在图理解任务中被低估的潜力，发现其在经典基准测试中达到了与图神经网络相当的性能，同时展现出截然不同的学习模式。这些差异化的行为特征，加之现有基准测试将领域特征与拓扑理解相互混淆的局限性，促使我们开发了GraphAbstract基准。该基准通过识别组织原型、检测对称性、感知连接强度及识别关键元素等维度，评估模型像人类一样感知全局图属性的能力。实验结果表明：在需要整体结构理解的任务中，视觉模型显著优于图神经网络，并能在不同图规模下保持泛化能力；而图神经网络则难以进行全局模式抽象，且性能随图规模增大而下降。本研究证实视觉模型具有卓越但未被充分利用的图结构理解能力，特别是在需要全局拓扑感知和尺度不变推理的问题上。这些发现为开发更有效的图基础模型开辟了新途径，尤其适用于以整体模式识别为主导的任务场景。",
    "url": "https://huggingface.co/papers/2510.24788",
    "arxiv_url": "https://arxiv.org/abs/2510.24788"
  },
  {
    "title": "UniLumos: Fast and Unified Image and Video Relighting with\n  Physics-Plausible Feedback",
    "summary": "Relighting is a crucial task with both practical demand and artistic value,\nand recent diffusion models have shown strong potential by enabling rich and\ncontrollable lighting effects. However, as they are typically optimized in\nsemantic latent space, where proximity does not guarantee physical correctness\nin visual space, they often produce unrealistic results, such as overexposed\nhighlights, misaligned shadows, and incorrect occlusions. We address this with\nUniLumos, a unified relighting framework for both images and videos that brings\nRGB-space geometry feedback into a flow matching backbone. By supervising the\nmodel with depth and normal maps extracted from its outputs, we explicitly\nalign lighting effects with the scene structure, enhancing physical\nplausibility. Nevertheless, this feedback requires high-quality outputs for\nsupervision in visual space, making standard multi-step denoising\ncomputationally expensive. To mitigate this, we employ path consistency\nlearning, allowing supervision to remain effective even under few-step training\nregimes. To enable fine-grained relighting control and supervision, we design a\nstructured six-dimensional annotation protocol capturing core illumination\nattributes. Building upon this, we propose LumosBench, a disentangled\nattribute-level benchmark that evaluates lighting controllability via large\nvision-language models, enabling automatic and interpretable assessment of\nrelighting precision across individual dimensions. Extensive experiments\ndemonstrate that UniLumos achieves state-of-the-art relighting quality with\nsignificantly improved physical consistency, while delivering a 20x speedup for\nboth image and video relighting. Code is available at\nhttps://github.com/alibaba-damo-academy/Lumos-Custom.",
    "translation": "标题：UniLumos：基于物理可信反馈的快速统一图像视频重照明框架\n\n摘要：重照明技术兼具实际应用需求与艺术价值，近期扩散模型通过实现丰富可控的照明效果展现出强大潜力。然而，由于这些模型通常在语义隐空间中进行优化，其邻近性无法保证视觉空间中的物理正确性，常产生过度曝光的高光、错位阴影及错误遮挡等非真实效果。我们提出UniLumos这一面向图像与视频的统一重照明框架，通过将RGB空间的几何反馈引入流匹配主干网络来解决该问题。借助从模型输出中提取的深度图与法向图进行监督，我们显式地将照明效果与场景结构对齐，增强物理可信度。然而，这种反馈机制需要视觉空间中的高质量输出进行监督，使得标准多步去噪过程计算成本高昂。为此，我们采用路径一致性学习，确保即使在少步数训练机制下监督依然有效。为实现细粒度重照明控制与监督，我们设计了结构化的六维标注协议以捕捉核心光照属性。在此基础上提出LumosBench——一个解耦的属性级基准测试集，通过大视觉语言模型评估光照可控性，实现对各个维度重照明精度的自动化可解释评估。大量实验表明，UniLumos在显著提升物理一致性的同时实现了最先进的重照明质量，并为图像和视频重照明带来20倍加速。代码已开源：https://github.com/alibaba-damo-academy/Lumos-Custom。",
    "url": "https://huggingface.co/papers/2511.01678",
    "arxiv_url": "https://arxiv.org/abs/2511.01678"
  },
  {
    "title": "ROVER: Benchmarking Reciprocal Cross-Modal Reasoning for Omnimodal\n  Generation",
    "summary": "Unified multimodal models (UMMs) have emerged as a powerful paradigm for\nseamlessly unifying text and image understanding and generation. However,\nprevailing evaluations treat these abilities in isolation, such that tasks with\nmultimodal inputs and outputs are scored primarily through unimodal reasoning,\ni.e., textual benchmarks emphasize language-based reasoning, while visual\nbenchmarks emphasize reasoning outcomes manifested in the pixels. We introduce\nROVER to address this pressing need to test reciprocal cross-modal reasoning,\nthe use of one modality to guide, verify, or refine outputs in the other, an\nability central to the vision of unified multimodal intelligence. ROVER is a\nhuman-annotated benchmark that explicitly targets reciprocal cross-modal\nreasoning, which contains 1312 tasks grounded in 1876 images, spanning two\ncomplementary settings. Verbally-augmented reasoning for visual generation\nevaluates whether models can use verbal prompts and reasoning chains to guide\nfaithful image synthesis. Visually-augmented reasoning for verbal generation\nevaluates whether models can generate intermediate visualizations that\nstrengthen their own reasoning processes for question answering. Experiments on\n17 unified models reveal two key findings: (i) Cross-modal reasoning determines\nvisual generation quality, with interleaved models significantly outperforming\nnon-interleaved ones; notably, combining strong unimodal models fails to\nachieve comparable reasoning. (ii) Models show dissociation between physical\nand symbolic reasoning: they succeed at interpreting perceptual concepts\nliterally but fail to construct visual abstractions for symbolic tasks, where\nfaulty reasoning harms performance. These results highlight reciprocal\ncross-modal reasoning as a critical frontier for enabling true omnimodal\ngeneration.",
    "translation": "标题：ROVER：面向全模态生成的逆向跨模态推理基准测试\n\n摘要：统一多模态模型已成为无缝整合文本与图像理解与生成的强大范式。然而主流评估方法往往孤立对待这些能力，导致多模态输入输出任务的评分主要基于单模态推理——文本基准强调语言推理能力，而视觉基准则关注像素层面的推理结果。为应对测试逆向跨模态推理的迫切需求，我们推出ROVER基准。逆向跨模态推理指运用一种模态来引导、验证或优化另一种模态输出的能力，这是实现统一多模态智能愿景的核心能力。ROVER作为人工标注的基准测试集，明确针对逆向跨模态推理设计，包含基于1876张图像构建的1312个任务，涵盖两个互补场景：视觉生成中的语言增强推理评估模型能否运用语言提示和推理链指导精确的图像合成；语言生成中的视觉增强推理评估模型能否生成中间可视化结果以强化问答任务的推理过程。通过对17个统一模型的实验，我们获得两个关键发现：(i)跨模态推理决定视觉生成质量，交错式模型显著优于非交错式模型，值得注意的是，组合强单模态模型无法实现可比推理能力；(ii)模型在物理推理与符号推理间存在解离现象：能成功解读具象感知概念，却难以构建符号任务的视觉抽象表征，这种缺陷会损害推理性能。这些结果表明逆向跨模态推理是实现真正全模态生成的关键前沿领域。",
    "url": "https://huggingface.co/papers/2511.01163",
    "arxiv_url": "https://arxiv.org/abs/2511.01163"
  },
  {
    "title": "PHUMA: Physically-Grounded Humanoid Locomotion Dataset",
    "summary": "Motion imitation is a promising approach for humanoid locomotion, enabling\nagents to acquire humanlike behaviors. Existing methods typically rely on\nhigh-quality motion capture datasets such as AMASS, but these are scarce and\nexpensive, limiting scalability and diversity. Recent studies attempt to scale\ndata collection by converting large-scale internet videos, exemplified by\nHumanoid-X. However, they often introduce physical artifacts such as floating,\npenetration, and foot skating, which hinder stable imitation. In response, we\nintroduce PHUMA, a Physically-grounded HUMAnoid locomotion dataset that\nleverages human video at scale, while addressing physical artifacts through\ncareful data curation and physics-constrained retargeting. PHUMA enforces joint\nlimits, ensures ground contact, and eliminates foot skating, producing motions\nthat are both large-scale and physically reliable. We evaluated PHUMA in two\nsets of conditions: (i) imitation of unseen motion from self-recorded test\nvideos and (ii) path following with pelvis-only guidance. In both cases,\nPHUMA-trained policies outperform Humanoid-X and AMASS, achieving significant\ngains in imitating diverse motions. The code is available at\nhttps://davian-robotics.github.io/PHUMA.",
    "translation": "标题：PHUMA：基于物理的人形机器人运动数据集\n\n摘要：运动模仿是实现人形机器人拟人化运动的重要方法。现有研究多依赖AMASS等高质量动作捕捉数据集，但这些数据稀缺且成本高昂，限制了方法的可扩展性与运动多样性。近期研究尝试通过转换互联网视频扩大数据规模（如Humanoid-X），但这类方法常产生漂浮、穿模、足部滑动等物理失真现象，影响运动稳定性。为此，我们提出PHUMA——基于物理验证的人形机器人运动数据集，该数据集在规模化利用人类视频数据的同时，通过精细化数据清洗与物理约束的重定向技术解决物理失真问题。PHUMA严格遵循关节活动限度，确保足部接地约束，消除足部滑动现象，最终生成兼具大规模与物理可靠性的运动数据。我们在两类场景中评估PHUMA：（1）对自采集测试视频中未见运动的模仿能力；（2）仅基于骨盆引导的路径跟踪任务。实验表明，基于PHUMA训练的策略在两项任务中均显著优于Humanoid-X和AMASS，在多样化运动模仿方面取得突破性进展。项目代码详见：https://davian-robotics.github.io/PHUMA。",
    "url": "https://huggingface.co/papers/2510.26236",
    "arxiv_url": "https://arxiv.org/abs/2510.26236"
  },
  {
    "title": "UniREditBench: A Unified Reasoning-based Image Editing Benchmark",
    "summary": "Recent advances in multi-modal generative models have driven substantial\nimprovements in image editing. However, current generative models still\nstruggle with handling diverse and complex image editing tasks that require\nimplicit reasoning, underscoring the need for a comprehensive benchmark to\nsystematically assess their performance across various reasoning scenarios.\nExisting benchmarks primarily focus on single-object attribute transformation\nin realistic scenarios, which, while effective, encounter two key challenges:\n(1) they largely overlook multi-object interactions as well as game-world\nscenarios that involve human-defined rules, which are common in real-life\napplications; (2) they only rely on textual references to evaluate the\ngenerated images, potentially leading to systematic misjudgments, especially in\ncomplex reasoning scenarios. To this end, this work proposes UniREditBench, a\nunified benchmark for reasoning-based image editing evaluation. It comprises\n2,700 meticulously curated samples, covering both real- and game-world\nscenarios across 8 primary dimensions and 18 sub-dimensions. To improve\nevaluation reliability, we introduce multimodal dual-reference evaluation,\nproviding both textual and ground-truth image references for each sample\nassessment. Furthermore, we design an automated multi-scenario data synthesis\npipeline and construct UniREdit-Data-100K, a large-scale synthetic dataset with\nhigh-quality chain-of-thought (CoT) reasoning annotations. We fine-tune Bagel\non this dataset and develop UniREdit-Bagel, demonstrating substantial\nimprovements in both in-domain and out-of-distribution settings. Through\nthorough benchmarking of both open-source and closed-source image editing\nmodels, we reveal their strengths and weaknesses across various aspects.",
    "translation": "标题：UniREditBench：基于统一推理机制的图像编辑基准测试框架\n\n摘要：多模态生成模型的最新进展显著推动了图像编辑技术的发展。然而，当前生成模型在处理需要隐式推理的多样化复杂图像编辑任务时仍面临挑战，这凸显了建立系统性评估各类推理场景下模型性能的综合基准的必要性。现有基准主要关注现实场景中的单目标属性转换，虽具实效性却存在两大关键局限：（1）普遍忽视多目标交互关系及涉及人为规则的虚拟场景，而这些要素在实际应用中十分常见；（2）仅依赖文本参照评估生成图像，可能导致系统性误判，尤其在复杂推理场景中。为此，本研究提出UniREditBench——基于统一推理机制的图像编辑评估基准。该基准包含2,700个精心构建的样本，涵盖现实与虚拟两大场景，涉及8个主维度和18个子维度。为提升评估可靠性，我们引入多模态双参照评估机制，为每个样本提供文本与真实图像双重参照。此外，我们设计了自动化多场景数据合成流程，构建了包含高质量思维链推理标注的大规模合成数据集UniREdit-Data-100K。基于该数据集对Bagel模型进行微调后开发的UniREdit-Bagel，在域内与域外场景均展现出显著性能提升。通过对开源与闭源图像编辑模型进行全面基准测试，系统揭示了各类模型在不同维度上的优势与不足。",
    "url": "https://huggingface.co/papers/2511.01295",
    "arxiv_url": "https://arxiv.org/abs/2511.01295"
  },
  {
    "title": "World Simulation with Video Foundation Models for Physical AI",
    "summary": "We introduce [Cosmos-Predict2.5], the latest generation of the Cosmos World\nFoundation Models for Physical AI. Built on a flow-based architecture,\n[Cosmos-Predict2.5] unifies Text2World, Image2World, and Video2World generation\nin a single model and leverages [Cosmos-Reason1], a Physical AI vision-language\nmodel, to provide richer text grounding and finer control of world simulation.\nTrained on 200M curated video clips and refined with reinforcement\nlearning-based post-training, [Cosmos-Predict2.5] achieves substantial\nimprovements over [Cosmos-Predict1] in video quality and instruction alignment,\nwith models released at 2B and 14B scales. These capabilities enable more\nreliable synthetic data generation, policy evaluation, and closed-loop\nsimulation for robotics and autonomous systems. We further extend the family\nwith [Cosmos-Transfer2.5], a control-net style framework for Sim2Real and\nReal2Real world translation. Despite being 3.5times smaller than\n[Cosmos-Transfer1], it delivers higher fidelity and robust long-horizon video\ngeneration. Together, these advances establish [Cosmos-Predict2.5] and\n[Cosmos-Transfer2.5] as versatile tools for scaling embodied intelligence. To\naccelerate research and deployment in Physical AI, we release source code,\npretrained checkpoints, and curated benchmarks under the NVIDIA Open Model\nLicense at https://github.com/nvidia-cosmos/cosmos-predict2.5 and\nhttps://github.com/nvidia-cosmos/cosmos-transfer2.5. We hope these open\nresources lower the barrier to adoption and foster innovation in building the\nnext generation of embodied intelligence.",
    "translation": "标题：基于视频基础模型的物理人工智能世界仿真\n\n摘要：我们推出新一代物理人工智能世界基础模型[Cosmos-Predict2.5]。该模型基于流式架构构建，在单一模型中实现了文本生成世界、图像生成世界与视频生成世界的统一，并利用物理人工智能视觉语言模型[Cosmos-Reason1]提供更丰富的文本语义基础与更精细的世界仿真控制。通过2亿条精选视频片段训练及基于强化学习的训练后优化，[Cosmos-Predict2.5]在视频质量与指令对齐方面较[Cosmos-Predict1]实现显著提升，同步发布20亿与140亿参数规模的模型。这些能力为机器人与自主系统领域提供了更可靠的合成数据生成、策略评估与闭环仿真方案。我们进一步推出控制网络框架[Cosmos-Transfer2.5]，实现仿真到现实及现实到现实的世界转换。尽管其参数量较[Cosmos-Transfer1]减少3.5倍，仍能提供更高保真度与鲁棒性的长时序视频生成。这些突破共同确立了[Cosmos-Predict2.5]与[Cosmos-Transfer2.5]作为扩展具身智能的通用工具地位。为加速物理人工智能领域的研究部署，我们在NVIDIA开放模型许可下于https://github.com/nvidia-cosmos/cosmos-predict2.5 与 https://github.com/nvidia-cosmos/cosmos-transfer2.5 发布源代码、预训练模型与精选基准测试集。期待这些开放资源能降低技术应用门槛，推动新一代具身智能建设的创新发展。",
    "url": "https://huggingface.co/papers/2511.00062",
    "arxiv_url": "https://arxiv.org/abs/2511.00062"
  },
  {
    "title": "ToolScope: An Agentic Framework for Vision-Guided and Long-Horizon Tool\n  Use",
    "summary": "Recently, large language models (LLMs) have demonstrated remarkable\nproblem-solving capabilities by autonomously integrating with external tools\nfor collaborative reasoning. However, due to the inherently complex and diverse\nnature of multimodal information, enabling multimodal large language models\n(MLLMs) to flexibly and efficiently utilize external tools during reasoning\nremains an underexplored challenge. In this work, we introduce ToolScope, an\nagentic framework designed to unify global planning with local multimodal\nperception, adopting a specialized Perceive tool to mitigates visual context\ndegradation in long-horizon VQA task. ToolScope comprises three primary\ncomponents: the Global Navigator, the Agentic Executor, and the Response\nSynthesizer. The Global Navigator functions as a \"telescope\", offering\nhigh-level strategic guidance. The Agentic Executor operates iteratively to\naugment MLLM with local perception through the integration of external\ntools-Search, Code, and Perceive. Finally, the Response Synthesizer\nconsolidates and organizes the reasoning process into a coherent, user-friendly\noutput. We evaluate ToolScope on four VQA benchmarks across diverse domains,\nincluding VQA 2.0, ScienceQA, MAT-Search and MathVista. It demonstrates strong\ngeneralization capabilities, achieving an average performance improvement of up\nto +6.69% across all datasets.",
    "translation": "标题：ToolScope：一种面向视觉引导与长周期工具使用的自主智能框架\n\n摘要：近年来，大型语言模型通过自主集成外部工具进行协同推理，展现出卓越的问题解决能力。然而由于多模态信息固有的复杂性和多样性，使多模态大语言模型在推理过程中灵活高效地利用外部工具仍是一个尚未充分探索的挑战。本研究提出ToolScope——一个将全局规划与局部多模态感知相统一的自主智能框架，通过专用感知工具缓解长周期视觉问答任务中的视觉语境退化问题。该框架包含三大核心组件：全局导航器作为\"望远镜\"提供高层策略指导；自主执行器通过集成搜索、代码和感知三类外部工具，以迭代方式增强模型的局部感知能力；响应合成器则将推理过程整合为连贯的用户友好型输出。我们在涵盖VQA 2.0、ScienceQA、MAT-Search和MathVista的四个跨领域VQA基准测试中评估ToolScope，该框架展现出强大的泛化能力，在所有数据集上平均性能提升最高达+6.69%。",
    "url": "https://huggingface.co/papers/2510.27363",
    "arxiv_url": "https://arxiv.org/abs/2510.27363"
  },
  {
    "title": "EBT-Policy: Energy Unlocks Emergent Physical Reasoning Capabilities",
    "summary": "Implicit policies parameterized by generative models, such as Diffusion\nPolicy, have become the standard for policy learning and Vision-Language-Action\n(VLA) models in robotics. However, these approaches often suffer from high\ncomputational cost, exposure bias, and unstable inference dynamics, which lead\nto divergence under distribution shifts. Energy-Based Models (EBMs) address\nthese issues by learning energy landscapes end-to-end and modeling equilibrium\ndynamics, offering improved robustness and reduced exposure bias. Yet, policies\nparameterized by EBMs have historically struggled to scale effectively. Recent\nwork on Energy-Based Transformers (EBTs) demonstrates the scalability of EBMs\nto high-dimensional spaces, but their potential for solving core challenges in\nphysically embodied models remains underexplored. We introduce a new\nenergy-based architecture, EBT-Policy, that solves core issues in robotic and\nreal-world settings. Across simulated and real-world tasks, EBT-Policy\nconsistently outperforms diffusion-based policies, while requiring less\ntraining and inference computation. Remarkably, on some tasks it converges\nwithin just two inference steps, a 50x reduction compared to Diffusion Policy's\n100. Moreover, EBT-Policy exhibits emergent capabilities not seen in prior\nmodels, such as zero-shot recovery from failed action sequences using only\nbehavior cloning and without explicit retry training. By leveraging its scalar\nenergy for uncertainty-aware inference and dynamic compute allocation,\nEBT-Policy offers a promising path toward robust, generalizable robot behavior\nunder distribution shifts.",
    "translation": "标题：EBT-策略：能量模型解锁涌现的物理推理能力\n\n摘要：由生成模型参数化的隐式策略（如扩散策略）已成为机器人领域策略学习与视觉-语言-动作模型的标准范式。然而，这类方法常面临计算成本高、曝光偏差和推理动态不稳定等问题，导致其在分布偏移下出现性能退化。基于能量的模型通过端到端学习能量景观并建模平衡动力学，能有效提升鲁棒性并减少曝光偏差。但传统基于能量的策略参数化方法始终难以实现规模化应用。近期基于能量的Transformer研究虽证明了该类模型在高维空间的扩展能力，但其在物理实体模型中解决核心挑战的潜力尚未被充分探索。我们提出新型能量架构EBT-策略，可有效解决机器人及现实场景中的核心问题。在仿真与真实任务中，EBT-策略持续超越基于扩散的策略，同时显著降低训练与推理计算量。值得注意的是，在某些任务中仅需两次推理迭代即可收敛，相较扩散策略所需的100次迭代减少50倍计算量。更突出的是，EBT-策略展现出前所未有的涌现能力：仅通过行为克隆而无需显式重试训练，即可实现失败动作序列的零样本恢复。通过利用标量能量实现不确定性感知推理与动态计算资源分配，EBT-策略为构建分布偏移下鲁棒、可泛化的机器人行为提供了可行路径。",
    "url": "https://huggingface.co/papers/2510.27545",
    "arxiv_url": "https://arxiv.org/abs/2510.27545"
  },
  {
    "title": "OpenSIR: Open-Ended Self-Improving Reasoner",
    "summary": "Recent advances in large language model (LLM) reasoning through reinforcement\nlearning rely on annotated datasets for verifiable rewards, which may limit\nmodels' ability to surpass human-level performance. While self-play offers a\npromising alternative, existing approaches depend on external verifiers or\ncannot learn open-endedly. We present Open-Ended Self-Improving Reasoner\n(OpenSIR), a self-play framework where an LLM learns to generate and solve\nnovel problems by alternating teacher and student roles without external\nsupervision. To generate novel problems, OpenSIR optimises for both difficulty\nand diversity, rewarding problems that challenge appropriately while exploring\ndistinct concepts, enabling open-ended mathematical discovery. Starting from a\nsingle trivial seed problem, OpenSIR substantially improves instruction models:\nLlama-3.2-3B-Instruct advances from 73.9 to 78.3 on GSM8K, and from 28.8 to\n34.4 on College Math, while Gemma-2-2B-Instruct rises from 38.5 to 58.7 on\nGSM8K. Our analyses reveal that OpenSIR achieves open-ended learning through\nco-evolving teacher-student roles that adaptively calibrate difficulty and\ndrive diverse exploration, progressing autonomously from basic to advanced\nmathematics.",
    "translation": "标题：OpenSIR：开放式自我改进推理系统\n\n摘要：当前基于强化学习的大语言模型推理方法依赖于带标注数据集的可验证奖励机制，这种模式可能限制模型超越人类水平的能力。虽然自我博弈机制提供了有前景的替代方案，但现有方法仍需依赖外部验证器或无法实现开放式学习。我们提出开放式自我改进推理系统（OpenSIR），该自博弈框架使大语言模型通过交替扮演教师与学生角色，在无外部监督条件下自主生成并解决新型问题。为实现问题创新，OpenSIR同步优化难度系数与多样性指标，通过奖励那些既能形成适度挑战又能探索独特概念的问题生成，最终实现开放式数学发现。从单个简单种子问题出发，OpenSIR显著提升了指令模型的性能：Llama-3.2-3B-Instruct在GSM8K上的得分从73.9提升至78.3，在大学数学数据集上从28.8提升至34.4；Gemma-2-2B-Instruct在GSM8K上的得分从38.5跃升至58.7。分析表明，OpenSIR通过协同进化的师生角色实现开放式学习，这种机制能自适应校准问题难度并驱动多样化探索，使系统实现从基础数学到高等数学的自主演进。",
    "url": "https://huggingface.co/papers/2511.00602",
    "arxiv_url": "https://arxiv.org/abs/2511.00602"
  },
  {
    "title": "MR-Align: Meta-Reasoning Informed Factuality Alignment for Large\n  Reasoning Models",
    "summary": "Large reasoning models (LRMs) show strong capabilities in complex reasoning,\nyet their marginal gains on evidence-dependent factual questions are limited.\nWe find this limitation is partially attributable to a reasoning-answer hit\ngap, where the model identifies the correct facts during reasoning but fails to\nincorporate them into the final response, thereby reducing factual fidelity. To\naddress this issue, we propose MR-ALIGN, a Meta-Reasoning informed alignment\nframework that enhances factuality without relying on external verifiers.\nMR-ALIGN quantifies state transition probabilities along the model's thinking\nprocess and constructs a transition-aware implicit reward that reinforces\nbeneficial reasoning patterns while suppressing defective ones at the atomic\nthinking segments. This re-weighting reshapes token-level signals into\nprobability-aware segment scores, encouraging coherent reasoning trajectories\nthat are more conducive to factual correctness. Empirical evaluations across\nfour factual QA datasets and one long-form factuality benchmark show that\nMR-ALIGN consistently improves accuracy and truthfulness while reducing\nmisleading reasoning. These results highlight that aligning the reasoning\nprocess itself, rather than merely the outputs, is pivotal for advancing\nfactuality in LRMs.",
    "translation": "标题：MR-Align：基于元推理认知的大规模推理模型事实性校准框架\n\n摘要：大规模推理模型在复杂推理任务中展现出强大能力，但其在证据依赖型事实问题上的边际提升有限。我们发现该局限部分源于推理-答案的命中差距：模型在推理过程中能识别正确事实，却未能将其有效整合至最终响应，从而导致事实保真度下降。为解决此问题，我们提出MR-ALIGN——一种基于元推理认知的校准框架，该框架无需依赖外部验证器即可增强事实准确性。MR-ALIGN通过量化模型思维过程中的状态转移概率，构建具有转移感知能力的隐式奖励机制，该机制在原子化思维片段层面强化有效推理模式的同时抑制缺陷推理模式。这种重加权策略将词元级信号重构为概率感知的片段评分，从而促生更有利于事实准确性的连贯推理轨迹。在四个事实问答数据集和一个长文本事实基准上的实证评估表明，MR-ALIGN能持续提升准确性与真实性，同时减少误导性推理。这些结果凸显出：对推理过程本身（而非仅对输出结果）进行校准，是推进大规模推理模型事实性进步的关键所在。",
    "url": "https://huggingface.co/papers/2510.24794",
    "arxiv_url": "https://arxiv.org/abs/2510.24794"
  },
  {
    "title": "LongCat-Flash-Omni Technical Report",
    "summary": "We introduce LongCat-Flash-Omni, a state-of-the-art open-source omni-modal\nmodel with 560 billion parameters, excelling at real-time audio-visual\ninteraction. By adopting a curriculum-inspired progressive training strategy\nthat transitions from simpler to increasingly complex modality sequence\nmodeling tasks, LongCat-Flash-Omni attains comprehensive multimodal\ncapabilities while maintaining strong unimodal capability. Building upon\nLongCat-Flash, which adopts a high-performance Shortcut-connected\nMixture-of-Experts (MoE) architecture with zero-computation experts,\nLongCat-Flash-Omni integrates efficient multimodal perception and speech\nreconstruction modules. Despite its immense size of 560B parameters (with 27B\nactivated), LongCat-Flash-Omni achieves low-latency real-time audio-visual\ninteraction. For training infrastructure, we developed a modality-decoupled\nparallelism scheme specifically designed to manage the data and model\nheterogeneity inherent in large-scale multimodal training. This innovative\napproach demonstrates exceptional efficiency by sustaining over 90% of the\nthroughput achieved by text-only training. Extensive evaluations show that\nLongCat-Flash-Omni achieves state-of-the-art performance on omni-modal\nbenchmarks among open-source models. Furthermore, it delivers highly\ncompetitive results across a wide range of modality-specific tasks, including\ntext, image, and video understanding, as well as audio understanding and\ngeneration. We provide a comprehensive overview of the model architecture\ndesign, training procedures, and data strategies, and open-source the model to\nfoster future research and development in the community.",
    "translation": "标题：LongCat-Flash-Omni技术报告\n\n摘要：本文介绍LongCat-Flash-Omni——一个拥有5600亿参数的前沿开源全模态模型，在实时音视频交互方面表现卓越。通过采用课程式渐进训练策略，从简单到复杂逐步推进模态序列建模任务，该模型在保持强大单模态能力的同时获得了全面的多模态能力。基于采用高性能零计算专家捷径连接混合专家架构的LongCat-Flash基础，LongCat-Flash-Omni集成了高效的多模态感知与语音重建模块。尽管参数量高达5600亿（激活参数270亿），该模型仍能实现低延迟的实时音视频交互。在训练基础设施方面，我们开发了专门应对大规模多模态训练中数据与模型异构性的模态解耦并行方案，这一创新方法通过维持纯文本训练90%以上的吞吐量，展现出卓越的效率优势。大量评估表明，LongCat-Flash-Omni在开源模型中实现了全模态基准测试的最优性能。此外，该模型在包括文本、图像、视频理解以及音频理解与生成等广泛模态专项任务中均展现出高度竞争力。我们全面阐述了模型架构设计、训练流程与数据策略，并将模型开源以促进学界未来的研究与发展。",
    "url": "https://huggingface.co/papers/2511.00279",
    "arxiv_url": "https://arxiv.org/abs/2511.00279"
  },
  {
    "title": "Towards Universal Video Retrieval: Generalizing Video Embedding via\n  Synthesized Multimodal Pyramid Curriculum",
    "summary": "The prevailing video retrieval paradigm is structurally misaligned, as narrow\nbenchmarks incentivize correspondingly limited data and single-task training.\nTherefore, universal capability is suppressed due to the absence of a\ndiagnostic evaluation that defines and demands multi-dimensional\ngeneralization. To break this cycle, we introduce a framework built on the\nco-design of evaluation, data, and modeling. First, we establish the Universal\nVideo Retrieval Benchmark (UVRB), a suite of 16 datasets designed not only to\nmeasure performance but also to diagnose critical capability gaps across tasks\nand domains. Second, guided by UVRB's diagnostics, we introduce a scalable\nsynthesis workflow that generates 1.55 million high-quality pairs to populate\nthe semantic space required for universality. Finally, we devise the Modality\nPyramid, a curriculum that trains our General Video Embedder (GVE) by\nexplicitly leveraging the latent interconnections within our diverse data.\nExtensive experiments show GVE achieves state-of-the-art zero-shot\ngeneralization on UVRB. In particular, our analysis reveals that popular\nbenchmarks are poor predictors of general ability and that partially relevant\nretrieval is a dominant but overlooked scenario. Overall, our co-designed\nframework provides a practical path to escape the limited scope and advance\ntoward truly universal video retrieval.",
    "translation": "标题：迈向通用视频检索：基于合成多模态金字塔课程学习的视频嵌入泛化方法\n\n摘要：当前主流视频检索范式存在结构性偏差，由于狭窄的基准测试催生了相应的有限数据和单任务训练模式，导致系统通用能力受到抑制——这源于缺乏能够定义并要求多维泛化能力的诊断性评估。为突破这一局限，我们提出了评估体系、数据构建与模型设计协同创新的框架。首先创建了通用视频检索基准（UVRB），该测试集包含16个数据集，不仅能衡量性能表现，更能诊断跨任务与跨领域的关键能力缺陷。其次，基于UVRB的诊断指导，我们开发了可扩展的合成工作流，生成155万高质量数据对以充实实现通用性所需的语义空间。最后设计了模态金字塔课程学习策略，通过显式利用异构数据间的潜在关联，训练出通用视频嵌入模型（GVE）。大量实验表明，GVE在UVRB上实现了零样本泛化的最先进性能。特别值得注意的是，我们的分析揭示了流行基准测试对通用能力的预测性较差，且部分相关检索是主流但被长期忽视的应用场景。总体而言，本协同设计框架为突破现有局限、迈向真正通用的视频检索提供了可行路径。",
    "url": "https://huggingface.co/papers/2510.27571",
    "arxiv_url": "https://arxiv.org/abs/2510.27571"
  },
  {
    "title": "TIR-Bench: A Comprehensive Benchmark for Agentic Thinking-with-Images\n  Reasoning",
    "summary": "The frontier of visual reasoning is shifting toward models like OpenAI o3,\nwhich can intelligently create and operate tools to transform images for\nproblem-solving, also known as thinking-with-images in\nchain-of-thought. Yet existing benchmarks fail to fully capture this advanced\ncapability. Even Visual Search, the most common benchmark for current\nthinking-with-images methods, tests only basic operations such as\nlocalization and cropping, offering little insight into more complex, dynamic,\nand tool-dependent reasoning. We introduce TIR-Bench, a comprehensive\nbenchmark for evaluating agentic thinking-with-images across 13 diverse tasks,\neach requiring novel tool use for image processing and manipulation in\nchain-of-thought. We evaluate 22 multimodal large language models (MLLMs), from\nleading open-sourced and proprietary models to those with explicit tool-use\naugmentation. Results show that TIR-Bench is universally challenging, and\nstrong performance requires genuine thinking-with-images capabilities. Finally,\nwe present a pilot study comparing direct versus agentic fine-tuning.",
    "translation": "标题：TIR-Bench：面向具身图像思维推理的综合评测基准\n\n摘要：视觉推理的研究前沿正转向OpenAI o3等模型，这类模型能智能创建并操作工具对图像进行转换以解决问题，即思维链中的图像思维推理。然而现有评测基准难以完整评估这种进阶能力。即便是当前图像思维方法最常用的评测基准Visual Search，也仅测试定位与裁剪等基础操作，无法深入探究更复杂、动态且依赖工具的推理能力。我们推出TIR-Bench这一综合评测基准，通过涵盖13类差异化任务来系统评估具身图像思维推理能力，每个任务均要求在思维链中运用创新工具进行图像处理与编辑。我们对22个多模态大语言模型（MLLMs）展开评估，涵盖领先的开源/闭源模型及经过显式工具增强的模型。实验表明TIR-Bench具有普适挑战性，优异表现需依赖真正的图像思维能力。最后，我们通过对照实验比较了直接微调与具身微调的效果差异。",
    "url": "https://huggingface.co/papers/2511.01833",
    "arxiv_url": "https://arxiv.org/abs/2511.01833"
  },
  {
    "title": "NaviTrace: Evaluating Embodied Navigation of Vision-Language Models",
    "summary": "Vision-language models demonstrate unprecedented performance and\ngeneralization across a wide range of tasks and scenarios. Integrating these\nfoundation models into robotic navigation systems opens pathways toward\nbuilding general-purpose robots. Yet, evaluating these models' navigation\ncapabilities remains constrained by costly real-world trials, overly simplified\nsimulations, and limited benchmarks. We introduce NaviTrace, a high-quality\nVisual Question Answering benchmark where a model receives an instruction and\nembodiment type (human, legged robot, wheeled robot, bicycle) and must output a\n2D navigation trace in image space. Across 1000 scenarios and more than 3000\nexpert traces, we systematically evaluate eight state-of-the-art VLMs using a\nnewly introduced semantic-aware trace score. This metric combines Dynamic Time\nWarping distance, goal endpoint error, and embodiment-conditioned penalties\nderived from per-pixel semantics and correlates with human preferences. Our\nevaluation reveals consistent gap to human performance caused by poor spatial\ngrounding and goal localization. NaviTrace establishes a scalable and\nreproducible benchmark for real-world robotic navigation. The benchmark and\nleaderboard can be found at\nhttps://leggedrobotics.github.io/navitrace_webpage/.",
    "translation": "标题：NaviTrace：视觉语言模型具身导航能力评估\n\n摘要：视觉语言模型在多样化任务与场景中展现出前所未有的性能表现与泛化能力。将这些基础模型集成到机器人导航系统中，为构建通用机器人开辟了新路径。然而，当前对这些模型导航能力的评估仍受限于昂贵的真实环境测试、过度简化的仿真环境以及有限的基准数据集。我们提出NaviTrace——一个高质量的视觉问答基准测试框架，模型在接收指令与具身类型（人类、足式机器人、轮式机器人、自行车）后，需在图像空间输出二维导航轨迹。基于1000个测试场景和3000余条专家轨迹，我们采用新提出的语义感知轨迹评分标准，对八种前沿视觉语言模型进行系统评估。该指标融合动态时间规整距离、目标端点误差，以及基于像素级语义与具身类型的惩罚项，并与人类偏好保持相关性。评估结果表明，由于空间定位与目标识别能力不足，现有模型与人类表现存在系统性差距。NaviTrace为真实环境机器人导航建立了可扩展、可复现的评估基准。测试集与排行榜详见：https://leggedrobotics.github.io/navitrace_webpage/。",
    "url": "https://huggingface.co/papers/2510.26909",
    "arxiv_url": "https://arxiv.org/abs/2510.26909"
  },
  {
    "title": "Trove: A Flexible Toolkit for Dense Retrieval",
    "summary": "We introduce Trove, an easy-to-use open-source retrieval toolkit that\nsimplifies research experiments without sacrificing flexibility or speed. For\nthe first time, we introduce efficient data management features that load and\nprocess (filter, select, transform, and combine) retrieval datasets on the fly,\nwith just a few lines of code. This gives users the flexibility to easily\nexperiment with different dataset configurations without the need to compute\nand store multiple copies of large datasets. Trove is highly customizable: in\naddition to many built-in options, it allows users to freely modify existing\ncomponents or replace them entirely with user-defined objects. It also provides\na low-code and unified pipeline for evaluation and hard negative mining, which\nsupports multi-node execution without any code changes. Trove's data management\nfeatures reduce memory consumption by a factor of 2.6. Moreover, Trove's\neasy-to-use inference pipeline incurs no overhead, and inference times decrease\nlinearly with the number of available nodes. Most importantly, we demonstrate\nhow Trove simplifies retrieval experiments and allows for arbitrary\ncustomizations, thus facilitating exploratory research.",
    "translation": "标题：Trove：面向稠密检索的灵活工具包\n\n摘要：本文推出Trove——一个易用的开源检索工具包，该工具能在保持灵活性与速度的同时简化研究实验。我们首次引入高效的数据管理功能，仅需少量代码即可动态加载和处理（筛选、选择、转换及合并）检索数据集。这使得用户能够灵活尝试不同数据集配置，无需计算和存储大型数据集的多个副本。Trove具备高度可定制性：除内置多种选项外，还允许用户自由修改现有组件或完全替换为自定义对象。该工具包同时提供支持多节点零代码改动的低代码统一评估流程与困难负例挖掘流程。Trove的数据管理功能可将内存消耗降低至原需求的2.6分之一，其易用的推理流程不会产生额外开销，且推理时间随可用节点数量呈线性下降。最重要的是，我们通过实验证明Trove能有效简化检索实验流程并支持任意定制需求，从而为探索性研究提供有力支持。",
    "url": "https://huggingface.co/papers/2511.01857",
    "arxiv_url": "https://arxiv.org/abs/2511.01857"
  },
  {
    "title": "left|,circlearrowright,text{BUS},right|: A Large and\n  Diverse Multimodal Benchmark for evaluating the ability of Vision-Language\n  Models to understand Rebus Puzzles",
    "summary": "Understanding Rebus Puzzles (Rebus Puzzles use pictures, symbols, and letters\nto represent words or phrases creatively) requires a variety of skills such as\nimage recognition, cognitive skills, commonsense reasoning, multi-step\nreasoning, image-based wordplay, etc., making this a challenging task for even\ncurrent Vision-Language Models. In this paper, we present\nleft|,circlearrowright,text{BUS},right|, a large and diverse\nbenchmark of 1,333 English Rebus Puzzles containing different artistic styles\nand levels of difficulty, spread across 18 categories such as food, idioms,\nsports, finance, entertainment, etc. We also propose RebusDescProgICE, a\nmodel-agnostic framework which uses a combination of an unstructured\ndescription and code-based, structured reasoning, along with better,\nreasoning-based in-context example selection, improving the performance of\nVision-Language Models on\nleft|,circlearrowright,text{BUS},right| by 2.1-4.1% and\n20-30% using closed-source and open-source models respectively compared to\nChain-of-Thought Reasoning.",
    "translation": "标题：左|,↻,文本{BUS},右|：一个用于评估视觉语言模型理解画谜能力的大规模多样化多模态基准\n\n摘要：理解画谜（画谜通过图片、符号和字母来创造性地表达词语或短语）需要多种技能，如图像识别、认知能力、常识推理、多步推理、基于图像的文字游戏等，这使得即使对当前的视觉语言模型而言也是一项具有挑战性的任务。本文提出左|,↻,文本{BUS},右|，一个包含1,333个英语画谜的大规模多样化基准，涵盖不同艺术风格和难度级别，分布于18个类别（如食物、习语、体育、金融、娱乐等）。我们还提出了RebusDescProgICE，一个模型无关的框架，结合非结构化描述和基于代码的结构化推理，并采用更优的基于推理的上下文示例选择方法，将视觉语言模型在左|,↻,文本{BUS},右|上的性能相比思维链推理分别提升了2.1-4.1%（闭源模型）和20-30%（开源模型）。",
    "url": "https://huggingface.co/papers/2511.01340",
    "arxiv_url": "https://arxiv.org/abs/2511.01340"
  },
  {
    "title": "Do Vision-Language Models Measure Up? Benchmarking Visual Measurement\n  Reading with MeasureBench",
    "summary": "Reading measurement instruments is effortless for humans and requires\nrelatively little domain expertise, yet it remains surprisingly challenging for\ncurrent vision-language models (VLMs) as we find in preliminary evaluation. In\nthis work, we introduce MeasureBench, a benchmark on visual measurement reading\ncovering both real-world and synthesized images of various types of\nmeasurements, along with an extensible pipeline for data synthesis. Our\npipeline procedurally generates a specified type of gauge with controllable\nvisual appearance, enabling scalable variation in key details such as pointers,\nscales, fonts, lighting, and clutter. Evaluation on popular proprietary and\nopen-weight VLMs shows that even the strongest frontier VLMs struggle\nmeasurement reading in general. A consistent failure mode is indicator\nlocalization: models can read digits or labels but misidentify the key\npositions of pointers or alignments, leading to big numeric errors despite\nplausible textual reasoning. We have also conducted preliminary experiments\nwith reinforcement learning over synthetic data, and find encouraging results\non in-domain synthetic subset but less promising for real-world images. Our\nanalysis highlights a fundamental limitation of current VLMs in fine-grained\nspatial grounding. We hope this resource can help future advances on visually\ngrounded numeracy and precise spatial perception of VLMs, bridging the gap\nbetween recognizing numbers and measuring the world.",
    "translation": "标题：视觉语言模型能否胜任测量任务？基于MeasureBench的视觉测量读数基准测试\n\n摘要：对人类而言，读取测量仪器读数轻松自如且所需领域专业知识较少，但我们在初步评估中发现，这项任务对当前视觉语言模型（VLMs）仍具有惊人挑战性。本研究推出MeasureBench——一个涵盖真实场景与合成图像的综合性视觉测量读数基准，同时提供可扩展的数据合成流程。该流程能程序化生成具有可控视觉特征的指定类型仪表，实现对指针、刻度、字体、光照及背景干扰等关键细节的大规模参数化调整。对主流专有模型和开源权重的VLMs评估表明，即使最先进的尖端模型在测量读数任务中仍普遍表现不佳。模型存在一致的失败模式：虽然能够识别数字或标签，但无法准确定位指针或对齐标记的关键位置，导致尽管文本推理看似合理却产生巨大数值误差。我们还在合成数据上开展了强化学习的初步实验，发现在域内合成子集上获得鼓舞人心的结果，但对真实场景图像的泛化能力仍有不足。本分析揭示了当前VLMs在细粒度空间定位方面的根本性局限。我们期望该资源能推动视觉基础计算能力与精确空间感知方面的研究进展，弥合数字识别与物理世界测量之间的能力鸿沟。",
    "url": "https://huggingface.co/papers/2510.26865",
    "arxiv_url": "https://arxiv.org/abs/2510.26865"
  },
  {
    "title": "Actial: Activate Spatial Reasoning Ability of Multimodal Large Language\n  Models",
    "summary": "Recent advances in Multimodal Large Language Models (MLLMs) have\nsignificantly improved 2D visual understanding, prompting interest in their\napplication to complex 3D reasoning tasks. However, it remains unclear whether\nthese models can effectively capture the detailed spatial information required\nfor robust real-world performance, especially cross-view consistency, a key\nrequirement for accurate 3D reasoning. Considering this issue, we introduce\nViewpoint Learning, a task designed to evaluate and improve the spatial\nreasoning capabilities of MLLMs. We present the Viewpoint-100K dataset,\nconsisting of 100K object-centric image pairs with diverse viewpoints and\ncorresponding question-answer pairs. Our approach employs a two-stage\nfine-tuning strategy: first, foundational knowledge is injected to the baseline\nMLLM via Supervised Fine-Tuning (SFT) on Viewpoint-100K, resulting in\nsignificant improvements across multiple tasks; second, generalization is\nenhanced through Reinforcement Learning using the Group Relative Policy\nOptimization (GRPO) algorithm on a broader set of questions. Additionally, we\nintroduce a hybrid cold-start initialization method designed to simultaneously\nlearn viewpoint representations and maintain coherent reasoning thinking.\nExperimental results show that our approach significantly activates the spatial\nreasoning ability of MLLM, improving performance on both in-domain and\nout-of-domain reasoning tasks. Our findings highlight the value of developing\nfoundational spatial skills in MLLMs, supporting future progress in robotics,\nautonomous systems, and 3D scene understanding.",
    "translation": "标题：Actial：激活多模态大语言模型的空间推理能力\n\n摘要：多模态大语言模型（MLLMs）在二维视觉理解方面取得显著进展，这促使研究者探索其在复杂三维推理任务中的应用潜力。然而，这些模型是否能有效捕捉现实场景中稳健性能所需的精细空间信息——尤其是跨视角一致性这一三维推理的关键要求——仍不明确。针对这一问题，我们提出视角学习任务，旨在评估和提升MLLMs的空间推理能力。我们构建了包含10万组以物体为中心的多视角图像及对应问答对的Viewpoint-100K数据集。该方法采用两阶段微调策略：首先通过监督微调（SFT）将基础空间知识注入基线MLLM，使其在多项任务中取得显著提升；随后基于群组相对策略优化（GRPO）算法，在更广泛的问题集上通过强化学习增强模型泛化能力。此外，我们提出混合冷启动初始化方法，可同步学习视角表征并保持连贯推理思维。实验结果表明，我们的方法显著激活了MLLMs的空间推理能力，在领域内和跨领域推理任务中均表现出性能提升。本研究凸显了培养MLLMs基础空间技能的重要价值，为机器人技术、自主系统和三维场景理解等领域的未来发展提供支撑。",
    "url": "https://huggingface.co/papers/2511.01618",
    "arxiv_url": "https://arxiv.org/abs/2511.01618"
  },
  {
    "title": "Towards Robust Mathematical Reasoning",
    "summary": "Finding the right north-star metrics is highly critical for advancing the\nmathematical reasoning capabilities of foundation models, especially given that\nexisting evaluations are either too easy or only focus on getting correct short\nanswers. To address these issues, we present IMO-Bench, a suite of advanced\nreasoning benchmarks, vetted by a panel of top specialists and that\nspecifically targets the level of the International Mathematical Olympiad\n(IMO), the most prestigious venue for young mathematicians. IMO-AnswerBench\nfirst tests models on 400 diverse Olympiad problems with verifiable short\nanswers. IMO-Proof Bench is the next-level evaluation for proof-writing\ncapabilities, which includes both basic and advanced IMO level problems as well\nas detailed grading guidelines to facilitate automatic grading. These\nbenchmarks played a crucial role in our historic achievement of the gold-level\nperformance at IMO 2025 with Gemini Deep Think (Luong and Lockhart, 2025). Our\nmodel achieved 80.0% on IMO-AnswerBench and 65.7% on the advanced IMO-Proof\nBench, surpassing the best non-Gemini models by large margins of 6.9% and 42.4%\nrespectively. We also showed that autograders built with Gemini reasoning\ncorrelate well with human evaluations and construct IMO-GradingBench, with 1000\nhuman gradings on proofs, to enable further progress in automatic evaluation of\nlong-form answers. We hope that IMO-Bench will help the community towards\nadvancing robust mathematical reasoning and release it at\nhttps://imobench.github.io/.",
    "translation": "标题：迈向稳健的数学推理\n\n摘要：确立正确的北极星指标对提升基础模型的数学推理能力至关重要，尤其是考虑到现有评估方法要么过于简单，要么仅关注获取简答题的正确答案。为解决这些问题，我们提出IMO-Bench——一套经顶尖专家团队审核的高级推理基准测试集，专门针对年轻数学家最高殿堂国际数学奥林匹克（IMO）的难度级别设计。IMO-AnswerBench首阶段包含400道可验证简短答案的多样化奥数题目，IMO-ProofBench则针对证明能力进行进阶评估，涵盖基础与高级IMO难度题目，并配备详细评分标准以实现自动化评分。这些基准在我们通过Gemini Deep Think模型（Luong与Lockhart，2025）实现IMO 2025金奖的历史性突破中发挥了关键作用。该模型在IMO-AnswerBench达到80.0%准确率，在高级IMO-ProofBench获得65.7%得分，分别以6.9%和42.4%的显著优势超越非Gemini最佳模型。我们还证实基于Gemini推理构建的自动评分器与人工评估高度相关，并构建包含1000份证明人工评分的IMO-GradingBench，以推动长文本答案自动评估的发展。我们期待IMO-Bench能助力学界推进稳健数学推理研究，相关资源已发布于https://imobench.github.io/。",
    "url": "https://huggingface.co/papers/2511.01846",
    "arxiv_url": "https://arxiv.org/abs/2511.01846"
  },
  {
    "title": "Data-Efficient RLVR via Off-Policy Influence Guidance",
    "summary": "Data selection is a critical aspect of Reinforcement Learning with Verifiable\nRewards (RLVR) for enhancing the reasoning capabilities of large language\nmodels (LLMs). Current data selection methods are largely heuristic-based,\nlacking theoretical guarantees and generalizability. This work proposes a\ntheoretically-grounded approach using influence functions to estimate the\ncontribution of each data point to the learning objective. To overcome the\nprohibitive computational cost of policy rollouts required for online influence\nestimation, we introduce an off-policy influence estimation method that\nefficiently approximates data influence using pre-collected offline\ntrajectories. Furthermore, to manage the high-dimensional gradients of LLMs, we\nemploy sparse random projection to reduce dimensionality and improve storage\nand computation efficiency. Leveraging these techniques, we develop\nCurriculum RL with Off-Policy\nInfluence guidance (CROPI), a multi-stage RL framework that\niteratively selects the most influential data for the current policy.\nExperiments on models up to 7B parameters demonstrate that CROPI significantly\naccelerates training. On a 1.5B model, it achieves a 2.66x step-level\nacceleration while using only 10\\% of the data per stage compared to\nfull-dataset training. Our results highlight the substantial potential of\ninfluence-based data selection for efficient RLVR.",
    "translation": "标题：基于离线策略影响指导的数据高效RLVR方法\n\n摘要：数据选择在可验证奖励强化学习（RLVR）中对于提升大语言模型（LLM）的推理能力具有关键作用。当前的数据选择方法主要基于启发式策略，缺乏理论保证和普适性。本研究提出一种基于影响函数的理论方法，通过量化每个数据点对学习目标的贡献度来指导数据选择。为解决在线影响评估中策略推演所需的过高计算成本，我们引入了离线策略影响评估方法，利用预先收集的离线轨迹高效近似数据影响力。针对大语言模型的高维梯度特性，采用稀疏随机投影技术降低维度，提升存储和计算效率。基于这些技术，我们开发了具有离线策略影响指导的课程强化学习框架（CROPI）——一种多阶段强化学习框架，能够迭代选择对当前策略最具影响力的数据。在参数量达70亿的模型实验表明，CROPI能显著加速训练过程。在15亿参数模型上，相较于全数据集训练，该方法仅使用每阶段10%的数据就实现了2.66倍的步骤级加速。我们的研究成果充分证明了基于影响度的数据选择方法在高效RLVR领域的巨大潜力。",
    "url": "https://huggingface.co/papers/2510.26491",
    "arxiv_url": "https://arxiv.org/abs/2510.26491"
  },
  {
    "title": "How Far Are Surgeons from Surgical World Models? A Pilot Study on\n  Zero-shot Surgical Video Generation with Expert Assessment",
    "summary": "Foundation models in video generation are demonstrating remarkable\ncapabilities as potential world models for simulating the physical world.\nHowever, their application in high-stakes domains like surgery, which demand\ndeep, specialized causal knowledge rather than general physical rules, remains\na critical unexplored gap. To systematically address this challenge, we present\nSurgVeo, the first expert-curated benchmark for video generation model\nevaluation in surgery, and the Surgical Plausibility Pyramid (SPP), a novel,\nfour-tiered framework tailored to assess model outputs from basic appearance to\ncomplex surgical strategy. On the basis of the SurgVeo benchmark, we task the\nadvanced Veo-3 model with a zero-shot prediction task on surgical clips from\nlaparoscopic and neurosurgical procedures. A panel of four board-certified\nsurgeons evaluates the generated videos according to the SPP. Our results\nreveal a distinct \"plausibility gap\": while Veo-3 achieves exceptional Visual\nPerceptual Plausibility, it fails critically at higher levels of the SPP,\nincluding Instrument Operation Plausibility, Environment Feedback Plausibility,\nand Surgical Intent Plausibility. This work provides the first quantitative\nevidence of the chasm between visually convincing mimicry and causal\nunderstanding in surgical AI. Our findings from SurgVeo and the SPP establish a\ncrucial foundation and roadmap for developing future models capable of\nnavigating the complexities of specialized, real-world healthcare domains.",
    "translation": "标题：外科医生距离手术世界模型还有多远？基于专家评估的零样本手术视频生成试点研究\n\n摘要：视频生成基础模型作为模拟物理世界的潜在世界模型，正展现出卓越的能力。然而，其在手术等高风险领域的应用仍存在关键空白——这些领域需要深度专业化的因果知识而非通用物理规则。为系统应对这一挑战，我们提出SurgVeo（首个经专家审定的手术视频生成模型评估基准）及手术合理性金字塔（SPP），这是一个新颖的四层级评估框架，专门用于从基础表象到复杂手术策略的模型输出评估。基于SurgVeo基准，我们让先进的Veo-3模型对腹腔镜与神经外科手术片段进行零样本预测任务，并由四位认证外科专家委员会根据SPP框架对生成视频进行评估。研究结果揭示出显著的“合理性鸿沟”：虽然Veo-3在视觉感知合理性层面表现卓越，但在SPP更高层级（包括器械操作合理性、环境反馈合理性与手术意图合理性）存在严重缺陷。这项研究首次为手术AI领域提供了视觉逼真模仿与因果理解之间存在鸿沟的量化证据。通过SurgVeo与SPP获得的发现，为开发能够驾驭专业现实医疗领域复杂性的未来模型奠定了关键基础并指明了发展路径。",
    "url": "https://huggingface.co/papers/2511.01775",
    "arxiv_url": "https://arxiv.org/abs/2511.01775"
  },
  {
    "title": "Unified Diffusion VLA: Vision-Language-Action Model via Joint Discrete\n  Denoising Diffusion Process",
    "summary": "Vision-language-action (VLA) models aim to understand natural language\ninstructions and visual observations and to execute corresponding actions as an\nembodied agent. Recent work integrates future images into the\nunderstanding-acting loop, yielding unified VLAs that jointly understand,\ngenerate, and act -- reading text and images and producing future images and\nactions. However, these models either rely on external experts for modality\nunification or treat image generation and action prediction as separate\nprocesses, limiting the benefits of direct synergy between these tasks. Our\ncore philosophy is to optimize generation and action jointly through a\nsynchronous denoising process, where the iterative refinement enables actions\nto evolve from initialization, under constant and sufficient visual guidance.\nWe ground this philosophy in our proposed Unified Diffusion VLA and Joint\nDiscrete Denoising Diffusion Process (JD3P), which is a joint diffusion process\nthat integrates multiple modalities into a single denoising trajectory to serve\nas the key mechanism enabling understanding, generation, and acting to be\nintrinsically synergistic. Our model and theory are built on a unified\ntokenized space of all modalities and a hybrid attention mechanism. We further\npropose a two-stage training pipeline and several inference-time techniques\nthat optimize performance and efficiency. Our approach achieves\nstate-of-the-art performance on benchmarks such as CALVIN, LIBERO, and\nSimplerEnv with 4times faster inference than autoregressive methods, and we\ndemonstrate its effectiveness through in-depth analysis and real-world\nevaluations. Our project page is available at\nhttps://irpn-eai.github.io/UD-VLA.github.io/.",
    "translation": "标题：统一扩散视觉语言行动模型：基于联合离散去噪扩散过程的多模态协同框架  \n\n摘要：视觉-语言-行动模型旨在理解自然语言指令与视觉观测信息，并作为具身智能体执行相应动作。近期研究将未来图像预测纳入感知-行动循环，构建出能协同理解、生成与行动的统一模型——这些模型可同时处理文本与图像输入，并输出未来图像与动作序列。然而，现有方法或依赖外部专家进行模态对齐，或将图像生成与动作预测视为独立过程，限制了任务间直接协同的效益。本研究的核心理念是通过同步去噪过程实现生成与行动的联合优化，在持续充分的视觉引导下，通过迭代优化使动作从初始状态逐步演进。基于此，我们提出统一扩散视觉语言行动模型与联合离散去噪扩散过程：该联合扩散框架将多模态信息融入单一去噪轨迹，使理解、生成与行动形成本质性协同。我们的模型与理论建立于统一的多模态令牌空间与混合注意力机制之上，进一步提出两阶段训练流程及多项推理优化技术以平衡性能与效率。在CALVIN、LIBERO和SimplerEnv等基准测试中，本方法以比自回归模型快4倍的推理速度达到最优性能，并通过深度分析与真实场景验证了其有效性。项目页面详见：https://irpn-eai.github.io/UD-VLA.github.io/。",
    "url": "https://huggingface.co/papers/2511.01718",
    "arxiv_url": "https://arxiv.org/abs/2511.01718"
  },
  {
    "title": "MotionStream: Real-Time Video Generation with Interactive Motion\n  Controls",
    "summary": "Current motion-conditioned video generation methods suffer from prohibitive\nlatency (minutes per video) and non-causal processing that prevents real-time\ninteraction. We present MotionStream, enabling sub-second latency with up to 29\nFPS streaming generation on a single GPU. Our approach begins by augmenting a\ntext-to-video model with motion control, which generates high-quality videos\nthat adhere to the global text prompt and local motion guidance, but does not\nperform inference on the fly. As such, we distill this bidirectional teacher\ninto a causal student through Self Forcing with Distribution Matching\nDistillation, enabling real-time streaming inference. Several key challenges\narise when generating videos of long, potentially infinite time-horizons: (1)\nbridging the domain gap from training on finite length and extrapolating to\ninfinite horizons, (2) sustaining high quality by preventing error\naccumulation, and (3) maintaining fast inference, without incurring growth in\ncomputational cost due to increasing context windows. A key to our approach is\nintroducing carefully designed sliding-window causal attention, combined with\nattention sinks. By incorporating self-rollout with attention sinks and KV\ncache rolling during training, we properly simulate inference-time\nextrapolations with a fixed context window, enabling constant-speed generation\nof arbitrarily long videos. Our models achieve state-of-the-art results in\nmotion following and video quality while being two orders of magnitude faster,\nuniquely enabling infinite-length streaming. With MotionStream, users can paint\ntrajectories, control cameras, or transfer motion, and see results unfold in\nreal-time, delivering a truly interactive experience.",
    "translation": "标题：MotionStream：基于交互式运动控制的实时视频生成技术  \n\n摘要：当前基于运动条件的视频生成方法存在延迟过高（每分钟生成数帧）及非因果性处理的问题，导致无法实现实时交互。本文提出的MotionStream框架在单GPU上实现了亚秒级延迟与最高29 FPS的流式生成。我们首先通过运动控制增强文本到视频模型，使其生成既符合全局文本提示又遵循局部运动引导的高质量视频，但该模型无法实现实时推理。为此，我们通过基于分布匹配蒸馏的自强制学习，将这种双向教师模型蒸馏为因果性学生模型，从而实现实时流式推理。在生成长时间乃至无限时长视频时面临三大挑战：（1）如何弥合有限时长训练与无限时长外推之间的领域差异；（2）如何通过抑制误差累积保持生成质量；（3）如何在不断增长的上下文窗口下维持快速推理而不增加计算成本。本方法的核心在于引入精心设计的滑动窗口因果注意力机制与注意力锚点技术。通过训练过程中结合注意力锚点的自展开策略和KV缓存滚动机制，我们以固定上下文窗口准确模拟推理时的外推过程，实现任意长度视频的恒速生成。该模型在运动跟随性与视频质量方面达到最先进水平，同时将生成速度提升两个数量级，独树一帜地实现了无限长度流式生成。借助MotionStream，用户可通过绘制轨迹、控制摄像机或迁移运动等方式实时观察生成结果，获得真正的交互体验。",
    "url": "https://huggingface.co/papers/2511.01266",
    "arxiv_url": "https://arxiv.org/abs/2511.01266"
  },
  {
    "title": "UME-R1: Exploring Reasoning-Driven Generative Multimodal Embeddings",
    "summary": "The remarkable success of multimodal large language models (MLLMs) has driven\nadvances in multimodal embeddings, yet existing models remain inherently\ndiscriminative, limiting their ability to benefit from reasoning-driven\ngeneration paradigm. In this work, we pioneer the exploration of generative\nembeddings, unifying embedding tasks within a generative paradigm. We propose\nUME-R1, a universal multimodal embedding framework consisting of a two-stage\ntraining strategy: a cold-start supervised fine-tuning equips the model with\nreasoning capabilities and enables it to generate both discriminative and\ngenerative embeddings; a subsequent reinforcement learning enhances reasoning\nand further optimizes generative embedding quality. This pioneering work\nreveals four key insights: 1) generative embeddings unlock substantial\nperformance gains over conventional discriminative embeddings by leveraging the\npowerful generative reasoning capabilities of MLLMs; 2) discriminative and\ngenerative embeddings are complementary, whose combined oracle performance far\nexceeding that of either alone; 3) RL can effectively enhance generative\nembeddings, establishing a scalable optimization paradigm.; 4) repeated\nsampling at inference boosts downstream task coverage (pass@k), highlighting\nthe inference-time scalability potential of generative embeddings. Evaluated on\nthe MMEB-V2 benchmark across 78 tasks spanning video, image, and visual\ndocuments, UME-R1 significantly outperforms conventional discriminative\nembedding models and offers a foundation for more interpretable,\nreasoning-driven generative multimodal embeddings. Our code, models, and\ndatasets will be publicly available at https://github.com/XMUDeepLIT/UME-R1.",
    "translation": "标题：UME-R1：探索推理驱动的生成式多模态嵌入方法\n\n摘要：多模态大语言模型（MLLMs）取得的显著成功推动了多模态嵌入技术的发展，然而现有模型本质上仍属于判别式模型，限制了其从推理驱动生成范式中获益的能力。本研究开创性地探索生成式嵌入方法，将嵌入任务统一于生成范式之中。我们提出UME-R1——一个通用多模态嵌入框架，采用两阶段训练策略：通过冷启动监督微调阶段赋予模型推理能力，使其能同时生成判别式与生成式嵌入；后续强化学习阶段则增强推理能力并进一步优化生成式嵌入质量。这项开创性研究揭示了四个关键发现：1）通过利用MLLMs强大的生成推理能力，生成式嵌入相较传统判别式嵌入实现了显著性能提升；2）判别式与生成式嵌入具有互补性，二者组合的预言机性能远超单一模式；3）强化学习能有效增强生成式嵌入，建立了可扩展的优化范式；4）推理阶段的重采样策略可提升下游任务覆盖度（pass@k），彰显了生成式嵌入在推理时的可扩展潜力。在涵盖视频、图像及视觉文档的78个任务MMEB-V2基准测试中，UME-R1显著优于传统判别式嵌入模型，为构建更具可解释性的推理驱动生成式多模态嵌入奠定了坚实基础。相关代码、模型及数据集将在https://github.com/XMUDeepLIT/UME-R1 公开。",
    "url": "https://huggingface.co/papers/2511.00405",
    "arxiv_url": "https://arxiv.org/abs/2511.00405"
  },
  {
    "title": "Vote-in-Context: Turning VLMs into Zero-Shot Rank Fusers",
    "summary": "In the retrieval domain, candidates' fusion from heterogeneous retrievers is\na long-standing challenge, particularly for complex, multi-modal data such as\nvideos. While typical fusion techniques are training-free, they rely solely on\nrank or score signals, disregarding candidates' representations. This work\nintroduces Vote-in-Context (ViC), a generalized, training-free framework that\nre-thinks list-wise reranking and fusion as a zero-shot reasoning task for a\nVision-Language Model (VLM). The core insight is to serialize both content\nevidence and retriever metadata directly within the VLM's prompt, allowing the\nmodel to adaptively weigh retriever consensus against visual-linguistic\ncontent. We demonstrate the generality of this framework by applying it to the\nchallenging domain of cross-modal video retrieval. To this end, we introduce\nthe S-Grid, a compact serialization map that represents each video as an image\ngrid, optionally paired with subtitles to enable list-wise reasoning over video\ncandidates. ViC is evaluated both as a single-list reranker, where it\ndramatically improves the precision of individual retrievers, and as an\nensemble fuser, where it consistently outperforms strong baselines like\nCombSUM. Across video retrieval benchmarks including ActivityNet and VATEX, the\nframework establishes new state-of-the-art zero-shot retrieval performance,\ndemonstrating its effectiveness in handling complex visual and temporal signals\nalongside text. In zero-shot settings, ViC achieves Recall@1 scores of 87.1%\n(t2v) / 89.0% (v2t) on MSR-VTT and 99.6% (v2t) on VATEX, representing massive\ngains of up to +40 Recall@1 over previous state-of-the-art baselines. We\npresent ViC as a simple, reproducible, and highly effective recipe for turning\nmodern VLMs into powerful zero-shot rerankers and fusers. Code and resources\nare publicly available at: https://github.com/mohammad2012191/ViC",
    "translation": "标题：上下文投票机制：将视觉语言模型转化为零样本排序融合器\n\n摘要：在检索领域，异构检索器的候选结果融合长期面临挑战，尤其在处理视频等复杂多模态数据时更为突出。传统融合技术虽无需训练，但仅依赖排序或分数信号，忽略了候选表征本身。本研究提出上下文投票机制（ViC），这一通用化免训练框架将列表重排序与融合重新定义为视觉语言模型的零样本推理任务。其核心创新在于将内容证据与检索器元数据直接序列化嵌入VLM提示中，使模型能自适应权衡检索器共识与视觉语言内容。我们通过跨模态视频检索这一挑战性任务验证该框架的通用性，为此引入S-Grid紧凑序列化图谱——将每个视频表示为图像网格，并可选择性搭配字幕以实现视频候选的列表推理。实验表明，ViC作为单列表重排序器能显著提升个体检索器精度，作为集成融合器则持续优于CombSUM等强基线方法。在ActivityNet、VATEX等视频检索基准测试中，该框架创造了零样本检索性能的新标杆，展现出处理复杂视觉、时序信号与文本的卓越能力。在零样本设定下，ViC于MSR-VTT数据集获得87.1%（文本到视频）/89.0%（视频到文本）的Recall@1分数，在VATEX数据集实现99.6%（视频到文本）的Recall@1分数，较先前最优基线提升高达+40个Recall@1百分点。本研究提出的ViC为将现代VLM转化为强大零样本重排序与融合器提供了简洁、可复现的高效方案。代码与资源已开源：https://github.com/mohammad2012191/ViC",
    "url": "https://huggingface.co/papers/2511.01617",
    "arxiv_url": "https://arxiv.org/abs/2511.01617"
  },
  {
    "title": "GUI-AIMA: Aligning Intrinsic Multimodal Attention with a Context Anchor\n  for GUI Grounding",
    "summary": "Graphical user interface (GUI) grounding is a key function of computer-use\nagents, which maps natural-language instructions to actionable screen regions.\nExisting approaches based on Multimodal Large Language Models (MLLMs) typically\nformulate it as a text-based coordinate generation task, yet directly\ngenerating precise coordinates from visual inputs remains challenging and\ncomputationally intensive. An intuitive way to implement GUI grounding is to\nfirst select visual patches relevant to the instructions and then determine the\nprecise click location within those patches. Based on the observations that\ngeneral MLLMs have some native grounding capability, nested within their\nattentions, we propose GUI-AIMA, an attention-based and coordinate-free\nsupervised fine-tuning framework for efficient GUI grounding. GUI-AIMA aligns\nthe intrinsic multimodal attention of MLLMs with patch-wise grounding signals.\nThese signals are calculated adaptively for diverse user instructions by\nmulti-head aggregation on simplified query-visual attention matrices. Besides,\nits coordinate-free manner can easily integrate a plug-and-play zoom-in stage.\nGUI-AIMA-3B was trained with only 85k screenshots, demonstrating exceptional\ndata efficiency and verifying that light training can trigger the native\ngrounding capability of MLLMs. It achieves state-of-the-art performance among\n3B models, attaining an average accuracy of 58.6% on ScreenSpot-Pro and 62.2%\non OSWorld-G. Project page: https://github.com/sjz5202/GUI-AIMA",
    "translation": "标题：GUI-AIMA：基于上下文锚点的图形用户界面多模态注意力对齐方法\n\n摘要：图形用户界面（GUI）定位是计算机使用代理的核心功能，其将自然语言指令映射至可操作的屏幕区域。现有基于多模态大语言模型（MLLM）的方法通常将其建模为基于文本的坐标生成任务，但直接从视觉输入生成精确坐标仍存在挑战且计算成本高昂。一种直观的GUI定位实现方式是先筛选与指令相关的视觉区块，再在这些区块内确定精确点击位置。基于通用MLLM的注意力机制中天然蕴含基础定位能力的观察，我们提出GUI-AIMA——一种基于注意力机制且无需坐标监督的高效GUI定位微调框架。该框架通过将MLLM固有的多模态注意力与区块级定位信号对齐，采用多头聚合算法在简化的查询-视觉注意力矩阵上自适应计算多样化用户指令的定位信号。此外，其无坐标特性可轻松集成即插即用的局部放大模块。仅使用8.5万张屏幕截图训练的GUI-AIMA-3B模型展现出卓越的数据效率，验证了轻量训练即可激发MLLM的先天定位能力。该模型在3B参数规模中达到最先进性能，在ScreenSpot-Pro和OSWorld-G数据集上分别取得58.6%和62.2%的平均准确率。项目主页：https://github.com/sjz5202/GUI-AIMA",
    "url": "https://huggingface.co/papers/2511.00810",
    "arxiv_url": "https://arxiv.org/abs/2511.00810"
  },
  {
    "title": "Multi-Step Knowledge Interaction Analysis via Rank-2 Subspace\n  Disentanglement",
    "summary": "Natural Language Explanations (NLEs) describe how Large Language Models\n(LLMs) make decisions, drawing on both external Context Knowledge (CK) and\nParametric Knowledge (PK) stored in model weights. Understanding their\ninteraction is key to assessing the grounding of NLEs, yet it remains\nunderexplored. Prior work has largely examined only single-step generation,\ntypically the final answer, and has modelled PK and CK interaction only as a\nbinary choice in a rank-1 subspace. This overlooks richer forms of interaction,\nsuch as complementary or supportive knowledge. We propose a novel rank-2\nprojection subspace that disentangles PK and CK contributions more accurately\nand use it for the first multi-step analysis of knowledge interactions across\nlonger NLE sequences. Experiments on four QA datasets and three open-weight\ninstruction-tuned LLMs show that diverse knowledge interactions are poorly\nrepresented in a rank-1 subspace but are effectively captured in our rank-2\nformulation. Our multi-step analysis reveals that hallucinated NLEs align\nstrongly with the PK direction, context-faithful ones balance PK and CK, and\nChain-of-Thought prompting for NLEs shifts generated NLEs toward CK by reducing\nPK reliance. This work provides the first framework for systematic studies of\nmulti-step knowledge interactions in LLMs through a richer rank-2 subspace\ndisentanglement. Code and data:\nhttps://github.com/copenlu/pk-ck-knowledge-disentanglement.",
    "translation": "标题：基于秩-2子空间解耦的多步知识交互分析\n\n摘要：自然语言解释通过结合外部语境知识与存储在模型权重中的参数知识，描述大语言模型的决策机制。理解这两种知识的交互作用是评估自然语言解释可靠性的关键，但目前研究尚不充分。现有工作大多仅考察单步生成（通常是最终答案），并将参数知识与语境知识的交互建模为秩-1子空间中的二元选择，这忽略了更丰富的交互形式（如知识互补或协同支持）。我们提出了一种新颖的秩-2投影子空间方法，能够更精确地解耦参数知识与语境知识的贡献，并首次实现针对长序列自然语言解释的多步知识交互分析。在四个问答数据集和三个开源指令微调大语言模型上的实验表明：秩-1子空间难以有效表征多样化的知识交互，而我们的秩-2模型能准确捕捉这些交互特征。多步分析揭示：产生幻觉的自然语言解释显著偏向参数知识方向，上下文可信的解释则平衡参数知识与语境知识，而思维链提示通过降低对参数知识的依赖使生成结果向语境知识偏移。本研究通过构建更丰富的秩-2子空间解耦框架，为系统研究大语言模型中的多步知识交互提供了首个方法论基础。代码与数据详见：https://github.com/copenlu/pk-ck-knowledge-disentanglement",
    "url": "https://huggingface.co/papers/2511.01706",
    "arxiv_url": "https://arxiv.org/abs/2511.01706"
  },
  {
    "title": "AthenaBench: A Dynamic Benchmark for Evaluating LLMs in Cyber Threat\n  Intelligence",
    "summary": "Large Language Models (LLMs) have demonstrated strong capabilities in natural\nlanguage reasoning, yet their application to Cyber Threat Intelligence (CTI)\nremains limited. CTI analysis involves distilling large volumes of unstructured\nreports into actionable knowledge, a process where LLMs could substantially\nreduce analyst workload. CTIBench introduced a comprehensive benchmark for\nevaluating LLMs across multiple CTI tasks. In this work, we extend CTIBench by\ndeveloping AthenaBench, an enhanced benchmark that includes an improved dataset\ncreation pipeline, duplicate removal, refined evaluation metrics, and a new\ntask focused on risk mitigation strategies. We evaluate twelve LLMs, including\nstate-of-the-art proprietary models such as GPT-5 and Gemini-2.5 Pro, alongside\nseven open-source models from the LLaMA and Qwen families. While proprietary\nLLMs achieve stronger results overall, their performance remains subpar on\nreasoning-intensive tasks, such as threat actor attribution and risk\nmitigation, with open-source models trailing even further behind. These\nfindings highlight fundamental limitations in the reasoning capabilities of\ncurrent LLMs and underscore the need for models explicitly tailored to CTI\nworkflows and automation.",
    "translation": "标题：AthenaBench：面向网络威胁情报领域大语言模型评估的动态基准框架\n\n摘要：大语言模型在自然语言推理方面展现出强大能力，但其在网络威胁情报领域的应用仍存在局限。CTI分析需要从海量非结构化报告中提炼可操作知识，这一过程本可通过大语言模型显著减轻分析人员工作负荷。CTIBench曾提出用于评估大语言模型在多任务CTI场景下性能的综合基准。本研究通过开发AthenaBench对CTIBench进行扩展，该增强版基准具备以下特征：改进的数据集构建流程、重复数据消除机制、优化的评估指标体系，以及新增专注于风险缓解策略的分析任务。我们评估了十二个大语言模型，包括GPT-5和Gemini-2.5 Pro等尖端专有模型，以及来自LLaMA和Qwen系列的七个开源模型。实验表明，虽然专有模型整体表现更优，但在威胁行为归因和风险缓解等需要深度推理的任务中仍显不足，开源模型则存在更大差距。这些发现揭示了当前大语言模型推理能力的根本局限，强调需要专门针对CTI工作流和自动化需求进行模型定制。",
    "url": "https://huggingface.co/papers/2511.01144",
    "arxiv_url": "https://arxiv.org/abs/2511.01144"
  }
]