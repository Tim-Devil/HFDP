[
  {
    "title": "EvoCUA: Evolving Computer Use Agents via Learning from Scalable Synthetic Experience",
    "summary": "The development of native computer-use agents (CUA) represents a significant leap in multimodal AI. However, their potential is currently bottlenecked by the constraints of static data scaling. Existing paradigms relying primarily on passive imitation of static datasets struggle to capture the intricate causal dynamics inherent in long-horizon computer tasks. In this work, we introduce EvoCUA, a native computer use agentic model. Unlike static imitation, EvoCUA integrates data generation and policy optimization into a self-sustaining evolutionary cycle. To mitigate data scarcity, we develop a verifiable synthesis engine that autonomously generates diverse tasks coupled with executable validators. To enable large-scale experience acquisition, we design a scalable infrastructure orchestrating tens of thousands of asynchronous sandbox rollouts. Building on these massive trajectories, we propose an iterative evolving learning strategy to efficiently internalize this experience. This mechanism dynamically regulates policy updates by identifying capability boundaries -- reinforcing successful routines while transforming failure trajectories into rich supervision through error analysis and self-correction. Empirical evaluations on the OSWorld benchmark demonstrate that EvoCUA achieves a success rate of 56.7%, establishing a new open-source state-of-the-art. Notably, EvoCUA significantly outperforms the previous best open-source model, OpenCUA-72B (45.0%), and surpasses leading closed-weights models such as UI-TARS-2 (53.1%). Crucially, our results underscore the generalizability of this approach: the evolving paradigm driven by learning from experience yields consistent performance gains across foundation models of varying scales, establishing a robust and scalable path for advancing native agent capabilities.",
    "translation": "标题：EvoCUA：通过可扩展合成经验学习进化的计算机使用智能体\n\n摘要：原生计算机使用智能体（CUA）的发展代表了多模态人工智能领域的重大飞跃。然而，其潜力目前受限于静态数据扩展的约束。现有范式主要依赖对静态数据集的被动模仿，难以捕捉长周期计算机任务中固有的复杂因果动态。本研究提出EvoCUA，一种原生计算机使用智能体模型。与静态模仿不同，EvoCUA将数据生成与策略优化整合为一个自我维持的进化循环。为缓解数据稀缺问题，我们开发了一种可验证的合成引擎，能够自主生成多样化任务并配备可执行的验证器。为实现大规模经验获取，我们设计了可扩展的基础设施，可协调数万个异步沙箱推演。基于这些海量轨迹数据，我们提出一种迭代进化学习策略，以高效内化这些经验。该机制通过识别能力边界动态调节策略更新——强化成功的行为模式，同时通过错误分析与自我修正将失败轨迹转化为丰富的监督信号。在OSWorld基准测试中的实证评估表明，EvoCUA实现了56.7%的成功率，创造了开源模型的新标杆。值得注意的是，EvoCUA显著超越先前最佳开源模型OpenCUA-72B（45.0%），并优于UI-TARS-2（53.1%）等领先的闭源权重模型。关键的是，我们的结果验证了该方法的泛化能力：这种通过经验学习驱动的进化范式，在不同规模的基础模型上均能产生持续的性能提升，为推进原生智能体能力建立了稳健且可扩展的发展路径。",
    "url": "https://huggingface.co/papers/2601.15876",
    "arxiv_url": "https://arxiv.org/abs/2601.15876"
  },
  {
    "title": "The Flexibility Trap: Why Arbitrary Order Limits Reasoning Potential in Diffusion Language Models",
    "summary": "Diffusion Large Language Models (dLLMs) break the rigid left-to-right constraint of traditional LLMs, enabling token generation in arbitrary orders. Intuitively, this flexibility implies a solution space that strictly supersets the fixed autoregressive trajectory, theoretically unlocking superior reasoning potential for general tasks like mathematics and coding. Consequently, numerous works have leveraged reinforcement learning (RL) to elicit the reasoning capability of dLLMs. In this paper, we reveal a counter-intuitive reality: arbitrary order generation, in its current form, narrows rather than expands the reasoning boundary of dLLMs. We find that dLLMs tend to exploit this order flexibility to bypass high-uncertainty tokens that are crucial for exploration, leading to a premature collapse of the solution space. This observation challenges the premise of existing RL approaches for dLLMs, where considerable complexities, such as handling combinatorial trajectories and intractable likelihoods, are often devoted to preserving this flexibility. We demonstrate that effective reasoning is better elicited by intentionally forgoing arbitrary order and applying standard Group Relative Policy Optimization (GRPO) instead. Our approach, JustGRPO, is minimalist yet surprisingly effective (e.g., 89.1% accuracy on GSM8K) while fully retaining the parallel decoding ability of dLLMs. Project page: https://nzl-thu.github.io/the-flexibility-trap",
    "translation": "标题：灵活性陷阱：为何任意顺序生成会限制扩散语言模型的推理潜能\n\n摘要：扩散大语言模型突破了传统大语言模型严格的从左到右生成约束，实现了按任意顺序生成词元。直观而言，这种灵活性意味着其解空间严格包含了固定自回归轨迹，理论上为数学与代码等通用任务解锁了更卓越的推理潜能。因此，众多研究尝试通过强化学习来激发扩散大语言模型的推理能力。本文揭示了一个反直觉的现实：当前形式的任意顺序生成非但未能拓展、反而收窄了扩散大语言模型的推理边界。我们发现扩散大语言模型倾向于利用顺序灵活性来规避对探索至关重要的高不确定性词元，导致解空间过早坍缩。这一发现挑战了现有扩散大语言模型强化学习方法的基本前提——这些方法往往为维持顺序灵活性而付出高昂代价，例如处理组合爆炸的轨迹路径与难以计算的似然概率。我们证明，通过主动放弃任意顺序生成并采用标准的分组相对策略优化方法，能更有效地激发模型的推理能力。我们提出的JustGRPO方法简洁却效果显著（如在GSM8K数据集上达到89.1%准确率），同时完整保留了扩散大语言模型的并行解码能力。项目页面：https://nzl-thu.github.io/the-flexibility-trap",
    "url": "https://huggingface.co/papers/2601.15165",
    "arxiv_url": "https://arxiv.org/abs/2601.15165"
  },
  {
    "title": "HERMES: KV Cache as Hierarchical Memory for Efficient Streaming Video Understanding",
    "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have demonstrated significant improvement in offline video understanding. However, extending these capabilities to streaming video inputs, remains challenging, as existing models struggle to simultaneously maintain stable understanding performance, real-time responses, and low GPU memory overhead. To address this challenge, we propose HERMES, a novel training-free architecture for real-time and accurate understanding of video streams. Based on a mechanistic attention investigation, we conceptualize KV cache as a hierarchical memory framework that encapsulates video information across multiple granularities. During inference, HERMES reuses a compact KV cache, enabling efficient streaming understanding under resource constraints. Notably, HERMES requires no auxiliary computations upon the arrival of user queries, thereby guaranteeing real-time responses for continuous video stream interactions, which achieves 10times faster TTFT compared to prior SOTA. Even when reducing video tokens by up to 68% compared with uniform sampling, HERMES achieves superior or comparable accuracy across all benchmarks, with up to 11.4% gains on streaming datasets.",
    "translation": "标题：HERMES：将KV缓存作为层次化内存以实现高效流式视频理解\n\n摘要：多模态大语言模型（MLLMs）的最新进展显著提升了离线视频理解能力。然而，将其扩展至流式视频输入仍面临挑战，现有模型难以同时保持稳定的理解性能、实时响应能力以及较低的GPU内存开销。为应对这一挑战，本文提出HERMES——一种无需训练的新型架构，用于实时、准确地理解视频流。基于对注意力机制的机理研究，我们将KV缓存概念化为一个层次化内存框架，该框架能够在多粒度上封装视频信息。在推理过程中，HERMES通过复用紧凑的KV缓存，实现在资源受限条件下的高效流式理解。值得注意的是，HERMES在用户查询到达时无需额外计算，从而保证了连续视频流交互的实时响应，其首词生成时间较先前最优方法提升了10倍。即使在视频令牌数量相比均匀采样减少高达68%的情况下，HERMES在所有基准测试中仍取得优于或可媲美的准确率，在流式数据集上最高可获得11.4%的性能提升。",
    "url": "https://huggingface.co/papers/2601.14724",
    "arxiv_url": "https://arxiv.org/abs/2601.14724"
  },
  {
    "title": "BayesianVLA: Bayesian Decomposition of Vision Language Action Models via Latent Action Queries",
    "summary": "Vision-Language-Action (VLA) models have shown promise in robot manipulation but often struggle to generalize to new instructions or complex multi-task scenarios. We identify a critical pathology in current training paradigms where goal-driven data collection creates a dataset bias. In such datasets, language instructions are highly predictable from visual observations alone, causing the conditional mutual information between instructions and actions to vanish, a phenomenon we term Information Collapse. Consequently, models degenerate into vision-only policies that ignore language constraints and fail in out-of-distribution (OOD) settings. To address this, we propose BayesianVLA, a novel framework that enforces instruction following via Bayesian decomposition. By introducing learnable Latent Action Queries, we construct a dual-branch architecture to estimate both a vision-only prior p(a mid v) and a language-conditioned posterior π(a mid v, ell). We then optimize the policy to maximize the conditional Pointwise Mutual Information (PMI) between actions and instructions. This objective effectively penalizes the vision shortcut and rewards actions that explicitly explain the language command. Without requiring new data, BayesianVLA significantly improves generalization. Extensive experiments across on SimplerEnv and RoboCasa demonstrate substantial gains, including an 11.3% improvement on the challenging OOD SimplerEnv benchmark, validating the ability of our approach to robustly ground language in action.",
    "translation": "标题：BayesianVLA：基于潜在动作查询的视觉-语言-动作模型贝叶斯分解\n\n摘要：视觉-语言-动作模型在机器人操作任务中展现出潜力，但其在新指令或复杂多任务场景中的泛化能力往往不足。我们发现当前训练范式存在一个关键缺陷：目标驱动的数据收集会导致数据集偏差。在此类数据集中，仅凭视觉观察即可高度预测语言指令，致使指令与动作之间的条件互信息趋于消失，我们将此现象称为“信息坍缩”。其结果是，模型退化为仅依赖视觉的策略，忽略了语言约束，并在分布外场景中失效。为解决此问题，我们提出BayesianVLA，一种通过贝叶斯分解来强化指令跟随的新颖框架。通过引入可学习的潜在动作查询，我们构建了一个双分支架构，分别估计仅基于视觉的先验分布 p(a|v) 和基于语言条件的后验分布 π(a|v, ℓ)。随后，我们通过优化策略来最大化动作与指令之间的条件点互信息。该目标有效抑制了视觉捷径，并奖励那些能显式解释语言指令的动作。在不需新数据的情况下，BayesianVLA显著提升了泛化性能。在SimplerEnv和RoboCasa上的大量实验证明了其显著优势，其中在具有挑战性的分布外基准测试SimplerEnv上实现了11.3%的性能提升，验证了本方法在动作中稳健地关联语言的能力。",
    "url": "https://huggingface.co/papers/2601.15197",
    "arxiv_url": "https://arxiv.org/abs/2601.15197"
  },
  {
    "title": "LLM-in-Sandbox Elicits General Agentic Intelligence",
    "summary": "We introduce LLM-in-Sandbox, enabling LLMs to explore within a code sandbox (i.e., a virtual computer), to elicit general intelligence in non-code domains. We first demonstrate that strong LLMs, without additional training, exhibit generalization capabilities to leverage the code sandbox for non-code tasks. For example, LLMs spontaneously access external resources to acquire new knowledge, leverage the file system to handle long contexts, and execute scripts to satisfy formatting requirements. We further show that these agentic capabilities can be enhanced through LLM-in-Sandbox Reinforcement Learning (LLM-in-Sandbox-RL), which uses only non-agentic data to train models for sandbox exploration. Experiments demonstrate that LLM-in-Sandbox, in both training-free and post-trained settings, achieves robust generalization spanning mathematics, physics, chemistry, biomedicine, long-context understanding, and instruction following. Finally, we analyze LLM-in-Sandbox's efficiency from computational and system perspectives, and open-source it as a Python package to facilitate real-world deployment.",
    "translation": "标题：沙盒环境中的大语言模型激发通用智能体智能\n\n摘要：本文提出“沙盒环境中的大语言模型”框架，使大语言模型能够在代码沙盒（即虚拟计算机）中进行探索，从而激发其在非代码领域的通用智能。我们首先证明，未经额外训练的强性能大语言模型展现出利用代码沙盒处理非代码任务的泛化能力。例如，大语言模型能够自主访问外部资源获取新知识，利用文件系统处理长文本语境，并执行脚本以满足格式要求。进一步研究表明，通过“沙盒环境中的大语言模型强化学习”方法，仅使用非智能体数据训练模型进行沙盒探索，即可增强这些智能体能力。实验表明，无论是否经过训练，该框架在数学、物理、化学、生物医学、长文本理解及指令遵循等领域均展现出稳健的泛化性能。最后，我们从计算与系统视角分析了该框架的运行效率，并将其开源为Python工具包以促进实际应用部署。",
    "url": "https://huggingface.co/papers/2601.16206",
    "arxiv_url": "https://arxiv.org/abs/2601.16206"
  },
  {
    "title": "Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders",
    "summary": "Representation Autoencoders (RAEs) have shown distinct advantages in diffusion modeling on ImageNet by training in high-dimensional semantic latent spaces. In this work, we investigate whether this framework can scale to large-scale, freeform text-to-image (T2I) generation. We first scale RAE decoders on the frozen representation encoder (SigLIP-2) beyond ImageNet by training on web, synthetic, and text-rendering data, finding that while scale improves general fidelity, targeted data composition is essential for specific domains like text. We then rigorously stress-test the RAE design choices originally proposed for ImageNet. Our analysis reveals that scaling simplifies the framework: while dimension-dependent noise scheduling remains critical, architectural complexities such as wide diffusion heads and noise-augmented decoding offer negligible benefits at scale Building on this simplified framework, we conduct a controlled comparison of RAE against the state-of-the-art FLUX VAE across diffusion transformer scales from 0.5B to 9.8B parameters. RAEs consistently outperform VAEs during pretraining across all model scales. Further, during finetuning on high-quality datasets, VAE-based models catastrophically overfit after 64 epochs, while RAE models remain stable through 256 epochs and achieve consistently better performance. Across all experiments, RAE-based diffusion models demonstrate faster convergence and better generation quality, establishing RAEs as a simpler and stronger foundation than VAEs for large-scale T2I generation. Additionally, because both visual understanding and generation can operate in a shared representation space, the multimodal model can directly reason over generated latents, opening new possibilities for unified models.",
    "translation": "标题：基于表征自编码器的文本到图像扩散变换器规模化研究\n\n摘要：表征自编码器通过在高层语义潜在空间中训练，已在ImageNet的扩散建模中展现出独特优势。本研究探讨该框架能否扩展至大规模自由格式的文本到图像生成任务。我们首先在冻结的表征编码器（SigLIP-2）基础上，通过融合网络数据、合成数据及文本渲染数据进行解码器规模化训练，发现虽然规模扩展能提升整体保真度，但针对文本等特定领域仍需精细化的数据组合策略。随后，我们对原为ImageNet设计的RAE架构选择进行了严格压力测试。分析表明：规模化使框架显著简化——维度依赖的噪声调度机制仍至关重要，而宽扩散头结构、噪声增强解码等复杂设计在大规模场景下收益微乎其微。基于此简化框架，我们在0.5B至9.8B参数规模的扩散变换器上，对RAE与前沿的FLUX VAE进行了系统对比。实验表明：在所有模型规模下，RAE在预训练阶段始终优于VAE；在高质量数据集微调阶段，基于VAE的模型在64轮训练后出现灾难性过拟合，而RAE模型在256轮训练中保持稳定且性能持续更优。所有实验均证明，基于RAE的扩散模型具有更快的收敛速度和更优的生成质量，表明RAE比VAE更适合作为大规模文本到图像生成的简捷而强大的基础架构。此外，由于视觉理解与生成可在共享表征空间中协同运作，多模态模型能直接对生成潜在表示进行推理，这为构建统一模型开辟了新路径。",
    "url": "https://huggingface.co/papers/2601.16208",
    "arxiv_url": "https://arxiv.org/abs/2601.16208"
  },
  {
    "title": "Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model",
    "summary": "Diffusion-based language models (DLLMs) offer non-sequential, block-wise generation and richer data reuse compared to autoregressive (AR) models, but existing code DLLMs still lag behind strong AR baselines under comparable budgets. We revisit this setting in a controlled study and introduce Stable-DiffCoder, a block diffusion code model that reuses the Seed-Coder architecture, data, and training pipeline. To enable efficient knowledge learning and stable training, we incorporate a block diffusion continual pretraining (CPT) stage enhanced by a tailored warmup and block-wise clipped noise schedule. Under the same data and architecture, Stable-DiffCoder overall outperforms its AR counterpart on a broad suite of code benchmarks. Moreover, relying only on the CPT and supervised fine-tuning stages, Stable-DiffCoder achieves stronger performance than a wide range of \\~8B ARs and DLLMs, demonstrating that diffusion-based training can improve code modeling quality beyond AR training alone. Moreover, diffusion-based any-order modeling improves structured code modeling for editing and reasoning, and through data augmentation, benefits low-resource coding languages.",
    "translation": "标题：Stable-DiffCoder：推进代码扩散大语言模型的前沿研究\n\n摘要：与自回归模型相比，基于扩散的语言模型具有非顺序的块级生成能力和更丰富的数据复用优势，然而在同等计算资源下，现有代码扩散模型仍落后于强大的自回归基线模型。本研究通过受控实验重新审视这一设定，提出了Stable-DiffCoder——一种复用Seed-Coder架构、数据与训练流程的块扩散代码模型。为实现高效知识学习和稳定训练，我们引入了块扩散持续预训练阶段，该阶段通过定制化的预热策略和块级截断噪声调度进行增强。在相同数据与架构条件下，Stable-DiffCoder在广泛的代码基准测试中整体表现优于其自回归对照模型。仅通过持续预训练和监督微调阶段，该模型即能超越多种约80亿参数的自回归与扩散模型，证明基于扩散的训练方法能够单独提升代码建模质量。此外，基于扩散的任意顺序建模能力提升了代码编辑与推理任务中的结构化建模性能，并通过数据增强机制有效促进了低资源编程语言的发展。",
    "url": "https://huggingface.co/papers/2601.15892",
    "arxiv_url": "https://arxiv.org/abs/2601.15892"
  },
  {
    "title": "SAMTok: Representing Any Mask with Two Words",
    "summary": "Pixel-wise capabilities are essential for building interactive intelligent systems. However, pixel-wise multi-modal LLMs (MLLMs) remain difficult to scale due to complex region-level encoders, specialized segmentation decoders, and incompatible training objectives. To address these challenges, we present SAMTok, a discrete mask tokenizer that converts any region mask into two special tokens and reconstructs the mask using these tokens with high fidelity. By treating masks as new language tokens, SAMTok enables base MLLMs (such as the QwenVL series) to learn pixel-wise capabilities through standard next-token prediction and simple reinforcement learning, without architectural modifications and specialized loss design. SAMTok builds on SAM2 and is trained on 209M diverse masks using a mask encoder and residual vector quantizer to produce discrete, compact, and information-rich tokens. With 5M SAMTok-formatted mask understanding and generation data samples, QwenVL-SAMTok attains state-of-the-art or comparable results on region captioning, region VQA, grounded conversation, referring segmentation, scene graph parsing, and multi-round interactive segmentation. We further introduce a textual answer-matching reward that enables efficient reinforcement learning for mask generation, delivering substantial improvements on GRES and GCG benchmarks. Our results demonstrate a scalable and straightforward paradigm for equipping MLLMs with strong pixel-wise capabilities. Our code and models are available.",
    "translation": "标题：SAMTok：用两个词表示任意掩码\n\n摘要：像素级能力对于构建交互式智能系统至关重要。然而，由于复杂的区域级编码器、专业化的分割解码器以及相互冲突的训练目标，像素级多模态大语言模型（MLLMs）的扩展仍然面临困难。为应对这些挑战，我们提出了SAMTok——一种离散掩码分词器，它能够将任意区域掩码转换为两个特殊标记，并以高保真度利用这些标记重建掩码。通过将掩码视为新的语言标记，SAMTok使得基础MLLMs（例如QwenVL系列）能够通过标准的下一个标记预测和简单的强化学习来掌握像素级能力，而无需修改模型架构或设计专门的损失函数。SAMTok基于SAM2构建，利用掩码编码器和残差向量量化器在2.09亿个多样化掩码上进行训练，以生成离散、紧凑且信息丰富的标记。通过使用500万个SAMTok格式的掩码理解与生成数据样本，QwenVL-SAMTok在区域描述、区域视觉问答、指代对话、指代分割、场景图解析以及多轮交互式分割任务上取得了最先进或可比的结果。我们进一步引入了一种文本答案匹配奖励机制，使得掩码生成的强化学习能够高效进行，在GRES和GCG基准测试中带来了显著提升。我们的研究结果展示了一种可扩展且简洁的范式，能够为MLLMs赋予强大的像素级能力。代码与模型均已开源。",
    "url": "https://huggingface.co/papers/2601.16093",
    "arxiv_url": "https://arxiv.org/abs/2601.16093"
  },
  {
    "title": "Learning to Discover at Test Time",
    "summary": "How can we use AI to discover a new state of the art for a scientific problem? Prior work in test-time scaling, such as AlphaEvolve, performs search by prompting a frozen LLM. We perform reinforcement learning at test time, so the LLM can continue to train, but now with experience specific to the test problem. This form of continual learning is quite special, because its goal is to produce one great solution rather than many good ones on average, and to solve this very problem rather than generalize to other problems. Therefore, our learning objective and search subroutine are designed to prioritize the most promising solutions. We call this method Test-Time Training to Discover (TTT-Discover). Following prior work, we focus on problems with continuous rewards. We report results for every problem we attempted, across mathematics, GPU kernel engineering, algorithm design, and biology. TTT-Discover sets the new state of the art in almost all of them: (i) Erdős' minimum overlap problem and an autocorrelation inequality; (ii) a GPUMode kernel competition (up to 2times faster than prior art); (iii) past AtCoder algorithm competitions; and (iv) denoising problem in single-cell analysis. Our solutions are reviewed by experts or the organizers. All our results are achieved with an open model, OpenAI gpt-oss-120b, and can be reproduced with our publicly available code, in contrast to previous best results that required closed frontier models. Our test-time training runs are performed using Tinker, an API by Thinking Machines, with a cost of only a few hundred dollars per problem.",
    "translation": "标题：测试时学习发现方法研究\n\n摘要：如何利用人工智能探索科学问题的新最优解？现有测试时扩展研究（如AlphaEvolve）通过调用冻结的大型语言模型进行搜索。本研究在测试时实施强化学习，使大型语言模型能够持续训练，并针对特定测试问题积累经验。这种持续学习形式具有特殊性：其目标在于产生单一优质解而非平均意义上的多个可行解，且专注于解决当前特定问题而非泛化至其他问题。因此，我们的学习目标与搜索子程序均以优先挖掘最具潜力的解决方案为核心设计原则。该方法被命名为\"测试时训练发现法\"。延续既有研究范式，我们聚焦于具有连续奖励机制的问题领域，并在数学、GPU内核工程、算法设计与生物学四大领域全面报告实验结果。测试时训练发现法在几乎所有领域均实现了当前最优性能：（1）埃尔德什最小重叠问题与自相关不等式证明；（2）GPUMode内核竞赛（较现有最优方案提速达2倍）；（3）历史AtCoder算法竞赛；（4）单细胞分析中的去噪问题。所有解决方案均经领域专家或竞赛组委会审核认证。值得注意的是，本研究全部成果均基于开源模型OpenAI gpt-oss-120b实现，并可通过公开代码复现，而此前最优结果均依赖封闭前沿模型达成。测试时训练过程通过Thinking Machines公司的Tinker API平台运行，单问题解决成本仅数百美元。",
    "url": "https://huggingface.co/papers/2601.16175",
    "arxiv_url": "https://arxiv.org/abs/2601.16175"
  },
  {
    "title": "Qwen3-TTS Technical Report",
    "summary": "In this report, we present the Qwen3-TTS series, a family of advanced multilingual, controllable, robust, and streaming text-to-speech models. Qwen3-TTS supports state-of-the-art 3-second voice cloning and description-based control, allowing both the creation of entirely novel voices and fine-grained manipulation over the output speech. Trained on over 5 million hours of speech data spanning 10 languages, Qwen3-TTS adopts a dual-track LM architecture for real-time synthesis, coupled with two speech tokenizers: 1) Qwen-TTS-Tokenizer-25Hz is a single-codebook codec emphasizing semantic content, which offers seamlessly integration with Qwen-Audio and enables streaming waveform reconstruction via a block-wise DiT. 2) Qwen-TTS-Tokenizer-12Hz achieves extreme bitrate reduction and ultra-low-latency streaming, enabling immediate first-packet emission (97,ms) through its 12.5 Hz, 16-layer multi-codebook design and a lightweight causal ConvNet. Extensive experiments indicate state-of-the-art performance across diverse objective and subjective benchmark (e.g., TTS multilingual test set, InstructTTSEval, and our long speech test set). To facilitate community research and development, we release both tokenizers and models under the Apache 2.0 license.",
    "translation": "标题：Qwen3-TTS技术报告\n\n摘要：本报告介绍了Qwen3-TTS系列模型——一个具备多语言、可控性、鲁棒性与流式合成能力的先进文本转语音模型家族。Qwen3-TTS支持业界领先的3秒语音克隆与基于描述的语音控制技术，既能生成全新的语音身份，也能对输出语音进行细粒度调控。该模型基于超过500万小时、覆盖10种语言的语音数据训练，采用双轨语言模型架构实现实时合成，并配备两种语音分词器：1）Qwen-TTS-Tokenizer-25Hz是专注于语义内容的单码本编解码器，可与Qwen-Audio无缝集成，并通过分块扩散变换器实现流式波形重建；2）Qwen-TTS-Tokenizer-12Hz通过12.5Hz频率的16层多码本设计与轻量因果卷积网络，实现了极致的比特率压缩与超低延迟流式合成，支持首包97毫秒即时响应。大量实验表明，该系列模型在多项主客观评测基准（如TTS多语言测试集、InstructTTSEval及长语音测试集）中均达到业界最优性能。为促进社区研究与发展，我们已依据Apache 2.0协议开源全部分词器与模型。",
    "url": "https://huggingface.co/papers/2601.15621",
    "arxiv_url": "https://arxiv.org/abs/2601.15621"
  },
  {
    "title": "Terminal-Bench: Benchmarking Agents on Hard, Realistic Tasks in Command Line Interfaces",
    "summary": "AI agents may soon become capable of autonomously completing valuable, long-horizon tasks in diverse domains. Current benchmarks either do not measure real-world tasks, or are not sufficiently difficult to meaningfully measure frontier models. To this end, we present Terminal-Bench 2.0: a carefully curated hard benchmark composed of 89 tasks in computer terminal environments inspired by problems from real workflows. Each task features a unique environment, human-written solution, and comprehensive tests for verification. We show that frontier models and agents score less than 65\\% on the benchmark and conduct an error analysis to identify areas for model and agent improvement. We publish the dataset and evaluation harness to assist developers and researchers in future work at https://www.tbench.ai/ .",
    "translation": "标题：Terminal-Bench：基于命令行界面的高难度现实任务智能体基准测试\n\n摘要：人工智能智能体有望在不久的将来，在多个领域自主完成具有长期价值的复杂任务。然而，现有基准测试要么未能有效衡量真实世界任务，要么难度不足，难以对前沿模型进行有意义的评估。为此，我们提出了Terminal-Bench 2.0：一个精心构建的高难度基准测试集，包含89个基于真实工作流程设计的计算机终端环境任务。每个任务均设有独立的环境配置、人工编写的解决方案以及用于验证的全面测试集。实验表明，当前前沿模型与智能体在该基准上的得分低于65%，我们进一步通过错误分析指出了模型与智能体需改进的方向。为支持后续研究与开发，我们在https://www.tbench.ai/ 公开了完整数据集及评估工具。",
    "url": "https://huggingface.co/papers/2601.11868",
    "arxiv_url": "https://arxiv.org/abs/2601.11868"
  },
  {
    "title": "Rethinking Composed Image Retrieval Evaluation: A Fine-Grained Benchmark from Image Editing",
    "summary": "Composed Image Retrieval (CIR) is a pivotal and complex task in multimodal understanding. Current CIR benchmarks typically feature limited query categories and fail to capture the diverse requirements of real-world scenarios. To bridge this evaluation gap, we leverage image editing to achieve precise control over modification types and content, enabling a pipeline for synthesizing queries across a broad spectrum of categories. Using this pipeline, we construct EDIR, a novel fine-grained CIR benchmark. EDIR encompasses 5,000 high-quality queries structured across five main categories and fifteen subcategories. Our comprehensive evaluation of 13 multimodal embedding models reveals a significant capability gap; even state-of-the-art models (e.g., RzenEmbed and GME) struggle to perform consistently across all subcategories, highlighting the rigorous nature of our benchmark. Through comparative analysis, we further uncover inherent limitations in existing benchmarks, such as modality biases and insufficient categorical coverage. Furthermore, an in-domain training experiment demonstrates the feasibility of our benchmark. This experiment clarifies the task challenges by distinguishing between categories that are solvable with targeted data and those that expose intrinsic limitations of current model architectures.",
    "translation": "标题：重新审视组合图像检索评估：基于图像编辑的细粒度基准\n\n摘要：组合图像检索是多模态理解领域一项关键且复杂的任务。当前主流的组合图像检索基准通常存在查询类别有限的问题，难以反映现实应用场景的多样化需求。为弥补这一评估缺口，本研究利用图像编辑技术实现对修改类型与内容的精确控制，构建了一个能够跨广泛类别合成查询的自动化流程。基于此流程，我们提出了EDIR——一个新颖的细粒度组合图像检索基准。EDIR包含5,000个高质量查询，这些查询按照5个主类别和15个子类别进行系统化组织。通过对13个多模态嵌入模型的全面评估，我们发现现有模型存在显著的能力缺陷：即使是当前最先进的模型（如RzenEmbed和GME）也难以在所有子类别上保持稳定性能，这凸显了我们基准的严格性。通过对比分析，我们进一步揭示了现有基准存在的固有局限，例如模态偏差与类别覆盖不足等问题。此外，领域内训练实验验证了我们基准的可行性，该实验通过区分“可通过针对性数据解决”与“暴露当前模型架构固有缺陷”的类别，进一步厘清了任务挑战的本质。",
    "url": "https://huggingface.co/papers/2601.16125",
    "arxiv_url": "https://arxiv.org/abs/2601.16125"
  },
  {
    "title": "OpenVision 3: A Family of Unified Visual Encoder for Both Understanding and Generation",
    "summary": "This paper presents a family of advanced vision encoder, named OpenVision 3, that learns a single, unified visual representation that can serve both image understanding and image generation. Our core architecture is simple: we feed VAE-compressed image latents to a ViT encoder and train its output to support two complementary roles. First, the encoder output is passed to the ViT-VAE decoder to reconstruct the original image, encouraging the representation to capture generative structure. Second, the same representation is optimized with contrastive learning and image-captioning objectives, strengthening semantic features. By jointly optimizing reconstruction- and semantics-driven signals in a shared latent space, the encoder learns representations that synergize and generalize well across both regimes. We validate this unified design through extensive downstream evaluations with the encoder frozen. For multimodal understanding, we plug the encoder into the LLaVA-1.5 framework: it performs comparably with a standard CLIP vision encoder (e.g., 62.4 vs 62.2 on SeedBench, and 83.7 vs 82.9 on POPE). For generation, we test it under the RAE framework: ours substantially surpasses the standard CLIP-based encoder (e.g., gFID: 1.89 vs 2.54 on ImageNet). We hope this work can spur future research on unified modeling.",
    "translation": "标题：OpenVision 3：一个用于理解与生成的统一视觉编码器家族\n\n摘要：本文提出了一种先进视觉编码器家族，命名为OpenVision 3，其学习单一、统一的视觉表示，可同时服务于图像理解与图像生成任务。我们的核心架构简洁明了：将VAE压缩的图像潜变量输入ViT编码器，并训练其输出以支持两种互补功能。首先，编码器输出被传递至ViT-VAE解码器以重建原始图像，促使表示捕捉生成结构；其次，同一表示通过对比学习与图像描述目标进行优化，以增强语义特征。通过在共享潜空间中联合优化重建驱动与语义驱动的信号，该编码器学习到的表示能够协同作用，并在两种任务范式中均展现出良好的泛化能力。我们通过大量下游评估（编码器参数冻结）验证了这一统一设计的有效性。在多模态理解方面，我们将该编码器嵌入LLaVA-1.5框架：其性能与标准CLIP视觉编码器相当（例如，在SeedBench上得分为62.4对62.2，在POPE上为83.7对82.9）。在生成任务中，我们在RAE框架下进行测试：本方法显著优于基于标准CLIP的编码器（例如，在ImageNet上的gFID指标为1.89对2.54）。我们希望这项工作能够推动统一建模方向的未来研究。",
    "url": "https://huggingface.co/papers/2601.15369",
    "arxiv_url": "https://arxiv.org/abs/2601.15369"
  },
  {
    "title": "Towards Automated Kernel Generation in the Era of LLMs",
    "summary": "The performance of modern AI systems is fundamentally constrained by the quality of their underlying kernels, which translate high-level algorithmic semantics into low-level hardware operations. Achieving near-optimal kernels requires expert-level understanding of hardware architectures and programming models, making kernel engineering a critical but notoriously time-consuming and non-scalable process. Recent advances in large language models (LLMs) and LLM-based agents have opened new possibilities for automating kernel generation and optimization. LLMs are well-suited to compress expert-level kernel knowledge that is difficult to formalize, while agentic systems further enable scalable optimization by casting kernel development as an iterative, feedback-driven loop. Rapid progress has been made in this area. However, the field remains fragmented, lacking a systematic perspective for LLM-driven kernel generation. This survey addresses this gap by providing a structured overview of existing approaches, spanning LLM-based approaches and agentic optimization workflows, and systematically compiling the datasets and benchmarks that underpin learning and evaluation in this domain. Moreover, key open challenges and future research directions are further outlined, aiming to establish a comprehensive reference for the next generation of automated kernel optimization. To keep track of this field, we maintain an open-source GitHub repository at https://github.com/flagos-ai/awesome-LLM-driven-kernel-generation.",
    "translation": "标题：迈向大语言模型时代的自动化内核生成\n\n摘要：现代人工智能系统的性能从根本上受限于其底层内核的质量，这些内核将高级算法语义转化为底层硬件操作。实现接近最优的内核需要对硬件架构和编程模型具备专家级理解，这使得内核工程成为一个关键但众所周知耗时且难以规模化的工作。近期，大语言模型（LLMs）及基于LLM的智能体在自动生成和优化内核方面展现出新的可能性。LLM非常适合压缩那些难以形式化的专家级内核知识，而智能体系统通过将内核开发构建为一个迭代的、反馈驱动的循环，进一步实现了可扩展的优化。该领域已取得快速进展，但目前研究仍较为零散，缺乏针对LLM驱动内核生成的系统性视角。本综述旨在填补这一空白，通过结构化梳理现有方法（涵盖基于LLM的路径与智能体优化工作流），并系统汇编支撑该领域学习与评估的数据集和基准测试，为相关研究提供系统概览。此外，本文进一步概述了关键的开放挑战与未来研究方向，旨在为下一代自动化内核优化建立全面的参考。为持续追踪该领域进展，我们在 https://github.com/flagos-ai/awesome-LLM-driven-kernel-generation 维护了一个开源GitHub仓库。",
    "url": "https://huggingface.co/papers/2601.15727",
    "arxiv_url": "https://arxiv.org/abs/2601.15727"
  },
  {
    "title": "PROGRESSLM: Towards Progress Reasoning in Vision-Language Models",
    "summary": "Estimating task progress requires reasoning over long-horizon dynamics rather than recognizing static visual content. While modern Vision-Language Models (VLMs) excel at describing what is visible, it remains unclear whether they can infer how far a task has progressed from partial observations. To this end, we introduce Progress-Bench, a benchmark for systematically evaluating progress reasoning in VLMs. Beyond benchmarking, we further explore a human-inspired two-stage progress reasoning paradigm through both training-free prompting and training-based approach based on curated dataset ProgressLM-45K. Experiments on 14 VLMs show that most models are not yet ready for task progress estimation, exhibiting sensitivity to demonstration modality and viewpoint changes, as well as poor handling of unanswerable cases. While training-free prompting that enforces structured progress reasoning yields limited and model-dependent gains, the training-based ProgressLM-3B achieves consistent improvements even at a small model scale, despite being trained on a task set fully disjoint from the evaluation tasks. Further analyses reveal characteristic error patterns and clarify when and why progress reasoning succeeds or fails.",
    "translation": "标题：PROGRESSLM：迈向视觉语言模型中的进度推理\n\n摘要：估计任务进度需要对长时程动态进行推理，而非仅仅识别静态视觉内容。尽管现代视觉语言模型（VLMs）在描述可见内容方面表现出色，但它们能否从部分观察中推断任务已进展到何种程度，目前尚不明确。为此，我们提出了Progress-Bench，这是一个用于系统评估VLMs中进度推理能力的基准。除了基准测试外，我们进一步探索了一种受人类启发的两阶段进度推理范式，包括基于无训练提示的方法和基于精心构建的数据集ProgressLM-45K的有训练方法。在14个VLM上的实验表明，大多数模型尚未准备好进行任务进度估计，它们对演示模态和视角变化表现出敏感性，且在处理不可回答的情况时表现不佳。尽管强制执行结构化进度推理的无训练提示方法带来的提升有限且依赖于模型，而有训练的ProgressLM-3B即使在小规模模型下也实现了持续改进，尽管其训练任务集与评估任务集完全不相交。进一步的分析揭示了典型的错误模式，并阐明了进度推理何时以及为何成功或失败。",
    "url": "https://huggingface.co/papers/2601.15224",
    "arxiv_url": "https://arxiv.org/abs/2601.15224"
  },
  {
    "title": "VideoMaMa: Mask-Guided Video Matting via Generative Prior",
    "summary": "Generalizing video matting models to real-world videos remains a significant challenge due to the scarcity of labeled data. To address this, we present Video Mask-to-Matte Model (VideoMaMa) that converts coarse segmentation masks into pixel accurate alpha mattes, by leveraging pretrained video diffusion models. VideoMaMa demonstrates strong zero-shot generalization to real-world footage, even though it is trained solely on synthetic data. Building on this capability, we develop a scalable pseudo-labeling pipeline for large-scale video matting and construct the Matting Anything in Video (MA-V) dataset, which offers high-quality matting annotations for more than 50K real-world videos spanning diverse scenes and motions. To validate the effectiveness of this dataset, we fine-tune the SAM2 model on MA-V to obtain SAM2-Matte, which outperforms the same model trained on existing matting datasets in terms of robustness on in-the-wild videos. These findings emphasize the importance of large-scale pseudo-labeled video matting and showcase how generative priors and accessible segmentation cues can drive scalable progress in video matting research.",
    "translation": "标题：VideoMaMa：基于生成先验的掩码引导视频抠图方法\n\n摘要：由于标注数据稀缺，将视频抠图模型泛化至真实世界视频仍面临重大挑战。为此，我们提出视频掩码转蒙版模型（VideoMaMa），该模型通过利用预训练的视频扩散模型，将粗粒度分割掩码转换为像素级精确的透明度蒙版。尽管仅使用合成数据进行训练，VideoMaMa在真实世界影像上展现出强大的零样本泛化能力。基于此能力，我们开发了可扩展的大规模视频抠图伪标注流程，并构建了“视频任意抠图”（MA-V）数据集。该数据集为超过5万段涵盖多样场景与动态的真实世界视频提供了高质量的抠图标注。为验证该数据集的有效性，我们在MA-V上对SAM2模型进行微调，得到SAM2-Matte模型。相较于基于现有抠图数据集训练的同类模型，该模型在真实场景视频的鲁棒性方面表现更优。这些发现凸显了大规模伪标注视频抠图数据的重要性，并展示了生成先验与易获取的分割线索如何推动视频抠图研究实现可扩展的进展。",
    "url": "https://huggingface.co/papers/2601.14255",
    "arxiv_url": "https://arxiv.org/abs/2601.14255"
  },
  {
    "title": "360Anything: Geometry-Free Lifting of Images and Videos to 360°",
    "summary": "Lifting perspective images and videos to 360° panoramas enables immersive 3D world generation. Existing approaches often rely on explicit geometric alignment between the perspective and the equirectangular projection (ERP) space. Yet, this requires known camera metadata, obscuring the application to in-the-wild data where such calibration is typically absent or noisy. We propose 360Anything, a geometry-free framework built upon pre-trained diffusion transformers. By treating the perspective input and the panorama target simply as token sequences, 360Anything learns the perspective-to-equirectangular mapping in a purely data-driven way, eliminating the need for camera information. Our approach achieves state-of-the-art performance on both image and video perspective-to-360° generation, outperforming prior works that use ground-truth camera information. We also trace the root cause of the seam artifacts at ERP boundaries to zero-padding in the VAE encoder, and introduce Circular Latent Encoding to facilitate seamless generation. Finally, we show competitive results in zero-shot camera FoV and orientation estimation benchmarks, demonstrating 360Anything's deep geometric understanding and broader utility in computer vision tasks. Additional results are available at https://360anything.github.io/.",
    "translation": "标题：360Anything：基于无几何约束的图像与视频升维至360度全景\n\n摘要：将透视图像与视频升维至360度全景可实现沉浸式三维场景生成。现有方法通常依赖于透视图像与等距柱状投影空间之间的显式几何对齐，但这需要已知相机元数据，限制了该方法在缺乏准确校准信息的真实场景数据中的应用。本文提出360Anything——一个基于预训练扩散变换器的无几何约束框架。通过将透视输入与全景目标简化为令牌序列，360Anything以纯数据驱动的方式学习透视到等距柱状投影的映射关系，无需依赖相机信息。我们的方法在图像与视频的透视转360度生成任务中均达到最先进性能，其表现优于使用真实相机信息的现有方法。同时，我们追踪了等距柱状投影边界处接缝伪影的根源，将其归因于VAE编码器中的零填充操作，并引入环形潜在编码以实现无缝生成。最后，我们在零样本相机视场角与方向估计基准测试中展示了具有竞争力的结果，证明了360Anything对几何关系的深度理解及其在计算机视觉任务中的广泛适用性。更多结果详见https://360anything.github.io/。",
    "url": "https://huggingface.co/papers/2601.16192",
    "arxiv_url": "https://arxiv.org/abs/2601.16192"
  },
  {
    "title": "Cosmos Policy: Fine-Tuning Video Models for Visuomotor Control and Planning",
    "summary": "Recent video generation models demonstrate remarkable ability to capture complex physical interactions and scene evolution over time. To leverage their spatiotemporal priors, robotics works have adapted video models for policy learning but introduce complexity by requiring multiple stages of post-training and new architectural components for action generation. In this work, we introduce Cosmos Policy, a simple approach for adapting a large pretrained video model (Cosmos-Predict2) into an effective robot policy through a single stage of post-training on the robot demonstration data collected on the target platform, with no architectural modifications. Cosmos Policy learns to directly generate robot actions encoded as latent frames within the video model's latent diffusion process, harnessing the model's pretrained priors and core learning algorithm to capture complex action distributions. Additionally, Cosmos Policy generates future state images and values (expected cumulative rewards), which are similarly encoded as latent frames, enabling test-time planning of action trajectories with higher likelihood of success. In our evaluations, Cosmos Policy achieves state-of-the-art performance on the LIBERO and RoboCasa simulation benchmarks (98.5% and 67.1% average success rates, respectively) and the highest average score in challenging real-world bimanual manipulation tasks, outperforming strong diffusion policies trained from scratch, video model-based policies, and state-of-the-art vision-language-action models fine-tuned on the same robot demonstrations. Furthermore, given policy rollout data, Cosmos Policy can learn from experience to refine its world model and value function and leverage model-based planning to achieve even higher success rates in challenging tasks. We release code, models, and training data at https://research.nvidia.com/labs/dir/cosmos-policy/",
    "translation": "标题：Cosmos策略：面向视觉运动控制与规划的视觉模型微调方法\n\n摘要：近期视频生成模型展现出捕捉复杂物理交互与时序场景演变的卓越能力。为利用其时空先验知识，机器人研究领域已尝试将视频模型应用于策略学习，但传统方法需通过多阶段后训练及新增动作生成架构组件，引入了显著复杂性。本研究提出Cosmos策略，这是一种通过单阶段后训练将大型预训练视频模型（Cosmos-Predict2）适配为高效机器人策略的简洁方法。该方法仅需在目标平台采集的机器人演示数据上进行训练，无需修改模型架构。Cosmos策略通过学习直接生成编码为视频模型潜在扩散过程中隐帧的机器人动作，充分利用模型的预训练先验知识与核心学习算法来捕捉复杂的动作分布。此外，该方法还能生成未来状态图像与价值函数（预期累积奖励），这些信息同样以隐帧形式编码，从而支持在测试阶段规划更高成功概率的动作轨迹。实验评估表明，Cosmos策略在LIBERO与RoboCasa仿真基准测试中分别达到98.5%与67.1%的平均成功率，取得最先进性能；在具有挑战性的真实世界双手操作任务中获得最高平均分，其表现优于从头训练的强扩散策略、基于视频模型的策略，以及在相同机器人演示数据上微调的先进视觉-语言-动作模型。进一步地，基于策略推演数据，Cosmos策略能够通过经验学习优化其世界模型与价值函数，并借助基于模型的规划在复杂任务中实现更高的成功率。相关代码、模型及训练数据已发布于https://research.nvidia.com/labs/dir/cosmos-policy/。",
    "url": "https://huggingface.co/papers/2601.16163",
    "arxiv_url": "https://arxiv.org/abs/2601.16163"
  },
  {
    "title": "ActionMesh: Animated 3D Mesh Generation with Temporal 3D Diffusion",
    "summary": "Generating animated 3D objects is at the heart of many applications, yet most advanced works are typically difficult to apply in practice because of their limited setup, their long runtime, or their limited quality. We introduce ActionMesh, a generative model that predicts production-ready 3D meshes \"in action\" in a feed-forward manner. Drawing inspiration from early video models, our key insight is to modify existing 3D diffusion models to include a temporal axis, resulting in a framework we dubbed \"temporal 3D diffusion\". Specifically, we first adapt the 3D diffusion stage to generate a sequence of synchronized latents representing time-varying and independent 3D shapes. Second, we design a temporal 3D autoencoder that translates a sequence of independent shapes into the corresponding deformations of a pre-defined reference shape, allowing us to build an animation. Combining these two components, ActionMesh generates animated 3D meshes from different inputs like a monocular video, a text description, or even a 3D mesh with a text prompt describing its animation. Besides, compared to previous approaches, our method is fast and produces results that are rig-free and topology consistent, hence enabling rapid iteration and seamless applications like texturing and retargeting. We evaluate our model on standard video-to-4D benchmarks (Consistent4D, Objaverse) and report state-of-the-art performances on both geometric accuracy and temporal consistency, demonstrating that our model can deliver animated 3D meshes with unprecedented speed and quality.",
    "translation": "标题：ActionMesh：基于时序三维扩散的动画三维网格生成\n\n摘要：生成动态三维对象是众多应用的核心任务，然而现有先进方法常因设定局限、计算耗时或质量不足而难以实际应用。本文提出ActionMesh——一种以前馈方式生成可直接投入生产的动态三维网格的生成模型。受早期视频模型启发，我们的核心思路是通过引入时间轴改进现有三维扩散模型，构建出“时序三维扩散”框架。具体而言，首先改进三维扩散阶段以生成表征时序变化且相互独立的三维形状序列隐变量；其次设计时序三维自编码器，将独立形状序列转化为预定义参考形状的对应形变，从而构建完整动画。通过整合这两个组件，ActionMesh能够从单目视频、文本描述甚至结合动画文本提示的三维网格等多种输入生成动态三维网格。相较于既有方法，本方案具有速度快、无需骨骼绑定且保持拓扑一致性的优势，支持快速迭代并兼容纹理映射与重定向等无缝应用。我们在标准视频转4D基准数据集（Consistent4D、Objaverse）上评估模型，在几何精度与时序一致性方面均达到最先进性能，证明本模型能够以前所未有的速度与质量生成动态三维网格。",
    "url": "https://huggingface.co/papers/2601.16148",
    "arxiv_url": "https://arxiv.org/abs/2601.16148"
  },
  {
    "title": "VIOLA: Towards Video In-Context Learning with Minimal Annotations",
    "summary": "Generalizing Multimodal Large Language Models (MLLMs) to novel video domains is essential for real-world deployment but remains challenging due to the scarcity of labeled data. While In-Context Learning (ICL) offers a training-free adaptation path, standard methods rely on large annotated pools, which are often impractical in specialized environments like industrial or surgical settings since they require the experts' annotations. To bridge this gap, we introduce VIOLA (Video In-cOntext Learning with minimal Annotation), a label-efficient framework that synergizes minimal expert supervision with abundant unlabeled data. First, to maximize the efficiency of a strict annotation budget, we propose density-uncertainty-weighted sampling. Unlike standard diversity or uncertainty strategies that risk selecting visual outliers, our method leverages density estimation to identify samples that are simultaneously diverse, representative, and informative. Second, to utilize the remaining unlabeled data without noise propagation, we construct a hybrid pool and introduce confidence-aware retrieval and confidence-aware prompting. These mechanisms explicitly model label reliability, retrieving demonstrations based on a composite score of similarity and confidence while enabling the MLLM to adaptively distinguish between verified ground truths and noisy pseudo-labels. Extensive experiments across nine diverse benchmarks using four MLLMs demonstrate that our framework significantly outperforms various baselines in low-resource settings, achieving robust adaptation with minimal annotation costs.",
    "translation": "标题：VIOLA：面向最小标注的视频上下文学习框架\n\n摘要：将多模态大语言模型（MLLMs）泛化至新兴视频领域对于实际部署至关重要，但由于标注数据稀缺，这一目标仍面临挑战。尽管上下文学习（ICL）提供了一种免训练的适应路径，但传统方法依赖于大规模标注数据池，这在工业或手术等专业场景中往往不切实际，因为这些场景需要专家进行标注。为弥补这一差距，我们提出了VIOLA（基于最小标注的视频上下文学习框架），这是一个标签高效的框架，它将最小化的专家监督与丰富的未标注数据协同结合。首先，为在严格标注预算下最大化效率，我们提出了密度-不确定性加权采样方法。与可能选择视觉异常样本的传统多样性或不确定性策略不同，我们的方法利用密度估计来识别同时具备多样性、代表性和信息量的样本。其次，为在不引入噪声传播的前提下利用剩余未标注数据，我们构建了混合数据池，并引入了置信度感知检索与置信度感知提示机制。这些机制显式建模标签可靠性，基于相似度与置信度的复合分数检索示例，同时使MLLM能够自适应地区分已验证的真实标签与噪声伪标签。通过在九个多样化基准测试中使用四种MLLM进行广泛实验，结果表明我们的框架在低资源设置下显著优于多种基线方法，实现了以最小标注成本达成鲁棒适应的目标。",
    "url": "https://huggingface.co/papers/2601.15549",
    "arxiv_url": "https://arxiv.org/abs/2601.15549"
  },
  {
    "title": "Agentic Confidence Calibration",
    "summary": "AI agents are rapidly advancing from passive language models to autonomous systems executing complex, multi-step tasks. Yet their overconfidence in failure remains a fundamental barrier to deployment in high-stakes settings. Existing calibration methods, built for static single-turn outputs, cannot address the unique challenges of agentic systems, such as compounding errors along trajectories, uncertainty from external tools, and opaque failure modes. To address these challenges, we introduce, for the first time, the problem of Agentic Confidence Calibration and propose Holistic Trajectory Calibration (HTC), a novel diagnostic framework that extracts rich process-level features ranging from macro dynamics to micro stability across an agent's entire trajectory. Powered by a simple, interpretable model, HTC consistently surpasses strong baselines in both calibration and discrimination, across eight benchmarks, multiple LLMs, and diverse agent frameworks. Beyond performance, HTC delivers three essential advances: it provides interpretability by revealing the signals behind failure, enables transferability by applying across domains without retraining, and achieves generalization through a General Agent Calibrator (GAC) that achieves the best calibration (lowest ECE) on the out-of-domain GAIA benchmark. Together, these contributions establish a new process-centric paradigm for confidence calibration, providing a framework for diagnosing and enhancing the reliability of AI agents.",
    "translation": "标题：智能体置信度校准\n\n摘要：人工智能智能体正从被动的语言模型快速演变为能够执行复杂多步骤任务的自主系统。然而，其在失败场景中的过度自信仍然是高风险场景部署的根本障碍。现有针对静态单轮输出的校准方法无法应对智能体系统的独特挑战，例如轨迹中的误差累积、外部工具的不确定性以及不透明的故障模式。为应对这些挑战，本文首次提出智能体置信度校准问题，并引入整体轨迹校准框架——一种创新的诊断框架，能够从智能体完整轨迹中提取从宏观动态到微观稳定性的丰富过程级特征。该框架基于简单可解释的模型构建，在八项基准测试、多种大语言模型及不同智能体框架中，其校准性能与判别能力均持续超越现有强基线方法。除性能优势外，该框架实现了三项重要突破：通过揭示故障背后的信号提供可解释性；无需重新训练即可跨领域应用实现可迁移性；通过通用智能体校准器实现泛化能力，该校准器在跨域GAIA基准测试中取得了最佳校准效果（最低预期校准误差）。这些成果共同建立了以过程为中心的置信度校准新范式，为诊断和提升人工智能智能体的可靠性提供了系统性框架。",
    "url": "https://huggingface.co/papers/2601.15778",
    "arxiv_url": "https://arxiv.org/abs/2601.15778"
  },
  {
    "title": "Agentic Uncertainty Quantification",
    "summary": "Although AI agents have demonstrated impressive capabilities in long-horizon reasoning, their reliability is severely hampered by the ``Spiral of Hallucination,'' where early epistemic errors propagate irreversibly. Existing methods face a dilemma: uncertainty quantification (UQ) methods typically act as passive sensors, only diagnosing risks without addressing them, while self-reflection mechanisms suffer from continuous or aimless corrections. To bridge this gap, we propose a unified Dual-Process Agentic UQ (AUQ) framework that transforms verbalized uncertainty into active, bi-directional control signals. Our architecture comprises two complementary mechanisms: System 1 (Uncertainty-Aware Memory, UAM), which implicitly propagates verbalized confidence and semantic explanations to prevent blind decision-making; and System 2 (Uncertainty-Aware Reflection, UAR), which utilizes these explanations as rational cues to trigger targeted inference-time resolution only when necessary. This enables the agent to balance efficient execution and deep deliberation dynamically. Extensive experiments on closed-loop benchmarks and open-ended deep research tasks demonstrate that our training-free approach achieves superior performance and trajectory-level calibration. We believe this principled framework AUQ represents a significant step towards reliable agents.",
    "translation": "标题：能动性不确定性量化\n\n摘要：尽管人工智能智能体在长程推理任务中展现出卓越能力，但其可靠性仍深受“幻觉螺旋”现象的制约——早期认知误差会不可逆地持续扩散。现有方法面临两难困境：不确定性量化方法通常作为被动传感器，仅能诊断风险而无法主动干预；而自我反思机制则易陷入持续或盲目的修正循环。为弥合这一鸿沟，本文提出一种统一的双过程能动性不确定性量化框架，将语言化不确定性转化为主动双向控制信号。该架构包含两个互补机制：系统一（不确定性感知记忆模块）通过隐式传播语言化置信度与语义解释来避免盲目决策；系统二（不确定性感知反思模块）则将这些解释作为理性线索，仅在必要时触发定向推理时解析。这种设计使智能体能够动态平衡高效执行与深度思辨。在闭环基准测试与开放式深度研究任务上的大量实验表明，我们这种无需训练的方法实现了卓越的性能与轨迹级校准效果。我们相信这一原则性框架标志着向可信智能体迈出了重要一步。",
    "url": "https://huggingface.co/papers/2601.15703",
    "arxiv_url": "https://arxiv.org/abs/2601.15703"
  },
  {
    "title": "From Passive Metric to Active Signal: The Evolving Role of Uncertainty Quantification in Large Language Models",
    "summary": "While Large Language Models (LLMs) show remarkable capabilities, their unreliability remains a critical barrier to deployment in high-stakes domains. This survey charts a functional evolution in addressing this challenge: the evolution of uncertainty from a passive diagnostic metric to an active control signal guiding real-time model behavior. We demonstrate how uncertainty is leveraged as an active control signal across three frontiers: in advanced reasoning to optimize computation and trigger self-correction; in autonomous agents to govern metacognitive decisions about tool use and information seeking; and in reinforcement learning to mitigate reward hacking and enable self-improvement via intrinsic rewards. By grounding these advancements in emerging theoretical frameworks like Bayesian methods and Conformal Prediction, we provide a unified perspective on this transformative trend. This survey provides a comprehensive overview, critical analysis, and practical design patterns, arguing that mastering the new trend of uncertainty is essential for building the next generation of scalable, reliable, and trustworthy AI.",
    "translation": "标题：从被动度量到主动信号：不确定性量化在大语言模型中的角色演变\n\n摘要：尽管大语言模型展现出卓越的能力，但其不可靠性仍是部署于高风险领域的关键障碍。本综述系统阐述了应对这一挑战的功能演进路径：不确定性从被动的诊断度量演变为引导实时模型行为的主动控制信号。我们展示了不确定性如何作为主动控制信号在三大前沿领域发挥作用：在高级推理中优化计算并触发自我修正；在自主智能体中调控工具使用与信息寻求的元认知决策；在强化学习中抑制奖励攻击并通过内在奖励实现自我改进。通过将这些进展锚定于贝叶斯方法和共形预测等新兴理论框架，我们为这一变革性趋势提供了统一视角。本综述提供了全面的概览、批判性分析及实用设计模式，论证了掌握不确定性的新范式对于构建下一代可扩展、可靠且可信的人工智能至关重要。",
    "url": "https://huggingface.co/papers/2601.15690",
    "arxiv_url": "https://arxiv.org/abs/2601.15690"
  },
  {
    "title": "LLM Prompt Evaluation for Educational Applications",
    "summary": "As large language models (LLMs) become increasingly common in educational applications, there is a growing need for evidence-based methods to design and evaluate LLM prompts that produce personalized and pedagogically aligned out-puts. This study presents a generalizable, systematic approach for evaluating prompts, demonstrated through an analysis of LLM-generated follow-up questions in a structured dialogue activity. Six prompt templates were designed and tested. The templates incorporated established prompt engineering patterns, with each prompt emphasizing distinct pedagogical strategies. The prompt templates were compared through a tournament-style evaluation framework that can be adapted for other educational applications. The tournament employed the Glicko2 rating system with eight judges evaluating question pairs across three dimensions: format, dialogue support, and appropriateness for learners. Data was sourced from 120 authentic user interactions across three distinct educational deployments. Results showed that a single prompt related to strategic reading out-performed other templates with win probabilities ranging from 81% to 100% in pairwise comparisons. This prompt combined persona and context manager pat-terns and was designed to support metacognitive learning strategies such as self-directed learning. The methodology showcases how educational technology re- searchers can systematically evaluate and improve prompt designs, moving beyond ad-hoc prompt engineering toward evidence-based prompt development for educational applications.",
    "translation": "标题：教育应用中的大语言模型提示评估研究\n\n摘要：随着大语言模型在教育应用中的日益普及，亟需基于实证的方法来设计和评估能够产生个性化且符合教学目标的提示语。本研究提出一种可推广的系统性提示评估方法，并通过分析结构化对话活动中大语言模型生成的后续问题进行实证演示。研究设计并测试了六种提示模板，这些模板融合了成熟的提示工程模式，每种提示侧重不同的教学策略。通过适用于各类教育场景的锦标赛式评估框架对提示模板进行比较，该框架采用Glicko2评分系统，由八位评委从格式规范性、对话支持度和学习者适配性三个维度对问题组进行评价。数据来源于三个独立教育场景中120组真实用户交互记录。结果显示，聚焦策略性阅读的单一提示模板在配对比较中以81%至100%的胜率显著优于其他模板。该提示融合了角色设定与语境管理模块，旨在支持自主学习等元认知学习策略。本方法论为教育技术研究者展示了如何系统评估并优化提示设计，推动教育应用从临时性提示工程向基于证据的提示开发范式转变。",
    "url": "https://huggingface.co/papers/2601.16134",
    "arxiv_url": "https://arxiv.org/abs/2601.16134"
  },
  {
    "title": "Numba-Accelerated 2D Diffusion-Limited Aggregation: Implementation and Fractal Characterization",
    "summary": "We present dla-ideal-solver, a high-performance framework for simulating two-dimensional Diffusion-Limited Aggregation (DLA) using Numba-accelerated Python. By leveraging just-in-time (JIT) compilation, we achieve computational throughput comparable to legacy static implementations while retaining high-level flexibility. We investigate the Laplacian growth instability across varying injection geometries and walker concentrations. Our analysis confirms the robustness of the standard fractal dimension D_f approx 1.71 for dilute regimes, consistent with the Witten-Sander universality class. However, we report a distinct crossover to Eden-like compact growth (D_f approx 1.87) in high-density environments, attributed to the saturation of the screening length. Beyond standard mass-radius scaling, we employ generalized Rényi dimensions and lacunarity metrics to quantify the monofractal character and spatial heterogeneity of the aggregates. This work establishes a reproducible, open-source testbed for exploring phase transitions in non-equilibrium statistical mechanics.",
    "translation": "标题：基于Numba加速的二维扩散限制聚集模拟：实现方法与分形表征\n\n摘要：本文提出dla-ideal-solver——一个基于Numba加速Python的高性能二维扩散限制聚集模拟框架。通过即时编译技术，该框架在保持高级编程灵活性的同时，实现了与传统静态编译方案相当的计算吞吐量。我们系统研究了不同注入几何结构与行走粒子浓度下的拉普拉斯生长不稳定性。分析结果证实，在低浓度体系中标准分形维数D_f≈1.71具有稳健性，符合Witten-Sander普适性类别。然而，在高密度环境中我们观察到向类伊甸园致密生长模式（D_f≈1.87）的显著跨越现象，这归因于屏蔽长度的饱和效应。除标准质量-半径标度分析外，本研究采用广义Rényi维数与空隙度度量来量化聚集体的单分形特征与空间异质性。本工作为探索非平衡统计力学中的相变现象建立了一个可复现的开源测试平台。",
    "url": "https://huggingface.co/papers/2601.15440",
    "arxiv_url": "https://arxiv.org/abs/2601.15440"
  },
  {
    "title": "MirrorBench: An Extensible Framework to Evaluate User-Proxy Agents for Human-Likeness",
    "summary": "Large language models (LLMs) are increasingly used as human simulators, both for evaluating conversational systems and for generating fine-tuning data. However, naive \"act-as-a-user\" prompting often yields verbose, unrealistic utterances, underscoring the need for principled evaluation of so-called user proxy agents. We present MIRRORBENCH, a reproducible, extensible benchmarking framework that evaluates user proxies solely on their ability to produce human-like user utterances across diverse conversational tasks, explicitly decoupled from downstream task success. MIRRORBENCH features a modular execution engine with typed interfaces, metadata-driven registries, multi-backend support, caching, and robust observability. The system supports pluggable user proxies, datasets, tasks, and metrics, enabling researchers to evaluate arbitrary simulators under a uniform, variance-aware harness. We include three lexical-diversity metrics (MATTR, YULE'S K, and HD-D) and three LLM-judge-based metrics (GTEval, Pairwise Indistinguishability, and Rubric-and-Reason). Across four open datasets, MIRRORBENCH yields variance-aware results and reveals systematic gaps between user proxies and real human users. The framework is open source and includes a simple command-line interface for running experiments, managing configurations and caching, and generating reports. The framework can be accessed at https://github.com/SAP/mirrorbench.",
    "translation": "标题：MirrorBench：一个用于评估拟人化用户代理的可扩展框架\n\n摘要：大语言模型正日益被用作人类模拟器，既用于评估对话系统，也用于生成微调数据。然而，简单的“扮演用户”提示往往会产生冗长、不真实的语句，这凸显了对所谓用户代理进行系统性评估的必要性。我们提出了MIRRORBENCH，这是一个可复现、可扩展的基准测试框架，其评估用户代理的唯一标准是它们在不同对话任务中生成拟人化用户语句的能力，并明确与下游任务的成功解耦。MIRRORBENCH采用模块化执行引擎，具备类型化接口、元数据驱动的注册机制、多后端支持、缓存功能和强大的可观测性。该系统支持可插拔的用户代理、数据集、任务和评估指标，使研究人员能够在统一且考虑方差影响的框架下评估任意模拟器。我们引入了三个词汇多样性指标（MATTR、YULE'S K和HD-D）和三个基于大语言模型评判的指标（GTEval、成对不可区分性和规则推理评估）。在四个开源数据集上的测试表明，MIRRORBENCH能够提供考虑方差的结果，并揭示用户代理与真实人类用户之间的系统性差距。该框架已开源，并包含一个简单的命令行界面，用于运行实验、管理配置与缓存以及生成报告。框架可通过 https://github.com/SAP/mirrorbench 访问。",
    "url": "https://huggingface.co/papers/2601.08118",
    "arxiv_url": "https://arxiv.org/abs/2601.08118"
  },
  {
    "title": "Wigner's Friend as a Circuit: Inter-Branch Communication Witness Benchmarks on Superconducting Quantum Hardware",
    "summary": "We implement and benchmark on IBM Quantum hardware the circuit family proposed by Violaris for estimating operational inter-branch communication witnesses, defined as correlations in classical measurement records produced by compiled Wigner's-friend-style circuits. We realize a five-qubit instance of the protocol as an inter-register message-transfer pattern within a single circuit, rather than physical signaling, and evaluate its behavior under realistic device noise and compilation constraints. The circuit encodes branch-conditioned evolution of an observer subsystem whose dynamics depend on a control qubit, followed by a controlled transfer operation that probes correlations between conditional measurement contexts.\n  Executing on the ibm_fez backend with 20000 shots, we observe population-based visibility of 0.877, coherence witnesses of 0.840 and -0.811 along orthogonal axes, and a phase-sensitive magnitude of approximately 1.17. While the visibility metric is insensitive to some classes of dephasing, the coherence witnesses provide complementary sensitivity to off-diagonal noise.\n  This work does not test or discriminate among interpretations of quantum mechanics. Instead, it provides a reproducible operational constraint pipeline for evaluating detectability of non-ideal channels relative to calibrated device noise.",
    "translation": "标题：作为电路的维格纳之友：超导量子硬件上的分支间通信见证基准测试\n\n摘要：我们在IBM量子硬件上实现并基准测试了Violaris提出的用于估计操作型分支间通信见证的电路族，该见证定义为由编译的维格纳之友式电路产生的经典测量记录中的关联性。我们将该协议的五量子比特实例实现为单个电路内的寄存器间消息传递模式（而非物理信号传输），并在实际设备噪声和编译约束下评估其行为。该电路编码了观察者子系统的分支条件演化，其动力学依赖于一个控制量子比特，随后通过受控传输操作来探测条件测量上下文之间的关联性。\n\n在ibm_fez后端上执行20000次测量，我们观察到基于布居数的可见度为0.877，沿正交轴的相干性见证值分别为0.840和-0.811，相位敏感幅度约为1.17。虽然可见度度量对某些类型的退相不敏感，但相干性见证提供了对非对角噪声的互补敏感性。\n\n本研究并非对量子力学解释进行检验或区分，而是提供了一个可复现的操作约束流程，用于评估非理想信道相对于校准设备噪声的可检测性。",
    "url": "https://huggingface.co/papers/2601.16004",
    "arxiv_url": "https://arxiv.org/abs/2601.16004"
  }
]