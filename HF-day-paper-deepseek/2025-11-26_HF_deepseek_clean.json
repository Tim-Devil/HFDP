[
  {
    "title": "GigaEvo: An Open Source Optimization Framework Powered By LLMs And Evolution Algorithms",
    "summary": "Recent advances in LLM-guided evolutionary computation, particularly AlphaEvolve (Novikov et al., 2025; Georgiev et al., 2025), have demonstrated remarkable success in discovering novel mathematical constructions and solving challenging optimization problems. However, the high-level descriptions in published work leave many implementation details unspecified, hindering reproducibility and further research. In this report we present GigaEvo, an extensible open-source framework that enables researchers to study and experiment with hybrid LLM-evolution approaches inspired by AlphaEvolve. Our system provides modular implementations of key components: MAP-Elites quality-diversity algorithms, asynchronous DAG-based evaluation pipelines, LLM-driven mutation operators with insight generation and bidirectional lineage tracking, and flexible multi-island evolutionary strategies. In order to assess reproducibility and validate our implementation we evaluate GigaEvo on challenging problems from the AlphaEvolve paper: Heilbronn triangle placement, circle packing in squares, and high-dimensional kissing numbers. The framework emphasizes modularity, concurrency, and ease of experimentation, enabling rapid prototyping through declarative configuration. We provide detailed descriptions of system architecture, implementation decisions, and experimental methodology to support further research in LLM driven evolutionary methods. The GigaEvo framework and all experimental code are available at https://github.com/AIRI-Institute/gigaevo-core.",
    "translation": "标题：GigaEvo：基于大语言模型与进化算法的开源优化框架\n\n摘要：大语言模型引导的进化计算领域近期取得重要进展，特别是AlphaEvolve系列研究（Novikov等人，2025；Georgiev等人，2025）在发现新型数学构造与解决复杂优化问题方面展现出卓越成效。然而现有成果的高度抽象描述导致诸多实现细节缺失，阻碍了研究的可复现性与深入探索。本报告提出GigaEvo——一个受AlphaEvolve启发的可扩展开源框架，支持研究人员对混合式LLM-进化方法进行系统化研究与实验。该框架提供以下核心组件的模块化实现：MAP-Elites质量多样性算法、基于有向无环图的异步评估流水线、具备洞察生成与双向谱系追踪功能的LLM驱动变异算子，以及灵活的多岛屿进化策略。为验证实现正确性与可复现性，我们在AlphaEvolve论文涉及的典型难题上进行测试：海伦布朗三角形布局、正方形内圆填充及高维接吻数问题。本框架强调模块化设计、并行计算与实验便捷性，支持通过声明式配置实现快速原型验证。我们详细阐述了系统架构、实现决策与实验方法学，以推动LLM驱动进化方法的持续研究。GigaEvo框架及全部实验代码已开源：https://github.com/AIRI-Institute/gigaevo-core。",
    "url": "https://huggingface.co/papers/2511.17592",
    "arxiv_url": "https://arxiv.org/abs/2511.17592"
  },
  {
    "title": "MedSAM3: Delving into Segment Anything with Medical Concepts",
    "summary": "Medical image segmentation is fundamental for biomedical discovery. Existing methods lack generalizability and demand extensive, time-consuming manual annotation for new clinical application. Here, we propose MedSAM-3, a text promptable medical segmentation model for medical image and video segmentation. By fine-tuning the Segment Anything Model (SAM) 3 architecture on medical images paired with semantic conceptual labels, our MedSAM-3 enables medical Promptable Concept Segmentation (PCS), allowing precise targeting of anatomical structures via open-vocabulary text descriptions rather than solely geometric prompts. We further introduce the MedSAM-3 Agent, a framework that integrates Multimodal Large Language Models (MLLMs) to perform complex reasoning and iterative refinement in an agent-in-the-loop workflow. Comprehensive experiments across diverse medical imaging modalities, including X-ray, MRI, Ultrasound, CT, and video, demonstrate that our approach significantly outperforms existing specialist and foundation models. We will release our code and model at https://github.com/Joey-S-Liu/MedSAM3.",
    "translation": "标题：MedSAM3：融合医学概念的可分割万物模型探究\n\n摘要：医学图像分割是生物医学发现的基础。现有方法缺乏泛化能力，且在新的临床应用场景中需要大量耗时的人工标注。本文提出MedSAM-3，一种支持文本提示的医学图像与视频分割模型。通过在配对的医学图像与语义概念标签上微调Segment Anything Model (SAM) 3架构，我们的MedSAM-3实现了医学可提示概念分割功能，能够通过开放词汇文本描述而非仅依赖几何提示来精确定位解剖结构。我们进一步提出MedSAM-3智能体框架，该框架集成多模态大语言模型，在智能体参与的工作流中执行复杂推理与迭代优化。涵盖X射线、磁共振、超声、CT及视频等多种医学影像模式的综合实验表明，本方法显著优于现有专业模型与基础模型。代码与模型将在https://github.com/Joey-S-Liu/MedSAM3发布。",
    "url": "https://huggingface.co/papers/2511.19046",
    "arxiv_url": "https://arxiv.org/abs/2511.19046"
  },
  {
    "title": "Agent0-VL: Exploring Self-Evolving Agent for Tool-Integrated Vision-Language Reasoning",
    "summary": "Vision-language agents have achieved remarkable progress in a variety of multimodal reasoning tasks; however, their learning remains constrained by the limitations of human-annotated supervision. Recent self-rewarding approaches attempt to overcome this constraint by allowing models to act as their own critics or reward providers. Yet, purely text-based self-evaluation struggles to verify complex visual reasoning steps and often suffers from evaluation hallucinations. To address these challenges, inspired by recent advances in tool-integrated reasoning, we propose Agent0-VL, a self-evolving vision-language agent that achieves continual improvement with tool-integrated reasoning. Agent0-VL incorporates tool usage not only into reasoning but also into self-evaluation and self-repair, enabling the model to introspect, verify, and refine its reasoning through evidence-grounded analysis. It unifies two synergistic roles within a single LVLM: a Solver that performs multi-turn tool-integrated reasoning, and a Verifier that generates structured feedback and fine-grained self-rewards through tool-grounded critique. These roles interact through a Self-Evolving Reasoning Cycle, where tool-based verification and reinforcement learning jointly align the reasoning and evaluation distributions for stable self-improvement. Through this zero-external-reward evolution, Agent0-VL aligns its reasoning and verification behaviors without any human annotation or external reward models, achieving continual self-improvement. Experiments on geometric problem solving and visual scientific analysis show that Agent0-VL achieves an 12.5% improvement over the base model. Our code is available at https://github.com/aiming-lab/Agent0/Agent0-VL{this https URL}.",
    "translation": "标题：Agent0-VL：面向工具集成视觉语言推理的自演进智能体探索\n\n摘要：视觉语言智能体在多模态推理任务中取得了显著进展，但其学习过程仍受限于人工标注监督的约束。近期自奖励方法尝试通过让模型充当自身评判者或奖励提供者来突破这一限制。然而，纯文本的自评估难以验证复杂的视觉推理步骤，且常出现评估幻觉问题。为解决这些挑战，受工具集成推理最新进展的启发，我们提出Agent0-VL——一种通过工具集成推理实现持续进化的自演进视觉语言智能体。该框架将工具使用不仅融入推理过程，还扩展至自我评估与自我修正环节，使模型能够通过证据驱动分析实现推理过程的反思、验证与优化。我们在单个大视觉语言模型中统一了两个协同角色：执行多轮工具集成推理的求解器，以及通过工具化批判生成结构化反馈与细粒度自奖励的验证器。这些角色通过“自演进推理循环”进行交互，其中基于工具的验证与强化学习共同对齐推理与评估分布，实现稳定的自我提升。通过这种零外部奖励的演进机制，Agent0-VL在无需人工标注或外部奖励模型的情况下，实现了推理行为与验证行为的自对齐和持续改进。在几何问题求解和视觉科学分析任务上的实验表明，Agent0-VL相比基线模型性能提升12.5%。代码已开源：https://github.com/aiming-lab/Agent0/Agent0-VL{此https链接}。",
    "url": "https://huggingface.co/papers/2511.19900",
    "arxiv_url": "https://arxiv.org/abs/2511.19900"
  },
  {
    "title": "SteadyDancer: Harmonized and Coherent Human Image Animation with First-Frame Preservation",
    "summary": "Preserving first-frame identity while ensuring precise motion control is a fundamental challenge in human image animation. The Image-to-Motion Binding process of the dominant Reference-to-Video (R2V) paradigm overlooks critical spatio-temporal misalignments common in real-world applications, leading to failures such as identity drift and visual artifacts. We introduce SteadyDancer, an Image-to-Video (I2V) paradigm-based framework that achieves harmonized and coherent animation and is the first to ensure first-frame preservation robustly. Firstly, we propose a Condition-Reconciliation Mechanism to harmonize the two conflicting conditions, enabling precise control without sacrificing fidelity. Secondly, we design Synergistic Pose Modulation Modules to generate an adaptive and coherent pose representation that is highly compatible with the reference image. Finally, we employ a Staged Decoupled-Objective Training Pipeline that hierarchically optimizes the model for motion fidelity, visual quality, and temporal coherence. Experiments demonstrate that SteadyDancer achieves state-of-the-art performance in both appearance fidelity and motion control, while requiring significantly fewer training resources than comparable methods.",
    "translation": "标题：SteadyDancer：基于首帧保持的协调连贯人体图像动画生成框架\n\n摘要：在人体图像动画生成中，如何保持首帧身份特征同时实现精准运动控制是核心挑战。主流参考视频生成范式中的图像-运动绑定过程忽略了实际应用中常见的时空错位问题，导致身份特征漂移和视觉伪影等故障。本文提出SteadyDancer——基于图像到视频范式的创新框架，该框架不仅能实现协调连贯的动画生成，更是首个能稳健保证首帧保持的方法。首先，我们提出条件协调机制来调和两种冲突条件，在保持保真度的同时实现精准控制；其次，设计协同姿态调制模块以生成与参考图像高度兼容的自适应连贯姿态表征；最后，采用阶段式解耦目标训练流程，分层优化模型的运动保真度、视觉质量与时序连贯性。实验表明，SteadyDancer在外观保真度与运动控制方面均达到最先进性能，且所需训练资源显著低于同类方法。",
    "url": "https://huggingface.co/papers/2511.19320",
    "arxiv_url": "https://arxiv.org/abs/2511.19320"
  },
  {
    "title": "iMontage: Unified, Versatile, Highly Dynamic Many-to-many Image Generation",
    "summary": "Pre-trained video models learn powerful priors for generating high-quality, temporally coherent content. While these models excel at temporal coherence, their dynamics are often constrained by the continuous nature of their training data. We hypothesize that by injecting the rich and unconstrained content diversity from image data into this coherent temporal framework, we can generate image sets that feature both natural transitions and a far more expansive dynamic range. To this end, we introduce iMontage, a unified framework designed to repurpose a powerful video model into an all-in-one image generator. The framework consumes and produces variable-length image sets, unifying a wide array of image generation and editing tasks. To achieve this, we propose an elegant and minimally invasive adaptation strategy, complemented by a tailored data curation process and training paradigm. This approach allows the model to acquire broad image manipulation capabilities without corrupting its invaluable original motion priors. iMontage excels across several mainstream many-in-many-out tasks, not only maintaining strong cross-image contextual consistency but also generating scenes with extraordinary dynamics that surpass conventional scopes. Find our homepage at: https://kr1sjfu.github.io/iMontage-web/.",
    "translation": "标题：iMontage：统一化、多功能、高动态范围的多元图像生成框架\n\n摘要：预训练视频模型通过学习强大的先验知识，能够生成高质量且时序连贯的内容。尽管这些模型在时序连贯性方面表现卓越，但其动态范围常受训练数据连续性质的限制。我们提出假设：通过将图像数据中丰富且无约束的内容多样性注入这一连贯时序框架，可生成兼具自然过渡与更广阔动态范围的图像集合。为此，我们推出iMontage——一个将强大视频模型重构为一体化图像生成器的统一框架。该框架支持可变长度图像集的输入与输出，可统一执行各类图像生成与编辑任务。为实现这一目标，我们提出了一种精巧且侵入性最低的适配策略，并辅以定制化数据筛选流程与训练范式。该方法使模型在保持原有宝贵运动先验不受损害的前提下，获得广泛的图像操控能力。iMontage在多项主流多输入多输出任务中表现卓越，不仅能保持强劲的跨图像上下文一致性，还可生成突破传统界限的极致动态场景。项目主页详见：https://kr1sjfu.github.io/iMontage-web/。",
    "url": "https://huggingface.co/papers/2511.20635",
    "arxiv_url": "https://arxiv.org/abs/2511.20635"
  },
  {
    "title": "Does Understanding Inform Generation in Unified Multimodal Models? From Analysis to Path Forward",
    "summary": "Recent years have witnessed significant progress in Unified Multimodal Models, yet a fundamental question remains: Does understanding truly inform generation? To investigate this, we introduce UniSandbox, a decoupled evaluation framework paired with controlled, synthetic datasets to avoid data leakage and enable detailed analysis. Our findings reveal a significant understanding-generation gap, which is mainly reflected in two key dimensions: reasoning generation and knowledge transfer. Specifically, for reasoning generation tasks, we observe that explicit Chain-of-Thought (CoT) in the understanding module effectively bridges the gap, and further demonstrate that a self-training approach can successfully internalize this ability, enabling implicit reasoning during generation. Additionally, for knowledge transfer tasks, we find that CoT assists the generative process by helping retrieve newly learned knowledge, and also discover that query-based architectures inherently exhibit latent CoT-like properties that affect this transfer. UniSandbox provides preliminary insights for designing future unified architectures and training strategies that truly bridge the gap between understanding and generation. Code and data are available at https://github.com/PKU-YuanGroup/UniSandBox",
    "translation": "标题：理解能力是否促进统一多模态模型的生成能力？从分析到发展路径\n\n摘要：近年来统一多模态模型取得了显著进展，但一个根本性问题依然存在：理解能力是否真正促进生成能力？为探究此问题，我们提出UniSandbox——一个结合受控合成数据的解耦评估框架，可避免数据泄漏并支持细粒度分析。研究结果揭示了理解与生成之间存在显著的能力鸿沟，主要体现在推理生成与知识迁移两个维度。具体而言，在推理生成任务中，我们发现理解模块中显式的思维链能有效弥合这一鸿沟，并通过自训练方法成功将该能力内化，实现生成过程中的隐式推理。在知识迁移任务中，思维链通过辅助检索新习得知识来促进生成过程，同时发现基于查询的架构天然具备影响知识迁移的类思维链潜质。UniSandbox为设计真正贯通理解与生成能力的统一架构与训练策略提供了重要洞见。代码与数据详见：https://github.com/PKU-YuanGroup/UniSandBox",
    "url": "https://huggingface.co/papers/2511.20561",
    "arxiv_url": "https://arxiv.org/abs/2511.20561"
  },
  {
    "title": "GigaWorld-0: World Models as Data Engine to Empower Embodied AI",
    "summary": "World models are emerging as a foundational paradigm for scalable, data-efficient embodied AI. In this work, we present GigaWorld-0, a unified world model framework designed explicitly as a data engine for Vision-Language-Action (VLA) learning. GigaWorld-0 integrates two synergistic components: GigaWorld-0-Video, which leverages large-scale video generation to produce diverse, texture-rich, and temporally coherent embodied sequences under fine-grained control of appearance, camera viewpoint, and action semantics; and GigaWorld-0-3D, which combines 3D generative modeling, 3D Gaussian Splatting reconstruction, physically differentiable system identification, and executable motion planning to ensure geometric consistency and physical realism. Their joint optimization enables the scalable synthesis of embodied interaction data that is visually compelling, spatially coherent, physically plausible, and instruction-aligned. Training at scale is made feasible through our efficient GigaTrain framework, which exploits FP8-precision and sparse attention to drastically reduce memory and compute requirements. We conduct comprehensive evaluations showing that GigaWorld-0 generates high-quality, diverse, and controllable data across multiple dimensions. Critically, VLA model (e.g., GigaBrain-0) trained on GigaWorld-0-generated data achieve strong real-world performance, significantly improving generalization and task success on physical robots without any real-world interaction during training.",
    "translation": "标题：GigaWorld-0：将世界模型作为数据引擎赋能具身智能体\n\n摘要：世界模型正逐渐成为可扩展、数据高效的具身智能体的基础范式。本研究提出GigaWorld-0——一个专门作为视觉-语言-动作学习数据引擎设计的统一世界模型框架。该框架包含两个协同组件：GigaWorld-0-Video通过大规模视频生成，在外观、摄像机视角和动作语义的细粒度控制下，生成具有丰富纹理、时序连贯的多样化具身序列；GigaWorld-0-3D则融合三维生成建模、3D高斯溅射重建、物理可微分系统辨识与可执行运动规划，确保几何一致性与物理真实性。二者的联合优化实现了视觉吸引力、空间连贯性、物理合理性与指令对齐的具身交互数据规模化合成。通过我们研发的高效GigaTrain训练框架——利用FP8精度与稀疏注意力显著降低内存与计算需求——实现了大规模训练可行性。综合评估表明，GigaWorld-0能在多维度生成高质量、多样化且可控的数据。关键的是，基于GigaWorld-0生成数据训练的视觉-语言-动作模型（如GigaBrain-0）在现实场景中表现出色，在训练阶段未接触真实世界交互的情况下，显著提升了物理机器人的泛化能力与任务成功率。",
    "url": "https://huggingface.co/papers/2511.19861",
    "arxiv_url": "https://arxiv.org/abs/2511.19861"
  },
  {
    "title": "Soft Adaptive Policy Optimization",
    "summary": "Reinforcement learning (RL) plays an increasingly important role in enhancing the reasoning capabilities of large language models (LLMs), yet stable and performant policy optimization remains challenging. Token-level importance ratios often exhibit high variance-a phenomenon exacerbated in Mixture-of-Experts models-leading to unstable updates. Existing group-based policy optimization methods, such as GSPO and GRPO, alleviate this problem via hard clipping, making it difficult to maintain both stability and effective learning. We propose Soft Adaptive Policy Optimization (SAPO), which replaces hard clipping with a smooth, temperature-controlled gate that adaptively attenuates off-policy updates while preserving useful learning signals. Compared with GSPO and GRPO, SAPO is both sequence-coherent and token-adaptive. Like GSPO, SAPO maintains sequence-level coherence, but its soft gating forms a continuous trust region that avoids the brittle hard clipping band used in GSPO. When a sequence contains a few highly off-policy tokens, GSPO suppresses all gradients for that sequence, whereas SAPO selectively down-weights only the offending tokens and preserves the learning signal from the near-on-policy ones, improving sample efficiency. Relative to GRPO, SAPO replaces hard token-level clipping with smooth, temperature-controlled scaling, enabling more informative and stable updates. Empirical results on mathematical reasoning benchmarks indicate that SAPO exhibits improved training stability and higher Pass@1 performance under comparable training budgets. Moreover, we employ SAPO to train the Qwen3-VL model series, demonstrating that SAPO yields consistent performance gains across diverse tasks and different model sizes. Overall, SAPO provides a more reliable, scalable, and effective optimization strategy for RL training of LLMs.",
    "translation": "标题：软自适应策略优化\n\n摘要：强化学习在提升大语言模型推理能力方面日益重要，但实现稳定高效的策略优化仍具挑战性。令牌级重要性比率常呈现高方差特性——该现象在混合专家模型中尤为显著——导致策略更新不稳定。现有基于分组的策略优化方法（如GSPO和GRPO）通过硬截断缓解该问题，但难以兼顾稳定性与学习效能。本文提出软自适应策略优化方法（SAPO），采用平滑的温度控制门替代硬截断，在保持有效学习信号的同时自适应衰减离策略更新。相较于GSPO与GRPO，SAPO兼具序列连贯性与令牌自适应性：与GSPO类似，SAPO保持序列级连贯性，但其软门控形成的连续信任区域避免了GSPO采用的脆弱硬截断带；当序列包含少量强离策略令牌时，GSPO会抑制整个序列梯度，而SAPO仅选择性降权异常令牌并保留近策略令牌的学习信号，从而提升样本效率。相对于GRPO，SAPO以平滑的温度控制缩放取代硬令牌截断，实现更具信息量的稳定更新。数学推理基准测试表明，在相同训练预算下，SAPO展现出更优的训练稳定性与Pass@1性能。此外，我们应用SAPO训练Qwen3-VL模型系列，证明该方法能在不同任务和模型规模下获得持续性能提升。总体而言，SAPO为大语言模型的强化学习训练提供了更可靠、可扩展且高效的优化策略。",
    "url": "https://huggingface.co/papers/2511.20347",
    "arxiv_url": "https://arxiv.org/abs/2511.20347"
  },
  {
    "title": "SSA: Sparse Sparse Attention by Aligning Full and Sparse Attention Outputs in Feature Space",
    "summary": "The quadratic complexity of full attention limits efficient long-context processing in large language models (LLMs). Sparse attention mitigates this cost by restricting each query to attend to a subset of previous tokens; however, training-free approaches often lead to severe performance degradation. Native sparse-attention methods (e.g., NSA, MoBA) alleviate this issue, yet exhibit a critical paradox: they produce lower attention sparsity than full-attention models, despite aiming to approximate full attention, which may constrain their effectiveness. We attribute this paradox to gradient update deficiency: low-ranked key-value pairs excluded during sparse training receive neither forward contribution nor backward gradients, and thus never learn proper suppression. To overcome this limitation, we propose SSA (Sparse Sparse Attention), a unified training framework that considers both sparse and full attention and enforces bidirectional alignment at every layer. This design preserves gradient flow to all tokens while explicitly encouraging sparse-attention outputs to align with their full-attention counterparts, thereby promoting stronger sparsity. As a result, SSA achieves state-of-the-art performance under both sparse and full attention inference across multiple commonsense benchmarks. Furthermore, SSA enables models to adapt smoothly to varying sparsity budgets; performance improves consistently as more tokens are allowed to attend, supporting flexible compute-performance trade-offs at inference time. Finally, we show that native sparse-attention training surprisingly improves long-context extrapolation by mitigating the over-allocation of attention values in sink areas, with SSA demonstrating the strongest extrapolation capability.",
    "translation": "标题：SSA：通过特征空间中对齐完全注意力与稀疏注意力输出的稀疏稀疏注意力机制\n\n摘要：完全注意力机制的二次计算复杂度限制了大型语言模型（LLM）中长上下文的高效处理。稀疏注意力通过限制每个查询仅关注前文标记的子集来降低计算成本，然而无需训练的方法通常会导致严重的性能下降。原生稀疏注意力方法（如NSA、MoBA）虽能缓解此问题，却呈现出关键悖论：尽管旨在逼近完全注意力，其产生的注意力稀疏度反而低于完全注意力模型，这可能制约其有效性。我们将此悖论归因于梯度更新缺陷：在稀疏训练期间被排除的低秩键值对既无前向贡献亦无反向梯度，因而无法学习有效抑制。为突破此局限，我们提出SSA（稀疏稀疏注意力）——一个同时考虑稀疏与完全注意力的统一训练框架，通过在每层强制执行双向对齐。该设计在保持所有标记梯度流动的同时，显式推动稀疏注意力输出对齐其完全注意力对应项，从而增强稀疏性。实验表明，SSA在多个常识推理基准测试中，无论采用稀疏还是完全注意力推理，均实现了最先进性能。此外，SSA使模型能自适应不同稀疏度预算：随着可关注标记数增加，性能持续提升，支持推理阶段灵活的计算-性能权衡。最后，我们发现原生稀疏注意力训练通过缓解注意力值在汇聚区的过度分配，意外提升了长上下文外推能力，其中SSA展现出最强的外推性能。",
    "url": "https://huggingface.co/papers/2511.20102",
    "arxiv_url": "https://arxiv.org/abs/2511.20102"
  },
  {
    "title": "UltraViCo: Breaking Extrapolation Limits in Video Diffusion Transformers",
    "summary": "Despite advances, video diffusion transformers still struggle to generalize beyond their training length, a challenge we term video length extrapolation. We identify two failure modes: model-specific periodic content repetition and a universal quality degradation. Prior works attempt to solve repetition via positional encodings, overlooking quality degradation and achieving only limited extrapolation. In this paper, we revisit this challenge from a more fundamental view: attention maps, which directly govern how context influences outputs. We identify that both failure modes arise from a unified cause: attention dispersion, where tokens beyond the training window dilute learned attention patterns. This leads to quality degradation and repetition emerges as a special case when this dispersion becomes structured into periodic attention patterns, induced by harmonic properties of positional encodings. Building on this insight, we propose UltraViCo, a training-free, plug-and-play method that suppresses attention for tokens beyond the training window via a constant decay factor. By jointly addressing both failure modes, we outperform a broad set of baselines largely across models and extrapolation ratios, pushing the extrapolation limit from 2x to 4x. Remarkably, it improves Dynamic Degree and Imaging Quality by 233% and 40.5% over the previous best method at 4x extrapolation. Furthermore, our method generalizes seamlessly to downstream tasks such as controllable video synthesis and editing.",
    "translation": "标题：UltraViCo：突破视频扩散变换器的外推极限\n\n摘要：尽管技术不断进步，视频扩散变换器仍难以泛化至超出其训练时长的范围，这一挑战我们称之为视频长度外推问题。我们识别出两种失效模式：模型特定的周期性内容重复和普遍存在的质量下降。先前研究尝试通过位置编码解决重复问题，却忽视了质量下降且仅实现有限的外推能力。本文从更本质的视角——直接控制上下文如何影响输出的注意力机制——重新审视这一挑战。我们发现两种失效模式源于统一原因：注意力分散，即超出训练窗口的令牌会稀释已学习的注意力模式。这导致质量下降，而当这种分散因位置编码的谐波特性形成周期性注意力模式时，内容重复便作为特例出现。基于此洞见，我们提出UltraViCo，一种无需训练即插即用的方法，通过恒定衰减因子抑制训练窗口外令牌的注意力。通过同时解决两种失效模式，我们在多种模型和外推比例下显著超越现有基线方法，将外推极限从2倍提升至4倍。值得注意的是，在4倍外推时，我们的方法在动态程度和成像质量指标上分别较先前最佳方法提升233%和40.5%。此外，本方法可无缝泛化至可控视频生成与编辑等下游任务。\n\n请按照以下格式返回：\n标题：UltraViCo：突破视频扩散变换器的外推极限\n摘要：尽管技术不断进步，视频扩散变换器仍难以泛化至超出其训练时长的范围，这一挑战我们称之为视频长度外推问题。我们识别出两种失效模式：模型特定的周期性内容重复和普遍存在的质量下降。先前研究尝试通过位置编码解决重复问题，却忽视了质量下降且仅实现有限的外推能力。本文从更本质的视角——直接控制上下文如何影响输出的注意力机制——重新审视这一挑战。我们发现两种失效模式源于统一原因：注意力分散，即超出训练窗口的令牌会稀释已学习的注意力模式。这导致质量下降，而当这种分散因位置编码的谐波特性形成周期性注意力模式时，内容重复便作为特例出现。基于此洞见，我们提出UltraViCo，一种无需训练即插即用的方法，通过恒定衰减因子抑制训练窗口外令牌的注意力。通过同时解决两种失效模式，我们在多种模型和外推比例下显著超越现有基线方法，将外推极限从2倍提升至4倍。值得注意的是，在4倍外推时，我们的方法在动态程度和成像质量指标上分别较先前最佳方法提升233%和40.5%。此外，本方法可无缝泛化至可控视频生成与编辑等下游任务。",
    "url": "https://huggingface.co/papers/2511.20123",
    "arxiv_url": "https://arxiv.org/abs/2511.20123"
  },
  {
    "title": "ROOT: Robust Orthogonalized Optimizer for Neural Network Training",
    "summary": "The optimization of large language models (LLMs) remains a critical challenge, particularly as model scaling exacerbates sensitivity to algorithmic imprecision and training instability. Recent advances in optimizers have improved convergence efficiency through momentum orthogonalization, but suffer from two key robustness limitations: dimensional fragility in orthogonalization precision and vulnerability to outlier-induced noise. To address these robustness challenges, we introduce ROOT, a Robust Orthogonalized Optimizer that enhances training stability through dual robustness mechanisms. First, we develop a dimension-robust orthogonalization scheme using adaptive Newton iterations with fine-grained coefficients tailored to specific matrix sizes, ensuring consistent precision across diverse architectural configurations. Second, we introduce an optimization-robust framework via proximal optimization that suppresses outlier noise while preserving meaningful gradient directions. Extensive experiments demonstrate that ROOT achieves significantly improved robustness, with faster convergence and superior final performance compared to both Muon and Adam-based optimizers, particularly in noisy and non-convex scenarios. Our work establishes a new paradigm for developing robust and precise optimizers capable of handling the complexities of modern large-scale model training. The code will be available at https://github.com/huawei-noah/noah-research/tree/master/ROOT.",
    "translation": "标题：ROOT：面向神经网络训练的鲁棒正交化优化器\n\n摘要：大语言模型的优化仍是关键挑战，尤其在模型规模扩大加剧算法不精确性与训练不稳定性的背景下。当前优化器虽通过动量正交化提升了收敛效率，却存在两大鲁棒性缺陷：正交化精度的维度敏感性及异常值引发噪声的脆弱性。为解决这些鲁棒性问题，我们提出ROOT——一种通过双重鲁棒机制增强训练稳定性的鲁棒正交化优化器。首先，我们设计了维度鲁棒的正交化方案，采用适配特定矩阵尺寸的细粒度系数进行自适应牛顿迭代，确保在不同架构配置下保持稳定精度。其次，我们通过近端优化构建优化鲁棒框架，在保留有效梯度方向的同时抑制异常噪声。大量实验表明，相较于Muon和基于Adam的优化器，ROOT在噪声环境与非凸场景中显著提升鲁棒性，实现更快的收敛速度与更优的最终性能。本研究为开发能够应对现代大规模模型训练复杂性的鲁棒精确优化器确立了新范式。代码将发布于：https://github.com/huawei-noah/noah-research/tree/master/ROOT。",
    "url": "https://huggingface.co/papers/2511.20626",
    "arxiv_url": "https://arxiv.org/abs/2511.20626"
  },
  {
    "title": "MagicWorld: Interactive Geometry-driven Video World Exploration",
    "summary": "Recent interactive video world model methods generate scene evolution conditioned on user instructions. Although they achieve impressive results, two key limitations remain. First, they fail to fully exploit the correspondence between instruction-driven scene motion and the underlying 3D geometry, which results in structural instability under viewpoint changes. Second, they easily forget historical information during multi-step interaction, resulting in error accumulation and progressive drift in scene semantics and structure. To address these issues, we propose MagicWorld, an interactive video world model that integrates 3D geometric priors and historical retrieval. MagicWorld starts from a single scene image, employs user actions to drive dynamic scene evolution, and autoregressively synthesizes continuous scenes. We introduce the Action-Guided 3D Geometry Module (AG3D), which constructs a point cloud from the first frame of each interaction and the corresponding action, providing explicit geometric constraints for viewpoint transitions and thereby improving structural consistency. We further propose History Cache Retrieval (HCR) mechanism, which retrieves relevant historical frames during generation and injects them as conditioning signals, helping the model utilize past scene information and mitigate error accumulation. Experimental results demonstrate that MagicWorld achieves notable improvements in scene stability and continuity across interaction iterations.",
    "translation": "标题：MagicWorld：基于几何驱动的交互式视频世界探索系统\n\n摘要：现有交互式视频世界建模方法能够根据用户指令生成场景演化内容，虽取得显著成果，但仍存在两个关键局限：其一，未能充分利用指令驱动场景运动与底层三维几何的对应关系，导致视角变换下的结构失稳；其二，在多步交互过程中易丢失历史信息，引发场景语义与结构的误差累积及渐进漂移。为解决上述问题，我们提出MagicWorld——融合三维几何先验与历史检索的交互式视频世界模型。该系统从单帧场景图像出发，通过用户操作驱动动态场景演化，以自回归方式合成连续场景。我们提出的动作引导三维几何模块（AG3D）从每次交互的首帧及对应动作构建点云，为视角转换提供显式几何约束，从而提升结构一致性。进一步提出历史缓存检索（HCR）机制，在生成过程中检索相关历史帧并将其作为条件信号注入，辅助模型利用过往场景信息以抑制误差累积。实验结果表明，MagicWorld在多次交互迭代中显著提升了场景稳定性与连续性。",
    "url": "https://huggingface.co/papers/2511.18886",
    "arxiv_url": "https://arxiv.org/abs/2511.18886"
  },
  {
    "title": "STARFlow-V: End-to-End Video Generative Modeling with Normalizing Flow",
    "summary": "Normalizing flows (NFs) are end-to-end likelihood-based generative models for continuous data, and have recently regained attention with encouraging progress on image generation. Yet in the video generation domain, where spatiotemporal complexity and computational cost are substantially higher, state-of-the-art systems almost exclusively rely on diffusion-based models. In this work, we revisit this design space by presenting STARFlow-V, a normalizing flow-based video generator with substantial benefits such as end-to-end learning, robust causal prediction, and native likelihood estimation. Building upon the recently proposed STARFlow, STARFlow-V operates in the spatiotemporal latent space with a global-local architecture which restricts causal dependencies to a global latent space while preserving rich local within-frame interactions. This eases error accumulation over time, a common pitfall of standard autoregressive diffusion model generation. Additionally, we propose flow-score matching, which equips the model with a light-weight causal denoiser to improve the video generation consistency in an autoregressive fashion. To improve the sampling efficiency, STARFlow-V employs a video-aware Jacobi iteration scheme that recasts inner updates as parallelizable iterations without breaking causality. Thanks to the invertible structure, the same model can natively support text-to-video, image-to-video as well as video-to-video generation tasks. Empirically, STARFlow-V achieves strong visual fidelity and temporal consistency with practical sampling throughput relative to diffusion-based baselines. These results present the first evidence, to our knowledge, that NFs are capable of high-quality autoregressive video generation, establishing them as a promising research direction for building world models. Code and generated samples are available at https://github.com/apple/ml-starflow.",
    "translation": "标题：STARFlow-V：基于标准化流的端到端视频生成建模\n\n摘要：标准化流（NFs）是面向连续数据的端到端基于似然的生成模型，近期在图像生成领域取得的突破性进展使其重获关注。然而在时空复杂度与计算成本显著更高的视频生成领域，现有最先进系统几乎完全依赖于基于扩散的模型。本研究通过提出STARFlow-V重新探索了这一设计空间，该基于标准化流的视频生成器具有端到端学习、鲁棒因果预测和原生似然估计等显著优势。基于近期提出的STARFlow架构，STARFlow-V在时空隐空间中进行操作，采用全局-局部架构将因果依赖限制于全局隐空间，同时保留帧内丰富的局部交互。这种设计缓解了时间维度上的误差累积问题——这是标准自回归扩散模型生成中常见的缺陷。此外，我们提出流-得分匹配方法，通过配备轻量化因果去噪器以自回归方式提升视频生成一致性。为提升采样效率，STARFlow-V采用视频感知的雅可比迭代方案，在保持因果性的前提下将内部更新重构为可并行化迭代。得益于可逆结构，该模型原生支持文本到视频、图像到视频及视频到视频的生成任务。实证研究表明，相较于基于扩散的基线模型，STARFlow-V在实现卓越视觉保真度与时间一致性的同时，具备实用化的采样吞吐量。据我们所知，这些成果首次证明了标准化流能够实现高质量的自回归视频生成，为构建世界模型开辟了新的研究方向。代码及生成样本详见https://github.com/apple/ml-starflow。",
    "url": "https://huggingface.co/papers/2511.20462",
    "arxiv_url": "https://arxiv.org/abs/2511.20462"
  },
  {
    "title": "OmniAlpha: A Sequence-to-Sequence Framework for Unified Multi-Task RGBA Generation",
    "summary": "Generative models have excelled in RGB synthesis, but real-world applications require RGBA manipulation. This has led to a fragmented landscape: specialized, single-task models handle alpha but lack versatility, while unified multi-task frameworks are confined to the RGB domain. To bridge this critical gap, we propose OmniAlpha, the first unified, multi-task generative framework for sequence-to-sequence RGBA image generation and editing. Its architecture features MSRoPE-BiL, a novel RoPE method with a bi-directionally extendable layer axis for its Diffusion Transformer (DiT) backbone, enabling the concurrent processing of multiple input and target RGBA layers. To power this framework, we introduce AlphaLayers, a new dataset of 1,000 high-quality, multi-layer triplets, built via a novel automated synthesis and filter pipeline. Jointly training OmniAlpha on this dataset across a comprehensive suite of 21 diverse tasks, extensive experiments demonstrate that our unified approach consistently outperforms strong, specialized baselines. Most notably, OmniAlpha achieves a dramatic 84.8% relative reduction in SAD for mask-free matting on AIM-500 and wins over 90% of human preferences in layer-conditioned completion. Our work proves that a unified, multi-task model can learn a superior shared representation for RGBA, paving the way for more powerful, layer-aware generative systems.",
    "translation": "标题：OmniAlpha：面向统一多任务RGBA生成的序列到序列框架\n\n摘要：生成模型在RGB合成领域表现出色，但实际应用需要RGBA操作能力。这导致了当前技术格局的碎片化：专业化的单任务模型虽能处理Alpha通道但缺乏通用性，而统一的多任务框架又局限于RGB领域。为填补这一关键空白，我们提出OmniAlpha——首个面向序列到序列RGBA图像生成与编辑的统一多任务生成框架。其架构核心MSRoPE-BiL是一种新颖的RoPE方法，在扩散变换器（DiT）骨干网络中采用双向可扩展的层轴设计，实现了对多输入/输出RGBA图层的并行处理。为支撑该框架，我们开发了AlphaLayers数据集，包含1,000组通过新型自动化合成过滤流程构建的高质量多层三元组。通过在该数据集上对21项多样化任务进行联合训练，大量实验表明我们的统一方法持续超越专业化的强基线模型。尤为突出的是，OmniAlpha在AIM-500数据集上实现无蒙版抠图的SAD指标相对降低84.8%，在图层条件补全任务中赢得超过90%的人类偏好评估。本研究证明，统一的多任务模型能够学习到更优的RGBA共享表示，为开发更强大的图层感知生成系统开辟了新路径。",
    "url": "https://huggingface.co/papers/2511.20211",
    "arxiv_url": "https://arxiv.org/abs/2511.20211"
  },
  {
    "title": "ReDirector: Creating Any-Length Video Retakes with Rotary Camera Encoding",
    "summary": "We present ReDirector, a novel camera-controlled video retake generation method for dynamically captured variable-length videos. In particular, we rectify a common misuse of RoPE in previous works by aligning the spatiotemporal positions of the input video and the target retake. Moreover, we introduce Rotary Camera Encoding (RoCE), a camera-conditioned RoPE phase shift that captures and integrates multi-view relationships within and across the input and target videos. By integrating camera conditions into RoPE, our method generalizes to out-of-distribution camera trajectories and video lengths, yielding improved dynamic object localization and static background preservation. Extensive experiments further demonstrate significant improvements in camera controllability, geometric consistency, and video quality across various trajectories and lengths.",
    "translation": "标题：ReDirector：利用旋转相机编码生成任意长度的视频重拍\n\n摘要：本文提出ReDirector，一种面向动态捕捉可变长度视频的新型相机控制视频重拍生成方法。我们通过对齐输入视频与目标重拍视频的时空位置，纠正了先前研究中旋转位置编码的误用问题。进一步提出旋转相机编码——一种相机条件化的旋转位置编码相位偏移机制，能够捕捉并整合输入视频与目标视频内部及之间的多视角关联。通过将相机条件融入旋转位置编码，本方法可泛化至分布外相机轨迹与视频长度，显著提升动态目标定位能力与静态背景保持效果。大量实验证明，该方法在不同轨迹与长度条件下，相机可控性、几何一致性与视频质量均取得显著提升。",
    "url": "https://huggingface.co/papers/2511.19827",
    "arxiv_url": "https://arxiv.org/abs/2511.19827"
  },
  {
    "title": "HunyuanOCR Technical Report",
    "summary": "This paper presents HunyuanOCR, a commercial-grade, open-source, and lightweight (1B parameters) Vision-Language Model (VLM) dedicated to OCR tasks. The architecture comprises a Native Vision Transformer (ViT) and a lightweight LLM connected via an MLP adapter. HunyuanOCR demonstrates superior performance, outperforming commercial APIs, traditional pipelines, and larger models (e.g., Qwen3-VL-4B). Specifically, it surpasses current public solutions in perception tasks (Text Spotting, Parsing) and excels in semantic tasks (IE, Text Image Translation), securing first place in the ICDAR 2025 DIMT Challenge (Small Model Track). Furthermore, it achieves state-of-the-art (SOTA) results on OCRBench among VLMs with fewer than 3B parameters.\n  HunyuanOCR achieves breakthroughs in three key aspects: 1) Unifying Versatility and Efficiency: We implement comprehensive support for core capabilities including spotting, parsing, IE, VQA, and translation within a lightweight framework. This addresses the limitations of narrow \"OCR expert models\" and inefficient \"General VLMs\". 2) Streamlined End-to-End Architecture: Adopting a pure end-to-end paradigm eliminates dependencies on pre-processing modules (e.g., layout analysis). This fundamentally resolves error propagation common in traditional pipelines and simplifies system deployment. 3) Data-Driven and RL Strategies: We confirm the critical role of high-quality data and, for the first time in the industry, demonstrate that Reinforcement Learning (RL) strategies yield significant performance gains in OCR tasks.\n  HunyuanOCR is officially open-sourced on HuggingFace. We also provide a high-performance deployment solution based on vLLM, placing its production efficiency in the top tier. We hope this model will advance frontier research and provide a solid foundation for industrial applications.",
    "translation": "标题：HunyuanOCR技术报告\n\n摘要：本文提出HunyuanOCR——一个商用级、开源轻量（10亿参数）的视觉语言模型（VLM），专注于OCR任务。该架构包含原生视觉Transformer（ViT）与轻量化大语言模型，通过MLP适配器进行连接。HunyuanOCR展现出卓越性能，在感知任务（文本检测与解析）中超越当前公开方案，在语义任务（信息抽取、图文翻译）中表现优异，荣获ICDAR 2025 DIMT挑战赛小模型赛道冠军。在参数量小于30亿的VLM中，其OCRBench评测成绩达到业界最优。\n\n该模型在三大关键维度实现突破：1）通用性与高效性统一：在轻量化框架内完整支持检测、解析、信息抽取、视觉问答及翻译等核心能力，突破传统“OCR专家模型”能力局限与“通用VLM”效率瓶颈；2）端到端架构革新：采用纯端到端范式，消除对预处理模块（如版面分析）的依赖，从根本上解决传统流水线的误差传播问题并简化系统部署；3）数据与强化学习策略：验证高质量数据的关键作用，并首次在业界证明强化学习策略可为OCR任务带来显著性能提升。\n\nHunyuanOCR已在HuggingFace平台开源，同时提供基于vLLM的高性能部署方案，使其生产效能达到业界第一梯队。我们期待该模型能推动前沿技术探索，并为工业应用提供坚实基础。",
    "url": "https://huggingface.co/papers/2511.19575",
    "arxiv_url": "https://arxiv.org/abs/2511.19575"
  },
  {
    "title": "VQ-VA World: Towards High-Quality Visual Question-Visual Answering",
    "summary": "This paper studies Visual Question-Visual Answering (VQ-VA): generating an image, rather than text, in response to a visual question -- an ability that has recently emerged in proprietary systems such as NanoBanana and GPT-Image. To also bring this capability to open-source models, we introduce VQ-VA World, a data-centric framework built around an agentic pipeline for large-scale, targeted data construction. Leveraging web-scale deployment, this pipeline crawls a massive amount of ~1.8M high-quality, interleaved image-text samples for model training. For evaluation, we further release IntelligentBench, a human-curated benchmark that systematically assesses VQ-VA along the aspects of world knowledge, design knowledge, and reasoning. Training with VQ-VA World data yields strong empirical gains: it helps LightFusion attain 53.06 on IntelligentBench, substantially surpassing the best prior open-source baselines (i.e., 7.78 from vanilla LightFusion; 1.94 from UniWorld-V1), and significantly narrowing the gap toward leading proprietary systems (e.g., 81.67 from NanoBanana; 82.64 from GPT-Image). By releasing the full suite of model weights, datasets, and pipelines, we hope to stimulate future research on VQ-VA.",
    "translation": "标题：VQ-VA世界：迈向高质量视觉问答-视觉应答新境界\n\n摘要：本文研究视觉问答-视觉应答（VQ-VA）任务：针对视觉问题生成图像而非文本回答——这种能力最近在NanoBanana和GPT-Image等专有系统中初现端倪。为使开源模型也具备此能力，我们提出VQ-VA世界，这是一个以数据为中心的框架，围绕大规模定向数据构建的智能流程而设计。通过网络级部署，该流程爬取了约180万组高质量图文交错样本用于模型训练。针对评估需求，我们进一步发布IntelligentBench——一个经人工校验的基准测试系统，从世界知识、设计知识和推理能力三个维度系统评估VQ-VA性能。采用VQ-VA世界数据训练带来了显著的实证提升：帮助LightFusion模型在IntelligentBench上获得53.06分，大幅超越先前最佳开源基线（原始LightFusion的7.78分；UniWorld-V1的1.94分），并显著缩小与领先专有系统的差距（如NanoBanana的81.67分；GPT-Image的82.64分）。通过完整发布模型权重、数据集及流程套件，我们期望推动VQ-VA领域的未来研究。",
    "url": "https://huggingface.co/papers/2511.20573",
    "arxiv_url": "https://arxiv.org/abs/2511.20573"
  },
  {
    "title": "Fara-7B: An Efficient Agentic Model for Computer Use",
    "summary": "Progress in computer use agents (CUAs) has been constrained by the absence of large and high-quality datasets that capture how humans interact with a computer. While LLMs have thrived on abundant textual data, no comparable corpus exists for CUA trajectories. To address these gaps, we introduce FaraGen, a novel synthetic data generation system for multi-step web tasks. FaraGen can propose diverse tasks from frequently used websites, generate multiple solution attempts, and filter successful trajectories using multiple verifiers. It achieves high throughput, yield, and diversity for multi-step web tasks, producing verified trajectories at approximately $1 each. We use this data to train Fara-7B, a native CUA model that perceives the computer using only screenshots, executes actions via predicted coordinates, and is small enough to run on-device. We find that Fara-7B outperforms other CUA models of comparable size on benchmarks like WebVoyager, Online-Mind2Web, and WebTailBench -- our novel benchmark that better captures under-represented web tasks in pre-existing benchmarks. Furthermore, Fara-7B is competitive with much larger frontier models, illustrating key benefits of scalable data generation systems in advancing small efficient agentic models. We are making Fara-7B open-weight on Microsoft Foundry and HuggingFace, and we are releasing WebTailBench.",
    "translation": "标题：Fara-7B：一种高效的计算机使用智能体模型\n\n摘要：计算机使用智能体（CUA）的发展一直受限于缺乏大规模高质量的人类计算机交互数据集。虽然大语言模型在丰富文本数据上取得了显著进展，但目前仍缺乏与之相当的CUA轨迹语料库。为填补这一空白，我们推出了FaraGen——一个面向多步骤网页任务的新型合成数据生成系统。该系统能够基于常用网站生成多样化任务，产生多个解决尝试，并通过多重验证器筛选成功轨迹。对于多步骤网页任务，FaraGen实现了高吞吐量、高产出率和高多样性，每条验证轨迹的生成成本约为1美元。基于这些数据，我们训练出Fara-7B模型：这是一个原生CUA模型，仅通过屏幕截图感知计算机状态，通过预测坐标执行操作，且体积小巧足以在终端设备上运行。实验表明，在WebVoyager、Online-Mind2Web以及我们新提出的WebTailBench（该基准能更好捕捉现有基准中代表性不足的网页任务）等测试平台上，Fara-7B的表现优于同规模CUA模型。更重要的是，该模型与规模大得多的前沿模型性能相当，这印证了可扩展数据生成系统在推进小型高效智能体模型发展中的关键价值。我们已在Microsoft Foundry和HuggingFace平台开源Fara-7B权重，并同步发布WebTailBench基准测试集。",
    "url": "https://huggingface.co/papers/2511.19663",
    "arxiv_url": "https://arxiv.org/abs/2511.19663"
  },
  {
    "title": "MajutsuCity: Language-driven Aesthetic-adaptive City Generation with Controllable 3D Assets and Layouts",
    "summary": "Generating realistic 3D cities is fundamental to world models, virtual reality, and game development, where an ideal urban scene must satisfy both stylistic diversity, fine-grained, and controllability. However, existing methods struggle to balance the creative flexibility offered by text-based generation with the object-level editability enabled by explicit structural representations. We introduce MajutsuCity, a natural language-driven and aesthetically adaptive framework for synthesizing structurally consistent and stylistically diverse 3D urban scenes. MajutsuCity represents a city as a composition of controllable layouts, assets, and materials, and operates through a four-stage pipeline. To extend controllability beyond initial generation, we further integrate MajutsuAgent, an interactive language-grounded editing agent} that supports five object-level operations. To support photorealistic and customizable scene synthesis, we also construct MajutsuDataset, a high-quality multimodal dataset} containing 2D semantic layouts and height maps, diverse 3D building assets, and curated PBR materials and skyboxes, each accompanied by detailed annotations. Meanwhile, we develop a practical set of evaluation metrics, covering key dimensions such as structural consistency, scene complexity, material fidelity, and lighting atmosphere. Extensive experiments demonstrate MajutsuCity reduces layout FID by 83.7% compared with CityDreamer and by 20.1% over CityCraft. Our method ranks first across all AQS and RDR scores, outperforming existing methods by a clear margin. These results confirm MajutsuCity as a new state-of-the-art in geometric fidelity, stylistic adaptability, and semantic controllability for 3D city generation. We expect our framework can inspire new avenues of research in 3D city generation. Our dataset and code will be released at https://github.com/LongHZ140516/MajutsuCity.",
    "translation": "标题：MajutsuCity：基于语言驱动与美学自适应、具备可控三维资产与布局的城市生成方法\n\n摘要：生成逼真的三维城市是世界建模、虚拟现实和游戏开发的基础任务，理想的城市场景需同时满足风格多样性、细粒度控制与结构可控性。然而现有方法难以平衡基于文本生成的创意自由度与显式结构表征带来的对象级编辑能力。本文提出MajutsuCity，一种基于自然语言驱动且具备美学自适应能力的框架，用于生成结构一致且风格多样的三维城市场景。该框架将城市解构为可控布局、资产与材质的组合，通过四阶段流程实现生成。为突破初始生成的限制，我们进一步集成MajutsuAgent——支持五种对象级操作的交互式语言编辑代理。为实现逼真可定制的场景合成，我们构建了MajutsuDataset高质量多模态数据集，包含二维语义布局与高度图、多样化三维建筑资产、精选PBR材质与天空盒，所有数据均附带精细标注。同时开发了一套实用评估指标，涵盖结构一致性、场景复杂度、材质保真度与光照氛围等关键维度。大量实验表明，MajutsuCity相较于CityDreamer将布局FID降低83.7%，较CityCraft降低20.1%。本方法在AQS与RDR所有指标中均位列第一，显著超越现有方法。这些结果证实MajutsuCity在三维城市生成的几何保真度、风格适应性与语义可控性方面达到全新标杆。我们期待该框架能为三维城市生成研究开辟新路径。数据集与代码将在https://github.com/LongHZ140516/MajutsuCity 发布。",
    "url": "https://huggingface.co/papers/2511.20415",
    "arxiv_url": "https://arxiv.org/abs/2511.20415"
  },
  {
    "title": "Scaling Agentic Reinforcement Learning for Tool-Integrated Reasoning in VLMs",
    "summary": "While recent vision-language models (VLMs) demonstrate strong image understanding, their ability to \"think with images\", i.e., to reason through multi-step visual interactions, remains limited. We introduce VISTA-Gym, a scalable training environment for incentivizing tool-integrated visual reasoning capabilities in VLMs. VISTA-Gym unifies diverse real-world multimodal reasoning tasks (7 tasks from 13 datasets in total) with a standardized interface for visual tools (e.g., grounding, parsing), executable interaction loops, verifiable feedback signals, and efficient trajectory logging, enabling visual agentic reinforcement learning at scale. While recent VLMs exhibit strong text-only reasoning, both proprietary and open-source models still struggle with tool selection, invocation, and coordination. With VISTA-Gym, we train VISTA-R1 to interleave tool-use with agentic reasoning via multi-turn trajectory sampling and end-to-end reinforcement learning. Extensive experiments across 11 public reasoning-intensive VQA benchmarks show that VISTA-R1-8B outperforms state-of-the-art baselines with similar sizes by 9.51%-18.72%, demonstrating VISTA-Gym as an effective training ground to unlock the tool-integrated reasoning capabilities for VLMs.",
    "translation": "标题：面向视觉语言模型工具集成推理的可扩展智能体强化学习\n\n摘要：尽管当前视觉语言模型展现出强大的图像理解能力，但其\"基于图像的思考\"能力——即通过多步骤视觉交互进行推理的能力——仍存在局限。我们提出VISTA-Gym这一可扩展的训练环境，旨在增强视觉语言模型的工具集成视觉推理能力。该环境通过标准化视觉工具接口（如定位、解析）、可执行交互循环、可验证反馈信号和高效轨迹记录，统一了多样化的现实世界多模态推理任务（共涵盖13个数据集的7类任务），为实现规模化视觉智能体强化学习提供了基础。虽然现有视觉语言模型在纯文本推理方面表现优异，但无论是专有模型还是开源模型在工具选择、调用与协调方面仍面临挑战。基于VISTA-Gym平台，我们通过多轮轨迹采样和端到端强化学习训练出VISTA-R1模型，实现了工具使用与智能推理的交替执行。在11个公开推理密集型视觉问答基准测试中的广泛实验表明，VISTA-R1-8B模型以9.51%-18.72%的优势超越同类规模的先进基线模型，证明VISTA-Gym能有效解锁视觉语言模型的工具集成推理能力。",
    "url": "https://huggingface.co/papers/2511.19773",
    "arxiv_url": "https://arxiv.org/abs/2511.19773"
  },
  {
    "title": "Cook and Clean Together: Teaching Embodied Agents for Parallel Task Execution",
    "summary": "Task scheduling is critical for embodied AI, enabling agents to follow natural language instructions and execute actions efficiently in 3D physical worlds. However, existing datasets often simplify task planning by ignoring operations research (OR) knowledge and 3D spatial grounding. In this work, we propose Operations Research knowledge-based 3D Grounded Task Scheduling (ORS3D), a new task that requires the synergy of language understanding, 3D grounding, and efficiency optimization. Unlike prior settings, ORS3D demands that agents minimize total completion time by leveraging parallelizable subtasks, e.g., cleaning the sink while the microwave operates. To facilitate research on ORS3D, we construct ORS3D-60K, a large-scale dataset comprising 60K composite tasks across 4K real-world scenes. Furthermore, we propose GRANT, an embodied multi-modal large language model equipped with a simple yet effective scheduling token mechanism to generate efficient task schedules and grounded actions. Extensive experiments on ORS3D-60K validate the effectiveness of GRANT across language understanding, 3D grounding, and scheduling efficiency. The code is available at https://github.com/H-EmbodVis/GRANT",
    "translation": "标题：协同烹饪与清洁：面向并行任务执行的具身智能体教学研究\n\n摘要：任务调度在具身人工智能领域具有关键意义，它使智能体能够遵循自然语言指令并在三维物理世界中高效执行动作。然而现有数据集常因忽略运筹学知识与三维空间 grounding 而简化任务规划。本研究提出基于运筹学知识的三维空间任务调度新任务ORS3D，该任务需要语言理解、三维空间感知与效率优化的协同作用。与传统设定不同，ORS3D要求智能体通过利用可并行子任务（如在微波炉运行期间同步清洁水槽）来最小化总完成时间。为促进ORS3D研究，我们构建了包含4,000个真实场景中60,000项复合任务的大规模数据集ORS3D-60K。此外，我们提出配备简单有效调度令牌机制的多模态大语言模型GRANT，可生成高效的任务调度方案与空间 grounded 动作。在ORS3D-60K上的大量实验验证了GRANT在语言理解、三维空间感知和调度效率方面的卓越性能。代码已开源：https://github.com/H-EmbodVis/GRANT",
    "url": "https://huggingface.co/papers/2511.19430",
    "arxiv_url": "https://arxiv.org/abs/2511.19430"
  },
  {
    "title": "PhysChoreo: Physics-Controllable Video Generation with Part-Aware Semantic Grounding",
    "summary": "While recent video generation models have achieved significant visual fidelity, they often suffer from the lack of explicit physical controllability and plausibility. To address this, some recent studies attempted to guide the video generation with physics-based rendering. However, these methods face inherent challenges in accurately modeling complex physical properties and effectively control ling the resulting physical behavior over extended temporal sequences. In this work, we introduce PhysChoreo, a novel framework that can generate videos with diverse controllability and physical realism from a single image. Our method consists of two stages: first, it estimates the static initial physical properties of all objects in the image through part-aware physical property reconstruction. Then, through temporally instructed and physically editable simulation, it synthesizes high-quality videos with rich dynamic behaviors and physical realism. Experimental results show that PhysChoreo can generate videos with rich behaviors and physical realism, outperforming state-of-the-art methods on multiple evaluation metrics.",
    "translation": "标题：PhysChoreo：基于部件感知语义关联的物理可控视频生成\n\n摘要：尽管当前视频生成模型已实现显著的视觉保真度，但其往往缺乏显式的物理可控性与合理性。为解决此问题，近期研究尝试通过基于物理的渲染技术指导视频生成。然而，这些方法在精确建模复杂物理属性及有效控制长时序物理行为方面仍面临固有挑战。本研究提出PhysChoreo创新框架，能够基于单张图像生成具有多样化可控性与物理真实感的视频。该方法包含两个阶段：首先通过部件感知的物理属性重建技术估计图像中所有物体的静态初始物理属性；随后通过时序引导与物理可编辑的仿真过程，合成具有丰富动态行为与物理真实感的高质量视频。实验结果表明，PhysChoreo能够生成兼具丰富行为表现与物理真实感的视频，在多项评估指标上均优于现有最优方法。",
    "url": "https://huggingface.co/papers/2511.20562",
    "arxiv_url": "https://arxiv.org/abs/2511.20562"
  },
  {
    "title": "Think Visually, Reason Textually: Vision-Language Synergy in ARC",
    "summary": "Abstract reasoning from minimal examples remains a core unsolved problem for frontier foundation models such as GPT-5 and Grok 4. These models still fail to infer structured transformation rules from a handful of examples, which is a key hallmark of human intelligence. The Abstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI) provides a rigorous testbed for this capability, demanding conceptual rule induction and transfer to novel tasks. Most existing methods treat ARC-AGI as a purely textual reasoning task, overlooking the fact that humans rely heavily on visual abstraction when solving such puzzles. However, our pilot experiments reveal a paradox: naively rendering ARC-AGI grids as images degrades performance due to imprecise rule execution. This leads to our central hypothesis that vision and language possess complementary strengths across distinct reasoning stages: vision supports global pattern abstraction and verification, whereas language specializes in symbolic rule formulation and precise execution. Building on this insight, we introduce two synergistic strategies: (1) Vision-Language Synergy Reasoning (VLSR), which decomposes ARC-AGI into modality-aligned subtasks; and (2) Modality-Switch Self-Correction (MSSC), which leverages vision to verify text-based reasoning for intrinsic error correction. Extensive experiments demonstrate that our approach yields up to a 4.33% improvement over text-only baselines across diverse flagship models and multiple ARC-AGI tasks. Our findings suggest that unifying visual abstraction with linguistic reasoning is a crucial step toward achieving generalizable, human-like intelligence in future foundation models. Source code will be released soon.",
    "translation": "标题：视觉思维与文本推理：ARC中的视觉-语言协同机制\n\n摘要：基于少量样本的抽象推理能力仍是GPT-5与Grok-4等前沿基础模型尚未解决的核心难题。这些模型仍难以从有限示例中推断结构化转换规则——这正是人类智能的关键特征。面向通用人工智能的抽象与推理语料库（ARC-AGI）为此能力提供了严格测试基准，要求实现概念规则归纳及向新任务的迁移。现有方法大多将ARC-AGI视为纯文本推理任务，却忽略了人类在解决此类难题时高度依赖视觉抽象的特性。然而初步实验揭示了一个悖论：简单地将ARC-AGI网格转换为图像会因规则执行不精确而导致性能下降。由此我们提出核心假设：视觉与语言在不同推理阶段具有互补优势——视觉擅长全局模式抽象与验证，而语言专精于符号化规则表述与精确执行。基于此洞见，我们提出两种协同策略：（1）视觉-语言协同推理（VLSR），将ARC-AGI分解为模态对齐的子任务；（2）模态切换自校正（MSSC），利用视觉验证文本推理以实现内在误差修正。大量实验表明，该方法在多种旗舰模型和多元ARC-AGI任务中相较纯文本基线最高可获得4.33%的性能提升。我们的研究结果表明，将视觉抽象与语言推理相统一，是未来基础模型实现可泛化类人智能的关键步骤。源代码即将发布。",
    "url": "https://huggingface.co/papers/2511.15703",
    "arxiv_url": "https://arxiv.org/abs/2511.15703"
  },
  {
    "title": "DiffSeg30k: A Multi-Turn Diffusion Editing Benchmark for Localized AIGC Detection",
    "summary": "Diffusion-based editing enables realistic modification of local image regions, making AI-generated content harder to detect. Existing AIGC detection benchmarks focus on classifying entire images, overlooking the localization of diffusion-based edits. We introduce DiffSeg30k, a publicly available dataset of 30k diffusion-edited images with pixel-level annotations, designed to support fine-grained detection. DiffSeg30k features: 1) In-the-wild images--we collect images or image prompts from COCO to reflect real-world content diversity; 2) Diverse diffusion models--local edits using eight SOTA diffusion models; 3) Multi-turn editing--each image undergoes up to three sequential edits to mimic real-world sequential editing; and 4) Realistic editing scenarios--a vision-language model (VLM)-based pipeline automatically identifies meaningful regions and generates context-aware prompts covering additions, removals, and attribute changes. DiffSeg30k shifts AIGC detection from binary classification to semantic segmentation, enabling simultaneous localization of edits and identification of the editing models. We benchmark three baseline segmentation approaches, revealing significant challenges in semantic segmentation tasks, particularly concerning robustness to image distortions. Experiments also reveal that segmentation models, despite being trained for pixel-level localization, emerge as highly reliable whole-image classifiers of diffusion edits, outperforming established forgery classifiers while showing great potential in cross-generator generalization. We believe DiffSeg30k will advance research in fine-grained localization of AI-generated content by demonstrating the promise and limitations of segmentation-based methods. DiffSeg30k is released at: https://huggingface.co/datasets/Chaos2629/Diffseg30k",
    "translation": "标题：DiffSeg30k：面向局部AIGC检测的多轮扩散编辑基准数据集\n\n摘要：基于扩散模型的编辑技术能够对图像局部区域进行逼真修改，使得人工智能生成内容（AIGC）的检测更具挑战性。现有AIGC检测基准主要关注整图分类，忽视了基于扩散编辑的局部定位能力。本文提出DiffSeg30k——一个包含3万张扩散编辑图像并带有像素级标注的公开数据集，旨在支持细粒度检测研究。该数据集具备四大特征：1）真实场景图像：从COCO数据集采集图像或图像提示词以反映现实内容多样性；2）多样化扩散模型：采用八种前沿扩散模型进行局部编辑；3）多轮次编辑：每张图像最多经历三次连续编辑以模拟实际序列编辑过程；4）逼真编辑场景：通过基于视觉语言模型（VLM）的流程自动识别语义区域，并生成涵盖添加、移除与属性变更的上下文感知提示词。DiffSeg30k将AIGC检测从二分类问题推进至语义分割任务，实现了编辑区域定位与编辑模型识别的同步处理。我们通过三种基线分割方法的基准测试，揭示了语义分割任务面临的重大挑战，特别是在图像失真鲁棒性方面。实验还表明，尽管分割模型接受的是像素级定位训练，却能成为高度可靠的扩散编辑整图分类器，其性能超越现有伪造分类器，并在跨生成器泛化方面展现出巨大潜力。我们相信DiffSeg30k将通过揭示基于分割方法的优势与局限，推动AI生成内容细粒度定位研究的发展。数据集发布于：https://huggingface.co/datasets/Chaos2629/Diffseg30k",
    "url": "https://huggingface.co/papers/2511.19111",
    "arxiv_url": "https://arxiv.org/abs/2511.19111"
  },
  {
    "title": "Yo'City: Personalized and Boundless 3D Realistic City Scene Generation via Self-Critic Expansion",
    "summary": "Realistic 3D city generation is fundamental to a wide range of applications, including virtual reality and digital twins. However, most existing methods rely on training a single diffusion model, which limits their ability to generate personalized and boundless city-scale scenes. In this paper, we present Yo'City, a novel agentic framework that enables user-customized and infinitely expandable 3D city generation by leveraging the reasoning and compositional capabilities of off-the-shelf large models. Specifically, Yo'City first conceptualize the city through a top-down planning strategy that defines a hierarchical \"City-District-Grid\" structure. The Global Planner determines the overall layout and potential functional districts, while the Local Designer further refines each district with detailed grid-level descriptions. Subsequently, the grid-level 3D generation is achieved through a \"produce-refine-evaluate\" isometric image synthesis loop, followed by image-to-3D generation. To simulate continuous city evolution, Yo'City further introduces a user-interactive, relationship-guided expansion mechanism, which performs scene graph-based distance- and semantics-aware layout optimization, ensuring spatially coherent city growth. To comprehensively evaluate our method, we construct a diverse benchmark dataset and design six multi-dimensional metrics that assess generation quality from the perspectives of semantics, geometry, texture, and layout. Extensive experiments demonstrate that Yo'City consistently outperforms existing state-of-the-art methods across all evaluation aspects.",
    "translation": "标题：Yo'City：基于自批判扩展的个性化无边界3D真实城市场景生成\n\n摘要：真实感三维城市生成对虚拟现实、数字孪生等众多应用具有重要意义。然而现有方法大多依赖训练单一扩散模型，限制了生成个性化、无边界城市场景的能力。本文提出Yo'City——一种新型智能框架，通过利用现成大语言模型的推理与组合能力，实现用户定制化且无限扩展的三维城市生成。具体而言，Yo'City首先采用自上而下的规划策略构建城市概念框架，定义“城市-区域-网格”的层级结构：全局规划器确定整体布局与潜在功能分区，局部设计器则通过网格级细粒度描述完善每个区域。随后通过“生成-优化-评估”的等距图像合成循环实现网格级三维生成，并经由图像到三维的转换流程。为模拟持续城市演进，本框架进一步引入用户交互的关系引导扩展机制，执行基于场景图的距离与语义感知布局优化，确保空间连贯的城市生长。为全面评估方法性能，我们构建了多样化基准数据集，设计六项从语义、几何、纹理及布局多维度评估生成质量的指标。大量实验表明，Yo'City在所有评估维度上均持续优于现有先进方法。",
    "url": "https://huggingface.co/papers/2511.18734",
    "arxiv_url": "https://arxiv.org/abs/2511.18734"
  },
  {
    "title": "Cognitive Foundations for Reasoning and Their Manifestation in LLMs",
    "summary": "Large language models (LLMs) solve complex problems yet fail on simpler variants, suggesting they achieve correct outputs through mechanisms fundamentally different from human reasoning. To understand this gap, we synthesize cognitive science research into a taxonomy of 28 cognitive elements spanning reasoning invariants, meta-cognitive controls, representations for organizing reasoning & knowledge, and transformation operations. We introduce a fine-grained evaluation framework and conduct the first large-scale empirical analysis of 192K traces from 18 models across text, vision, and audio, complemented by 54 human think-aloud traces, which we make publicly available. We find that models under-utilize cognitive elements correlated with success, narrowing to rigid sequential processing on ill-structured problems where diverse representations and meta-cognitive monitoring are critical. Human traces show more abstraction and conceptual processing, while models default to surface-level enumeration. Meta-analysis of 1.6K LLM reasoning papers reveals the research community concentrates on easily quantifiable elements (sequential organization: 55%, decomposition: 60%) but neglecting meta-cognitive controls (self-awareness: 16%) that correlate with success. Models possess behavioral repertoires associated with success but fail to deploy them spontaneously. Leveraging these patterns, we develop test-time reasoning guidance that automatically scaffold successful structures, improving performance by up to 66.7% on complex problems. By establishing a shared vocabulary between cognitive science and LLM research, our framework enables systematic diagnosis of reasoning failures and principled development of models that reason through robust cognitive mechanisms rather than spurious shortcuts, while providing tools to test theories of human cognition at scale.",
    "translation": "标题：推理的认知基础及其在大语言模型中的体现\n\n摘要：大语言模型虽能解决复杂问题，却在简单变体任务中表现不佳，这表明其获得正确输出的机制与人类推理存在本质差异。为理解这一差距，我们整合认知科学研究成果，构建包含28种认知要素的分类体系，涵盖推理恒常性、元认知控制、组织推理与知识的表征系统以及转换操作。我们提出细粒度评估框架，首次对来自18个文本、视觉和音频模型的19.2万条推理轨迹进行大规模实证分析，并辅以54条人类有声思维轨迹（已公开）。研究发现：模型未能充分利用与成功相关的认知要素，面对非良构问题时（此类问题需要多样化表征和元认知监控）会退化为僵化的序列处理；人类轨迹呈现更多抽象化与概念化处理，而模型倾向于表层枚举。对1,600篇大语言模型推理论文的元分析显示，研究界集中于易量化要素（序列组织：55%，问题分解：60%），却忽视与成功相关的元认知控制（自我监控：16%）。模型虽具备与成功相关的行为库，却无法自主调用。基于这些模式，我们开发了测试时推理引导技术，自动构建成功推理结构，在复杂问题上将模型性能最高提升66.7%。通过建立认知科学与大语言模型研究的共享概念体系，我们的框架能够系统诊断推理失败，推动模型基于稳健认知机制（而非伪关联捷径）实现推理的 principled 开发，同时为大规模验证人类认知理论提供工具支持。",
    "url": "https://huggingface.co/papers/2511.16660",
    "arxiv_url": "https://arxiv.org/abs/2511.16660"
  },
  {
    "title": "Diverse Video Generation with Determinantal Point Process-Guided Policy Optimization",
    "summary": "While recent text-to-video (T2V) diffusion models have achieved impressive quality and prompt alignment, they often produce low-diversity outputs when sampling multiple videos from a single text prompt. We tackle this challenge by formulating it as a set-level policy optimization problem, with the goal of training a policy that can cover the diverse range of plausible outcomes for a given prompt. To address this, we introduce DPP-GRPO, a novel framework for diverse video generation that combines Determinantal Point Processes (DPPs) and Group Relative Policy Optimization (GRPO) theories to enforce explicit reward on diverse generations. Our objective turns diversity into an explicit signal by imposing diminishing returns on redundant samples (via DPP) while supplies groupwise feedback over candidate sets (via GRPO). Our framework is plug-and-play and model-agnostic, and encourages diverse generations across visual appearance, camera motions, and scene structure without sacrificing prompt fidelity or perceptual quality. We implement our method on WAN and CogVideoX, and show that our method consistently improves video diversity on state-of-the-art benchmarks such as VBench, VideoScore, and human preference studies. Moreover, we release our code and a new benchmark dataset of 30,000 diverse prompts to support future research.",
    "translation": "标题：基于行列式点过程引导策略优化的多样化视频生成\n\n摘要：尽管当前文本到视频（T2V）扩散模型已实现卓越的生成质量与提示词对齐能力，但在基于单一文本提示生成多个视频时往往存在输出多样性不足的问题。我们通过将该问题构建为集合级策略优化任务来解决这一挑战，旨在训练能够覆盖给定提示词对应多种合理结果的策略模型。为此，我们提出DPP-GRPO创新框架，该框架融合行列式点过程（DPPs）与群组相对策略优化（GRPO）理论，通过对多样化生成结果施加显式奖励来增强视频多样性。我们的目标函数通过以下方式将多样性转化为显式信号：对冗余样本施加收益递减约束（通过DPP实现），同时对候选集提供分组反馈（通过GRPO实现）。本框架具备即插即用和模型无关的特性，在保持提示词忠实度与感知质量的前提下，有效促进视觉外观、摄像机运动和场景结构等多维度的多样性生成。我们在WAN和CogVideoX平台上实现该方法，实验表明在VBench、VideoScore等前沿基准测试及人类偏好研究中，我们的方法持续提升视频生成多样性。此外，我们开源了代码并发布了包含30,000条多样化提示词的新基准数据集，以支持后续研究。\n\n请按照以下格式返回：\n标题：基于行列式点过程引导策略优化的多样化视频生成\n摘要：尽管当前文本到视频（T2V）扩散模型已实现卓越的生成质量与提示词对齐能力，但在基于单一文本提示生成多个视频时往往存在输出多样性不足的问题。我们通过将该问题构建为集合级策略优化任务来解决这一挑战，旨在训练能够覆盖给定提示词对应多种合理结果的策略模型。为此，我们提出DPP-GRPO创新框架，该框架融合行列式点过程（DPPs）与群组相对策略优化（GRPO）理论，通过对多样化生成结果施加显式奖励来增强视频多样性。我们的目标函数通过以下方式将多样性转化为显式信号：对冗余样本施加收益递减约束（通过DPP实现），同时对候选集提供分组反馈（通过GRPO实现）。本框架具备即插即用和模型无关的特性，在保持提示词忠实度与感知质量的前提下，有效促进视觉外观、摄像机运动和场景结构等多维度的多样性生成。我们在WAN和CogVideoX平台上实现该方法，实验表明在VBench、VideoScore等前沿基准测试及人类偏好研究中，我们的方法持续提升视频生成多样性。此外，我们开源了代码并发布了包含30,000条多样化提示词的新基准数据集，以支持后续研究。",
    "url": "https://huggingface.co/papers/2511.20647",
    "arxiv_url": "https://arxiv.org/abs/2511.20647"
  },
  {
    "title": "Unified all-atom molecule generation with neural fields",
    "summary": "Generative models for structure-based drug design are often limited to a specific modality, restricting their broader applicability. To address this challenge, we introduce FuncBind, a framework based on computer vision to generate target-conditioned, all-atom molecules across atomic systems. FuncBind uses neural fields to represent molecules as continuous atomic densities and employs score-based generative models with modern architectures adapted from the computer vision literature. This modality-agnostic representation allows a single unified model to be trained on diverse atomic systems, from small to large molecules, and handle variable atom/residue counts, including non-canonical amino acids. FuncBind achieves competitive in silico performance in generating small molecules, macrocyclic peptides, and antibody complementarity-determining region loops, conditioned on target structures. FuncBind also generated in vitro novel antibody binders via de novo redesign of the complementarity-determining region H3 loop of two chosen co-crystal structures. As a final contribution, we introduce a new dataset and benchmark for structure-conditioned macrocyclic peptide generation. The code is available at https://github.com/prescient-design/funcbind.",
    "translation": "标题：基于神经场的统一全原子分子生成\n\n摘要：基于结构的药物设计生成模型通常受限于特定模态，制约了其更广泛的应用。为解决这一难题，我们提出FuncBind框架——一种基于计算机视觉技术、能够跨原子系统生成靶标条件化全原子分子的创新方法。该框架运用神经场将分子表示为连续原子密度，并采用基于分数的生成模型，其现代架构源自计算机视觉领域的先进成果。这种模态无关的表征方式使得单一统一模型能够训练于从微小分子到大分子的多样化原子系统，并可处理包括非经典氨基酸在内的可变原子/残基数量。在计算机模拟中，FuncBind在靶标结构条件下生成小分子、大环肽和抗体互补决定区环状结构方面展现出卓越性能。通过重新设计两种选定共晶结构的互补决定区H3环，FuncBind还在实验中成功生成了新型抗体结合物。作为最终贡献，我们建立了用于结构条件化大环肽生成的新数据集与基准测试平台。代码已发布于https://github.com/prescient-design/funcbind。",
    "url": "https://huggingface.co/papers/2511.15906",
    "arxiv_url": "https://arxiv.org/abs/2511.15906"
  },
  {
    "title": "Concept-Aware Batch Sampling Improves Language-Image Pretraining",
    "summary": "What data should a vision-language model be trained on? To answer this question, many data curation efforts center on the quality of a dataset. However, most of these existing methods are (i) offline, i.e. they produce a static dataset from a set of predetermined filtering criteria, and (ii) concept-agnostic, i.e. they use model-based filters which induce additional data biases. In this work, we go beyond such offline, concept-agnostic methods and advocate for more flexible, task-adaptive online concept-based curation. Our first contribution is DataConcept, a collection of 128M web-crawled image-text pairs annotated with fine-grained details about their concept composition. Building on DataConcept, we introduce Concept-Aware Batch Sampling (CABS), a simple yet effective batch sampling framework that flexibly constructs batches on-the-fly based on specific target distributions. We propose two variants: (i) Diversity Maximization (CABS-DM) to curate batches with a broad coverage of available concepts, and (ii) Frequency Maximization (CABS-FM) to curate batches with high object multiplicity. Through extensive evaluations across 28 benchmarks, we demonstrate that our CABS method significantly benefits CLIP/SigLIP model classes and yields highly performant models. Overall, CABS represents a strong open-source alternative to proprietary online data curation algorithms, enabling practitioners to define custom concept distributions that optimize for specific downstream tasks.",
    "translation": "标题：概念感知批量采样提升语言-图像预训练效果\n\n摘要：视觉语言模型应使用何种数据进行训练？针对该问题，当前多数数据筛选方法聚焦于数据集质量，但存在两大局限：(i) 离线性——依赖预设过滤标准生成静态数据集；(ii) 概念无关性——采用基于模型的过滤器会引入额外数据偏差。本研究突破传统离线式、概念无关的方法桎梏，提出更具灵活性的任务自适应在线概念化筛选方案。我们首先构建DataConcept数据集——包含1.28亿网络爬取的图文对，并标注其细粒度概念构成。基于此，我们提出概念感知批量采样框架（CABS），该轻量而高效的动态批构建机制能根据特定目标分布灵活组织批次。我们开发两种变体：(i) 多样性最大化（CABS-DM）——构建覆盖广泛概念的批次；(ii) 频次最大化（CABS-FM）——构建高目标密度的批次。通过对28个基准的广泛评估，我们证明CABS方法能显著提升CLIP/SigLIP模型性能，生成高竞争力模型。总体而言，CABS为专有在线数据筛选算法提供了强大的开源替代方案，使实践者能通过自定义概念分布优化特定下游任务。",
    "url": "https://huggingface.co/papers/2511.20643",
    "arxiv_url": "https://arxiv.org/abs/2511.20643"
  },
  {
    "title": "Uplifting Table Tennis: A Robust, Real-World Application for 3D Trajectory and Spin Estimation",
    "summary": "Obtaining the precise 3D motion of a table tennis ball from standard monocular videos is a challenging problem, as existing methods trained on synthetic data struggle to generalize to the noisy, imperfect ball and table detections of the real world. This is primarily due to the inherent lack of 3D ground truth trajectories and spin annotations for real-world video. To overcome this, we propose a novel two-stage pipeline that divides the problem into a front-end perception task and a back-end 2D-to-3D uplifting task. This separation allows us to train the front-end components with abundant 2D supervision from our newly created TTHQ dataset, while the back-end uplifting network is trained exclusively on physically-correct synthetic data. We specifically re-engineer the uplifting model to be robust to common real-world artifacts, such as missing detections and varying frame rates. By integrating a ball detector and a table keypoint detector, our approach transforms a proof-of-concept uplifting method into a practical, robust, and high-performing end-to-end application for 3D table tennis trajectory and spin analysis.",
    "translation": "标题：提升乒乓球运动分析：基于三维轨迹与旋转估计的鲁棒性现实应用\n\n摘要：从标准单目视频中精确获取乒乓球的三维运动轨迹是一个具有挑战性的难题，现有基于合成数据训练的方法难以适应现实场景中存在的噪声干扰及球体与球台检测不完善的问题。这主要源于现实视频中三维真实轨迹与旋转标注数据的固有缺失。为解决此问题，我们提出一种新颖的两阶段处理流程，将问题分解为前端感知任务与后端二维转三维提升任务。这种分离设计使我们能够利用新构建的TTHQ数据集中的海量二维标注数据训练前端组件，而后端提升网络则完全基于符合物理规律合成的数据进行训练。我们特别对提升模型进行重新设计，使其对现实场景中常见的检测缺失和帧率波动等干扰因素具有鲁棒性。通过整合球体检测器与球台关键点检测器，本方法将概念验证性的提升技术转化为实用、鲁棒且高性能的端到端应用系统，可实现乒乓球三维轨迹与旋转分析的完整解决方案。",
    "url": "https://huggingface.co/papers/2511.20250",
    "arxiv_url": "https://arxiv.org/abs/2511.20250"
  },
  {
    "title": "CLaRa: Bridging Retrieval and Generation with Continuous Latent Reasoning",
    "summary": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) with external knowledge but still suffers from long contexts and disjoint retrieval-generation optimization. In this work, we propose CLaRa (Continuous Latent Reasoning), a unified framework that performs embedding-based compression and joint optimization in a shared continuous space. To obtain semantically rich and retrievable compressed vectors, we introduce SCP, a key-preserving data synthesis framework using QA and paraphrase supervision. CLaRa then trains the reranker and generator end-to-end via a single language modeling loss, with gradients flowing through both modules using a differentiable top-k estimator. Theoretically, this unified optimization aligns retrieval relevance with answer quality. Experiments across multiple QA benchmarks show that CLaRa achieves state-of-the-art compression and reranking performance, often surpassing text-based fine-tuned baselines.",
    "translation": "标题：CLaRa：通过连续潜在推理桥接检索与生成任务\n\n摘要：检索增强生成（RAG）通过外部知识增强大语言模型（LLM）的能力，但仍面临长上下文处理及检索-生成优化割裂的问题。本研究提出CLaRa（连续潜在推理）框架，在共享连续空间内实现基于嵌入的压缩与联合优化。为获得语义丰富且可检索的压缩向量，我们设计了SCP框架——通过问答与复述监督实现关键信息保留的数据合成方法。CLaRa通过单一语言建模损失函数端到端训练重排序器与生成器，并采用可微分top-k估计器实现双模块梯度传导。理论分析表明，该联合优化使检索相关性与答案质量达成对齐。在多组问答基准测试上的实验表明，CLaRa在压缩与重排序任务中达到最优性能，其表现往往超越基于文本的微调基线模型。",
    "url": "https://huggingface.co/papers/2511.18659",
    "arxiv_url": "https://arxiv.org/abs/2511.18659"
  },
  {
    "title": "Future Is Unevenly Distributed: Forecasting Ability of LLMs Depends on What We're Asking",
    "summary": "Large Language Models (LLMs) demonstrate partial forecasting competence across social, political, and economic events. Yet, their predictive ability varies sharply with domain structure and prompt framing. We investigate how forecasting performance varies with different model families on real-world questions about events that happened beyond the model cutoff date. We analyze how context, question type, and external knowledge affect accuracy and calibration, and how adding factual news context modifies belief formation and failure modes. Our results show that forecasting ability is highly variable as it depends on what, and how, we ask.",
    "translation": "标题：未来并非均匀分布：大型语言模型的预测能力取决于提问内容\n\n摘要：大型语言模型在社会、政治和经济事件中展现出部分预测能力，但其预测性能随领域结构和提示框架存在显著差异。本研究针对模型截止日期后发生的现实事件，探究不同模型家族在预测表现上的异同。我们系统分析了语境因素、问题类型及外部知识如何影响预测准确度与校准效果，并探讨事实性新闻语境的引入如何改变信念形成机制与错误模式。研究结果表明，预测能力具有高度可变性，其表现本质上取决于我们提问的内容与方式。",
    "url": "https://huggingface.co/papers/2511.18394",
    "arxiv_url": "https://arxiv.org/abs/2511.18394"
  },
  {
    "title": "SciEducator: Scientific Video Understanding and Educating via Deming-Cycle Multi-Agent System",
    "summary": "Recent advancements in multimodal large language models (MLLMs) and video agent systems have significantly improved general video understanding. However, when applied to scientific video understanding and educating, a domain that demands external professional knowledge integration and rigorous step-wise reasoning, existing approaches often struggle. To bridge this gap, we propose SciEducator, the first iterative self-evolving multi-agent system for scientific video comprehension and education. Rooted in the classical Deming Cycle from management science, our design reformulates its Plan-Do-Study-Act philosophy into a self-evolving reasoning and feedback mechanism, which facilitates the interpretation of intricate scientific activities in videos. Moreover, SciEducator can produce multimodal educational content tailored to specific scientific processes, including textual instructions, visual guides, audio narrations, and interactive references. To support evaluation, we construct SciVBench, a benchmark consisting of 500 expert-verified and literature-grounded science QA pairs across five categories, covering physical, chemical, and everyday phenomena. Extensive experiments demonstrate that SciEducator substantially outperforms leading closed-source MLLMs (e.g., Gemini, GPT-4o) and state-of-the-art video agents on the benchmark, establishing a new paradigm for the community.",
    "translation": "标题：SciEducator：基于戴明循环多智能体系统的科学视频理解与教育框架\n\n摘要：多模态大语言模型与视频智能体系统的最新进展显著提升了通用视频理解能力。然而在需要外部专业知识整合与严格分步推理的科学视频理解与教育领域，现有方法仍存在明显不足。为弥补这一空白，我们提出SciEducator——首个面向科学视频解析与教育的迭代式自进化多智能体系统。该系统植根于管理科学的经典戴明循环理论，将其“计划-执行-研究-改进”理念重构为自进化推理与反馈机制，有效支撑视频中复杂科学活动的解析。此外，SciEducator能针对特定科学过程生成多模态教育内容，包括文本指令、可视化导引、音频解说及交互式参考资料。为支持评估，我们构建了SciVBench基准数据集，包含经专家验证和文献支持的500个科学问答对，涵盖物理、化学及日常现象五大类别。大量实验表明，SciEducator在基准测试中显著优于主流闭源多模态大语言模型（如Gemini、GPT-4o）及最先进的视频智能体，为该领域确立了新范式。",
    "url": "https://huggingface.co/papers/2511.17943",
    "arxiv_url": "https://arxiv.org/abs/2511.17943"
  }
]