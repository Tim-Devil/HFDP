[
  {
    "title": "One Small Step in Latent, One Giant Leap for Pixels: Fast Latent Upscale Adapter for Your Diffusion Models",
    "summary": "Diffusion models struggle to scale beyond their training resolutions, as direct high-resolution sampling is slow and costly, while post-hoc image super-resolution (ISR) introduces artifacts and additional latency by operating after decoding. We present the Latent Upscaler Adapter (LUA), a lightweight module that performs super-resolution directly on the generator's latent code before the final VAE decoding step. LUA integrates as a drop-in component, requiring no modifications to the base model or additional diffusion stages, and enables high-resolution synthesis through a single feed-forward pass in latent space. A shared Swin-style backbone with scale-specific pixel-shuffle heads supports 2x and 4x factors and remains compatible with image-space SR baselines, achieving comparable perceptual quality with nearly 3x lower decoding and upscaling time (adding only +0.42 s for 1024 px generation from 512 px, compared to 1.87 s for pixel-space SR using the same SwinIR architecture). Furthermore, LUA shows strong generalization across the latent spaces of different VAEs, making it easy to deploy without retraining from scratch for each new decoder. Extensive experiments demonstrate that LUA closely matches the fidelity of native high-resolution generation while offering a practical and efficient path to scalable, high-fidelity image synthesis in modern diffusion pipelines.",
    "translation": "标题：潜在空间一小步，像素生成大飞跃：面向扩散模型的快速潜在上采样适配器\n\n摘要：扩散模型难以突破训练分辨率限制，因为直接进行高分辨率采样速度缓慢且成本高昂，而事后图像超分辨率（ISR）方法在解码后执行操作，不仅会引入伪影还会增加额外延迟。我们提出潜在上采样适配器（LUA），这是一种轻量级模块，可在最终VAE解码步骤之前直接在生成器的潜在代码上执行超分辨率。LUA可作为即插即用组件集成，无需修改基础模型或增加额外扩散阶段，通过潜在空间中的单次前向传播即可实现高分辨率合成。采用共享Swin风格主干网络配合尺度特异性像素重组头，支持2倍和4倍缩放因子，同时保持与图像空间SR基线的兼容性，在实现相当感知质量的前提下，将解码与上采样时间降低近3倍（从512px生成1024px仅增加0.42秒，而使用相同SwinIR架构的像素空间SR需1.87秒）。此外，LUA在不同VAE的潜在空间中展现出强大泛化能力，无需为每个新解码器从头训练即可快速部署。大量实验表明，LUA在保真度上可媲美原生高分辨率生成，同时为现代扩散流程中的可扩展高保真图像合成提供了实用高效的技术路径。",
    "url": "https://huggingface.co/papers/2511.10629",
    "arxiv_url": "https://arxiv.org/abs/2511.10629"
  },
  {
    "title": "PAN: A World Model for General, Interactable, and Long-Horizon World Simulation",
    "summary": "A world model enables an intelligent agent to imagine, predict, and reason about how the world evolves in response to its actions, and accordingly to plan and strategize. While recent video generation models produce realistic visual sequences, they typically operate in the prompt-to-full-video manner without causal control, interactivity, or long-horizon consistency required for purposeful reasoning. Existing world modeling efforts, on the other hand, often focus on restricted domains (e.g., physical, game, or 3D-scene dynamics) with limited depth and controllability, and struggle to generalize across diverse environments and interaction formats. In this work, we introduce PAN, a general, interactable, and long-horizon world model that predicts future world states through high-quality video simulation conditioned on history and natural language actions. PAN employs the Generative Latent Prediction (GLP) architecture that combines an autoregressive latent dynamics backbone based on a large language model (LLM), which grounds simulation in extensive text-based knowledge and enables conditioning on language-specified actions, with a video diffusion decoder that reconstructs perceptually detailed and temporally coherent visual observations, to achieve a unification between latent space reasoning (imagination) and realizable world dynamics (reality). Trained on large-scale video-action pairs spanning diverse domains, PAN supports open-domain, action-conditioned simulation with coherent, long-term dynamics. Extensive experiments show that PAN achieves strong performance in action-conditioned world simulation, long-horizon forecasting, and simulative reasoning compared to other video generators and world models, taking a step towards general world models that enable predictive simulation of future world states for reasoning and acting.",
    "translation": "标题：PAN：面向通用、可交互与长时程世界模拟的世界模型\n\n摘要：世界模型使智能体能够通过想象来预测和推理世界如何响应其行为而演变，并据此进行规划与决策。尽管当前视频生成模型能生成逼真的视觉序列，但它们通常采用提示到完整视频的生成模式，缺乏因果控制、交互能力以及 purposeful 推理所需的长时程一致性。而现有的世界建模研究往往局限于特定领域（如物理系统、游戏或三维场景动态），其深度与可控性有限，且难以泛化至多样化环境与交互形式。本文提出PAN模型——一个通用、可交互且具备长时程预测能力的世界模型，能够基于历史状态与自然语言指令，通过高质量视频模拟预测未来世界状态。PAN采用生成式潜在预测架构，结合基于大语言模型的自回归潜在动态主干（利用文本知识实现语义 grounding 并支持语言指令条件生成）与视频扩散解码器（重建感知细节丰富且时序一致的视觉观测），实现了潜在空间推理（想象）与可实现世界动态（现实）的统一。通过在大规模跨领域视频-动作配对数据上训练，PAN支持开放领域、动作条件化的长时程动态模拟。大量实验表明，PAN在动作条件化世界模拟、长时程预测和模拟推理任务中均优于现有视频生成器与世界模型，为构建能够通过预测性模拟未来世界状态以支持推理与决策的通用世界模型迈出关键一步。",
    "url": "https://huggingface.co/papers/2511.09057",
    "arxiv_url": "https://arxiv.org/abs/2511.09057"
  },
  {
    "title": "UniVA: Universal Video Agent towards Open-Source Next-Generation Video Generalist",
    "summary": "While specialized AI models excel at isolated video tasks like generation or understanding, real-world applications demand complex, iterative workflows that combine these capabilities. To bridge this gap, we introduce UniVA, an open-source, omni-capable multi-agent framework for next-generation video generalists that unifies video understanding, segmentation, editing, and generation into cohesive workflows. UniVA employs a Plan-and-Act dual-agent architecture that drives a highly automated and proactive workflow: a planner agent interprets user intentions and decomposes them into structured video-processing steps, while executor agents execute these through modular, MCP-based tool servers (for analysis, generation, editing, tracking, etc.). Through a hierarchical multi-level memory (global knowledge, task context, and user-specific preferences), UniVA sustains long-horizon reasoning, contextual continuity, and inter-agent communication, enabling interactive and self-reflective video creation with full traceability. This design enables iterative and any-conditioned video workflows (e.g., text/image/video-conditioned generation rightarrow multi-round editing rightarrow object segmentation rightarrow compositional synthesis) that were previously cumbersome to achieve with single-purpose models or monolithic video-language models. We also introduce UniVA-Bench, a benchmark suite of multi-step video tasks spanning understanding, editing, segmentation, and generation, to rigorously evaluate such agentic video systems. Both UniVA and UniVA-Bench are fully open-sourced, aiming to catalyze research on interactive, agentic, and general-purpose video intelligence for the next generation of multimodal AI systems. (https://univa.online/)",
    "translation": "标题：UniVA：面向开源的新一代视频通用智能体的通用视频智能体框架\n\n摘要：尽管专业人工智能模型在视频生成或理解等独立任务中表现出色，但实际应用需要结合这些能力的复杂迭代工作流。为弥补这一差距，我们推出UniVA——一个面向新一代视频通用场景的开源全能力多智能体框架，将视频理解、分割、编辑与生成统一为连贯的工作流。UniVA采用规划-执行双智能体架构驱动高度自动化的工作流程：规划智能体解析用户意图并将其分解为结构化视频处理步骤，执行智能体则通过基于模型控制协议（MCP）的模块化工具服务器（支持分析、生成、编辑、跟踪等功能）实施这些步骤。通过分层多级记忆系统（全局知识、任务上下文与用户特定偏好），UniVA支持长周期推理、上下文连续性及智能体间通信，实现具有全链路可追溯性的交互式自反思视频创作。该设计使迭代式任意条件视频工作流（例如文本/图像/视频条件生成→多轮编辑→对象分割→组合合成）成为可能，这些流程以往使用单功能模型或单体视频语言模型难以实现。我们同时推出UniVA-Bench基准测试套件，涵盖理解、编辑、分割与生成的多步骤视频任务，用于严格评估此类智能视频系统。UniVA与UniVA-Bench均已全面开源，旨在推动面向新一代多模态AI系统的交互式、智能化通用视频智能研究。(https://univa.online/)",
    "url": "https://huggingface.co/papers/2511.08521",
    "arxiv_url": "https://arxiv.org/abs/2511.08521"
  },
  {
    "title": "Black-Box On-Policy Distillation of Large Language Models",
    "summary": "Black-box distillation creates student large language models (LLMs) by learning from a proprietary teacher model's text outputs alone, without access to its internal logits or parameters. In this work, we introduce Generative Adversarial Distillation (GAD), which enables on-policy and black-box distillation. GAD frames the student LLM as a generator and trains a discriminator to distinguish its responses from the teacher LLM's, creating a minimax game. The discriminator acts as an on-policy reward model that co-evolves with the student, providing stable, adaptive feedback. Experimental results show that GAD consistently surpasses the commonly used sequence-level knowledge distillation. In particular, Qwen2.5-14B-Instruct (student) trained with GAD becomes comparable to its teacher, GPT-5-Chat, on the LMSYS-Chat automatic evaluation. The results establish GAD as a promising and effective paradigm for black-box LLM distillation.",
    "translation": "标题：大语言模型的黑盒同策略蒸馏\n\n摘要：黑盒蒸馏技术仅通过专有教师模型的文本输出训练学生大语言模型，无需访问其内部逻辑值或参数。本研究提出生成对抗蒸馏方法，实现同策略的黑盒蒸馏。该方法将学生大语言模型构建为生成器，并训练判别器区分其响应与教师模型的输出，形成极小极大博弈框架。判别器作为随学生模型协同演进的同策略奖励模型，可提供稳定自适应的反馈机制。实验结果表明，生成对抗蒸馏方法持续优于常用的序列级知识蒸馏。特别值得注意的是，采用该方法训练的Qwen2.5-14B-Instruct模型在LMSYS-Chat自动评估中达到了与教师模型GPT-5-Chat相当的性能。这些成果确立了生成对抗蒸馏作为黑盒大语言模型蒸馏领域具有前景的有效范式。",
    "url": "https://huggingface.co/papers/2511.10643",
    "arxiv_url": "https://arxiv.org/abs/2511.10643"
  },
  {
    "title": "Hail to the Thief: Exploring Attacks and Defenses in Decentralised GRPO",
    "summary": "Group Relative Policy Optimization (GRPO) has demonstrated great utilization in post-training of Large Language Models (LLMs). In GRPO, prompts are answered by the model and, through reinforcement learning, preferred completions are learnt. Owing to the small communication volume, GRPO is inherently suitable for decentralised training as the prompts can be concurrently answered by multiple nodes and then exchanged in the forms of strings. In this work, we present the first adversarial attack in decentralised GRPO. We demonstrate that malicious parties can poison such systems by injecting arbitrary malicious tokens in benign models in both out-of-context and in-context attacks. Using empirical examples of math and coding tasks, we show that adversarial attacks can easily poison the benign nodes, polluting their local LLM post-training, achieving attack success rates up to 100% in as few as 50 iterations. We propose two ways to defend against these attacks, depending on whether all users train the same model or different models. We show that these defenses can achieve stop rates of up to 100%, making the attack impossible.",
    "translation": "标题：窃国者诛：探索去中心化GRPO中的攻击与防御策略\n\n摘要：群体相对策略优化（GRPO）在大语言模型（LLM）的后训练中展现出显著效用。该机制通过模型对提示词生成回答，并借助强化学习掌握更优的完成模式。由于通信量较小，GRPO天然适用于去中心化训练——多个节点可并行处理提示词，随后以字符串形式交换结果。本研究首次揭示了去中心化GRPO环境中的对抗攻击现象。我们论证了恶意参与方可通过上下文无关与上下文关联两种攻击模式，在良性模型中注入任意恶意标记以毒化系统。通过数学运算与编程任务的实证案例，我们证明对抗攻击能轻易污染良性节点，破坏其本地LLM后训练过程，在不足50次迭代中即可实现高达100%的攻击成功率。针对用户训练相同模型或不同模型两种场景，我们提出两种防御方案。实验表明这些防御措施可实现最高100%的攻击阻断率，从而有效遏制此类攻击。",
    "url": "https://huggingface.co/papers/2511.09780",
    "arxiv_url": "https://arxiv.org/abs/2511.09780"
  },
  {
    "title": "Depth Anything 3: Recovering the Visual Space from Any Views",
    "summary": "We present Depth Anything 3 (DA3), a model that predicts spatially consistent geometry from an arbitrary number of visual inputs, with or without known camera poses. In pursuit of minimal modeling, DA3 yields two key insights: a single plain transformer (e.g., vanilla DINO encoder) is sufficient as a backbone without architectural specialization, and a singular depth-ray prediction target obviates the need for complex multi-task learning. Through our teacher-student training paradigm, the model achieves a level of detail and generalization on par with Depth Anything 2 (DA2). We establish a new visual geometry benchmark covering camera pose estimation, any-view geometry and visual rendering. On this benchmark, DA3 sets a new state-of-the-art across all tasks, surpassing prior SOTA VGGT by an average of 44.3% in camera pose accuracy and 25.1% in geometric accuracy. Moreover, it outperforms DA2 in monocular depth estimation. All models are trained exclusively on public academic datasets.",
    "translation": "标题：Depth Anything 3：从任意视角重建视觉空间\n\n摘要：本文提出Depth Anything 3（DA3）模型，该模型能够从任意数量的视觉输入中预测空间一致的几何结构，且无需已知相机位姿。为实现极简建模，DA3获得两项关键发现：采用单一标准Transformer（如原始DINO编码器）作为主干网络即可满足需求，无需专门架构设计；采用单一深度射线预测目标即可规避复杂的多任务学习。通过师生训练范式，该模型在细节还原与泛化能力方面达到与Depth Anything 2（DA2）相当的水平。我们建立了涵盖相机位姿估计、任意视角几何重建与视觉渲染的新视觉几何基准测试集。在该基准测试中，DA3在所有任务中均创下最新性能纪录，相机位姿估计精度较先前最优方法VGGT平均提升44.3%，几何精度平均提升25.1%。此外，该模型在单目深度估计任务中的表现也优于DA2。所有模型均仅使用公开学术数据集进行训练。",
    "url": "https://huggingface.co/papers/2511.10647",
    "arxiv_url": "https://arxiv.org/abs/2511.10647"
  },
  {
    "title": "Superpositional Gradient Descent: Harnessing Quantum Principles for\n  Model Training",
    "summary": "Large language models (LLMs) are increasingly trained with classical\noptimization techniques like AdamW to improve convergence and generalization.\nHowever, the mechanisms by which quantum-inspired methods enhance classical\ntraining remain underexplored. We introduce Superpositional Gradient Descent\n(SGD), a novel optimizer linking gradient updates with quantum superposition by\ninjecting quantum circuit perturbations. We present a mathematical framework\nand implement hybrid quantum-classical circuits in PyTorch and Qiskit. On\nsynthetic sequence classification and large-scale LLM fine-tuning, SGD\nconverges faster and yields lower final loss than AdamW. Despite promising\nresults, scalability and hardware constraints limit adoption. Overall, this\nwork provides new insights into the intersection of quantum computing and deep\nlearning, suggesting practical pathways for leveraging quantum principles to\ncontrol and enhance model behavior.",
    "translation": "标题：叠加梯度下降法：运用量子原理优化模型训练\n\n摘要：当前大规模语言模型普遍采用AdamW等经典优化技术进行训练以提升收敛性与泛化能力，然而量子启发性方法增强经典训练的内在机制仍待深入探索。本文提出叠加梯度下降法——一种通过注入量子电路扰动将梯度更新与量子叠加态相关联的新型优化器。我们构建了完整的数学框架，并在PyTorch和Qiskit平台上实现了混合量子-经典电路。在合成序列分类任务和大规模语言模型微调实验中，该方法相较AdamW实现了更快的收敛速度与更低的最终损失值。尽管实验结果令人鼓舞，可扩展性与硬件限制仍是实际应用的瓶颈。本研究为量子计算与深度学习的交叉领域提供了新视角，揭示了运用量子原理调控模型行为的可行路径。",
    "url": "https://huggingface.co/papers/2511.01918",
    "arxiv_url": "https://arxiv.org/abs/2511.01918"
  },
  {
    "title": "Solving a Million-Step LLM Task with Zero Errors",
    "summary": "LLMs have achieved remarkable breakthroughs in reasoning, insights, and tool use, but chaining these abilities into extended processes at the scale of those routinely executed by humans, organizations, and societies has remained out of reach. The models have a persistent error rate that prevents scale-up: for instance, recent experiments in the Towers of Hanoi benchmark domain showed that the process inevitably becomes derailed after at most a few hundred steps. Thus, although LLM research is often still benchmarked on tasks with relatively few dependent logical steps, there is increasing attention on the ability (or inability) of LLMs to perform long range tasks. This paper describes MAKER, the first system that successfully solves a task with over one million LLM steps with zero errors, and, in principle, scales far beyond this level. The approach relies on an extreme decomposition of a task into subtasks, each of which can be tackled by focused microagents. The high level of modularity resulting from the decomposition allows error correction to be applied at each step through an efficient multi-agent voting scheme. This combination of extreme decomposition and error correction makes scaling possible. Thus, the results suggest that instead of relying on continual improvement of current LLMs, massively decomposed agentic processes (MDAPs) may provide a way to efficiently solve problems at the level of organizations and societies.",
    "translation": "标题：零误差完成百万步大语言模型任务的实现方案\n\n摘要：大语言模型在推理能力、洞察深度和工具使用方面取得了显著突破，但将这些能力串联成人类、组织和社会日常执行的扩展流程仍面临挑战。模型存在的持续错误率阻碍了规模扩展：例如近期在汉诺塔基准领域的实验表明，该过程在最多数百步后必然失控。因此，尽管大语言模型研究仍常以逻辑依赖步骤较少的任务作为基准，但学界日益关注其执行长程任务的能力局限。本文提出的MAKER系统首次实现了零误差完成超百万步大语言模型任务的突破，且理论上可扩展至更大规模。该方案通过将任务极致分解为可由专注微智能体处理的子任务，借助分解产生的高度模块化特性，通过高效的多智能体投票机制在每一步实施纠错。这种极致分解与纠错机制的结合使规模扩展成为可能。研究结果表明，相较于持续改进现有大语言模型，采用大规模分解智能体流程可能为有效解决组织与社会层级问题提供新路径。",
    "url": "https://huggingface.co/papers/2511.09030",
    "arxiv_url": "https://arxiv.org/abs/2511.09030"
  },
  {
    "title": "AlphaResearch: Accelerating New Algorithm Discovery with Language Models",
    "summary": "Large language models have made significant progress in complex but easy-to-verify problems, yet they still struggle with discovering the unknown. In this paper, we present AlphaResearch, an autonomous research agent designed to discover new algorithms on open-ended problems. To synergize the feasibility and innovation of the discovery process, we construct a novel dual research environment by combining the execution-based verify and simulated real-world peer review environment. AlphaResearch discovers new algorithm by iteratively running the following steps: (1) propose new ideas (2) verify the ideas in the dual research environment (3) optimize the research proposals for better performance. To promote a transparent evaluation process, we construct AlphaResearchComp, a new evaluation benchmark that includes an eight open-ended algorithmic problems competition, with each problem carefully curated and verified through executable pipelines, objective metrics, and reproducibility checks. AlphaResearch gets a 2/8 win rate in head-to-head comparison with human researchers, demonstrate the possibility of accelerating algorithm discovery with LLMs. Notably, the algorithm discovered by AlphaResearch on the ``packing circles'' problem achieves the best-of-known performance, surpassing the results of human researchers and strong baselines from recent work (e.g., AlphaEvolve). Additionally, we conduct a comprehensive analysis of the remaining challenges of the 6/8 failure cases, providing valuable insights for future research.",
    "translation": "标题：AlphaResearch：利用语言模型加速新算法发现的探索\n\n摘要：大型语言模型在复杂但易于验证的问题上已取得显著进展，但在探索未知领域方面仍面临挑战。本文提出AlphaResearch——一种面向开放性问题自主探索新算法的研究智能体。为协同实现发现过程的可行性与创新性，我们通过融合基于执行的验证机制与模拟现实同行评审环境，构建了新型双重研究环境。该系统的算法发现流程通过迭代执行以下步骤实现：（1）提出新构想；（2）在双重研究环境中验证构想；（3）优化研究方案以提升性能。为建立透明化评估体系，我们构建了AlphaResearchComp评估基准，包含八项开放式算法问题的竞赛，每个问题均通过可执行流程、客观指标与可复现性检验进行精心设计与验证。在与人类研究者的直接对比中，AlphaResearch取得了2/8的胜率，证明了利用大语言模型加速算法发现的可行性。值得注意的是，该系统在\"圆形填充\"问题上发现的算法实现了当前最佳性能，超越了人类研究者及现有强基线方法（如AlphaEvolve）的结果。此外，我们对剩余6/8失败案例进行了全面归因分析，为后续研究提供了重要参考。",
    "url": "https://huggingface.co/papers/2511.08522",
    "arxiv_url": "https://arxiv.org/abs/2511.08522"
  },
  {
    "title": "Music Flamingo: Scaling Music Understanding in Audio Language Models",
    "summary": "We introduce Music Flamingo, a novel large audio-language model designed to advance music (including song) understanding in foundational audio models. While audio-language research has progressed rapidly, music remains challenging due to its dynamic, layered, and information-dense nature. Progress has been further limited by the difficulty of scaling open audio understanding models, primarily because of the scarcity of high-quality music data and annotations. As a result, prior models are restricted to producing short, high-level captions, answering only surface-level questions, and showing limited generalization across diverse musical cultures. To address these challenges, we curate MF-Skills, a large-scale dataset labeled through a multi-stage pipeline that yields rich captions and question-answer pairs covering harmony, structure, timbre, lyrics, and cultural context. We fine-tune an enhanced Audio Flamingo 3 backbone on MF-Skills and further strengthen multiple skills relevant to music understanding. To improve the model's reasoning abilities, we introduce a post-training recipe: we first cold-start with MF-Think, a novel chain-of-thought dataset grounded in music theory, followed by GRPO-based reinforcement learning with custom rewards. Music Flamingo achieves state-of-the-art results across 10+ benchmarks for music understanding and reasoning, establishing itself as a generalist and musically intelligent audio-language model. Beyond strong empirical results, Music Flamingo sets a new standard for advanced music understanding by demonstrating how models can move from surface-level recognition toward layered, human-like perception of songs. We believe this work provides both a benchmark and a foundation for the community to build the next generation of models that engage with music as meaningfully as humans do.",
    "translation": "标题：Music Flamingo：音频语言模型中音乐理解能力的规模化拓展\n\n摘要：本文提出Music Flamingo——一种新型大规模音频语言模型，旨在提升基础音频模型对音乐（含歌曲）的理解能力。尽管音频语言研究发展迅速，但由于音乐具有动态性、层次性和信息密集性等特点，其理解仍面临挑战。开放音频理解模型的规模化进展进一步受限于高质量音乐数据与标注的稀缺性，导致现有模型仅能生成简短的高层描述、回答浅层问题，且在不同音乐文化间的泛化能力有限。为应对这些挑战，我们构建了MF-Skills大规模数据集，通过多阶段标注流程生成涵盖和声、曲式、音色、歌词及文化背景的丰富描述与问答对。基于该数据集，我们对增强版Audio Flamingo 3主干网络进行微调，并强化了多项音乐理解相关技能。为提升模型推理能力，我们提出一种后训练方案：首先基于音乐理论构建的思维链数据集MF-Think进行冷启动训练，随后采用定制奖励函数通过GRPO强化学习进行优化。Music Flamingo在10余项音乐理解与推理基准测试中达到最先进水平，确立了其作为通用型音乐智能音频语言模型的地位。除实证结果外，该模型通过展现从表层识别转向人类式多层次歌曲感知的能力，为高级音乐理解设立了新标准。我们相信这项工作既为学界提供了基准参照，也为构建具有人类级音乐交互能力的新一代模型奠定了基石。",
    "url": "https://huggingface.co/papers/2511.10289",
    "arxiv_url": "https://arxiv.org/abs/2511.10289"
  },
  {
    "title": "Rubric-Based Benchmarking and Reinforcement Learning for Advancing LLM Instruction Following",
    "summary": "Recent progress in large language models (LLMs) has led to impressive performance on a range of tasks, yet advanced instruction following (IF)-especially for complex, multi-turn, and system-prompted instructions-remains a significant challenge. Rigorous evaluation and effective training for such capabilities are hindered by the lack of high-quality, human-annotated benchmarks and reliable, interpretable reward signals. In this work, we introduce AdvancedIF (we will release this benchmark soon), a comprehensive benchmark featuring over 1,600 prompts and expert-curated rubrics that assess LLMs ability to follow complex, multi-turn, and system-level instructions. We further propose RIFL (Rubric-based Instruction-Following Learning), a novel post-training pipeline that leverages rubric generation, a finetuned rubric verifier, and reward shaping to enable effective reinforcement learning for instruction following. Extensive experiments demonstrate that RIFL substantially improves the instruction-following abilities of LLMs, achieving a 6.7% absolute gain on AdvancedIF and strong results on public benchmarks. Our ablation studies confirm the effectiveness of each component in RIFL. This work establishes rubrics as a powerful tool for both training and evaluating advanced IF in LLMs, paving the way for more capable and reliable AI systems.",
    "translation": "标题：基于量规的基准测试与强化学习在提升大语言模型指令遵循能力中的应用\n\n摘要：大语言模型近期取得的进展使其在一系列任务中展现出卓越性能，然而在复杂、多轮次和系统提示的指令遵循方面仍面临重大挑战。由于缺乏高质量人工标注的基准测试和可靠可解释的奖励信号，针对此类能力的严格评估和有效训练受到制约。本研究提出AdvancedIF（即将发布该基准），这是一个包含1,600余条提示词和专家制定量规的综合基准，用于评估大语言模型遵循复杂、多轮次及系统级指令的能力。我们进一步提出RIFL（基于量规的指令遵循学习），这是一种创新的后训练流程，通过量规生成、微调的量规验证器和奖励塑造来实现有效的指令遵循强化学习。大量实验表明，RIFL显著提升了大语言模型的指令遵循能力，在AdvancedIF基准上取得6.7%的绝对性能提升，并在公共基准测试中表现优异。消融研究证实了RIFL各组成部分的有效性。本工作确立了量规作为大语言模型高级指令遵循训练与评估的强大工具，为开发更具能力与可靠性的AI系统开辟了新路径。",
    "url": "https://huggingface.co/papers/2511.10507",
    "arxiv_url": "https://arxiv.org/abs/2511.10507"
  },
  {
    "title": "Benchmarking Diversity in Image Generation via Attribute-Conditional Human Evaluation",
    "summary": "Despite advances in generation quality, current text-to-image (T2I) models often lack diversity, generating homogeneous outputs. This work introduces a framework to address the need for robust diversity evaluation in T2I models. Our framework systematically assesses diversity by evaluating individual concepts and their relevant factors of variation. Key contributions include: (1) a novel human evaluation template for nuanced diversity assessment; (2) a curated prompt set covering diverse concepts with their identified factors of variation (e.g. prompt: An image of an apple, factor of variation: color); and (3) a methodology for comparing models in terms of human annotations via binomial tests.\n  Furthermore, we rigorously compare various image embeddings for diversity measurement. Notably, our principled approach enables ranking of T2I models by diversity, identifying categories where they particularly struggle. This research offers a robust methodology and insights, paving the way for improvements in T2I model diversity and metric development.",
    "translation": "标题：基于属性条件人工评估的图像生成多样性基准测试\n\n摘要：尽管生成质量不断提升，当前文生图模型仍普遍存在输出同质化、缺乏多样性的问题。本研究提出了一套系统性评估框架，以解决文生图模型多样性稳健评估的需求。该框架通过评估独立概念及其相关变异因子，实现对多样性的系统化度量。主要贡献包括：（1）提出用于精细化多样性评估的新型人工评估模板；（2）构建涵盖多维度概念的提示词集，并标注对应的变异因子（如提示词：\"苹果图像\"，变异因子：颜色）；（3）建立基于二项检验的人工标注模型对比方法。此外，我们系统比较了多种图像嵌入方法在多样性度量中的表现。值得强调的是，本方法能够根据多样性表现对文生图模型进行排序，并精准识别其表现薄弱的类别。本研究不仅提供了严谨的评估方法论与关键见解，更为提升文生图模型多样性及改进评估指标奠定了坚实基础。",
    "url": "https://huggingface.co/papers/2511.10547",
    "arxiv_url": "https://arxiv.org/abs/2511.10547"
  },
  {
    "title": "ResearchRubrics: A Benchmark of Prompts and Rubrics For Evaluating Deep Research Agents",
    "summary": "Deep Research (DR) is an emerging agent application that leverages large language models (LLMs) to address open-ended queries. It requires the integration of several capabilities, including multi-step reasoning, cross-document synthesis, and the generation of evidence-backed, long-form answers. Evaluating DR remains challenging because responses are lengthy and diverse, admit many valid solutions, and often depend on dynamic information sources. We introduce ResearchRubrics, a standardized benchmark for DR built with over 2,800+ hours of human labor that pairs realistic, domain-diverse prompts with 2,500+ expert-written, fine-grained rubrics to assess factual grounding, reasoning soundness, and clarity. We also propose a new complexity framework for categorizing DR tasks along three axes: conceptual breadth, logical nesting, and exploration. In addition, we develop human and model-based evaluation protocols that measure rubric adherence for DR agents. We evaluate several state-of-the-art DR systems and find that even leading agents like Gemini's DR and OpenAI's DR achieve under 68% average compliance with our rubrics, primarily due to missed implicit context and inadequate reasoning about retrieved information. Our results highlight the need for robust, scalable assessment of deep research capabilities, to which end we release ResearchRubrics(including all prompts, rubrics, and evaluation code) to facilitate progress toward well-justified research assistants.",
    "translation": "标题：ResearchRubrics：深度研究智能体评估的提示与评分标准基准\n\n摘要：深度研究是一种新兴的智能体应用，其利用大语言模型处理开放式查询。该技术需要整合多重能力，包括多步推理、跨文档综合以及生成具有证据支撑的长篇回答。由于回答内容冗长多样、存在多种有效解决方案且常依赖动态信息源，深度研究的评估仍具挑战性。本文提出ResearchRubrics——一个基于2,800+人工工时构建的标准化深度研究基准，包含真实且领域多样的提示语，以及2,500+专家撰写的细粒度评分标准，用于评估事实依据、推理严谨性和表述清晰度。我们还提出了新的复杂度框架，从概念广度、逻辑嵌套度和探索维度三个轴向对深度研究任务进行分类。此外，我们开发了人工与模型结合的评估方案，用以衡量深度研究智能体对评分标准的符合程度。通过对若干前沿深度研究系统的评估，我们发现即使是Gemini DR和OpenAI DR等领先智能体，其平均标准符合度也不足68%，主要归因于对隐含语境的遗漏以及对检索信息推理不足。研究结果凸显了对深度研究能力进行稳健、可扩展评估的必要性。为此我们开源ResearchRubrics（包含全部提示语、评分标准和评估代码），以推动具有充分论证能力的研究助手的发展。",
    "url": "https://huggingface.co/papers/2511.07685",
    "arxiv_url": "https://arxiv.org/abs/2511.07685"
  },
  {
    "title": "MuSc-V2: Zero-Shot Multimodal Industrial Anomaly Classification and Segmentation with Mutual Scoring of Unlabeled Samples",
    "summary": "Zero-shot anomaly classification (AC) and segmentation (AS) methods aim to identify and outline defects without using any labeled samples. In this paper, we reveal a key property that is overlooked by existing methods: normal image patches across industrial products typically find many other similar patches, not only in 2D appearance but also in 3D shapes, while anomalies remain diverse and isolated. To explicitly leverage this discriminative property, we propose a Mutual Scoring framework (MuSc-V2) for zero-shot AC/AS, which flexibly supports single 2D/3D or multimodality. Specifically, our method begins by improving 3D representation through Iterative Point Grouping (IPG), which reduces false positives from discontinuous surfaces. Then we use Similarity Neighborhood Aggregation with Multi-Degrees (SNAMD) to fuse 2D/3D neighborhood cues into more discriminative multi-scale patch features for mutual scoring. The core comprises a Mutual Scoring Mechanism (MSM) that lets samples within each modality to assign score to each other, and Cross-modal Anomaly Enhancement (CAE) that fuses 2D and 3D scores to recover modality-specific missing anomalies. Finally, Re-scoring with Constrained Neighborhood (RsCon) suppresses false classification based on similarity to more representative samples. Our framework flexibly works on both the full dataset and smaller subsets with consistently robust performance, ensuring seamless adaptability across diverse product lines. In aid of the novel framework, MuSc-V2 achieves significant performance improvements: a +23.7% AP gain on the MVTec 3D-AD dataset and a +19.3% boost on the Eyecandies dataset, surpassing previous zero-shot benchmarks and even outperforming most few-shot methods. The code will be available at The code will be available at https://github.com/HUST-SLOW/MuSc-V2{https://github.com/HUST-SLOW/MuSc-V2}.",
    "translation": "标题：MuSc-V2：基于无标签样本互评的零样本多模态工业异常分类与分割方法\n\n摘要：零样本异常分类与分割方法旨在无需任何标注样本的情况下实现缺陷识别与定位。本文揭示了现有方法忽视的关键特性：工业产品中的正常图像块通常在二维外观和三维形状上存在大量相似样本，而异常则保持多样性和孤立性。为显式利用这一判别特性，我们提出用于零样本异常分类/分割的互评框架MuSc-V2，该框架灵活支持单模态（2D/3D）或多模态应用。具体而言，我们首先通过迭代点分组技术改进三维表征，降低因表面不连续性产生的误判；接着采用多阶相似邻域聚合方法，将2D/3D邻域信息融合为更具判别力的多尺度图像块特征用于互评。核心机制包括：模态内样本相互赋分的互评机制，以及融合2D与3D评分以恢复模态特异性缺失异常的跨模态异常增强模块。最后，通过约束邻域重评分机制，基于与更具代表性样本的相似性来抑制误分类。本框架在完整数据集和较小子集上均能保持稳定的强劲性能，确保跨产品线的无缝适配。该创新框架使MuSc-V2实现显著性能提升：在MVTec 3D-AD数据集上平均精度提升23.7%，在Eyecandies数据集上提升19.3%，不仅超越现有零样本基准，更优于多数小样本方法。代码将发布于https://github.com/HUST-SLOW/MuSc-V2。",
    "url": "https://huggingface.co/papers/2511.10047",
    "arxiv_url": "https://arxiv.org/abs/2511.10047"
  },
  {
    "title": "AffordBot: 3D Fine-grained Embodied Reasoning via Multimodal Large Language Models",
    "summary": "Effective human-agent collaboration in physical environments requires understanding not only what to act upon, but also where the actionable elements are and how to interact with them. Existing approaches often operate at the object level or disjointedly handle fine-grained affordance reasoning, lacking coherent, instruction-driven grounding and reasoning. In this work, we introduce a new task: Fine-grained 3D Embodied Reasoning, which requires an agent to predict, for each referenced affordance element in a 3D scene, a structured triplet comprising its spatial location, motion type, and motion axis, based on a task instruction. To solve this task, we propose AffordBot, a novel framework that integrates Multimodal Large Language Models (MLLMs) with a tailored chain-of-thought (CoT) reasoning paradigm. To bridge the gap between 3D input and 2D-compatible MLLMs, we render surround-view images of the scene and project 3D element candidates into these views, forming a rich visual representation aligned with the scene geometry. Our CoT pipeline begins with an active perception stage, prompting the MLLM to select the most informative viewpoint based on the instruction, before proceeding with step-by-step reasoning to localize affordance elements and infer plausible interaction motions. Evaluated on the SceneFun3D dataset, AffordBot achieves state-of-the-art performance, demonstrating strong generalization and physically grounded reasoning with only 3D point cloud input and MLLMs.",
    "translation": "标题：AffordBot：基于多模态大语言模型的3D细粒度具身推理\n\n摘要：在物理环境中实现有效的人机协作，不仅需要理解操作对象，还需精确定位可操作元素的位置并规划交互方式。现有方法多在物体层面进行操作，或将细粒度功能推理割裂处理，缺乏基于指令的连贯性 grounding 与推理机制。本研究提出\"细粒度3D具身推理\"新任务，要求智能体根据任务指令，为三维场景中每个被引用的功能元素预测包含空间位置、运动类型与运动轴的结构化三元组。为此，我们设计AffordBot创新框架，通过整合多模态大语言模型与定制化的思维链推理范式来解决该任务。为弥合3D输入与2D兼容MLLMs之间的鸿沟，我们渲染场景环视图像并将3D候选元素投影至这些视图，构建与场景几何对齐的丰富视觉表征。我们的思维链流程始于主动感知阶段：首先引导MLLM根据指令选择最具信息量的视角，随后通过逐步推理实现功能元素定位及合理交互运动推断。在SceneFun3D数据集上的评估表明，AffordBot仅凭3D点云输入与MLLMs即达到最先进性能，展现出强大的泛化能力与物理 grounded 推理能力。",
    "url": "https://huggingface.co/papers/2511.10017",
    "arxiv_url": "https://arxiv.org/abs/2511.10017"
  },
  {
    "title": "SliderEdit: Continuous Image Editing with Fine-Grained Instruction Control",
    "summary": "Instruction-based image editing models have recently achieved impressive performance, enabling complex edits to an input image from a multi-instruction prompt. However, these models apply each instruction in the prompt with a fixed strength, limiting the user's ability to precisely and continuously control the intensity of individual edits. We introduce SliderEdit, a framework for continuous image editing with fine-grained, interpretable instruction control. Given a multi-part edit instruction, SliderEdit disentangles the individual instructions and exposes each as a globally trained slider, allowing smooth adjustment of its strength. Unlike prior works that introduced slider-based attribute controls in text-to-image generation, typically requiring separate training or fine-tuning for each attribute or concept, our method learns a single set of low-rank adaptation matrices that generalize across diverse edits, attributes, and compositional instructions. This enables continuous interpolation along individual edit dimensions while preserving both spatial locality and global semantic consistency. We apply SliderEdit to state-of-the-art image editing models, including FLUX-Kontext and Qwen-Image-Edit, and observe substantial improvements in edit controllability, visual consistency, and user steerability. To the best of our knowledge, we are the first to explore and propose a framework for continuous, fine-grained instruction control in instruction-based image editing models. Our results pave the way for interactive, instruction-driven image manipulation with continuous and compositional control.",
    "translation": "标题：SliderEdit：基于细粒度指令控制的连续图像编辑\n\n摘要：基于指令的图像编辑模型近期取得了显著进展，能够通过多指令提示对输入图像进行复杂编辑。然而，现有模型对提示中的每条指令均采用固定强度执行，限制了用户对单次编辑强度进行精确连续控制的能力。本文提出SliderEdit框架，通过细粒度、可解释的指令控制实现连续图像编辑。该框架在接收复合编辑指令后，可解耦各子指令并将其转化为全局训练的滑动控制器，支持通过平滑调节实现编辑强度控制。与文本到图像生成领域需要为每个属性或概念单独训练滑动控制器的方法不同，本方法仅需学习一组低秩适配矩阵，即可泛化应用于多样化编辑任务、属性调整及组合指令场景。该方法在实现单维度编辑连续插值的同时，能有效保持空间局部特征与全局语义一致性。我们将SliderEdit应用于FLUX-Kontext和Qwen-Image-Edit等前沿图像编辑模型，在编辑可控性、视觉一致性和用户导向性方面均取得显著提升。据我们所知，这是首个在基于指令的图像编辑模型中实现细粒度连续控制的研究框架。本研究成果为构建具有连续组合控制能力的交互式指令驱动图像处理系统开辟了新途径。",
    "url": "https://huggingface.co/papers/2511.09715",
    "arxiv_url": "https://arxiv.org/abs/2511.09715"
  },
  {
    "title": "MM-CRITIC: A Holistic Evaluation of Large Multimodal Models as Multimodal Critique",
    "summary": "The ability of critique is vital for models to self-improve and serve as reliable AI assistants. While extensively studied in language-only settings, multimodal critique of Large Multimodal Models (LMMs) remains underexplored despite their growing capabilities in tasks like captioning and visual reasoning. In this work, we introduce MM-CRITIC, a holistic benchmark for evaluating the critique ability of LMMs across multiple dimensions: basic, correction, and comparison. Covering 8 main task types and over 500 tasks, MM-CRITIC collects responses from various LMMs with different model sizes and is composed of 4471 samples. To enhance the evaluation reliability, we integrate expert-informed ground answers into scoring rubrics that guide GPT-4o in annotating responses and generating reference critiques, which serve as anchors for trustworthy judgments. Extensive experiments validate the effectiveness of MM-CRITIC and provide a comprehensive assessment of leading LMMs' critique capabilities under multiple dimensions. Further analysis reveals some key insights, including the correlation between response quality and critique, and varying critique difficulty across evaluation dimensions. Our code is available at https://github.com/MichealZeng0420/MM-Critic.",
    "translation": "标题：MM-CRITIC：大型多模态模型多模态评判能力的系统性评估\n\n摘要：评判能力对模型实现自我提升并成为可靠AI助手至关重要。尽管在纯语言场景中已有深入研究，但大型多模态模型（LMMs）的多模态评判能力——尽管其在图像描述和视觉推理等任务中的性能日益增强——仍未得到充分探索。本研究提出MM-CRITIC，一个从基础评判、修正评判和比较评判三个维度系统评估LMMs评判能力的基准框架。该框架涵盖8种主要任务类型超过500项任务，收集了不同参数规模LMMs的响应数据，共构成4471个评估样本。为提升评估信度，我们通过专家知识构建标准答案并融入评分标准，指导GPT-4o对模型响应进行标注并生成参考评判，形成可靠判断基准。大规模实验验证了MM-CRITIC的有效性，并对主流LMMs在多维度的评判能力进行了全面评估。深入分析揭示了若干关键发现，包括响应质量与评判能力的相关性，以及不同评估维度中评判难度的差异性。代码已开源：https://github.com/MichealZeng0420/MM-Critic。",
    "url": "https://huggingface.co/papers/2511.09067",
    "arxiv_url": "https://arxiv.org/abs/2511.09067"
  },
  {
    "title": "CC30k: A Citation Contexts Dataset for Reproducibility-Oriented Sentiment Analysis",
    "summary": "Sentiments about the reproducibility of cited papers in downstream literature offer community perspectives and have shown as a promising signal of the actual reproducibility of published findings. To train effective models to effectively predict reproducibility-oriented sentiments and further systematically study their correlation with reproducibility, we introduce the CC30k dataset, comprising a total of 30,734 citation contexts in machine learning papers. Each citation context is labeled with one of three reproducibility-oriented sentiment labels: Positive, Negative, or Neutral, reflecting the cited paper's perceived reproducibility or replicability. Of these, 25,829 are labeled through crowdsourcing, supplemented with negatives generated through a controlled pipeline to counter the scarcity of negative labels. Unlike traditional sentiment analysis datasets, CC30k focuses on reproducibility-oriented sentiments, addressing a research gap in resources for computational reproducibility studies. The dataset was created through a pipeline that includes robust data cleansing, careful crowd selection, and thorough validation. The resulting dataset achieves a labeling accuracy of 94%. We then demonstrated that the performance of three large language models significantly improves on the reproducibility-oriented sentiment classification after fine-tuning using our dataset. The dataset lays the foundation for large-scale assessments of the reproducibility of machine learning papers. The CC30k dataset and the Jupyter notebooks used to produce and analyze the dataset are publicly available at https://github.com/lamps-lab/CC30k .",
    "translation": "标题：CC30k：面向可复现性情感分析的引用上下文数据集\n\n摘要：下游文献中对被引论文可复现性的情感评价反映了学术界的共识，已被证明是预测已发表研究成果实际可复现性的有效指标。为训练能精准预测可复现性情感并系统研究其与可复现性关联的模型，我们构建了CC30k数据集，该数据集包含机器学习领域论文中的30,734条引用上下文。每条引用上下文均标注有三种面向可复现性的情感标签之一：积极、消极或中立，用以反映被引论文的感知可复现性。其中25,829条通过众包完成标注，并采用受控流程生成负面样本以缓解负面标签稀缺问题。与传统情感分析数据集不同，CC30k专注于可复现性情感分析，填补了计算可复现性研究领域的资源空白。本数据集通过包含数据清洗、严格众包筛选和多重验证的流程构建，最终标注准确率达94%。实验表明，使用本数据集微调后，三种大语言模型在可复现性情感分类任务上的性能显著提升。该数据集为大规模评估机器学习论文的可复现性奠定了基础。CC30k数据集及相关的数据生成分析代码已公开于https://github.com/lamps-lab/CC30k。",
    "url": "https://huggingface.co/papers/2511.07790",
    "arxiv_url": "https://arxiv.org/abs/2511.07790"
  }
]