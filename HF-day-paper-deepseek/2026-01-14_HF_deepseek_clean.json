[
  {
    "title": "MemGovern: Enhancing Code Agents through Learning from Governed Human Experiences",
    "summary": "While autonomous software engineering (SWE) agents are reshaping programming paradigms, they currently suffer from a \"closed-world\" limitation: they attempt to fix bugs from scratch or solely using local context, ignoring the immense historical human experience available on platforms like GitHub. Accessing this open-world experience is hindered by the unstructured and fragmented nature of real-world issue-tracking data. In this paper, we introduce MemGovern, a framework designed to govern and transform raw GitHub data into actionable experiential memory for agents. MemGovern employs experience governance to convert human experience into agent-friendly experience cards and introduces an agentic experience search strategy that enables logic-driven retrieval of human expertise. By producing 135K governed experience cards, MemGovern achieves a significant performance boost, improving resolution rates on the SWE-bench Verified by 4.65%. As a plug-in approach, MemGovern provides a solution for agent-friendly memory infrastructure.",
    "translation": "标题：MemGovern：通过治理化人类经验学习增强代码智能体\n\n摘要：尽管自主软件工程智能体正在重塑编程范式，但其当前存在“封闭世界”局限：它们倾向于从零开始或仅依赖局部上下文修复缺陷，忽视了GitHub等平台上可获取的丰富历史人类经验。现实世界中非结构化、碎片化的议题追踪数据阻碍了对这种开放世界经验的利用。本文提出MemGovern框架，该框架通过对原始GitHub数据进行治理与转化，将其构建为可供智能体操作的经验记忆库。MemGovern采用经验治理机制将人类经验转化为智能体可读的经验卡片，并引入智能体经验检索策略，实现基于逻辑驱动的人类专业知识检索。通过生成13.5万张治理化经验卡片，MemGovern在SWE-bench Verified基准测试中取得显著性能提升，问题解决率提高4.65%。作为插件式方案，MemGovern为构建智能体友好的记忆基础设施提供了有效解决方案。",
    "url": "https://huggingface.co/papers/2601.06789",
    "arxiv_url": "https://arxiv.org/abs/2601.06789"
  },
  {
    "title": "Solar Open Technical Report",
    "summary": "We introduce Solar Open, a 102B-parameter bilingual Mixture-of-Experts language model for underserved languages. Solar Open demonstrates a systematic methodology for building competitive LLMs by addressing three interconnected challenges. First, to train effectively despite data scarcity for underserved languages, we synthesize 4.5T tokens of high-quality, domain-specific, and RL-oriented data. Second, we coordinate this data through a progressive curriculum jointly optimizing composition, quality thresholds, and domain coverage across 20 trillion tokens. Third, to enable reasoning capabilities through scalable RL, we apply our proposed framework SnapPO for efficient optimization. Across benchmarks in English and Korean, Solar Open achieves competitive performance, demonstrating the effectiveness of this methodology for underserved language AI development.",
    "translation": "标题：Solar Open技术报告\n\n摘要：本文介绍Solar Open——一个针对资源稀缺语言开发的1020亿参数双语专家混合语言模型。该模型通过系统化方法应对三大相互关联的挑战，构建出具有竞争力的语言模型。首先，针对资源稀缺语言训练数据不足的问题，我们合成了4.5万亿个高质量、领域特定且强化学习导向的文本单元。其次，我们通过渐进式课程学习框架协调这些数据，在20万亿文本单元的规模上同步优化数据构成、质量阈值和领域覆盖度。第三，为构建可扩展的推理能力，我们采用自主研发的SnapPO框架进行高效强化学习优化。在英语和韩语的基准测试中，Solar Open展现出卓越性能，验证了该方法在资源稀缺语言人工智能开发领域的有效性。",
    "url": "https://huggingface.co/papers/2601.07022",
    "arxiv_url": "https://arxiv.org/abs/2601.07022"
  },
  {
    "title": "KnowMe-Bench: Benchmarking Person Understanding for Lifelong Digital Companions",
    "summary": "Existing long-horizon memory benchmarks mostly use multi-turn dialogues or synthetic user histories, which makes retrieval performance an imperfect proxy for person understanding. We present \\BenchName, a publicly releasable benchmark built from long-form autobiographical narratives, where actions, context, and inner thoughts provide dense evidence for inferring stable motivations and decision principles. \\BenchName~reconstructs each narrative into a flashback-aware, time-anchored stream and evaluates models with evidence-linked questions spanning factual recall, subjective state attribution, and principle-level reasoning. Across diverse narrative sources, retrieval-augmented systems mainly improve factual accuracy, while errors persist on temporally grounded explanations and higher-level inferences, highlighting the need for memory mechanisms beyond retrieval. Our data is in KnowMeBench{https://github.com/QuantaAlpha/KnowMeBench}.",
    "translation": "标题：KnowMe-Bench：面向终身数字伴侣的人物理解基准测试\n\n摘要：现有的长时记忆基准测试大多采用多轮对话或合成用户历史数据，这使得检索性能难以准确反映模型对人物的理解能力。本文提出 \\BenchName，这是一个基于长篇自传式叙事构建的可公开获取的基准测试，其中行动、情境与内心活动为推断稳定的动机与决策原则提供了密集的证据支撑。\\BenchName 将每段叙事重构为具有回溯感知能力且时间锚定的序列，并通过证据关联性问题对模型进行评估，问题涵盖事实回忆、主观状态归因及原则层面推理等多个维度。在不同叙事来源的测试中，检索增强系统主要提升了事实准确性，但在基于时间线索的解释及更高层次的推理任务上仍存在错误，这表明需要发展超越检索的记忆机制。相关数据已发布于 KnowMeBench{https://github.com/QuantaAlpha/KnowMeBench}。",
    "url": "https://huggingface.co/papers/2601.04745",
    "arxiv_url": "https://arxiv.org/abs/2601.04745"
  },
  {
    "title": "User-Oriented Multi-Turn Dialogue Generation with Tool Use at scale",
    "summary": "The recent paradigm shift toward large reasoning models (LRMs) as autonomous agents has intensified the demand for sophisticated, multi-turn tool-use capabilities. Yet, existing datasets and data-generation approaches are limited by static, predefined toolsets that cannot scale to the complexity of open-ended human-agent collaboration. To address this, we initially developed a framework for automated task-oriented multi-turn dialogue generation at scale, utilizing an LRM-based simulator to dynamically generate high-value, domain-specific tools to solve specified tasks. However, we observe that a purely task-oriented design often results in \"solely task-solving\" trajectories, where the agent completes the objective with minimal interaction, failing to generate the high turn-count conversations seen in realistic scenarios. To bridge this gap, we shift toward a user-oriented simulation paradigm. By decoupling task generation from a dedicated user simulator that mimics human behavioral rules - such as incremental request-making and turn-by-turn feedback - we facilitate more authentic, extended multi-turn dialogues that reflect the iterative nature of real-world problem solving. Our generation pipeline operates as a versatile, plug-and-play module capable of initiating generation from any state, ensuring high scalability in producing extended tool-use data. Furthermore, by facilitating multiple task completions within a single trajectory, it yields a high-density dataset that reflects the multifaceted demands of real-world human-agent interaction.",
    "translation": "标题：面向用户的大规模工具使用多轮对话生成\n\n摘要：近期向大型推理模型作为自主智能体的范式转变，显著提升了对复杂多轮工具使用能力的需求。然而，现有数据集与数据生成方法受限于静态预定义工具集，难以适应开放域人机协作的复杂性。为此，我们首先开发了一个面向任务的大规模自动化多轮对话生成框架，该框架利用基于大型推理模型的模拟器动态生成高价值、领域特定的工具以完成指定任务。但我们发现，纯任务导向的设计常导致“单一任务解决”轨迹，即智能体以最小交互完成目标，无法生成现实场景中常见的高轮次对话。为弥合这一差距，我们转向面向用户的模拟范式。通过将任务生成与模拟人类行为规则（如渐进式请求和逐轮反馈）的专用用户模拟器解耦，我们促成了更真实、更延展的多轮对话，体现了现实问题解决的迭代特性。我们的生成流程作为一个多功能即插即用模块，能够从任意状态启动生成，确保在扩展工具使用数据生产方面的高可扩展性。此外，通过支持在单一路径中完成多项任务，该方法可产出高密度数据集，充分反映现实人机交互的多维度需求。",
    "url": "https://huggingface.co/papers/2601.08225",
    "arxiv_url": "https://arxiv.org/abs/2601.08225"
  },
  {
    "title": "ShowUI-π: Flow-based Generative Models as GUI Dexterous Hands",
    "summary": "Building intelligent agents capable of dexterous manipulation is essential for achieving human-like automation in both robotics and digital environments. However, existing GUI agents rely on discrete click predictions (x,y), which prohibits free-form, closed-loop trajectories (e.g. dragging a progress bar) that require continuous, on-the-fly perception and adjustment. In this work, we develop ShowUI-π, the first flow-based generative model as GUI dexterous hand, featuring the following designs: (i) Unified Discrete-Continuous Actions, integrating discrete clicks and continuous drags within a shared model, enabling flexible adaptation across diverse interaction modes; (ii) Flow-based Action Generation for drag modeling, which predicts incremental cursor adjustments from continuous visual observations via a lightweight action expert, ensuring smooth and stable trajectories; (iii) Drag Training data and Benchmark, where we manually collect and synthesize 20K drag trajectories across five domains (e.g. PowerPoint, Adobe Premiere Pro), and introduce ScreenDrag, a benchmark with comprehensive online and offline evaluation protocols for assessing GUI agents' drag capabilities. Our experiments show that proprietary GUI agents still struggle on ScreenDrag (e.g. Operator scores 13.27, and the best Gemini-2.5-CUA reaches 22.18). In contrast, ShowUI-π achieves 26.98 with only 450M parameters, underscoring both the difficulty of the task and the effectiveness of our approach. We hope this work advances GUI agents toward human-like dexterous control in digital world. The code is available at https://github.com/showlab/showui-pi.",
    "translation": "标题：ShowUI-π：基于流的生成模型作为图形用户界面灵巧操作手\n\n摘要：构建能够进行灵巧操作的智能体，对于在机器人学和数字环境中实现类人自动化至关重要。然而，现有的图形用户界面（GUI）智能体依赖于离散的点击预测（x, y坐标），这限制了需要连续、实时感知与调整的自由形式、闭环轨迹操作（例如拖动进度条）。在本研究中，我们开发了ShowUI-π，首个作为GUI灵巧操作手的基于流的生成模型，其设计具有以下特点：（i）统一离散-连续动作，将离散点击和连续拖动整合在一个共享模型中，实现了跨多种交互模式的灵活适应；（ii）用于拖动建模的基于流的动作生成，通过一个轻量级动作专家，根据连续的视觉观察预测光标增量调整，确保轨迹平滑稳定；（iii）拖动训练数据与基准测试，我们手动收集并合成了涵盖五个领域（如PowerPoint、Adobe Premiere Pro）的2万条拖动轨迹，并引入了ScreenDrag基准测试，该基准包含全面的在线和离线评估协议，用于评估GUI智能体的拖动能力。实验表明，现有的专有GUI智能体在ScreenDrag上仍表现不佳（例如Operator得分为13.27，最佳模型Gemini-2.5-CUA达到22.18）。相比之下，ShowUI-π仅以4.5亿参数便取得了26.98的得分，既凸显了该任务的难度，也证明了我们方法的有效性。我们希望这项工作能推动GUI智能体在数字世界中实现类人的灵巧控制。代码可在 https://github.com/showlab/showui-pi 获取。",
    "url": "https://huggingface.co/papers/2512.24965",
    "arxiv_url": "https://arxiv.org/abs/2512.24965"
  },
  {
    "title": "ArenaRL: Scaling RL for Open-Ended Agents via Tournament-based Relative Ranking",
    "summary": "Reinforcement learning has substantially improved the performance of LLM agents on tasks with verifiable outcomes, but it still struggles on open-ended agent tasks with vast solution spaces (e.g., complex travel planning). Due to the absence of objective ground-truth for these tasks, current RL algorithms largely rely on reward models that assign scalar scores to individual responses. We contend that such pointwise scoring suffers from an inherent discrimination collapse: the reward model struggles to distinguish subtle advantages among different trajectories, resulting in scores within a group being compressed into a narrow range. Consequently, the effective reward signal becomes dominated by noise from the reward model, leading to optimization stagnation. To address this, we propose ArenaRL, a reinforcement learning paradigm that shifts from pointwise scalar scoring to intra-group relative ranking. ArenaRL introduces a process-aware pairwise evaluation mechanism, employing multi-level rubrics to assign fine-grained relative scores to trajectories. Additionally, we construct an intra-group adversarial arena and devise a tournament-based ranking scheme to obtain stable advantage signals. Empirical results confirm that the built seeded single-elimination scheme achieves nearly equivalent advantage estimation accuracy to full pairwise comparisons with O(N^2) complexity, while operating with only O(N) complexity, striking an optimal balance between efficiency and precision. Furthermore, to address the lack of full-cycle benchmarks for open-ended agents, we build Open-Travel and Open-DeepResearch, two high-quality benchmarks featuring a comprehensive pipeline covering SFT, RL training, and multi-dimensional evaluation. Extensive experiments show that ArenaRL substantially outperforms standard RL baselines, enabling LLM agents to generate more robust solutions for complex real-world tasks.",
    "translation": "标题：ArenaRL：基于锦标赛相对排序的开放智能体强化学习规模化方法\n\n摘要：强化学习在可验证结果的任务上显著提升了大型语言模型智能体的性能，但在解决方案空间广阔的开放智能体任务（如复杂旅行规划）中仍面临挑战。由于此类任务缺乏客观真实标准，现有强化学习算法主要依赖为单个响应分配标量分数的奖励模型。我们认为这种逐点评分存在固有的判别力坍缩问题：奖励模型难以区分不同轨迹间的细微优势，导致组内分数被压缩至狭窄区间。因此，有效奖励信号被奖励模型的噪声主导，引发优化停滞。为解决该问题，我们提出ArenaRL强化学习范式，将逐点标量评分转变为组内相对排序。ArenaRL引入过程感知的成对评估机制，采用多级评价准则为轨迹分配细粒度相对分数。此外，我们构建组内对抗竞技场，设计基于锦标赛的排序方案以获取稳定的优势信号。实验结果表明，所构建的种子单败淘汰制方案在仅需O(N)复杂度的情况下，其优势估计精度与O(N²)复杂度的全成对比较近乎相当，实现了效率与精度的最优平衡。针对开放智能体缺乏全周期基准测试的问题，我们构建了Open-Travel与Open-DeepResearch两个高质量基准测试集，其完整流程覆盖监督微调、强化训练与多维度评估。大量实验表明，ArenaRL显著优于标准强化学习基线，使大型语言模型智能体能为复杂现实任务生成更具鲁棒性的解决方案。",
    "url": "https://huggingface.co/papers/2601.06487",
    "arxiv_url": "https://arxiv.org/abs/2601.06487"
  },
  {
    "title": "MemoBrain: Executive Memory as an Agentic Brain for Reasoning",
    "summary": "Complex reasoning in tool-augmented agent frameworks is inherently long-horizon, causing reasoning traces and transient tool artifacts to accumulate and strain the bounded working context of large language models. Without explicit memory mechanisms, such accumulation disrupts logical continuity and undermines task alignment. This positions memory not as an auxiliary efficiency concern, but as a core component for sustaining coherent, goal-directed reasoning over long horizons.\n  We propose MemoBrain, an executive memory model for tool-augmented agents that constructs a dependency-aware memory over reasoning steps, capturing salient intermediate states and their logical relations. Operating as a co-pilot alongside the reasoning agent, MemoBrain organizes reasoning progress without blocking execution and actively manages the working context. Specifically, it prunes invalid steps, folds completed sub-trajectories, and preserves a compact, high-salience reasoning backbone under a fixed context budget. Together, these mechanisms enable explicit cognitive control over reasoning trajectories rather than passive context accumulation.\n  We evaluate MemoBrain on challenging long-horizon benchmarks, including GAIA, WebWalker, and BrowseComp-Plus, demonstrating consistent improvements over strong baselines.",
    "translation": "标题：MemoBrain：作为推理智能体执行核心的记忆大脑\n\n摘要：在工具增强型智能体框架中，复杂推理本质上是长程的，这导致推理轨迹和临时工具产物不断积累，从而挤占大型语言模型有限的工作上下文容量。缺乏显式记忆机制时，此类积累会破坏逻辑连续性并削弱任务对齐能力。这使记忆不再仅是辅助性的效率问题，而成为维持长程连贯、目标导向推理的核心组件。\n\n我们提出MemoBrain，一种面向工具增强型智能体的执行记忆模型。该模型在推理步骤上构建依赖感知的记忆系统，捕获关键的中间状态及其逻辑关系。MemoBrain作为推理智能体的协同处理器运行，在不阻断执行流程的前提下组织推理进程，并主动管理工作上下文。具体而言，它能在固定上下文预算下修剪无效步骤、折叠已完成的子轨迹，并保留紧凑且高显著性的推理主干。这些机制共同实现了对推理轨迹的显式认知控制，而非被动的上下文堆积。\n\n我们在具有挑战性的长程基准测试（包括GAIA、WebWalker和BrowseComp-Plus）上评估MemoBrain，实验结果表明其相较强基线模型能取得持续的性能提升。",
    "url": "https://huggingface.co/papers/2601.08079",
    "arxiv_url": "https://arxiv.org/abs/2601.08079"
  },
  {
    "title": "Ministral 3",
    "summary": "We introduce the Ministral 3 series, a family of parameter-efficient dense language models designed for compute and memory constrained applications, available in three model sizes: 3B, 8B, and 14B parameters. For each model size, we release three variants: a pretrained base model for general-purpose use, an instruction finetuned, and a reasoning model for complex problem-solving. In addition, we present our recipe to derive the Ministral 3 models through Cascade Distillation, an iterative pruning and continued training with distillation technique. Each model comes with image understanding capabilities, all under the Apache 2.0 license.",
    "translation": "标题：Ministral 3系列模型\n\n摘要：本文介绍Ministral 3系列模型——一组专为计算和内存受限应用设计的参数高效稠密语言模型，提供三种参数量版本：30亿、80亿和140亿参数。针对每种参数量版本，我们发布三种变体：面向通用场景的预训练基础模型、经过指令微调的模型，以及适用于复杂问题求解的推理模型。此外，我们提出了通过级联蒸馏技术推导Ministral 3模型的方法论，该技术融合迭代剪枝与持续蒸馏训练。所有模型均具备图像理解能力，并以Apache 2.0许可证开源发布。",
    "url": "https://huggingface.co/papers/2601.08584",
    "arxiv_url": "https://arxiv.org/abs/2601.08584"
  },
  {
    "title": "3AM: Segment Anything with Geometric Consistency in Videos",
    "summary": "Video object segmentation methods like SAM2 achieve strong performance through memory-based architectures but struggle under large viewpoint changes due to reliance on appearance features. Traditional 3D instance segmentation methods address viewpoint consistency but require camera poses, depth maps, and expensive preprocessing. We introduce 3AM, a training-time enhancement that integrates 3D-aware features from MUSt3R into SAM2. Our lightweight Feature Merger fuses multi-level MUSt3R features that encode implicit geometric correspondence. Combined with SAM2's appearance features, the model achieves geometry-consistent recognition grounded in both spatial position and visual similarity. We propose a field-of-view aware sampling strategy ensuring frames observe spatially consistent object regions for reliable 3D correspondence learning. Critically, our method requires only RGB input at inference, with no camera poses or preprocessing. On challenging datasets with wide-baseline motion (ScanNet++, Replica), 3AM substantially outperforms SAM2 and extensions, achieving 90.6% IoU and 71.7% Positive IoU on ScanNet++'s Selected Subset, improving over state-of-the-art VOS methods by +15.9 and +30.4 points. Project page: https://jayisaking.github.io/3AM-Page/",
    "translation": "标题：3AM：视频中基于几何一致性的任意物体分割\n\n摘要：基于记忆架构的视频物体分割方法（如SAM2）虽能实现强劲性能，但在视角剧烈变化时，因过度依赖外观特征而表现受限。传统三维实例分割方法虽能保证视角一致性，却需依赖相机位姿、深度图及昂贵的预处理流程。本文提出3AM，一种训练时增强方法，将MUSt3R提取的三维感知特征集成至SAM2框架中。我们设计的轻量级特征融合器能整合MUSt3R的多层次特征——这些特征编码了隐式的几何对应关系。结合SAM2的外观特征，该模型实现了基于空间位置与视觉相似性的几何一致性识别。我们进一步提出视场感知采样策略，确保采样帧能观测到空间一致的目标区域，从而建立可靠的三维对应学习。关键的是，本方法在推理阶段仅需RGB输入，无需相机位姿或预处理。在具有宽基线运动的挑战性数据集（ScanNet++、Replica）上，3AM显著优于SAM2及其扩展方法：在ScanNet++精选子集上分别达到90.6%的交并比和71.7%的正交并比，较当前最优视频物体分割方法提升15.9和30.4个百分点。项目页面：https://jayisking.github.io/3AM-Page/",
    "url": "https://huggingface.co/papers/2601.08831",
    "arxiv_url": "https://arxiv.org/abs/2601.08831"
  },
  {
    "title": "Motion Attribution for Video Generation",
    "summary": "Despite the rapid progress of video generation models, the role of data in influencing motion is poorly understood. We present Motive (MOTIon attribution for Video gEneration), a motion-centric, gradient-based data attribution framework that scales to modern, large, high-quality video datasets and models. We use this to study which fine-tuning clips improve or degrade temporal dynamics. Motive isolates temporal dynamics from static appearance via motion-weighted loss masks, yielding efficient and scalable motion-specific influence computation. On text-to-video models, Motive identifies clips that strongly affect motion and guides data curation that improves temporal consistency and physical plausibility. With Motive-selected high-influence data, our method improves both motion smoothness and dynamic degree on VBench, achieving a 74.1% human preference win rate compared with the pretrained base model. To our knowledge, this is the first framework to attribute motion rather than visual appearance in video generative models and to use it to curate fine-tuning data.",
    "translation": "标题：视频生成中的运动归因\n\n摘要：尽管视频生成模型发展迅速，但数据对运动特性的影响机制尚不明确。本文提出Motive（视频生成运动归因框架），这是一种以运动为核心、基于梯度的数据归因框架，可适配现代大规模高质量视频数据集与模型。通过该框架，我们能够分析哪些微调片段会改善或损害时序动态特性。Motive通过运动加权损失掩码将时序动态与静态表观特征解耦，实现了高效可扩展的运动特异性影响计算。在文本到视频模型中，Motive能识别对运动特性具有显著影响的数据片段，并据此指导数据筛选工作，从而提升时序一致性与物理合理性。采用Motive筛选的高影响力数据进行训练后，我们的方法在VBench评测中同时提升了运动平滑度与动态幅度指标，相比预训练基础模型获得74.1%的人类偏好胜率。据我们所知，这是首个在视频生成模型中针对运动特性（而非视觉表观）进行归因的框架，并首次将其应用于微调数据筛选。",
    "url": "https://huggingface.co/papers/2601.08828",
    "arxiv_url": "https://arxiv.org/abs/2601.08828"
  },
  {
    "title": "The Confidence Dichotomy: Analyzing and Mitigating Miscalibration in Tool-Use Agents",
    "summary": "Autonomous agents based on large language models (LLMs) are rapidly evolving to handle multi-turn tasks, but ensuring their trustworthiness remains a critical challenge. A fundamental pillar of this trustworthiness is calibration, which refers to an agent's ability to express confidence that reliably reflects its actual performance. While calibration is well-established for static models, its dynamics in tool-integrated agentic workflows remain underexplored. In this work, we systematically investigate verbalized calibration in tool-use agents, revealing a fundamental confidence dichotomy driven by tool type. Specifically, our pilot study identifies that evidence tools (e.g., web search) systematically induce severe overconfidence due to inherent noise in retrieved information, while verification tools (e.g., code interpreters) can ground reasoning through deterministic feedback and mitigate miscalibration. To robustly improve calibration across tool types, we propose a reinforcement learning (RL) fine-tuning framework that jointly optimizes task accuracy and calibration, supported by a holistic benchmark of reward designs. We demonstrate that our trained agents not only achieve superior calibration but also exhibit robust generalization from local training environments to noisy web settings and to distinct domains such as mathematical reasoning. Our results highlight the necessity of domain-specific calibration strategies for tool-use agents. More broadly, this work establishes a foundation for building self-aware agents that can reliably communicate uncertainty in high-stakes, real-world deployments.",
    "translation": "标题：置信度二分法：工具使用智能体中的误校准分析与缓解\n\n摘要：基于大语言模型的自主智能体正快速发展以处理多轮任务，但确保其可信度仍是关键挑战。可信度的核心支柱之一是校准，即智能体表达与其实际性能可靠匹配的置信度的能力。尽管静态模型的校准研究已较为成熟，但在集成工具的动态智能体工作流中，其校准机制仍缺乏深入探索。本研究系统性地探究了工具使用智能体中的言语化校准现象，揭示了由工具类型驱动的根本性置信度二分法。具体而言，初步研究发现：由于检索信息固有的噪声，证据型工具（如网络搜索）会系统性地引发严重过度自信；而验证型工具（如代码解释器）可通过确定性反馈锚定推理过程，从而缓解误校准问题。为全面提升跨工具类型的校准能力，我们提出一种强化学习微调框架，该框架通过综合奖励设计基准，联合优化任务准确性与校准度。实验表明，经训练的智能体不仅实现了更优的校准性能，还能从局部训练环境稳健地泛化至嘈杂的网络环境以及数学推理等不同领域。我们的研究结果凸显了针对工具使用智能体开发领域特定校准策略的必要性。更广泛而言，这项工作为构建能在高风险现实场景中可靠传达不确定性的自感知智能体奠定了理论基础。",
    "url": "https://huggingface.co/papers/2601.07264",
    "arxiv_url": "https://arxiv.org/abs/2601.07264"
  },
  {
    "title": "Parallel Context-of-Experts Decoding for Retrieval Augmented Generation",
    "summary": "Retrieval Augmented Generation faces a trade-off: concatenating documents in a long prompt enables multi-document reasoning but creates prefill bottlenecks, while encoding document KV caches separately offers speed but breaks cross-document interaction. We propose Parallel Context-of-Experts Decoding (Pced), a training-free framework that shifts evidence aggregation from the attention mechanism to the decoding. Pced treats retrieved documents as isolated \"experts\", synchronizing their predictions via a novel retrieval-aware contrastive decoding rule that weighs expert logits against the model prior. This approach recovers cross-document reasoning capabilities without constructing a shared attention across documents.",
    "translation": "标题：面向检索增强生成的并行专家上下文解码方法\n\n摘要：检索增强生成面临一个权衡困境：将多篇文档拼接为长提示虽能实现跨文档推理，但会导致预填充阶段的瓶颈；而将文档键值缓存分别编码虽能提升速度，却会破坏文档间的交互关联。本文提出并行专家上下文解码框架，该免训练框架将证据聚合机制从注意力层转移至解码层。该方法将检索到的文档视为独立的“专家”，通过一种新颖的检索感知对比解码规则同步各专家的预测结果，该规则依据模型先验对专家对数概率进行加权处理。该方案无需构建跨文档共享注意力机制，即可恢复跨文档推理能力。",
    "url": "https://huggingface.co/papers/2601.08670",
    "arxiv_url": "https://arxiv.org/abs/2601.08670"
  },
  {
    "title": "ViDoRe V3: A Comprehensive Evaluation of Retrieval Augmented Generation in Complex Real-World Scenarios",
    "summary": "Retrieval-Augmented Generation (RAG) pipelines must address challenges beyond simple single-document retrieval, such as interpreting visual elements (tables, charts, images), synthesizing information across documents, and providing accurate source grounding. Existing benchmarks fail to capture this complexity, often focusing on textual data, single-document comprehension, or evaluating retrieval and generation in isolation. We introduce ViDoRe v3, a comprehensive multimodal RAG benchmark featuring multi-type queries over visually rich document corpora. It covers 10 datasets across diverse professional domains, comprising ~26,000 document pages paired with 3,099 human-verified queries, each available in 6 languages. Through 12,000 hours of human annotation effort, we provide high-quality annotations for retrieval relevance, bounding box localization, and verified reference answers. Our evaluation of state-of-the-art RAG pipelines reveals that visual retrievers outperform textual ones, late-interaction models and textual reranking substantially improve performance, and hybrid or purely visual contexts enhance answer generation quality. However, current models still struggle with non-textual elements, open-ended queries, and fine-grained visual grounding. To encourage progress in addressing these challenges, the benchmark is released under a commercially permissive license at https://hf.co/vidore.",
    "translation": "标题：ViDoRe V3：复杂现实场景中检索增强生成技术的综合评估\n\n摘要：检索增强生成（RAG）流程需应对超越简单单文档检索的多重挑战，例如解析视觉元素（表格、图表、图像）、跨文档信息综合以及提供精确的溯源依据。现有基准测试未能充分涵盖此类复杂性，往往仅关注文本数据、单文档理解或孤立评估检索与生成环节。本文提出ViDoRe v3——一个全面的多模态RAG基准测试体系，其特点在于针对视觉密集型文档集设计多类型查询任务。该基准涵盖10个不同专业领域的数据集，包含约26,000份文档页面与3,099条人工验证查询的配对数据，每条查询均支持6种语言版本。通过累计12,000小时的人工标注工作，我们为检索相关性、边界框定位及验证参考答案提供了高质量标注。对前沿RAG流程的评估表明：视觉检索器性能优于文本检索器，延迟交互模型与文本重排序技术能显著提升系统表现，混合型或纯视觉上下文可提高答案生成质量。然而，现有模型在处理非文本元素、开放式查询及细粒度视觉定位方面仍存在局限。为推动相关挑战的突破，本基准测试已通过商业友好许可发布于https://hf.co/vidore。",
    "url": "https://huggingface.co/papers/2601.08620",
    "arxiv_url": "https://arxiv.org/abs/2601.08620"
  },
  {
    "title": "SnapGen++: Unleashing Diffusion Transformers for Efficient High-Fidelity Image Generation on Edge Devices",
    "summary": "Recent advances in diffusion transformers (DiTs) have set new standards in image generation, yet remain impractical for on-device deployment due to their high computational and memory costs. In this work, we present an efficient DiT framework tailored for mobile and edge devices that achieves transformer-level generation quality under strict resource constraints. Our design combines three key components. First, we propose a compact DiT architecture with an adaptive global-local sparse attention mechanism that balances global context modeling and local detail preservation. Second, we propose an elastic training framework that jointly optimizes sub-DiTs of varying capacities within a unified supernetwork, allowing a single model to dynamically adjust for efficient inference across different hardware. Finally, we develop Knowledge-Guided Distribution Matching Distillation, a step-distillation pipeline that integrates the DMD objective with knowledge transfer from few-step teacher models, producing high-fidelity and low-latency generation (e.g., 4-step) suitable for real-time on-device use. Together, these contributions enable scalable, efficient, and high-quality diffusion models for deployment on diverse hardware.",
    "translation": "标题：SnapGen++：释放扩散变换器在边缘设备上实现高效高保真图像生成的潜力\n\n摘要：扩散变换器（DiTs）的最新进展为图像生成树立了新标杆，但由于其高昂的计算与内存成本，仍难以在实际设备上部署。本研究提出一种专为移动及边缘设备设计的高效DiT框架，可在严格资源限制下实现变换器级别的生成质量。我们的设计融合了三个关键组成部分：首先，我们提出一种紧凑的DiT架构，采用自适应全局-局部稀疏注意力机制，以平衡全局上下文建模与局部细节保留；其次，我们设计了一种弹性训练框架，可在统一超网络内联合优化不同容量的子DiT模型，使单一模型能够动态调整以适应不同硬件的高效推理需求；最后，我们开发了知识引导分布匹配蒸馏技术，该分步蒸馏流程将DMD目标与少步数教师模型的知识迁移相结合，生成适用于设备端实时应用的高保真、低延迟图像（例如仅需4步生成）。这些贡献共同构建了可扩展、高效且高质量的扩散模型，为多样化硬件部署提供了可行方案。",
    "url": "https://huggingface.co/papers/2601.08303",
    "arxiv_url": "https://arxiv.org/abs/2601.08303"
  },
  {
    "title": "VLingNav: Embodied Navigation with Adaptive Reasoning and Visual-Assisted Linguistic Memory",
    "summary": "VLA models have shown promising potential in embodied navigation by unifying perception and planning while inheriting the strong generalization abilities of large VLMs. However, most existing VLA models rely on reactive mappings directly from observations to actions, lacking the explicit reasoning capabilities and persistent memory required for complex, long-horizon navigation tasks. To address these challenges, we propose VLingNav, a VLA model for embodied navigation grounded in linguistic-driven cognition. First, inspired by the dual-process theory of human cognition, we introduce an adaptive chain-of-thought mechanism, which dynamically triggers explicit reasoning only when necessary, enabling the agent to fluidly switch between fast, intuitive execution and slow, deliberate planning. Second, to handle long-horizon spatial dependencies, we develop a visual-assisted linguistic memory module that constructs a persistent, cross-modal semantic memory, enabling the agent to recall past observations to prevent repetitive exploration and infer movement trends for dynamic environments. For the training recipe, we construct Nav-AdaCoT-2.9M, the largest embodied navigation dataset with reasoning annotations to date, enriched with adaptive CoT annotations that induce a reasoning paradigm capable of adjusting both when to think and what to think about. Moreover, we incorporate an online expert-guided reinforcement learning stage, enabling the model to surpass pure imitation learning and to acquire more robust, self-explored navigation behaviors. Extensive experiments demonstrate that VLingNav achieves state-of-the-art performance across a wide range of embodied navigation benchmarks. Notably, VLingNav transfers to real-world robotic platforms in a zero-shot manner, executing various navigation tasks and demonstrating strong cross-domain and cross-task generalization.",
    "translation": "标题：VLingNav：基于自适应推理与视觉辅助语言记忆的具身导航\n\n摘要：视觉语言动作模型通过统一感知与规划，并继承大型视觉语言模型的强大泛化能力，在具身导航任务中展现出巨大潜力。然而，现有视觉语言动作模型大多依赖从观测到动作的被动映射，缺乏处理复杂长程导航任务所需的显式推理能力与持久记忆机制。为应对这些挑战，我们提出VLingNav——一种基于语言驱动认知的具身导航视觉语言动作模型。首先，受人类认知双过程理论启发，我们引入自适应思维链机制，该机制仅在必要时动态触发显式推理，使智能体能够在快速直觉执行与慢速审慎规划之间灵活切换。其次，为处理长程空间依赖关系，我们开发了视觉辅助语言记忆模块，构建跨模态持久语义记忆，使智能体能够回溯历史观测以避免重复探索，并推断动态环境中的运动趋势。在训练方案上，我们构建了Nav-AdaCoT-2.9M——迄今为止规模最大的具身导航推理标注数据集，其中包含可引导模型自主调整“何时思考”与“思考内容”的自适应思维链标注。此外，我们引入在线专家引导强化学习阶段，使模型能够超越纯模仿学习，获得更鲁棒、更具自主探索能力的导航行为。大量实验表明，VLingNav在多种具身导航基准测试中均达到最先进性能。值得注意的是，VLingNav能够以零样本方式迁移至真实机器人平台，执行多样化导航任务，展现出强大的跨领域与跨任务泛化能力。",
    "url": "https://huggingface.co/papers/2601.08665",
    "arxiv_url": "https://arxiv.org/abs/2601.08665"
  },
  {
    "title": "End-to-End Video Character Replacement without Structural Guidance",
    "summary": "Controllable video character replacement with a user-provided identity remains a challenging problem due to the lack of paired video data. Prior works have predominantly relied on a reconstruction-based paradigm that requires per-frame segmentation masks and explicit structural guidance (e.g., skeleton, depth). This reliance, however, severely limits their generalizability in complex scenarios involving occlusions, character-object interactions, unusual poses, or challenging illumination, often leading to visual artifacts and temporal inconsistencies. In this paper, we propose MoCha, a pioneering framework that bypasses these limitations by requiring only a single arbitrary frame mask. To effectively adapt the multi-modal input condition and enhance facial identity, we introduce a condition-aware RoPE and employ an RL-based post-training stage. Furthermore, to overcome the scarcity of qualified paired-training data, we propose a comprehensive data construction pipeline. Specifically, we design three specialized datasets: a high-fidelity rendered dataset built with Unreal Engine 5 (UE5), an expression-driven dataset synthesized by current portrait animation techniques, and an augmented dataset derived from existing video-mask pairs. Extensive experiments demonstrate that our method substantially outperforms existing state-of-the-art approaches. We will release the code to facilitate further research. Please refer to our project page for more details: orange-3dv-team.github.io/MoCha",
    "translation": "标题：无需结构引导的端到端视频角色替换\n\n摘要：由于缺乏成对的视频数据，基于用户提供身份信息的可控视频角色替换仍是一个具有挑战性的问题。现有研究主要依赖于基于重建的范式，该方法需要逐帧分割掩码和明确的结构引导（如骨骼、深度信息）。然而，这种依赖性严重限制了其在复杂场景中的泛化能力，例如存在遮挡、角色-物体交互、非常规姿态或复杂光照的情况，往往导致视觉伪影和时间不一致性。本文提出MoCha框架，该开创性方法仅需单帧任意掩码即可突破上述限制。为有效适配多模态输入条件并增强面部身份特征，我们引入了条件感知的RoPE机制，并采用基于强化学习的后训练阶段。此外，为克服高质量配对训练数据的稀缺问题，我们提出了一套完整的数据构建流程。具体而言，我们设计了三个专用数据集：基于虚幻引擎5构建的高保真渲染数据集、通过当前肖像动画技术合成的表情驱动数据集，以及从现有视频-掩码对衍生的增强数据集。大量实验表明，我们的方法显著优于现有最先进技术。我们将公开代码以促进后续研究。更多细节请访问项目页面：orange-3dv-team.github.io/MoCha",
    "url": "https://huggingface.co/papers/2601.08587",
    "arxiv_url": "https://arxiv.org/abs/2601.08587"
  },
  {
    "title": "VideoLoom: A Video Large Language Model for Joint Spatial-Temporal Understanding",
    "summary": "This paper presents VideoLoom, a unified Video Large Language Model (Video LLM) for joint spatial-temporal understanding. To facilitate the development of fine-grained spatial and temporal localization capabilities, we curate LoomData-8.7k, a human-centric video dataset with temporally grounded and spatially localized captions. With this, VideoLoom achieves state-of-the-art or highly competitive performance across a variety of spatial and temporal benchmarks (e.g., 63.1 J&F on ReVOS for referring video object segmentation, and 48.3 R1@0.7 on Charades-STA for temporal grounding). In addition, we introduce LoomBench, a novel benchmark consisting of temporal, spatial, and compositional video-question pairs, enabling a comprehensive evaluation of Video LLMs from diverse aspects. Collectively, these contributions offer a universal and effective suite for joint spatial-temporal video understanding, setting a new standard in multimodal intelligence.",
    "translation": "标题：VideoLoom：一种用于联合时空理解的视频大语言模型\n\n摘要：本文提出VideoLoom，一种用于联合时空理解的统一视频大语言模型。为促进细粒度时空定位能力的发展，我们构建了LoomData-8.7k——一个以人为中心的视频数据集，包含时间锚定与空间定位的描述文本。基于此，VideoLoom在多种时空基准测试中取得了领先或极具竞争力的性能（例如，在指代视频目标分割任务ReVOS上达到63.1 J&F，在时序定位任务Charades-STA上达到48.3 R1@0.7）。此外，我们提出了LoomBench，这是一个由时序、空间及组合型视频-问题对构成的新基准，能够从多维度全面评估视频大语言模型。这些贡献共同为联合时空视频理解提供了一套通用而有效的工具集，为多模态智能领域设立了新标准。",
    "url": "https://huggingface.co/papers/2601.07290",
    "arxiv_url": "https://arxiv.org/abs/2601.07290"
  },
  {
    "title": "JudgeRLVR: Judge First, Generate Second for Efficient Reasoning",
    "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has become a standard paradigm for reasoning in Large Language Models. However, optimizing solely for final-answer correctness often drives models into aimless, verbose exploration, where they rely on exhaustive trial-and-error tactics rather than structured planning to reach solutions. While heuristic constraints like length penalties can reduce verbosity, they often truncate essential reasoning steps, creating a difficult trade-off between efficiency and verification. In this paper, we argue that discriminative capability is a prerequisite for efficient generation: by learning to distinguish valid solutions, a model can internalize a guidance signal that prunes the search space. We propose JudgeRLVR, a two-stage judge-then-generate paradigm. In the first stage, we train the model to judge solution responses with verifiable answers. In the second stage, we fine-tune the same model with vanilla generating RLVR initialized from the judge. Compared to Vanilla RLVR using the same math-domain training data, JudgeRLVR achieves a better quality--efficiency trade-off for Qwen3-30B-A3B: on in-domain math, it delivers about +3.7 points average accuracy gain with -42\\% average generation length; on out-of-domain benchmarks, it delivers about +4.5 points average accuracy improvement, demonstrating enhanced generalization.",
    "translation": "标题：JudgeRLVR：先判别后生成的高效推理方法\n\n摘要：基于可验证奖励的强化学习已成为大语言模型推理任务的标准范式。然而，仅针对最终答案正确性进行优化常导致模型陷入盲目、冗长的探索，使其依赖穷举试错策略而非结构化规划来求解。虽然长度惩罚等启发式约束可降低冗余性，但往往截断关键推理步骤，造成效率与验证准确性之间的两难权衡。本文提出判别能力是高效生成的前提：通过学习区分有效解，模型可内化一种能剪枝搜索空间的引导信号。我们提出JudgeRLVR——一种“先判别后生成”的两阶段范式。第一阶段训练模型对含可验证答案的解题响应进行判别；第二阶段以判别模型为初始化基础，通过标准生成式RLVR对同一模型进行微调。在使用相同数学领域训练数据的情况下，相较于传统RLVR，JudgeRLVR为Qwen3-30B-A3B模型实现了更优的质量-效率平衡：在领域内数学任务中，平均准确率提升约3.7分的同时平均生成长度减少42%；在领域外基准测试中，平均准确率提升约4.5分，展现出更强的泛化能力。",
    "url": "https://huggingface.co/papers/2601.08468",
    "arxiv_url": "https://arxiv.org/abs/2601.08468"
  },
  {
    "title": "EpiCaR: Knowing What You Don't Know Matters for Better Reasoning in LLMs",
    "summary": "Improving the reasoning abilities of large language models (LLMs) has largely relied on iterative self-training with model-generated data. While effective at boosting accuracy, existing approaches primarily reinforce successful reasoning paths, incurring a substantial calibration cost: models become overconfident and lose the ability to represent uncertainty. This failure has been characterized as a form of model collapse in alignment, where predictive distributions degenerate toward low-variance point estimates. We address this issue by reframing reasoning training as an epistemic learning problem, in which models must learn not only how to reason, but also when their reasoning should be trusted. We propose epistemically-calibrated reasoning (EpiCaR) as a training objective that jointly optimizes reasoning performance and calibration, and instantiate it within an iterative supervised fine-tuning framework using explicit self-evaluation signals. Experiments on Llama-3 and Qwen-3 families demonstrate that our approach achieves Pareto-superiority over standard baselines in both accuracy and calibration, particularly in models with sufficient reasoning capacity (e.g., 3B+). This framework generalizes effectively to OOD mathematical reasoning (GSM8K) and code generation (MBPP). Ultimately, our approach enables a 3X reduction in inference compute, matching the K=30 performance of STaR with only K=10 samples in capable models.",
    "translation": "标题：EpiCaR：认知未知对提升大语言模型推理能力的重要性\n\n摘要：提升大语言模型（LLMs）的推理能力主要依赖于基于模型生成数据的迭代自训练。现有方法虽能有效提高准确性，但主要强化了成功的推理路径，导致显著的校准代价：模型变得过度自信并丧失表征不确定性的能力。这种失败被描述为对齐过程中的一种模型崩溃形式，即预测分布退化为低方差的点估计。我们通过将推理训练重新定义为认知学习问题来解决此问题，在该框架中，模型不仅需要学习如何推理，还需学会何时应信任其推理过程。我们提出认知校准推理（EpiCaR）作为联合优化推理性能与校准的训练目标，并利用显式自评估信号在迭代监督微调框架中实现该方法。基于Llama-3和Qwen-3系列模型的实验表明，我们的方法在准确性与校准性上均对标准基线实现帕累托优势，尤其在具备充分推理能力的模型（如3B+参数规模）中表现显著。该框架能有效泛化至分布外数学推理（GSM8K）与代码生成（MBPP）任务。最终，我们的方法在具备足够能力的模型中仅需K=10个样本即可匹配STaR方法K=30样本的性能，实现了推理计算量3倍的降低。",
    "url": "https://huggingface.co/papers/2601.06786",
    "arxiv_url": "https://arxiv.org/abs/2601.06786"
  },
  {
    "title": "UM-Text: A Unified Multimodal Model for Image Understanding",
    "summary": "With the rapid advancement of image generation, visual text editing using natural language instructions has received increasing attention. The main challenge of this task is to fully understand the instruction and reference image, and thus generate visual text that is style-consistent with the image. Previous methods often involve complex steps of specifying the text content and attributes, such as font size, color, and layout, without considering the stylistic consistency with the reference image. To address this, we propose UM-Text, a unified multimodal model for context understanding and visual text editing by natural language instructions. Specifically, we introduce a Visual Language Model (VLM) to process the instruction and reference image, so that the text content and layout can be elaborately designed according to the context information. To generate an accurate and harmonious visual text image, we further propose the UM-Encoder to combine the embeddings of various condition information, where the combination is automatically configured by VLM according to the input instruction. During training, we propose a regional consistency loss to offer more effective supervision for glyph generation on both latent and RGB space, and design a tailored three-stage training strategy to further enhance model performance. In addition, we contribute the UM-DATA-200K, a large-scale visual text image dataset on diverse scenes for model training. Extensive qualitative and quantitative results on multiple public benchmarks demonstrate that our method achieves state-of-the-art performance.",
    "translation": "标题：UM-Text：面向图像理解与视觉文字编辑的统一多模态模型\n\n摘要：随着图像生成技术的快速发展，基于自然语言指令的视觉文字编辑日益受到关注。该任务的核心挑战在于充分理解指令与参考图像，从而生成与图像风格协调的视觉文字。现有方法通常需要分别指定文字内容及字体大小、颜色、布局等属性，且未充分考虑生成结果与参考图像的整体风格一致性。为此，本文提出UM-Text——一个通过自然语言指令实现场景理解与视觉文字编辑的统一多模态模型。具体而言，我们引入视觉语言模型解析指令与参考图像，使文字内容与布局能依据上下文信息进行精细化设计。为生成准确且和谐的视觉文字图像，我们进一步提出UM-Encoder以融合多维度条件信息的嵌入表示，其融合方式由视觉语言模型根据输入指令自动配置。在训练阶段，我们提出区域一致性损失函数，在潜在空间与RGB空间为字形生成提供更有效的监督信号，并设计定制化的三阶段训练策略以提升模型性能。此外，我们构建了包含20万张多场景视觉文字图像的大规模数据集UM-DATA-200K用于模型训练。在多个公开基准测试上的定性与定量实验结果表明，本方法取得了当前最优性能。",
    "url": "https://huggingface.co/papers/2601.08321",
    "arxiv_url": "https://arxiv.org/abs/2601.08321"
  },
  {
    "title": "The Agent's First Day: Benchmarking Learning, Exploration, and Scheduling in the Workplace Scenarios",
    "summary": "The rapid evolution of Multi-modal Large Language Models (MLLMs) has advanced workflow automation; however, existing research mainly targets performance upper bounds in static environments, overlooking robustness for stochastic real-world deployment. We identify three key challenges: dynamic task scheduling, active exploration under uncertainty, and continuous learning from experience. To bridge this gap, we introduce , a dynamic evaluation environment that simulates a \"trainee\" agent continuously exploring a novel setting. Unlike traditional benchmarks,  evaluates agents along three dimensions: (1) context-aware scheduling for streaming tasks with varying priorities; (2) prudent information acquisition to reduce hallucination via active exploration; and (3) continuous evolution by distilling generalized strategies from rule-based, dynamically generated tasks. Experiments show that cutting-edge agents have significant deficiencies in dynamic environments, especially in active exploration and continual learning. Our work establishes a framework for assessing agent reliability, shifting evaluation from static tests to realistic, production-oriented scenarios. Our codes are available at https://github.com/KnowledgeXLab/EvoEnv",
    "translation": "标题：智能体首日工作：职场场景中的学习、探索与调度能力基准测试\n\n摘要：多模态大语言模型的快速发展推动了工作流程自动化进程；然而现有研究主要关注静态环境下的性能上限，忽视了随机现实部署场景中的鲁棒性需求。我们识别出三大核心挑战：动态任务调度、不确定性下的主动探索以及基于经验的持续学习。为弥补这一研究空白，我们提出动态评估环境，该环境通过模拟\"实习生\"智能体持续探索全新工作场景来构建。与传统基准测试不同，本框架从三个维度评估智能体：(1) 针对不同优先级流式任务的上下文感知调度能力；(2) 通过主动探索进行审慎信息获取以减少幻觉现象；(3) 从基于规则动态生成的任务中提炼泛化策略以实现持续进化。实验表明，前沿智能体在动态环境中存在显著缺陷，尤其在主动探索与持续学习方面。本研究建立了评估智能体可靠性的框架，将评估重点从静态测试转向现实生产导向场景。代码已开源：https://github.com/KnowledgeXLab/EvoEnv",
    "url": "https://huggingface.co/papers/2601.08173",
    "arxiv_url": "https://arxiv.org/abs/2601.08173"
  },
  {
    "title": "Aligning Text, Code, and Vision: A Multi-Objective Reinforcement Learning Framework for Text-to-Visualization",
    "summary": "Text-to-Visualization (Text2Vis) systems translate natural language queries over tabular data into concise answers and executable visualizations. While closed-source LLMs generate functional code, the resulting charts often lack semantic alignment and clarity, qualities that can only be assessed post-execution. Open-source models struggle even more, frequently producing non-executable or visually poor outputs. Although supervised fine-tuning can improve code executability, it fails to enhance overall visualization quality, as traditional SFT loss cannot capture post-execution feedback. To address this gap, we propose RL-Text2Vis, the first reinforcement learning framework for Text2Vis generation. Built on Group Relative Policy Optimization (GRPO), our method uses a novel multi-objective reward that jointly optimizes textual accuracy, code validity, and visualization quality using post-execution feedback. By training Qwen2.5 models (7B and 14B), RL-Text2Vis achieves a 22% relative improvement in chart quality over GPT-4o on the Text2Vis benchmark and boosts code execution success from 78% to 97% relative to its zero-shot baseline. Our models significantly outperform strong zero-shot and supervised baselines and also demonstrate robust generalization to out-of-domain datasets like VIS-Eval and NVBench. These results establish GRPO as an effective strategy for structured, multimodal reasoning in visualization generation. We release our code at https://github.com/vis-nlp/RL-Text2Vis.",
    "translation": "标题：对齐文本、代码与视觉：基于多目标强化学习的文本到可视化生成框架\n\n摘要：文本到可视化（Text2Vis）系统能够将针对表格数据的自然语言查询转化为简洁答案与可执行的可视化图表。尽管闭源大语言模型能够生成功能性代码，但其生成的图表常存在语义对齐不足与清晰度欠佳的问题，而这些质量指标仅能在执行后进行评估。开源模型的表现更为局限，常生成无法执行或视觉效果较差的输出。虽然监督微调能够提升代码可执行性，但由于传统监督微调损失函数无法捕捉执行后反馈，该方法难以全面提升可视化质量。为弥补这一缺陷，我们提出RL-Text2Vis——首个面向Text2Vis生成的强化学习框架。该方法基于分组相对策略优化（GRPO）构建，通过设计新型多目标奖励函数，利用执行后反馈联合优化文本准确性、代码有效性与可视化质量。通过对Qwen2.5模型（7B与14B参数规模）进行训练，RL-Text2Vis在Text2Vis基准测试中实现了相较于GPT-4o 22%的图表质量相对提升，并将代码执行成功率从零样本基线的78%提高至97%。我们的模型显著超越了强零样本与监督基线模型，并在VIS-Eval、NVBench等跨领域数据集上展现出优异的泛化能力。这些成果证实了GRPO在可视化生成这一结构化多模态推理任务中的有效性。代码已发布于https://github.com/vis-nlp/RL-Text2Vis。",
    "url": "https://huggingface.co/papers/2601.04582",
    "arxiv_url": "https://arxiv.org/abs/2601.04582"
  },
  {
    "title": "Towards Comprehensive Stage-wise Benchmarking of Large Language Models in Fact-Checking",
    "summary": "Large Language Models (LLMs) are increasingly deployed in real-world fact-checking systems, yet existing evaluations focus predominantly on claim verification and overlook the broader fact-checking workflow, including claim extraction and evidence retrieval. This narrow focus prevents current benchmarks from revealing systematic reasoning failures, factual blind spots, and robustness limitations of modern LLMs. To bridge this gap, we present FactArena, a fully automated arena-style evaluation framework that conducts comprehensive, stage-wise benchmarking of LLMs across the complete fact-checking pipeline. FactArena integrates three key components: (i) an LLM-driven fact-checking process that standardizes claim decomposition, evidence retrieval via tool-augmented interactions, and justification-based verdict prediction; (ii) an arena-styled judgment mechanism guided by consolidated reference guidelines to ensure unbiased and consistent pairwise comparisons across heterogeneous judge agents; and (iii) an arena-driven claim-evolution module that adaptively generates more challenging and semantically controlled claims to probe LLMs' factual robustness beyond fixed seed data. Across 16 state-of-the-art LLMs spanning seven model families, FactArena produces stable and interpretable rankings. Our analyses further reveal significant discrepancies between static claim-verification accuracy and end-to-end fact-checking competence, highlighting the necessity of holistic evaluation. The proposed framework offers a scalable and trustworthy paradigm for diagnosing LLMs' factual reasoning, guiding future model development, and advancing the reliable deployment of LLMs in safety-critical fact-checking applications.",
    "translation": "标题：迈向大型语言模型在事实核查中的全面分阶段基准测试\n\n摘要：大型语言模型正日益应用于现实世界的事实核查系统，但现有评估主要集中于主张验证环节，忽视了包括主张提取与证据检索在内的更广泛事实核查工作流程。这种局限使当前基准测试无法充分揭示现代大型语言模型的系统性推理缺陷、事实盲区及鲁棒性限制。为弥补这一空白，我们提出FactArena——一个全自动的竞技场式评估框架，该框架能针对完整事实核查流程对大型语言模型进行全面的分阶段基准测试。FactArena整合了三个核心组件：（一）由LLM驱动的事实核查流程，标准化主张解构、通过工具增强交互实现证据检索，以及基于论证的判定预测；（二）遵循统一参考准则的竞技场式评判机制，确保异构评判代理间进行无偏见且一致的成对比较；（三）竞技场驱动的主张演化模块，能自适应生成更具挑战性且语义受控的主张，以探测LLM在固定种子数据之外的事实鲁棒性。通过对涵盖七个模型系列的16个前沿大型语言模型的测试，FactArena产生了稳定且可解释的性能排序。我们的分析进一步揭示了静态主张验证准确率与端到端事实核查能力之间的显著差异，凸显了整体性评估的必要性。该框架为诊断大型语言模型的事实推理能力、指导未来模型发展，以及推动LLM在安全关键型事实核查应用中的可靠部署，提供了可扩展且可信赖的范式。",
    "url": "https://huggingface.co/papers/2601.02669",
    "arxiv_url": "https://arxiv.org/abs/2601.02669"
  },
  {
    "title": "GeoMotionGPT: Geometry-Aligned Motion Understanding with Large Language Models",
    "summary": "Discrete motion tokenization has recently enabled Large Language Models (LLMs) to serve as versatile backbones for motion understanding and motion-language reasoning. However, existing pipelines typically decouple motion quantization from semantic embedding learning, linking them solely via token IDs. This approach fails to effectively align the intrinsic geometry of the motion space with the embedding space, thereby hindering the LLM's capacity for nuanced motion reasoning. We argue that alignment is most effective when both modalities share a unified geometric basis. Therefore, instead of forcing the LLM to reconstruct the complex geometry among motion tokens from scratch, we present a novel framework that explicitly enforces orthogonality on both the motion codebook and the LLM embedding space, ensuring that their relational structures naturally mirror each other. Specifically, we employ a decoder-only quantizer with Gumbel-Softmax for differentiable training and balanced codebook usage. To bridge the modalities, we use a sparse projection that maps motion codes into the LLM embedding space while preserving orthogonality. Finally, a two-stage orthonormal regularization schedule enforces soft constraints during tokenizer training and LLM fine-tuning to maintain geometric alignment without hindering semantic adaptation. Extensive experiments on HumanML3D demonstrate that our framework achieves a 20% performance improvement over current state-of-the-art methods, validating that a unified geometric basis effectively empowers the LLM for nuanced motion reasoning.",
    "translation": "标题：GeoMotionGPT：基于大语言模型的几何对齐运动理解框架\n\n摘要：离散运动标记化技术近期使得大语言模型能够作为运动理解与运动-语言推理的多功能基础架构。然而，现有流程通常将运动量化与语义嵌入学习解耦，仅通过标记ID建立关联。这种方法未能有效对齐运动空间的内在几何结构与嵌入空间，从而限制了大语言模型进行精细运动推理的能力。我们认为，当两种模态共享统一的几何基础时，对齐效果最为显著。因此，我们提出了一种新颖框架，该框架不再强制大语言模型从零开始重构运动标记间的复杂几何关系，而是通过对运动码本和大语言模型嵌入空间同时施加正交性约束，确保二者的关系结构自然映射。具体而言，我们采用基于Gumbel-Softmax的仅解码器量化器实现可微分训练与均衡化码本使用；通过稀疏投影将运动编码映射至大语言模型嵌入空间并保持正交特性；最后设计两阶段正交正则化方案，在标记器训练与大语言模型微调过程中实施柔性约束，在维持几何对齐的同时不阻碍语义适应。在HumanML3D数据集上的大量实验表明，本框架相比当前最优方法实现20%的性能提升，验证了统一几何基础能有效增强大语言模型的精细运动推理能力。",
    "url": "https://huggingface.co/papers/2601.07632",
    "arxiv_url": "https://arxiv.org/abs/2601.07632"
  }
]