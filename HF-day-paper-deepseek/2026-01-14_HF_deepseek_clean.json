[
  {
    "title": "MemGovern: Enhancing Code Agents through Learning from Governed Human Experiences",
    "summary": "While autonomous software engineering (SWE) agents are reshaping programming paradigms, they currently suffer from a \"closed-world\" limitation: they attempt to fix bugs from scratch or solely using local context, ignoring the immense historical human experience available on platforms like GitHub. Accessing this open-world experience is hindered by the unstructured and fragmented nature of real-world issue-tracking data. In this paper, we introduce MemGovern, a framework designed to govern and transform raw GitHub data into actionable experiential memory for agents. MemGovern employs experience governance to convert human experience into agent-friendly experience cards and introduces an agentic experience search strategy that enables logic-driven retrieval of human expertise. By producing 135K governed experience cards, MemGovern achieves a significant performance boost, improving resolution rates on the SWE-bench Verified by 4.65%. As a plug-in approach, MemGovern provides a solution for agent-friendly memory infrastructure.",
    "translation": "标题：MemGovern：通过治理化人类经验学习增强代码智能体\n\n摘要：尽管自主软件工程智能体正在重塑编程范式，但其目前存在“封闭世界”局限：它们倾向于从零开始或仅依赖局部上下文修复缺陷，而忽略了GitHub等平台上可获取的丰富历史人类经验。现实世界中非结构化、碎片化的缺陷追踪数据阻碍了对这些开放世界经验的利用。本文提出MemGovern框架，旨在治理原始GitHub数据并将其转化为可供智能体使用的可执行经验记忆。MemGovern通过经验治理机制将人类经验转化为智能体友好的经验卡片，并引入智能体经验检索策略，实现基于逻辑驱动的人类专业知识检索。通过生成13.5万张治理化经验卡片，MemGovern在SWE-bench Verified基准上的缺陷解决率显著提升4.65%。作为一种插件式方案，MemGovern为构建智能体友好的记忆基础设施提供了有效解决方案。",
    "url": "https://huggingface.co/papers/2601.06789",
    "arxiv_url": "https://arxiv.org/abs/2601.06789"
  },
  {
    "title": "Solar Open Technical Report",
    "summary": "We introduce Solar Open, a 102B-parameter bilingual Mixture-of-Experts language model for underserved languages. Solar Open demonstrates a systematic methodology for building competitive LLMs by addressing three interconnected challenges. First, to train effectively despite data scarcity for underserved languages, we synthesize 4.5T tokens of high-quality, domain-specific, and RL-oriented data. Second, we coordinate this data through a progressive curriculum jointly optimizing composition, quality thresholds, and domain coverage across 20 trillion tokens. Third, to enable reasoning capabilities through scalable RL, we apply our proposed framework SnapPO for efficient optimization. Across benchmarks in English and Korean, Solar Open achieves competitive performance, demonstrating the effectiveness of this methodology for underserved language AI development.",
    "translation": "标题：Solar Open技术报告\n\n摘要：本文介绍Solar Open——一个针对资源稀缺语言开发的1020亿参数双语专家混合模型。我们通过解决三个相互关联的挑战，系统性地构建了具有竞争力的语言模型。首先，针对资源稀缺语言训练数据不足的问题，我们合成了4.5万亿个高质量、领域特定且强化学习导向的文本单元。其次，我们通过渐进式课程学习框架协调这些数据，在20万亿文本单元的规模上联合优化数据构成、质量阈值和领域覆盖。第三，为实现可扩展的推理能力，我们应用自主研发的SnapPO框架进行高效强化学习优化。在英语和韩语的基准测试中，Solar Open展现出具有竞争力的性能，验证了该方法对资源稀缺语言人工智能发展的有效性。",
    "url": "https://huggingface.co/papers/2601.07022",
    "arxiv_url": "https://arxiv.org/abs/2601.07022"
  },
  {
    "title": "KnowMe-Bench: Benchmarking Person Understanding for Lifelong Digital Companions",
    "summary": "Existing long-horizon memory benchmarks mostly use multi-turn dialogues or synthetic user histories, which makes retrieval performance an imperfect proxy for person understanding. We present \\BenchName, a publicly releasable benchmark built from long-form autobiographical narratives, where actions, context, and inner thoughts provide dense evidence for inferring stable motivations and decision principles. \\BenchName~reconstructs each narrative into a flashback-aware, time-anchored stream and evaluates models with evidence-linked questions spanning factual recall, subjective state attribution, and principle-level reasoning. Across diverse narrative sources, retrieval-augmented systems mainly improve factual accuracy, while errors persist on temporally grounded explanations and higher-level inferences, highlighting the need for memory mechanisms beyond retrieval. Our data is in KnowMeBench{https://github.com/QuantaAlpha/KnowMeBench}.",
    "translation": "标题：KnowMe-Bench：面向终身数字伴侣的人物理解基准测试\n\n摘要：现有的长时记忆基准测试多采用多轮对话或合成用户历史数据，这使得检索性能难以准确反映真实的人物理解能力。本文提出\\BenchName，一个基于长篇自传体叙事构建的可公开发布的基准测试，其中行动、情境与内心思考为推断稳定的动机与决策原则提供了密集证据。\\BenchName将每段叙事重构为具有回溯感知、时间锚定的序列流，并通过涵盖事实回忆、主观状态归因及原则层面推理的证据关联问题对模型进行评估。在不同叙事来源中，检索增强系统主要提升了事实准确性，但在基于时间线的解释与高层推理上错误依然存在，这凸显了超越单纯检索的记忆机制的必要性。相关数据已发布于KnowMeBench{https://github.com/QuantaAlpha/KnowMeBench}。",
    "url": "https://huggingface.co/papers/2601.04745",
    "arxiv_url": "https://arxiv.org/abs/2601.04745"
  },
  {
    "title": "User-Oriented Multi-Turn Dialogue Generation with Tool Use at scale",
    "summary": "The recent paradigm shift toward large reasoning models (LRMs) as autonomous agents has intensified the demand for sophisticated, multi-turn tool-use capabilities. Yet, existing datasets and data-generation approaches are limited by static, predefined toolsets that cannot scale to the complexity of open-ended human-agent collaboration. To address this, we initially developed a framework for automated task-oriented multi-turn dialogue generation at scale, utilizing an LRM-based simulator to dynamically generate high-value, domain-specific tools to solve specified tasks. However, we observe that a purely task-oriented design often results in \"solely task-solving\" trajectories, where the agent completes the objective with minimal interaction, failing to generate the high turn-count conversations seen in realistic scenarios. To bridge this gap, we shift toward a user-oriented simulation paradigm. By decoupling task generation from a dedicated user simulator that mimics human behavioral rules - such as incremental request-making and turn-by-turn feedback - we facilitate more authentic, extended multi-turn dialogues that reflect the iterative nature of real-world problem solving. Our generation pipeline operates as a versatile, plug-and-play module capable of initiating generation from any state, ensuring high scalability in producing extended tool-use data. Furthermore, by facilitating multiple task completions within a single trajectory, it yields a high-density dataset that reflects the multifaceted demands of real-world human-agent interaction.",
    "translation": "标题：面向用户的大规模多轮工具使用对话生成\n\n摘要：近期向大型推理模型作为自主智能体的范式转变，显著增强了对复杂多轮工具使用能力的需求。然而，现有数据集与数据生成方法受限于静态、预定义的工具集，难以适应开放式人机协作的复杂性。为此，我们首先开发了一个面向任务的大规模自动化多轮对话生成框架，利用基于大型推理模型的模拟器动态生成高价值、领域特定的工具以解决指定任务。但我们发现，纯任务导向的设计常导致“仅限任务解决”的轨迹，即智能体以最少交互完成任务目标，无法生成现实场景中常见的高轮次对话。为弥合这一差距，我们转向面向用户的模拟范式。通过将任务生成与模拟人类行为规则（如渐进式请求和逐轮反馈）的专用用户模拟器解耦，我们促成了更真实、更延展的多轮对话，反映了现实世界问题解决的迭代特性。我们的生成流程作为一个多功能即插即用模块，能够从任意状态启动生成，确保在产生扩展性工具使用数据时的高度可扩展性。此外，通过支持在单一路径中完成多重任务，该流程生成了高密度数据集，体现了现实人机交互的多维度需求。",
    "url": "https://huggingface.co/papers/2601.08225",
    "arxiv_url": "https://arxiv.org/abs/2601.08225"
  },
  {
    "title": "ShowUI-π: Flow-based Generative Models as GUI Dexterous Hands",
    "summary": "Building intelligent agents capable of dexterous manipulation is essential for achieving human-like automation in both robotics and digital environments. However, existing GUI agents rely on discrete click predictions (x,y), which prohibits free-form, closed-loop trajectories (e.g. dragging a progress bar) that require continuous, on-the-fly perception and adjustment. In this work, we develop ShowUI-π, the first flow-based generative model as GUI dexterous hand, featuring the following designs: (i) Unified Discrete-Continuous Actions, integrating discrete clicks and continuous drags within a shared model, enabling flexible adaptation across diverse interaction modes; (ii) Flow-based Action Generation for drag modeling, which predicts incremental cursor adjustments from continuous visual observations via a lightweight action expert, ensuring smooth and stable trajectories; (iii) Drag Training data and Benchmark, where we manually collect and synthesize 20K drag trajectories across five domains (e.g. PowerPoint, Adobe Premiere Pro), and introduce ScreenDrag, a benchmark with comprehensive online and offline evaluation protocols for assessing GUI agents' drag capabilities. Our experiments show that proprietary GUI agents still struggle on ScreenDrag (e.g. Operator scores 13.27, and the best Gemini-2.5-CUA reaches 22.18). In contrast, ShowUI-π achieves 26.98 with only 450M parameters, underscoring both the difficulty of the task and the effectiveness of our approach. We hope this work advances GUI agents toward human-like dexterous control in digital world. The code is available at https://github.com/showlab/showui-pi.",
    "translation": "标题：ShowUI-π：基于流的生成模型作为图形用户界面的灵巧操作手\n\n摘要：构建能够进行灵巧操作的智能体，对于在机器人学和数字环境中实现类人自动化至关重要。然而，现有的图形用户界面（GUI）智能体依赖于离散的点击预测（x, y坐标），这限制了需要连续、实时感知与调整的自由形式、闭环轨迹操作（例如拖动进度条）。在本工作中，我们开发了ShowUI-π，这是首个作为GUI灵巧操作手的基于流的生成模型，其设计特点包括：（i）统一离散-连续动作，将离散点击与连续拖动整合在一个共享模型中，实现了跨多种交互模式的灵活适应；（ii）用于拖动建模的基于流的动作生成，通过一个轻量级动作专家，根据连续的视觉观察预测光标的增量调整，确保轨迹平滑稳定；（iii）拖动训练数据与基准测试，我们手动收集并合成了涵盖五个领域（如PowerPoint、Adobe Premiere Pro）的2万条拖动轨迹，并引入了ScreenDrag基准，该基准包含全面的在线与离线评估协议，用于评估GUI智能体的拖动能力。实验表明，现有的专有GUI智能体在ScreenDrag上仍表现不佳（例如Operator得分为13.27，表现最佳的Gemini-2.5-CUA达到22.18）。相比之下，ShowUI-π仅以4.5亿参数便取得了26.98的得分，既凸显了该任务的难度，也证明了我们方法的有效性。我们希望这项工作能推动GUI智能体在数字世界中实现类人的灵巧控制。代码可在 https://github.com/showlab/showui-pi 获取。",
    "url": "https://huggingface.co/papers/2512.24965",
    "arxiv_url": "https://arxiv.org/abs/2512.24965"
  },
  {
    "title": "ArenaRL: Scaling RL for Open-Ended Agents via Tournament-based Relative Ranking",
    "summary": "Reinforcement learning has substantially improved the performance of LLM agents on tasks with verifiable outcomes, but it still struggles on open-ended agent tasks with vast solution spaces (e.g., complex travel planning). Due to the absence of objective ground-truth for these tasks, current RL algorithms largely rely on reward models that assign scalar scores to individual responses. We contend that such pointwise scoring suffers from an inherent discrimination collapse: the reward model struggles to distinguish subtle advantages among different trajectories, resulting in scores within a group being compressed into a narrow range. Consequently, the effective reward signal becomes dominated by noise from the reward model, leading to optimization stagnation. To address this, we propose ArenaRL, a reinforcement learning paradigm that shifts from pointwise scalar scoring to intra-group relative ranking. ArenaRL introduces a process-aware pairwise evaluation mechanism, employing multi-level rubrics to assign fine-grained relative scores to trajectories. Additionally, we construct an intra-group adversarial arena and devise a tournament-based ranking scheme to obtain stable advantage signals. Empirical results confirm that the built seeded single-elimination scheme achieves nearly equivalent advantage estimation accuracy to full pairwise comparisons with O(N^2) complexity, while operating with only O(N) complexity, striking an optimal balance between efficiency and precision. Furthermore, to address the lack of full-cycle benchmarks for open-ended agents, we build Open-Travel and Open-DeepResearch, two high-quality benchmarks featuring a comprehensive pipeline covering SFT, RL training, and multi-dimensional evaluation. Extensive experiments show that ArenaRL substantially outperforms standard RL baselines, enabling LLM agents to generate more robust solutions for complex real-world tasks.",
    "translation": "标题：ArenaRL：基于锦标赛相对排序的开放式智能体强化学习规模化方法\n\n摘要：强化学习在可验证结果的任务上显著提升了大型语言模型智能体的性能，但在具有广阔解空间的开放式智能体任务（如复杂旅行规划）中仍面临挑战。由于此类任务缺乏客观真实值，现有强化学习算法主要依赖为单个响应分配标量分数的奖励模型。我们认为这种逐点评分存在固有的判别力坍缩问题：奖励模型难以区分不同轨迹间的细微优势，导致组内分数被压缩至狭窄区间。因此，有效奖励信号被奖励模型的噪声主导，引发优化停滞。为解决该问题，我们提出ArenaRL——一种从逐点标量评分转向组内相对排序的强化学习范式。ArenaRL引入过程感知的成对评估机制，采用多级量规为轨迹分配细粒度相对分数。此外，我们构建组内对抗竞技场并设计基于锦标赛的排序方案，以获取稳定的优势信号。实验结果表明，所构建的种子单败淘汰制方案在仅需O(N)复杂度的同时，实现了与O(N^2)复杂度的全成对比较近乎等效的优势估计精度，在效率与精度间达到最优平衡。针对开放式智能体缺乏全周期基准测试的问题，我们构建了Open-Travel与Open-DeepResearch两个高质量基准，其完整流程涵盖监督微调、强化训练与多维度评估。大量实验表明，ArenaRL显著优于标准强化学习基线，能使大型语言模型智能体为复杂现实任务生成更稳健的解决方案。",
    "url": "https://huggingface.co/papers/2601.06487",
    "arxiv_url": "https://arxiv.org/abs/2601.06487"
  },
  {
    "title": "MemoBrain: Executive Memory as an Agentic Brain for Reasoning",
    "summary": "Complex reasoning in tool-augmented agent frameworks is inherently long-horizon, causing reasoning traces and transient tool artifacts to accumulate and strain the bounded working context of large language models. Without explicit memory mechanisms, such accumulation disrupts logical continuity and undermines task alignment. This positions memory not as an auxiliary efficiency concern, but as a core component for sustaining coherent, goal-directed reasoning over long horizons.\n  We propose MemoBrain, an executive memory model for tool-augmented agents that constructs a dependency-aware memory over reasoning steps, capturing salient intermediate states and their logical relations. Operating as a co-pilot alongside the reasoning agent, MemoBrain organizes reasoning progress without blocking execution and actively manages the working context. Specifically, it prunes invalid steps, folds completed sub-trajectories, and preserves a compact, high-salience reasoning backbone under a fixed context budget. Together, these mechanisms enable explicit cognitive control over reasoning trajectories rather than passive context accumulation.\n  We evaluate MemoBrain on challenging long-horizon benchmarks, including GAIA, WebWalker, and BrowseComp-Plus, demonstrating consistent improvements over strong baselines.",
    "translation": "标题：MemoBrain：作为推理智能体核心的执行记忆系统\n\n摘要：在工具增强型智能体框架中，复杂推理本质上是长程的，这导致推理轨迹和临时性工具产物不断累积，从而对大型语言模型的有限工作上下文造成压力。若无显式记忆机制，此类累积会破坏逻辑连续性并削弱任务对齐能力。这使得记忆不再仅是辅助效率的考量，而成为维持长程连贯、目标导向推理的核心组件。\n\n我们提出 MemoBrain，一种面向工具增强型智能体的执行记忆模型。该模型在推理步骤之上构建依赖感知的记忆系统，捕获关键的中间状态及其逻辑关系。MemoBrain 作为推理智能体的协同处理器运行，在不阻断执行的前提下组织推理进程，并主动管理工作上下文。具体而言，它在固定上下文容量下修剪无效步骤、折叠已完成的子轨迹，并保留紧凑且高显著性的推理主干。这些机制共同实现了对推理轨迹的显式认知控制，而非被动的上下文累积。\n\n我们在具有挑战性的长程基准测试（包括 GAIA、WebWalker 和 BrowseComp-Plus）上评估 MemoBrain，结果表明其相较于强基线模型取得了持续的性能提升。",
    "url": "https://huggingface.co/papers/2601.08079",
    "arxiv_url": "https://arxiv.org/abs/2601.08079"
  },
  {
    "title": "Ministral 3",
    "summary": "We introduce the Ministral 3 series, a family of parameter-efficient dense language models designed for compute and memory constrained applications, available in three model sizes: 3B, 8B, and 14B parameters. For each model size, we release three variants: a pretrained base model for general-purpose use, an instruction finetuned, and a reasoning model for complex problem-solving. In addition, we present our recipe to derive the Ministral 3 models through Cascade Distillation, an iterative pruning and continued training with distillation technique. Each model comes with image understanding capabilities, all under the Apache 2.0 license.",
    "translation": "标题：Ministral 3\n\n摘要：本文介绍Ministral 3系列模型，这是一个专为计算和内存受限应用设计的参数高效密集型语言模型家族，提供三种参数量版本：30亿、80亿和140亿参数。针对每种规模，我们发布了三个变体：适用于通用场景的预训练基础模型、经过指令微调的模型，以及用于复杂问题求解的推理模型。此外，我们提出了通过级联蒸馏技术推导Ministral 3模型的方法论，该技术融合了迭代剪枝与持续蒸馏训练。全系列模型均具备图像理解能力，并基于Apache 2.0开源协议发布。",
    "url": "https://huggingface.co/papers/2601.08584",
    "arxiv_url": "https://arxiv.org/abs/2601.08584"
  },
  {
    "title": "3AM: Segment Anything with Geometric Consistency in Videos",
    "summary": "Video object segmentation methods like SAM2 achieve strong performance through memory-based architectures but struggle under large viewpoint changes due to reliance on appearance features. Traditional 3D instance segmentation methods address viewpoint consistency but require camera poses, depth maps, and expensive preprocessing. We introduce 3AM, a training-time enhancement that integrates 3D-aware features from MUSt3R into SAM2. Our lightweight Feature Merger fuses multi-level MUSt3R features that encode implicit geometric correspondence. Combined with SAM2's appearance features, the model achieves geometry-consistent recognition grounded in both spatial position and visual similarity. We propose a field-of-view aware sampling strategy ensuring frames observe spatially consistent object regions for reliable 3D correspondence learning. Critically, our method requires only RGB input at inference, with no camera poses or preprocessing. On challenging datasets with wide-baseline motion (ScanNet++, Replica), 3AM substantially outperforms SAM2 and extensions, achieving 90.6% IoU and 71.7% Positive IoU on ScanNet++'s Selected Subset, improving over state-of-the-art VOS methods by +15.9 and +30.4 points. Project page: https://jayisaking.github.io/3AM-Page/",
    "translation": "标题：3AM：基于几何一致性的视频通用分割方法\n\n摘要：基于记忆架构的视频目标分割方法（如SAM2）虽具备较强性能，但因其依赖外观特征，在视角剧烈变化时表现受限。传统三维实例分割方法虽能保持视角一致性，却需要相机位姿、深度图及昂贵的预处理流程。本文提出3AM——一种训练时增强方法，将MUSt3R的三维感知特征集成至SAM2框架中。我们设计的轻量级特征融合器能够整合MUSt3R中编码隐式几何对应关系的多层级特征。结合SAM2的外观特征，该模型实现了基于空间位置与视觉相似性的几何一致性识别。我们进一步提出视场感知采样策略，确保帧序列观测到空间一致的目标区域，从而建立可靠的三维对应学习。关键的是，本方法在推理时仅需RGB输入，无需相机位姿或预处理。在具有宽基线运动的挑战性数据集（ScanNet++、Replica）上，3AM显著优于SAM2及其扩展方法，在ScanNet++精选子集上达到90.6%的交并比和71.7%正向交并比，较当前最优视频目标分割方法分别提升15.9和30.4个百分点。项目页面：https://jayisking.github.io/3AM-Page/",
    "url": "https://huggingface.co/papers/2601.08831",
    "arxiv_url": "https://arxiv.org/abs/2601.08831"
  },
  {
    "title": "Motion Attribution for Video Generation",
    "summary": "Despite the rapid progress of video generation models, the role of data in influencing motion is poorly understood. We present Motive (MOTIon attribution for Video gEneration), a motion-centric, gradient-based data attribution framework that scales to modern, large, high-quality video datasets and models. We use this to study which fine-tuning clips improve or degrade temporal dynamics. Motive isolates temporal dynamics from static appearance via motion-weighted loss masks, yielding efficient and scalable motion-specific influence computation. On text-to-video models, Motive identifies clips that strongly affect motion and guides data curation that improves temporal consistency and physical plausibility. With Motive-selected high-influence data, our method improves both motion smoothness and dynamic degree on VBench, achieving a 74.1% human preference win rate compared with the pretrained base model. To our knowledge, this is the first framework to attribute motion rather than visual appearance in video generative models and to use it to curate fine-tuning data.",
    "translation": "标题：视频生成中的运动归因\n\n摘要：尽管视频生成模型发展迅速，但数据对运动特性的影响机制尚未得到充分理解。本文提出Motive（视频生成运动归因框架），这是一个以运动为核心、基于梯度的数据归因框架，可适配现代大规模高质量视频数据集与模型。我们运用该框架探究哪些微调片段会改善或损害时序动态特性。Motive通过运动加权损失掩码将时序动态与静态表观特征解耦，实现了高效可扩展的运动特异性影响计算。在文本到视频模型中，Motive能识别对运动特性具有显著影响的数据片段，并指导数据筛选以提升时序一致性与物理合理性。使用Motive筛选的高影响力数据进行训练后，我们的方法在VBench评测中同时提升了运动平滑度与动态程度指标，相较于预训练基础模型获得74.1%的人类偏好胜率。据我们所知，这是首个在视频生成模型中针对运动特性（而非视觉表观）进行归因，并以此指导微调数据筛选的框架。",
    "url": "https://huggingface.co/papers/2601.08828",
    "arxiv_url": "https://arxiv.org/abs/2601.08828"
  },
  {
    "title": "The Confidence Dichotomy: Analyzing and Mitigating Miscalibration in Tool-Use Agents",
    "summary": "Autonomous agents based on large language models (LLMs) are rapidly evolving to handle multi-turn tasks, but ensuring their trustworthiness remains a critical challenge. A fundamental pillar of this trustworthiness is calibration, which refers to an agent's ability to express confidence that reliably reflects its actual performance. While calibration is well-established for static models, its dynamics in tool-integrated agentic workflows remain underexplored. In this work, we systematically investigate verbalized calibration in tool-use agents, revealing a fundamental confidence dichotomy driven by tool type. Specifically, our pilot study identifies that evidence tools (e.g., web search) systematically induce severe overconfidence due to inherent noise in retrieved information, while verification tools (e.g., code interpreters) can ground reasoning through deterministic feedback and mitigate miscalibration. To robustly improve calibration across tool types, we propose a reinforcement learning (RL) fine-tuning framework that jointly optimizes task accuracy and calibration, supported by a holistic benchmark of reward designs. We demonstrate that our trained agents not only achieve superior calibration but also exhibit robust generalization from local training environments to noisy web settings and to distinct domains such as mathematical reasoning. Our results highlight the necessity of domain-specific calibration strategies for tool-use agents. More broadly, this work establishes a foundation for building self-aware agents that can reliably communicate uncertainty in high-stakes, real-world deployments.",
    "translation": "标题：置信度二分法：工具使用智能体的校准偏差分析与缓解策略\n\n摘要：基于大语言模型的自主智能体正快速发展以处理多轮任务，但其可信度保障仍是关键挑战。可信度的核心支柱在于校准能力，即智能体表达的信心与其实际性能可靠匹配的程度。尽管静态模型的校准研究已较为成熟，工具集成智能体工作流中的动态校准机制仍待深入探索。本研究系统探究工具使用智能体的言语化校准现象，揭示了由工具类型驱动的根本性置信度二分效应。具体而言，初步研究发现证据型工具（如网络搜索）因检索信息的固有噪声会导致系统性严重过度自信，而验证型工具（如代码解释器）可通过确定性反馈锚定推理过程从而缓解校准偏差。为全面提升跨工具类型的校准能力，我们提出基于强化学习的微调框架，通过综合奖励设计基准联合优化任务准确率与校准度。实验表明，经训练的智能体不仅实现更优校准性能，还能从局部训练环境稳健泛化至嘈杂网络场景及数学推理等新领域。本研究结果凸显了针对工具使用智能体开发领域特异性校准策略的必要性。更广泛而言，本工作为构建能在高风险现实部署中可靠传达不确定性的自感知智能体奠定了理论基础。",
    "url": "https://huggingface.co/papers/2601.07264",
    "arxiv_url": "https://arxiv.org/abs/2601.07264"
  },
  {
    "title": "Parallel Context-of-Experts Decoding for Retrieval Augmented Generation",
    "summary": "Retrieval Augmented Generation faces a trade-off: concatenating documents in a long prompt enables multi-document reasoning but creates prefill bottlenecks, while encoding document KV caches separately offers speed but breaks cross-document interaction. We propose Parallel Context-of-Experts Decoding (Pced), a training-free framework that shifts evidence aggregation from the attention mechanism to the decoding. Pced treats retrieved documents as isolated \"experts\", synchronizing their predictions via a novel retrieval-aware contrastive decoding rule that weighs expert logits against the model prior. This approach recovers cross-document reasoning capabilities without constructing a shared attention across documents.",
    "translation": "标题：面向检索增强生成的并行专家上下文解码方法\n\n摘要：检索增强生成技术面临一个权衡困境：将多篇文档拼接为长提示文本虽能实现跨文档推理，却会导致预填充阶段的性能瓶颈；而将文档键值缓存分别编码虽能提升速度，却会破坏文档间的交互关联。本文提出并行专家上下文解码框架，该免训练框架将证据聚合机制从注意力层转移至解码层。该方法将检索文档视为独立的“专家”，通过创新的检索感知对比解码规则同步各专家预测结果，该规则依据模型先验对专家对数概率进行加权处理。该方案无需构建跨文档共享注意力机制，即可恢复跨文档推理能力。",
    "url": "https://huggingface.co/papers/2601.08670",
    "arxiv_url": "https://arxiv.org/abs/2601.08670"
  },
  {
    "title": "ViDoRe V3: A Comprehensive Evaluation of Retrieval Augmented Generation in Complex Real-World Scenarios",
    "summary": "Retrieval-Augmented Generation (RAG) pipelines must address challenges beyond simple single-document retrieval, such as interpreting visual elements (tables, charts, images), synthesizing information across documents, and providing accurate source grounding. Existing benchmarks fail to capture this complexity, often focusing on textual data, single-document comprehension, or evaluating retrieval and generation in isolation. We introduce ViDoRe v3, a comprehensive multimodal RAG benchmark featuring multi-type queries over visually rich document corpora. It covers 10 datasets across diverse professional domains, comprising ~26,000 document pages paired with 3,099 human-verified queries, each available in 6 languages. Through 12,000 hours of human annotation effort, we provide high-quality annotations for retrieval relevance, bounding box localization, and verified reference answers. Our evaluation of state-of-the-art RAG pipelines reveals that visual retrievers outperform textual ones, late-interaction models and textual reranking substantially improve performance, and hybrid or purely visual contexts enhance answer generation quality. However, current models still struggle with non-textual elements, open-ended queries, and fine-grained visual grounding. To encourage progress in addressing these challenges, the benchmark is released under a commercially permissive license at https://hf.co/vidore.",
    "translation": "标题：ViDoRe V3：复杂现实场景中检索增强生成技术的综合评估\n\n摘要：检索增强生成（RAG）流程需应对超越简单单文档检索的挑战，例如解析视觉元素（表格、图表、图像）、跨文档信息综合以及提供精确的溯源依据。现有基准测试未能涵盖此类复杂性，往往局限于文本数据、单文档理解或孤立评估检索与生成环节。本文推出ViDoRe v3——一个全面的多模态RAG基准测试体系，其特点在于针对视觉密集型文档集设计多类型查询任务。该基准涵盖10个跨专业领域数据集，包含约26,000份文档页面与3,099条人工验证查询的配对数据，每条查询均支持6种语言版本。通过12,000小时的人工标注工作，我们为检索相关性、边界框定位及验证参考答案提供了高质量标注。对前沿RAG流程的评估表明：视觉检索器性能优于文本检索器，延迟交互模型与文本重排序技术能显著提升表现，混合或纯视觉上下文可增强答案生成质量。然而，现有模型在处理非文本元素、开放式查询及细粒度视觉定位方面仍存在不足。为促进相关挑战的攻关，本基准测试已通过商业友好许可发布于https://hf.co/vidore。",
    "url": "https://huggingface.co/papers/2601.08620",
    "arxiv_url": "https://arxiv.org/abs/2601.08620"
  },
  {
    "title": "SnapGen++: Unleashing Diffusion Transformers for Efficient High-Fidelity Image Generation on Edge Devices",
    "summary": "Recent advances in diffusion transformers (DiTs) have set new standards in image generation, yet remain impractical for on-device deployment due to their high computational and memory costs. In this work, we present an efficient DiT framework tailored for mobile and edge devices that achieves transformer-level generation quality under strict resource constraints. Our design combines three key components. First, we propose a compact DiT architecture with an adaptive global-local sparse attention mechanism that balances global context modeling and local detail preservation. Second, we propose an elastic training framework that jointly optimizes sub-DiTs of varying capacities within a unified supernetwork, allowing a single model to dynamically adjust for efficient inference across different hardware. Finally, we develop Knowledge-Guided Distribution Matching Distillation, a step-distillation pipeline that integrates the DMD objective with knowledge transfer from few-step teacher models, producing high-fidelity and low-latency generation (e.g., 4-step) suitable for real-time on-device use. Together, these contributions enable scalable, efficient, and high-quality diffusion models for deployment on diverse hardware.",
    "translation": "标题：SnapGen++：释放扩散变换器在边缘设备上实现高效高保真图像生成的潜力\n\n摘要：扩散变换器（DiTs）的最新进展为图像生成树立了新的标杆，但其高昂的计算与内存成本使其难以在实际设备上部署。本研究提出一种专为移动与边缘设备设计的高效DiT框架，能在严格资源限制下实现变换器级别的生成质量。我们的设计融合了三个关键组成部分：首先，提出一种紧凑的DiT架构，采用自适应全局-局部稀疏注意力机制，平衡全局上下文建模与局部细节保留；其次，设计弹性训练框架，在统一超网络内联合优化不同容量的子DiT模型，使单一模型能够动态调整以适应不同硬件的高效推理需求；最后，开发知识引导分布匹配蒸馏技术，该分步蒸馏流程将DMD目标与少步数教师模型的知识迁移相结合，生成适用于设备端实时应用的高保真、低延迟图像（例如4步生成）。这些贡献共同构建了可扩展、高效且高质量的扩散模型，为多样化硬件部署提供了可行方案。",
    "url": "https://huggingface.co/papers/2601.08303",
    "arxiv_url": "https://arxiv.org/abs/2601.08303"
  },
  {
    "title": "VLingNav: Embodied Navigation with Adaptive Reasoning and Visual-Assisted Linguistic Memory",
    "summary": "VLA models have shown promising potential in embodied navigation by unifying perception and planning while inheriting the strong generalization abilities of large VLMs. However, most existing VLA models rely on reactive mappings directly from observations to actions, lacking the explicit reasoning capabilities and persistent memory required for complex, long-horizon navigation tasks. To address these challenges, we propose VLingNav, a VLA model for embodied navigation grounded in linguistic-driven cognition. First, inspired by the dual-process theory of human cognition, we introduce an adaptive chain-of-thought mechanism, which dynamically triggers explicit reasoning only when necessary, enabling the agent to fluidly switch between fast, intuitive execution and slow, deliberate planning. Second, to handle long-horizon spatial dependencies, we develop a visual-assisted linguistic memory module that constructs a persistent, cross-modal semantic memory, enabling the agent to recall past observations to prevent repetitive exploration and infer movement trends for dynamic environments. For the training recipe, we construct Nav-AdaCoT-2.9M, the largest embodied navigation dataset with reasoning annotations to date, enriched with adaptive CoT annotations that induce a reasoning paradigm capable of adjusting both when to think and what to think about. Moreover, we incorporate an online expert-guided reinforcement learning stage, enabling the model to surpass pure imitation learning and to acquire more robust, self-explored navigation behaviors. Extensive experiments demonstrate that VLingNav achieves state-of-the-art performance across a wide range of embodied navigation benchmarks. Notably, VLingNav transfers to real-world robotic platforms in a zero-shot manner, executing various navigation tasks and demonstrating strong cross-domain and cross-task generalization.",
    "translation": "标题：VLingNav：基于自适应推理与视觉辅助语言记忆的具身导航\n\n摘要：视觉语言动作模型通过统一感知与规划，并继承大型视觉语言模型的强大泛化能力，在具身导航任务中展现出显著潜力。然而，现有视觉语言动作模型大多依赖从观测到动作的被动映射，缺乏应对复杂长程导航任务所需的显式推理能力与持久记忆机制。为应对这些挑战，我们提出VLingNav——一种基于语言驱动认知的具身导航视觉语言动作模型。首先，受人类认知双过程理论启发，我们引入自适应思维链机制，该机制仅在必要时动态触发显式推理，使智能体能够在快速直觉执行与慢速审慎规划之间灵活切换。其次，为处理长程空间依赖关系，我们开发了视觉辅助语言记忆模块，构建持久跨模态语义记忆，使智能体能够回溯历史观测以避免重复探索，并推断动态环境中的运动趋势。在训练策略方面，我们构建了Nav-AdaCoT-2.9M数据集——迄今为止规模最大的含推理标注具身导航数据集，其中增强的自适应思维链标注可引导模型形成兼具“何时思考”与“思考内容”调节能力的推理范式。此外，我们引入在线专家引导强化学习阶段，使模型能够超越纯模仿学习，获得更鲁棒、自主探索的导航行为。大量实验表明，VLingNav在广泛的具身导航基准测试中均达到最先进性能。值得注意的是，VLingNav能够以零样本方式迁移至真实机器人平台，执行多样化导航任务，并展现出强大的跨领域与跨任务泛化能力。",
    "url": "https://huggingface.co/papers/2601.08665",
    "arxiv_url": "https://arxiv.org/abs/2601.08665"
  },
  {
    "title": "End-to-End Video Character Replacement without Structural Guidance",
    "summary": "Controllable video character replacement with a user-provided identity remains a challenging problem due to the lack of paired video data. Prior works have predominantly relied on a reconstruction-based paradigm that requires per-frame segmentation masks and explicit structural guidance (e.g., skeleton, depth). This reliance, however, severely limits their generalizability in complex scenarios involving occlusions, character-object interactions, unusual poses, or challenging illumination, often leading to visual artifacts and temporal inconsistencies. In this paper, we propose MoCha, a pioneering framework that bypasses these limitations by requiring only a single arbitrary frame mask. To effectively adapt the multi-modal input condition and enhance facial identity, we introduce a condition-aware RoPE and employ an RL-based post-training stage. Furthermore, to overcome the scarcity of qualified paired-training data, we propose a comprehensive data construction pipeline. Specifically, we design three specialized datasets: a high-fidelity rendered dataset built with Unreal Engine 5 (UE5), an expression-driven dataset synthesized by current portrait animation techniques, and an augmented dataset derived from existing video-mask pairs. Extensive experiments demonstrate that our method substantially outperforms existing state-of-the-art approaches. We will release the code to facilitate further research. Please refer to our project page for more details: orange-3dv-team.github.io/MoCha",
    "translation": "标题：无需结构引导的端到端视频角色替换\n\n摘要：由于缺乏成对的视频数据，基于用户提供身份信息的可控视频角色替换仍是一个具有挑战性的问题。现有研究主要依赖于基于重建的范式，该方法需要逐帧分割掩码和明确的结构引导（如骨骼、深度信息）。然而，这种依赖性严重限制了其在复杂场景中的泛化能力，例如存在遮挡、角色与物体交互、非常规姿态或复杂光照等情况时，常导致视觉伪影和时间不一致性。本文提出MoCha框架，该开创性方法仅需单帧任意掩码即可突破上述限制。为有效适配多模态输入条件并增强面部身份特征，我们引入了条件感知的旋转位置编码，并采用基于强化学习的后训练阶段。此外，为克服高质量配对训练数据的稀缺问题，我们提出了完整的数据构建流程：专门设计了基于虚幻引擎5构建的高保真渲染数据集、通过当前人像动画技术合成的表情驱动数据集，以及从现有视频-掩码对衍生的增强数据集。大量实验表明，本方法显著优于现有最先进技术。我们将公开代码以促进后续研究，更多细节请访问项目页面：orange-3dv-team.github.io/MoCha",
    "url": "https://huggingface.co/papers/2601.08587",
    "arxiv_url": "https://arxiv.org/abs/2601.08587"
  },
  {
    "title": "VideoLoom: A Video Large Language Model for Joint Spatial-Temporal Understanding",
    "summary": "This paper presents VideoLoom, a unified Video Large Language Model (Video LLM) for joint spatial-temporal understanding. To facilitate the development of fine-grained spatial and temporal localization capabilities, we curate LoomData-8.7k, a human-centric video dataset with temporally grounded and spatially localized captions. With this, VideoLoom achieves state-of-the-art or highly competitive performance across a variety of spatial and temporal benchmarks (e.g., 63.1 J&F on ReVOS for referring video object segmentation, and 48.3 R1@0.7 on Charades-STA for temporal grounding). In addition, we introduce LoomBench, a novel benchmark consisting of temporal, spatial, and compositional video-question pairs, enabling a comprehensive evaluation of Video LLMs from diverse aspects. Collectively, these contributions offer a universal and effective suite for joint spatial-temporal video understanding, setting a new standard in multimodal intelligence.",
    "translation": "标题：VideoLoom：一种用于联合时空理解的视频大语言模型\n\n摘要：本文提出VideoLoom，一种用于联合时空理解的统一视频大语言模型。为促进细粒度时空定位能力的发展，我们构建了LoomData-8.7k数据集——一个以人为中心、包含时间锚定与空间定位描述的视频数据集。基于此，VideoLoom在多项时空基准测试中取得了领先或极具竞争力的性能（例如，在指代视频目标分割任务ReVOS上达到63.1 J&F分数，在时序定位任务Charades-STA上达到48.3 R1@0.7分数）。此外，我们提出了LoomBench——一个由时序、空间及组合型视频-问题对构成的新型评测基准，能够从多维度对视频大语言模型进行全面评估。这些成果共同构成了一套通用且高效的联合时空视频理解方案，为多模态智能领域树立了新标准。",
    "url": "https://huggingface.co/papers/2601.07290",
    "arxiv_url": "https://arxiv.org/abs/2601.07290"
  },
  {
    "title": "JudgeRLVR: Judge First, Generate Second for Efficient Reasoning",
    "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has become a standard paradigm for reasoning in Large Language Models. However, optimizing solely for final-answer correctness often drives models into aimless, verbose exploration, where they rely on exhaustive trial-and-error tactics rather than structured planning to reach solutions. While heuristic constraints like length penalties can reduce verbosity, they often truncate essential reasoning steps, creating a difficult trade-off between efficiency and verification. In this paper, we argue that discriminative capability is a prerequisite for efficient generation: by learning to distinguish valid solutions, a model can internalize a guidance signal that prunes the search space. We propose JudgeRLVR, a two-stage judge-then-generate paradigm. In the first stage, we train the model to judge solution responses with verifiable answers. In the second stage, we fine-tune the same model with vanilla generating RLVR initialized from the judge. Compared to Vanilla RLVR using the same math-domain training data, JudgeRLVR achieves a better quality--efficiency trade-off for Qwen3-30B-A3B: on in-domain math, it delivers about +3.7 points average accuracy gain with -42\\% average generation length; on out-of-domain benchmarks, it delivers about +4.5 points average accuracy improvement, demonstrating enhanced generalization.",
    "translation": "标题：JudgeRLVR：先判别后生成的高效推理方法\n\n摘要：基于可验证奖励的强化学习已成为大语言模型推理的标准范式。然而，仅针对最终答案正确性进行优化常导致模型陷入盲目、冗长的探索，使其依赖穷举试错策略而非结构化规划来求解。虽然长度惩罚等启发式约束可减少冗余，但常会截断关键推理步骤，造成效率与验证之间的艰难权衡。本文提出判别能力是高效生成的前提：通过学习区分有效解，模型可内化一种能剪枝搜索空间的引导信号。我们提出JudgeRLVR，一种“先判别后生成”的两阶段范式。第一阶段，训练模型对含可验证答案的求解响应进行判别；第二阶段，以判别模型初始化，通过标准生成式RLVR对同一模型进行微调。在使用相同数学领域训练数据的情况下，与原始RLVR相比，JudgeRLVR为Qwen3-30B-A3B模型实现了更优的质量-效率权衡：在领域内数学任务上，平均准确率提升约3.7分的同时平均生成长度减少42%；在领域外基准测试中，平均准确率提升约4.5分，展现出更强的泛化能力。",
    "url": "https://huggingface.co/papers/2601.08468",
    "arxiv_url": "https://arxiv.org/abs/2601.08468"
  },
  {
    "title": "EpiCaR: Knowing What You Don't Know Matters for Better Reasoning in LLMs",
    "summary": "Improving the reasoning abilities of large language models (LLMs) has largely relied on iterative self-training with model-generated data. While effective at boosting accuracy, existing approaches primarily reinforce successful reasoning paths, incurring a substantial calibration cost: models become overconfident and lose the ability to represent uncertainty. This failure has been characterized as a form of model collapse in alignment, where predictive distributions degenerate toward low-variance point estimates. We address this issue by reframing reasoning training as an epistemic learning problem, in which models must learn not only how to reason, but also when their reasoning should be trusted. We propose epistemically-calibrated reasoning (EpiCaR) as a training objective that jointly optimizes reasoning performance and calibration, and instantiate it within an iterative supervised fine-tuning framework using explicit self-evaluation signals. Experiments on Llama-3 and Qwen-3 families demonstrate that our approach achieves Pareto-superiority over standard baselines in both accuracy and calibration, particularly in models with sufficient reasoning capacity (e.g., 3B+). This framework generalizes effectively to OOD mathematical reasoning (GSM8K) and code generation (MBPP). Ultimately, our approach enables a 3X reduction in inference compute, matching the K=30 performance of STaR with only K=10 samples in capable models.",
    "translation": "标题：EpiCaR：认知不确定性对提升大语言模型推理能力的重要性\n\n摘要：提升大语言模型（LLMs）的推理能力主要依赖于利用模型生成数据进行迭代式自训练。尽管现有方法能有效提高准确性，但其主要强化了成功的推理路径，并带来了显著的校准代价：模型会变得过度自信，丧失表征不确定性的能力。这种缺陷被描述为对齐过程中的一种模型坍缩现象，即预测分布退化为低方差的点估计。为解决此问题，我们将推理训练重新定义为认知学习问题，要求模型不仅学习如何推理，还需学会判断何时应信任自身的推理过程。我们提出认知校准推理（EpiCaR）作为联合优化推理性能与校准度的训练目标，并基于显式自评估信号在迭代式监督微调框架中实现该方法。在Llama-3和Qwen-3系列模型上的实验表明，我们的方法在准确性与校准度上均对标准基线实现了帕累托优化，尤其在具备充分推理能力的模型（如3B+参数规模）中效果显著。该框架能有效泛化至分布外数学推理（GSM8K）与代码生成（MBPP）任务。最终，我们的方法在具备足够能力的模型中，仅需K=10个样本即可匹配STaR方法K=30样本的推理性能，实现了推理计算量三倍的降低。\n\n请按照以下格式返回：\n标题：EpiCaR：认知不确定性对提升大语言模型推理能力的重要性\n摘要：提升大语言模型（LLMs）的推理能力主要依赖于利用模型生成数据进行迭代式自训练。尽管现有方法能有效提高准确性，但其主要强化了成功的推理路径，并带来了显著的校准代价：模型会变得过度自信，丧失表征不确定性的能力。这种缺陷被描述为对齐过程中的一种模型坍缩现象，即预测分布退化为低方差的点估计。为解决此问题，我们将推理训练重新定义为认知学习问题，要求模型不仅学习如何推理，还需学会判断何时应信任自身的推理过程。我们提出认知校准推理（EpiCaR）作为联合优化推理性能与校准度的训练目标，并基于显式自评估信号在迭代式监督微调框架中实现该方法。在Llama-3和Qwen-3系列模型上的实验表明，我们的方法在准确性与校准度上均对标准基线实现了帕累托优化，尤其在具备充分推理能力的模型（如3B+参数规模）中效果显著。该框架能有效泛化至分布外数学推理（GSM8K）与代码生成（MBPP）任务。最终，我们的方法在具备足够能力的模型中，仅需K=10个样本即可匹配STaR方法K=30样本的推理性能，实现了推理计算量三倍的降低。",
    "url": "https://huggingface.co/papers/2601.06786",
    "arxiv_url": "https://arxiv.org/abs/2601.06786"
  },
  {
    "title": "UM-Text: A Unified Multimodal Model for Image Understanding",
    "summary": "With the rapid advancement of image generation, visual text editing using natural language instructions has received increasing attention. The main challenge of this task is to fully understand the instruction and reference image, and thus generate visual text that is style-consistent with the image. Previous methods often involve complex steps of specifying the text content and attributes, such as font size, color, and layout, without considering the stylistic consistency with the reference image. To address this, we propose UM-Text, a unified multimodal model for context understanding and visual text editing by natural language instructions. Specifically, we introduce a Visual Language Model (VLM) to process the instruction and reference image, so that the text content and layout can be elaborately designed according to the context information. To generate an accurate and harmonious visual text image, we further propose the UM-Encoder to combine the embeddings of various condition information, where the combination is automatically configured by VLM according to the input instruction. During training, we propose a regional consistency loss to offer more effective supervision for glyph generation on both latent and RGB space, and design a tailored three-stage training strategy to further enhance model performance. In addition, we contribute the UM-DATA-200K, a large-scale visual text image dataset on diverse scenes for model training. Extensive qualitative and quantitative results on multiple public benchmarks demonstrate that our method achieves state-of-the-art performance.",
    "translation": "标题：UM-Text：一种用于图像理解的多模态统一模型\n\n摘要：随着图像生成技术的快速发展，基于自然语言指令的视觉文本编辑日益受到关注。该任务的主要挑战在于充分理解指令与参考图像，从而生成与图像风格一致的视觉文本。现有方法通常涉及指定文本内容及字体大小、颜色、布局等属性的复杂步骤，且未充分考虑与参考图像之间的风格一致性。为此，我们提出UM-Text——一个通过自然语言指令实现上下文理解与视觉文本编辑的多模态统一模型。具体而言，我们引入视觉语言模型处理指令与参考图像，使文本内容与布局能够依据上下文信息进行精细化设计。为生成准确且协调的视觉文本图像，我们进一步提出UM-Encoder以融合多类条件信息的嵌入表示，其融合方式由视觉语言模型根据输入指令自动配置。在训练阶段，我们提出区域一致性损失函数，在潜在空间与RGB空间为字形生成提供更有效的监督，并设计定制化的三阶段训练策略以进一步提升模型性能。此外，我们构建了包含20万张多场景视觉文本图像的大规模数据集UM-DATA-200K用于模型训练。在多个公开基准测试上的大量定性与定量实验表明，本方法取得了当前最优性能。",
    "url": "https://huggingface.co/papers/2601.08321",
    "arxiv_url": "https://arxiv.org/abs/2601.08321"
  },
  {
    "title": "The Agent's First Day: Benchmarking Learning, Exploration, and Scheduling in the Workplace Scenarios",
    "summary": "The rapid evolution of Multi-modal Large Language Models (MLLMs) has advanced workflow automation; however, existing research mainly targets performance upper bounds in static environments, overlooking robustness for stochastic real-world deployment. We identify three key challenges: dynamic task scheduling, active exploration under uncertainty, and continuous learning from experience. To bridge this gap, we introduce , a dynamic evaluation environment that simulates a \"trainee\" agent continuously exploring a novel setting. Unlike traditional benchmarks,  evaluates agents along three dimensions: (1) context-aware scheduling for streaming tasks with varying priorities; (2) prudent information acquisition to reduce hallucination via active exploration; and (3) continuous evolution by distilling generalized strategies from rule-based, dynamically generated tasks. Experiments show that cutting-edge agents have significant deficiencies in dynamic environments, especially in active exploration and continual learning. Our work establishes a framework for assessing agent reliability, shifting evaluation from static tests to realistic, production-oriented scenarios. Our codes are available at https://github.com/KnowledgeXLab/EvoEnv",
    "translation": "标题：智能体首日：工作场景中的学习、探索与调度基准测试\n\n摘要：多模态大语言模型的快速发展推动了工作流程自动化，但现有研究主要关注静态环境下的性能上限，忽视了随机现实部署场景中的鲁棒性问题。我们识别出三个关键挑战：动态任务调度、不确定性下的主动探索以及基于经验的持续学习。为弥补这一空白，我们提出了动态评估环境，该环境模拟\"受训\"智能体在全新场景中的持续探索过程。与传统基准测试不同，本框架从三个维度评估智能体：(1) 针对不同优先级流式任务的上下文感知调度能力；(2) 通过主动探索进行审慎信息获取以减少幻觉现象；(3) 从基于规则动态生成的任务中提炼泛化策略以实现持续进化。实验表明，前沿智能体在动态环境中存在显著缺陷，尤其在主动探索与持续学习方面。本研究建立了评估智能体可靠性的框架，将评估重点从静态测试转向真实的生产导向场景。代码已开源：https://github.com/KnowledgeXLab/EvoEnv",
    "url": "https://huggingface.co/papers/2601.08173",
    "arxiv_url": "https://arxiv.org/abs/2601.08173"
  },
  {
    "title": "Aligning Text, Code, and Vision: A Multi-Objective Reinforcement Learning Framework for Text-to-Visualization",
    "summary": "Text-to-Visualization (Text2Vis) systems translate natural language queries over tabular data into concise answers and executable visualizations. While closed-source LLMs generate functional code, the resulting charts often lack semantic alignment and clarity, qualities that can only be assessed post-execution. Open-source models struggle even more, frequently producing non-executable or visually poor outputs. Although supervised fine-tuning can improve code executability, it fails to enhance overall visualization quality, as traditional SFT loss cannot capture post-execution feedback. To address this gap, we propose RL-Text2Vis, the first reinforcement learning framework for Text2Vis generation. Built on Group Relative Policy Optimization (GRPO), our method uses a novel multi-objective reward that jointly optimizes textual accuracy, code validity, and visualization quality using post-execution feedback. By training Qwen2.5 models (7B and 14B), RL-Text2Vis achieves a 22% relative improvement in chart quality over GPT-4o on the Text2Vis benchmark and boosts code execution success from 78% to 97% relative to its zero-shot baseline. Our models significantly outperform strong zero-shot and supervised baselines and also demonstrate robust generalization to out-of-domain datasets like VIS-Eval and NVBench. These results establish GRPO as an effective strategy for structured, multimodal reasoning in visualization generation. We release our code at https://github.com/vis-nlp/RL-Text2Vis.",
    "translation": "标题：对齐文本、代码与视觉：一种面向文本到可视化的多目标强化学习框架\n\n摘要：文本到可视化系统能够将针对表格数据的自然语言查询转化为简洁的答案与可执行的可视化图表。尽管闭源大语言模型能够生成功能代码，但其生成的图表往往在语义对齐性和清晰度方面存在不足，而这些质量指标通常只能在执行后才能评估。开源模型的表现则更为困难，常常产生无法执行或视觉效果不佳的输出。虽然监督微调可以提升代码可执行性，但由于传统的监督微调损失函数无法捕捉执行后的反馈，它难以改善可视化的整体质量。为弥补这一不足，我们提出了RL-Text2Vis，这是首个用于文本到可视化生成的强化学习框架。该方法基于分组相对策略优化构建，采用一种新颖的多目标奖励机制，利用执行后反馈联合优化文本准确性、代码有效性和可视化质量。通过训练Qwen2.5模型，RL-Text2Vis在Text2Vis基准测试中实现了相较于GPT-4o图表质量22%的相对提升，并将代码执行成功率从零样本基线的78%提高至97%。我们的模型显著超越了强大的零样本与监督基线，并在VIS-Eval和NVBench等域外数据集上展现出良好的泛化能力。这些结果表明，分组相对策略优化是可视化生成中结构化多模态推理的有效策略。代码已发布于https://github.com/vis-nlp/RL-Text2Vis。",
    "url": "https://huggingface.co/papers/2601.04582",
    "arxiv_url": "https://arxiv.org/abs/2601.04582"
  },
  {
    "title": "Towards Comprehensive Stage-wise Benchmarking of Large Language Models in Fact-Checking",
    "summary": "Large Language Models (LLMs) are increasingly deployed in real-world fact-checking systems, yet existing evaluations focus predominantly on claim verification and overlook the broader fact-checking workflow, including claim extraction and evidence retrieval. This narrow focus prevents current benchmarks from revealing systematic reasoning failures, factual blind spots, and robustness limitations of modern LLMs. To bridge this gap, we present FactArena, a fully automated arena-style evaluation framework that conducts comprehensive, stage-wise benchmarking of LLMs across the complete fact-checking pipeline. FactArena integrates three key components: (i) an LLM-driven fact-checking process that standardizes claim decomposition, evidence retrieval via tool-augmented interactions, and justification-based verdict prediction; (ii) an arena-styled judgment mechanism guided by consolidated reference guidelines to ensure unbiased and consistent pairwise comparisons across heterogeneous judge agents; and (iii) an arena-driven claim-evolution module that adaptively generates more challenging and semantically controlled claims to probe LLMs' factual robustness beyond fixed seed data. Across 16 state-of-the-art LLMs spanning seven model families, FactArena produces stable and interpretable rankings. Our analyses further reveal significant discrepancies between static claim-verification accuracy and end-to-end fact-checking competence, highlighting the necessity of holistic evaluation. The proposed framework offers a scalable and trustworthy paradigm for diagnosing LLMs' factual reasoning, guiding future model development, and advancing the reliable deployment of LLMs in safety-critical fact-checking applications.",
    "translation": "标题：迈向大语言模型在事实核查中的全面分阶段基准测试\n\n摘要：大语言模型正日益广泛应用于现实世界的事实核查系统中，然而现有评估主要集中于主张验证环节，忽视了包括主张提取与证据检索在内的更广泛的事实核查工作流程。这种局限使得当前基准测试无法充分揭示现代大语言模型的系统性推理缺陷、事实盲点及鲁棒性限制。为弥补这一空白，我们提出FactArena——一个全自动的竞技场式评估框架，针对完整的事实核查流程对大语言模型进行全面的分阶段基准测试。FactArena整合了三个核心模块：（一）由大语言模型驱动的事实核查流程，实现了主张解构、通过工具增强交互的证据检索以及基于论证的判定预测的标准化；（二）以统一参考准则为导向的竞技场式评判机制，确保异构评判代理之间进行无偏且一致的成对比较；（三）竞技场驱动的主张演化模块，能够自适应生成更具挑战性且语义受控的主张，以探究大语言模型在固定种子数据之外的事实鲁棒性。通过对涵盖七个模型家族的16个前沿大语言模型进行测试，FactArena产生了稳定且可解释的性能排序。我们的分析进一步揭示了静态主张验证准确率与端到端事实核查能力之间的显著差异，凸显了整体性评估的必要性。该框架为诊断大语言模型的事实推理能力、指导未来模型发展，以及推动大语言模型在安全关键型事实核查应用中的可靠部署，提供了一个可扩展且可信的评估范式。",
    "url": "https://huggingface.co/papers/2601.02669",
    "arxiv_url": "https://arxiv.org/abs/2601.02669"
  },
  {
    "title": "GeoMotionGPT: Geometry-Aligned Motion Understanding with Large Language Models",
    "summary": "Discrete motion tokenization has recently enabled Large Language Models (LLMs) to serve as versatile backbones for motion understanding and motion-language reasoning. However, existing pipelines typically decouple motion quantization from semantic embedding learning, linking them solely via token IDs. This approach fails to effectively align the intrinsic geometry of the motion space with the embedding space, thereby hindering the LLM's capacity for nuanced motion reasoning. We argue that alignment is most effective when both modalities share a unified geometric basis. Therefore, instead of forcing the LLM to reconstruct the complex geometry among motion tokens from scratch, we present a novel framework that explicitly enforces orthogonality on both the motion codebook and the LLM embedding space, ensuring that their relational structures naturally mirror each other. Specifically, we employ a decoder-only quantizer with Gumbel-Softmax for differentiable training and balanced codebook usage. To bridge the modalities, we use a sparse projection that maps motion codes into the LLM embedding space while preserving orthogonality. Finally, a two-stage orthonormal regularization schedule enforces soft constraints during tokenizer training and LLM fine-tuning to maintain geometric alignment without hindering semantic adaptation. Extensive experiments on HumanML3D demonstrate that our framework achieves a 20% performance improvement over current state-of-the-art methods, validating that a unified geometric basis effectively empowers the LLM for nuanced motion reasoning.",
    "translation": "标题：GeoMotionGPT：基于大语言模型的几何对齐运动理解\n\n摘要：离散运动标记化技术近期使得大语言模型能够作为运动理解与运动-语言推理的多功能基础架构。然而，现有流程通常将运动量化与语义嵌入学习解耦，仅通过标记ID建立关联。这种方法未能有效对齐运动空间的内在几何结构与嵌入空间，从而限制了大语言模型进行精细运动推理的能力。我们认为，当两种模态共享统一的几何基础时，对齐效果最为显著。因此，我们提出了一种新型框架，该框架不再强制大语言模型从零开始重构运动标记间的复杂几何关系，而是通过对运动码本和大语言模型嵌入空间同时施加正交性约束，确保二者的关系结构自然映射。具体而言，我们采用基于Gumbel-Softmax的仅解码器量化器实现可微分训练与平衡的码本使用。为 bridging 模态间隙，我们使用稀疏投影将运动编码映射至大语言模型嵌入空间，同时保持正交特性。最后，通过两阶段正交正则化方案，在标记器训练和大语言模型微调过程中实施软约束，在维持几何对齐的同时不阻碍语义适应。在HumanML3D数据集上的大量实验表明，我们的框架相比当前最优方法实现了20%的性能提升，验证了统一几何基础能有效增强大语言模型的精细运动推理能力。",
    "url": "https://huggingface.co/papers/2601.07632",
    "arxiv_url": "https://arxiv.org/abs/2601.07632"
  }
]