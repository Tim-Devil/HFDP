[
  {
    "title": "Z-Image: An Efficient Image Generation Foundation Model with Single-Stream Diffusion Transformer",
    "summary": "The landscape of high-performance image generation models is currently dominated by proprietary systems, such as Nano Banana Pro and Seedream 4.0. Leading open-source alternatives, including Qwen-Image, Hunyuan-Image-3.0 and FLUX.2, are characterized by massive parameter counts (20B to 80B), making them impractical for inference, and fine-tuning on consumer-grade hardware. To address this gap, we propose Z-Image, an efficient 6B-parameter foundation generative model built upon a Scalable Single-Stream Diffusion Transformer (S3-DiT) architecture that challenges the \"scale-at-all-costs\" paradigm. By systematically optimizing the entire model lifecycle -- from a curated data infrastructure to a streamlined training curriculum -- we complete the full training workflow in just 314K H800 GPU hours (approx. $630K). Our few-step distillation scheme with reward post-training further yields Z-Image-Turbo, offering both sub-second inference latency on an enterprise-grade H800 GPU and compatibility with consumer-grade hardware (<16GB VRAM). Additionally, our omni-pre-training paradigm also enables efficient training of Z-Image-Edit, an editing model with impressive instruction-following capabilities. Both qualitative and quantitative experiments demonstrate that our model achieves performance comparable to or surpassing that of leading competitors across various dimensions. Most notably, Z-Image exhibits exceptional capabilities in photorealistic image generation and bilingual text rendering, delivering results that rival top-tier commercial models, thereby demonstrating that state-of-the-art results are achievable with significantly reduced computational overhead. We publicly release our code, weights, and online demo to foster the development of accessible, budget-friendly, yet state-of-the-art generative models.",
    "translation": "标题：Z-Image：一种基于单流扩散Transformer的高效图像生成基础模型\n\n摘要：当前高性能图像生成模型领域主要由专有系统主导，例如Nano Banana Pro和Seedream 4.0。领先的开源替代方案，包括Qwen-Image、Hunyuan-Image-3.0和FLUX.2，普遍具有参数量巨大（200亿至800亿）的特点，导致其在消费级硬件上进行推理和微调时面临实际困难。为弥补这一空白，我们提出了Z-Image，这是一个基于可扩展单流扩散Transformer（S3-DiT）架构构建的高效60亿参数生成基础模型，旨在挑战“不计成本追求规模”的范式。通过对整个模型生命周期——从精心构建的数据基础设施到精简的训练流程——进行系统优化，我们仅用31.4万H800 GPU小时（约合63万美元）即完成了完整的训练工作流。我们结合奖励训练后处理的少步蒸馏方案进一步产生了Z-Image-Turbo，该模型不仅在企业级H800 GPU上实现亚秒级推理延迟，同时兼容消费级硬件（显存<16GB）。此外，我们的全预训练范式也支持高效训练Z-Image-Edit，这是一个具备出色指令跟随能力的编辑模型。定性与定量实验均表明，我们的模型在多个维度上取得了与领先竞品相当或更优的性能。尤为突出的是，Z-Image在逼真图像生成和双语文本渲染方面展现出卓越能力，其生成效果可与顶级商业模型媲美，从而证明大幅降低计算开销同样能够实现最先进的结果。我们将公开代码、模型权重及在线演示，以促进可访问、低成本且具备前沿性能的生成模型的发展。",
    "url": "https://huggingface.co/papers/2511.22699",
    "arxiv_url": "https://arxiv.org/abs/2511.22699"
  },
  {
    "title": "REASONEDIT: Towards Reasoning-Enhanced Image Editing Models",
    "summary": "Recent advances in image editing models have shown remarkable progress. A common architectural design couples a multimodal large language model (MLLM) encoder with a diffusion decoder, as seen in systems such as Step1X-Edit and Qwen-Image-Edit, where the MLLM encodes both the reference image and the instruction but remains frozen during training. In this work, we demonstrate that unlocking the reasoning capabilities of MLLM can further push the boundaries of editing models. Specifically, we explore two reasoning mechanisms, thinking and reflection, which enhance instruction understanding and editing accuracy. Based on that, our proposed framework enables image editing in a thinking-editing-reflection loop: the thinking mechanism leverages the world knowledge of MLLM to interpret abstract instructions, while the reflection reviews editing results, automatically corrects unintended manipulations, and identifies the stopping round. Extensive experiments demonstrate that our reasoning approach achieves significant performance gains, with improvements of ImgEdit (+4.3%), GEdit (+4.7%), and Kris (+8.2%) when initializing our DiT from the Step1X-Edit (ReasonEdit-S), and also outperforms previous open-source methods on both GEdit and Kris when integrated with Qwen-Image-Edit (ReasonEdit-Q).",
    "translation": "标题：REASONEDIT：迈向推理增强的图像编辑模型\n\n摘要：近期图像编辑模型取得了显著进展。一种常见的架构设计将多模态大语言模型编码器与扩散解码器相结合，例如Step1X-Edit和Qwen-Image-Edit等系统，其中多模态大语言模型负责编码参考图像和编辑指令，但在训练过程中保持冻结。本研究证明，释放多模态大语言模型的推理能力能够进一步拓展编辑模型的边界。具体而言，我们探索了思维与反思两种推理机制，以增强指令理解与编辑精度。基于此，我们提出的框架实现了“思维-编辑-反思”循环的图像编辑流程：思维机制利用多模态大语言模型的世界知识解析抽象指令，而反思机制则评估编辑结果、自动修正非预期操作并确定终止轮次。大量实验表明，我们的推理方法在性能上取得显著提升：当基于Step1X-Edit初始化我们的DiT模型时（ReasonEdit-S），在ImgEdit（+4.3%）、GEdit（+4.7%）和Kris（+8.2%）指标上均实现改进；当与Qwen-Image-Edit结合时（ReasonEdit-Q），在GEdit和Kris指标上也超越了以往开源方法。",
    "url": "https://huggingface.co/papers/2511.22625",
    "arxiv_url": "https://arxiv.org/abs/2511.22625"
  },
  {
    "title": "AnyTalker: Scaling Multi-Person Talking Video Generation with Interactivity Refinement",
    "summary": "Recently, multi-person video generation has started to gain prominence. While a few preliminary works have explored audio-driven multi-person talking video generation, they often face challenges due to the high costs of diverse multi-person data collection and the difficulty of driving multiple identities with coherent interactivity. To address these challenges, we propose AnyTalker, a multi-person generation framework that features an extensible multi-stream processing architecture. Specifically, we extend Diffusion Transformer's attention block with a novel identity-aware attention mechanism that iteratively processes identity-audio pairs, allowing arbitrary scaling of drivable identities. Besides, training multi-person generative models demands massive multi-person data. Our proposed training pipeline depends solely on single-person videos to learn multi-person speaking patterns and refines interactivity with only a few real multi-person clips. Furthermore, we contribute a targeted metric and dataset designed to evaluate the naturalness and interactivity of the generated multi-person videos. Extensive experiments demonstrate that AnyTalker achieves remarkable lip synchronization, visual quality, and natural interactivity, striking a favorable balance between data costs and identity scalability.",
    "translation": "标题：AnyTalker：通过交互性优化实现多人对话视频生成的可扩展化\n\n摘要：近年来，多人视频生成技术开始受到广泛关注。尽管已有初步研究探索了音频驱动的多人对话视频生成，但由于多样化多人数据采集的高成本以及驱动多个身份实现连贯交互的困难，这些方法常面临挑战。为应对这些挑战，本文提出AnyTalker——一个具备可扩展多流处理架构的多人视频生成框架。具体而言，我们通过一种新颖的身份感知注意力机制扩展了扩散变换器的注意力模块，该机制能迭代处理身份-音频对，从而实现可驱动身份数量的任意扩展。此外，训练多人生成模型需要海量多人数据。我们提出的训练流程仅依赖单人视频学习多人说话模式，并仅需少量真实多人视频片段即可优化交互表现。同时，我们构建了专用评估指标与数据集，用于衡量生成多人视频的自然度与交互质量。大量实验表明，AnyTalker在唇部同步、视觉质量和自然交互性方面表现卓越，在数据成本与身份可扩展性之间实现了良好平衡。",
    "url": "https://huggingface.co/papers/2511.23475",
    "arxiv_url": "https://arxiv.org/abs/2511.23475"
  },
  {
    "title": "Vision Bridge Transformer at Scale",
    "summary": "We introduce Vision Bridge Transformer (ViBT), a large-scale instantiation of Brownian Bridge Models designed for conditional generation. Unlike traditional diffusion models that transform noise into data, Bridge Models directly model the trajectory between inputs and outputs, creating an efficient data-to-data translation paradigm. By scaling these models to 20B and 1.3B parameters, we demonstrate their effectiveness for image and video translation tasks. To support this scale, we adopt a Transformer architecture and propose a variance-stabilized velocity-matching objective for robust training. Together, these advances highlight the power of scaling Bridge Models for instruction-based image editing and complex video translation.",
    "translation": "标题：大规模视觉桥接变换器\n\n摘要：本文提出视觉桥接变换器（ViBT），这是一种专为条件生成设计的大规模布朗桥模型实现。与传统扩散模型将噪声转化为数据不同，桥接模型直接建模输入与输出之间的轨迹，构建了高效的数据到数据转换范式。通过将模型参数量扩展至200亿和13亿，我们验证了其在图像与视频转换任务中的有效性。为支撑此规模，我们采用变换器架构，并提出方差稳定的速度匹配目标函数以实现鲁棒训练。这些进展共同揭示了桥接模型在基于指令的图像编辑和复杂视频转换任务中通过规模化所展现的强大能力。",
    "url": "https://huggingface.co/papers/2511.23199",
    "arxiv_url": "https://arxiv.org/abs/2511.23199"
  },
  {
    "title": "DeepSeekMath-V2: Towards Self-Verifiable Mathematical Reasoning",
    "summary": "Large language models have made significant progress in mathematical reasoning, which serves as an important testbed for AI and could impact scientific research if further advanced. By scaling reasoning with reinforcement learning that rewards correct final answers, LLMs have improved from poor performance to saturating quantitative reasoning competitions like AIME and HMMT in one year. However, this approach faces fundamental limitations. Pursuing higher final answer accuracy doesn't address a key issue: correct answers don't guarantee correct reasoning. Moreover, many mathematical tasks like theorem proving require rigorous step-by-step derivation rather than numerical answers, making final answer rewards inapplicable. To push the limits of deep reasoning, we believe it is necessary to verify the comprehensiveness and rigor of mathematical reasoning. Self-verification is particularly important for scaling test-time compute, especially for open problems without known solutions. Towards self-verifiable mathematical reasoning, we investigate how to train an accurate and faithful LLM-based verifier for theorem proving. We then train a proof generator using the verifier as the reward model, and incentivize the generator to identify and resolve as many issues as possible in their own proofs before finalizing them. To maintain the generation-verification gap as the generator becomes stronger, we propose to scale verification compute to automatically label new hard-to-verify proofs, creating training data to further improve the verifier. Our resulting model, DeepSeekMath-V2, demonstrates strong theorem-proving capabilities, achieving gold-level scores on IMO 2025 and CMO 2024 and a near-perfect 118/120 on Putnam 2024 with scaled test-time compute.",
    "translation": "标题：DeepSeekMath-V2：迈向可自我验证的数学推理\n\n摘要：大型语言模型在数学推理方面已取得显著进展，这不仅是人工智能的重要测试平台，其进一步发展还可能对科学研究产生影响。通过采用强化学习对最终正确答案进行奖励以扩展推理能力，大型语言模型在一年内从表现不佳提升至在AIME、HMMT等定量推理竞赛中达到饱和水平。然而，该方法面临根本性局限。追求更高的最终答案准确率并未解决一个关键问题：正确答案并不能保证推理过程的正确性。此外，许多数学任务（如定理证明）需要严谨的逐步推导而非数值答案，这使得基于最终答案的奖励机制无法适用。为突破深度推理的极限，我们认为必须对数学推理的完备性与严谨性进行验证。自我验证对于扩展测试时计算资源尤为重要，特别是针对尚无已知解的开放性问题。为实现可自我验证的数学推理，我们研究了如何训练一个基于大型语言模型的精确且可靠的定理证明验证器。随后，我们以该验证器作为奖励模型训练证明生成器，并激励生成器在最终确定证明前尽可能识别并解决其自身证明中的问题。为在生成器能力增强时保持生成与验证之间的差距，我们提出扩展验证计算资源以自动标注新的难验证证明，从而创建训练数据以持续改进验证器。我们最终得到的模型DeepSeekMath-V2展现出强大的定理证明能力，在扩展测试时计算资源下，于IMO 2025和CMO 2024中获得金奖级别分数，并在Putnam 2024中取得接近满分的118/120分。",
    "url": "https://huggingface.co/papers/2511.22570",
    "arxiv_url": "https://arxiv.org/abs/2511.22570"
  },
  {
    "title": "Architecture Decoupling Is Not All You Need For Unified Multimodal Model",
    "summary": "Unified multimodal models for image generation and understanding represent a significant step toward AGI and have attracted widespread attention from researchers. The main challenge of this task lies in the difficulty in establishing an optimal training paradigm due to inherent conflicting targets in understanding and generation tasks. To alleviate these conflicts and pursue higher performance, many researchers adopt varying degrees of model decoupling (e.g., Double image encoders, MOE/MOT architecture, or frozen MLLM). However, excessive model decoupling can lead to the loss of interleave generation ability, undermining the original intent of unified models. In this work, we aim to explore how to mitigate task conflicts without resorting to model decoupling. Firstly, we analyze why decoupling alleviates conflicts by studying the cross-modal attention behavior of models. We observe that model decoupling essentially drives models toward task-specific multimodal interaction patterns, as seen in Qwen-VL and HunyuanImage, and that the more thorough the decoupling, the more consistent the behavior becomes. Motivated by this observation, we propose Attention Interaction Alignment (AIA) loss, which explicitly learns Task-Specific multimodal interaction patterns during training. To demonstrate the generalizability of our AIA loss, we apply it to Emu3 and Janus-Pro during SFT and post-training stage respectively. Without bells and whistles, AIA not only refines cross-modal attention patterns, but also boosts both generation and understanding performance.",
    "translation": "标题：架构解耦并非统一多模态模型的全部所需\n\n摘要：用于图像生成与理解的统一多模态模型是迈向通用人工智能的重要一步，已引起研究者的广泛关注。该任务的主要挑战在于，由于理解与生成任务内在的目标冲突，难以建立最优的训练范式。为缓解这些冲突并追求更高性能，许多研究者采用不同程度的模型解耦策略（例如双图像编码器、MOE/MOT架构或冻结多模态大语言模型）。然而，过度的模型解耦可能导致交错生成能力的丧失，背离统一模型的初衷。本研究旨在探索如何在不依赖模型解耦的情况下缓解任务冲突。首先，我们通过分析模型的跨模态注意力行为，探究解耦为何能缓解冲突。我们发现，模型解耦本质上驱动模型形成任务特定的多模态交互模式（如Qwen-VL与HunyuanImage所示），且解耦越彻底，行为一致性越高。受此启发，我们提出注意力交互对齐损失函数，在训练中显式学习任务特定的多模态交互模式。为验证该损失函数的泛化性，我们分别将其应用于Emu3和Janus-Pro模型的监督微调与后训练阶段。实验表明，该损失函数不仅优化了跨模态注意力模式，同时显著提升了生成与理解任务的性能。",
    "url": "https://huggingface.co/papers/2511.22663",
    "arxiv_url": "https://arxiv.org/abs/2511.22663"
  },
  {
    "title": "CaptionQA: Is Your Caption as Useful as the Image Itself?",
    "summary": "Image captions serve as efficient surrogates for visual content in multimodal systems such as retrieval, recommendation, and multi-step agentic inference pipelines. Yet current evaluation practices miss a fundamental question: Can captions stand-in for images in real downstream tasks? We propose a utility-based benchmark, CaptionQA, to evaluate model-generated captions, where caption quality is measured by how well it supports downstream tasks. CaptionQA is an extensible domain-dependent benchmark covering 4 domains--Natural, Document, E-commerce, and Embodied AI--each with fine-grained taxonomies (25 top-level and 69 subcategories) that identify useful information for domain-specific tasks. CaptionQA builds 33,027 densely annotated multiple-choice questions (50.3 per image on average) that explicitly require visual information to answer, providing a comprehensive probe of caption utility. In our evaluation protocol, an LLM answers these questions using captions alone, directly measuring whether captions preserve image-level utility and are utilizable by a downstream LLM. Evaluating state-of-the-art MLLMs reveals substantial gaps between the image and its caption utility. Notably, models nearly identical on traditional image-QA benchmarks lower by up to 32% in caption utility. We release CaptionQA along with an open-source pipeline for extension to new domains. The code is available at https://github.com/bronyayang/CaptionQA.",
    "translation": "标题：CaptionQA：图像描述能否替代图像本身的实际效用？\n\n摘要：在多模态系统（如检索、推荐及多步智能体推理流程）中，图像描述常作为视觉内容的高效替代。然而，现有评估方法忽略了一个根本问题：描述文本能否在实际下游任务中有效替代图像？本文提出基于实用性的基准测试CaptionQA，通过描述文本对下游任务的支持程度来评估模型生成描述的质量。CaptionQA是一个可扩展的领域相关基准，涵盖自然图像、文档、电子商务和具身人工智能四大领域，每个领域均包含细粒度分类体系（25个顶层类别与69个子类别），以识别领域特定任务所需的关键信息。该基准构建了33,027道密集标注的多选题（平均每幅图像对应50.3题），这些问题明确需要视觉信息进行解答，从而全面检验描述文本的实用性。在我们的评估框架中，大型语言模型仅依据描述文本回答问题，直接衡量描述是否保留图像层面的信息效用以及能否被下游大型语言模型有效利用。通过对前沿多模态大模型的评估，我们发现图像与其描述文本的效用存在显著差距：在传统图像问答基准上表现相近的模型，其描述文本的效用评分最大可降低32%。我们开源CaptionQA基准及可扩展至新领域的工具链，代码发布于https://github.com/bronyayang/CaptionQA。",
    "url": "https://huggingface.co/papers/2511.21025",
    "arxiv_url": "https://arxiv.org/abs/2511.21025"
  },
  {
    "title": "DualVLA: Building a Generalizable Embodied Agent via Partial Decoupling of Reasoning and Action",
    "summary": "To build a generalizable Vision-Language-Action (VLA) model with strong reasoning ability, a common strategy is to first train a specialist VLA on robot demonstrations to acquire reliable manipulation skills, and then incorporate mixed annotated robot data together with multimodal data to restore broader reasoning capabilities. However, we observe that the resulting reasoning VLA often suffers from degraded action performance compared to the specialist model before fine-tuning, a phenomenon we refer to as action degeneration. To address this issue, we propose DualVLA, which enhances action performance through carefully designed post-training while still preserving reasoning capability. We first introduce a dual-layer data pruning method that removes redundant embodied reasoning, preventing it from adversely influencing action learning. To further strengthen action generation, we design a dual-teacher adaptive distillation strategy that assigns different supervision signals to different data domains while maintaining reasoning ability. To fill the evaluation gap for generalist VLAs, we also propose VLA Score, which decouples VLA capability into reasoning, intention, action, and alignment dimensions for a more fine-grained assessment. Experiments show that DualVLA achieves an average success rate of 61.0 in SimplerEnv and an average score of 65.4 across eight competitive multimodal benchmarks, demonstrating a stronger balance between precise action execution and multimodal understanding. Project Website: https://costaliya.github.io/DualVLA/.",
    "translation": "标题：DualVLA：通过部分解耦推理与行动构建可泛化的具身智能体\n\n摘要：为构建具有强大推理能力的可泛化视觉-语言-行动（VLA）模型，常见策略是先在机器人演示数据上训练专用VLA模型以获取可靠操作技能，再融合多源标注的机器人数据与多模态数据以恢复广泛推理能力。然而，我们发现经此流程得到的推理型VLA模型往往会出现动作性能相较于微调前的专用模型显著下降的现象，即“动作退化”。为解决该问题，我们提出DualVLA模型，通过精心设计的后训练方法提升动作性能，同时保持推理能力。我们首先提出双层数据筛选方法，剔除冗余的具身推理数据，防止其对动作学习产生负面影响。为进一步强化动作生成能力，我们设计了双教师自适应蒸馏策略，针对不同数据域分配差异化的监督信号，同时维持模型推理性能。为填补通用型VLA模型的评估空白，我们还提出VLA综合评分体系，将VLA能力解耦为推理、意图理解、动作执行与多模态对齐四个维度进行细粒度评估。实验表明，DualVLA在SimplerEnv环境中取得61.0%的平均成功率，并在八项具有竞争力的多模态基准测试中获得65.4的平均分，展现出在精确动作执行与多模态理解之间更优的平衡能力。项目网站：https://costaliya.github.io/DualVLA/。",
    "url": "https://huggingface.co/papers/2511.22134",
    "arxiv_url": "https://arxiv.org/abs/2511.22134"
  },
  {
    "title": "DiP: Taming Diffusion Models in Pixel Space",
    "summary": "Diffusion models face a fundamental trade-off between generation quality and computational efficiency. Latent Diffusion Models (LDMs) offer an efficient solution but suffer from potential information loss and non-end-to-end training. In contrast, existing pixel space models bypass VAEs but are computationally prohibitive for high-resolution synthesis. To resolve this dilemma, we propose DiP, an efficient pixel space diffusion framework. DiP decouples generation into a global and a local stage: a Diffusion Transformer (DiT) backbone operates on large patches for efficient global structure construction, while a co-trained lightweight Patch Detailer Head leverages contextual features to restore fine-grained local details. This synergistic design achieves computational efficiency comparable to LDMs without relying on a VAE. DiP is accomplished with up to 10times faster inference speeds than previous method while increasing the total number of parameters by only 0.3%, and achieves an 1.79 FID score on ImageNet 256times256.",
    "translation": "标题：DiP：像素空间扩散模型的高效调控框架\n\n摘要：扩散模型在生成质量与计算效率之间存在固有权衡。潜在扩散模型（LDMs）虽提供高效解决方案，但存在潜在信息丢失与非端到端训练的缺陷。相比之下，现有像素空间模型虽绕过了变分自编码器，却因计算成本过高而难以实现高分辨率合成。为解决这一困境，我们提出DiP——一种高效的像素空间扩散框架。DiP将生成过程解耦为全局与局部两阶段：扩散Transformer（DiT）主干网络通过大尺度图像块操作实现高效的全局结构构建，同时协同训练的轻量化局部细节增强器利用上下文特征恢复细粒度细节。这种协同设计在不依赖变分自编码器的情况下实现了与LDMs相当的计算效率。DiP在仅增加0.3%参数总量的前提下，推理速度较现有方法提升最高达10倍，并在ImageNet 256×256数据集上取得了1.79的FID分数。",
    "url": "https://huggingface.co/papers/2511.18822",
    "arxiv_url": "https://arxiv.org/abs/2511.18822"
  },
  {
    "title": "Every Token Counts: Generalizing 16M Ultra-Long Context in Large Language Models",
    "summary": "This work explores the challenge of building ``Machines that Can Remember'', framing long-term memory as the problem of efficient ultra-long context modeling. We argue that this requires three key properties: sparsity, random-access flexibility, and length generalization. To address ultra-long-context modeling, we leverage Hierarchical Sparse Attention (HSA), a novel attention mechanism that satisfies all three properties. We integrate HSA into Transformers to build HSA-UltraLong, which is an 8B-parameter MoE model trained on over 8 trillion tokens and is rigorously evaluated on different tasks with in-domain and out-of-domain context lengths to demonstrate its capability in handling ultra-long contexts. Results show that our model performs comparably to full-attention baselines on in-domain lengths while achieving over 90\\% accuracy on most in-context retrieval tasks with contexts up to 16M. This report outlines our experimental insights and open problems, contributing a foundation for future research in ultra-long context modeling.",
    "translation": "标题：每个标记都重要：大语言模型中1600万超长上下文的泛化能力研究\n\n摘要：本研究探讨构建“具备记忆能力的机器”所面临的挑战，将长期记忆问题定义为高效超长上下文建模任务。我们认为这需要具备三个关键特性：稀疏性、随机访问灵活性以及长度泛化能力。针对超长上下文建模问题，我们提出分层稀疏注意力机制——一种同时满足上述三个特性的新型注意力机制。通过将HSA集成至Transformer架构中，我们构建了HSA-UltraLong模型。该模型为包含80亿参数的混合专家模型，基于超过8万亿标记进行训练，并在领域内与领域外不同长度上下文任务中接受严格评估，以验证其处理超长上下文的能力。实验结果表明：在领域内长度任务中，本模型性能与全注意力基线模型相当；在上下文长度达1600万标记的情境检索任务中，大多数任务准确率超过90%。本报告系统阐述了实验发现与待解难题，为超长上下文建模的未来研究奠定了理论基础。",
    "url": "https://huggingface.co/papers/2511.23319",
    "arxiv_url": "https://arxiv.org/abs/2511.23319"
  },
  {
    "title": "Adversarial Flow Models",
    "summary": "We present adversarial flow models, a class of generative models that unifies adversarial models and flow models. Our method supports native one-step or multi-step generation and is trained using the adversarial objective. Unlike traditional GANs, where the generator learns an arbitrary transport plan between the noise and the data distributions, our generator learns a deterministic noise-to-data mapping, which is the same optimal transport as in flow-matching models. This significantly stabilizes adversarial training. Also, unlike consistency-based methods, our model directly learns one-step or few-step generation without needing to learn the intermediate timesteps of the probability flow for propagation. This saves model capacity, reduces training iterations, and avoids error accumulation. Under the same 1NFE setting on ImageNet-256px, our B/2 model approaches the performance of consistency-based XL/2 models, while our XL/2 model creates a new best FID of 2.38. We additionally show the possibility of end-to-end training of 56-layer and 112-layer models through depth repetition without any intermediate supervision, and achieve FIDs of 2.08 and 1.94 using a single forward pass, surpassing their 2NFE and 4NFE counterparts.",
    "translation": "标题：对抗流模型\n\n摘要：本文提出对抗流模型，这是一种将对抗模型与流模型相统一的生成模型类别。该方法支持原生单步或多步生成，并采用对抗目标进行训练。与传统生成对抗网络（GAN）中生成器学习噪声分布与数据分布之间的任意传输方案不同，我们的生成器学习确定性的噪声到数据映射，该映射与流匹配模型中的最优传输方案一致。这显著提升了对抗训练的稳定性。同时，与基于一致性的方法相比，我们的模型直接学习单步或少步生成，无需学习概率流传播的中间时间步。这节省了模型容量，减少了训练迭代次数，并避免了误差累积。在ImageNet-256px数据集相同的1NFE设置下，我们的B/2模型性能接近基于一致性的XL/2模型，而我们的XL/2模型创造了2.38的最新最佳FID指标。此外，我们展示了通过深度重复实现56层和112层模型端到端训练的可能性，在单次前向传播中分别达到2.08和1.94的FID，超越了对应的2NFE和4NFE模型性能。",
    "url": "https://huggingface.co/papers/2511.22475",
    "arxiv_url": "https://arxiv.org/abs/2511.22475"
  },
  {
    "title": "Decoupled DMD: CFG Augmentation as the Spear, Distribution Matching as the Shield",
    "summary": "Diffusion model distillation has emerged as a powerful technique for creating efficient few-step and single-step generators. Among these, Distribution Matching Distillation (DMD) and its variants stand out for their impressive performance, which is widely attributed to their core mechanism of matching the student's output distribution to that of a pre-trained teacher model. In this work, we challenge this conventional understanding. Through a rigorous decomposition of the DMD training objective, we reveal that in complex tasks like text-to-image generation, where CFG is typically required for desirable few-step performance, the primary driver of few-step distillation is not distribution matching, but a previously overlooked component we identify as CFG Augmentation (CA). We demonstrate that this term acts as the core ``engine'' of distillation, while the Distribution Matching (DM) term functions as a ``regularizer'' that ensures training stability and mitigates artifacts. We further validate this decoupling by demonstrating that while the DM term is a highly effective regularizer, it is not unique; simpler non-parametric constraints or GAN-based objectives can serve the same stabilizing function, albeit with different trade-offs. This decoupling of labor motivates a more principled analysis of the properties of both terms, leading to a more systematic and in-depth understanding. This new understanding further enables us to propose principled modifications to the distillation process, such as decoupling the noise schedules for the engine and the regularizer, leading to further performance gains. Notably, our method has been adopted by the Z-Image ( https://github.com/Tongyi-MAI/Z-Image ) project to develop a top-tier 8-step image generation model, empirically validating the generalization and robustness of our findings.",
    "translation": "标题：解耦DMD：以CFG增强为矛，以分布匹配为盾\n\n摘要：扩散模型蒸馏已成为创建高效少步与单步生成器的强大技术。其中，分布匹配蒸馏（DMD）及其变体因卓越性能而备受瞩目，其核心机制通常被归结为将学生模型的输出分布与预训练教师模型的分布相匹配。本研究挑战了这一传统认知。通过对DMD训练目标的严格分解，我们揭示在文本到图像生成等复杂任务中（通常需要CFG以获得理想的少步生成性能），少步蒸馏的主要驱动力并非分布匹配，而是一个先前被忽视的、我们定义为CFG增强（CA）的组成部分。我们证明该组件充当蒸馏的核心“引擎”，而分布匹配（DM）项则作为“正则化器”确保训练稳定性并减少伪影。我们进一步通过实验验证：虽然DM项是高效的正则化器，但其作用并非唯一；更简单的非参数约束或基于GAN的目标函数也能实现相同的稳定功能，尽管存在不同的权衡。这种作用解耦促使我们对两项组件的性质进行更原理性的分析，从而获得更系统深入的理解。基于这一新认知，我们进一步提出对蒸馏过程的原理性改进，例如为引擎和正则化器解耦噪声调度策略，从而实现了额外的性能提升。值得注意的是，我们的方法已被Z-Image（https://github.com/Tongyi-MAI/Z-Image）项目采用，用于开发顶尖的8步图像生成模型，这从实证角度验证了我们发现的普适性与鲁棒性。",
    "url": "https://huggingface.co/papers/2511.22677",
    "arxiv_url": "https://arxiv.org/abs/2511.22677"
  },
  {
    "title": "RefineBench: Evaluating Refinement Capability of Language Models via Checklists",
    "summary": "Can language models (LMs) self-refine their own responses? This question is increasingly relevant as a wide range of real-world user interactions involve refinement requests. However, prior studies have largely tested LMs' refinement abilities on verifiable tasks such as competition math or symbolic reasoning with simplified scaffolds, whereas users often pose open-ended queries and provide varying degrees of feedback on what they desire. The recent advent of reasoning models that exhibit self-reflection patterns in their chains-of-thought further motivates this question. To analyze this, we introduce RefineBench, a benchmark of 1,000 challenging problems across 11 domains paired with a checklist-based evaluation framework. We evaluate two refinement modes: (1) guided refinement, where an LM is provided natural language feedback, and (2) self-refinement, where LMs attempt to improve without guidance. In the self-refinement setting, even frontier LMs such as Gemini 2.5 Pro and GPT-5 achieve modest baseline scores of 31.3% and 29.1%, respectively, and most models fail to consistently improve across iterations (e.g., Gemini-2.5-Pro gains only +1.8%, while DeepSeek-R1 declines by -0.1%). By contrast, in guided refinement, both proprietary LMs and large open-weight LMs (>70B) can leverage targeted feedback to refine responses to near-perfect levels within five turns. These findings suggest that frontier LMs require breakthroughs to self-refine their incorrect responses, and that RefineBench provides a valuable testbed for tracking progress.",
    "translation": "标题：RefineBench：基于检查表的语言模型精炼能力评估框架\n\n摘要：语言模型能否对其自身生成的回答进行自我精炼？随着现实世界中大量用户交互涉及精炼需求，这一问题日益凸显。然而，现有研究主要在可验证任务（如竞赛数学或带有简化框架的符号推理）上测试语言模型的精炼能力，而用户往往提出开放式查询，并对其期望目标提供不同程度的反馈。近期涌现的推理模型在思维链中展现出自我反思模式，进一步推动了该问题的探讨。为此，我们提出RefineBench：一个包含11个领域共1000个挑战性问题的基准测试集，并配套基于检查表的评估框架。我们评估两种精炼模式：（1）引导式精炼：向语言模型提供自然语言反馈；（2）自我精炼：语言模型在无引导情况下尝试改进回答。在自我精炼场景中，即使如Gemini 2.5 Pro和GPT-5等前沿模型也仅分别获得31.3%和29.1%的基准分数，且多数模型无法在迭代中持续改进（例如Gemini-2.5-Pro仅提升+1.8%，而DeepSeek-R1反而下降-0.1%）。相比之下，在引导式精炼中，无论是专有模型还是大型开源权重模型（>700亿参数），均能利用定向反馈在五轮对话内将回答精炼至接近完美的水平。这些发现表明，前沿语言模型要实现对其错误回答的自我精炼仍需突破性进展，而RefineBench为追踪该领域进展提供了重要测试平台。",
    "url": "https://huggingface.co/papers/2511.22173",
    "arxiv_url": "https://arxiv.org/abs/2511.22173"
  },
  {
    "title": "Nemotron-Flash: Towards Latency-Optimal Hybrid Small Language Models",
    "summary": "Efficient deployment of small language models (SLMs) is essential for numerous real-world applications with stringent latency constraints. While previous work on SLM design has primarily focused on reducing the number of parameters to achieve parameter-optimal SLMs, parameter efficiency does not necessarily translate into proportional real-device speed-ups. This work aims to identify the key determinants of SLMs' real-device latency and offer generalizable principles and methodologies for SLM design and training when real-device latency is the primary consideration. Specifically, we identify two central architectural factors: depth-width ratios and operator choices. The former is crucial for small-batch-size latency, while the latter affects both latency and large-batch-size throughput. In light of this, we first study latency-optimal depth-width ratios, with the key finding that although deep-thin models generally achieve better accuracy under the same parameter budget, they may not lie on the accuracy-latency trade-off frontier. Next, we explore emerging efficient attention alternatives to evaluate their potential as candidate building operators. Using the identified promising operators, we construct an evolutionary search framework to automatically discover latency-optimal combinations of these operators within hybrid SLMs, thereby advancing the accuracy-latency frontier. In addition to architectural improvements, we further enhance SLM training using a weight normalization technique that enables more effective weight updates and improves final convergence. Combining these methods, we introduce a new family of hybrid SLMs, called Nemotron-Flash, which significantly advances the accuracy-efficiency frontier of state-of-the-art SLMs, e.g., achieving over +5.5% average accuracy, 1.3x/1.9x lower latency, and 18.7x/45.6x higher throughput compared to Qwen3-1.7B/0.6B, respectively.",
    "translation": "标题：Nemotron-Flash：面向延迟最优的混合式小语言模型\n\n摘要：在众多具有严格延迟约束的实际应用中，高效部署小语言模型至关重要。以往关于小语言模型设计的研究主要集中于减少参数量以实现参数最优模型，但参数效率的提升未必能带来实际设备上成比例的速度增益。本研究旨在识别影响小语言模型实际设备延迟的关键因素，并为以实际延迟为首要考虑的小语言模型设计与训练提供可推广的原则与方法。具体而言，我们聚焦两个核心架构因素：深度-宽度比例与算子选择。前者对小批量处理的延迟至关重要，而后者同时影响延迟与大批量处理的吞吐量。基于此，我们首先研究了延迟最优的深度-宽度比例，关键发现是：尽管在相同参数量预算下，深窄型模型通常能获得更优精度，但其可能并未处于精度-延迟权衡的前沿边界。接着，我们探索了新兴的高效注意力替代方案，以评估其作为候选构建算子的潜力。利用识别出的潜力算子，我们构建了一个进化搜索框架，以在混合式小语言模型中自动发现这些算子的延迟最优组合，从而推进精度-延迟前沿边界。除架构改进外，我们进一步通过权重归一化技术增强小语言模型的训练，该技术能实现更有效的权重更新并提升最终收敛效果。综合这些方法，我们提出了名为Nemotron-Flash的新型混合式小语言模型系列。该系列显著推进了当前先进小语言模型的精度-效率边界，例如：相较于Qwen3-1.7B/0.6B模型，Nemotron-Flash在平均精度上提升超过5.5%，延迟降低至1.3倍/1.9倍，吞吐量提升至18.7倍/45.6倍。",
    "url": "https://huggingface.co/papers/2511.18890",
    "arxiv_url": "https://arxiv.org/abs/2511.18890"
  },
  {
    "title": "Captain Safari: A World Engine",
    "summary": "World engines aim to synthesize long, 3D-consistent videos that support interactive exploration of a scene under user-controlled camera motion. However, existing systems struggle under aggressive 6-DoF trajectories and complex outdoor layouts: they lose long-range geometric coherence, deviate from the target path, or collapse into overly conservative motion. To this end, we introduce Captain Safari, a pose-conditioned world engine that generates videos by retrieving from a persistent world memory. Given a camera path, our method maintains a dynamic local memory and uses a retriever to fetch pose-aligned world tokens, which then condition video generation along the trajectory. This design enables the model to maintain stable 3D structure while accurately executing challenging camera maneuvers. To evaluate this setting, we curate OpenSafari, a new in-the-wild FPV dataset containing high-dynamic drone videos with verified camera trajectories, constructed through a multi-stage geometric and kinematic validation pipeline. Across video quality, 3D consistency, and trajectory following, Captain Safari substantially outperforms state-of-the-art camera-controlled generators. It reduces MEt3R from 0.3703 to 0.3690, improves AUC@30 from 0.181 to 0.200, and yields substantially lower FVD than all camera-controlled baselines. More importantly, in a 50-participant, 5-way human study where annotators select the best result among five anonymized models, 67.6% of preferences favor our method across all axes. Our results demonstrate that pose-conditioned world memory is a powerful mechanism for long-horizon, controllable video generation and provide OpenSafari as a challenging new benchmark for future world-engine research.",
    "translation": "标题：Captain Safari：一种世界引擎系统\n\n摘要：世界引擎旨在合成支持用户控制相机运动下场景交互式探索的长时、三维一致视频。然而，现有系统在剧烈六自由度轨迹和复杂户外场景中表现欠佳：它们会丧失长程几何一致性、偏离目标路径或退化为过度保守的运动模式。为此，我们提出Captain Safari——一种基于位姿条件的世界引擎，通过从持久化世界记忆中检索来生成视频。给定相机路径，我们的方法维护动态局部记忆库，并利用检索器获取位姿对齐的世界标记，这些标记进而沿轨迹条件化视频生成。该设计使模型能在精确执行复杂相机运动的同时保持稳定的三维结构。为评估该设定，我们构建了OpenSafari数据集，这是一个包含经过多阶段几何与运动学验证流程校准的无人机高动态第一视角视频的野外实测数据集。在视频质量、三维一致性与轨迹跟随性方面，Captain Safari显著优于当前最先进的相机控制生成模型：将MEt3R指标从0.3703降至0.3690，将AUC@30从0.181提升至0.200，且FVD值远低于所有相机控制基线模型。更重要的是，在50人参与的五盲选人类评估中，注释者在五个匿名模型结果中选择最佳输出时，67.6%的偏好指向我们的方法。实验结果表明，位姿条件化世界记忆是实现长时序可控视频生成的有效机制，同时OpenSafari数据集为未来世界引擎研究提供了具有挑战性的新基准。",
    "url": "https://huggingface.co/papers/2511.22815",
    "arxiv_url": "https://arxiv.org/abs/2511.22815"
  },
  {
    "title": "World in a Frame: Understanding Culture Mixing as a New Challenge for Vision-Language Models",
    "summary": "In a globalized world, cultural elements from diverse origins frequently appear together within a single visual scene. We refer to these as culture mixing scenarios, yet how Large Vision-Language Models (LVLMs) perceive them remains underexplored. We investigate culture mixing as a critical challenge for LVLMs and examine how current models behave when cultural items from multiple regions appear together. To systematically analyze these behaviors, we construct CultureMix, a food Visual Question Answering (VQA) benchmark with 23k diffusion-generated, human-verified culture mixing images across four subtasks: (1) food-only, (2) food+food, (3) food+background, and (4) food+food+background. Evaluating 10 LVLMs, we find consistent failures to preserve individual cultural identities in mixed settings. Models show strong background reliance, with accuracy dropping 14% when cultural backgrounds are added to food-only baselines, and they produce inconsistent predictions for identical foods across different contexts. To address these limitations, we explore three robustness strategies. We find supervised fine-tuning using a diverse culture mixing dataset substantially improve model consistency and reduce background sensitivity. We call for increased attention to culture mixing scenarios as a critical step toward developing LVLMs capable of operating reliably in culturally diverse real-world environments.",
    "translation": "标题：框架中的世界：理解文化混合作为视觉语言模型的新挑战\n\n摘要：在全球化背景下，源自不同文化的元素频繁共存于单一视觉场景中。我们将此类现象定义为文化混合场景，然而大型视觉语言模型（LVLMs）如何感知这些场景仍缺乏深入研究。本文探讨文化混合作为LVLMs面临的关键挑战，并检验当多地域文化元素同时出现时现有模型的表现。为系统分析模型行为，我们构建了CultureMix基准数据集——一个包含2.3万张扩散生成、人工核验的文化混合图像的食物视觉问答（VQA）评测集，涵盖四个子任务：（1）纯食物、（2）食物+食物、（3）食物+背景、（4）食物+食物+背景。通过对10个LVLMs的评估，发现模型在混合场景中普遍无法保持个体文化特征：表现出强烈的背景依赖性（添加文化背景使纯食物基线准确率下降14%），且对相同食物在不同语境中产生不一致的预测。为应对这些局限，我们探索了三种鲁棒性增强策略。研究表明，使用多样化文化混合数据集进行监督微调能显著提升模型一致性并降低背景敏感性。我们呼吁学界重视文化混合场景研究，将其作为开发能够可靠运用于多元文化现实环境的LVLMs的关键步骤。",
    "url": "https://huggingface.co/papers/2511.22787",
    "arxiv_url": "https://arxiv.org/abs/2511.22787"
  },
  {
    "title": "The Collapse of Patches",
    "summary": "Observing certain patches in an image reduces the uncertainty of others. Their realization lowers the distribution entropy of each remaining patch feature, analogous to collapsing a particle's wave function in quantum mechanics. This phenomenon can intuitively be called patch collapse. To identify which patches are most relied on during a target region's collapse, we learn an autoencoder that softly selects a subset of patches to reconstruct each target patch. Graphing these learned dependencies for each patch's PageRank score reveals the optimal patch order to realize an image. We show that respecting this order benefits various masked image modeling methods. First, autoregressive image generation can be boosted by retraining the state-of-the-art model MAR. Next, we introduce a new setup for image classification by exposing Vision Transformers only to high-rank patches in the collapse order. Seeing 22\\% of such patches is sufficient to achieve high accuracy. With these experiments, we propose patch collapse as a novel image modeling perspective that promotes vision efficiency. Our project is available at https://github.com/wguo-ai/CoP .",
    "translation": "标题：图像块坍缩现象研究\n\n摘要：观测图像中的特定块会降低其他块的不确定性，其实质化过程会缩减各剩余块特征的分布熵，类似于量子力学中粒子波函数的坍缩现象。该效应可直观地称为图像块坍缩。为识别目标区域坍缩过程中最依赖的关键块，我们训练了一种自编码器，通过软选择机制筛选块子集以重建每个目标块。通过计算各块在依赖关系图中的PageRank分值，可推导出图像实质化的最优块序列。实验表明，遵循该序列能有效提升多种掩码图像建模方法的性能：首先，通过重新训练前沿模型MAR可增强自回归图像生成效果；其次，我们提出一种基于坍缩序列的新型图像分类范式，仅向视觉Transformer暴露高排序块即可实现高效识别——仅需观测22%的高排序块即可达到高精度分类。基于上述实验，我们提出将图像块坍缩作为一种促进视觉效率的新建模视角。项目代码已开源：https://github.com/wguo-ai/CoP。",
    "url": "https://huggingface.co/papers/2511.22281",
    "arxiv_url": "https://arxiv.org/abs/2511.22281"
  },
  {
    "title": "OralGPT-Omni: A Versatile Dental Multimodal Large Language Model",
    "summary": "Multimodal Large Language Models (MLLMs) have exhibited immense potential across numerous medical specialties; yet, dentistry remains underexplored, in part due to limited domain-specific data, scarce dental expert annotations, insufficient modality-specific modeling, and challenges in reliability. In this paper, we present OralGPT-Omni, the first dental-specialized MLLM designed for comprehensive and trustworthy analysis across diverse dental imaging modalities and clinical tasks. To explicitly capture dentists' diagnostic reasoning, we construct TRACE-CoT, a clinically grounded chain-of-thought dataset that mirrors dental radiologists' decision-making processes. This reasoning supervision, combined with our proposed four-stage training paradigm, substantially strengthens the model's capacity for dental image understanding and analysis. In parallel, we introduce MMOral-Uni, the first unified multimodal benchmark for dental image analysis. It comprises 2,809 open-ended question-answer pairs spanning five modalities and five tasks, offering a comprehensive evaluation suite to date for MLLMs in digital dentistry. OralGPT-Omni achieves an overall score of 51.84 on the MMOral-Uni benchmark and 45.31 on the MMOral-OPG benchmark, dramatically outperforming the scores of GPT-5. Our work promotes intelligent dentistry and paves the way for future advances in dental image analysis. All code, benchmark, and models will be made publicly available.",
    "translation": "标题：OralGPT-Omni：一种多功能口腔医学多模态大语言模型\n\n摘要：多模态大语言模型（MLLMs）已在众多医学专业领域展现出巨大潜力；然而，口腔医学领域的研究仍显不足，部分原因在于领域特定数据有限、口腔专家标注稀缺、模态特异性建模不充分以及可靠性方面的挑战。本文提出OralGPT-Omni，这是首个面向口腔医学的专用多模态大语言模型，旨在实现对多种口腔影像模态和临床任务的全面、可靠分析。为明确捕捉口腔医师的诊断推理过程，我们构建了TRACE-CoT——一个基于临床实践的思维链数据集，该数据集模拟了口腔影像科医师的决策流程。这种推理监督机制与我们提出的四阶段训练范式相结合，显著增强了模型对口腔影像的理解与分析能力。同时，我们推出了MMOral-Uni，这是首个统一的口腔影像多模态评估基准。该基准包含2,809个开放式问答对，涵盖五种影像模态和五类临床任务，为数字口腔医学中的多模态大语言模型提供了迄今最全面的评估体系。OralGPT-Omni在MMOral-Uni基准测试中获得51.84的综合得分，在MMOral-OPG基准测试中获得45.31分，显著超越GPT-5的表现。本研究推动了智能口腔医学的发展，并为未来口腔影像分析的进步开辟了道路。所有代码、基准数据集及模型将公开发布。",
    "url": "https://huggingface.co/papers/2511.22055",
    "arxiv_url": "https://arxiv.org/abs/2511.22055"
  },
  {
    "title": "Test-time scaling of diffusions with flow maps",
    "summary": "A common recipe to improve diffusion models at test-time so that samples score highly against a user-specified reward is to introduce the gradient of the reward into the dynamics of the diffusion itself. This procedure is often ill posed, as user-specified rewards are usually only well defined on the data distribution at the end of generation. While common workarounds to this problem are to use a denoiser to estimate what a sample would have been at the end of generation, we propose a simple solution to this problem by working directly with a flow map. By exploiting a relationship between the flow map and velocity field governing the instantaneous transport, we construct an algorithm, Flow Map Trajectory Tilting (FMTT), which provably performs better ascent on the reward than standard test-time methods involving the gradient of the reward. The approach can be used to either perform exact sampling via importance weighting or principled search that identifies local maximizers of the reward-tilted distribution. We demonstrate the efficacy of our approach against other look-ahead techniques, and show how the flow map enables engagement with complicated reward functions that make possible new forms of image editing, e.g. by interfacing with vision language models.",
    "translation": "标题：基于流映射的扩散模型测试时缩放\n\n摘要：为提升扩散模型在测试时生成样本对用户指定奖励函数的高评分表现，常见方法是将奖励梯度引入扩散过程动态机制。然而该过程往往存在理论缺陷，因为用户定义的奖励函数通常仅在生成过程末端的数据分布上具有明确定义。尽管现有解决方案多采用去噪器估计生成末端样本状态，本研究提出通过直接运用流映射解决该问题的简明方案。通过利用流映射与主导瞬时传输的速度场之间的数学关系，我们构建了流映射轨迹倾斜算法（FMTT），该算法在理论上比传统基于奖励梯度的测试时方法能实现更优的奖励提升效果。该方法既可通过重要性加权实现精确采样，也可执行原则性搜索以定位奖励倾斜分布的局部最优点。实验证明本方法相较于其他前瞻性技术具有显著优势，并展示了流映射如何支持复杂奖励函数的交互应用，例如通过与视觉语言模型对接实现新型图像编辑功能。\n\n请按照以下格式返回：\n标题：基于流映射的扩散模型测试时缩放\n摘要：为提升扩散模型在测试时生成样本对用户指定奖励函数的高评分表现，常见方法是将奖励梯度引入扩散过程动态机制。然而该过程往往存在理论缺陷，因为用户定义的奖励函数通常仅在生成过程末端的数据分布上具有明确定义。尽管现有解决方案多采用去噪器估计生成末端样本状态，本研究提出通过直接运用流映射解决该问题的简明方案。通过利用流映射与主导瞬时传输的速度场之间的数学关系，我们构建了流映射轨迹倾斜算法（FMTT），该算法在理论上比传统基于奖励梯度的测试时方法能实现更优的奖励提升效果。该方法既可通过重要性加权实现精确采样，也可执行原则性搜索以定位奖励倾斜分布的局部最优点。实验证明本方法相较于其他前瞻性技术具有显著优势，并展示了流映射如何支持复杂奖励函数的交互应用，例如通过与视觉语言模型对接实现新型图像编辑功能。",
    "url": "https://huggingface.co/papers/2511.22688",
    "arxiv_url": "https://arxiv.org/abs/2511.22688"
  },
  {
    "title": "Geometrically-Constrained Agent for Spatial Reasoning",
    "summary": "Vision Language Models (VLMs) exhibit a fundamental semantic-to-geometric gap in spatial reasoning: they excel at qualitative semantic inference but their reasoning operates within a lossy semantic space, misaligned with high-fidelity geometry. Current paradigms fail to bridge this gap. Training-based methods suffer from an ``oracle paradox,'' learning flawed spatial logic from imperfect oracles. Tool-integrated methods constrain the final computation but critically leave the VLM's planning process unconstrained, resulting in geometrically flawed plans. In this work, we propose Geometrically-Constrained Agent (GCA), a training-free agentic paradigm that resolves this gap by introducing a formal task constraint. Specifically, we strategically decouples the VLM's role into two stages. First, acting as a semantic analyst, the VLM translates the user's ambiguous query into the formal, verifiable task constraint, which defines the reference frame and objective. Second, acting as a task solver, the VLM generates and executes tool calls strictly within the deterministic bounds defined by the constraint. This geometrically-constrained reasoning strategy successfully resolve the semantic-to-geometric gap, yielding a robust and verifiable reasoning pathway for spatial reasoning. Comprehensive experiments demonstrate that GCA achieves SOTA performance on multiple spatial reasoning benchmarks, surpassing existing training-based and tool-integrated methods by ~27%. Please see our homepage at https://gca-spatial-reasoning.github.io.",
    "translation": "标题：几何约束智能体在空间推理中的应用\n\n摘要：视觉语言模型在空间推理中存在本质的语义-几何鸿沟：其擅长定性语义推断，但推理过程运行于有损语义空间，与高保真几何表征存在错位。现有范式未能弥合这一鸿沟：基于训练的方法受困于“预言悖论”，从不完善的预言机制中习得错误的空间逻辑；工具集成方法虽能约束最终计算，却未对视觉语言模型的规划过程施加关键约束，导致产生几何缺陷的规划方案。本研究提出几何约束智能体——一种无需训练的智能体范式，通过引入形式化任务约束解决该问题。具体而言，我们策略性地将视觉语言模型角色解耦为两个阶段：首先作为语义分析器，将用户模糊查询转化为可验证的形式化任务约束，该约束明确定义参考系与目标任务；其次作为任务求解器，在约束确定的确定性边界内严格生成并执行工具调用。这种几何约束推理策略成功化解了语义-几何鸿沟，为空间推理构建了鲁棒且可验证的推理路径。综合实验表明，几何约束智能体在多项空间推理基准测试中达到最先进性能，较现有基于训练和工具集成的方法提升约27%。项目主页详见：https://gca-spatial-reasoning.github.io。",
    "url": "https://huggingface.co/papers/2511.22659",
    "arxiv_url": "https://arxiv.org/abs/2511.22659"
  },
  {
    "title": "Focused Chain-of-Thought: Efficient LLM Reasoning via Structured Input Information",
    "summary": "Recent large language models achieve strong reasoning performance by generating detailed chain-of-thought traces, but this often leads to excessive token use and high inference latency. Existing efficiency approaches typically focus on model-centric interventions, such as reinforcement learning or supervised fine-tuning, to reduce verbosity. In contrast, we propose a training-free, input-centric approach. Inspired by cognitive psychology, we introduce Focused Chain-of-Thought (F-CoT), which separates information extraction from the reasoning process. F-CoT first organizes the essential information from a query into a concise, structured context and then guides the model to reason exclusively over this context. By preventing attention to irrelevant details, F-CoT naturally produces shorter reasoning paths. On arithmetic word problems, F-CoT reduces generated tokens by 2-3x while maintaining accuracy comparable to standard zero-shot CoT. These results highlight structured input as a simple yet effective lever for more efficient LLM reasoning.",
    "translation": "标题：聚焦思维链：通过结构化输入信息实现高效大语言模型推理\n\n摘要：近期的大语言模型通过生成详细的思维链轨迹实现了强大的推理性能，但这通常会导致令牌使用量过大和推理延迟过高。现有的效率提升方法通常侧重于以模型为中心的干预，例如强化学习或有监督微调，以减少冗余表述。与之相反，我们提出了一种无需训练、以输入为中心的方法。受认知心理学启发，我们引入了聚焦思维链方法，该方法将信息提取与推理过程分离。F-CoT首先将查询中的关键信息组织成简洁的结构化上下文，然后引导模型仅在此上下文中进行推理。通过避免关注无关细节，F-CoT自然生成更短的推理路径。在算术文字题测试中，F-CoT将生成的令牌数量减少了2-3倍，同时保持了与标准零样本思维链相当的准确率。这些结果表明，结构化输入是实现更高效大语言模型推理的一种简单而有效的途径。",
    "url": "https://huggingface.co/papers/2511.22176",
    "arxiv_url": "https://arxiv.org/abs/2511.22176"
  },
  {
    "title": "SO-Bench: A Structural Output Evaluation of Multimodal LLMs",
    "summary": "Multimodal large language models (MLLMs) are increasingly deployed in real-world, agentic settings where outputs must not only be correct, but also conform to predefined data schemas. Despite recent progress in structured generation in textual domain, there is still no benchmark that systematically evaluates schema-grounded information extraction and reasoning over visual inputs. In this work, we conduct a comprehensive study of visual structural output capabilities for MLLMs with our carefully designed SO-Bench benchmark. Covering four visual domains, including UI screens, natural images, documents, and charts, SO-Bench is built from over 6.5K diverse JSON schemas and 1.8K curated image-schema pairs with human-verified quality. Benchmarking experiments on open-sourced and frontier proprietary models reveal persistent gaps in predicting accurate, schema compliant outputs, highlighting the need for better multimodal structured reasoning. Beyond benchmarking, we further conduct training experiments to largely improve the model's structured output capability. We plan to make the benchmark available to the community.",
    "translation": "标题：SO-Bench：多模态大语言模型的结构化输出评估\n\n摘要：多模态大语言模型正越来越多地应用于现实世界的智能体场景中，其输出不仅需要正确，还必须符合预定义的数据模式。尽管近期在文本领域结构化生成方面取得了进展，但目前仍缺乏系统评估基于视觉输入的、以模式为基础的信息抽取与推理能力的基准。本研究通过精心设计的SO-Bench基准，对多模态大语言模型的视觉结构化输出能力进行了全面评估。该基准涵盖用户界面屏幕、自然图像、文档和图表四大视觉领域，基于超过6500个多样化JSON模式及1800组经人工质量验证的图像-模式配对数据构建而成。对开源模型与前沿商业模型的基准测试表明，当前模型在生成准确且符合模式要求的输出方面仍存在明显差距，凸显了提升多模态结构化推理能力的必要性。除基准测试外，我们进一步通过训练实验显著提升了模型的结构化输出能力。我们计划向学术界开放此基准资源。",
    "url": "https://huggingface.co/papers/2511.21750",
    "arxiv_url": "https://arxiv.org/abs/2511.21750"
  },
  {
    "title": "From Pixels to Feelings: Aligning MLLMs with Human Cognitive Perception of Images",
    "summary": "While Multimodal Large Language Models (MLLMs) are adept at answering what is in an image-identifying objects and describing scenes-they often lack the ability to understand how an image feels to a human observer. This gap is most evident when considering subjective cognitive properties, such as what makes an image memorable, funny, aesthetically pleasing, or emotionally evocative. To systematically address this challenge, we introduce CogIP-Bench, a comprehensive benchmark for evaluating MLLMs on such image cognitive properties. Our evaluation reveals a significant gap: current models are poorly aligned with human perception of these nuanced properties. We then demonstrate that a post-training phase can effectively bridge this gap, significantly enhancing the model's alignment with human judgments. Furthermore, we show that this learned cognitive alignment is not merely predictive but also transferable to downstream creative tasks. By integrating our cognitively-aligned MLLM into an image generation pipeline, we can guide the synthesis process to produce images that better embody desired traits, such as being more memorable or visually appealing. Our work provides a benchmark to measure this human-like perception, a post-training pipeline to enhance it, and a demonstration that this alignment unlocks more human-centric AI.",
    "translation": "标题：从像素到情感：对齐多模态大语言模型与人类图像认知感知\n\n摘要：尽管多模态大语言模型（MLLMs）擅长回答图像内容——识别物体并描述场景——但它们往往缺乏理解人类观察者对图像主观感受的能力。这一差距在涉及主观认知属性时尤为明显，例如图像为何令人难忘、有趣、具有美感或能引发情感共鸣。为系统性地应对这一挑战，我们提出了CogIP-Bench，这是一个用于评估MLLMs在图像认知属性表现的综合基准。评估结果显示当前模型与人类对这些细微属性的感知存在显著偏差。我们进一步证明，通过后训练阶段可有效弥合这一差距，显著提升模型与人类判断的对齐度。此外，这种习得的认知对齐能力不仅具有预测性，还可迁移至下游创意任务。通过将认知对齐的MLLM集成至图像生成流程，我们能够引导合成过程生成更符合预期特质（如更令人难忘或更具视觉吸引力）的图像。本研究提出了衡量类人感知的基准、增强对齐度的后训练方案，并论证了这种对齐能力可推动人工智能向更以人为本的方向发展。",
    "url": "https://huggingface.co/papers/2511.22805",
    "arxiv_url": "https://arxiv.org/abs/2511.22805"
  },
  {
    "title": "Layer-Aware Video Composition via Split-then-Merge",
    "summary": "We present Split-then-Merge (StM), a novel framework designed to enhance control in generative video composition and address its data scarcity problem. Unlike conventional methods relying on annotated datasets or handcrafted rules, StM splits a large corpus of unlabeled videos into dynamic foreground and background layers, then self-composes them to learn how dynamic subjects interact with diverse scenes. This process enables the model to learn the complex compositional dynamics required for realistic video generation. StM introduces a novel transformation-aware training pipeline that utilizes a multi-layer fusion and augmentation to achieve affordance-aware composition, alongside an identity-preservation loss that maintains foreground fidelity during blending. Experiments show StM outperforms SoTA methods in both quantitative benchmarks and in humans/VLLM-based qualitative evaluations. More details are available at our project page: https://split-then-merge.github.io",
    "translation": "标题：基于拆分-合并策略的层感知视频合成方法\n\n摘要：本文提出拆分-合并框架，这是一种旨在增强生成式视频合成的控制能力并解决其数据稀缺问题的新型框架。与传统依赖标注数据集或人工规则的方法不同，该框架将大规模无标注视频库拆分为动态前景层与背景层，继而通过自主重组学习动态主体与多样化场景的交互机制。这一过程使模型能够掌握实现逼真视频生成所需的复杂组合动态。本框架创新性地引入感知变换的训练流程，采用多层融合与增强技术实现可供性感知的合成，同时结合保持前景特征一致性的损失函数，确保融合过程中前景内容的保真度。实验表明，该框架在定量基准测试以及基于人类/大语言模型的定性评估中均优于当前最先进方法。更多细节详见项目页面：https://split-then-merge.github.io",
    "url": "https://huggingface.co/papers/2511.20809",
    "arxiv_url": "https://arxiv.org/abs/2511.20809"
  },
  {
    "title": "OmniRefiner: Reinforcement-Guided Local Diffusion Refinement",
    "summary": "Reference-guided image generation has progressed rapidly, yet current diffusion models still struggle to preserve fine-grained visual details when refining a generated image using a reference. This limitation arises because VAE-based latent compression inherently discards subtle texture information, causing identity- and attribute-specific cues to vanish. Moreover, post-editing approaches that amplify local details based on existing methods often produce results inconsistent with the original image in terms of lighting, texture, or shape. To address this, we introduce , a detail-aware refinement framework that performs two consecutive stages of reference-driven correction to enhance pixel-level consistency. We first adapt a single-image diffusion editor by fine-tuning it to jointly ingest the draft image and the reference image, enabling globally coherent refinement while maintaining structural fidelity. We then apply reinforcement learning to further strengthen localized editing capability, explicitly optimizing for detail accuracy and semantic consistency. Extensive experiments demonstrate that  significantly improves reference alignment and fine-grained detail preservation, producing faithful and visually coherent edits that surpass both open-source and commercial models on challenging reference-guided restoration benchmarks.",
    "translation": "标题：OmniRefiner：基于强化学习的局部扩散细化方法\n\n摘要：参考引导的图像生成技术发展迅速，但现有扩散模型在使用参考图像对生成图像进行细化时，仍难以保持细粒度的视觉细节。这一局限源于基于VAE的潜在压缩机制本质上会丢失细微的纹理信息，导致身份特征与属性相关的视觉线索消失。此外，基于现有方法的局部细节增强后处理方案，常常在光照、纹理或形状方面产生与原始图像不一致的结果。为此，我们提出一种细节感知的精细化框架，通过连续两阶段的参考驱动校正来提升像素级一致性。我们首先对单图像扩散编辑器进行适配，通过微调使其能够同时接收草图图像与参考图像，在保持结构保真度的同时实现全局协调的精细化处理。随后应用强化学习进一步强化局部编辑能力，显式优化细节精度与语义一致性。大量实验表明，该方法在参考对齐与细粒度细节保留方面显著优于现有方案，能够生成忠实且视觉连贯的编辑结果，在具有挑战性的参考引导修复基准测试中超越了开源模型与商业模型。",
    "url": "https://huggingface.co/papers/2511.19990",
    "arxiv_url": "https://arxiv.org/abs/2511.19990"
  },
  {
    "title": "YOLO Meets Mixture-of-Experts: Adaptive Expert Routing for Robust Object Detection",
    "summary": "This paper presents a novel Mixture-of-Experts framework for object detection, incorporating adaptive routing among multiple YOLOv9-T experts to enable dynamic feature specialization and achieve higher mean Average Precision (mAP) and Average Recall (AR) compared to a single YOLOv9-T model.",
    "translation": "标题：YOLO与专家混合模型：面向鲁棒目标检测的自适应专家路由机制\n\n摘要：本文提出了一种新颖的专家混合目标检测框架，通过在多组YOLOv9-T专家模型间引入自适应路由机制，实现了动态特征特化处理。相较于单一YOLOv9-T模型，该框架在平均精度均值与平均召回率指标上均表现出显著提升。",
    "url": "https://huggingface.co/papers/2511.13344",
    "arxiv_url": "https://arxiv.org/abs/2511.13344"
  },
  {
    "title": "Fast3Dcache: Training-free 3D Geometry Synthesis Acceleration",
    "summary": "Diffusion models have achieved impressive generative quality across modalities like 2D images, videos, and 3D shapes, but their inference remains computationally expensive due to the iterative denoising process. While recent caching-based methods effectively reuse redundant computations to speed up 2D and video generation, directly applying these techniques to 3D diffusion models can severely disrupt geometric consistency. In 3D synthesis, even minor numerical errors in cached latent features accumulate, causing structural artifacts and topological inconsistencies. To overcome this limitation, we propose Fast3Dcache, a training-free geometry-aware caching framework that accelerates 3D diffusion inference while preserving geometric fidelity. Our method introduces a Predictive Caching Scheduler Constraint (PCSC) to dynamically determine cache quotas according to voxel stabilization patterns and a Spatiotemporal Stability Criterion (SSC) to select stable features for reuse based on velocity magnitude and acceleration criterion. Comprehensive experiments show that Fast3Dcache accelerates inference significantly, achieving up to a 27.12% speed-up and a 54.8% reduction in FLOPs, with minimal degradation in geometric quality as measured by Chamfer Distance (2.48%) and F-Score (1.95%).",
    "translation": "标题：Fast3Dcache：无需训练的三维几何合成加速方法\n\n摘要：扩散模型在二维图像、视频和三维形状等多种模态上已展现出卓越的生成质量，但其迭代去噪过程导致推理计算成本高昂。尽管近期基于缓存的方法通过复用冗余计算有效加速了二维图像与视频生成，但将这些技术直接应用于三维扩散模型会严重破坏几何一致性。在三维合成中，缓存潜在特征中微小的数值误差也会不断累积，进而导致结构伪影与拓扑不一致问题。为克服这一局限，我们提出了Fast3Dcache——一种无需训练的几何感知缓存框架，可在保持几何保真度的同时加速三维扩散推理。该方法引入了预测性缓存调度约束，以根据体素稳定模式动态确定缓存配额；同时提出时空稳定性准则，依据速度幅值与加速度准则筛选稳定特征进行复用。综合实验表明，Fast3Dcache能显著加速推理过程，最高实现27.12%的加速效果与54.8%的浮点运算量降低，且通过倒角距离（2.48%）与F-Score（1.95%）指标衡量，其几何质量损失极小。",
    "url": "https://huggingface.co/papers/2511.22533",
    "arxiv_url": "https://arxiv.org/abs/2511.22533"
  },
  {
    "title": "FedRE: A Representation Entanglement Framework for Model-Heterogeneous Federated Learning",
    "summary": "Federated learning (FL) enables collaborative training across clients without compromising privacy. While most existing FL methods assume homogeneous model architectures, client heterogeneity in data and resources renders this assumption impractical, motivating model-heterogeneous FL. To address this problem, we propose Federated Representation Entanglement (FedRE), a framework built upon a novel form of client knowledge termed entangled representation. In FedRE, each client aggregates its local representations into a single entangled representation using normalized random weights and applies the same weights to integrate the corresponding one-hot label encodings into the entangled-label encoding. Those are then uploaded to the server to train a global classifier. During training, each entangled representation is supervised across categories via its entangled-label encoding, while random weights are resampled each round to introduce diversity, mitigating the global classifier's overconfidence and promoting smoother decision boundaries. Furthermore, each client uploads a single cross-category entangled representation along with its entangled-label encoding, mitigating the risk of representation inversion attacks and reducing communication overhead. Extensive experiments demonstrate that FedRE achieves an effective trade-off among model performance, privacy protection, and communication overhead. The codes are available at https://github.com/AIResearch-Group/FedRE.",
    "translation": "标题：FedRE：一种面向模型异构联邦学习的表征纠缠框架\n\n摘要：联邦学习（FL）能够在保护隐私的前提下实现跨客户端的协同训练。现有联邦学习方法大多假设客户端采用同构模型架构，然而客户端在数据与资源层面的异构性使得这一假设难以成立，从而催生了模型异构联邦学习的研究。为解决此问题，本文提出联邦表征纠缠（FedRE）框架，其核心在于一种称为纠缠表征的新型客户端知识形式。在FedRE中，各客户端首先使用归一化随机权重将本地表征聚合成单一纠缠表征，并采用相同权重将对应的独热标签编码整合为纠缠标签编码。随后，客户端将二者上传至服务器用于训练全局分类器。训练过程中，每个纠缠表征通过其对应的纠缠标签编码进行跨类别监督学习；同时每轮训练重新采样随机权重以引入多样性，从而有效缓解全局分类器的过度自信问题，并促进更平滑的决策边界形成。此外，各客户端仅需上传单个跨类别纠缠表征及其对应的纠缠标签编码，这既降低了表征逆推攻击的风险，也显著减少了通信开销。大量实验表明，FedRE在模型性能、隐私保护与通信开销之间实现了有效平衡。相关代码已开源：https://github.com/AIResearch-Group/FedRE。",
    "url": "https://huggingface.co/papers/2511.22265",
    "arxiv_url": "https://arxiv.org/abs/2511.22265"
  },
  {
    "title": "Xmodel-2.5: 1.3B Data-Efficient Reasoning SLM",
    "summary": "Large language models deliver strong reasoning and tool-use skills, yet their computational demands make them impractical for edge or cost-sensitive deployments. We present Xmodel-2.5, a 1.3-billion-parameter small language model designed as a drop-in agent core. Training with maximal-update parameterization (μP) allows hyper-parameters tuned on a 20M-parameter proxy to transfer directly to the full model, even under the parameter-tied tie-word-embedding architecture. A 1.4T-token Warmup--Stable--Decay curriculum is used, and we further show that switching from AdamW to Muon during the decay phase improves the 13-task reasoning average by 4.58\\,\\% while keeping every other hyper-parameter fixed, verifying that early AdamW stability can be paired with late Muon sharpening for better downstream performance. FP8-mixed-precision training balances accuracy and throughput. All checkpoints, recipes, and evaluation code are released under the Apache-2.0 license.https://huggingface.co/XiaoduoAILab/Xmodel-2.5 and https://huggingface.co/XiaoduoAILab/Xmodel-2.5-history (training checkpoints). Training code and evaluation harness: https://github.com/XiaoduoAILab/Xmodel-2.5.",
    "translation": "标题：Xmodel-2.5：13亿参数的高数据效率推理小型语言模型\n\n摘要：大语言模型展现出强大的推理与工具调用能力，但其计算需求使其难以在边缘设备或成本敏感场景中部署。本文提出Xmodel-2.5，这是一个13亿参数的小型语言模型，设计为即插即用的智能体核心。通过采用最大更新参数化（μP）方法进行训练，使得在2000万参数代理模型上调优的超参数能够直接迁移至完整模型，即使在参数绑定与词嵌入绑定的架构下仍能生效。训练采用1.4万亿token的“预热-稳定-衰减”课程学习策略，并进一步证明在衰减阶段将优化器从AdamW切换为Muon，可在保持其他超参数不变的情况下将13项推理任务的平均性能提升4.58%，这验证了早期AdamW的稳定性与后期Muon的锐化能力相结合可提升下游任务表现。采用FP8混合精度训练在精度与吞吐量之间取得平衡。所有模型检查点、训练方案和评估代码均基于Apache-2.0协议开源发布：模型地址 https://huggingface.co/XiaoduoAILab/Xmodel-2.5 与 https://huggingface.co/XiaoduoAILab/Xmodel-2.5-history（训练检查点）；训练代码与评估工具：https://github.com/XiaoduoAILab/Xmodel-2.5。",
    "url": "https://huggingface.co/papers/2511.19496",
    "arxiv_url": "https://arxiv.org/abs/2511.19496"
  },
  {
    "title": "MRI Super-Resolution with Deep Learning: A Comprehensive Survey",
    "summary": "High-resolution (HR) magnetic resonance imaging (MRI) is crucial for many clinical and research applications. However, achieving it remains costly and constrained by technical trade-offs and experimental limitations. Super-resolution (SR) presents a promising computational approach to overcome these challenges by generating HR images from more affordable low-resolution (LR) scans, potentially improving diagnostic accuracy and efficiency without requiring additional hardware. This survey reviews recent advances in MRI SR techniques, with a focus on deep learning (DL) approaches. It examines DL-based MRI SR methods from the perspectives of computer vision, computational imaging, inverse problems, and MR physics, covering theoretical foundations, architectural designs, learning strategies, benchmark datasets, and performance metrics. We propose a systematic taxonomy to categorize these methods and present an in-depth study of both established and emerging SR techniques applicable to MRI, considering unique challenges in clinical and research contexts. We also highlight open challenges and directions that the community needs to address. Additionally, we provide a collection of essential open-access resources, tools, and tutorials, available on our GitHub: https://github.com/mkhateri/Awesome-MRI-Super-Resolution.\n  IEEE keywords: MRI, Super-Resolution, Deep Learning, Computational Imaging, Inverse Problem, Survey.",
    "translation": "标题：基于深度学习的磁共振成像超分辨率技术：全面综述\n\n摘要：高分辨率磁共振成像对众多临床诊疗与科学研究至关重要，但其实现仍受制于高昂成本、技术权衡及实验条件限制。超分辨率技术通过从更易获取的低分辨率扫描图像中重建高分辨率图像，为突破这些限制提供了极具前景的计算解决方案，有望在不增加硬件负担的前提下提升诊断精度与效率。本综述系统梳理了磁共振成像超分辨率技术的最新进展，重点关注深度学习方法。研究从计算机视觉、计算成像、逆问题求解及磁共振物理等维度，对基于深度学习的磁共振超分辨率方法进行剖析，涵盖理论基础、架构设计、学习策略、基准数据集与性能评估体系。我们提出系统性分类框架以归纳现有方法，并结合临床与科研场景中的特殊挑战，对成熟技术与新兴方案展开深入探讨。同时，本文指明了该领域亟待解决的关键问题与发展方向。此外，我们整合了开源资源、工具及教程合集，可通过GitHub项目获取：https://github.com/mkhateri/Awesome-MRI-Super-Resolution。\n\nIEEE关键词：磁共振成像，超分辨率，深度学习，计算成像，逆问题，综述",
    "url": "https://huggingface.co/papers/2511.16854",
    "arxiv_url": "https://arxiv.org/abs/2511.16854"
  },
  {
    "title": "Find the Leak, Fix the Split: Cluster-Based Method to Prevent Leakage in Video-Derived Datasets",
    "summary": "We propose a cluster-based frame selection strategy to mitigate information leakage in video-derived frames datasets. By grouping visually similar frames before splitting into training, validation, and test sets, the method produces more representative, balanced, and reliable dataset partitions.",
    "translation": "标题：定位泄露，修复分割：基于聚类的视频衍生数据集防泄露方法\n\n摘要：本文提出一种基于聚类的帧选择策略，以缓解视频衍生帧数据集中的信息泄露问题。该方法通过在数据划分为训练集、验证集和测试集之前对视觉相似的帧进行聚类分组，从而生成更具代表性、更均衡且更可靠的数据集划分。",
    "url": "https://huggingface.co/papers/2511.13944",
    "arxiv_url": "https://arxiv.org/abs/2511.13944"
  },
  {
    "title": "Recognition of Abnormal Events in Surveillance Videos using Weakly Supervised Dual-Encoder Models",
    "summary": "We address the challenge of detecting rare and diverse anomalies in surveillance videos using only video-level supervision. Our dual-backbone framework combines convolutional and transformer representations through top-k pooling, achieving 90.7% area under the curve (AUC) on the UCF-Crime dataset.",
    "translation": "标题：基于弱监督双编码器模型的监控视频异常事件识别\n\n摘要：本研究针对仅使用视频级监督检测监控视频中罕见且多样异常事件的挑战，提出一种双主干网络框架。该框架通过top-k池化策略融合卷积与Transformer表征，在UCF-Crime数据集上实现了90.7%的曲线下面积（AUC）检测性能。",
    "url": "https://huggingface.co/papers/2511.13276",
    "arxiv_url": "https://arxiv.org/abs/2511.13276"
  }
]