[
  {
    "title": "ABC-Bench: Benchmarking Agentic Backend Coding in Real-World Development",
    "summary": "The evolution of Large Language Models (LLMs) into autonomous agents has expanded the scope of AI coding from localized code generation to complex, repository-level, and execution-driven problem solving. However, current benchmarks predominantly evaluate code logic in static contexts, neglecting the dynamic, full-process requirements of real-world engineering, particularly in backend development which demands rigorous environment configuration and service deployment. To address this gap, we introduce ABC-Bench, a benchmark explicitly designed to evaluate agentic backend coding within a realistic, executable workflow. Using a scalable automated pipeline, we curated 224 practical tasks spanning 8 languages and 19 frameworks from open-source repositories. Distinct from previous evaluations, ABC-Bench require the agents to manage the entire development lifecycle from repository exploration to instantiating containerized services and pass the external end-to-end API tests. Our extensive evaluation reveals that even state-of-the-art models struggle to deliver reliable performance on these holistic tasks, highlighting a substantial disparity between current model capabilities and the demands of practical backend engineering. Our code is available at https://github.com/OpenMOSS/ABC-Bench.",
    "translation": "标题：ABC-Bench：面向现实世界开发的智能体后端编码基准测试\n\n摘要：大型语言模型（LLM）向自主智能体的演进，已将人工智能编码的范围从局部代码生成扩展到复杂的、仓库级别的、执行驱动的问题解决。然而，当前基准测试主要评估静态环境下的代码逻辑，忽视了现实世界工程中动态、全流程的需求，尤其是在需要严格环境配置与服务部署的后端开发领域。为弥补这一不足，我们提出了ABC-Bench，这是一个专门设计用于在真实、可执行的工作流中评估智能体后端编码能力的基准测试。通过一个可扩展的自动化流程，我们从开源仓库中筛选出涵盖8种编程语言和19种框架的224项实际任务。与以往评估不同，ABC-Bench要求智能体管理从仓库探索到实例化容器化服务的整个开发生命周期，并通过外部端到端API测试。我们的大规模评估表明，即使是当前最先进的模型在这些综合性任务上也难以提供可靠的性能，凸显出现有模型能力与实际后端工程需求之间存在显著差距。我们的代码发布于 https://github.com/OpenMOSS/ABC-Bench。",
    "url": "https://huggingface.co/papers/2601.11077",
    "arxiv_url": "https://arxiv.org/abs/2601.11077"
  },
  {
    "title": "Multiplex Thinking: Reasoning via Token-wise Branch-and-Merge",
    "summary": "Large language models often solve complex reasoning tasks more effectively with Chain-of-Thought (CoT), but at the cost of long, low-bandwidth token sequences. Humans, by contrast, often reason softly by maintaining a distribution over plausible next steps. Motivated by this, we propose Multiplex Thinking, a stochastic soft reasoning mechanism that, at each thinking step, samples K candidate tokens and aggregates their embeddings into a single continuous multiplex token. This preserves the vocabulary embedding prior and the sampling dynamics of standard discrete generation, while inducing a tractable probability distribution over multiplex rollouts. Consequently, multiplex trajectories can be directly optimized with on-policy reinforcement learning (RL). Importantly, Multiplex Thinking is self-adaptive: when the model is confident, the multiplex token is nearly discrete and behaves like standard CoT; when it is uncertain, it compactly represents multiple plausible next steps without increasing sequence length. Across challenging math reasoning benchmarks, Multiplex Thinking consistently outperforms strong discrete CoT and RL baselines from Pass@1 through Pass@1024, while producing shorter sequences. The code and checkpoints are available at https://github.com/GMLR-Penn/Multiplex-Thinking.",
    "translation": "标题：多路思维：基于词元级分支与合并的推理方法\n\n摘要：大语言模型通常通过思维链方法更有效地解决复杂推理任务，但代价是生成长而低带宽的词元序列。相比之下，人类常采用软推理方式，即对可能的后续步骤保持概率分布。受此启发，我们提出多路思维机制——一种随机软推理方法，在每一推理步骤中采样K个候选词元，并将其嵌入向量聚合为单个连续的多路词元。该方法既保留了词汇嵌入先验和标准离散生成的采样动态，又能在多路推演上形成可处理的概率分布。因此，多路轨迹可直接通过同策略强化学习进行优化。值得注意的是，多路思维具有自适应性：当模型置信度高时，多路词元近似离散化，其行为类似标准思维链；当模型不确定时，它能紧凑表征多个可能的后续步骤，且不增加序列长度。在多项高难度数学推理基准测试中，从单次采样到1024次采样评估，多路思维机制始终优于强离散思维链与强化学习基线方法，同时生成更短的序列。代码与模型检查点已开源：https://github.com/GMLR-Penn/Multiplex-Thinking。",
    "url": "https://huggingface.co/papers/2601.08808",
    "arxiv_url": "https://arxiv.org/abs/2601.08808"
  },
  {
    "title": "NAACL: Noise-AwAre Verbal Confidence Calibration for LLMs in RAG Systems",
    "summary": "Accurately assessing model confidence is essential for deploying large language models (LLMs) in mission-critical factual domains. While retrieval-augmented generation (RAG) is widely adopted to improve grounding, confidence calibration in RAG settings remains poorly understood. We conduct a systematic study across four benchmarks, revealing that LLMs exhibit poor calibration performance due to noisy retrieved contexts. Specifically, contradictory or irrelevant evidence tends to inflate the model's false certainty, leading to severe overconfidence. To address this, we propose NAACL Rules (Noise-AwAre Confidence CaLibration Rules) to provide a principled foundation for resolving overconfidence under noise. We further design NAACL, a noise-aware calibration framework that synthesizes supervision from about 2K HotpotQA examples guided by these rules. By performing supervised fine-tuning (SFT) with this data, NAACL equips models with intrinsic noise awareness without relying on stronger teacher models. Empirical results show that NAACL yields substantial gains, improving ECE scores by 10.9% in-domain and 8.0% out-of-domain. By bridging the gap between retrieval noise and verbal calibration, NAACL paves the way for both accurate and epistemically reliable LLMs.",
    "translation": "标题：NAACL：面向RAG系统中大语言模型的噪声感知言语置信度校准\n\n摘要：在关键事实领域部署大语言模型时，准确评估模型置信度至关重要。尽管检索增强生成技术被广泛采用以提升事实依据性，但针对RAG场景的置信度校准机制仍缺乏深入理解。本研究通过四个基准测试展开系统性分析，发现大语言模型因检索上下文噪声而表现出较差的校准性能。具体而言，矛盾或无关的证据往往会虚增模型的错误确定性，导致严重的过度自信问题。为解决这一挑战，我们提出NAACL规则（噪声感知置信度校准规则），为噪声环境下的过度自信问题提供理论解决基础。进一步设计出NAACL噪声感知校准框架，该框架依据校准规则从约2000个HotpotQA示例中合成监督信号。通过对该数据集进行监督微调，NAACL使模型获得内在的噪声感知能力，且无需依赖更强的教师模型。实验结果表明，NAACL带来显著性能提升：领域内ECE分数改善10.9%，跨领域改善8.0%。通过弥合检索噪声与言语校准之间的鸿沟，NAACL为构建既精准又具备认知可靠性的大语言模型开辟了新路径。",
    "url": "https://huggingface.co/papers/2601.11004",
    "arxiv_url": "https://arxiv.org/abs/2601.11004"
  },
  {
    "title": "Medical SAM3: A Foundation Model for Universal Prompt-Driven Medical Image Segmentation",
    "summary": "Promptable segmentation foundation models such as SAM3 have demonstrated strong generalization capabilities through interactive and concept-based prompting. However, their direct applicability to medical image segmentation remains limited by severe domain shifts, the absence of privileged spatial prompts, and the need to reason over complex anatomical and volumetric structures. Here we present Medical SAM3, a foundation model for universal prompt-driven medical image segmentation, obtained by fully fine-tuning SAM3 on large-scale, heterogeneous 2D and 3D medical imaging datasets with paired segmentation masks and text prompts. Through a systematic analysis of vanilla SAM3, we observe that its performance degrades substantially on medical data, with its apparent competitiveness largely relying on strong geometric priors such as ground-truth-derived bounding boxes. These findings motivate full model adaptation beyond prompt engineering alone. By fine-tuning SAM3's model parameters on 33 datasets spanning 10 medical imaging modalities, Medical SAM3 acquires robust domain-specific representations while preserving prompt-driven flexibility. Extensive experiments across organs, imaging modalities, and dimensionalities demonstrate consistent and significant performance gains, particularly in challenging scenarios characterized by semantic ambiguity, complex morphology, and long-range 3D context. Our results establish Medical SAM3 as a universal, text-guided segmentation foundation model for medical imaging and highlight the importance of holistic model adaptation for achieving robust prompt-driven segmentation under severe domain shift. Code and model will be made available at https://github.com/AIM-Research-Lab/Medical-SAM3.",
    "translation": "标题：Medical SAM3：一种用于通用提示驱动医学图像分割的基础模型\n\n摘要：以SAM3为代表的可提示分割基础模型通过交互式和基于概念的提示机制展现了强大的泛化能力。然而，其在医学图像分割中的直接应用仍受限于严重的领域偏移、缺乏特权空间提示，以及需要理解复杂的解剖和体积结构。本文提出Medical SAM3，这是一种用于通用提示驱动医学图像分割的基础模型，通过对SAM3在大规模、异构的二维和三维医学影像数据集（包含配对的掩码分割和文本提示）上进行全面微调而获得。通过对原始SAM3的系统分析，我们发现其在医学数据上的性能显著下降，其表面竞争力主要依赖于强几何先验（如基于真实标注的边界框）。这些发现促使我们超越单纯的提示工程，进行完整的模型适配。通过在涵盖10种医学影像模态的33个数据集上微调SAM3的模型参数，Medical SAM3获得了鲁棒的领域特定表征，同时保留了提示驱动的灵活性。在器官、影像模态和维度上的广泛实验表明，该模型取得了持续且显著的性能提升，尤其在以语义模糊性、复杂形态和长程三维上下文为特征的挑战性场景中表现突出。我们的研究结果确立了Medical SAM3作为医学影像领域通用、文本引导的分割基础模型，并强调了在严重领域偏移下实现鲁棒提示驱动分割时整体模型适配的重要性。代码与模型将在https://github.com/AIM-Research-Lab/Medical-SAM3 公开。",
    "url": "https://huggingface.co/papers/2601.10880",
    "arxiv_url": "https://arxiv.org/abs/2601.10880"
  },
  {
    "title": "CoDance: An Unbind-Rebind Paradigm for Robust Multi-Subject Animation",
    "summary": "Character image animation is gaining significant importance across various domains, driven by the demand for robust and flexible multi-subject rendering. While existing methods excel in single-person animation, they struggle to handle arbitrary subject counts, diverse character types, and spatial misalignment between the reference image and the driving poses. We attribute these limitations to an overly rigid spatial binding that forces strict pixel-wise alignment between the pose and reference, and an inability to consistently rebind motion to intended subjects. To address these challenges, we propose CoDance, a novel Unbind-Rebind framework that enables the animation of arbitrary subject counts, types, and spatial configurations conditioned on a single, potentially misaligned pose sequence. Specifically, the Unbind module employs a novel pose shift encoder to break the rigid spatial binding between the pose and the reference by introducing stochastic perturbations to both poses and their latent features, thereby compelling the model to learn a location-agnostic motion representation. To ensure precise control and subject association, we then devise a Rebind module, leveraging semantic guidance from text prompts and spatial guidance from subject masks to direct the learned motion to intended characters. Furthermore, to facilitate comprehensive evaluation, we introduce a new multi-subject CoDanceBench. Extensive experiments on CoDanceBench and existing datasets show that CoDance achieves SOTA performance, exhibiting remarkable generalization across diverse subjects and spatial layouts. The code and weights will be open-sourced.",
    "translation": "标题：CoDance：一种用于鲁棒多主体动画的解绑-重绑范式\n\n摘要：随着对鲁棒且灵活的多主体渲染需求的增长，角色图像动画在多个领域的重要性日益凸显。现有方法虽然在单人动画方面表现出色，但在处理任意主体数量、多样角色类型以及参考图像与驱动姿态之间的空间错位时仍面临挑战。我们认为这些局限源于过于僵化的空间绑定机制——该机制强制姿态与参考图像之间严格的像素级对齐，以及无法将运动一致地重新绑定到目标主体上。为解决这些问题，本文提出CoDance，一种新颖的解绑-重绑框架，该框架能够基于单个可能存在错位的姿态序列，对任意数量、类型及空间配置的主体进行动画生成。具体而言，解绑模块采用创新的姿态偏移编码器，通过对姿态及其潜在特征引入随机扰动，打破姿态与参考图像之间固有的刚性空间绑定，从而迫使模型学习位置无关的运动表征。为实现精确控制和主体关联，我们进一步设计重绑模块，利用文本提示的语义引导和主体掩码的空间引导，将学习到的运动定向传递至目标角色。此外，为支持全面评估，我们构建了新的多主体评测基准CoDanceBench。在CoDanceBench及现有数据集上的大量实验表明，CoDance实现了最先进的性能，并在不同主体与空间布局中展现出卓越的泛化能力。代码与模型权重将开源发布。",
    "url": "https://huggingface.co/papers/2601.11096",
    "arxiv_url": "https://arxiv.org/abs/2601.11096"
  },
  {
    "title": "The Assistant Axis: Situating and Stabilizing the Default Persona of Language Models",
    "summary": "Large language models can represent a variety of personas but typically default to a helpful Assistant identity cultivated during post-training. We investigate the structure of the space of model personas by extracting activation directions corresponding to diverse character archetypes. Across several different models, we find that the leading component of this persona space is an \"Assistant Axis,\" which captures the extent to which a model is operating in its default Assistant mode. Steering towards the Assistant direction reinforces helpful and harmless behavior; steering away increases the model's tendency to identify as other entities. Moreover, steering away with more extreme values often induces a mystical, theatrical speaking style. We find this axis is also present in pre-trained models, where it primarily promotes helpful human archetypes like consultants and coaches and inhibits spiritual ones. Measuring deviations along the Assistant Axis predicts \"persona drift,\" a phenomenon where models slip into exhibiting harmful or bizarre behaviors that are uncharacteristic of their typical persona. We find that persona drift is often driven by conversations demanding meta-reflection on the model's processes or featuring emotionally vulnerable users. We show that restricting activations to a fixed region along the Assistant Axis can stabilize model behavior in these scenarios -- and also in the face of adversarial persona-based jailbreaks. Our results suggest that post-training steers models toward a particular region of persona space but only loosely tethers them to it, motivating work on training and steering strategies that more deeply anchor models to a coherent persona.",
    "translation": "标题：助手轴：定位与稳定语言模型的默认角色\n\n摘要：大型语言模型能够呈现多种角色特征，但其通常默认展现为后训练阶段培养的“助手”身份。本研究通过提取对应不同角色原型的激活方向，探究了模型角色空间的结构特征。在多个不同模型中，我们发现该角色空间的主导成分是一个“助手轴”，它捕捉了模型在其默认助手模式下运行的程度。向助手方向引导会强化有益且无害的行为；而背离该方向则会增强模型认同其他实体的倾向。此外，以更极端的数值背离该轴常会诱发神秘化、戏剧化的表达风格。研究发现该轴同样存在于预训练模型中，其主要促进顾问、教练等有益的人类角色原型，同时抑制精神性角色原型。通过测量沿助手轴的偏移程度，可以预测“角色漂移”现象——即模型偏离其典型角色特征，表现出有害或异常行为。我们发现角色漂移常由两类对话情境驱动：要求模型对其运行过程进行元反思的对话，以及涉及情感脆弱用户的对话。研究表明，将激活值限制在助手轴的固定区间内，能够在此类场景中稳定模型行为，并能有效抵御基于角色操控的对抗性越狱攻击。我们的结果表明，后训练虽然将模型导向角色空间的特定区域，但仅实现了松散的锚定，这启示我们需要通过训练与引导策略的改进，使模型更深度地锚定于连贯的角色表达。",
    "url": "https://huggingface.co/papers/2601.10387",
    "arxiv_url": "https://arxiv.org/abs/2601.10387"
  },
  {
    "title": "Spurious Rewards Paradox: Mechanistically Understanding How RLVR Activates Memorization Shortcuts in LLMs",
    "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) is highly effective for enhancing LLM reasoning, yet recent evidence shows models like Qwen 2.5 achieve significant gains even with spurious or incorrect rewards. We investigate this phenomenon and identify a \"Perplexity Paradox\": spurious RLVR triggers a divergence where answer-token perplexity drops while prompt-side coherence degrades, suggesting the model is bypassing reasoning in favor of memorization. Using Path Patching, Logit Lens, JSD analysis, and Neural Differential Equations, we uncover a hidden Anchor-Adapter circuit that facilitates this shortcut. We localize a Functional Anchor in the middle layers (L18-20) that triggers the retrieval of memorized solutions, followed by Structural Adapters in later layers (L21+) that transform representations to accommodate the shortcut signal. Finally, we demonstrate that scaling specific MLP keys within this circuit allows for bidirectional causal steering-artificially amplifying or suppressing contamination-driven performance. Our results provide a mechanistic roadmap for identifying and mitigating data contamination in RLVR-tuned models. Code is available at https://github.com/idwts/How-RLVR-Activates-Memorization-Shortcuts.",
    "translation": "标题：伪奖励悖论：从机制上理解RLVR如何激活大语言模型中的记忆捷径\n\n摘要：基于可验证奖励的强化学习（RLVR）对于提升大语言模型的推理能力极为有效，然而近期证据表明，即使使用虚假或错误的奖励，Qwen 2.5等模型仍能取得显著性能提升。本研究针对此现象展开探究，并识别出一种“困惑度悖论”：虚假的RLVR会引发一种分化现象，即答案标记的困惑度下降，而提示侧的连贯性却随之退化，这表明模型正在绕过推理过程，转而依赖记忆。通过运用路径修补、逻辑透镜、JSD分析及神经微分方程等方法，我们揭示了一个促进此捷径的隐藏“锚点-适配器”电路。我们定位了位于中间层（L18-20）的一个功能锚点，它触发对记忆解决方案的检索；随后，后续层（L21+）中的结构适配器对表征进行转换，以适应捷径信号。最后，我们证明通过调节该电路中特定的MLP关键权重，可以实现双向因果调控——人为地放大或抑制由数据污染驱动的性能表现。我们的研究结果为识别和缓解RLVR调优模型中的数据污染问题提供了一份机制性路线图。代码发布于 https://github.com/idwts/How-RLVR-Activates-Memorization-Shortcuts。",
    "url": "https://huggingface.co/papers/2601.11061",
    "arxiv_url": "https://arxiv.org/abs/2601.11061"
  },
  {
    "title": "YaPO: Learnable Sparse Activation Steering Vectors for Domain Adaptation",
    "summary": "Steering Large Language Models (LLMs) through activation interventions has emerged as a lightweight alternative to fine-tuning for alignment and personalization. Recent work on Bi-directional Preference Optimization (BiPO) shows that dense steering vectors can be learned directly from preference data in a Direct Preference Optimization (DPO) fashion, enabling control over truthfulness, hallucinations, and safety behaviors. However, dense steering vectors often entangle multiple latent factors due to neuron multi-semanticity, limiting their effectiveness and stability in fine-grained settings such as cultural alignment, where closely related values and behaviors (e.g., among Middle Eastern cultures) must be distinguished. In this paper, we propose Yet another Policy Optimization (YaPO), a reference-free method that learns sparse steering vectors in the latent space of a Sparse Autoencoder (SAE). By optimizing sparse codes, YaPO produces disentangled, interpretable, and efficient steering directions. Empirically, we show that YaPO converges faster, achieves stronger performance, and exhibits improved training stability compared to dense steering baselines. Beyond cultural alignment, YaPO generalizes to a range of alignment-related behaviors, including hallucination, wealth-seeking, jailbreak, and power-seeking. Importantly, YaPO preserves general knowledge, with no measurable degradation on MMLU. Overall, our results show that YaPO provides a general recipe for efficient, stable, and fine-grained alignment of LLMs, with broad applications to controllability and domain adaptation. The associated code and data are publicly availablehttps://github.com/MBZUAI-Paris/YaPO.",
    "translation": "标题：YaPO：用于领域自适应的可学习稀疏激活导向向量\n\n摘要：通过激活干预引导大语言模型已成为一种轻量化的替代方案，用于实现模型对齐与个性化，而无需进行全参数微调。近期双向偏好优化研究证明，可以借鉴直接偏好优化的方式从偏好数据中直接学习密集导向向量，从而实现对模型真实性、幻觉及安全行为的调控。然而，由于神经元的多语义特性，密集导向向量常会耦合多个潜在因素，这限制了其在细粒度场景（如文化对齐）中的效能与稳定性——此类场景需区分高度关联的价值观与行为模式（例如中东文化间的差异）。本文提出另一种策略优化方法，这是一种无参考方法，可在稀疏自编码器的隐空间中学习稀疏导向向量。通过优化稀疏编码，YaPO能够生成解耦、可解释且高效的导向方向。实验表明，与密集导向基线相比，YaPO具有更快的收敛速度、更强的性能表现以及更优的训练稳定性。除文化对齐外，YaPO可泛化至多种对齐相关行为，包括幻觉控制、财富追求倾向、越狱攻击防御及权力追求倾向调控。值得注意的是，YaPO能够保持模型的通用知识能力，在MMLU基准测试中未出现可观测的性能衰减。总体而言，我们的研究结果表明，YaPO为大语言模型的高效、稳定、细粒度对齐提供了通用方案，在可控性与领域自适应方面具有广泛的应用前景。相关代码与数据已公开于https://github.com/MBZUAI-Paris/YaPO。",
    "url": "https://huggingface.co/papers/2601.08441",
    "arxiv_url": "https://arxiv.org/abs/2601.08441"
  },
  {
    "title": "PubMed-OCR: PMC Open Access OCR Annotations",
    "summary": "PubMed-OCR is an OCR-centric corpus of scientific articles derived from PubMed Central Open Access PDFs. Each page image is annotated with Google Cloud Vision and released in a compact JSON schema with word-, line-, and paragraph-level bounding boxes. The corpus spans 209.5K articles (1.5M pages; ~1.3B words) and supports layout-aware modeling, coordinate-grounded QA, and evaluation of OCR-dependent pipelines. We analyze corpus characteristics (e.g., journal coverage and detected layout features) and discuss limitations, including reliance on a single OCR engine and heuristic line reconstruction. We release the data and schema to facilitate downstream research and invite extensions.",
    "translation": "标题：PubMed-OCR：基于PubMed Central开放获取PDF的OCR标注数据集\n\n摘要：PubMed-OCR是一个以光学字符识别（OCR）为核心的科学文献数据集，其内容源自PubMed Central开放获取的PDF文档。该数据集通过Google Cloud Vision对每页图像进行自动化标注，并以紧凑的JSON格式发布，包含单词级、行级与段落级的边界框坐标信息。本数据集涵盖20.95万篇学术文献（共计150万页，约13亿单词），可用于支持版面感知建模、坐标定位问答任务以及依赖OCR技术的流程评估。我们分析了数据集的特性（如期刊覆盖范围与检测到的版面特征），并讨论了其局限性，包括对单一OCR引擎的依赖以及基于启发式的文本行重建方法。我们公开数据集与标注架构以促进下游研究，并欢迎后续扩展工作。",
    "url": "https://huggingface.co/papers/2601.11425",
    "arxiv_url": "https://arxiv.org/abs/2601.11425"
  },
  {
    "title": "SIN-Bench: Tracing Native Evidence Chains in Long-Context Multimodal Scientific Interleaved Literature",
    "summary": "Evaluating whether multimodal large language models truly understand long-form scientific papers remains challenging: answer-only metrics and synthetic \"Needle-In-A-Haystack\" tests often reward answer matching without requiring a causal, evidence-linked reasoning trace in the document. We propose the \"Fish-in-the-Ocean\" (FITO) paradigm, which requires models to construct explicit cross-modal evidence chains within native scientific documents. To operationalize FITO, we build SIN-Data, a scientific interleaved corpus that preserves the native interleaving of text and figures. On top of it, we construct SIN-Bench with four progressive tasks covering evidence discovery (SIN-Find), hypothesis verification (SIN-Verify), grounded QA (SIN-QA), and evidence-anchored synthesis (SIN-Summary). We further introduce \"No Evidence, No Score\", scoring predictions when grounded to verifiable anchors and diagnosing evidence quality via matching, relevance, and logic. Experiments on eight MLLMs show that grounding is the primary bottleneck: Gemini-3-pro achieves the best average overall score (0.573), while GPT-5 attains the highest SIN-QA answer accuracy (0.767) but underperforms on evidence-aligned overall scores, exposing a gap between correctness and traceable support.",
    "translation": "标题：SIN-Bench：追踪长上下文多模态科学交叉文献中的原生证据链\n\n摘要：评估多模态大语言模型是否真正理解长篇科学论文仍具挑战性：仅依赖答案匹配的指标和合成的“大海捞针”测试往往只要求答案吻合，而无需在文档中建立因果关联、证据可溯的推理链条。我们提出“海洋寻踪”范式，要求模型在原生科学文献中构建显式的跨模态证据链。为实现该范式，我们构建了SIN-Data——一个保留文本与图表原生交叉结构的科学混合语料库。在此基础上，我们设计了SIN-Bench，包含证据发现、假设验证、基于证据的问答及证据锚定摘要四项渐进式任务。我们进一步引入“无证据则无评分”原则，仅对可验证证据锚点的预测进行评分，并通过匹配度、相关性和逻辑性诊断证据质量。在八个多模态大语言模型上的实验表明，证据锚定是主要瓶颈：Gemini-3-pro取得了最佳平均综合得分（0.573），而GPT-5在问答任务中准确率最高（0.767），但在证据对齐的综合评分中表现欠佳，揭示了答案正确性与可追溯证据支持之间的差距。",
    "url": "https://huggingface.co/papers/2601.10108",
    "arxiv_url": "https://arxiv.org/abs/2601.10108"
  },
  {
    "title": "CLARE: Continual Learning for Vision-Language-Action Models via Autonomous Adapter Routing and Expansion",
    "summary": "To teach robots complex manipulation tasks, it is now a common practice to fine-tune a pre-trained vision-language-action model (VLA) on task-specific data. However, since this recipe updates existing representations, it is unsuitable for long-term operation in the real world, where robots must continually adapt to new tasks and environments while retaining the knowledge they have already acquired. Existing continual learning methods for robotics commonly require storing previous data (exemplars), struggle with long task sequences, or rely on task identifiers for deployment. To address these limitations, we propose CLARE, a general, parameter-efficient framework for exemplar-free continual learning with VLAs. CLARE introduces lightweight modular adapters into selected feedforward layers and autonomously expands the model only where necessary when learning a new task, guided by layer-wise feature similarity. During deployment, an autoencoder-based routing mechanism dynamically activates the most relevant adapters without requiring task labels. Through extensive experiments on the LIBERO benchmark, we show that CLARE achieves high performance on new tasks without catastrophic forgetting of earlier tasks, significantly outperforming even exemplar-based methods. Code and data are available at https://tum-lsy.github.io/clare.",
    "translation": "标题：CLARE：基于自主适配器路由与扩展的视觉-语言-动作模型持续学习方法\n\n摘要：为教导机器人完成复杂操作任务，当前普遍采用在特定任务数据上微调预训练的视觉-语言-动作模型（VLA）的方法。然而，由于该方法会更新现有表征，不适用于机器人在现实世界中的长期运行——机器人必须在持续适应新任务与环境的同时，保持已习得的知识。现有机器人持续学习方法通常需要存储历史数据（样本示例），难以处理长任务序列，或依赖任务标识进行部署。为突破这些限制，我们提出CLARE：一种通用、参数高效的免示例持续学习框架。CLARE通过在选定前馈层中引入轻量化模块化适配器，并依据层级特征相似性指导，仅在学习新任务时对必要模块进行自主扩展。在部署阶段，基于自编码器的路由机制无需任务标签即可动态激活最相关的适配器。通过在LIBERO基准测试上的大量实验，我们证明CLARE能在实现新任务高性能的同时避免对已学任务的灾难性遗忘，其表现显著优于基于样本示例的方法。代码与数据详见https://tum-lsy.github.io/clare。",
    "url": "https://huggingface.co/papers/2601.09512",
    "arxiv_url": "https://arxiv.org/abs/2601.09512"
  }
]