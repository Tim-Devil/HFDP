[
  {
    "title": "InsertAnywhere: Bridging 4D Scene Geometry and Diffusion Models for Realistic Video Object Insertion",
    "summary": "Recent advances in diffusion-based video generation have opened new possibilities for controllable video editing, yet realistic video object insertion (VOI) remains challenging due to limited 4D scene understanding and inadequate handling of occlusion and lighting effects. We present InsertAnywhere, a new VOI framework that achieves geometrically consistent object placement and appearance-faithful video synthesis. Our method begins with a 4D aware mask generation module that reconstructs the scene geometry and propagates user specified object placement across frames while maintaining temporal coherence and occlusion consistency. Building upon this spatial foundation, we extend a diffusion based video generation model to jointly synthesize the inserted object and its surrounding local variations such as illumination and shading. To enable supervised training, we introduce ROSE++, an illumination aware synthetic dataset constructed by transforming the ROSE object removal dataset into triplets of object removed video, object present video, and a VLM generated reference image. Through extensive experiments, we demonstrate that our framework produces geometrically plausible and visually coherent object insertions across diverse real world scenarios, significantly outperforming existing research and commercial models.",
    "translation": "标题：InsertAnywhere：融合4D场景几何与扩散模型实现逼真视频物体插入\n\n摘要：基于扩散模型的视频生成技术的最新进展为可控视频编辑开辟了新的可能性，然而，由于对4D场景理解的局限性以及对遮挡和光照效果处理不足，实现逼真的视频物体插入仍然面临挑战。本文提出InsertAnywhere，一种新的视频物体插入框架，能够实现几何一致的物体放置和外观保真的视频合成。我们的方法始于一个4D感知掩码生成模块，该模块重建场景几何结构，并在保持时间连贯性和遮挡一致性的同时，将用户指定的物体放置跨帧传播。在此空间基础上，我们扩展了一种基于扩散的视频生成模型，以联合合成插入的物体及其周围的局部变化（如光照和阴影）。为了支持有监督训练，我们引入了ROSE++，这是一个光照感知的合成数据集，通过将ROSE物体移除数据集转换为包含物体移除视频、物体存在视频以及视觉语言模型生成的参考图像的三元组而构建。通过大量实验，我们证明该框架能够在多样化的真实世界场景中生成几何合理且视觉连贯的物体插入效果，显著优于现有研究和商业模型。",
    "url": "https://huggingface.co/papers/2512.17504",
    "arxiv_url": "https://arxiv.org/abs/2512.17504"
  },
  {
    "title": "Mindscape-Aware Retrieval Augmented Generation for Improved Long Context Understanding",
    "summary": "Humans understand long and complex texts by relying on a holistic semantic representation of the content. This global view helps organize prior knowledge, interpret new information, and integrate evidence dispersed across a document, as revealed by the Mindscape-Aware Capability of humans in psychology. Current Retrieval-Augmented Generation (RAG) systems lack such guidance and therefore struggle with long-context tasks. In this paper, we propose Mindscape-Aware RAG (MiA-RAG), the first approach that equips LLM-based RAG systems with explicit global context awareness. MiA-RAG builds a mindscape through hierarchical summarization and conditions both retrieval and generation on this global semantic representation. This enables the retriever to form enriched query embeddings and the generator to reason over retrieved evidence within a coherent global context. We evaluate MiA-RAG across diverse long-context and bilingual benchmarks for evidence-based understanding and global sense-making. It consistently surpasses baselines, and further analysis shows that it aligns local details with a coherent global representation, enabling more human-like long-context retrieval and reasoning.",
    "translation": "标题：基于心智图景感知的检索增强生成技术提升长文本理解能力\n\n摘要：人类通过构建内容的整体语义表征来理解长而复杂的文本。心理学研究揭示的人类心智图景感知能力表明，这种全局视角有助于组织先验知识、解读新信息，并整合分散在文档各处的证据。当前检索增强生成系统缺乏此类全局引导机制，因此在处理长文本任务时面临挑战。本文提出心智图景感知检索增强生成方法，首次为基于大语言模型的检索增强生成系统赋予显式的全局上下文感知能力。该方法通过分层摘要构建心智图景，并基于该全局语义表征同步优化检索与生成过程。这一机制使检索器能够构建增强型查询嵌入，同时使生成器能够在连贯的全局语境中对检索证据进行推理。我们在多类长文本及双语基准测试中，针对证据理解与全局语义构建任务对方法进行评估。实验表明该方法持续超越基线模型，进一步分析显示其能将局部细节与连贯的全局表征相融合，从而实现更趋近人类认知模式的长文本检索与推理。",
    "url": "https://huggingface.co/papers/2512.17220",
    "arxiv_url": "https://arxiv.org/abs/2512.17220"
  },
  {
    "title": "MAI-UI Technical Report: Real-World Centric Foundation GUI Agents",
    "summary": "The development of GUI agents could revolutionize the next generation of human-computer interaction. Motivated by this vision, we present MAI-UI, a family of foundation GUI agents spanning the full spectrum of sizes, including 2B, 8B, 32B, and 235B-A22B variants. We identify four key challenges to realistic deployment: the lack of native agent-user interaction, the limits of UI-only operation, the absence of a practical deployment architecture, and brittleness in dynamic environments. MAI-UI addresses these issues with a unified methodology: a self-evolving data pipeline that expands the navigation data to include user interaction and MCP tool calls, a native device-cloud collaboration system routes execution by task state, and an online RL framework with advanced optimizations to scale parallel environments and context length. MAI-UI establishes new state-of-the-art across GUI grounding and mobile navigation. On grounding benchmarks, it reaches 73.5% on ScreenSpot-Pro, 91.3% on MMBench GUI L2, 70.9% on OSWorld-G, and 49.2% on UI-Vision, surpassing Gemini-3-Pro and Seed1.8 on ScreenSpot-Pro. On mobile GUI navigation, it sets a new SOTA of 76.7% on AndroidWorld, surpassing UI-Tars-2, Gemini-2.5-Pro and Seed1.8. On MobileWorld, MAI-UI obtains 41.7% success rate, significantly outperforming end-to-end GUI models and competitive with Gemini-3-Pro based agentic frameworks. Our online RL experiments show significant gains from scaling parallel environments from 32 to 512 (+5.2 points) and increasing environment step budget from 15 to 50 (+4.3 points). Finally, the native device-cloud collaboration system improves on-device performance by 33%, reduces cloud model calls by over 40%, and preserves user privacy.",
    "translation": "标题：MAI-UI技术报告：以现实世界为中心的基础图形用户界面智能体\n\n摘要：图形用户界面（GUI）智能体的发展有望彻底革新下一代人机交互。基于这一愿景，我们提出了MAI-UI，这是一个覆盖全尺寸谱系的基础GUI智能体家族，包括2B、8B、32B以及235B-A22B等多种变体。我们识别出现实部署面临的四个关键挑战：缺乏原生的智能体-用户交互、仅依赖UI操作的限制、实用部署架构的缺失，以及在动态环境中的脆弱性。MAI-UI通过一套统一的方法论应对这些问题：一个自演进的数据管道，将导航数据扩展至包含用户交互与MCP工具调用；一个原生的设备-云端协作系统，根据任务状态路由执行；以及一个具备先进优化能力的在线强化学习框架，用于扩展并行环境与上下文长度。MAI-UI在GUI基础任务与移动导航任务上均取得了新的最先进性能。在基础任务基准测试中，其在ScreenSpot-Pro上达到73.5%，在MMBench GUI L2上达到91.3%，在OSWorld-G上达到70.9%，在UI-Vision上达到49.2%，并在ScreenSpot-Pro上超越了Gemini-3-Pro和Seed1.8。在移动GUI导航任务上，其在AndroidWorld上创造了76.7%的新SOTA，超越了UI-Tars-2、Gemini-2.5-Pro和Seed1.8。在MobileWorld上，MAI-UI获得了41.7%的成功率，显著优于端到端GUI模型，并与基于Gemini-3-Pro的智能体框架表现相当。我们的在线强化学习实验表明，将并行环境从32个扩展到512个带来了+5.2个百分点的显著提升，将环境步数预算从15步增加到50步则带来了+4.3个百分点的提升。最后，原生的设备-云端协作系统将设备端性能提升了33%，减少了超过40%的云端模型调用，并有效保护了用户隐私。",
    "url": "https://huggingface.co/papers/2512.22047",
    "arxiv_url": "https://arxiv.org/abs/2512.22047"
  },
  {
    "title": "UniPercept: Towards Unified Perceptual-Level Image Understanding across Aesthetics, Quality, Structure, and Texture",
    "summary": "Multimodal large language models (MLLMs) have achieved remarkable progress in visual understanding tasks such as visual grounding, segmentation, and captioning. However, their ability to perceive perceptual-level image features remains limited. In this work, we present UniPercept-Bench, a unified framework for perceptual-level image understanding across three key domains: Aesthetics, Quality, Structure and Texture. We establish a hierarchical definition system and construct large-scale datasets to evaluate perceptual-level image understanding. Based on this foundation, we develop a strong baseline UniPercept trained via Domain-Adaptive Pre-Training and Task-Aligned RL, enabling robust generalization across both Visual Rating (VR) and Visual Question Answering (VQA) tasks. UniPercept outperforms existing MLLMs on perceptual-level image understanding and can serve as a plug-and-play reward model for text-to-image generation. This work defines Perceptual-Level Image Understanding in the era of MLLMs and, through the introduction of a comprehensive benchmark together with a strong baseline, provides a solid foundation for advancing perceptual-level multimodal image understanding.",
    "translation": "标题：UniPercept：迈向跨美学、质量、结构与纹理的统一感知级图像理解\n\n摘要：多模态大语言模型在视觉定位、分割和描述等视觉理解任务中取得了显著进展，但其感知图像底层特征的能力仍存在局限。本研究提出UniPercept-Bench——一个跨美学、质量、结构与纹理三大关键领域的统一感知级图像理解框架。我们建立了层次化定义体系并构建大规模数据集以评估感知级图像理解能力。在此基础上，通过领域自适应预训练与任务对齐强化学习，开发出具有强泛化能力的基线模型UniPercept，该模型在视觉评分与视觉问答任务中均表现出色。UniPercept在感知级图像理解任务上超越现有多模态大语言模型，并能作为即插即用的奖励模型服务于文本到图像生成任务。本研究在多模态大语言模型时代明确了感知级图像理解的定义，通过构建综合性基准与强基线模型，为推进感知级多模态图像理解研究奠定了坚实基础。",
    "url": "https://huggingface.co/papers/2512.21675",
    "arxiv_url": "https://arxiv.org/abs/2512.21675"
  },
  {
    "title": "ProEdit: Inversion-based Editing From Prompts Done Right",
    "summary": "Inversion-based visual editing provides an effective and training-free way to edit an image or a video based on user instructions. Existing methods typically inject source image information during the sampling process to maintain editing consistency. However, this sampling strategy overly relies on source information, which negatively affects the edits in the target image (e.g., failing to change the subject's atributes like pose, number, or color as instructed). In this work, we propose ProEdit to address this issue both in the attention and the latent aspects. In the attention aspect, we introduce KV-mix, which mixes KV features of the source and the target in the edited region, mitigating the influence of the source image on the editing region while maintaining background consistency. In the latent aspect, we propose Latents-Shift, which perturbs the edited region of the source latent, eliminating the influence of the inverted latent on the sampling. Extensive experiments on several image and video editing benchmarks demonstrate that our method achieves SOTA performance. In addition, our design is plug-and-play, which can be seamlessly integrated into existing inversion and editing methods, such as RF-Solver, FireFlow and UniEdit.",
    "translation": "标题：ProEdit：基于反演技术的提示驱动编辑方法优化\n\n摘要：基于反演技术的视觉编辑方法为用户提供了一种无需训练即可根据指令编辑图像或视频的有效途径。现有方法通常在采样过程中注入源图像信息以保持编辑一致性，但这种采样策略过度依赖源信息，会对目标图像的编辑效果产生负面影响（例如无法按指令改变主体的姿态、数量或颜色等属性）。本研究提出ProEdit方法，从注意力机制和潜在空间两个维度解决该问题。在注意力机制方面，我们引入KV混合技术，在编辑区域混合源图像与目标图像的键值特征，既降低源图像对编辑区域的影响，又保持背景一致性。在潜在空间方面，我们提出潜在偏移技术，通过对源潜在空间的编辑区域施加扰动，消除反演潜在向量对采样过程的影响。在多个图像与视频编辑基准测试上的大量实验表明，本方法达到了当前最优性能。此外，本设计具备即插即用特性，可无缝集成至现有反演与编辑方法（如RF-Solver、FireFlow和UniEdit）中。",
    "url": "https://huggingface.co/papers/2512.22118",
    "arxiv_url": "https://arxiv.org/abs/2512.22118"
  },
  {
    "title": "TimeBill: Time-Budgeted Inference for Large Language Models",
    "summary": "Large Language Models (LLMs) are increasingly deployed in time-critical systems, such as robotics, autonomous driving, embodied intelligence, and industrial automation, where generating accurate responses within a given time budget is crucial for decision-making, control, or safety-critical tasks. However, the auto-regressive generation process of LLMs makes it challenging to model and estimate the end-to-end execution time. Furthermore, existing efficient inference methods based on a fixed key-value (KV) cache eviction ratio struggle to adapt to varying tasks with diverse time budgets, where an improper eviction ratio may lead to incomplete inference or a drop in response performance. In this paper, we propose TimeBill, a novel time-budgeted inference framework for LLMs that balances the inference efficiency and response performance. To be more specific, we propose a fine-grained response length predictor (RLP) and an execution time estimator (ETE) to accurately predict the end-to-end execution time of LLMs. Following this, we develop a time-budgeted efficient inference approach that adaptively adjusts the KV cache eviction ratio based on execution time prediction and the given time budget. Finally, through extensive experiments, we demonstrate the advantages of TimeBill in improving task completion rate and maintaining response performance under various overrun strategies.",
    "translation": "标题：TimeBill：面向大语言模型的时间预算推理框架\n\n摘要：大语言模型正日益应用于时间敏感系统，如机器人、自动驾驶、具身智能和工业自动化等领域。在这些场景中，在给定时间预算内生成准确响应对于决策、控制或安全关键任务至关重要。然而，大语言模型的自回归生成特性使其端到端执行时间的建模与估计面临挑战。此外，现有基于固定键值缓存淘汰比例的高效推理方法难以适应具有不同时间预算的多样化任务，不恰当的淘汰比例可能导致推理无法完成或响应性能下降。本文提出TimeBill，一种新颖的面向大语言模型的时间预算推理框架，旨在平衡推理效率与响应性能。具体而言，我们设计了细粒度响应长度预测器与执行时间估计器，以精准预测大语言模型的端到端执行时间。在此基础上，我们开发了一种时间预算高效推理方法，能够根据执行时间预测与给定时间预算自适应调整键值缓存淘汰比例。最后，通过大量实验，我们验证了TimeBill在多种超时策略下提升任务完成率并保持响应性能的优势。",
    "url": "https://huggingface.co/papers/2512.21859",
    "arxiv_url": "https://arxiv.org/abs/2512.21859"
  },
  {
    "title": "Omni-Weather: Unified Multimodal Foundation Model for Weather Generation and Understanding",
    "summary": "Weather modeling requires both accurate prediction and mechanistic interpretation, yet existing methods treat these goals in isolation, separating generation from understanding. To address this gap, we present Omni-Weather, the first multimodal foundation model that unifies weather generation and understanding within a single architecture. Omni-Weather integrates a radar encoder for weather generation tasks, followed by unified processing using a shared self-attention mechanism. Moreover, we construct a Chain-of-Thought dataset for causal reasoning in weather generation, enabling interpretable outputs and improved perceptual quality. Extensive experiments show Omni-Weather achieves state-of-the-art performance in both weather generation and understanding. Our findings further indicate that generative and understanding tasks in the weather domain can mutually enhance each other. Omni-Weather also demonstrates the feasibility and value of unifying weather generation and understanding.",
    "translation": "标题：Omni-Weather：面向天气生成与理解的多模态统一基础模型\n\n摘要：天气建模既需要精确的预测，也需要机制性的解释，然而现有方法往往将这两个目标割裂处理，将生成与理解分离开来。为弥补这一不足，我们提出了Omni-Weather——首个将天气生成与理解统一于单一架构的多模态基础模型。该模型通过雷达编码器处理天气生成任务，并利用共享的自注意力机制进行统一处理。此外，我们构建了一个用于天气生成因果推理的思维链数据集，以实现可解释的输出并提升感知质量。大量实验表明，Omni-Weather在天气生成与理解任务上均达到了最先进的性能。我们的研究进一步表明，天气领域的生成任务与理解任务能够相互促进。Omni-Weather也证明了统一天气生成与理解的可行性与重要价值。",
    "url": "https://huggingface.co/papers/2512.21643",
    "arxiv_url": "https://arxiv.org/abs/2512.21643"
  },
  {
    "title": "See Less, See Right: Bi-directional Perceptual Shaping For Multimodal Reasoning",
    "summary": "Large vision-language models (VLMs) often benefit from intermediate visual cues, either injected via external tools or generated as latent visual tokens during reasoning, but these mechanisms still overlook fine-grained visual evidence (e.g., polylines in charts), generalize poorly across domains, and incur high inference-time cost. In this paper, we propose Bi-directional Perceptual Shaping (BiPS), which transforms question-conditioned masked views into bidirectional where-to-look signals that shape perception during training. BiPS first applies a KL-consistency constraint between the original image and an evidence-preserving view that keeps only question-relevant regions, encouraging coarse but complete coverage of supporting pixels. It then applies a KL-separation constraint between the original and an evidence-ablated view where critical pixels are masked so the image no longer supports the original answer, discouraging text-only shortcuts (i.e., answering from text alone) and enforcing fine-grained visual reliance. Across eight benchmarks, BiPS boosts Qwen2.5-VL-7B by 8.2% on average and shows strong out-of-domain generalization to unseen datasets and image types.",
    "translation": "标题：少看而精看：双向感知塑形在多模态推理中的应用\n\n摘要：大型视觉语言模型（VLMs）通常受益于中间视觉线索的辅助，这些线索可通过外部工具注入或在推理过程中生成为潜在视觉标记。然而，现有机制仍存在以下局限：忽略细粒度视觉证据（如图表中的折线）、跨领域泛化能力较弱，且推理成本较高。本文提出双向感知塑形方法，该方法将问题导向的掩码视图转化为双向的“何处关注”信号，从而在训练过程中塑造模型的感知能力。BiPS首先在原始图像与仅保留问题相关区域的证据保留视图之间施加KL一致性约束，以鼓励模型对支持性像素进行粗略但完整的覆盖。随后，在原始图像与关键像素被掩码的证据消除视图之间施加KL分离约束，使得图像不再支持原始答案，从而抑制仅依赖文本的捷径策略（即仅从文本中获取答案），并强化模型对细粒度视觉信息的依赖。在八个基准测试中，BiPS将Qwen2.5-VL-7B模型的平均性能提升了8.2%，并在未见过的数据集和图像类型上展现出强大的跨领域泛化能力。",
    "url": "https://huggingface.co/papers/2512.22120",
    "arxiv_url": "https://arxiv.org/abs/2512.22120"
  },
  {
    "title": "InSight-o3: Empowering Multimodal Foundation Models with Generalized Visual Search",
    "summary": "The ability for AI agents to \"think with images\" requires a sophisticated blend of reasoning and perception. However, current open multimodal agents still largely fall short on the reasoning aspect crucial for real-world tasks like analyzing documents with dense charts/diagrams and navigating maps. To address this gap, we introduce O3-Bench, a new benchmark designed to evaluate multimodal reasoning with interleaved attention to visual details. O3-Bench features challenging problems that require agents to piece together subtle visual information from distinct image areas through multi-step reasoning. The problems are highly challenging even for frontier systems like OpenAI o3, which only obtains 40.8% accuracy on O3-Bench. To make progress, we propose InSight-o3, a multi-agent framework consisting of a visual reasoning agent (vReasoner) and a visual search agent (vSearcher) for which we introduce the task of generalized visual search -- locating relational, fuzzy, or conceptual regions described in free-form language, beyond just simple objects or figures in natural images. We then present a multimodal LLM purpose-trained for this task via reinforcement learning. As a plug-and-play agent, our vSearcher empowers frontier multimodal models (as vReasoners), significantly improving their performance on a wide range of benchmarks. This marks a concrete step towards powerful o3-like open systems. Our code and dataset can be found at https://github.com/m-Just/InSight-o3 .",
    "translation": "标题：InSight-o3：以广义视觉搜索增强多模态基础模型能力\n\n摘要：人工智能代理“以图像思考”的能力需要推理与感知的深度融合。然而，当前开源的多数多模态代理在推理能力方面仍显不足，而这种能力对于分析包含密集图表/图形的文档、地图导航等现实任务至关重要。为弥补这一差距，我们提出了O3-Bench——一个旨在评估多模态推理能力、并强调对视觉细节交错关注的新基准。O3-Bench包含一系列具有挑战性的问题，要求代理通过多步推理，从图像的不同区域整合细微的视觉信息。即使对于OpenAI o3等前沿系统，这些问题也极具挑战性，其在O3-Bench上的准确率仅为40.8%。为推进进展，我们提出了InSight-o3，这是一个由视觉推理代理（vReasoner）和视觉搜索代理（vSearcher）组成的多代理框架。我们为vSearcher引入了广义视觉搜索任务——即超越自然图像中简单物体或图形的定位，能够根据自由形式语言描述，定位关系性、模糊性或概念性的图像区域。随后，我们提出了一种通过强化学习专门为此任务训练的多模态大语言模型。作为一个即插即用代理，我们的vSearcher能够增强前沿多模态模型（作为vReasoner），显著提升其在广泛基准测试上的性能。这标志着向构建强大的类o3开源系统迈出了坚实一步。我们的代码与数据集可在 https://github.com/m-Just/InSight-o3 获取。",
    "url": "https://huggingface.co/papers/2512.18745",
    "arxiv_url": "https://arxiv.org/abs/2512.18745"
  },
  {
    "title": "SWE-RM: Execution-free Feedback For Software Engineering Agents",
    "summary": "Execution-based feedback like unit testing is widely used in the development of coding agents through test-time scaling (TTS) and reinforcement learning (RL). This paradigm requires scalable and reliable collection of unit test cases to provide accurate feedback, and the resulting feedback is often sparse and cannot effectively distinguish between trajectories that are both successful or both unsuccessful. In contrast, execution-free feedback from reward models can provide more fine-grained signals without depending on unit test cases. Despite this potential, execution-free feedback for realistic software engineering (SWE) agents remains underexplored. Aiming to develop versatile reward models that are effective across TTS and RL, however, we observe that two verifiers with nearly identical TTS performance can nevertheless yield very different results in RL. Intuitively, TTS primarily reflects the model's ability to select the best trajectory, but this ability does not necessarily generalize to RL. To address this limitation, we identify two additional aspects that are crucial for RL training: classification accuracy and calibration. We then conduct comprehensive controlled experiments to investigate how to train a robust reward model that performs well across these metrics. In particular, we analyze the impact of various factors such as training data scale, policy mixtures, and data source composition. Guided by these investigations, we introduce SWE-RM, an accurate and robust reward model adopting a mixture-of-experts architecture with 30B total parameters and 3B activated during inference. SWE-RM substantially improves SWE agents on both TTS and RL performance. For example, it increases the accuracy of Qwen3-Coder-Flash from 51.6% to 62.0%, and Qwen3-Coder-Max from 67.0% to 74.6% on SWE-Bench Verified using TTS, achieving new state-of-the-art performance among open-source models.",
    "translation": "标题：SWE-RM：面向软件工程智能体的免执行反馈机制\n\n摘要：基于执行的反馈（如单元测试）通过测试时扩展（TTS）和强化学习（RL）被广泛用于编码智能体的开发。该范式需要可扩展且可靠的单元测试用例收集以提供准确反馈，但由此产生的反馈往往较为稀疏，且难以有效区分同为成功或同为失败的执行轨迹。相比之下，来自奖励模型的免执行反馈能够在不依赖单元测试用例的情况下提供更细粒度的信号。尽管具有潜力，面向实际软件工程（SWE）智能体的免执行反馈机制仍未得到充分探索。为开发在TTS与RL场景下均有效的通用奖励模型，我们观察到两个在TTS性能上几乎相同的验证器在RL中可能产生截然不同的结果。直观而言，TTS主要反映模型选择最优轨迹的能力，但该能力未必能泛化至RL场景。为突破此局限，我们识别出对RL训练至关重要的两个额外维度：分类准确性与校准性。通过开展系统性对照实验，我们深入探究如何训练能够在这类指标上均表现稳健的奖励模型，具体分析了训练数据规模、策略混合方式及数据源构成等多重因素的影响。基于上述研究，我们提出SWE-RM——一个采用专家混合架构的精准稳健奖励模型，其总参数量为300亿，推理时激活参数量为30亿。SWE-RM显著提升了SWE智能体在TTS与RL场景下的性能表现。例如，在SWE-Bench Verified基准测试中，通过TTS方法，该模型将Qwen3-Coder-Flash的准确率从51.6%提升至62.0%，将Qwen3-Coder-Max的准确率从67.0%提升至74.6%，在开源模型中实现了新的最优性能。",
    "url": "https://huggingface.co/papers/2512.21919",
    "arxiv_url": "https://arxiv.org/abs/2512.21919"
  },
  {
    "title": "SlideTailor: Personalized Presentation Slide Generation for Scientific Papers",
    "summary": "Automatic presentation slide generation can greatly streamline content creation. However, since preferences of each user may vary, existing under-specified formulations often lead to suboptimal results that fail to align with individual user needs. We introduce a novel task that conditions paper-to-slides generation on user-specified preferences. We propose a human behavior-inspired agentic framework, SlideTailor, that progressively generates editable slides in a user-aligned manner. Instead of requiring users to write their preferences in detailed textual form, our system only asks for a paper-slides example pair and a visual template - natural and easy-to-provide artifacts that implicitly encode rich user preferences across content and visual style. Despite the implicit and unlabeled nature of these inputs, our framework effectively distills and generalizes the preferences to guide customized slide generation. We also introduce a novel chain-of-speech mechanism to align slide content with planned oral narration. Such a design significantly enhances the quality of generated slides and enables downstream applications like video presentations. To support this new task, we construct a benchmark dataset that captures diverse user preferences, with carefully designed interpretable metrics for robust evaluation. Extensive experiments demonstrate the effectiveness of our framework.",
    "translation": "标题：SlideTailor：面向科研论文的个性化演示文稿幻灯片生成系统\n\n摘要：自动演示文稿幻灯片生成技术能够显著简化内容创作流程。然而，由于不同用户的偏好存在差异，现有研究中未充分明确的生成框架往往导致生成结果难以满足用户的个性化需求。本文提出一种基于用户偏好条件的论文至幻灯片生成新任务，并设计了一种受人类行为启发的智能体框架SlideTailor，该框架能够以渐进式、适应用户偏好的方式生成可编辑的幻灯片。与要求用户以详细文本形式描述偏好的传统方法不同，本系统仅需用户提供一个论文-幻灯片示例对和一个视觉模板——这些自然且易于提供的素材隐式地编码了用户在内容组织与视觉风格方面的丰富偏好。尽管这些输入信息具有隐式且无标注的特性，本框架仍能有效提炼并泛化用户偏好，从而指导定制化幻灯片的生成。此外，我们提出一种新颖的语音链式机制，使幻灯片内容与预设的口头讲述规划保持协同。这一设计显著提升了生成幻灯片的质量，并支持视频演示等下游应用。为支撑该新任务的研究，我们构建了一个涵盖多样化用户偏好的基准数据集，并设计了具有可解释性的评估指标以进行鲁棒性验证。大量实验结果表明，本框架在个性化幻灯片生成方面具有显著优势。",
    "url": "https://huggingface.co/papers/2512.20292",
    "arxiv_url": "https://arxiv.org/abs/2512.20292"
  },
  {
    "title": "SVBench: Evaluation of Video Generation Models on Social Reasoning",
    "summary": "Recent text-to-video generation models exhibit remarkable progress in visual realism, motion fidelity, and text-video alignment, yet they remain fundamentally limited in their ability to generate socially coherent behavior. Unlike humans, who effortlessly infer intentions, beliefs, emotions, and social norms from brief visual cues, current models tend to render literal scenes without capturing the underlying causal or psychological logic. To systematically evaluate this gap, we introduce the first benchmark for social reasoning in video generation. Grounded in findings from developmental and social psychology, our benchmark organizes thirty classic social cognition paradigms into seven core dimensions, including mental-state inference, goal-directed action, joint attention, social coordination, prosocial behavior, social norms, and multi-agent strategy. To operationalize these paradigms, we develop a fully training-free agent-based pipeline that (i) distills the reasoning mechanism of each experiment, (ii) synthesizes diverse video-ready scenarios, (iii) enforces conceptual neutrality and difficulty control through cue-based critique, and (iv) evaluates generated videos using a high-capacity VLM judge across five interpretable dimensions of social reasoning. Using this framework, we conduct the first large-scale study across seven state-of-the-art video generation systems. Our results reveal substantial performance gaps: while modern models excel in surface-level plausibility, they systematically fail in intention recognition, belief reasoning, joint attention, and prosocial inference.",
    "translation": "标题：SVBench：视频生成模型在社会推理能力上的评估\n\n摘要：近期文本到视频生成模型在视觉真实性、运动连贯性及文本-视频对齐方面展现出显著进展，但其生成社会一致性行为的能力仍存在根本性局限。与人类能够轻松从简短视觉线索中推断意图、信念、情感及社会规范不同，当前模型往往仅呈现字面场景，未能捕捉深层的因果或心理逻辑。为系统评估这一差距，我们提出了首个面向视频生成的社会推理基准测试。基于发展心理学与社会心理学的研究成果，该基准将三十个经典社会认知范式归纳为七个核心维度，包括心理状态推断、目标导向行为、共同注意、社会协调、亲社会行为、社会规范及多智能体策略。为实现这些范式的可操作化，我们开发了一套完全无需训练的基于智能体的流程，该流程能够：（1）提炼每个实验的推理机制，（2）合成多样化的视频适用场景，（3）通过基于线索的批判机制确保概念中立性与难度控制，（4）利用高性能视觉语言模型作为评估者，在社会推理的五个可解释维度上对生成视频进行评价。基于此框架，我们对七种前沿视频生成系统进行了首次大规模研究。结果显示存在显著的性能差距：尽管现代模型在表层合理性方面表现优异，但在意图识别、信念推理、共同注意及亲社会行为推断等维度上普遍存在系统性缺陷。",
    "url": "https://huggingface.co/papers/2512.21507",
    "arxiv_url": "https://arxiv.org/abs/2512.21507"
  },
  {
    "title": "Rethinking Sample Polarity in Reinforcement Learning with Verifiable Rewards",
    "summary": "Large reasoning models (LRMs) are typically trained using reinforcement learning with verifiable reward (RLVR) to enhance their reasoning abilities. In this paradigm, policies are updated using both positive and negative self-generated rollouts, which correspond to distinct sample polarities. In this paper, we provide a systematic investigation into how these sample polarities affect RLVR training dynamics and behaviors. We find that positive samples sharpen existing correct reasoning patterns, while negative samples encourage exploration of new reasoning paths. We further explore how adjusting the advantage values of positive and negative samples at both the sample level and the token level affects RLVR training. Based on these insights, we propose an Adaptive and Asymmetric token-level Advantage shaping method for Policy Optimization, namely A3PO, that more precisely allocates advantage signals to key tokens across different polarities. Experiments across five reasoning benchmarks demonstrate the effectiveness of our approach.",
    "translation": "标题：基于可验证奖励的强化学习中样本极性再思考\n\n摘要：大型推理模型通常采用基于可验证奖励的强化学习方法进行训练，以提升其推理能力。在该范式下，策略更新同时使用正负两种自生成轨迹样本，这两种样本对应着不同的极性特征。本文系统研究了样本极性对可验证奖励强化学习训练动态与行为模式的影响机制。研究发现：正样本能够强化已有的正确推理模式，而负样本则有助于探索新的推理路径。我们进一步探究了在样本层面与词元层面对正负样本优势值进行调整如何影响训练过程。基于这些发现，本文提出一种面向策略优化的自适应非对称词元级优势塑造方法（A3PO），该方法能够根据不同极性特征，将优势信号更精确地分配到关键词元上。在五个推理基准测试上的实验验证了该方法的有效性。",
    "url": "https://huggingface.co/papers/2512.21625",
    "arxiv_url": "https://arxiv.org/abs/2512.21625"
  },
  {
    "title": "A 58-Addition, Rank-23 Scheme for General 3x3 Matrix Multiplication",
    "summary": "This paper presents a new state-of-the-art algorithm for exact 3times3 matrix multiplication over general non-commutative rings, achieving a rank-23 scheme with only 58 scalar additions. This improves the previous best additive complexity of 60 additions without a change of basis. The result was discovered through an automated search combining ternary-restricted flip-graph exploration with greedy intersection reduction for common subexpression elimination. The resulting scheme uses only coefficients from {-1, 0, 1}, ensuring both efficiency and portability across arbitrary fields. The total scalar operation count is reduced from 83 to 81.",
    "translation": "标题：一种适用于通用3×3矩阵乘法的58次加法、秩23算法\n\n摘要：本文提出了一种针对一般非交换环上精确3×3矩阵乘法的最新算法，该算法实现了仅需58次标量加法的秩23计算方案。这一结果在不改变基的前提下，将先前最佳的加法复杂度从60次进一步降低。该算法是通过结合三元受限翻转图探索与贪婪交集约简的自动搜索方法发现的，该方法能有效消除公共子表达式。所得计算方案仅使用{-1, 0, 1}范围内的系数，确保了算法在任意域上的高效性与可移植性。标量运算总量从83次减少至81次。",
    "url": "https://huggingface.co/papers/2512.21980",
    "arxiv_url": "https://arxiv.org/abs/2512.21980"
  },
  {
    "title": "Masking Teacher and Reinforcing Student for Distilling Vision-Language Models",
    "summary": "Large-scale vision-language models (VLMs) have recently achieved remarkable multimodal understanding, but their massive size makes them impractical for deployment on mobile or edge devices. This raises the need for compact yet capable VLMs that can efficiently learn from powerful large teachers. However, distilling knowledge from a large teacher to a small student remains challenging due to their large size gap: the student often fails to reproduce the teacher's complex, high-dimensional representations, leading to unstable learning and degraded performance. To address this, we propose Masters (Masking Teacher and Reinforcing Student), a mask-progressive reinforcement learning (RL) distillation framework. Masters first masks non-dominant weights of the teacher to reduce unnecessary complexity, then progressively restores the teacher by gradually increasing its capacity during training. This strategy allows the student to learn richer representations from the teacher in a smooth and stable manner. To further refine knowledge transfer, Masters integrates an offline RL stage with two complementary rewards: an accuracy reward that measures the correctness of the generated responses, and a distillation reward that quantifies the ease of transferring responses from teacher to student. Unlike online think-answer RL paradigms that are computationally expensive and generate lengthy responses, our offline RL leverages pre-generated responses from masked teachers. These provide rich yet efficient guidance, enabling students to achieve strong performance without requiring the think-answer process.",
    "translation": "标题：遮蔽教师与强化学生：视觉语言模型的知识蒸馏方法\n\n摘要：大规模视觉语言模型（VLMs）近期在多模态理解方面取得了显著进展，但其庞大的参数量使其难以部署于移动或边缘设备。这催生了对于紧凑且高性能VLMs的需求，此类模型需要能够从强大的大型教师模型中高效学习知识。然而，由于师生模型之间存在巨大的规模差距，将知识从大型教师模型蒸馏到小型学生模型仍面临挑战：学生模型往往难以复现教师模型复杂的高维表征，导致学习过程不稳定且性能下降。为解决这一问题，我们提出Masters（遮蔽教师与强化学生）框架——一种基于掩码渐进强化学习（RL）的蒸馏方法。该框架首先遮蔽教师模型中非主导权重以降低不必要的复杂度，随后通过在训练过程中逐步恢复教师模型容量，实现渐进式知识传递。这种策略使学生模型能够以平稳、稳定的方式从教师模型中学习更丰富的表征。为进一步优化知识迁移，Masters框架整合了离线强化学习阶段，该阶段包含两种互补奖励机制：衡量生成响应正确性的精度奖励，以及量化从教师到学生响应迁移难易程度的蒸馏奖励。相较于计算成本高昂且生成冗长响应的在线“思考-回答”强化学习范式，本方法利用遮蔽教师模型预生成的响应进行离线强化学习。这些响应提供了丰富而高效的指导，使学生模型无需经过“思考-回答”过程即可实现强劲性能。",
    "url": "https://huggingface.co/papers/2512.22238",
    "arxiv_url": "https://arxiv.org/abs/2512.22238"
  }
]