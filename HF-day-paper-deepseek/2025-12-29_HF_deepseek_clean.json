[
  {
    "title": "InsertAnywhere: Bridging 4D Scene Geometry and Diffusion Models for Realistic Video Object Insertion",
    "summary": "Recent advances in diffusion-based video generation have opened new possibilities for controllable video editing, yet realistic video object insertion (VOI) remains challenging due to limited 4D scene understanding and inadequate handling of occlusion and lighting effects. We present InsertAnywhere, a new VOI framework that achieves geometrically consistent object placement and appearance-faithful video synthesis. Our method begins with a 4D aware mask generation module that reconstructs the scene geometry and propagates user specified object placement across frames while maintaining temporal coherence and occlusion consistency. Building upon this spatial foundation, we extend a diffusion based video generation model to jointly synthesize the inserted object and its surrounding local variations such as illumination and shading. To enable supervised training, we introduce ROSE++, an illumination aware synthetic dataset constructed by transforming the ROSE object removal dataset into triplets of object removed video, object present video, and a VLM generated reference image. Through extensive experiments, we demonstrate that our framework produces geometrically plausible and visually coherent object insertions across diverse real world scenarios, significantly outperforming existing research and commercial models.",
    "translation": "标题：InsertAnywhere：融合4D场景几何与扩散模型以实现逼真的视频对象插入\n\n摘要：基于扩散模型的视频生成技术的最新进展为可控视频编辑开辟了新的可能性，然而，由于对4D场景理解的局限以及对遮挡和光照效果处理不足，实现逼真的视频对象插入仍然面临挑战。本文提出InsertAnywhere，一种新的视频对象插入框架，能够实现几何一致的对象放置和外观保真的视频合成。我们的方法始于一个4D感知掩码生成模块，该模块重建场景几何结构，并在保持时间连贯性和遮挡一致性的同时，将用户指定的对象放置跨帧传播。在此空间基础上，我们扩展了一种基于扩散的视频生成模型，以联合合成插入的对象及其周围的局部变化（如光照和阴影）。为了支持有监督训练，我们引入了ROSE++，这是一个光照感知的合成数据集，通过将ROSE对象移除数据集转换为对象移除视频、对象存在视频以及由视觉语言模型生成的参考图像的三元组而构建。通过大量实验，我们证明该框架能够在多样化的真实场景中生成几何合理且视觉连贯的对象插入效果，显著优于现有研究和商业模型。",
    "url": "https://huggingface.co/papers/2512.17504",
    "arxiv_url": "https://arxiv.org/abs/2512.17504"
  },
  {
    "title": "Mindscape-Aware Retrieval Augmented Generation for Improved Long Context Understanding",
    "summary": "Humans understand long and complex texts by relying on a holistic semantic representation of the content. This global view helps organize prior knowledge, interpret new information, and integrate evidence dispersed across a document, as revealed by the Mindscape-Aware Capability of humans in psychology. Current Retrieval-Augmented Generation (RAG) systems lack such guidance and therefore struggle with long-context tasks. In this paper, we propose Mindscape-Aware RAG (MiA-RAG), the first approach that equips LLM-based RAG systems with explicit global context awareness. MiA-RAG builds a mindscape through hierarchical summarization and conditions both retrieval and generation on this global semantic representation. This enables the retriever to form enriched query embeddings and the generator to reason over retrieved evidence within a coherent global context. We evaluate MiA-RAG across diverse long-context and bilingual benchmarks for evidence-based understanding and global sense-making. It consistently surpasses baselines, and further analysis shows that it aligns local details with a coherent global representation, enabling more human-like long-context retrieval and reasoning.",
    "translation": "标题：基于心智图景感知的检索增强生成技术提升长文本理解能力\n\n摘要：人类通过构建内容的整体语义表征来理解长篇复杂文本。心理学研究揭示的人类心智图景感知能力表明，这种全局视角有助于组织先验知识、解读新信息并整合散布在文档中的证据。当前检索增强生成系统缺乏此类引导机制，因此在长文本处理任务中表现受限。本文提出心智图景感知的检索增强生成方法，首次为基于大语言模型的检索增强生成系统赋予显式的全局上下文感知能力。该方法通过分层摘要构建心智图景，并基于该全局语义表征同步优化检索与生成过程。这种设计使检索器能够形成增强的查询嵌入表示，同时使生成器能够在连贯的全局语境中对检索证据进行推理。我们在多类长文本及双语基准测试中评估该方法在证据理解与全局语义构建方面的性能。实验表明该方法持续超越基线模型，进一步分析显示其能够将局部细节与连贯的全局表征相融合，从而实现更接近人类认知模式的长文本检索与推理。",
    "url": "https://huggingface.co/papers/2512.17220",
    "arxiv_url": "https://arxiv.org/abs/2512.17220"
  },
  {
    "title": "MAI-UI Technical Report: Real-World Centric Foundation GUI Agents",
    "summary": "The development of GUI agents could revolutionize the next generation of human-computer interaction. Motivated by this vision, we present MAI-UI, a family of foundation GUI agents spanning the full spectrum of sizes, including 2B, 8B, 32B, and 235B-A22B variants. We identify four key challenges to realistic deployment: the lack of native agent-user interaction, the limits of UI-only operation, the absence of a practical deployment architecture, and brittleness in dynamic environments. MAI-UI addresses these issues with a unified methodology: a self-evolving data pipeline that expands the navigation data to include user interaction and MCP tool calls, a native device-cloud collaboration system routes execution by task state, and an online RL framework with advanced optimizations to scale parallel environments and context length. MAI-UI establishes new state-of-the-art across GUI grounding and mobile navigation. On grounding benchmarks, it reaches 73.5% on ScreenSpot-Pro, 91.3% on MMBench GUI L2, 70.9% on OSWorld-G, and 49.2% on UI-Vision, surpassing Gemini-3-Pro and Seed1.8 on ScreenSpot-Pro. On mobile GUI navigation, it sets a new SOTA of 76.7% on AndroidWorld, surpassing UI-Tars-2, Gemini-2.5-Pro and Seed1.8. On MobileWorld, MAI-UI obtains 41.7% success rate, significantly outperforming end-to-end GUI models and competitive with Gemini-3-Pro based agentic frameworks. Our online RL experiments show significant gains from scaling parallel environments from 32 to 512 (+5.2 points) and increasing environment step budget from 15 to 50 (+4.3 points). Finally, the native device-cloud collaboration system improves on-device performance by 33%, reduces cloud model calls by over 40%, and preserves user privacy.",
    "translation": "标题：MAI-UI技术报告：面向真实世界的通用图形用户界面智能体\n\n摘要：图形用户界面（GUI）智能体的发展有望彻底革新下一代人机交互。基于这一愿景，我们提出了MAI-UI——一个涵盖全尺寸谱系（包括2B、8B、32B及235B-A22B变体）的通用GUI智能体家族。我们识别出现实部署面临的四大关键挑战：缺乏原生智能体-用户交互、纯UI操作的限制、实用部署架构的缺失，以及在动态环境中的脆弱性。MAI-UI通过一套统一方法论应对这些问题：采用自演进数据管道将导航数据扩展至包含用户交互与MCP工具调用；设计原生设备-云协作系统，依据任务状态路由执行流程；并构建具备先进优化能力的在线强化学习框架，以扩展并行环境规模与上下文长度。MAI-UI在GUI基础任务与移动导航任务上均取得了最先进的性能。在基础任务基准测试中，其在ScreenSpot-Pro达到73.5%，在MMBench GUI L2达到91.3%，在OSWorld-G达到70.9%，在UI-Vision达到49.2%，其中ScreenSpot-Pro成绩超越Gemini-3-Pro与Seed1.8。在移动GUI导航任务中，其于AndroidWorld创下76.7%的新SOTA记录，超越UI-Tars-2、Gemini-2.5-Pro及Seed1.8；在MobileWorld上获得41.7%的成功率，显著优于端到端GUI模型，并与基于Gemini-3-Pro的智能体框架性能相当。我们的在线强化学习实验表明，将并行环境从32扩展至512可带来5.2个百分点的性能提升，将环境步数预算从15增加至50可带来4.3个百分点的提升。最后，原生设备-云协作系统使设备端性能提升33%，云端模型调用减少超40%，同时有效保障了用户隐私。",
    "url": "https://huggingface.co/papers/2512.22047",
    "arxiv_url": "https://arxiv.org/abs/2512.22047"
  },
  {
    "title": "UniPercept: Towards Unified Perceptual-Level Image Understanding across Aesthetics, Quality, Structure, and Texture",
    "summary": "Multimodal large language models (MLLMs) have achieved remarkable progress in visual understanding tasks such as visual grounding, segmentation, and captioning. However, their ability to perceive perceptual-level image features remains limited. In this work, we present UniPercept-Bench, a unified framework for perceptual-level image understanding across three key domains: Aesthetics, Quality, Structure and Texture. We establish a hierarchical definition system and construct large-scale datasets to evaluate perceptual-level image understanding. Based on this foundation, we develop a strong baseline UniPercept trained via Domain-Adaptive Pre-Training and Task-Aligned RL, enabling robust generalization across both Visual Rating (VR) and Visual Question Answering (VQA) tasks. UniPercept outperforms existing MLLMs on perceptual-level image understanding and can serve as a plug-and-play reward model for text-to-image generation. This work defines Perceptual-Level Image Understanding in the era of MLLMs and, through the introduction of a comprehensive benchmark together with a strong baseline, provides a solid foundation for advancing perceptual-level multimodal image understanding.",
    "translation": "标题：UniPercept：面向美学、质量、结构与纹理的统一感知级图像理解\n\n摘要：多模态大语言模型在视觉定位、分割与描述等视觉理解任务中取得了显著进展，但其对感知级图像特征的认知能力仍存在局限。本研究提出UniPercept-Bench，一个跨美学、质量、结构与纹理三大关键领域的统一感知级图像理解框架。我们建立了层次化定义体系并构建大规模数据集以评估感知级图像理解能力。在此基础上，通过领域自适应预训练与任务对齐强化学习，开发出具有强泛化能力的基线模型UniPercept，该模型在视觉评分与视觉问答任务中均表现优异。UniPercept在感知级图像理解任务上超越现有多模态大语言模型，并可作为即插即用的奖励模型服务于文本到图像生成任务。本研究在多模态大语言模型时代明确定义了感知级图像理解，并通过构建综合性基准与强基线模型，为推进感知级多模态图像理解研究奠定了坚实基础。",
    "url": "https://huggingface.co/papers/2512.21675",
    "arxiv_url": "https://arxiv.org/abs/2512.21675"
  },
  {
    "title": "ProEdit: Inversion-based Editing From Prompts Done Right",
    "summary": "Inversion-based visual editing provides an effective and training-free way to edit an image or a video based on user instructions. Existing methods typically inject source image information during the sampling process to maintain editing consistency. However, this sampling strategy overly relies on source information, which negatively affects the edits in the target image (e.g., failing to change the subject's atributes like pose, number, or color as instructed). In this work, we propose ProEdit to address this issue both in the attention and the latent aspects. In the attention aspect, we introduce KV-mix, which mixes KV features of the source and the target in the edited region, mitigating the influence of the source image on the editing region while maintaining background consistency. In the latent aspect, we propose Latents-Shift, which perturbs the edited region of the source latent, eliminating the influence of the inverted latent on the sampling. Extensive experiments on several image and video editing benchmarks demonstrate that our method achieves SOTA performance. In addition, our design is plug-and-play, which can be seamlessly integrated into existing inversion and editing methods, such as RF-Solver, FireFlow and UniEdit.",
    "translation": "标题：ProEdit：基于提示的反演编辑方法优化研究\n\n摘要：基于反演的视觉编辑技术为用户指令驱动的图像或视频编辑提供了一种高效且无需训练的方法。现有方法通常在采样过程中注入源图像信息以保持编辑一致性，但该采样策略过度依赖源信息，会对目标图像的编辑效果产生负面影响（例如无法按指令改变主体的姿态、数量或颜色等属性）。本研究提出ProEdit方法，从注意力机制与潜在空间两个维度解决该问题。在注意力机制方面，我们提出KV混合技术，通过在编辑区域混合源图像与目标图像的键值特征，在保持背景一致性的同时减弱源图像对编辑区域的影响。在潜在空间方面，我们提出潜在偏移技术，通过对源潜在空间的编辑区域施加扰动，消除反演潜在向量对采样过程的影响。在多个图像与视频编辑基准测试上的大量实验表明，本方法达到了当前最优性能。此外，本设计具备即插即用特性，可无缝集成至现有反演与编辑方法（如RF-Solver、FireFlow和UniEdit）中。",
    "url": "https://huggingface.co/papers/2512.22118",
    "arxiv_url": "https://arxiv.org/abs/2512.22118"
  },
  {
    "title": "TimeBill: Time-Budgeted Inference for Large Language Models",
    "summary": "Large Language Models (LLMs) are increasingly deployed in time-critical systems, such as robotics, autonomous driving, embodied intelligence, and industrial automation, where generating accurate responses within a given time budget is crucial for decision-making, control, or safety-critical tasks. However, the auto-regressive generation process of LLMs makes it challenging to model and estimate the end-to-end execution time. Furthermore, existing efficient inference methods based on a fixed key-value (KV) cache eviction ratio struggle to adapt to varying tasks with diverse time budgets, where an improper eviction ratio may lead to incomplete inference or a drop in response performance. In this paper, we propose TimeBill, a novel time-budgeted inference framework for LLMs that balances the inference efficiency and response performance. To be more specific, we propose a fine-grained response length predictor (RLP) and an execution time estimator (ETE) to accurately predict the end-to-end execution time of LLMs. Following this, we develop a time-budgeted efficient inference approach that adaptively adjusts the KV cache eviction ratio based on execution time prediction and the given time budget. Finally, through extensive experiments, we demonstrate the advantages of TimeBill in improving task completion rate and maintaining response performance under various overrun strategies.",
    "translation": "标题：TimeBill：面向大语言模型的预算时间推理框架\n\n摘要：大语言模型正日益应用于时间敏感系统，如机器人、自动驾驶、具身智能和工业自动化等领域。在这些场景中，在给定时间预算内生成准确响应对于决策、控制或安全关键任务至关重要。然而，大语言模型的自回归生成特性使其端到端执行时间的建模与估计面临挑战。此外，现有基于固定键值缓存淘汰比例的高效推理方法难以适应具有不同时间预算的多样化任务，不恰当的淘汰比例可能导致推理不完整或响应性能下降。本文提出TimeBill，一种新颖的面向大语言模型的预算时间推理框架，旨在平衡推理效率与响应性能。具体而言，我们设计了细粒度响应长度预测器与执行时间估计器，以精准预测大语言模型的端到端执行时间。在此基础上，开发了一种预算时间高效推理方法，能够根据执行时间预测与给定时间预算自适应调整键值缓存淘汰比例。最后，通过大量实验验证了TimeBill在多种超时策略下提升任务完成率并保持响应性能的优势。",
    "url": "https://huggingface.co/papers/2512.21859",
    "arxiv_url": "https://arxiv.org/abs/2512.21859"
  },
  {
    "title": "Omni-Weather: Unified Multimodal Foundation Model for Weather Generation and Understanding",
    "summary": "Weather modeling requires both accurate prediction and mechanistic interpretation, yet existing methods treat these goals in isolation, separating generation from understanding. To address this gap, we present Omni-Weather, the first multimodal foundation model that unifies weather generation and understanding within a single architecture. Omni-Weather integrates a radar encoder for weather generation tasks, followed by unified processing using a shared self-attention mechanism. Moreover, we construct a Chain-of-Thought dataset for causal reasoning in weather generation, enabling interpretable outputs and improved perceptual quality. Extensive experiments show Omni-Weather achieves state-of-the-art performance in both weather generation and understanding. Our findings further indicate that generative and understanding tasks in the weather domain can mutually enhance each other. Omni-Weather also demonstrates the feasibility and value of unifying weather generation and understanding.",
    "translation": "标题：Omni-Weather：面向天气生成与理解的多模态统一基础模型\n\n摘要：天气建模既需要精确的预测，也需要机理性的解释，然而现有方法将这两个目标割裂处理，将生成与理解分离开来。为弥补这一不足，我们提出了Omni-Weather，这是首个将天气生成与理解统一于单一架构内的多模态基础模型。Omni-Weather集成了用于天气生成任务的雷达编码器，并采用共享的自注意力机制进行统一处理。此外，我们构建了一个用于天气生成因果推理的思维链数据集，以实现可解释的输出并提升感知质量。大量实验表明，Omni-Weather在天气生成与理解两方面均达到了最先进的性能。我们的研究进一步表明，天气领域的生成任务与理解任务能够相互促进。Omni-Weather也证明了统一天气生成与理解的可行性与价值。",
    "url": "https://huggingface.co/papers/2512.21643",
    "arxiv_url": "https://arxiv.org/abs/2512.21643"
  },
  {
    "title": "See Less, See Right: Bi-directional Perceptual Shaping For Multimodal Reasoning",
    "summary": "Large vision-language models (VLMs) often benefit from intermediate visual cues, either injected via external tools or generated as latent visual tokens during reasoning, but these mechanisms still overlook fine-grained visual evidence (e.g., polylines in charts), generalize poorly across domains, and incur high inference-time cost. In this paper, we propose Bi-directional Perceptual Shaping (BiPS), which transforms question-conditioned masked views into bidirectional where-to-look signals that shape perception during training. BiPS first applies a KL-consistency constraint between the original image and an evidence-preserving view that keeps only question-relevant regions, encouraging coarse but complete coverage of supporting pixels. It then applies a KL-separation constraint between the original and an evidence-ablated view where critical pixels are masked so the image no longer supports the original answer, discouraging text-only shortcuts (i.e., answering from text alone) and enforcing fine-grained visual reliance. Across eight benchmarks, BiPS boosts Qwen2.5-VL-7B by 8.2% on average and shows strong out-of-domain generalization to unseen datasets and image types.",
    "translation": "标题：少看而精看：面向多模态推理的双向感知塑造\n\n摘要：大型视觉语言模型（VLMs）通常受益于中间视觉线索的辅助，这些线索或通过外部工具注入，或在推理过程中生成为潜在视觉标记。然而，现有机制仍存在以下局限：忽视细粒度视觉证据（如图表中的折线）、跨领域泛化能力不足，且推理时计算成本高昂。本文提出双向感知塑造方法，该方法将问题引导的掩码视图转化为双向的“关注何处”信号，从而在训练过程中塑造模型的感知能力。BiPS首先在原始图像与仅保留问题相关区域的证据保留视图之间施加KL一致性约束，以鼓励模型对支持性像素进行粗略但完整的覆盖。随后，在原始图像与关键像素被掩码的证据消除视图之间施加KL分离约束，该视图因掩码操作而不再支持原始答案，从而抑制仅依赖文本的捷径策略（即仅从文本中获取答案），并强化模型对细粒度视觉信息的依赖。在八个基准测试中，BiPS将Qwen2.5-VL-7B模型的平均性能提升了8.2%，并在未见过的数据集和图像类型上展现出强大的跨领域泛化能力。",
    "url": "https://huggingface.co/papers/2512.22120",
    "arxiv_url": "https://arxiv.org/abs/2512.22120"
  },
  {
    "title": "InSight-o3: Empowering Multimodal Foundation Models with Generalized Visual Search",
    "summary": "The ability for AI agents to \"think with images\" requires a sophisticated blend of reasoning and perception. However, current open multimodal agents still largely fall short on the reasoning aspect crucial for real-world tasks like analyzing documents with dense charts/diagrams and navigating maps. To address this gap, we introduce O3-Bench, a new benchmark designed to evaluate multimodal reasoning with interleaved attention to visual details. O3-Bench features challenging problems that require agents to piece together subtle visual information from distinct image areas through multi-step reasoning. The problems are highly challenging even for frontier systems like OpenAI o3, which only obtains 40.8% accuracy on O3-Bench. To make progress, we propose InSight-o3, a multi-agent framework consisting of a visual reasoning agent (vReasoner) and a visual search agent (vSearcher) for which we introduce the task of generalized visual search -- locating relational, fuzzy, or conceptual regions described in free-form language, beyond just simple objects or figures in natural images. We then present a multimodal LLM purpose-trained for this task via reinforcement learning. As a plug-and-play agent, our vSearcher empowers frontier multimodal models (as vReasoners), significantly improving their performance on a wide range of benchmarks. This marks a concrete step towards powerful o3-like open systems. Our code and dataset can be found at https://github.com/m-Just/InSight-o3 .",
    "translation": "标题：InSight-o3：通过广义视觉搜索增强多模态基础模型能力\n\n摘要：人工智能系统“以图像进行思考”的能力需要推理与感知的深度融合。然而，当前开源的智能体在推理能力方面仍存在明显不足，而这对于分析包含密集图表/图示的文档、地图导航等现实任务至关重要。为弥补这一差距，我们提出了O3-Bench——一个专门用于评估多模态推理能力、并强调对视觉细节交错关注的新基准。该基准包含一系列具有挑战性的问题，要求智能体通过多步推理整合来自图像不同区域的细微视觉信息。即使对于OpenAI o3等前沿系统，这些问题也极具挑战性，其在O3-Bench上的准确率仅为40.8%。为推进此领域发展，我们提出了InSight-o3，这是一个由视觉推理智能体（vReasoner）与视觉搜索智能体（vSearcher）构成的多智能体框架。我们为此框架引入了广义视觉搜索任务——其目标不仅是定位自然图像中的简单物体或图形，更在于根据自由形式语言描述，定位具有关联性、模糊性或概念性的图像区域。随后，我们通过强化学习训练了一个专为此任务设计的模态大语言模型。作为即插即用模块，我们的vSearcher能够有效增强前沿多模态模型（作为vReasoner），显著提升它们在多种基准测试上的性能。这标志着我们在构建强大的类o3开源系统方面迈出了坚实一步。相关代码与数据集可在 https://github.com/m-Just/InSight-o3 获取。",
    "url": "https://huggingface.co/papers/2512.18745",
    "arxiv_url": "https://arxiv.org/abs/2512.18745"
  },
  {
    "title": "SWE-RM: Execution-free Feedback For Software Engineering Agents",
    "summary": "Execution-based feedback like unit testing is widely used in the development of coding agents through test-time scaling (TTS) and reinforcement learning (RL). This paradigm requires scalable and reliable collection of unit test cases to provide accurate feedback, and the resulting feedback is often sparse and cannot effectively distinguish between trajectories that are both successful or both unsuccessful. In contrast, execution-free feedback from reward models can provide more fine-grained signals without depending on unit test cases. Despite this potential, execution-free feedback for realistic software engineering (SWE) agents remains underexplored. Aiming to develop versatile reward models that are effective across TTS and RL, however, we observe that two verifiers with nearly identical TTS performance can nevertheless yield very different results in RL. Intuitively, TTS primarily reflects the model's ability to select the best trajectory, but this ability does not necessarily generalize to RL. To address this limitation, we identify two additional aspects that are crucial for RL training: classification accuracy and calibration. We then conduct comprehensive controlled experiments to investigate how to train a robust reward model that performs well across these metrics. In particular, we analyze the impact of various factors such as training data scale, policy mixtures, and data source composition. Guided by these investigations, we introduce SWE-RM, an accurate and robust reward model adopting a mixture-of-experts architecture with 30B total parameters and 3B activated during inference. SWE-RM substantially improves SWE agents on both TTS and RL performance. For example, it increases the accuracy of Qwen3-Coder-Flash from 51.6% to 62.0%, and Qwen3-Coder-Max from 67.0% to 74.6% on SWE-Bench Verified using TTS, achieving new state-of-the-art performance among open-source models.",
    "translation": "标题：SWE-RM：面向软件工程智能体的免执行反馈\n\n摘要：基于执行的反馈（如单元测试）通过测试时扩展（TTS）和强化学习（RL）被广泛用于编码智能体的开发。该范式需要可扩展且可靠的单元测试用例收集以提供准确反馈，但由此产生的反馈往往较为稀疏，且难以有效区分同为成功或同为失败的执行轨迹。相比之下，来自奖励模型的免执行反馈能够在不依赖单元测试用例的情况下提供更细粒度的信号。尽管具备这一潜力，针对实际软件工程（SWE）智能体的免执行反馈研究仍显不足。为开发在TTS和RL中均有效的通用奖励模型，我们观察到两个在TTS性能上几乎相同的验证器在RL中可能产生截然不同的结果。直观而言，TTS主要反映模型选择最佳轨迹的能力，但该能力未必能泛化至RL场景。为克服这一局限，我们识别出对RL训练至关重要的两个额外维度：分类准确性与校准性。随后，我们通过全面的对照实验探究如何训练一个在这些指标上均表现稳健的奖励模型，特别分析了训练数据规模、策略混合方式及数据源构成等多种因素的影响。基于这些研究，我们提出了SWE-RM——一个采用专家混合架构的精准且鲁棒的奖励模型，其总参数量为300亿，推理时激活参数量为30亿。SWE-RM显著提升了SWE智能体在TTS和RL上的性能表现：例如在SWE-Bench Verified基准测试中，通过TTS将Qwen3-Coder-Flash的准确率从51.6%提升至62.0%，将Qwen3-Coder-Max的准确率从67.0%提升至74.6%，在开源模型中实现了新的最优性能。",
    "url": "https://huggingface.co/papers/2512.21919",
    "arxiv_url": "https://arxiv.org/abs/2512.21919"
  },
  {
    "title": "SlideTailor: Personalized Presentation Slide Generation for Scientific Papers",
    "summary": "Automatic presentation slide generation can greatly streamline content creation. However, since preferences of each user may vary, existing under-specified formulations often lead to suboptimal results that fail to align with individual user needs. We introduce a novel task that conditions paper-to-slides generation on user-specified preferences. We propose a human behavior-inspired agentic framework, SlideTailor, that progressively generates editable slides in a user-aligned manner. Instead of requiring users to write their preferences in detailed textual form, our system only asks for a paper-slides example pair and a visual template - natural and easy-to-provide artifacts that implicitly encode rich user preferences across content and visual style. Despite the implicit and unlabeled nature of these inputs, our framework effectively distills and generalizes the preferences to guide customized slide generation. We also introduce a novel chain-of-speech mechanism to align slide content with planned oral narration. Such a design significantly enhances the quality of generated slides and enables downstream applications like video presentations. To support this new task, we construct a benchmark dataset that captures diverse user preferences, with carefully designed interpretable metrics for robust evaluation. Extensive experiments demonstrate the effectiveness of our framework.",
    "translation": "标题：SlideTailor：面向科研论文的个性化演示文稿幻灯片生成系统\n\n摘要：自动演示文稿幻灯片生成技术能够显著简化内容创作流程。然而，由于不同用户的偏好存在差异，现有研究中定义模糊的生成框架往往产生不符合个性化需求的结果。本文提出一种基于用户偏好条件的论文到幻灯片生成新任务，并设计了一种受人类行为启发的智能体框架——SlideTailor，该框架能够以适应用户偏好的方式逐步生成可编辑的幻灯片。与要求用户以详细文本形式描述偏好的传统方法不同，本系统仅需用户提供一对论文-幻灯片示例及一个视觉模板——这些自然且易于获取的素材隐式编码了用户在内容组织与视觉风格层面的丰富偏好。尽管输入信息具有隐式且无标注的特性，本框架仍能有效提炼并泛化用户偏好，从而指导定制化幻灯片的生成。此外，我们创新性地引入语音链式机制，使幻灯片内容与预设的口头讲述规划保持协同。这一设计显著提升了生成幻灯片的质量，并为视频演示等下游应用提供了支持。为推进该新任务的研究，我们构建了一个涵盖多样化用户偏好的基准数据集，并设计了具有可解释性的评估指标以进行鲁棒性验证。大量实验结果表明，本框架在个性化幻灯片生成方面具有显著优势。",
    "url": "https://huggingface.co/papers/2512.20292",
    "arxiv_url": "https://arxiv.org/abs/2512.20292"
  },
  {
    "title": "SVBench: Evaluation of Video Generation Models on Social Reasoning",
    "summary": "Recent text-to-video generation models exhibit remarkable progress in visual realism, motion fidelity, and text-video alignment, yet they remain fundamentally limited in their ability to generate socially coherent behavior. Unlike humans, who effortlessly infer intentions, beliefs, emotions, and social norms from brief visual cues, current models tend to render literal scenes without capturing the underlying causal or psychological logic. To systematically evaluate this gap, we introduce the first benchmark for social reasoning in video generation. Grounded in findings from developmental and social psychology, our benchmark organizes thirty classic social cognition paradigms into seven core dimensions, including mental-state inference, goal-directed action, joint attention, social coordination, prosocial behavior, social norms, and multi-agent strategy. To operationalize these paradigms, we develop a fully training-free agent-based pipeline that (i) distills the reasoning mechanism of each experiment, (ii) synthesizes diverse video-ready scenarios, (iii) enforces conceptual neutrality and difficulty control through cue-based critique, and (iv) evaluates generated videos using a high-capacity VLM judge across five interpretable dimensions of social reasoning. Using this framework, we conduct the first large-scale study across seven state-of-the-art video generation systems. Our results reveal substantial performance gaps: while modern models excel in surface-level plausibility, they systematically fail in intention recognition, belief reasoning, joint attention, and prosocial inference.",
    "translation": "标题：SVBench：视频生成模型在社会推理能力上的评估\n\n摘要：当前的文本到视频生成模型在视觉真实性、运动流畅度以及文本-视频对齐方面取得了显著进展，但其生成社会性连贯行为的能力仍存在根本性局限。与人类能够轻松从短暂视觉线索中推断意图、信念、情感和社会规范不同，现有模型往往仅呈现表面场景，而未能捕捉背后的因果或心理逻辑。为系统评估这一差距，我们首次提出了针对视频生成中社会推理能力的评测基准。基于发展心理学与社会心理学的研究成果，本基准将三十个经典社会认知范式归纳为七个核心维度，包括心理状态推断、目标导向行为、共同注意、社会协调、亲社会行为、社会规范以及多智能体策略。为实现这些范式的可操作化，我们开发了一套完全无需训练的基于智能体的流程，该流程能够：（一）提炼每个实验的推理机制；（二）合成多样化的视频适用场景；（三）通过基于线索的批判机制确保概念中立性与难度控制；（四）利用高性能视觉语言模型作为评判者，从社会推理的五个可解释维度对生成视频进行评估。基于此框架，我们对七种前沿视频生成系统进行了首次大规模研究。结果表明存在显著的性能差距：尽管现代模型在表层合理性方面表现优异，但在意图识别、信念推理、共同注意和亲社会行为推断等维度上均存在系统性缺陷。",
    "url": "https://huggingface.co/papers/2512.21507",
    "arxiv_url": "https://arxiv.org/abs/2512.21507"
  },
  {
    "title": "Rethinking Sample Polarity in Reinforcement Learning with Verifiable Rewards",
    "summary": "Large reasoning models (LRMs) are typically trained using reinforcement learning with verifiable reward (RLVR) to enhance their reasoning abilities. In this paradigm, policies are updated using both positive and negative self-generated rollouts, which correspond to distinct sample polarities. In this paper, we provide a systematic investigation into how these sample polarities affect RLVR training dynamics and behaviors. We find that positive samples sharpen existing correct reasoning patterns, while negative samples encourage exploration of new reasoning paths. We further explore how adjusting the advantage values of positive and negative samples at both the sample level and the token level affects RLVR training. Based on these insights, we propose an Adaptive and Asymmetric token-level Advantage shaping method for Policy Optimization, namely A3PO, that more precisely allocates advantage signals to key tokens across different polarities. Experiments across five reasoning benchmarks demonstrate the effectiveness of our approach.",
    "translation": "标题：基于可验证奖励的强化学习中样本极性问题的再思考\n\n摘要：大型推理模型通常通过基于可验证奖励的强化学习进行训练，以提升其推理能力。在该范式下，策略更新同时使用正负两种自生成轨迹，这两种轨迹对应着不同的样本极性。本文系统研究了样本极性如何影响基于可验证奖励的强化学习的训练动态与行为表现。研究发现，正样本能够强化已有的正确推理模式，而负样本则有助于探索新的推理路径。我们进一步探讨了在样本层面和词元层面对正负样本优势值进行调整如何影响训练过程。基于这些发现，我们提出了一种面向策略优化的自适应非对称词元级优势塑造方法（A3PO），该方法能够更精准地针对不同极性的关键词元分配优势信号。在五个推理基准测试上的实验验证了该方法的有效性。",
    "url": "https://huggingface.co/papers/2512.21625",
    "arxiv_url": "https://arxiv.org/abs/2512.21625"
  },
  {
    "title": "A 58-Addition, Rank-23 Scheme for General 3x3 Matrix Multiplication",
    "summary": "This paper presents a new state-of-the-art algorithm for exact 3times3 matrix multiplication over general non-commutative rings, achieving a rank-23 scheme with only 58 scalar additions. This improves the previous best additive complexity of 60 additions without a change of basis. The result was discovered through an automated search combining ternary-restricted flip-graph exploration with greedy intersection reduction for common subexpression elimination. The resulting scheme uses only coefficients from {-1, 0, 1}, ensuring both efficiency and portability across arbitrary fields. The total scalar operation count is reduced from 83 to 81.",
    "translation": "标题：通用3x3矩阵乘法的58次加法、秩23算法方案\n\n摘要：本文提出了一种针对一般非交换环上精确3×3矩阵乘法的最新算法，该算法仅需58次标量加法即可实现秩23的计算方案。在不改变基的情况下，此结果将先前最佳的加法复杂度从60次加法进一步降低。该算法是通过结合三元限制翻转图探索与贪婪交集约简以消除公共子表达式的自动化搜索所发现的。所得方案仅使用系数集{-1, 0, 1}，确保了算法在任意域上的高效性与可移植性。标量运算总次数从83次减少至81次。",
    "url": "https://huggingface.co/papers/2512.21980",
    "arxiv_url": "https://arxiv.org/abs/2512.21980"
  },
  {
    "title": "Masking Teacher and Reinforcing Student for Distilling Vision-Language Models",
    "summary": "Large-scale vision-language models (VLMs) have recently achieved remarkable multimodal understanding, but their massive size makes them impractical for deployment on mobile or edge devices. This raises the need for compact yet capable VLMs that can efficiently learn from powerful large teachers. However, distilling knowledge from a large teacher to a small student remains challenging due to their large size gap: the student often fails to reproduce the teacher's complex, high-dimensional representations, leading to unstable learning and degraded performance. To address this, we propose Masters (Masking Teacher and Reinforcing Student), a mask-progressive reinforcement learning (RL) distillation framework. Masters first masks non-dominant weights of the teacher to reduce unnecessary complexity, then progressively restores the teacher by gradually increasing its capacity during training. This strategy allows the student to learn richer representations from the teacher in a smooth and stable manner. To further refine knowledge transfer, Masters integrates an offline RL stage with two complementary rewards: an accuracy reward that measures the correctness of the generated responses, and a distillation reward that quantifies the ease of transferring responses from teacher to student. Unlike online think-answer RL paradigms that are computationally expensive and generate lengthy responses, our offline RL leverages pre-generated responses from masked teachers. These provide rich yet efficient guidance, enabling students to achieve strong performance without requiring the think-answer process.",
    "translation": "标题：遮蔽教师与强化学生：视觉语言模型的知识蒸馏方法\n\n摘要：大规模视觉语言模型（VLMs）近期在多模态理解方面取得了显著成就，但其庞大的参数量使其难以部署于移动或边缘设备。这催生了对紧凑且高性能VLMs的需求，这类模型需能够高效地从强大的大型教师模型中学习。然而，由于师生模型间巨大的规模差异，将知识从大型教师模型蒸馏至小型学生模型仍面临挑战：学生模型往往难以复现教师模型复杂的高维表示，导致学习过程不稳定且性能下降。为解决这一问题，我们提出Masters（遮蔽教师与强化学生）框架——一种基于掩码渐进强化学习（RL）的蒸馏方法。Masters首先遮蔽教师模型中的非主导权重以降低不必要的复杂度，随后在训练过程中通过逐步恢复教师模型容量来实现渐进式知识传递。该策略使学生模型能够以平稳、稳定的方式从教师模型中学习更丰富的表示。为进一步优化知识迁移，Masters整合了离线强化学习阶段，包含两种互补奖励机制：衡量生成响应正确性的准确度奖励，以及量化从教师到学生响应迁移难度的蒸馏奖励。与计算成本高昂且生成冗长响应的在线“思考-回答”强化学习范式不同，我们的离线强化学习利用来自遮蔽教师模型的预生成响应。这些响应提供了丰富而高效的指导，使学生模型无需经过“思考-回答”过程即可实现强劲性能。",
    "url": "https://huggingface.co/papers/2512.22238",
    "arxiv_url": "https://arxiv.org/abs/2512.22238"
  }
]