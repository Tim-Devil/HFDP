[
  {
    "title": "First Frame Is the Place to Go for Video Content Customization",
    "summary": "What role does the first frame play in video generation models? Traditionally, it's viewed as the spatial-temporal starting point of a video, merely a seed for subsequent animation. In this work, we reveal a fundamentally different perspective: video models implicitly treat the first frame as a conceptual memory buffer that stores visual entities for later reuse during generation. Leveraging this insight, we show that it's possible to achieve robust and generalized video content customization in diverse scenarios, using only 20-50 training examples without architectural changes or large-scale finetuning. This unveils a powerful, overlooked capability of video generation models for reference-based video customization.",
    "translation": "标题：首帧：视频内容定制化的关键所在\n\n摘要：首帧在视频生成模型中究竟扮演着何种角色？传统观点将其视为视频时空序列的起始点，仅作为后续动画生成的种子。本研究提出了一个根本性不同的视角：视频模型隐式地将首帧作为概念记忆缓冲区，用于存储视觉实体以供后续生成过程重复调用。基于这一发现，我们证明仅需20-50个训练样本即可在多样化场景中实现鲁棒且泛化性强的视频内容定制，无需调整模型架构或进行大规模微调。这一研究揭示了视频生成模型在基于参考视频的内容定制方面被长期忽视的强大能力。",
    "url": "https://huggingface.co/papers/2511.15700",
    "arxiv_url": "https://arxiv.org/abs/2511.15700"
  },
  {
    "title": "V-ReasonBench: Toward Unified Reasoning Benchmark Suite for Video Generation Models",
    "summary": "Recent progress in generative video models, such as Veo-3, has shown surprising zero-shot reasoning abilities, creating a growing need for systematic and reliable evaluation. We introduce V-ReasonBench, a benchmark designed to assess video reasoning across four key dimensions: structured problem-solving, spatial cognition, pattern-based inference, and physical dynamics. The benchmark is built from both synthetic and real-world image sequences and provides a diverse set of answer-verifiable tasks that are reproducible, scalable, and unambiguous. Evaluations of six state-of-the-art video models reveal clear dimension-wise differences, with strong variation in structured, spatial, pattern-based, and physical reasoning. We further compare video models with strong image models, analyze common hallucination behaviors, and study how video duration affects Chain-of-Frames reasoning. Overall, V-ReasonBench offers a unified and reproducible framework for measuring video reasoning and aims to support the development of models with more reliable, human-aligned reasoning skills.",
    "translation": "标题：V-ReasonBench：面向视频生成模型的统一推理基准测试套件\n\n摘要：随着Veo-3等生成式视频模型的最新进展展现出惊人的零样本推理能力，对系统化可靠评估的需求日益增长。我们推出V-ReasonBench这一基准测试框架，旨在从四个关键维度评估视频推理能力：结构化问题解决、空间认知、基于模式的推理和物理动态理解。该基准集成了合成与真实世界图像序列，提供一系列可验证答案的多样化任务，具备可复现、可扩展和无歧义的特点。对六款前沿视频模型的评估显示出明显的维度差异，在结构化、空间、模式化及物理推理方面存在显著波动。我们进一步将视频模型与强图像模型进行对比，分析常见的幻觉生成行为，并研究视频时长对帧序列推理链的影响。总体而言，V-ReasonBench为衡量视频推理能力提供了统一可复现的框架，旨在推动开发具有更可靠、更符合人类思维的推理能力的视频生成模型。",
    "url": "https://huggingface.co/papers/2511.16668",
    "arxiv_url": "https://arxiv.org/abs/2511.16668"
  },
  {
    "title": "Step-Audio-R1 Technical Report",
    "summary": "Recent advances in reasoning models have demonstrated remarkable success in text and vision domains through extended chain-of-thought deliberation. However, a perplexing phenomenon persists in audio language models: they consistently perform better with minimal or no reasoning, raising a fundamental question - can audio intelligence truly benefit from deliberate thinking? We introduce Step-Audio-R1, the first audio reasoning model that successfully unlocks reasoning capabilities in the audio domain. Through our proposed Modality-Grounded Reasoning Distillation (MGRD) framework, Step-Audio-R1 learns to generate audio-relevant reasoning chains that genuinely ground themselves in acoustic features rather than hallucinating disconnected deliberations. Our model exhibits strong audio reasoning capabilities, surpassing Gemini 2.5 Pro and achieving performance comparable to the state-of-the-art Gemini 3 Pro across comprehensive audio understanding and reasoning benchmarks spanning speech, environmental sounds, and music. These results demonstrate that reasoning is a transferable capability across modalities when appropriately anchored, transforming extended deliberation from a liability into a powerful asset for audio intelligence. By establishing the first successful audio reasoning model, Step-Audio-R1 opens new pathways toward building truly multimodal reasoning systems that think deeply across all sensory modalities.",
    "translation": "标题：Step-Audio-R1技术报告\n\n摘要：推理模型通过扩展的思维链推演在文本和视觉领域取得了显著成功。然而音频语言模型领域始终存在一个令人困惑的现象：模型在极少或无需推理的情况下表现更优，这引发了一个根本性问题——音频智能能否真正受益于深度思考？我们推出Step-Audio-R1，这是首个成功在音频领域解锁推理能力的音频推理模型。通过我们提出的模态锚定推理蒸馏框架，Step-Audio-R1学会了生成与音频相关的推理链，这些推理链能真正植根于声学特征，而非产生脱离实际的推演。我们的模型展现出强大的音频推理能力，在涵盖语音、环境声和音乐的综合音频理解与推理基准测试中，不仅超越了Gemini 2.5 Pro，更达到了与最先进的Gemini 3 Pro相媲美的性能。这些结果表明，当推理过程被恰当锚定时，推理能力可成为跨模态的可迁移能力，从而将扩展推演从音频智能的负担转化为强大优势。通过建立首个成功的音频推理模型，Step-Audio-R1为构建真正跨感官模态深度思考的多模态推理系统开辟了新路径。",
    "url": "https://huggingface.co/papers/2511.15848",
    "arxiv_url": "https://arxiv.org/abs/2511.15848"
  },
  {
    "title": "Scaling Spatial Intelligence with Multimodal Foundation Models",
    "summary": "Despite remarkable progress, multimodal foundation models still exhibit surprising deficiencies in spatial intelligence. In this work, we explore scaling up multimodal foundation models to cultivate spatial intelligence within the SenseNova-SI family, built upon established multimodal foundations including visual understanding models (i.e., Qwen3-VL and InternVL3) and unified understanding and generation models (i.e., Bagel). We take a principled approach to constructing high-performing and robust spatial intelligence by systematically curating SenseNova-SI-8M: eight million diverse data samples under a rigorous taxonomy of spatial capabilities. SenseNova-SI demonstrates unprecedented performance across a broad range of spatial intelligence benchmarks: 68.7% on VSI-Bench, 43.3% on MMSI, 85.6% on MindCube, 54.6% on ViewSpatial, and 50.1% on SITE, while maintaining strong general multimodal understanding (e.g., 84.9% on MMBench-En). More importantly, we analyze the impact of data scaling, discuss early signs of emergent generalization capabilities enabled by diverse data training, analyze the risk of overfitting and language shortcuts, present a preliminary study on spatial chain-of-thought reasoning, and validate the potential downstream application. SenseNova-SI is an ongoing project, and this report will be updated continuously. All newly trained multimodal foundation models are publicly released to facilitate further research in this direction.",
    "translation": "标题：基于多模态基础模型的空间智能规模化研究\n\n摘要：尽管取得了显著进展，多模态基础模型在空间智能方面仍存在明显不足。本研究通过扩展多模态基础模型规模，在SenseNova-SI系列中培育空间智能能力。该系列建立在成熟的多模态基础之上，包括视觉理解模型（如Qwen3-VL和InternVL3）以及统一理解与生成模型（如Bagel）。我们采用系统化方法构建了包含800万多样化数据样本的SenseNova-SI-8M数据集，并基于严格的空间能力分类体系进行数据筛选，旨在建立高性能且稳健的空间智能模型。SenseNova-SI在广泛的空间智能基准测试中展现出卓越性能：VSI-Bench达68.7%，MMSI达43.3%，MindCube达85.6%，ViewSpatial达54.6%，SITE达50.1%，同时保持强大的通用多模态理解能力（如MMBench-En达84.9%）。更重要的是，我们分析了数据规模化的影响，探讨了多样化数据训练带来的涌现泛化能力早期迹象，研究了过拟合和语言捷径风险，提出了空间思维链推理的初步研究，并验证了潜在的下游应用价值。SenseNova-SI是持续发展的研究项目，本报告将定期更新。所有新训练的多模态基础模型均已公开发布，以推动该领域的深入研究。",
    "url": "https://huggingface.co/papers/2511.13719",
    "arxiv_url": "https://arxiv.org/abs/2511.13719"
  },
  {
    "title": "SAM 3D: 3Dfy Anything in Images",
    "summary": "We present SAM 3D, a generative model for visually grounded 3D object reconstruction, predicting geometry, texture, and layout from a single image. SAM 3D excels in natural images, where occlusion and scene clutter are common and visual recognition cues from context play a larger role. We achieve this with a human- and model-in-the-loop pipeline for annotating object shape, texture, and pose, providing visually grounded 3D reconstruction data at unprecedented scale. We learn from this data in a modern, multi-stage training framework that combines synthetic pretraining with real-world alignment, breaking the 3D \"data barrier\". We obtain significant gains over recent work, with at least a 5:1 win rate in human preference tests on real-world objects and scenes. We will release our code and model weights, an online demo, and a new challenging benchmark for in-the-wild 3D object reconstruction.",
    "translation": "标题：SAM 3D：图像三维化通用框架\n\n摘要：本文提出SAM 3D——一种基于视觉感知的三维物体重建生成模型，能够通过单张图像预测几何结构、纹理特征和空间布局。该模型在自然场景图像中表现卓越，能有效处理常见遮挡与场景杂乱问题，并充分利用上下文环境中的视觉识别线索。我们通过构建人机协同标注流程，实现了物体形状、纹理及位姿的精准标注，从而提供了规模空前的视觉基三维重建数据集。采用结合合成预训练与真实场景对齐的现代多阶段训练框架，成功突破了三维数据的\"资源壁垒\"。实验表明，本方法相较现有研究取得显著提升，在真实场景物体与环境的用户偏好测试中获胜率超过5:1。我们将公开核心代码与模型权重，提供在线演示系统，并建立具有挑战性的真实场景三维物体重建新基准。",
    "url": "https://huggingface.co/papers/2511.16624",
    "arxiv_url": "https://arxiv.org/abs/2511.16624"
  },
  {
    "title": "Video-as-Answer: Predict and Generate Next Video Event with Joint-GRPO",
    "summary": "While language models have become impactful in many real-world applications, video generation remains largely confined to entertainment. Motivated by video's inherent capacity to demonstrate physical-world information that is difficult to convey through language alone (e.g., imagine teaching someone to tie a tie using only text), we identify an underutilized opportunity to extend video as a new answer modality for Next-Event Prediction (NEP), formalized as Video-Next-Event Prediction (VNEP). While the established NEP task takes a video with a procedural or predictive question as input to predict the next event in text, VNEP requires dynamic video responses. This shift from telling to showing unlocks more intuitive and customized answers for procedural learning and creative exploration. However, this task remains challenging for existing models, as it demands an understanding of multimodal input, instruction-conditioned reasoning, and the generation of video with visual and semantic consistency. To address this, we introduce VANS, a model that leverages reinforcement learning to align a Vision-Language Model (VLM) with a Video Diffusion Model (VDM) for VNEP. The core of VANS is our proposed Joint-GRPO that orchestrates the VLM and VDM to function as a unit. Driven by a shared reward on their respective output, it optimizes the VLM to produce captions that are both accurate and friendly to visualize, while guiding the VDM to generate videos that are faithful to these captions and the input visual context. To enable this learning, we craft VANS-Data-100K, a dedicated dataset for the VNEP task. Experiments on procedural and predictive benchmarks demonstrate that VANS achieves state-of-the-art performance in both video event prediction and visualization. Codes are released in https://github.com/KlingTeam/VANS.",
    "translation": "标题：视频即答案：基于联合GRPO的下一事件预测与生成方法  \n\n摘要：尽管语言模型已在众多现实应用中产生重要影响，视频生成领域仍主要局限于娱乐用途。受视频与生俱来的物理世界信息呈现能力启发（例如仅通过文本描述系领带的教学场景），我们发现将视频拓展为下一代事件预测的新型答案模态存在未被充分利用的潜力，由此提出视频化下一代事件预测任务框架。传统NEP任务通过输入流程性视频与预测性问题来生成文本形式的下一事件预测，而VNEP要求以动态视频作为响应。这种从“讲述”到“呈现”的范式转变，为流程化学习和创意探索提供了更直观、可定制的解答方案。然而，该任务对现有模型仍具挑战性，因其需要理解多模态输入、完成指令条件推理，并生成具备视觉与语义一致性的视频。为此，我们提出VANS模型，通过强化学习将视觉语言模型与视频扩散模型对齐以解决VNEP任务。该模型核心是我们设计的联合GRPO机制，可协调VLM与VDM作为整体运行：基于各自输出的共享奖励信号，该机制在优化VLM生成兼具准确性与可视化友好性描述的同时，引导VDM生成符合描述内容与输入视觉语境的视频。为支撑此学习过程，我们构建了专用于VNEP任务的VANS-Data-100K数据集。在流程性与预测性基准测试上的实验表明，VANS在视频事件预测与可视化方面均达到最先进性能。代码已发布于https://github.com/KlingTeam/VANS。",
    "url": "https://huggingface.co/papers/2511.16669",
    "arxiv_url": "https://arxiv.org/abs/2511.16669"
  },
  {
    "title": "MiMo-Embodied: X-Embodied Foundation Model Technical Report",
    "summary": "We open-source MiMo-Embodied, the first cross-embodied foundation model to successfully integrate and achieve state-of-the-art performance in both Autonomous Driving and Embodied AI. MiMo-Embodied sets new records across 17 embodied AI benchmarks in Task Planning, Affordance Prediction and Spatial Understanding, while also excelling in 12 autonomous driving benchmarks across Environmental Perception, Status Prediction, and Driving Planning. Across these tasks, MiMo-Embodied significantly outperforms existing open-source, closed-source, and specialized baselines. Our results indicate that through multi-stage learning, curated data construction, and CoT/RL fine-tuning, these two domains exhibit strong positive transfer and mutually reinforce one another. We provide a detailed analysis of our model design and training methodologies to facilitate further research. Code and models are available at https://github.com/XiaomiMiMo/MiMo-Embodied.",
    "translation": "标题：MiMo-Embodied：跨具身基础模型技术报告\n\n摘要：我们开源了MiMo-Embodied——首个成功整合自动驾驶与具身人工智能两大领域并实现最先进性能的跨具身基础模型。该模型在任务规划、功能可供性预测与空间理解等17项具身AI基准测试中创下新纪录，同时在环境感知、状态预测与驾驶规划等12项自动驾驶基准测试中表现卓越。在所有任务中，MiMo-Embodied显著超越了现有开源、闭源及专业基线模型。研究表明，通过多阶段学习、精选数据构建以及思维链/强化学习微调，这两个领域展现出显著的积极迁移效应并形成相互增强。我们详细解析了模型设计与训练方法，以促进后续研究。代码与模型详见：https://github.com/XiaomiMiMo/MiMo-Embodied",
    "url": "https://huggingface.co/papers/2511.16518",
    "arxiv_url": "https://arxiv.org/abs/2511.16518"
  },
  {
    "title": "Agent0: Unleashing Self-Evolving Agents from Zero Data via Tool-Integrated Reasoning",
    "summary": "Large Language Model (LLM) Agents, often trained with Reinforcement Learning (RL), are constrained by a dependency on human-curated data, limiting scalability and tethering AI to human knowledge. Existing self-evolution frameworks offer an alternative but are typically restricted by the model's inherent capabilities and single-round interactions, hindering the development of complex curricula involving tool use or dynamic reasoning. We introduce Agent0, a fully autonomous framework that evolves high-performing agents without external data through multi-step co-evolution and seamless tool integration. Agent0 establishes a symbiotic competition between two agents initialized from the same base LLM: a curriculum agent that proposes increasingly challenging frontier tasks, and an executor agent that learns to solve them. We integrate external tools to enhance the executor's problem-solving capacity; this improvement, in turn, pressures the curriculum agent to construct more complex, tool-aware tasks. Through this iterative process, Agent0 establishes a self-reinforcing cycle that continuously produces high-quality curricula. Empirically, Agent0 substantially boosts reasoning capabilities, improving the Qwen3-8B-Base model by 18% on mathematical reasoning and 24% on general reasoning benchmarks. Code is available at https://github.com/aiming-lab/Agent0.",
    "translation": "标题：Agent0：通过工具集成推理实现零数据自演进智能体\n\n摘要：基于强化学习训练的大语言模型智能体长期受限于对人类标注数据的依赖，这不仅制约了系统扩展性，也将人工智能束缚在人类知识范畴内。现有自演进框架虽提供替代方案，但通常受限于模型固有能力和单轮交互机制，难以发展涉及工具使用与动态推理的复杂课程体系。我们提出Agent0——完全自主的智能体演进框架，通过多步协同进化与无缝工具集成，无需外部数据即可培育高性能智能体。该框架在相同基座大语言模型上构建两个智能体的共生竞争机制：课程智能体负责提出日益挑战的前沿任务，执行智能体则学习解决这些任务。我们集成外部工具以增强执行者的问题解决能力，这种提升反过来迫使课程智能体构建更具复杂性、工具感知的新型任务。通过此迭代过程，Agent0建立起自我强化的循环体系，持续生成高质量课程。实验表明，Agent0显著提升推理能力，在数学推理基准上使Qwen3-8B-Base模型提升18%，通用推理基准提升24%。代码已开源：https://github.com/aiming-lab/Agent0。",
    "url": "https://huggingface.co/papers/2511.16043",
    "arxiv_url": "https://arxiv.org/abs/2511.16043"
  },
  {
    "title": "Nemotron Elastic: Towards Efficient Many-in-One Reasoning LLMs",
    "summary": "Training a family of large language models targeting multiple scales and deployment objectives is prohibitively expensive, requiring separate training runs for each different size. Recent work on model compression through pruning and knowledge distillation has reduced this cost; however, this process still incurs hundreds of billions of tokens worth of training cost per compressed model. In this paper, we present Nemotron Elastic, a framework for building reasoning-oriented LLMs, including hybrid Mamba-Attention architectures, that embed multiple nested submodels within a single parent model, each optimized for different deployment configurations and budgets. Each of these submodels shares weights with the parent model and can be extracted zero-shot during deployment without additional training or fine-tuning. We enable this functionality through an end-to-end trained router, tightly coupled to a two-stage training curriculum designed specifically for reasoning models. We additionally introduce group-aware SSM elastification that preserves Mamba's structural constraints, heterogeneous MLP elastification, normalized MSE-based layer importance for improved depth selection, and knowledge distillation enabling simultaneous multi-budget optimization. We apply Nemotron Elastic to the Nemotron Nano V2 12B model, simultaneously producing a 9B and a 6B model using only 110B training tokens; this results in over 360x cost reduction compared to training model families from scratch, and around 7x compared to SoTA compression techniques. Each of the nested models performs on par or better than the SoTA in accuracy. Moreover, unlike other compression methods, the nested capability of our approach allows having a many-in-one reasoning model that has constant deployment memory against the number of models in the family.",
    "translation": "标题：Nemotron Elastic：迈向高效多合一推理大语言模型\n\n摘要：针对多尺度部署目标训练大语言模型家族的成本极其高昂，需要为不同规模模型分别进行独立训练。近期通过剪枝和知识蒸馏实现的模型压缩方法虽降低了成本，但每个压缩模型仍需消耗数千亿训练令牌。本文提出Nemotron Elastic框架，用于构建面向推理的混合Mamba-Attention架构大语言模型，该框架可在单一父模型中嵌入多个嵌套子模型，每个子模型针对不同部署配置和预算进行优化。这些子模型与父模型共享权重，无需额外训练或微调即可在部署时实现零样本提取。我们通过端到端训练的路由器实现此功能，该路由器与专为推理模型设计的两阶段训练课程紧密耦合。我们还提出了保持Mamba结构约束的分组感知SSM弹性化机制、异构MLP弹性化技术、基于归一化MSE的层重要性评估以改进深度选择，以及支持多预算同步优化的知识蒸馏方法。将Nemotron Elastic应用于Nemotron Nano V2 12B模型，仅使用1100亿训练令牌即可同步生成90亿和60亿参数模型：相比从头训练模型家族可实现360倍以上的成本降低，相较现有压缩技术也有约7倍的提升。所有嵌套模型在准确度上均达到或超越现有最优水平。更重要的是，与其他压缩方法不同，我们的嵌套特性可实现多合一推理模型，其部署内存需求在模型家族数量增加时保持恒定。",
    "url": "https://huggingface.co/papers/2511.16664",
    "arxiv_url": "https://arxiv.org/abs/2511.16664"
  },
  {
    "title": "Generalist Foundation Models Are Not Clinical Enough for Hospital Operations",
    "summary": "Hospitals and healthcare systems rely on operational decisions that determine patient flow, cost, and quality of care. Despite strong performance on medical knowledge and conversational benchmarks, foundation models trained on general text may lack the specialized knowledge required for these operational decisions. We introduce Lang1, a family of models (100M-7B parameters) pretrained on a specialized corpus blending 80B clinical tokens from NYU Langone Health's EHRs and 627B tokens from the internet. To rigorously evaluate Lang1 in real-world settings, we developed the REalistic Medical Evaluation (ReMedE), a benchmark derived from 668,331 EHR notes that evaluates five critical tasks: 30-day readmission prediction, 30-day mortality prediction, length of stay, comorbidity coding, and predicting insurance claims denial. In zero-shot settings, both general-purpose and specialized models underperform on four of five tasks (36.6%-71.7% AUROC), with mortality prediction being an exception. After finetuning, Lang1-1B outperforms finetuned generalist models up to 70x larger and zero-shot models up to 671x larger, improving AUROC by 3.64%-6.75% and 1.66%-23.66% respectively. We also observed cross-task scaling with joint finetuning on multiple tasks leading to improvement on other tasks. Lang1-1B effectively transfers to out-of-distribution settings, including other clinical tasks and an external health system. Our findings suggest that predictive capabilities for hospital operations require explicit supervised finetuning, and that this finetuning process is made more efficient by in-domain pretraining on EHR. Our findings support the emerging view that specialized LLMs can compete with generalist models in specialized tasks, and show that effective healthcare systems AI requires the combination of in-domain pretraining, supervised finetuning, and real-world evaluation beyond proxy benchmarks.",
    "translation": "标题：通用基础模型在医院运营场景中的临床适用性不足\n\n摘要：医院与医疗系统的运营决策直接影响患者流、成本控制及医疗质量。尽管通用文本训练的基础模型在医学知识和对话基准测试中表现优异，但其可能缺乏医疗运营决策所需的专业知识。我们推出Lang1模型系列（参数量1亿至70亿），其预训练语料融合了纽约大学朗格尼医疗中心电子健康记录的800亿临床标记符及互联网来源的6270亿标记符。为在真实场景中严格评估Lang1，我们开发了现实医疗评估基准（ReMedE），该基准基于668,331份电子健康记录笔记，涵盖五大关键任务：30天再入院预测、30天死亡率预测、住院时长预测、共病编码及保险拒赔预测。在零样本场景下，通用模型与专业模型在五项任务中有四项表现不佳（AUROC值36.6%-71.7%），仅死亡率预测例外。经微调后，Lang1-1B模型的表现优于参数量达其70倍的微调通用模型及参数量达其671倍的零样本模型，AUROC指标分别提升3.64%-6.75%和1.66%-23.66%。我们还观察到跨任务扩展效应——多任务联合微调可提升其他任务表现。Lang1-1B能有效迁移至分布外场景，包括其他临床任务及外部医疗系统。研究表明，医院运营的预测能力需要显式监督微调，而基于电子健康记录的领域内预训练可提升微调效率。这些发现印证了新兴观点：专业大语言模型可在特定任务中与通用模型竞争，同时表明构建高效医疗系统人工智能需要结合领域内预训练、监督微调及超越代理基准的真实场景评估。",
    "url": "https://huggingface.co/papers/2511.13703",
    "arxiv_url": "https://arxiv.org/abs/2511.13703"
  },
  {
    "title": "Thinking-while-Generating: Interleaving Textual Reasoning throughout Visual Generation",
    "summary": "Recent advances in visual generation have increasingly explored the integration of reasoning capabilities. They incorporate textual reasoning, i.e., think, either before (as pre-planning) or after (as post-refinement) the generation process, yet they lack on-the-fly multimodal interaction during the generation itself. In this preliminary study, we introduce Thinking-while-Generating (TwiG), the first interleaved framework that enables co-evolving textual reasoning throughout the visual generation process. As visual content is progressively generating, textual reasoning is interleaved to both guide upcoming local regions and reflect on previously synthesized ones. This dynamic interplay produces more context-aware and semantically rich visual outputs. To unveil the potential of this framework, we investigate three candidate strategies, zero-shot prompting, supervised fine-tuning (SFT) on our curated TwiG-50K dataset, and reinforcement learning (RL) via a customized TwiG-GRPO strategy, each offering unique insights into the dynamics of interleaved reasoning. We hope this work inspires further research into interleaving textual reasoning for enhanced visual generation. Code will be released at: https://github.com/ZiyuGuo99/Thinking-while-Generating.",
    "translation": "标题：边生成边思考：视觉生成过程中的文本推理交错机制\n\n摘要：视觉生成领域的最新进展逐渐探索推理能力的整合。现有方法通常在生成前（作为预规划）或生成后（作为后优化）引入文本推理，但缺乏生成过程中实时多模态交互。在本初步研究中，我们提出“边生成边思考”框架，这是首个在视觉生成全过程中实现文本推理协同演进的交错式架构。随着视觉内容的渐进生成，文本推理被交错运用于指导即将生成的局部区域，并对已合成内容进行反思。这种动态交互产生了更具上下文感知能力和语义丰富的视觉输出。为挖掘该框架潜力，我们研究了三类实现策略：零样本提示法、基于自建TwiG-50K数据集的有监督微调法，以及通过定制化TwiG-GRPO策略的强化学习法，每种策略都为交错推理的动态机制提供了独特见解。我们期待这项工作能推动文本推理交错技术助力视觉生成优化的深入研究。代码将发布于：https://github.com/ZiyuGuo99/Thinking-while-Generating。",
    "url": "https://huggingface.co/papers/2511.16671",
    "arxiv_url": "https://arxiv.org/abs/2511.16671"
  },
  {
    "title": "TurkColBERT: A Benchmark of Dense and Late-Interaction Models for Turkish Information Retrieval",
    "summary": "Neural information retrieval systems excel in high-resource languages but remain underexplored for morphologically rich, lower-resource languages such as Turkish. Dense bi-encoders currently dominate Turkish IR, yet late-interaction models -- which retain token-level representations for fine-grained matching -- have not been systematically evaluated. We introduce TurkColBERT, the first comprehensive benchmark comparing dense encoders and late-interaction models for Turkish retrieval. Our two-stage adaptation pipeline fine-tunes English and multilingual encoders on Turkish NLI/STS tasks, then converts them into ColBERT-style retrievers using PyLate trained on MS MARCO-TR. We evaluate 10 models across five Turkish BEIR datasets covering scientific, financial, and argumentative domains. Results show strong parameter efficiency: the 1.0M-parameter colbert-hash-nano-tr is 600times smaller than the 600M turkish-e5-large dense encoder while preserving over 71\\% of its average mAP. Late-interaction models that are 3--5times smaller than dense encoders significantly outperform them; ColmmBERT-base-TR yields up to +13.8\\% mAP on domain-specific tasks. For production-readiness, we compare indexing algorithms: MUVERA+Rerank is 3.33times faster than PLAID and offers +1.7\\% relative mAP gain. This enables low-latency retrieval, with ColmmBERT-base-TR achieving 0.54 ms query times under MUVERA. We release all checkpoints, configs, and evaluation scripts. Limitations include reliance on moderately sized datasets (leq50K documents) and translated benchmarks, which may not fully reflect real-world Turkish retrieval conditions; larger-scale MUVERA evaluations remain necessary.",
    "translation": "标题：TurkColBERT：土耳其语信息检索的稠密与延迟交互模型基准测试\n\n摘要：神经信息检索系统在高资源语言中表现卓越，但对土耳其语这类形态丰富、资源相对匮乏的语言研究仍显不足。当前土耳其语信息检索主要采用稠密双编码器，而保留词元级表示以进行细粒度匹配的延迟交互模型尚未得到系统评估。我们推出TurkColBERT——首个针对土耳其语检索的稠密编码器与延迟交互模型综合基准。通过两阶段适配流程：先在土耳其语NLI/STS任务上微调英语和多语言编码器，再使用基于MS MARCO-TR训练的PyLate将其转换为ColBERT式检索器。我们在涵盖科学、金融及论证领域的五个土耳其语BEIR数据集上评估了10个模型。结果显示卓越的参数效率：仅含1.0M参数的colbert-hash-nano-tr比600M参数的turkish-e5-large稠密编码器缩小600倍，同时保持其平均mAP值的71%以上。参数量比稠密编码器小3-5倍的延迟交互模型显著优于后者：ColmmBERT-base-TR在特定领域任务中mAP提升最高达+13.8%。针对生产就绪需求，我们比较了索引算法：MUVERA+重排序比PLAID快3.33倍，并实现+1.7%的相对mAP提升。这使得ColmmBERT-base-TR在MUVERA架构下可实现0.54毫秒查询延迟。我们公开了所有检查点、配置及评估脚本。局限性包括对中等规模数据集（≤5万文档）和翻译基准的依赖，这可能无法完全反映真实场景的土耳其语检索条件；更大规模的MUVERA评估仍有待开展。",
    "url": "https://huggingface.co/papers/2511.16528",
    "arxiv_url": "https://arxiv.org/abs/2511.16528"
  },
  {
    "title": "SRPO: Self-Referential Policy Optimization for Vision-Language-Action Models",
    "summary": "Vision-Language-Action (VLA) models excel in robotic manipulation but are constrained by their heavy reliance on expert demonstrations, leading to demonstration bias and limiting performance. Reinforcement learning (RL) is a vital post-training strategy to overcome these limits, yet current VLA-RL methods, including group-based optimization approaches, are crippled by severe reward sparsity. Relying on binary success indicators wastes valuable information in failed trajectories, resulting in low training efficiency. To solve this, we propose Self-Referential Policy Optimization (SRPO), a novel VLA-RL framework. SRPO eliminates the need for external demonstrations or manual reward engineering by leveraging the model's own successful trajectories, generated within the current training batch, as a self-reference. This allows us to assign a progress-wise reward to failed attempts. A core innovation is the use of latent world representations to measure behavioral progress robustly. Instead of relying on raw pixels or requiring domain-specific fine-tuning, we utilize the compressed, transferable encodings from a world model's latent space. These representations naturally capture progress patterns across environments, enabling accurate, generalized trajectory comparison. Empirical evaluations on the LIBERO benchmark demonstrate SRPO's efficiency and effectiveness. Starting from a supervised baseline with 48.9% success, SRPO achieves a new state-of-the-art success rate of 99.2% in just 200 RL steps, representing a 103% relative improvement without any extra supervision. Furthermore, SRPO shows substantial robustness, achieving a 167% performance improvement on the LIBERO-Plus benchmark.",
    "translation": "标题：SRPO：视觉-语言-动作模型的自参照策略优化方法\n\n摘要：视觉-语言-动作模型在机器人操作任务中表现出色，但其性能受限于对专家示范数据的严重依赖，导致存在示范偏差问题。强化学习作为克服这些局限的关键后训练策略，在现有VLA-RL方法（包括基于群体的优化方法）中却受困于严重的奖励稀疏性。仅依赖二元成功指标会浪费失败轨迹中的宝贵信息，导致训练效率低下。为此，我们提出自参照策略优化框架——一种创新的VLA-RL方法。该框架通过利用当前训练批次中模型自身生成的成功轨迹作为参照基准，无需外部示范数据或人工奖励工程，即可为失败尝试分配渐进式奖励。其核心创新在于采用潜在世界表征来稳健度量行为进展：通过世界模型潜在空间获得的压缩化、可迁移编码，无需依赖原始像素或领域特定微调，即可自然捕获跨环境进展模式，实现精准的通用化轨迹比较。在LIBERO基准测试中的实证研究表明，SRPO从监督基线48.9%的成功率起步，仅通过200步强化学习训练就将成功率提升至99.2%，相对改进幅度达103%且无需额外监督。在LIBERO-Plus基准上更实现了167%的性能提升，展现出卓越的鲁棒性。",
    "url": "https://huggingface.co/papers/2511.15605",
    "arxiv_url": "https://arxiv.org/abs/2511.15605"
  },
  {
    "title": "SAM2S: Segment Anything in Surgical Videos via Semantic Long-term Tracking",
    "summary": "Surgical video segmentation is crucial for computer-assisted surgery, enabling precise localization and tracking of instruments and tissues. Interactive Video Object Segmentation (iVOS) models such as Segment Anything Model 2 (SAM2) provide prompt-based flexibility beyond methods with predefined categories, but face challenges in surgical scenarios due to the domain gap and limited long-term tracking. To address these limitations, we construct SA-SV, the largest surgical iVOS benchmark with instance-level spatio-temporal annotations (masklets) spanning eight procedure types (61k frames, 1.6k masklets), enabling comprehensive development and evaluation for long-term tracking and zero-shot generalization. Building on SA-SV, we propose SAM2S, a foundation model enhancing SAM2 for Surgical iVOS through: (1) DiveMem, a trainable diverse memory mechanism for robust long-term tracking; (2) temporal semantic learning for instrument understanding; and (3) ambiguity-resilient learning to mitigate annotation inconsistencies across multi-source datasets. Extensive experiments demonstrate that fine-tuning on SA-SV enables substantial performance gains, with SAM2 improving by 12.99 average J\\&F over vanilla SAM2. SAM2S further advances performance to 80.42 average J\\&F, surpassing vanilla and fine-tuned SAM2 by 17.10 and 4.11 points respectively, while maintaining 68 FPS real-time inference and strong zero-shot generalization. Code and dataset will be released at https://jinlab-imvr.github.io/SAM2S.",
    "translation": "标题：SAM2S：通过语义长期追踪实现手术视频中的任意目标分割\n\n摘要：手术视频分割对计算机辅助手术至关重要，能够实现器械与组织的精确定位与追踪。基于预定义类别的方法存在局限性，而交互式视频目标分割（iVOS）模型（如SAM2）通过提示机制提供了更高灵活性，但在手术场景中因领域差异和长期追踪能力不足面临挑战。为突破这些限制，我们构建了SA-SV——目前规模最大的手术iVOS基准数据集，包含跨越8种手术类型的实例级时空标注（61千帧，1.6千个掩码片段），为长期追踪与零样本泛化研究提供全面支撑。基于该数据集，我们提出SAM2S基础模型，通过三大创新增强SAM2在手术iVOS中的性能：（1）DiveMem可训练多样性记忆机制，实现鲁棒长期追踪；（2）面向器械理解的时序语义学习；（3）抗标注歧义学习以缓解多源数据标注不一致问题。大量实验表明，在SA-SV上进行微调可显著提升性能，SAM2的平均J&F指标较原始版本提升12.99。SAM2S进一步将平均J&F提升至80.42，分别超越原始版与微调版SAM2达17.10和4.11个点，同时保持68 FPS的实时推理速度与强大的零样本泛化能力。代码与数据集将在https://jinlab-imvr.github.io/SAM2S 发布。",
    "url": "https://huggingface.co/papers/2511.16618",
    "arxiv_url": "https://arxiv.org/abs/2511.16618"
  },
  {
    "title": "NaTex: Seamless Texture Generation as Latent Color Diffusion",
    "summary": "We present NaTex, a native texture generation framework that predicts texture color directly in 3D space. In contrast to previous approaches that rely on baking 2D multi-view images synthesized by geometry-conditioned Multi-View Diffusion models (MVDs), NaTex avoids several inherent limitations of the MVD pipeline. These include difficulties in handling occluded regions that require inpainting, achieving precise mesh-texture alignment along boundaries, and maintaining cross-view consistency and coherence in both content and color intensity. NaTex features a novel paradigm that addresses the aforementioned issues by viewing texture as a dense color point cloud. Driven by this idea, we propose latent color diffusion, which comprises a geometry-awared color point cloud VAE and a multi-control diffusion transformer (DiT), entirely trained from scratch using 3D data, for texture reconstruction and generation. To enable precise alignment, we introduce native geometry control that conditions the DiT on direct 3D spatial information via positional embeddings and geometry latents. We co-design the VAE-DiT architecture, where the geometry latents are extracted via a dedicated geometry branch tightly coupled with the color VAE, providing fine-grained surface guidance that maintains strong correspondence with the texture. With these designs, NaTex demonstrates strong performance, significantly outperforming previous methods in texture coherence and alignment. Moreover, NaTex also exhibits strong generalization capabilities, either training-free or with simple tuning, for various downstream applications, e.g., material generation, texture refinement, and part segmentation and texturing.",
    "translation": "标题：NaTex：作为潜在颜色扩散的无缝纹理生成方法\n\n摘要：本文提出NaTex——一种在三维空间中直接预测纹理颜色的原生纹理生成框架。与现有基于几何条件多视图扩散模型（MVD）合成二维多视图图像进行烘焙的方法不同，NaTex规避了MVD流程的若干固有局限：包括处理需修复的遮挡区域、实现边界处网格与纹理的精确对齐、保持跨视图内容与色彩强度的一致性等难题。NaTex采用创新范式，将纹理视为稠密颜色点云，通过潜在颜色扩散技术解决上述问题。该技术包含几何感知的颜色点云变分自编码器（VAE）与多控制扩散变换器（DiT），全部基于三维数据从头训练，实现纹理重建与生成。为实现精确对齐，我们引入原生几何控制机制，通过位置嵌入与几何潜在变量将直接三维空间信息作为DiT的条件输入。我们协同设计VAE-DiT架构：通过专设几何分支与颜色VAE紧密耦合提取几何潜在变量，提供与纹理保持强对应关系的细粒度表面引导。实验表明，NaTex在纹理一致性与对齐精度上显著优于现有方法，并展现出强大的泛化能力——无需训练或仅需简单调参即可应用于材质生成、纹理优化、部件分割与纹理化等多种下游任务。",
    "url": "https://huggingface.co/papers/2511.16317",
    "arxiv_url": "https://arxiv.org/abs/2511.16317"
  },
  {
    "title": "PartUV: Part-Based UV Unwrapping of 3D Meshes",
    "summary": "UV unwrapping flattens 3D surfaces to 2D with minimal distortion, often requiring the complex surface to be decomposed into multiple charts. Although extensively studied, existing UV unwrapping methods frequently struggle with AI-generated meshes, which are typically noisy, bumpy, and poorly conditioned. These methods often produce highly fragmented charts and suboptimal boundaries, introducing artifacts and hindering downstream tasks. We introduce PartUV, a part-based UV unwrapping pipeline that generates significantly fewer, part-aligned charts while maintaining low distortion. Built on top of a recent learning-based part decomposition method PartField, PartUV combines high-level semantic part decomposition with novel geometric heuristics in a top-down recursive framework. It ensures each chart's distortion remains below a user-specified threshold while minimizing the total number of charts. The pipeline integrates and extends parameterization and packing algorithms, incorporates dedicated handling of non-manifold and degenerate meshes, and is extensively parallelized for efficiency. Evaluated across four diverse datasets, including man-made, CAD, AI-generated, and Common Shapes, PartUV outperforms existing tools and recent neural methods in chart count and seam length, achieves comparable distortion, exhibits high success rates on challenging meshes, and enables new applications like part-specific multi-tiles packing. Our project page is at https://www.zhaoningwang.com/PartUV.",
    "translation": "标题：PartUV：基于部件划分的三维网格UV展开方法\n\n摘要：UV展开技术将三维曲面以最小失真度展开为二维平面，通常需要将复杂曲面分解为多个图块。尽管该技术已被广泛研究，现有UV展开方法在处理AI生成网格时仍面临诸多挑战，这类网格通常存在噪点、凹凸不平和条件不良等问题。现有方法往往产生高度碎片化的图块和欠优的边界划分，导致伪影产生并影响下游任务。本文提出PartUV——一种基于部件划分的UV展开流程，在保持低失真度的同时能生成数量显著减少且与部件对齐的图块。该方案基于最新基于学习的部件分解方法PartField构建，通过自上而下的递归框架将高层语义部件分解与新型几何启发式算法相结合，在确保每个图块失真度低于用户设定阈值的同时，最小化图块总数。该流程集成并扩展了参数化与排布算法，包含对非流形与退化网格的专门处理，并采用大规模并行化以提升效率。在涵盖人造物体、CAD模型、AI生成网格和通用形状的四个数据集上的评估表明，PartUV在图块数量和接缝长度指标上优于现有工具与近期神经方法，达到可比拟的失真度，在挑战性网格上呈现高成功率，并支持部件级多贴图排布等新应用。项目主页详见：https://www.zhaoningwang.com/PartUV。",
    "url": "https://huggingface.co/papers/2511.16659",
    "arxiv_url": "https://arxiv.org/abs/2511.16659"
  },
  {
    "title": "TimeViper: A Hybrid Mamba-Transformer Vision-Language Model for Efficient Long Video Understanding",
    "summary": "We introduce TimeViper, a hybrid vision-language model designed to tackle challenges of long video understanding. Processing long videos demands both an efficient model architecture and an effective mechanism for handling extended temporal contexts. To this end, TimeViper adopts a hybrid Mamba-Transformer backbone that combines the efficiency of state-space models with the expressivity of attention mechanisms. Through this hybrid design, we reveal the vision-to-text information aggregation phenomenon, where information progressively flows from vision tokens to text tokens across increasing LLM depth, resulting in severe vision token redundancy. Motivated by this observation, we propose TransV, a token information transfer module that transfers and compresses vision tokens into instruction tokens while maintaining multimodal understanding capabilities. This design enables TimeViper to process hour-long videos exceeding 10,000 frames. Extensive experiments across multiple benchmarks demonstrate that TimeViper competes with state-of-the-art models while extending frame numbers. We further analyze attention behaviors of both Mamba and Transformer layers, offering new insights into hybrid model interpretability. This work represents an initial step towards developing, interpreting, and compressing hybrid Mamba-Transformer architectures.",
    "translation": "标题：TimeViper：面向高效长视频理解的混合Mamba-Transformer视觉语言模型\n\n摘要：本文提出TimeViper混合视觉语言模型，旨在解决长视频理解中的关键挑战。处理长视频既需要高效的模型架构，又需要有效的长时序上下文处理机制。为此，TimeViper采用混合Mamba-Transformer主干网络，将状态空间模型的高效性与注意力机制的强表达能力相结合。通过这种混合设计，我们揭示了视觉到文本的信息聚合现象：随着大语言模型层深增加，信息持续从视觉标记向文本标记流动，导致视觉标记出现严重冗余。基于此发现，我们提出TransV模块——一种在保持多模态理解能力的同时，将视觉标记转移并压缩至指令标记的令牌信息传输机制。该设计使TimeViper能够处理超过10,000帧的时长一小时视频。在多个基准测试上的广泛实验表明，TimeViper在显著扩展处理帧数的同时，仍可与最先进模型保持竞争力。我们进一步分析了Mamba与Transformer层的注意力机制特性，为混合模型的可解释性研究提供了新视角。本工作代表了在开发、解析和压缩混合Mamba-Transformer架构方向上的初步探索。",
    "url": "https://huggingface.co/papers/2511.16595",
    "arxiv_url": "https://arxiv.org/abs/2511.16595"
  },
  {
    "title": "EntroPIC: Towards Stable Long-Term Training of LLMs via Entropy Stabilization with Proportional-Integral Control",
    "summary": "Long-term training of large language models (LLMs) requires maintaining stable exploration to prevent the model from collapsing into sub-optimal behaviors. Entropy is crucial in this context, as it controls exploration and helps avoid premature convergence to sub-optimal solutions. However, existing reinforcement learning methods struggle to maintain an appropriate level of entropy, as the training process involves a mix of positive and negative samples, each affecting entropy in different ways across steps. To address this, we propose Entropy stablilization via Proportional-Integral Control (EntroPIC), a novel method that adaptively adjusts the influence of positive and negative samples by dynamically tuning their loss coefficients. This approach stabilizes entropy throughout training, ensuring efficient exploration and steady progress. We provide a comprehensive theoretical analysis for both on-policy and off-policy learning settings, demonstrating that EntroPIC is effective at controlling entropy in large-scale LLM training. Experimental results show that our method successfully maintains desired entropy levels, enabling stable and optimal RL training for LLMs.",
    "translation": "标题：EntroPIC：基于比例-积分控制的熵稳定方法实现大语言模型的长期稳定训练\n\n摘要：大语言模型的长期训练需要保持稳定的探索性，以防止模型坍缩至次优行为。熵在此过程中具有关键作用，它既能控制探索强度，又有助于避免过早收敛到次优解。然而现有强化学习方法难以维持恰当的熵水平，因为训练过程同时包含正负样本，且不同样本在训练步长中对熵的影响机制存在差异。为此，我们提出基于比例-积分控制的熵稳定方法（EntroPIC），该方法通过动态调整正负样本的损失系数来自适应调节其影响，从而在整个训练过程中实现熵稳定，确保高效探索与稳定进展。我们为同策略与异策略学习场景提供了完整的理论分析，证明EntroPIC能够有效控制大规模语言模型训练中的熵变化。实验结果表明，本方法能成功维持目标熵水平，为大语言模型实现稳定且最优的强化学习训练。",
    "url": "https://huggingface.co/papers/2511.15248",
    "arxiv_url": "https://arxiv.org/abs/2511.15248"
  },
  {
    "title": "FinTRec: Transformer Based Unified Contextual Ads Targeting and Personalization for Financial Applications",
    "summary": "Transformer-based architectures are widely adopted in sequential recommendation systems, yet their application in Financial Services (FS) presents distinct practical and modeling challenges for real-time recommendation. These include:a) long-range user interactions (implicit and explicit) spanning both digital and physical channels generating temporally heterogeneous context, b) the presence of multiple interrelated products require coordinated models to support varied ad placements and personalized feeds, while balancing competing business goals. We propose FinTRec, a transformer-based framework that addresses these challenges and its operational objectives in FS. While tree-based models have traditionally been preferred in FS due to their explainability and alignment with regulatory requirements, our study demonstrate that FinTRec offers a viable and effective shift toward transformer-based architectures. Through historic simulation and live A/B test correlations, we show FinTRec consistently outperforms the production-grade tree-based baseline. The unified architecture, when fine-tuned for product adaptation, enables cross-product signal sharing, reduces training cost and technical debt, while improving offline performance across all products. To our knowledge, this is the first comprehensive study of unified sequential recommendation modeling in FS that addresses both technical and business considerations.",
    "translation": "标题：FinTRec：基于Transformer的金融应用统一上下文广告定向与个性化框架\n\n摘要：基于Transformer的架构虽已广泛应用于序列推荐系统，但其在金融服务领域的实时推荐应用中仍面临独特的实践与建模挑战。这些挑战包括：a) 用户跨数字与实体渠道产生的长周期交互行为（隐式与显式）形成时序异构上下文；b) 多类关联产品并存需协同建模以支持多样化广告位投放与个性化信息流，同时平衡相互竞争的业务目标。我们提出FinTRec这一基于Transformer的框架，旨在解决金融服务领域的这些挑战及运营目标。尽管传统上基于树形模型的方案因可解释性及符合监管要求而更受金融服务领域青睐，但本研究证明FinTRec为转向基于Transformer的架构提供了可行有效的路径。通过历史模拟与线上A/B测试关联分析，我们验证了FinTRec持续优于生产级树形基线模型。该统一架构经过产品适配微调后，可实现跨产品信号共享，降低训练成本与技术负债，同时提升所有产品的离线性能。据我们所知，这是首个在金融服务领域兼顾技术实现与商业考量的统一序列推荐建模综合性研究。",
    "url": "https://huggingface.co/papers/2511.14865",
    "arxiv_url": "https://arxiv.org/abs/2511.14865"
  },
  {
    "title": "BioBench: A Blueprint to Move Beyond ImageNet for Scientific ML Benchmarks",
    "summary": "ImageNet-1K linear-probe transfer accuracy remains the default proxy for visual representation quality, yet it no longer predicts performance on scientific imagery. Across 46 modern vision model checkpoints, ImageNet top-1 accuracy explains only 34% of variance on ecology tasks and mis-ranks 30% of models above 75% accuracy. We present BioBench, an open ecology vision benchmark that captures what ImageNet misses. BioBench unifies 9 publicly released, application-driven tasks, 4 taxonomic kingdoms, and 6 acquisition modalities (drone RGB, web video, micrographs, in-situ and specimen photos, camera-trap frames), totaling 3.1M images. A single Python API downloads data, fits lightweight classifiers to frozen backbones, and reports class-balanced macro-F1 (plus domain metrics for FishNet and FungiCLEF); ViT-L models evaluate in 6 hours on an A6000 GPU. BioBench provides new signal for computer vision in ecology and a template recipe for building reliable AI-for-science benchmarks in any domain. Code and predictions are available at https://github.com/samuelstevens/biobench and results at https://samuelstevens.me/biobench.",
    "translation": "标题：BioBench：超越ImageNet的科学机器学习基准测试蓝图\n\n摘要：ImageNet-1K线性探针迁移准确度仍是衡量视觉表征质量的标准指标，但其已无法有效预测科学影像的性能表现。通过对46个现代视觉模型检查点的测试，ImageNet top-1准确度仅能解释生态学任务中34%的方差差异，且在准确度超过75%的模型中存在30%的误判。我们推出BioBench——一个开源的生态视觉基准测试集，旨在捕捉ImageNet所遗漏的关键维度。该基准整合了9项公开发布的应用驱动型任务，涵盖4个生物分类界和6种采集模态（无人机RGB影像、网络视频、显微图像、原位与标本照片、相机陷阱帧），总计310万张图像。通过统一的Python接口可实现数据下载、轻量级分类器与冻结主干网络的适配，并输出类别均衡的宏观F1值（同时提供FishNet和FungiCLEF的领域特定指标）；在A6000 GPU上评估ViT-L模型仅需6小时。BioBench不仅为生态计算机视觉研究提供了新的衡量标准，更为构建跨领域可靠人工智能科学基准建立了可复用的模板方案。代码及预测结果详见https://github.com/samuelstevens/biobench，完整结果可访问https://samuelstevens.me/biobench。",
    "url": "https://huggingface.co/papers/2511.16315",
    "arxiv_url": "https://arxiv.org/abs/2511.16315"
  },
  {
    "title": "Boosting Medical Visual Understanding From Multi-Granular Language Learning",
    "summary": "Recent advances in image-text pretraining have significantly enhanced visual understanding by aligning visual and textual representations. Contrastive Language-Image Pretraining (CLIP) has played a pivotal role in multimodal learning. However, its focus on single-label, single-granularity alignment limits its effectiveness in complex domains such as medical imaging, where images often correspond to multiple high-level labels (e.g., disease categories) across different annotation granularities (e.g., diagnostic description, clinical explanation). To address this, we propose Multi-Granular Language Learning (MGLL), a contrastive learning framework designed to improve both multi-label and cross-granularity alignment. MGLL leverages structured multi-label supervision, integrates textual descriptions across granularities, and introduces soft-label supervision with point-wise constraints to enhance alignment. MGLL employs smooth Kullback-Leibler (KL) divergence to ensure cross-granularity consistency while maintaining computational efficiency as a plug-and-play module for vision-language models. Pretrained on our constructed large-scale multi-granular datasets and evaluated across multiple datasets, MGLL outperforms other state-of-the-art methods in downstream tasks. The code is available at https://github.com/HUANGLIZI/MGLL{https://github.com/HUANGLIZI/MGLL}.",
    "translation": "标题：基于多粒度语言学习的医学视觉理解增强方法\n\n摘要：图像-文本预训练技术通过对齐视觉与文本表征，显著提升了视觉理解能力。对比语言-图像预训练（CLIP）在多模态学习中发挥了关键作用。然而该方法专注于单标签单粒度对齐，在医学影像等复杂领域存在局限性——该类图像通常对应多个高层级标签（如疾病类别）且具有不同注释粒度（如诊断描述、临床解释）。为此，我们提出多粒度语言学习（MGLL）框架，该对比学习框架旨在同时提升多标签与跨粒度对齐能力。MGLL通过结构化多标签监督机制，整合多粒度文本描述，并引入带逐点约束的软标签监督来增强对齐效果。该方法采用平滑KL散度确保跨粒度一致性，同时保持即插即用模块的计算效率，可适配各类视觉语言模型。基于构建的大规模多粒度数据集进行预训练，并在多个数据集上验证表明，MGLL在下游任务中性能优于现有先进方法。代码已开源：https://github.com/HUANGLIZI/MGLL。",
    "url": "https://huggingface.co/papers/2511.15943",
    "arxiv_url": "https://arxiv.org/abs/2511.15943"
  },
  {
    "title": "Draft and Refine with Visual Experts",
    "summary": "While recent Large Vision-Language Models (LVLMs) exhibit strong multimodal reasoning abilities, they often produce ungrounded or hallucinated responses because they rely too heavily on linguistic priors instead of visual evidence. This limitation highlights the absence of a quantitative measure of how much these models actually use visual information during reasoning. We propose Draft and Refine (DnR), an agent framework driven by a question-conditioned utilization metric. The metric quantifies the model's reliance on visual evidence by first constructing a query-conditioned relevance map to localize question-specific cues and then measuring dependence through relevance-guided probabilistic masking. Guided by this metric, the DnR agent refines its initial draft using targeted feedback from external visual experts. Each expert's output (such as boxes or masks) is rendered as visual cues on the image, and the model is re-queried to select the response that yields the largest improvement in utilization. This process strengthens visual grounding without retraining or architectural changes. Experiments across VQA and captioning benchmarks show consistent accuracy gains and reduced hallucination, demonstrating that measuring visual utilization provides a principled path toward more interpretable and evidence-driven multimodal agent systems.",
    "translation": "标题：基于视觉专家的草拟与优化框架\n\n摘要：当前的大型视觉语言模型虽展现出强大的多模态推理能力，但由于过度依赖语言先验而非视觉证据，常产生缺乏依据的幻觉响应。这一局限凸显出现有方法缺乏对模型在推理过程中实际使用视觉信息程度的量化评估。我们提出草拟与优化框架，该智能体框架由问题条件化利用度指标驱动。该指标通过构建查询条件化关联图来定位问题相关线索，再通过关联引导的概率掩码测量依赖程度，从而量化模型对视觉证据的依赖水平。在此指标引导下，DnR智能体通过外部视觉专家的针对性反馈优化初始回答。每位专家输出（如检测框或掩码）以视觉线索形式呈现在图像上，通过重新查询模型选择能最大程度提升利用度的响应。该过程无需重新训练或改变架构即可增强视觉基础。在视觉问答和图像描述基准测试中的实验表明，该方法持续提升准确率并降低幻觉现象，证明视觉利用度测量为构建更可解释、证据驱动的多模态智能体系统提供了原理性路径。",
    "url": "https://huggingface.co/papers/2511.11005",
    "arxiv_url": "https://arxiv.org/abs/2511.11005"
  }
]