[
  {
    "title": "First Frame Is the Place to Go for Video Content Customization",
    "summary": "What role does the first frame play in video generation models? Traditionally, it's viewed as the spatial-temporal starting point of a video, merely a seed for subsequent animation. In this work, we reveal a fundamentally different perspective: video models implicitly treat the first frame as a conceptual memory buffer that stores visual entities for later reuse during generation. Leveraging this insight, we show that it's possible to achieve robust and generalized video content customization in diverse scenarios, using only 20-50 training examples without architectural changes or large-scale finetuning. This unveils a powerful, overlooked capability of video generation models for reference-based video customization.",
    "translation": "标题：首帧：视频内容定制化的关键所在\n\n摘要：首帧在视频生成模型中究竟扮演着何种角色？传统观点将其视为视频的时空起点，仅是后续动画的生成种子。本研究提出一个根本性不同的视角：视频模型隐式地将首帧作为概念记忆缓冲区，存储视觉实体以供后续生成过程重复利用。基于这一发现，我们证明仅需20-50个训练样本，无需改变模型架构或进行大规模微调，即可在多样化场景中实现鲁棒且泛化性强的视频内容定制。这一发现揭示了视频生成模型在基于参考视频的定制化任务中强大却长期被忽视的能力。",
    "url": "https://huggingface.co/papers/2511.15700",
    "arxiv_url": "https://arxiv.org/abs/2511.15700"
  },
  {
    "title": "V-ReasonBench: Toward Unified Reasoning Benchmark Suite for Video Generation Models",
    "summary": "Recent progress in generative video models, such as Veo-3, has shown surprising zero-shot reasoning abilities, creating a growing need for systematic and reliable evaluation. We introduce V-ReasonBench, a benchmark designed to assess video reasoning across four key dimensions: structured problem-solving, spatial cognition, pattern-based inference, and physical dynamics. The benchmark is built from both synthetic and real-world image sequences and provides a diverse set of answer-verifiable tasks that are reproducible, scalable, and unambiguous. Evaluations of six state-of-the-art video models reveal clear dimension-wise differences, with strong variation in structured, spatial, pattern-based, and physical reasoning. We further compare video models with strong image models, analyze common hallucination behaviors, and study how video duration affects Chain-of-Frames reasoning. Overall, V-ReasonBench offers a unified and reproducible framework for measuring video reasoning and aims to support the development of models with more reliable, human-aligned reasoning skills.",
    "translation": "标题：V-ReasonBench：面向视频生成模型的统一推理基准测试套件\n\n摘要：随着Veo-3等生成式视频模型的最新进展展现出惊人的零样本推理能力，对系统化可靠评估的需求日益增长。我们推出V-ReasonBench基准测试框架，旨在从四个关键维度评估视频推理能力：结构化问题解决、空间认知、基于模式的推理及物理动态理解。该基准集成了合成与真实场景的图像序列，提供多样化且答案可验证的任务，具备可复现、可扩展和明确性等特性。通过对六个前沿视频模型的评估，我们观察到各维度推理能力存在显著差异，尤其在结构化、空间、模式识别及物理推理方面表现参差不齐。我们进一步将视频模型与强图像模型进行对比，分析常见的幻觉生成行为，并研究视频时长对帧序列推理链的影响。总体而言，V-ReasonBench为衡量视频推理能力提供了统一可复现的框架，旨在推动开发具有更可靠、更符合人类思维的推理能力的视频生成模型。",
    "url": "https://huggingface.co/papers/2511.16668",
    "arxiv_url": "https://arxiv.org/abs/2511.16668"
  },
  {
    "title": "Step-Audio-R1 Technical Report",
    "summary": "Recent advances in reasoning models have demonstrated remarkable success in text and vision domains through extended chain-of-thought deliberation. However, a perplexing phenomenon persists in audio language models: they consistently perform better with minimal or no reasoning, raising a fundamental question - can audio intelligence truly benefit from deliberate thinking? We introduce Step-Audio-R1, the first audio reasoning model that successfully unlocks reasoning capabilities in the audio domain. Through our proposed Modality-Grounded Reasoning Distillation (MGRD) framework, Step-Audio-R1 learns to generate audio-relevant reasoning chains that genuinely ground themselves in acoustic features rather than hallucinating disconnected deliberations. Our model exhibits strong audio reasoning capabilities, surpassing Gemini 2.5 Pro and achieving performance comparable to the state-of-the-art Gemini 3 Pro across comprehensive audio understanding and reasoning benchmarks spanning speech, environmental sounds, and music. These results demonstrate that reasoning is a transferable capability across modalities when appropriately anchored, transforming extended deliberation from a liability into a powerful asset for audio intelligence. By establishing the first successful audio reasoning model, Step-Audio-R1 opens new pathways toward building truly multimodal reasoning systems that think deeply across all sensory modalities.",
    "translation": "标题：Step-Audio-R1技术报告\n\n摘要：推理模型通过扩展的思维链推演在文本和视觉领域取得了显著成功。然而音频语言模型领域始终存在一个令人困惑的现象：模型在极少或无需推理的情况下表现更佳，这引发了一个根本性问题——音频智能是否真正受益于深度思考？我们推出Step-Audio-R1，这是首个成功在音频领域解锁推理能力的声音推理模型。通过我们提出的模态锚定推理蒸馏框架，该模型学会了生成与声学特征真正锚定的音频相关推理链，而非产生脱离实际的虚构推演。我们的模型展现出强大的音频推理能力，在涵盖语音、环境声和音乐的综合音频理解与推理基准测试中，不仅超越了Gemini 2.5 Pro，更达到了与最先进模型Gemini 3 Pro相媲美的性能。这些结果表明当推理过程被恰当锚定时，跨模态的推理能力具有可迁移性，从而将扩展推演从音频智能的负担转化为强大优势。通过建立首个成功的音频推理模型，Step-Audio-R1为构建真正跨感官模态深度思考的多模态推理系统开辟了新路径。",
    "url": "https://huggingface.co/papers/2511.15848",
    "arxiv_url": "https://arxiv.org/abs/2511.15848"
  },
  {
    "title": "Scaling Spatial Intelligence with Multimodal Foundation Models",
    "summary": "Despite remarkable progress, multimodal foundation models still exhibit surprising deficiencies in spatial intelligence. In this work, we explore scaling up multimodal foundation models to cultivate spatial intelligence within the SenseNova-SI family, built upon established multimodal foundations including visual understanding models (i.e., Qwen3-VL and InternVL3) and unified understanding and generation models (i.e., Bagel). We take a principled approach to constructing high-performing and robust spatial intelligence by systematically curating SenseNova-SI-8M: eight million diverse data samples under a rigorous taxonomy of spatial capabilities. SenseNova-SI demonstrates unprecedented performance across a broad range of spatial intelligence benchmarks: 68.7% on VSI-Bench, 43.3% on MMSI, 85.6% on MindCube, 54.6% on ViewSpatial, and 50.1% on SITE, while maintaining strong general multimodal understanding (e.g., 84.9% on MMBench-En). More importantly, we analyze the impact of data scaling, discuss early signs of emergent generalization capabilities enabled by diverse data training, analyze the risk of overfitting and language shortcuts, present a preliminary study on spatial chain-of-thought reasoning, and validate the potential downstream application. SenseNova-SI is an ongoing project, and this report will be updated continuously. All newly trained multimodal foundation models are publicly released to facilitate further research in this direction.",
    "translation": "标题：基于多模态基础模型的空间智能规模化研究\n\n摘要：尽管取得了显著进展，多模态基础模型在空间智能方面仍存在明显不足。本研究探索通过规模化扩展多模态基础模型来培育SenseNova-SI系列的空间智能，该系列建立在成熟的多模态基础之上，包括视觉理解模型（如Qwen3-VL和InternVL3）以及统一理解与生成模型（如Bagel）。我们采用系统化方法构建了高性能且稳健的空间智能模型：通过严格的空间能力分类体系，精心构建包含八百万个多样化数据样本的SenseNova-SI-8M数据集。SenseNova-SI在广泛的空间智能基准测试中展现出卓越性能：VSI-Bench达68.7%，MMSI达43.3%，MindCube达85.6%，ViewSpatial达54.6%，SITE达50.1%，同时保持强大的通用多模态理解能力（如MMBench-En达84.9%）。更重要的是，我们分析了数据规模化的影响，探讨了通过多样化数据训练实现涌现泛化能力的早期迹象，研究了过拟合和语言捷径风险，提出了空间思维链推理的初步研究，并验证了潜在的下游应用价值。SenseNova-SI是持续发展的项目，本报告将定期更新。所有新训练的多模态基础模型均已公开发布，以推动该领域的深入研究。",
    "url": "https://huggingface.co/papers/2511.13719",
    "arxiv_url": "https://arxiv.org/abs/2511.13719"
  },
  {
    "title": "SAM 3D: 3Dfy Anything in Images",
    "summary": "We present SAM 3D, a generative model for visually grounded 3D object reconstruction, predicting geometry, texture, and layout from a single image. SAM 3D excels in natural images, where occlusion and scene clutter are common and visual recognition cues from context play a larger role. We achieve this with a human- and model-in-the-loop pipeline for annotating object shape, texture, and pose, providing visually grounded 3D reconstruction data at unprecedented scale. We learn from this data in a modern, multi-stage training framework that combines synthetic pretraining with real-world alignment, breaking the 3D \"data barrier\". We obtain significant gains over recent work, with at least a 5:1 win rate in human preference tests on real-world objects and scenes. We will release our code and model weights, an online demo, and a new challenging benchmark for in-the-wild 3D object reconstruction.",
    "translation": "标题：SAM 3D：图像三维化通用框架\n\n摘要：本文提出SAM 3D——一种基于视觉感知的三维物体重建生成模型，能够通过单张图像预测几何结构、纹理特征和空间布局。该模型在自然图像处理中表现卓越，尤其适用于存在遮挡和场景杂乱的场景，其通过上下文视觉识别线索发挥重要作用。我们采用人机协同标注流程来获取物体形状、纹理及位姿信息，从而构建了规模空前的视觉感知三维重建数据集。通过结合合成预训练与真实场景对齐的现代多阶段训练框架，我们成功突破了三维数据的\"数据壁垒\"。实验结果表明，本方法相较现有研究取得显著提升，在真实物体与场景的人类偏好测试中获胜率至少达5:1。我们将公开源代码与模型权重、在线演示系统，以及针对真实场景三维重建的新挑战性基准测试集。",
    "url": "https://huggingface.co/papers/2511.16624",
    "arxiv_url": "https://arxiv.org/abs/2511.16624"
  },
  {
    "title": "Video-as-Answer: Predict and Generate Next Video Event with Joint-GRPO",
    "summary": "While language models have become impactful in many real-world applications, video generation remains largely confined to entertainment. Motivated by video's inherent capacity to demonstrate physical-world information that is difficult to convey through language alone (e.g., imagine teaching someone to tie a tie using only text), we identify an underutilized opportunity to extend video as a new answer modality for Next-Event Prediction (NEP), formalized as Video-Next-Event Prediction (VNEP). While the established NEP task takes a video with a procedural or predictive question as input to predict the next event in text, VNEP requires dynamic video responses. This shift from telling to showing unlocks more intuitive and customized answers for procedural learning and creative exploration. However, this task remains challenging for existing models, as it demands an understanding of multimodal input, instruction-conditioned reasoning, and the generation of video with visual and semantic consistency. To address this, we introduce VANS, a model that leverages reinforcement learning to align a Vision-Language Model (VLM) with a Video Diffusion Model (VDM) for VNEP. The core of VANS is our proposed Joint-GRPO that orchestrates the VLM and VDM to function as a unit. Driven by a shared reward on their respective output, it optimizes the VLM to produce captions that are both accurate and friendly to visualize, while guiding the VDM to generate videos that are faithful to these captions and the input visual context. To enable this learning, we craft VANS-Data-100K, a dedicated dataset for the VNEP task. Experiments on procedural and predictive benchmarks demonstrate that VANS achieves state-of-the-art performance in both video event prediction and visualization. Codes are released in https://github.com/KlingTeam/VANS.",
    "translation": "标题：视频即答案：基于联合GRPO的下一视频事件预测与生成方法\n\n摘要：尽管语言模型已在众多实际应用中产生重要影响，视频生成领域仍主要局限于娱乐用途。鉴于视频具有展现物理世界信息的天然优势——这些信息往往难以仅通过语言传达（例如仅用文字指导他人打领带）——我们发现了一个尚未充分开发的机遇：将视频扩展为下一代事件预测的新型答案模态，并将其形式化为视频下一代事件预测任务。传统NEP任务以包含流程性或预测性问题的视频作为输入，通过文本来预测下一事件，而VNEP则要求生成动态视频响应。这种从“讲述”到“展示”的转变，为流程化学习和创意探索开启了更直观、可定制的解答方式。然而，该任务对现有模型仍具挑战性，因其需要理解多模态输入、执行指令条件推理，并生成具有视觉与语义一致性的视频。为此，我们提出VANS模型，通过强化学习将视觉语言模型与视频扩散模型对齐以解决VNEP任务。VANS的核心是我们提出的联合GRPO机制，该机制协调VLM和VDM作为整体单元运行。基于对其各自输出的共享奖励驱动，该机制既优化VLM生成兼具准确性和可视化友好度的描述文本，同时指导VDM生成忠实于文本描述及输入视觉语境的视频。为支持此学习过程，我们构建了专用于VNEP任务的VANS-Data-100K数据集。在流程性和预测性基准测试上的实验表明，VANS在视频事件预测与可视化方面均实现了最先进的性能。代码已发布于https://github.com/KlingTeam/VANS。",
    "url": "https://huggingface.co/papers/2511.16669",
    "arxiv_url": "https://arxiv.org/abs/2511.16669"
  },
  {
    "title": "MiMo-Embodied: X-Embodied Foundation Model Technical Report",
    "summary": "We open-source MiMo-Embodied, the first cross-embodied foundation model to successfully integrate and achieve state-of-the-art performance in both Autonomous Driving and Embodied AI. MiMo-Embodied sets new records across 17 embodied AI benchmarks in Task Planning, Affordance Prediction and Spatial Understanding, while also excelling in 12 autonomous driving benchmarks across Environmental Perception, Status Prediction, and Driving Planning. Across these tasks, MiMo-Embodied significantly outperforms existing open-source, closed-source, and specialized baselines. Our results indicate that through multi-stage learning, curated data construction, and CoT/RL fine-tuning, these two domains exhibit strong positive transfer and mutually reinforce one another. We provide a detailed analysis of our model design and training methodologies to facilitate further research. Code and models are available at https://github.com/XiaomiMiMo/MiMo-Embodied.",
    "translation": "标题：MiMo-Embodied：跨具身基础模型技术报告\n\n摘要：我们开源了MiMo-Embodied——首个成功整合自动驾驶与具身人工智能两大领域并实现最先进性能的跨具身基础模型。该模型在任务规划、功能预测与空间理解等17项具身AI基准测试中创下新纪录，同时在环境感知、状态预测与驾驶规划等12项自动驾驶基准测试中表现卓越。在所有任务中，MiMo-Embodied显著超越了现有开源、闭源及专业基线模型。研究表明，通过多阶段学习、精细化数据构建以及思维链/强化学习微调，这两个领域展现出显著的积极迁移效应并形成相互增强。我们详细解析了模型设计与训练方法，以推动后续研究。代码与模型已发布于https://github.com/XiaomiMiMo/MiMo-Embodied。",
    "url": "https://huggingface.co/papers/2511.16518",
    "arxiv_url": "https://arxiv.org/abs/2511.16518"
  },
  {
    "title": "Agent0: Unleashing Self-Evolving Agents from Zero Data via Tool-Integrated Reasoning",
    "summary": "Large Language Model (LLM) Agents, often trained with Reinforcement Learning (RL), are constrained by a dependency on human-curated data, limiting scalability and tethering AI to human knowledge. Existing self-evolution frameworks offer an alternative but are typically restricted by the model's inherent capabilities and single-round interactions, hindering the development of complex curricula involving tool use or dynamic reasoning. We introduce Agent0, a fully autonomous framework that evolves high-performing agents without external data through multi-step co-evolution and seamless tool integration. Agent0 establishes a symbiotic competition between two agents initialized from the same base LLM: a curriculum agent that proposes increasingly challenging frontier tasks, and an executor agent that learns to solve them. We integrate external tools to enhance the executor's problem-solving capacity; this improvement, in turn, pressures the curriculum agent to construct more complex, tool-aware tasks. Through this iterative process, Agent0 establishes a self-reinforcing cycle that continuously produces high-quality curricula. Empirically, Agent0 substantially boosts reasoning capabilities, improving the Qwen3-8B-Base model by 18% on mathematical reasoning and 24% on general reasoning benchmarks. Code is available at https://github.com/aiming-lab/Agent0.",
    "translation": "标题：Agent0：通过工具集成推理实现零数据自进化智能体\n\n摘要：基于强化学习训练的大语言模型智能体长期受限于对人类标注数据的依赖，这不仅制约了系统扩展性，更将人工智能发展束缚于人类知识范畴。现有自进化框架虽提供替代方案，但通常受限于模型固有能力和单轮交互机制，难以发展涉及工具使用或动态推理的复杂课程体系。本文提出Agent0——一个完全自主的进化框架，通过多步协同进化与无缝工具集成，无需外部数据即可培育高性能智能体。该框架在相同基础大语言模型上构建两个智能体的共生竞争：课程智能体负责提出日益挑战性的前沿任务，执行智能体则学习解决这些任务。我们集成外部工具以增强执行者的问题解决能力，这种进步反过来推动课程智能体构建更具复杂性、工具感知的新任务。通过这种迭代机制，Agent0建立了自我强化的循环体系，持续生成高质量课程。实验结果表明，Agent0显著提升推理能力，在数学推理基准上使Qwen3-8B-Base模型提升18%，通用推理基准提升24%。代码已开源：https://github.com/aiming-lab/Agent0。",
    "url": "https://huggingface.co/papers/2511.16043",
    "arxiv_url": "https://arxiv.org/abs/2511.16043"
  },
  {
    "title": "Nemotron Elastic: Towards Efficient Many-in-One Reasoning LLMs",
    "summary": "Training a family of large language models targeting multiple scales and deployment objectives is prohibitively expensive, requiring separate training runs for each different size. Recent work on model compression through pruning and knowledge distillation has reduced this cost; however, this process still incurs hundreds of billions of tokens worth of training cost per compressed model. In this paper, we present Nemotron Elastic, a framework for building reasoning-oriented LLMs, including hybrid Mamba-Attention architectures, that embed multiple nested submodels within a single parent model, each optimized for different deployment configurations and budgets. Each of these submodels shares weights with the parent model and can be extracted zero-shot during deployment without additional training or fine-tuning. We enable this functionality through an end-to-end trained router, tightly coupled to a two-stage training curriculum designed specifically for reasoning models. We additionally introduce group-aware SSM elastification that preserves Mamba's structural constraints, heterogeneous MLP elastification, normalized MSE-based layer importance for improved depth selection, and knowledge distillation enabling simultaneous multi-budget optimization. We apply Nemotron Elastic to the Nemotron Nano V2 12B model, simultaneously producing a 9B and a 6B model using only 110B training tokens; this results in over 360x cost reduction compared to training model families from scratch, and around 7x compared to SoTA compression techniques. Each of the nested models performs on par or better than the SoTA in accuracy. Moreover, unlike other compression methods, the nested capability of our approach allows having a many-in-one reasoning model that has constant deployment memory against the number of models in the family.",
    "translation": "标题：Nemotron Elastic：迈向高效多合一推理大语言模型  \n\n摘要：针对多尺度与多部署目标训练大语言模型家族的成本极其高昂，每个不同规模的模型均需独立训练。近期通过剪枝与知识蒸馏实现的模型压缩方法虽降低了成本，但每个压缩模型仍需消耗数千亿标记的训练开销。本文提出Nemotron Elastic框架，用于构建面向推理的混合Mamba-Attention架构大语言模型，该框架可在单一父模型中嵌入多个嵌套子模型，每个子模型针对不同部署配置与预算进行优化。这些子模型与父模型共享权重，无需额外训练或微调即可在部署时零样本提取。我们通过端到端训练的路由器实现此功能，该路由器与专为推理模型设计的两阶段训练课程紧密耦合。我们还提出保留Mamba结构约束的组感知SSM弹性化方法、异构MLP弹性化技术、基于归一化MSE的层重要性评估以改进深度选择，以及支持多预算同步优化的知识蒸馏机制。将Nemotron Elastic应用于Nemotron Nano V2 12B模型，仅用1100亿训练标记即可同步生成90亿与60亿参数模型，相比从头训练模型家族成本降低360倍以上，相较前沿压缩技术缩减约7倍成本。所有嵌套模型在准确度上均达到或超越现有最优水平。此外，与其他压缩方法不同，本方案的嵌套特性可实现“多合一”推理模型，其部署内存占用随模型家族数量增加保持恒定。",
    "url": "https://huggingface.co/papers/2511.16664",
    "arxiv_url": "https://arxiv.org/abs/2511.16664"
  },
  {
    "title": "Generalist Foundation Models Are Not Clinical Enough for Hospital Operations",
    "summary": "Hospitals and healthcare systems rely on operational decisions that determine patient flow, cost, and quality of care. Despite strong performance on medical knowledge and conversational benchmarks, foundation models trained on general text may lack the specialized knowledge required for these operational decisions. We introduce Lang1, a family of models (100M-7B parameters) pretrained on a specialized corpus blending 80B clinical tokens from NYU Langone Health's EHRs and 627B tokens from the internet. To rigorously evaluate Lang1 in real-world settings, we developed the REalistic Medical Evaluation (ReMedE), a benchmark derived from 668,331 EHR notes that evaluates five critical tasks: 30-day readmission prediction, 30-day mortality prediction, length of stay, comorbidity coding, and predicting insurance claims denial. In zero-shot settings, both general-purpose and specialized models underperform on four of five tasks (36.6%-71.7% AUROC), with mortality prediction being an exception. After finetuning, Lang1-1B outperforms finetuned generalist models up to 70x larger and zero-shot models up to 671x larger, improving AUROC by 3.64%-6.75% and 1.66%-23.66% respectively. We also observed cross-task scaling with joint finetuning on multiple tasks leading to improvement on other tasks. Lang1-1B effectively transfers to out-of-distribution settings, including other clinical tasks and an external health system. Our findings suggest that predictive capabilities for hospital operations require explicit supervised finetuning, and that this finetuning process is made more efficient by in-domain pretraining on EHR. Our findings support the emerging view that specialized LLMs can compete with generalist models in specialized tasks, and show that effective healthcare systems AI requires the combination of in-domain pretraining, supervised finetuning, and real-world evaluation beyond proxy benchmarks.",
    "translation": "标题：通用基础模型尚不足以满足医院运营的临床需求\n\n摘要：医院与医疗系统的运行依赖于决定患者流、成本及护理质量的操作性决策。尽管通用文本训练的基础模型在医学知识和对话基准测试中表现优异，但其可能缺乏此类操作性决策所需的专业知识。我们推出Lang1模型系列（参数量1亿至70亿），其预训练语料融合了来自纽约大学朗格尼健康中心电子健康记录的800亿临床标记符及互联网来源的6270亿标记符。为在真实场景中严格评估Lang1，我们开发了现实医疗评估基准（ReMedE）——该基准源自668,331份电子健康记录，评估五大关键任务：30天再入院预测、30天死亡率预测、住院时长、共病编码及保险拒赔预测。在零样本设定下，通用模型与专业模型在五项任务中有四项表现不佳（AUROC值36.6%-71.7%），仅死亡率预测例外。经微调后，Lang1-1B模型的表现优于参数量达其70倍的微调通用模型及参数量达其671倍的零样本模型，AUROC指标分别提升3.64%-6.75%和1.66%-23.66%。我们还观察到跨任务扩展效应：对多任务联合微调可提升其他任务表现。Lang1-1B能有效迁移至分布外场景，包括其他临床任务及外部医疗系统。研究结果表明：医院运营的预测能力需要显式监督微调，而基于电子健康记录的领域内预训练可提升该微调过程的效率。我们的发现支持新兴观点——专业大语言模型可在特定任务中与通用模型竞争，并证明构建高效医疗系统人工智能需要结合领域内预训练、监督微调及超越代理基准的真实场景评估。",
    "url": "https://huggingface.co/papers/2511.13703",
    "arxiv_url": "https://arxiv.org/abs/2511.13703"
  },
  {
    "title": "Thinking-while-Generating: Interleaving Textual Reasoning throughout Visual Generation",
    "summary": "Recent advances in visual generation have increasingly explored the integration of reasoning capabilities. They incorporate textual reasoning, i.e., think, either before (as pre-planning) or after (as post-refinement) the generation process, yet they lack on-the-fly multimodal interaction during the generation itself. In this preliminary study, we introduce Thinking-while-Generating (TwiG), the first interleaved framework that enables co-evolving textual reasoning throughout the visual generation process. As visual content is progressively generating, textual reasoning is interleaved to both guide upcoming local regions and reflect on previously synthesized ones. This dynamic interplay produces more context-aware and semantically rich visual outputs. To unveil the potential of this framework, we investigate three candidate strategies, zero-shot prompting, supervised fine-tuning (SFT) on our curated TwiG-50K dataset, and reinforcement learning (RL) via a customized TwiG-GRPO strategy, each offering unique insights into the dynamics of interleaved reasoning. We hope this work inspires further research into interleaving textual reasoning for enhanced visual generation. Code will be released at: https://github.com/ZiyuGuo99/Thinking-while-Generating.",
    "translation": "标题：生成中思考：视觉生成过程中的文本推理交错机制\n\n摘要：视觉生成领域的最新进展逐渐探索推理能力的整合。现有方法通常在生成前（作为预规划）或生成后（作为后优化）引入文本推理，但缺乏生成过程中的实时多模态交互。在本初步研究中，我们提出\"生成中思考\"框架，这是首个在视觉生成全过程中实现文本推理与视觉内容协同演进的交错式架构。随着视觉内容的渐进生成，系统通过交错进行的文本推理既指导后续局部区域的生成，又对已合成内容进行反思。这种动态交互产生了更具上下文感知能力与语义丰富性的视觉输出。为探索该框架潜力，我们研究了三种实现策略：基于零样本提示的方法、基于自建TwiG-50K数据集的有监督微调方法，以及通过定制化TwiG-GRPO策略的强化学习方法，每种策略都为交错推理的动态机制提供了独特见解。我们期待这项工作能推动文本推理交错机制在增强视觉生成方面的深入研究。代码发布地址：https://github.com/ZiyuGuo99/Thinking-while-Generating。",
    "url": "https://huggingface.co/papers/2511.16671",
    "arxiv_url": "https://arxiv.org/abs/2511.16671"
  },
  {
    "title": "TurkColBERT: A Benchmark of Dense and Late-Interaction Models for Turkish Information Retrieval",
    "summary": "Neural information retrieval systems excel in high-resource languages but remain underexplored for morphologically rich, lower-resource languages such as Turkish. Dense bi-encoders currently dominate Turkish IR, yet late-interaction models -- which retain token-level representations for fine-grained matching -- have not been systematically evaluated. We introduce TurkColBERT, the first comprehensive benchmark comparing dense encoders and late-interaction models for Turkish retrieval. Our two-stage adaptation pipeline fine-tunes English and multilingual encoders on Turkish NLI/STS tasks, then converts them into ColBERT-style retrievers using PyLate trained on MS MARCO-TR. We evaluate 10 models across five Turkish BEIR datasets covering scientific, financial, and argumentative domains. Results show strong parameter efficiency: the 1.0M-parameter colbert-hash-nano-tr is 600times smaller than the 600M turkish-e5-large dense encoder while preserving over 71\\% of its average mAP. Late-interaction models that are 3--5times smaller than dense encoders significantly outperform them; ColmmBERT-base-TR yields up to +13.8\\% mAP on domain-specific tasks. For production-readiness, we compare indexing algorithms: MUVERA+Rerank is 3.33times faster than PLAID and offers +1.7\\% relative mAP gain. This enables low-latency retrieval, with ColmmBERT-base-TR achieving 0.54 ms query times under MUVERA. We release all checkpoints, configs, and evaluation scripts. Limitations include reliance on moderately sized datasets (leq50K documents) and translated benchmarks, which may not fully reflect real-world Turkish retrieval conditions; larger-scale MUVERA evaluations remain necessary.",
    "translation": "标题：TurkColBERT：土耳其语信息检索的稠密与延迟交互模型基准测试\n\n摘要：神经信息检索系统在高资源语言中表现卓越，但对土耳其语这类形态复杂、资源相对匮乏的语言研究仍显不足。当前土耳其语信息检索主要采用稠密双编码器，而保留词元级表示以进行细粒度匹配的延迟交互模型尚未得到系统评估。我们推出TurkColBERT——首个针对土耳其语检索的稠密编码器与延迟交互模型综合基准。通过两阶段适配流程，我们首先在土耳其语NLI/STS任务上微调英语和多语言编码器，随后利用基于MS MARCO-TR训练的PyLate将其转换为ColBERT式检索器。我们在涵盖科学、金融及论证领域的五个土耳其语BEIR数据集上评估了10个模型。结果显示卓越的参数效率：仅含1.0M参数的colbert-hash-nano-tr比600M参数的turkish-e5-large稠密编码器缩小600倍，却能保持其平均mAP值的71%以上。参数量比稠密编码器小3-5倍的延迟交互模型显著优于后者，其中ColmmBERT-base-TR在特定领域任务中mAP提升高达+13.8%。针对生产就绪需求，我们比较了索引算法：MUVERA+重排序比PLAID快3.33倍，并实现+1.7%的相对mAP增益。这使得ColmmBERT-base-TR在MUVERA架构下达到0.54毫秒查询延迟的低延迟检索。我们公开了所有检查点、配置和评估脚本。局限性包括对中等规模数据集（≤5万文档）和翻译基准的依赖，这可能无法完全反映真实场景的土耳其语检索条件；更大规模的MUVERA评估仍有待开展。",
    "url": "https://huggingface.co/papers/2511.16528",
    "arxiv_url": "https://arxiv.org/abs/2511.16528"
  },
  {
    "title": "SRPO: Self-Referential Policy Optimization for Vision-Language-Action Models",
    "summary": "Vision-Language-Action (VLA) models excel in robotic manipulation but are constrained by their heavy reliance on expert demonstrations, leading to demonstration bias and limiting performance. Reinforcement learning (RL) is a vital post-training strategy to overcome these limits, yet current VLA-RL methods, including group-based optimization approaches, are crippled by severe reward sparsity. Relying on binary success indicators wastes valuable information in failed trajectories, resulting in low training efficiency. To solve this, we propose Self-Referential Policy Optimization (SRPO), a novel VLA-RL framework. SRPO eliminates the need for external demonstrations or manual reward engineering by leveraging the model's own successful trajectories, generated within the current training batch, as a self-reference. This allows us to assign a progress-wise reward to failed attempts. A core innovation is the use of latent world representations to measure behavioral progress robustly. Instead of relying on raw pixels or requiring domain-specific fine-tuning, we utilize the compressed, transferable encodings from a world model's latent space. These representations naturally capture progress patterns across environments, enabling accurate, generalized trajectory comparison. Empirical evaluations on the LIBERO benchmark demonstrate SRPO's efficiency and effectiveness. Starting from a supervised baseline with 48.9% success, SRPO achieves a new state-of-the-art success rate of 99.2% in just 200 RL steps, representing a 103% relative improvement without any extra supervision. Furthermore, SRPO shows substantial robustness, achieving a 167% performance improvement on the LIBERO-Plus benchmark.",
    "translation": "标题：SRPO：视觉-语言-动作模型的自参照策略优化方法\n\n摘要：视觉-语言-动作模型在机器人操作任务中表现卓越，但其性能受限于对专家示范数据的严重依赖，导致存在示范偏差问题。强化学习作为克服这些限制的关键后训练策略，现有VLA-RL方法（包括基于群体的优化方法）却受困于严重的奖励稀疏性。仅依赖二元成功指标会浪费失败轨迹中的宝贵信息，导致训练效率低下。为此，我们提出自参照策略优化方法——一种新型VLA-RL框架。该方法通过利用当前训练批次中模型自身生成的成功轨迹作为自参照基准，无需外部示范数据或人工奖励工程，即可为失败尝试分配渐进式奖励。其核心创新在于利用潜在世界表征来稳健度量行为进展：通过世界模型潜在空间中的压缩可迁移编码，替代原始像素输入或领域特定微调需求，这些表征能自然捕捉跨环境进展模式，实现精准通用的轨迹比较。在LIBERO基准上的实证研究表明，SRPO从监督基线48.9%的成功率起步，仅通过200步强化学习训练即达到99.2%的最新最优成功率，在无额外监督情况下实现103%的相对提升。此外，SRPO在LIBERO-Plus基准上展现出显著鲁棒性，取得167%的性能提升。",
    "url": "https://huggingface.co/papers/2511.15605",
    "arxiv_url": "https://arxiv.org/abs/2511.15605"
  },
  {
    "title": "SAM2S: Segment Anything in Surgical Videos via Semantic Long-term Tracking",
    "summary": "Surgical video segmentation is crucial for computer-assisted surgery, enabling precise localization and tracking of instruments and tissues. Interactive Video Object Segmentation (iVOS) models such as Segment Anything Model 2 (SAM2) provide prompt-based flexibility beyond methods with predefined categories, but face challenges in surgical scenarios due to the domain gap and limited long-term tracking. To address these limitations, we construct SA-SV, the largest surgical iVOS benchmark with instance-level spatio-temporal annotations (masklets) spanning eight procedure types (61k frames, 1.6k masklets), enabling comprehensive development and evaluation for long-term tracking and zero-shot generalization. Building on SA-SV, we propose SAM2S, a foundation model enhancing SAM2 for Surgical iVOS through: (1) DiveMem, a trainable diverse memory mechanism for robust long-term tracking; (2) temporal semantic learning for instrument understanding; and (3) ambiguity-resilient learning to mitigate annotation inconsistencies across multi-source datasets. Extensive experiments demonstrate that fine-tuning on SA-SV enables substantial performance gains, with SAM2 improving by 12.99 average J\\&F over vanilla SAM2. SAM2S further advances performance to 80.42 average J\\&F, surpassing vanilla and fine-tuned SAM2 by 17.10 and 4.11 points respectively, while maintaining 68 FPS real-time inference and strong zero-shot generalization. Code and dataset will be released at https://jinlab-imvr.github.io/SAM2S.",
    "translation": "标题：SAM2S：通过语义长期跟踪实现手术视频中任意目标分割\n\n摘要：手术视频分割对计算机辅助手术至关重要，能够实现器械与组织的精确定位与跟踪。基于预定义类别的方法存在局限性，而交互式视频目标分割（iVOS）模型（如SAM2）通过提示机制提供了更灵活的解决方案，但在手术场景下面临领域差异和长期跟踪能力不足的挑战。为突破这些限制，我们构建了SA-SV——目前最大的手术iVOS基准数据集，包含跨越八种手术类型的实例级时空标注（61千帧，1.6千个掩码片段），为长期跟踪与零样本泛化研究提供全面支撑。基于该数据集，我们提出SAM2S基础模型，通过三大创新增强SAM2在手术iVOS中的性能：（1）DiveMem可训练多样性记忆机制，实现鲁棒长期跟踪；（2）面向器械理解的时序语义学习；（3）抗模糊学习策略以缓解多源数据标注不一致问题。大量实验表明，在SA-SV上进行微调可显著提升性能：SAM2的平均J&F指标较原始版本提升12.99。SAM2S进一步将性能推至80.42平均J&F，分别超越原始版与微调版SAM2达17.10和4.11个点，同时保持68 FPS实时推理速度与强大的零样本泛化能力。代码与数据集将在https://jinlab-imvr.github.io/SAM2S 发布。",
    "url": "https://huggingface.co/papers/2511.16618",
    "arxiv_url": "https://arxiv.org/abs/2511.16618"
  },
  {
    "title": "NaTex: Seamless Texture Generation as Latent Color Diffusion",
    "summary": "We present NaTex, a native texture generation framework that predicts texture color directly in 3D space. In contrast to previous approaches that rely on baking 2D multi-view images synthesized by geometry-conditioned Multi-View Diffusion models (MVDs), NaTex avoids several inherent limitations of the MVD pipeline. These include difficulties in handling occluded regions that require inpainting, achieving precise mesh-texture alignment along boundaries, and maintaining cross-view consistency and coherence in both content and color intensity. NaTex features a novel paradigm that addresses the aforementioned issues by viewing texture as a dense color point cloud. Driven by this idea, we propose latent color diffusion, which comprises a geometry-awared color point cloud VAE and a multi-control diffusion transformer (DiT), entirely trained from scratch using 3D data, for texture reconstruction and generation. To enable precise alignment, we introduce native geometry control that conditions the DiT on direct 3D spatial information via positional embeddings and geometry latents. We co-design the VAE-DiT architecture, where the geometry latents are extracted via a dedicated geometry branch tightly coupled with the color VAE, providing fine-grained surface guidance that maintains strong correspondence with the texture. With these designs, NaTex demonstrates strong performance, significantly outperforming previous methods in texture coherence and alignment. Moreover, NaTex also exhibits strong generalization capabilities, either training-free or with simple tuning, for various downstream applications, e.g., material generation, texture refinement, and part segmentation and texturing.",
    "translation": "标题：NaTex：作为潜在颜色扩散的无缝纹理生成方法\n\n摘要：本文提出NaTex——一种在三维空间中直接预测纹理颜色的原生纹理生成框架。与现有基于几何条件多视图扩散模型（MVD）合成二维多视图图像再进行烘焙的方法不同，NaTex规避了MVD流程的固有局限：包括处理遮挡区域需依赖修复、难以实现边界精确的网格-纹理对齐、以及保持跨视图内容与色彩强度的一致性等问题。NaTex采用将纹理视为稠密颜色点云的新范式，通过潜在颜色扩散技术解决上述挑战。该技术包含几何感知的颜色点云变分自编码器（VAE）与多控制扩散变换器（DiT），全部基于三维数据从零开始训练，用于纹理重建与生成。为实现精确对齐，我们引入原生几何控制机制，通过位置嵌入与几何潜在编码将直接三维空间信息作为DiT的条件。我们协同设计VAE-DiT架构，其中几何潜在编码通过专设的几何分支提取，该分支与颜色VAE紧密耦合，提供与纹理保持强对应关系的细粒度表面引导。实验表明，NaTex在纹理一致性与对齐精度上显著优于现有方法。此外，该框架在无需训练或简单调参的情况下，对材质生成、纹理优化、部件分割与纹理化等下游任务展现出强大的泛化能力。",
    "url": "https://huggingface.co/papers/2511.16317",
    "arxiv_url": "https://arxiv.org/abs/2511.16317"
  },
  {
    "title": "PartUV: Part-Based UV Unwrapping of 3D Meshes",
    "summary": "UV unwrapping flattens 3D surfaces to 2D with minimal distortion, often requiring the complex surface to be decomposed into multiple charts. Although extensively studied, existing UV unwrapping methods frequently struggle with AI-generated meshes, which are typically noisy, bumpy, and poorly conditioned. These methods often produce highly fragmented charts and suboptimal boundaries, introducing artifacts and hindering downstream tasks. We introduce PartUV, a part-based UV unwrapping pipeline that generates significantly fewer, part-aligned charts while maintaining low distortion. Built on top of a recent learning-based part decomposition method PartField, PartUV combines high-level semantic part decomposition with novel geometric heuristics in a top-down recursive framework. It ensures each chart's distortion remains below a user-specified threshold while minimizing the total number of charts. The pipeline integrates and extends parameterization and packing algorithms, incorporates dedicated handling of non-manifold and degenerate meshes, and is extensively parallelized for efficiency. Evaluated across four diverse datasets, including man-made, CAD, AI-generated, and Common Shapes, PartUV outperforms existing tools and recent neural methods in chart count and seam length, achieves comparable distortion, exhibits high success rates on challenging meshes, and enables new applications like part-specific multi-tiles packing. Our project page is at https://www.zhaoningwang.com/PartUV.",
    "translation": "标题：PartUV：基于部件划分的三维网格UV展开方法\n\n摘要：UV展开技术将三维曲面以最小失真度展开为二维平面，通常需要将复杂曲面分解为多个图块。尽管该领域已被广泛研究，现有UV展开方法在处理AI生成网格时仍面临严峻挑战，这类网格通常存在噪声干扰、表面崎岖和条件恶劣等问题。现有方法往往产生高度碎片化的图块和欠优的边界划分，导致伪影产生并影响下游任务。本文提出PartUV——基于部件划分的UV展开流程，在保持低失真度的同时生成数量显著减少且与部件对齐的图块。该方案基于最新基于学习的部件分解方法PartField构建，在自上而下的递归框架中，将高层语义部件分解与新型几何启发式算法相结合。该方法在确保每个图块失真度低于用户设定阈值的同时，最小化图块总数。该流程集成并拓展了参数化与排布算法，包含对非流形与退化网格的专门处理，并采用大规模并行化以提升效率。在涵盖人造物体、CAD模型、AI生成网格和通用形状的四个多样化数据集上的评估表明，PartUV在图块数量和接缝长度指标上优于现有工具与近期神经方法，达到可比拟的失真度，在挑战性网格上呈现高成功率，并支持部件级多贴图排布等新应用。项目主页详见：https://www.zhaoningwang.com/PartUV。",
    "url": "https://huggingface.co/papers/2511.16659",
    "arxiv_url": "https://arxiv.org/abs/2511.16659"
  },
  {
    "title": "TimeViper: A Hybrid Mamba-Transformer Vision-Language Model for Efficient Long Video Understanding",
    "summary": "We introduce TimeViper, a hybrid vision-language model designed to tackle challenges of long video understanding. Processing long videos demands both an efficient model architecture and an effective mechanism for handling extended temporal contexts. To this end, TimeViper adopts a hybrid Mamba-Transformer backbone that combines the efficiency of state-space models with the expressivity of attention mechanisms. Through this hybrid design, we reveal the vision-to-text information aggregation phenomenon, where information progressively flows from vision tokens to text tokens across increasing LLM depth, resulting in severe vision token redundancy. Motivated by this observation, we propose TransV, a token information transfer module that transfers and compresses vision tokens into instruction tokens while maintaining multimodal understanding capabilities. This design enables TimeViper to process hour-long videos exceeding 10,000 frames. Extensive experiments across multiple benchmarks demonstrate that TimeViper competes with state-of-the-art models while extending frame numbers. We further analyze attention behaviors of both Mamba and Transformer layers, offering new insights into hybrid model interpretability. This work represents an initial step towards developing, interpreting, and compressing hybrid Mamba-Transformer architectures.",
    "translation": "标题：TimeViper：面向高效长视频理解的混合Mamba-Transformer视觉语言模型\n\n摘要：本文提出TimeViper混合视觉语言模型，旨在解决长视频理解中的关键挑战。处理长视频既需要高效的模型架构，又需要有效的长时序上下文处理机制。为此，TimeViper采用混合Mamba-Transformer主干网络，将状态空间模型的高效性与注意力机制的强表达能力相结合。通过这种混合设计，我们揭示了视觉到文本的信息聚合现象：随着大语言模型层数加深，信息持续从视觉标记向文本标记流动，导致视觉标记出现严重冗余。基于此发现，我们提出TransV模块——一种在保持多模态理解能力的同时，将视觉标记转移并压缩至指令标记的传输机制。该设计使TimeViper能够处理超过10,000帧的时长一小时视频。在多个基准测试上的广泛实验表明，TimeViper在显著扩展处理帧数的同时，性能仍可与最先进模型媲美。我们进一步分析了Mamba与Transformer层的注意力机制，为混合模型的可解释性研究提供了新视角。本工作代表了在开发、解析和压缩混合Mamba-Transformer架构方向上的初步探索。",
    "url": "https://huggingface.co/papers/2511.16595",
    "arxiv_url": "https://arxiv.org/abs/2511.16595"
  },
  {
    "title": "EntroPIC: Towards Stable Long-Term Training of LLMs via Entropy Stabilization with Proportional-Integral Control",
    "summary": "Long-term training of large language models (LLMs) requires maintaining stable exploration to prevent the model from collapsing into sub-optimal behaviors. Entropy is crucial in this context, as it controls exploration and helps avoid premature convergence to sub-optimal solutions. However, existing reinforcement learning methods struggle to maintain an appropriate level of entropy, as the training process involves a mix of positive and negative samples, each affecting entropy in different ways across steps. To address this, we propose Entropy stablilization via Proportional-Integral Control (EntroPIC), a novel method that adaptively adjusts the influence of positive and negative samples by dynamically tuning their loss coefficients. This approach stabilizes entropy throughout training, ensuring efficient exploration and steady progress. We provide a comprehensive theoretical analysis for both on-policy and off-policy learning settings, demonstrating that EntroPIC is effective at controlling entropy in large-scale LLM training. Experimental results show that our method successfully maintains desired entropy levels, enabling stable and optimal RL training for LLMs.",
    "translation": "标题：EntroPIC：基于比例-积分控制的熵稳定方法实现大语言模型的长期稳定训练\n\n摘要：大语言模型的长期训练需要保持稳定的探索性，以防止模型坍缩至次优行为。熵在此过程中具有关键作用，它既能控制探索强度，又有助于避免过早收敛到次优解。然而现有强化学习方法难以维持恰当的熵水平，因为训练过程同时包含正负样本，且每类样本在不同训练阶段对熵的影响方式存在差异。为此，我们提出基于比例-积分控制的熵稳定方法（EntroPIC），该创新方法通过动态调整正负样本的损失系数来自适应调节其影响强度，从而在全程训练中稳定熵值，确保持续有效的探索与稳定进展。我们为同策略与异策略学习场景提供了完整的理论分析，证明EntroPIC能够有效控制大规模语言模型训练中的熵变。实验结果表明，本方法可成功将熵值维持在预期水平，为实现大语言模型的稳定最优强化学习训练提供了有效途径。",
    "url": "https://huggingface.co/papers/2511.15248",
    "arxiv_url": "https://arxiv.org/abs/2511.15248"
  },
  {
    "title": "FinTRec: Transformer Based Unified Contextual Ads Targeting and Personalization for Financial Applications",
    "summary": "Transformer-based architectures are widely adopted in sequential recommendation systems, yet their application in Financial Services (FS) presents distinct practical and modeling challenges for real-time recommendation. These include:a) long-range user interactions (implicit and explicit) spanning both digital and physical channels generating temporally heterogeneous context, b) the presence of multiple interrelated products require coordinated models to support varied ad placements and personalized feeds, while balancing competing business goals. We propose FinTRec, a transformer-based framework that addresses these challenges and its operational objectives in FS. While tree-based models have traditionally been preferred in FS due to their explainability and alignment with regulatory requirements, our study demonstrate that FinTRec offers a viable and effective shift toward transformer-based architectures. Through historic simulation and live A/B test correlations, we show FinTRec consistently outperforms the production-grade tree-based baseline. The unified architecture, when fine-tuned for product adaptation, enables cross-product signal sharing, reduces training cost and technical debt, while improving offline performance across all products. To our knowledge, this is the first comprehensive study of unified sequential recommendation modeling in FS that addresses both technical and business considerations.",
    "translation": "标题：FinTRec：基于Transformer的金融应用统一上下文广告定向与个性化系统\n\n摘要：基于Transformer的架构虽已广泛应用于序列推荐系统，但其在金融服务领域的实时推荐应用中仍面临独特的实践与建模挑战。这些挑战包括：a) 用户跨数字与实体渠道产生的长周期交互行为（隐式与显式）形成时序异构上下文；b) 多类关联产品并存需协调建模以支持多样化广告位投放与个性化信息流，同时平衡相互竞争的业务目标。我们提出FinTRec这一基于Transformer的框架，旨在应对金融服务领域的这些挑战及运营目标。尽管传统上基于树的模型因可解释性及符合监管要求更受金融服务领域青睐，但本研究证明FinTRec为转向基于Transformer的架构提供了可行有效的解决方案。通过历史模拟与线上A/B测试关联分析，我们证实FinTRec持续优于生产级基于树的基线模型。该统一架构经过产品适配微调后，可实现跨产品信号共享，降低训练成本与技术负债，同时全面提升所有产品的离线性能。据我们所知，这是首个在金融服务领域兼顾技术与业务考量的统一序列推荐建模综合性研究。",
    "url": "https://huggingface.co/papers/2511.14865",
    "arxiv_url": "https://arxiv.org/abs/2511.14865"
  },
  {
    "title": "BioBench: A Blueprint to Move Beyond ImageNet for Scientific ML Benchmarks",
    "summary": "ImageNet-1K linear-probe transfer accuracy remains the default proxy for visual representation quality, yet it no longer predicts performance on scientific imagery. Across 46 modern vision model checkpoints, ImageNet top-1 accuracy explains only 34% of variance on ecology tasks and mis-ranks 30% of models above 75% accuracy. We present BioBench, an open ecology vision benchmark that captures what ImageNet misses. BioBench unifies 9 publicly released, application-driven tasks, 4 taxonomic kingdoms, and 6 acquisition modalities (drone RGB, web video, micrographs, in-situ and specimen photos, camera-trap frames), totaling 3.1M images. A single Python API downloads data, fits lightweight classifiers to frozen backbones, and reports class-balanced macro-F1 (plus domain metrics for FishNet and FungiCLEF); ViT-L models evaluate in 6 hours on an A6000 GPU. BioBench provides new signal for computer vision in ecology and a template recipe for building reliable AI-for-science benchmarks in any domain. Code and predictions are available at https://github.com/samuelstevens/biobench and results at https://samuelstevens.me/biobench.",
    "translation": "标题：BioBench：超越ImageNet的科学机器学习基准蓝图\n\n摘要：ImageNet-1K线性探针迁移精度虽仍是衡量视觉表征质量的默认指标，但其对科学影像的性能预测已然失效。通过对46个现代视觉模型检查点的测试，ImageNet top-1准确率仅能解释生态学任务中34%的方差差异，且在准确率超过75%的模型中存在30%的误判。我们推出BioBench——一个能捕捉ImageNet缺失维度的开放式生态视觉基准。该基准整合了9项公开的应用驱动型任务，覆盖4个生物分类界域和6种采集模态（无人机RGB影像、网络视频、显微图像、原位与标本照片、相机陷阱帧），总计310万张图像。通过统一的Python接口可实现数据下载、轻量级分类器与冻结主干网络的适配，并输出类别均衡宏F1值（另含FishNet与FungiCLEF的领域特定指标）；在A6000 GPU上ViT-L模型的完整评估仅需6小时。BioBench不仅为生态计算机视觉提供了新的评估标尺，更为构建跨领域可靠的科学人工智能基准建立了可复现的范式模板。代码及预测结果详见https://github.com/samuelstevens/biobench，完整结果可访问https://samuelstevens.me/biobench。",
    "url": "https://huggingface.co/papers/2511.16315",
    "arxiv_url": "https://arxiv.org/abs/2511.16315"
  },
  {
    "title": "Boosting Medical Visual Understanding From Multi-Granular Language Learning",
    "summary": "Recent advances in image-text pretraining have significantly enhanced visual understanding by aligning visual and textual representations. Contrastive Language-Image Pretraining (CLIP) has played a pivotal role in multimodal learning. However, its focus on single-label, single-granularity alignment limits its effectiveness in complex domains such as medical imaging, where images often correspond to multiple high-level labels (e.g., disease categories) across different annotation granularities (e.g., diagnostic description, clinical explanation). To address this, we propose Multi-Granular Language Learning (MGLL), a contrastive learning framework designed to improve both multi-label and cross-granularity alignment. MGLL leverages structured multi-label supervision, integrates textual descriptions across granularities, and introduces soft-label supervision with point-wise constraints to enhance alignment. MGLL employs smooth Kullback-Leibler (KL) divergence to ensure cross-granularity consistency while maintaining computational efficiency as a plug-and-play module for vision-language models. Pretrained on our constructed large-scale multi-granular datasets and evaluated across multiple datasets, MGLL outperforms other state-of-the-art methods in downstream tasks. The code is available at https://github.com/HUANGLIZI/MGLL{https://github.com/HUANGLIZI/MGLL}.",
    "translation": "标题：基于多粒度语言学习的医学视觉理解能力提升\n\n摘要：图像-文本预训练技术通过对齐视觉与文本表征，显著提升了视觉理解能力。对比语言-图像预训练（CLIP）在多模态学习中发挥了关键作用。然而该方法侧重于单标签、单粒度对齐，在医学影像等复杂领域存在局限性——该类图像通常对应多个高层级标签（如疾病类别）且具有不同注释粒度（如诊断描述、临床解释）。为此，我们提出多粒度语言学习（MGLL）框架，该对比学习框架旨在同时提升多标签与跨粒度对齐能力。MGLL通过以下机制实现优化：利用结构化多标签监督，整合跨粒度文本描述，引入带逐点约束的软标签监督以增强对齐效果。该框架采用平滑KL散度确保跨粒度一致性，同时作为即插即用模块保持视觉语言模型的计算效率。基于我们构建的大规模多粒度数据集进行预训练，并在多个数据集上验证，MGLL在下游任务中超越了现有先进方法。代码已开源：https://github.com/HUANGLIZI/MGLL。",
    "url": "https://huggingface.co/papers/2511.15943",
    "arxiv_url": "https://arxiv.org/abs/2511.15943"
  },
  {
    "title": "Draft and Refine with Visual Experts",
    "summary": "While recent Large Vision-Language Models (LVLMs) exhibit strong multimodal reasoning abilities, they often produce ungrounded or hallucinated responses because they rely too heavily on linguistic priors instead of visual evidence. This limitation highlights the absence of a quantitative measure of how much these models actually use visual information during reasoning. We propose Draft and Refine (DnR), an agent framework driven by a question-conditioned utilization metric. The metric quantifies the model's reliance on visual evidence by first constructing a query-conditioned relevance map to localize question-specific cues and then measuring dependence through relevance-guided probabilistic masking. Guided by this metric, the DnR agent refines its initial draft using targeted feedback from external visual experts. Each expert's output (such as boxes or masks) is rendered as visual cues on the image, and the model is re-queried to select the response that yields the largest improvement in utilization. This process strengthens visual grounding without retraining or architectural changes. Experiments across VQA and captioning benchmarks show consistent accuracy gains and reduced hallucination, demonstrating that measuring visual utilization provides a principled path toward more interpretable and evidence-driven multimodal agent systems.",
    "translation": "标题：基于视觉专家协同的草拟-精修框架\n\n摘要：尽管当前的大型视觉语言模型展现出强大的多模态推理能力，但由于过度依赖语言先验而忽视视觉证据，常常产生缺乏依据或虚构的响应。这一局限凸显出现有方法缺乏对模型在推理过程中实际利用视觉信息程度的量化评估。我们提出草拟-精修框架，该智能体框架由问题条件化利用度量驱动：首先通过构建查询条件化关联图来定位问题相关视觉线索，继而通过关联引导的概率掩码测量模型依赖度，从而量化模型对视觉证据的依赖程度。在此度量引导下，DnR智能体通过外部视觉专家提供的定向反馈持续优化初始回答。各视觉专家输出（如检测框或分割掩码）以视觉标记形式呈现在图像上，通过重新查询模型选择能最大程度提升视觉利用率的响应。该流程无需重新训练或改变架构即可增强视觉基础。在视觉问答与图像描述基准测试中的实验表明，该方法持续提升准确率并有效减少虚构内容，证明视觉利用率量化为构建更可解释、证据驱动的多模态智能体系统提供了理论路径。",
    "url": "https://huggingface.co/papers/2511.11005",
    "arxiv_url": "https://arxiv.org/abs/2511.11005"
  }
]