[
  {
    "title": "DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models",
    "summary": "We introduce DeepSeek-V3.2, a model that harmonizes high computational efficiency with superior reasoning and agent performance. The key technical breakthroughs of DeepSeek-V3.2 are as follows: (1) DeepSeek Sparse Attention (DSA): We introduce DSA, an efficient attention mechanism that substantially reduces computational complexity while preserving model performance in long-context scenarios. (2) Scalable Reinforcement Learning Framework: By implementing a robust reinforcement learning protocol and scaling post-training compute, DeepSeek-V3.2 performs comparably to GPT-5. Notably, our high-compute variant, DeepSeek-V3.2-Speciale, surpasses GPT-5 and exhibits reasoning proficiency on par with Gemini-3.0-Pro, achieving gold-medal performance in both the 2025 International Mathematical Olympiad (IMO) and the International Olympiad in Informatics (IOI). (3) Large-Scale Agentic Task Synthesis Pipeline: To integrate reasoning into tool-use scenarios, we developed a novel synthesis pipeline that systematically generates training data at scale. This methodology facilitates scalable agentic post-training, yielding substantial improvements in generalization and instruction-following robustness within complex, interactive environments.",
    "translation": "标题：DeepSeek-V3.2：推动开源大型语言模型的前沿发展\n\n摘要：本文介绍DeepSeek-V3.2模型，该模型实现了高计算效率与卓越推理及智能体性能的协同优化。DeepSeek-V3.2的核心技术突破包括：（1）DeepSeek稀疏注意力机制：我们提出了一种高效注意力机制，在长上下文场景中显著降低计算复杂度的同时保持模型性能。（2）可扩展强化学习框架：通过实施鲁棒的强化学习协议并扩展后训练计算规模，DeepSeek-V3.2达到与GPT-5相当的性能。特别值得注意的是，我们的高计算变体DeepSeek-V3.2-Speciale在推理能力上超越GPT-5，达到与Gemini-3.0-Pro同等水平，并在2025年国际数学奥林匹克竞赛和国际信息学奥林匹克竞赛中均获得金牌级表现。（3）大规模智能体任务合成流程：为将推理能力融入工具使用场景，我们开发了创新的合成流程，系统化生成大规模训练数据。该方法实现了可扩展的智能体后训练，在复杂交互环境中显著提升了泛化能力与指令遵循的鲁棒性。",
    "url": "https://huggingface.co/papers/2512.02556",
    "arxiv_url": "https://arxiv.org/abs/2512.02556"
  },
  {
    "title": "ToolOrchestra: Elevating Intelligence via Efficient Model and Tool Orchestration",
    "summary": "Large language models are powerful generalists, yet solving deep and complex problems such as those of the Humanity's Last Exam (HLE) remains both conceptually challenging and computationally expensive. We show that small orchestrators managing other models and a variety of tools can both push the upper bound of intelligence and improve efficiency in solving difficult agentic tasks. We introduce ToolOrchestra, a method for training small orchestrators that coordinate intelligent tools. ToolOrchestra explicitly uses reinforcement learning with outcome-, efficiency-, and user-preference-aware rewards. Using ToolOrchestra, we produce Orchestrator, an 8B model that achieves higher accuracy at lower cost than previous tool-use agents while aligning with user preferences on which tools are to be used for a given query. On HLE, Orchestrator achieves a score of 37.1%, outperforming GPT-5 (35.1%) while being 2.5x more efficient. On tau2-Bench and FRAMES, Orchestrator surpasses GPT-5 by a wide margin while using only about 30% of the cost. Extensive analysis shows that Orchestrator achieves the best trade-off between performance and cost under multiple metrics, and generalizes robustly to unseen tools. These results demonstrate that composing diverse tools with a lightweight orchestration model is both more efficient and more effective than existing methods, paving the way for practical and scalable tool-augmented reasoning systems.",
    "translation": "标题：ToolOrchestra：通过高效模型与工具编排提升智能水平\n\n摘要：大型语言模型是强大的通用系统，但解决如“人类终极考试”（HLE）所涉及的深层复杂问题，在概念上仍具挑战性且计算成本高昂。本文研究表明，通过小型编排器管理其他模型及多样化工具，既能提升智能上限，又能提高解决困难智能体任务的效率。我们提出了ToolOrchestra方法，用于训练能够协调智能工具的小型编排器。该方法明确采用强化学习，并结合结果感知、效率感知和用户偏好感知的奖励机制。基于ToolOrchestra，我们训练出Orchestrator模型（参数量80亿），该模型在实现比以往工具使用智能体更高准确率的同时成本更低，并能根据用户偏好为给定查询选择合适的工具。在HLE测试中，Orchestrator获得37.1%的得分，优于GPT-5（35.1%），且效率提升2.5倍。在tau2-Bench和FRAMES基准测试中，Orchestrator以显著优势超越GPT-5，而成本仅为其约30%。深入分析表明，Orchestrator在多项指标下实现了性能与成本的最佳平衡，并对未见工具展现出稳健的泛化能力。这些结果证明，通过轻量级编排模型整合多样化工具，比现有方法更高效、更有效，为实用且可扩展的工具增强推理系统开辟了道路。",
    "url": "https://huggingface.co/papers/2511.21689",
    "arxiv_url": "https://arxiv.org/abs/2511.21689"
  },
  {
    "title": "MultiShotMaster: A Controllable Multi-Shot Video Generation Framework",
    "summary": "Current video generation techniques excel at single-shot clips but struggle to produce narrative multi-shot videos, which require flexible shot arrangement, coherent narrative, and controllability beyond text prompts. To tackle these challenges, we propose MultiShotMaster, a framework for highly controllable multi-shot video generation. We extend a pretrained single-shot model by integrating two novel variants of RoPE. First, we introduce Multi-Shot Narrative RoPE, which applies explicit phase shift at shot transitions, enabling flexible shot arrangement while preserving the temporal narrative order. Second, we design Spatiotemporal Position-Aware RoPE to incorporate reference tokens and grounding signals, enabling spatiotemporal-grounded reference injection. In addition, to overcome data scarcity, we establish an automated data annotation pipeline to extract multi-shot videos, captions, cross-shot grounding signals and reference images. Our framework leverages the intrinsic architectural properties to support multi-shot video generation, featuring text-driven inter-shot consistency, customized subject with motion control, and background-driven customized scene. Both shot count and duration are flexibly configurable. Extensive experiments demonstrate the superior performance and outstanding controllability of our framework.",
    "translation": "标题：MultiShotMaster：一种可控的多镜头视频生成框架\n\n摘要：当前的视频生成技术擅长生成单镜头片段，但在生成叙事性多镜头视频方面仍面临挑战。这类视频不仅需要灵活的镜头编排和连贯的叙事逻辑，还要求具备超越文本提示的可控性。为应对这些挑战，我们提出了MultiShotMaster，一个高度可控的多镜头视频生成框架。我们通过集成两种新颖的RoPE变体，对预训练的单镜头模型进行了扩展。首先，我们引入了多镜头叙事RoPE，它在镜头转场处施加显式的相位偏移，从而在保持时序叙事顺序的同时实现灵活的镜头编排。其次，我们设计了时空位置感知RoPE，以融入参考标记和定位信号，实现基于时空定位的参考信息注入。此外，为克服数据稀缺问题，我们建立了一个自动化数据标注流程，用于提取多镜头视频、描述文本、跨镜头定位信号以及参考图像。本框架利用其内在的架构特性支持多镜头视频生成，具备文本驱动的镜头间一致性、支持运动控制的定制主体以及背景驱动的定制场景等特征。镜头数量和时长均可灵活配置。大量实验证明了我们框架的卓越性能和出色的可控性。",
    "url": "https://huggingface.co/papers/2512.03041",
    "arxiv_url": "https://arxiv.org/abs/2512.03041"
  },
  {
    "title": "MG-Nav: Dual-Scale Visual Navigation via Sparse Spatial Memory",
    "summary": "We present MG-Nav (Memory-Guided Navigation), a dual-scale framework for zero-shot visual navigation that unifies global memory-guided planning with local geometry-enhanced control. At its core is the Sparse Spatial Memory Graph (SMG), a compact, region-centric memory where each node aggregates multi-view keyframe and object semantics, capturing both appearance and spatial structure while preserving viewpoint diversity. At the global level, the agent is localized on SMG and a goal-conditioned node path is planned via an image-to-instance hybrid retrieval, producing a sequence of reachable waypoints for long-horizon guidance. At the local level, a navigation foundation policy executes these waypoints in point-goal mode with obstacle-aware control, and switches to image-goal mode when navigating from the final node towards the visual target. To further enhance viewpoint alignment and goal recognition, we introduce VGGT-adapter, a lightweight geometric module built on the pre-trained VGGT model, which aligns observation and goal features in a shared 3D-aware space. MG-Nav operates global planning and local control at different frequencies, using periodic re-localization to correct errors. Experiments on HM3D Instance-Image-Goal and MP3D Image-Goal benchmarks demonstrate that MG-Nav achieves state-of-the-art zero-shot performance and remains robust under dynamic rearrangements and unseen scene conditions.",
    "translation": "标题：MG-Nav：基于稀疏空间记忆的双尺度视觉导航框架\n\n摘要：本文提出MG-Nav（记忆引导导航），一种用于零样本视觉导航的双尺度框架，该框架将全局记忆引导规划与局部几何增强控制相统一。其核心是稀疏空间记忆图（SMG），这是一种以区域为中心的紧凑记忆结构，其中每个节点聚合多视角关键帧与物体语义信息，在保持视角多样性的同时捕捉外观与空间结构。在全局层面，智能体基于SMG进行定位，并通过图像-实例混合检索规划目标条件节点路径，生成一系列可达航点以实现长程导航引导。在局部层面，导航基础策略以点目标模式结合障碍物感知控制执行这些航点，并在从最终节点向视觉目标导航时切换至图像目标模式。为进一步增强视角对齐与目标识别能力，我们提出VGGT适配器——一种基于预训练VGGT模型构建的轻量化几何模块，可在共享三维感知空间中对齐观测特征与目标特征。MG-Nav以不同频率执行全局规划与局部控制，并通过周期性重定位修正误差。在HM3D实例-图像-目标与MP3D图像-目标基准测试上的实验表明，MG-Nav实现了最先进的零样本性能，并在动态场景重组与未知场景条件下保持鲁棒性。",
    "url": "https://huggingface.co/papers/2511.22609",
    "arxiv_url": "https://arxiv.org/abs/2511.22609"
  },
  {
    "title": "Skywork-R1V4: Toward Agentic Multimodal Intelligence through Interleaved Thinking with Images and DeepResearch",
    "summary": "Despite recent progress in multimodal agentic systems, existing approaches often treat image manipulation and web search as disjoint capabilities, rely heavily on costly reinforcement learning, and lack planning grounded in real tool-execution traces. To address these limitations, we present Skywork-R1V4, a 30B (A3B) parameter multimodal agentic model that unifies multimodal planning, active image manipulation (\"thinking with images\"), deep multimodal search, and, most critically, interleaved reasoning that dynamically alternates between visual operations and external knowledge retrieval. Trained solely via supervised fine-tuning on fewer than 30,000 high-quality, planning-execution-consistent trajectories and validated through stepwise consistency filtering, Skywork-R1V4 achieves state-of-the-art results across perception and multimodal search benchmarks: it scores 66.1 on MMSearch and 67.2 on FVQA, surpassing Gemini 2.5 Flash on all 11 metrics. Skywork-R1V4 exhibits emergent long-horizon reasoning at inference time, successfully orchestrating more than 10 tool calls to solve complex, multi-step tasks. Our results demonstrate that sophisticated agentic multimodal intelligence can be achieved through carefully curated supervised learning alone, without any reliance on reinforcement learning.",
    "translation": "标题：Skywork-R1V4：通过图像与深度研究的交错思考迈向具身多模态智能\n\n摘要：尽管多模态具身系统近期取得了进展，但现有方法通常将图像操作与网络搜索视为分离的能力，严重依赖高成本的强化学习，且缺乏基于真实工具执行轨迹的规划。为应对这些局限，我们提出了Skywork-R1V4——一个拥有300亿（实际30亿）参数的多模态具身模型。该模型统一了多模态规划、主动图像操作（“图像思考”）、深度多模态搜索，以及最关键的在视觉操作与外部知识检索之间动态交替的交错推理能力。通过仅对少于3万条高质量、规划-执行一致的轨迹进行监督微调训练，并经过逐步一致性过滤验证，Skywork-R1V4在感知与多模态搜索基准测试中取得了领先性能：在MMSearch上获得66.1分，在FVQA上获得67.2分，全部11项指标均超越Gemini 2.5 Flash模型。Skywork-R1V4在推理时展现出新兴的长程推理能力，能成功协调超过10次工具调用来解决复杂的多步骤任务。我们的结果表明，无需依赖强化学习，仅通过精心构建的监督学习即可实现高度复杂的具身多模态智能。",
    "url": "https://huggingface.co/papers/2512.02395",
    "arxiv_url": "https://arxiv.org/abs/2512.02395"
  },
  {
    "title": "DualCamCtrl: Dual-Branch Diffusion Model for Geometry-Aware Camera-Controlled Video Generation",
    "summary": "This paper presents DualCamCtrl, a novel end-to-end diffusion model for camera-controlled video generation. Recent works have advanced this field by representing camera poses as ray-based conditions, yet they often lack sufficient scene understanding and geometric awareness. DualCamCtrl specifically targets this limitation by introducing a dual-branch framework that mutually generates camera-consistent RGB and depth sequences. To harmonize these two modalities, we further propose the Semantic Guided Mutual Alignment (SIGMA) mechanism, which performs RGB-depth fusion in a semantics-guided and mutually reinforced manner. These designs collectively enable DualCamCtrl to better disentangle appearance and geometry modeling, generating videos that more faithfully adhere to the specified camera trajectories. Additionally, we analyze and reveal the distinct influence of depth and camera poses across denoising stages and further demonstrate that early and late stages play complementary roles in forming global structure and refining local details. Extensive experiments demonstrate that DualCamCtrl achieves more consistent camera-controlled video generation, with over 40\\% reduction in camera motion errors compared with prior methods. Our project page: https://soyouthinkyoucantell.github.io/dualcamctrl-page/",
    "translation": "标题：DualCamCtrl：用于几何感知相机控制视频生成的双分支扩散模型\n\n摘要：本文提出DualCamCtrl，一种用于相机控制视频生成的新型端到端扩散模型。现有研究通过将相机位姿表示为基于光线的条件推动了该领域发展，但这些方法往往缺乏充分的场景理解与几何感知能力。DualCamCtrl针对这一局限性，设计了能协同生成相机一致RGB序列与深度序列的双分支框架。为协调这两种模态，我们进一步提出语义引导互对齐机制，以语义引导、相互增强的方式实现RGB与深度信息的融合。这些设计使DualCamCtrl能更好解耦外观与几何建模，生成更精准遵循指定相机轨迹的视频。此外，我们分析揭示了深度信息与相机位姿在去噪各阶段的差异化影响，并论证了早期与晚期阶段在构建全局结构与细化局部细节方面的互补作用。大量实验表明，DualCamCtrl实现了更一致的相机控制视频生成，其相机运动误差较现有方法降低超过40%。项目页面：https://soyouthinkyoucantell.github.io/dualcamctrl-page/",
    "url": "https://huggingface.co/papers/2511.23127",
    "arxiv_url": "https://arxiv.org/abs/2511.23127"
  },
  {
    "title": "Guided Self-Evolving LLMs with Minimal Human Supervision",
    "summary": "AI self-evolution has long been envisioned as a path toward superintelligence, where models autonomously acquire, refine, and internalize knowledge from their own learning experiences. Yet in practice, unguided self-evolving systems often plateau quickly or even degrade as training progresses. These failures arise from issues such as concept drift, diversity collapse, and mis-evolution, as models reinforce their own biases and converge toward low-entropy behaviors. To enable models to self-evolve in a stable and controllable manner while minimizing reliance on human supervision, we introduce R-Few, a guided Self-Play Challenger-Solver framework that incorporates lightweight human oversight through in-context grounding and mixed training. At each iteration, the Challenger samples a small set of human-labeled examples to guide synthetic question generation, while the Solver jointly trains on human and synthetic examples under an online, difficulty-based curriculum. Across math and general reasoning benchmarks, R-Few achieves consistent and iterative improvements. For example, Qwen3-8B-Base improves by +3.0 points over R-Zero on math tasks and achieves performance on par with General-Reasoner, despite the latter being trained on 20 times more human data. Ablation studies confirm the complementary contributions of grounded challenger training and curriculum-based solver training, and further analysis shows that R-Few mitigates drift, yielding more stable and controllable co-evolutionary dynamics.",
    "translation": "标题：基于最小化人工监督的引导式大语言模型自演化\n\n摘要：人工智能的自演化长期以来被视为通往超级智能的路径，即模型能够从其自身学习经验中自主获取、精化并内化知识。然而在实践中，无引导的自演化系统往往在训练过程中迅速进入平台期甚至性能退化。这类失败源于概念漂移、多样性崩溃与错误演化等问题，即模型不断强化自身偏见并收敛至低熵行为。为实现模型在稳定可控的前提下进行自演化，并最大限度减少对人类监督的依赖，本文提出R-Few——一种融合情境锚定与混合训练的引导式自我博弈挑战者-求解器框架。在每轮迭代中，挑战者通过少量人工标注样本引导合成问题生成，而求解器则依据动态难度课程，对人工与合成样本进行联合训练。在数学与通用推理基准测试中，R-Few实现了持续迭代的性能提升。以Qwen3-8B-Base模型为例，其在数学任务上较R-Zero提升3.0分，且性能与使用20倍人工数据训练的General-Reasoner模型相当。消融实验证实了锚定式挑战者训练与课程化求解器训练的互补作用，进一步分析表明R-Few能有效缓解概念漂移，产生更稳定可控的协同演化动态。",
    "url": "https://huggingface.co/papers/2512.02472",
    "arxiv_url": "https://arxiv.org/abs/2512.02472"
  },
  {
    "title": "SimScale: Learning to Drive via Real-World Simulation at Scale",
    "summary": "Achieving fully autonomous driving systems requires learning rational decisions in a wide span of scenarios, including safety-critical and out-of-distribution ones. However, such cases are underrepresented in real-world corpus collected by human experts. To complement for the lack of data diversity, we introduce a novel and scalable simulation framework capable of synthesizing massive unseen states upon existing driving logs. Our pipeline utilizes advanced neural rendering with a reactive environment to generate high-fidelity multi-view observations controlled by the perturbed ego trajectory. Furthermore, we develop a pseudo-expert trajectory generation mechanism for these newly simulated states to provide action supervision. Upon the synthesized data, we find that a simple co-training strategy on both real-world and simulated samples can lead to significant improvements in both robustness and generalization for various planning methods on challenging real-world benchmarks, up to +6.8 EPDMS on navhard and +2.9 on navtest. More importantly, such policy improvement scales smoothly by increasing simulation data only, even without extra real-world data streaming in. We further reveal several crucial findings of such a sim-real learning system, which we term SimScale, including the design of pseudo-experts and the scaling properties for different policy architectures. Our simulation data and code would be released.",
    "translation": "标题：SimScale：基于大规模真实世界模拟的驾驶学习\n\n摘要：实现完全自动驾驶系统需要在广泛场景中学习理性决策，包括安全关键场景和分布外场景。然而，人类专家收集的真实世界数据集中此类案例代表性不足。为弥补数据多样性的缺失，我们提出一种新颖且可扩展的模拟框架，能够在现有驾驶日志基础上合成海量未见状态。该框架采用先进神经渲染技术与反应式环境，通过扰动自车轨迹生成高保真多视角观测数据。此外，我们为这些新模拟状态开发了伪专家轨迹生成机制以提供动作监督。基于合成数据的研究发现，对真实样本与模拟样本采用简单的协同训练策略，能够显著提升多种规划方法在挑战性真实世界基准测试中的鲁棒性与泛化能力——在navhard基准上最高提升+6.8 EPDMS，在navtest基准上提升+2.9。更重要的是，仅通过增加模拟数据（无需额外真实世界数据流），策略改进效果即可实现平滑扩展。我们进一步揭示了此类模拟-真实学习系统（命名为SimScale）的若干关键发现，包括伪专家设计原则及不同策略架构的扩展特性。本研究的模拟数据与代码将予以公开。",
    "url": "https://huggingface.co/papers/2511.23369",
    "arxiv_url": "https://arxiv.org/abs/2511.23369"
  },
  {
    "title": "InnoGym: Benchmarking the Innovation Potential of AI Agents",
    "summary": "LLMs and Agents have achieved impressive progress in code generation, mathematical reasoning, and scientific discovery. However, existing benchmarks primarily measure correctness, overlooking the diversity of methods behind solutions. True innovation depends not only on producing correct answers but also on the originality of the approach. We present InnoGym, the first benchmark and framework designed to systematically evaluate the innovation potential of AI agents. InnoGym introduces two complementary metrics: performance gain, which measures improvement over the best-known solutions, and novelty, which captures methodological differences from prior approaches. The benchmark includes 18 carefully curated tasks from real-world engineering and scientific domains, each standardized through resource filtering, evaluator validation, and solution collection. In addition, we provide iGym, a unified execution environment for reproducible and long-horizon evaluations. Extensive experiments show that while some agents produce novel approaches, their lack of robustness limits performance gains. These results highlight a key gap between creativity and effectiveness, underscoring the need for benchmarks that evaluate both.",
    "translation": "标题：InnoGym：评估AI智能体创新潜力的基准测试\n\n摘要：大语言模型与智能体在代码生成、数学推理和科学发现等领域已取得显著进展。然而，现有基准测试主要关注解决方案的正确性，忽视了方法背后的多样性。真正的创新不仅取决于答案的正确性，更取决于方法的原创性。本文提出InnoGym——首个系统评估AI智能体创新潜力的基准测试框架。InnoGym引入两项互补指标：性能增益（衡量对已知最优方案的改进程度）和新颖性（捕捉与既有方法的方法论差异）。该基准包含18项从真实工程与科学领域精心筛选的任务，每项任务均通过资源过滤、评估验证和方案收集实现标准化。此外，我们提供iGym统一执行环境，支持可复现的长周期评估。大量实验表明，虽然部分智能体能产生新颖方法，但其鲁棒性不足限制了性能提升。这些结果揭示了创造力与有效性之间的关键差距，凸显了需要同时评估两者的基准测试体系。",
    "url": "https://huggingface.co/papers/2512.01822",
    "arxiv_url": "https://arxiv.org/abs/2512.01822"
  },
  {
    "title": "ViSAudio: End-to-End Video-Driven Binaural Spatial Audio Generation",
    "summary": "Despite progress in video-to-audio generation, the field focuses predominantly on mono output, lacking spatial immersion. Existing binaural approaches remain constrained by a two-stage pipeline that first generates mono audio and then performs spatialization, often resulting in error accumulation and spatio-temporal inconsistencies. To address this limitation, we introduce the task of end-to-end binaural spatial audio generation directly from silent video. To support this task, we present the BiAudio dataset, comprising approximately 97K video-binaural audio pairs spanning diverse real-world scenes and camera rotation trajectories, constructed through a semi-automated pipeline. Furthermore, we propose ViSAudio, an end-to-end framework that employs conditional flow matching with a dual-branch audio generation architecture, where two dedicated branches model the audio latent flows. Integrated with a conditional spacetime module, it balances consistency between channels while preserving distinctive spatial characteristics, ensuring precise spatio-temporal alignment between audio and the input video. Comprehensive experiments demonstrate that ViSAudio outperforms existing state-of-the-art methods across both objective metrics and subjective evaluations, generating high-quality binaural audio with spatial immersion that adapts effectively to viewpoint changes, sound-source motion, and diverse acoustic environments. Project website: https://kszpxxzmc.github.io/ViSAudio-project.",
    "translation": "标题：ViSAudio：端到端视频驱动的双耳空间音频生成\n\n摘要：尽管视频到音频生成领域已取得进展，但现有研究主要集中于单声道输出，缺乏空间沉浸感。现有的双耳音频生成方法仍受限于两阶段流程，即首先生成单声道音频再进行空间化处理，这往往导致误差累积与时空不一致问题。为突破这一局限，本文提出从无声视频直接端到端生成双耳空间音频的任务。为支持该任务，我们构建了BiAudio数据集，该数据集通过半自动化流程整合了约9.7万个视频-双耳音频对，涵盖多样化的真实场景与摄像机旋转轨迹。进一步，我们提出ViSAudio端到端框架，该框架采用条件流匹配与双分支音频生成架构，通过两个独立分支对音频隐空间流进行建模。结合条件时空模块，该框架在保持声道间一致性的同时保留了独特的空间特征，确保了音频与输入视频间精确的时空对齐。综合实验表明，ViSAudio在客观指标与主观评估上均优于现有先进方法，能够生成具有空间沉浸感的高质量双耳音频，并能有效适应视角变化、声源运动及多样声学环境。项目网站：https://kszpxxzmc.github.io/ViSAudio-project。",
    "url": "https://huggingface.co/papers/2512.03036",
    "arxiv_url": "https://arxiv.org/abs/2512.03036"
  },
  {
    "title": "Glance: Accelerating Diffusion Models with 1 Sample",
    "summary": "Diffusion models have achieved remarkable success in image generation, yet their deployment remains constrained by the heavy computational cost and the need for numerous inference steps. Previous efforts on fewer-step distillation attempt to skip redundant steps by training compact student models, yet they often suffer from heavy retraining costs and degraded generalization. In this work, we take a different perspective: we accelerate smartly, not evenly, applying smaller speedups to early semantic stages and larger ones to later redundant phases. We instantiate this phase-aware strategy with two experts that specialize in slow and fast denoising phases. Surprisingly, instead of investing massive effort in retraining student models, we find that simply equipping the base model with lightweight LoRA adapters achieves both efficient acceleration and strong generalization. We refer to these two adapters as Slow-LoRA and Fast-LoRA. Through extensive experiments, our method achieves up to 5 acceleration over the base model while maintaining comparable visual quality across diverse benchmarks. Remarkably, the LoRA experts are trained with only 1 samples on a single V100 within one hour, yet the resulting models generalize strongly on unseen prompts.",
    "translation": "标题：Glance：基于单样本的扩散模型加速方法\n\n摘要：扩散模型在图像生成领域取得了显著成功，但其应用仍受限于高昂的计算成本和大量的推理步骤需求。先前关于少步蒸馏的研究试图通过训练紧凑的学生模型来跳过冗余步骤，但这些方法往往面临繁重的重新训练成本与泛化性能下降的问题。本研究提出一种新视角：采用智能而非均匀的加速策略，对早期语义阶段施加较小的加速，而对后期冗余阶段实施较大的加速。我们通过两个分别专注于慢速去噪阶段和快速去噪阶段的专家模型来实现这种阶段感知策略。令人惊讶的是，与投入大量资源重新训练学生模型不同，我们发现仅需为基础模型配备轻量级LoRA适配器即可同时实现高效加速与强大泛化能力。我们将这两种适配器分别命名为慢速LoRA与快速LoRA。大量实验表明，该方法在保持多种基准测试中视觉质量相当的前提下，实现了相较于基础模型最高达5倍的加速效果。值得注意的是，LoRA专家模型仅需在单张V100显卡上使用1个样本训练一小时，所得模型在未见过的提示词上仍展现出强大的泛化能力。",
    "url": "https://huggingface.co/papers/2512.02899",
    "arxiv_url": "https://arxiv.org/abs/2512.02899"
  },
  {
    "title": "WorldMM: Dynamic Multimodal Memory Agent for Long Video Reasoning",
    "summary": "Recent advances in video large language models have demonstrated strong capabilities in understanding short clips. However, scaling them to hours- or days-long videos remains highly challenging due to limited context capacity and the loss of critical visual details during abstraction. Existing memory-augmented methods mitigate this by leveraging textual summaries of video segments, yet they heavily rely on text and fail to utilize visual evidence when reasoning over complex scenes. Moreover, retrieving from fixed temporal scales further limits their flexibility in capturing events that span variable durations. To address this, we introduce WorldMM, a novel multimodal memory agent that constructs and retrieves from multiple complementary memories, encompassing both textual and visual representations. WorldMM comprises three types of memory: episodic memory indexes factual events across multiple temporal scales, semantic memory continuously updates high-level conceptual knowledge, and visual memory preserves detailed information about scenes. During inference, an adaptive retrieval agent iteratively selects the most relevant memory source and leverages multiple temporal granularities based on the query, continuing until it determines that sufficient information has been gathered. WorldMM significantly outperforms existing baselines across five long video question-answering benchmarks, achieving an average 8.4% performance gain over previous state-of-the-art methods, showing its effectiveness on long video reasoning.",
    "translation": "标题：WorldMM：面向长视频推理的动态多模态记忆智能体\n\n摘要：视频大语言模型的最新进展在理解短视频片段方面展现出强大能力。然而，由于上下文容量有限以及在抽象过程中关键视觉细节的丢失，将其扩展至数小时甚至数天时长的视频仍极具挑战性。现有的记忆增强方法通过利用视频片段的文本摘要来缓解这一问题，但这些方法严重依赖文本，在复杂场景推理时未能有效利用视觉证据。此外，基于固定时间尺度的检索方式进一步限制了其捕捉不同持续时间事件的灵活性。为此，我们提出WorldMM，一种新颖的多模态记忆智能体，它构建并检索多种互补记忆，涵盖文本与视觉表征。WorldMM包含三类记忆：情景记忆通过多时间尺度索引事实事件，语义记忆持续更新高层概念知识，视觉记忆则保留场景的细节信息。在推理过程中，自适应检索智能体根据查询内容迭代选择最相关的记忆源，并利用多时间粒度进行分析，直至确定已收集足够信息。在五个长视频问答基准测试中，WorldMM显著优于现有基线方法，相较先前最优方法平均性能提升8.4%，充分证明了其在长视频推理任务上的有效性。",
    "url": "https://huggingface.co/papers/2512.02425",
    "arxiv_url": "https://arxiv.org/abs/2512.02425"
  },
  {
    "title": "Deep Research: A Systematic Survey",
    "summary": "Large language models (LLMs) have rapidly evolved from text generators into powerful problem solvers. Yet, many open tasks demand critical thinking, multi-source, and verifiable outputs, which are beyond single-shot prompting or standard retrieval-augmented generation. Recently, numerous studies have explored Deep Research (DR), which aims to combine the reasoning capabilities of LLMs with external tools, such as search engines, thereby empowering LLMs to act as research agents capable of completing complex, open-ended tasks. This survey presents a comprehensive and systematic overview of deep research systems, including a clear roadmap, foundational components, practical implementation techniques, important challenges, and future directions. Specifically, our main contributions are as follows: (i) we formalize a three-stage roadmap and distinguish deep research from related paradigms; (ii) we introduce four key components: query planning, information acquisition, memory management, and answer generation, each paired with fine-grained sub-taxonomies; (iii) we summarize optimization techniques, including prompting, supervised fine-tuning, and agentic reinforcement learning; and (iv) we consolidate evaluation criteria and open challenges, aiming to guide and facilitate future development. As the field of deep research continues to evolve rapidly, we are committed to continuously updating this survey to reflect the latest progress in this area.",
    "translation": "标题：深度研究：一项系统性综述\n\n摘要：大语言模型已从文本生成器迅速发展为强大的问题解决工具。然而，许多开放性任务需要批判性思维、多源信息与可验证的输出，这已超出单次提示或标准检索增强生成的能力范围。近期，大量研究开始探索深度研究，其目标是将大语言模型的推理能力与搜索引擎等外部工具相结合，从而使大语言模型能够作为研究智能体完成复杂、开放式的任务。本综述对深度研究系统进行了全面而系统的梳理，包括清晰的路线图、基础组件、实践实现技术、重要挑战与未来方向。具体而言，我们的主要贡献如下：（一）提出了三阶段路线图的形式化框架，并明确了深度研究与相关范式的区别；（二）介绍了四个关键组件：查询规划、信息获取、记忆管理与答案生成，每个组件均配有细粒度的子分类体系；（三）总结了包括提示工程、监督微调与智能体强化学习在内的优化技术；（四）整合了评估标准与开放挑战，旨在引导和促进该领域的未来发展。随着深度研究领域的持续快速演进，我们将持续更新本综述以反映该领域的最新进展。",
    "url": "https://huggingface.co/papers/2512.02038",
    "arxiv_url": "https://arxiv.org/abs/2512.02038"
  },
  {
    "title": "PixelDiT: Pixel Diffusion Transformers for Image Generation",
    "summary": "Latent-space modeling has been the standard for Diffusion Transformers (DiTs). However, it relies on a two-stage pipeline where the pretrained autoencoder introduces lossy reconstruction, leading to error accumulation while hindering joint optimization. To address these issues, we propose PixelDiT, a single-stage, end-to-end model that eliminates the need for the autoencoder and learns the diffusion process directly in the pixel space. PixelDiT adopts a fully transformer-based architecture shaped by a dual-level design: a patch-level DiT that captures global semantics and a pixel-level DiT that refines texture details, enabling efficient training of a pixel-space diffusion model while preserving fine details. Our analysis reveals that effective pixel-level token modeling is essential to the success of pixel diffusion. PixelDiT achieves 1.61 FID on ImageNet 256x256, surpassing existing pixel generative models by a large margin. We further extend PixelDiT to text-to-image generation and pretrain it at the 1024x1024 resolution in pixel space. It achieves 0.74 on GenEval and 83.5 on DPG-bench, approaching the best latent diffusion models.",
    "translation": "标题：PixelDiT：用于图像生成的像素扩散Transformer\n\n摘要：潜空间建模一直是扩散Transformer（DiTs）的标准范式。然而，该方法依赖于两阶段流程，其中预训练的自编码器会引入有损重构，导致误差累积并阻碍联合优化。为解决这些问题，我们提出PixelDiT——一种单阶段、端到端的模型，无需依赖自编码器，直接在像素空间中学习扩散过程。PixelDiT采用全Transformer架构，其设计包含双重层级：捕捉全局语义的块级DiT与细化纹理细节的像素级DiT，在保留精细细节的同时实现了像素空间扩散模型的高效训练。我们的分析表明，有效的像素级令牌建模是像素扩散成功的关键。PixelDiT在ImageNet 256×256分辨率上取得了1.61的FID分数，显著超越了现有像素生成模型。我们进一步将PixelDiT扩展至文本到图像生成任务，并在像素空间中以1024×1024分辨率进行预训练。该模型在GenEval评测中达到0.74分，在DPG-bench中取得83.5分，性能已接近最佳潜扩散模型。",
    "url": "https://huggingface.co/papers/2511.20645",
    "arxiv_url": "https://arxiv.org/abs/2511.20645"
  },
  {
    "title": "Mixture of Horizons in Action Chunking",
    "summary": "Vision-language-action (VLA) models have shown remarkable capabilities in robotic manipulation, but their performance is sensitive to the action chunk length used during training, termed horizon. Our empirical study reveals an inherent trade-off: longer horizons provide stronger global foresight but degrade fine-grained accuracy, while shorter ones sharpen local control yet struggle on long-term tasks, implying fixed choice of single horizons being suboptimal. To mitigate the trade-off, we propose a mixture of horizons (MoH) strategy. MoH rearranges the action chunk into several segments with different horizons, processes them in parallel with a shared action transformer, and fuses outputs with a light linear gate. It has three appealing benefits. 1) MoH exploits long-term foresight and short-term precision jointly within a single model, improving both performance and generalizability to complex tasks. 2) MoH is plug-and-play for full-attention action modules with minimal training or inference overhead. 3) MoH enables dynamic inference with adaptive horizons, which selects stable actions through cross-horizon consensus, achieving 2.5times higher throughput than baselines while preserving superior performance. Extensive experiments over flow-based policies π_0, π_{0.5}, and one-step regression policy π_{reg} demonstrate that MoH yields consistent and significant gains on both simulations and real-world tasks. Notably, under mixed-task setting, π_{0.5} with MoH reaches a new state-of-the-art with 99% average success rate on LIBERO after only 30k training iterations. Project page: https://github.com/Timsty1/MixtureOfHorizons",
    "translation": "标题：动作分块中的混合视野策略\n\n摘要：视觉-语言-动作模型在机器人操作任务中展现出卓越能力，但其性能对训练时采用的动作块长度（称为视野）十分敏感。实证研究表明其中存在固有权衡：较长视野能提供更强的全局预见性，但会降低细粒度动作精度；较短视野虽能提升局部控制精度，却难以应对长期任务，这意味着固定单一视野的选择具有次优性。为缓解这一矛盾，本文提出混合视野策略。该策略将动作块重组为具有不同视野的多个片段，通过共享动作变换器进行并行处理，并利用轻量级线性门融合输出。该方法具有三大优势：1）在单一模型内协同利用长期预见性与短期精确性，提升复杂任务中的性能与泛化能力；2）可作为即插即用模块适配全注意力动作架构，仅引入极少的训练与推理开销；3）支持自适应视野的动态推理机制，通过跨视野一致性筛选稳定动作，在保持优异性能的同时实现比基线方法高2.5倍的吞吐量。基于流式策略π_0、π_{0.5}及单步回归策略π_{reg}的广泛实验表明，混合视野策略在仿真与真实任务中均能带来持续显著的性能提升。值得注意的是，在混合任务场景下，搭载混合视野的π_{0.5}策略仅经过3万次训练迭代即在LIBERO基准上达到99%的平均成功率，创造了新的性能纪录。项目页面：https://github.com/Timsty1/MixtureOfHorizons",
    "url": "https://huggingface.co/papers/2511.19433",
    "arxiv_url": "https://arxiv.org/abs/2511.19433"
  },
  {
    "title": "WUSH: Near-Optimal Adaptive Transforms for LLM Quantization",
    "summary": "Quantization to low bitwidth is a standard approach for deploying large language models, however, a few extreme weights and activations stretch the dynamic range and reduce the effective resolution of the quantizer. A common mitigation approach is to apply some fixed orthogonal transforms, such as Hadamard matrices, before quantization, which typically reduces the dynamic range. Yet, these transforms ignore the statistics of the data, and their optimality is currently not understood. In this work, we derive, for the first time, closed-form optimal linear blockwise transforms for joint weight-activation quantization using standard data-free quantizers for common numerical formats. Specifically, we provide derivations of the optimal adaptive (data-aware) transforms for round-to-nearest (RTN), AbsMax-scaled block quantizers for both integer and floating-point formats. The resulting construction, which we call WUSH, combines a Hadamard backbone with a data-dependent component based on second-order moments, yielding a non-orthogonal transform that is provably optimal under mild assumptions and remains structured for efficient implementation. Preliminary experimental results show that our approach consistently improves upon the Hadamard transform for common formats.",
    "translation": "标题：WUSH：面向大语言模型量化的近最优自适应变换\n\n摘要：低比特量化是部署大语言模型的常用方法，但少数极端权重和激活值会扩大动态范围，降低量化器的有效分辨率。常见的缓解方法是在量化前应用固定的正交变换（如哈达玛矩阵），这通常能压缩动态范围。然而，这类变换忽略了数据统计特性，其最优性尚未得到充分论证。本研究首次针对常见数值格式，推导出基于标准无数据量化器的联合权重-激活值最优线性分块变换闭式解。具体而言，我们推导了适用于整数与浮点格式的最近舍入（RTN）和绝对值最大分块缩放量化器的最优自适应（数据感知）变换。所得构建方法命名为WUSH，其将哈达玛矩阵主干与基于二阶矩的数据依赖组件相结合，形成一种在温和假设下可证明最优的非正交变换，同时保持结构化特性以实现高效计算。初步实验结果表明，该方法在常见数值格式上均能稳定超越哈达玛变换的效果。",
    "url": "https://huggingface.co/papers/2512.00956",
    "arxiv_url": "https://arxiv.org/abs/2512.00956"
  },
  {
    "title": "GoRL: An Algorithm-Agnostic Framework for Online Reinforcement Learning with Generative Policies",
    "summary": "Reinforcement learning (RL) faces a persistent tension: policies that are stable to optimize are often too simple to represent the multimodal action distributions needed for complex control. Gaussian policies provide tractable likelihoods and smooth gradients, but their unimodal form limits expressiveness. Conversely, generative policies based on diffusion or flow matching can model rich multimodal behaviors; however, in online RL, they are frequently unstable due to intractable likelihoods and noisy gradients propagating through deep sampling chains. We address this tension with a key structural principle: decoupling optimization from generation. Building on this insight, we introduce GoRL (Generative Online Reinforcement Learning), a framework that optimizes a tractable latent policy while utilizing a conditional generative decoder to synthesize actions. A two-timescale update schedule enables the latent policy to learn stably while the decoder steadily increases expressiveness, without requiring tractable action likelihoods. Across a range of continuous-control tasks, GoRL consistently outperforms both Gaussian policies and recent generative-policy baselines. Notably, on the HopperStand task, it reaches a normalized return above 870, more than 3 times that of the strongest baseline. These results demonstrate that separating optimization from generation provides a practical path to policies that are both stable and highly expressive.",
    "translation": "标题：GoRL：一种与算法无关的生成策略在线强化学习框架\n\n摘要：强化学习（RL）面临一个长期存在的矛盾：易于优化的策略往往过于简单，难以表达复杂控制所需的多模态动作分布。高斯策略提供了易于处理的似然函数和平滑梯度，但其单模态形式限制了表达能力。相反，基于扩散或流匹配的生成策略能够建模丰富的多模态行为；然而，在在线强化学习中，由于似然函数难以处理以及梯度在深度采样链中传播时的噪声问题，这些策略常常不稳定。我们通过一个关键的结构性原则来解决这一矛盾：将优化过程与生成过程解耦。基于这一思路，我们提出了GoRL（生成式在线强化学习），该框架通过优化一个易于处理的潜在策略，并利用条件生成解码器来合成动作。采用双时间尺度更新机制，使得潜在策略能够稳定学习，同时解码器逐步增强表达能力，且无需计算可处理的动作似然函数。在一系列连续控制任务中，GoRL的表现始终优于高斯策略和近期提出的生成策略基线方法。值得注意的是，在HopperStand任务中，其归一化回报达到870以上，超过最强基线方法的三倍。这些结果表明，将优化与生成分离为实现既稳定又具有高度表达能力的策略提供了一条实用路径。",
    "url": "https://huggingface.co/papers/2512.02581",
    "arxiv_url": "https://arxiv.org/abs/2512.02581"
  },
  {
    "title": "CUDA-L2: Surpassing cuBLAS Performance for Matrix Multiplication through Reinforcement Learning",
    "summary": "In this paper, we propose CUDA-L2, a system that combines large language models (LLMs) and reinforcement learning (RL) to automatically optimize Half-precision General Matrix Multiply (HGEMM) CUDA kernels. Using CUDA execution speed as the RL reward, CUDA-L2 automatically optimizes HGEMM kernels across 1,000 configurations. CUDA-L2 systematically outperforms major matmul baselines to date, from the widely-used {\\it torch.matmul} to state-of-the-art Nvidia's closed-source libraries, i.e., {\\it cuBLAS}, {\\it cuBLASLt}. In offline mode, where kernels are executed consecutively without time intervals, CUDA-L2 yields +22.0\\% over {\\it torch.matmul} on average; +19.2\\% over {\\it cuBLAS} using the optimal layout configuration (normal-normal NN and transposed-normal TN); +16.8\\% over {\\it cuBLASLt-heuristic}, which queries {\\it cuBLASLt} library and selects the algorithm based on the heuristic's suggestion; and +11.4\\% over the most competitive {\\it cuBLASLt-AutoTuning} model, which selects the fastest algorithm from up to 100 candidates from {\\it cuBLASLt}'s suggestions. In server mode, where kernels are executed at random intervals simulating real-time inference, the speedups further increase to +28.7\\%, +26.0\\%, +22.4\\%, and +15.9\\% for {\\it torch.matmul}, {\\it cuBLAS}, {\\it cuBLASLt-heuristic}, and {\\it cuBLASLt-AutoTuning} respectively. CUDA-L2 shows that even the most performance-critical, heavily-optimized kernels like HGEMM can be improved through LLM-guided RL automation by systematically exploring configuration spaces at scales impractical for humans. Project and code can be found at github.com/deepreinforce-ai/CUDA-L2",
    "translation": "标题：CUDA-L2：通过强化学习超越cuBLAS的矩阵乘法性能\n\n摘要：本文提出CUDA-L2系统，该系统结合大语言模型（LLMs）与强化学习（RL），以自动化方式优化半精度通用矩阵乘法（HGEMM）的CUDA内核。以CUDA执行速度作为强化学习奖励，CUDA-L2在1,000种配置上自动优化HGEMM内核。实验表明，CUDA-L2系统性地超越了当前主要的矩阵乘法基准方法，从广泛使用的{\\it torch.matmul}到最先进的英伟达闭源库（即{\\it cuBLAS}和{\\it cuBLASLt}）。在离线模式下（内核连续执行无时间间隔），CUDA-L2相比{\\it torch.matmul}平均提升22.0%；相比采用最优布局配置（normal-normal NN与transposed-normal TN）的{\\it cuBLAS}提升19.2%；相比基于启发式建议从{\\it cuBLASLt}库选择算法的{\\it cuBLASLt-heuristic}提升16.8%；相比最具竞争力的{\\it cuBLASLt-AutoTuning}模型（从{\\it cuBLASLt}提供的多达100个候选算法中选择最优方案）提升11.4%。在模拟实时推理的随机间隔执行内核的服务器模式下，加速效果进一步提升：相比{\\it torch.matmul}、{\\it cuBLAS}、{\\it cuBLASLt-heuristic}和{\\it cuBLASLt-AutoTuning}分别达到+28.7%、+26.0%、+22.4%和+15.9%。CUDA-L2证明，即使是如HGEMM这类对性能要求极高且经过深度优化的内核，仍可通过大语言模型引导的强化学习自动化实现性能突破——该系统以人类难以实现的规模系统探索配置空间。项目与代码详见github.com/deepreinforce-ai/CUDA-L2。",
    "url": "https://huggingface.co/papers/2512.02551",
    "arxiv_url": "https://arxiv.org/abs/2512.02551"
  },
  {
    "title": "Does Hearing Help Seeing? Investigating Audio-Video Joint Denoising for Video Generation",
    "summary": "Recent audio-video generative systems suggest that coupling modalities benefits not only audio-video synchrony but also the video modality itself. We pose a fundamental question: Does audio-video joint denoising training improve video generation, even when we only care about video quality? To study this, we introduce a parameter-efficient Audio-Video Full DiT (AVFullDiT) architecture that leverages pre-trained text-to-video (T2V) and text-to-audio (T2A) modules for joint denoising. We train (i) a T2AV model with AVFullDiT and (ii) a T2V-only counterpart under identical settings. Our results provide the first systematic evidence that audio-video joint denoising can deliver more than synchrony. We observe consistent improvements on challenging subsets featuring large and object contact motions. We hypothesize that predicting audio acts as a privileged signal, encouraging the model to internalize causal relationships between visual events and their acoustic consequences (e.g., collision times impact sound), which in turn regularizes video dynamics. Our findings suggest that cross-modal co-training is a promising approach to developing stronger, more physically grounded world models. Code and dataset will be made publicly available.",
    "translation": "标题：听觉能否辅助视觉？视频生成中音视频联合去噪机制研究\n\n摘要：近期音视频生成系统研究表明，多模态耦合不仅有助于提升音视频同步性，更能优化视频模态本身的生成质量。本文提出一个基础性问题：即使仅关注视频质量，音视频联合去噪训练是否仍能提升视频生成效果？为探究此问题，我们设计了一种参数高效的音视频全扩散变换器（AVFullDiT）架构，该架构利用预训练的文本到视频（T2V）与文本到音频（T2A）模块进行联合去噪训练。我们在相同实验设置下分别训练了：（1）基于AVFullDiT的T2AV多模态模型；（2）仅使用视频模态的T2V对照模型。实验结果首次系统性地证明，音视频联合去噪训练带来的效益超越同步性提升。在包含大幅物体运动及物体接触动作的挑战性数据子集上，我们观察到模型性能的持续改善。我们推测，音频预测作为一种特权信号，能够促使模型内化视觉事件与其声学后果之间的因果关系（例如碰撞时机对声音的影响），从而实现对视频动态特性的正则化约束。本研究结果表明，跨模态协同训练是构建更强大、更符合物理规律的世界模型的有效路径。代码与数据集将公开发布。",
    "url": "https://huggingface.co/papers/2512.02457",
    "arxiv_url": "https://arxiv.org/abs/2512.02457"
  },
  {
    "title": "The Curious Case of Analogies: Investigating Analogical Reasoning in Large Language Models",
    "summary": "Analogical reasoning is at the core of human cognition, serving as an important foundation for a variety of intellectual activities. While prior work has shown that LLMs can represent task patterns and surface-level concepts, it remains unclear whether these models can encode high-level relational concepts and apply them to novel situations through structured comparisons. In this work, we explore this fundamental aspect using proportional and story analogies, and identify three key findings. First, LLMs effectively encode the underlying relationships between analogous entities; both attributive and relational information propagate through mid-upper layers in correct cases, whereas reasoning failures reflect missing relational information within these layers. Second, unlike humans, LLMs often struggle not only when relational information is missing, but also when attempting to apply it to new entities. In such cases, strategically patching hidden representations at critical token positions can facilitate information transfer to a certain extent. Lastly, successful analogical reasoning in LLMs is marked by strong structural alignment between analogous situations, whereas failures often reflect degraded or misplaced alignment. Overall, our findings reveal that LLMs exhibit emerging but limited capabilities in encoding and applying high-level relational concepts, highlighting both parallels and gaps with human cognition.",
    "translation": "标题：类比推理的独特案例：探究大语言模型中的类比推理机制\n\n摘要：类比推理是人类认知的核心，为多种智力活动提供了重要基础。尽管已有研究表明大语言模型能够表征任务模式与表层概念，但这些模型能否编码高层次关系概念并通过结构化比较将其应用于新情境仍不明确。本研究通过比例类比与故事类比探究这一基础问题，并得出三个关键发现：首先，大语言模型能有效编码类比实体间的深层关系——在正确推理案例中，属性信息与关系信息均通过中高层网络传播；而推理失败案例则反映出这些层级中关系信息的缺失。其次，与人类不同，大语言模型不仅在关系信息缺失时表现困难，在尝试将关系信息迁移至新实体时也常遇阻碍。在此类情境中，对关键标记位置的隐层表征进行策略性修补可在一定程度上促进信息传递。最后，成功的类比推理以类比情境间强烈的结构对齐为特征，而推理失败往往表现为结构对齐的弱化或错位。总体而言，本研究表明大语言模型在编码与应用高层次关系概念方面呈现出初现但有限的能力，既揭示了其与人类认知的相通之处，也凸显了二者间的显著差距。",
    "url": "https://huggingface.co/papers/2511.20344",
    "arxiv_url": "https://arxiv.org/abs/2511.20344"
  },
  {
    "title": "DiG-Flow: Discrepancy-Guided Flow Matching for Robust VLA Models",
    "summary": "Vision-Language-Action (VLA) models trained with flow matching have demonstrated impressive capabilities on robotic manipulation tasks. However, their performance often degrades under distribution shift and on complex multi-step tasks, suggesting that the learned representations may not robustly capture task-relevant semantics. We introduce DiG-Flow, a principled framework that enhances VLA robustness through geometric regularization. Our key insight is that the distributional discrepancy between observation and action embeddings provides a meaningful geometric signal: lower transport cost indicates compatible representations, while higher cost suggests potential misalignment. DiG-Flow computes a discrepancy measure between empirical distributions of observation and action embeddings, maps it to a modulation weight via a monotone function, and applies residual updates to the observation embeddings before flow matching. Crucially, this intervention operates at the representation level without modifying the flow matching path or target vector field. We provide theoretical guarantees showing that discrepancy-guided training provably decreases the training objective, and that guided inference refinement converges with contraction. Empirically, DiG-Flow integrates into existing VLA architectures with negligible overhead and consistently improves performance, with particularly pronounced gains on complex multi-step tasks and under limited training data.",
    "translation": "标题：DiG-Flow：基于差异引导的流匹配方法用于增强视觉-语言-动作模型的鲁棒性\n\n摘要：通过流匹配训练的视觉-语言-动作模型在机器人操作任务中展现出卓越的性能。然而，在分布偏移和复杂多步骤任务中，其性能常出现下降，这表明所学表征可能未能鲁棒地捕捉任务相关的语义信息。本文提出DiG-Flow，一个通过几何正则化增强VLA模型鲁棒性的理论框架。我们的核心见解是：观测嵌入与动作嵌入之间的分布差异能够提供有意义的几何信号——较低的传输成本表征兼容的表征，而较高的成本则暗示潜在的对齐偏差。DiG-Flow通过计算观测与动作嵌入经验分布间的差异度量，借助单调函数将其映射为调制权重，并在流匹配前对观测嵌入施加残差更新。关键的是，这种干预操作在表征层面进行，无需修改流匹配路径或目标向量场。我们提供了理论证明：差异引导的训练可严格降低训练目标函数，且引导的推断优化过程具有收敛性与收缩性。实验表明，DiG-Flow能以可忽略的开销集成到现有VLA架构中，持续提升模型性能，在复杂多步骤任务及有限训练数据场景下提升尤为显著。",
    "url": "https://huggingface.co/papers/2512.01715",
    "arxiv_url": "https://arxiv.org/abs/2512.01715"
  },
  {
    "title": "RULER-Bench: Probing Rule-based Reasoning Abilities of Next-level Video Generation Models for Vision Foundation Intelligence",
    "summary": "Recent advances in video generation have enabled the synthesis of videos with strong temporal consistency and impressive visual quality, marking a crucial step toward vision foundation models. To evaluate these video generation models, existing benchmarks primarily focus on factors related to visual perception and understanding, like visual aesthetics, instruction adherence, and temporal coherence. However, the rule-based reasoning capabilities of video generation models remain largely unexplored. Although recent studies have carried out preliminary explorations into whether video models can serve as zero-shot learners, they still lack a fine-grained decomposition of reasoning capabilities and a comprehensive evaluation protocol. To address this gap, we introduce RULER-Bench, a benchmark designed to evaluate the reasoning ability of video generation models from the perspective of cognitive rules. Built upon two fundamental paradigms: text-to-video and image-to-video, RULER-Bench covers 40 representative tasks spanning six rule categories with 622 high-quality annotated instances. For the evaluation of each generated video, we construct a checklist covering four metrics and leverage GPT-o3 to assign scores to each question, achieving 85% alignment with human judgements. Extensive experiments show that the state-of-the-art model achieves only 48.87% on the rule coherence metric, highlighting significant room for improvement in the reasoning capability of next-level video models. We expect that the insight obtained from RULER-Bench will facilitate further development of reasoning-aware video generation, advancing video generation models toward vision foundation intelligence.",
    "translation": "标题：RULER-Bench：面向视觉基础智能的新一代视频生成模型规则推理能力测评基准\n\n摘要：视频生成技术的最新进展使得合成视频在时间连贯性和视觉质量方面表现突出，这标志着向视觉基础模型迈出了关键一步。为评估这些视频生成模型，现有基准主要关注与视觉感知和理解相关的因素，如视觉美感、指令遵循度和时间一致性。然而，视频生成模型的规则推理能力在很大程度上仍未得到充分探索。尽管近期研究已对视频模型能否作为零样本学习者进行了初步探索，但仍缺乏对推理能力的细粒度分解和全面的评估方案。为填补这一空白，我们提出了RULER-Bench，这是一个旨在从认知规则角度评估视频生成模型推理能力的基准。基于文本到视频和图像到视频两种基本范式，RULER-Bench涵盖了六大规则类别下的40项代表性任务，包含622个高质量标注实例。针对每个生成视频的评估，我们构建了涵盖四项指标的检查表，并利用GPT-4o为每个问题分配分数，实现了与人工判断85%的一致性。大量实验表明，当前最先进的模型在规则连贯性指标上仅达到48.87%，凸显了新一代视频模型在推理能力方面仍有显著提升空间。我们期望通过RULER-Bench获得的见解将推动具有推理意识的视频生成的进一步发展，促进视频生成模型向视觉基础智能迈进。",
    "url": "https://huggingface.co/papers/2512.02622",
    "arxiv_url": "https://arxiv.org/abs/2512.02622"
  },
  {
    "title": "TRivia: Self-supervised Fine-tuning of Vision-Language Models for Table Recognition",
    "summary": "Table recognition (TR) aims to transform table images into semi-structured representations such as HTML or Markdown. As a core component of document parsing, TR has long relied on supervised learning, with recent efforts dominated by fine-tuning vision-language models (VLMs) using labeled data. While VLMs have brought TR to the next level, pushing performance further demands large-scale labeled data that is costly to obtain. Consequently, although proprietary models have continuously pushed the performance boundary, open-source models, often trained with limited resources and, in practice, the only viable option for many due to privacy regulations, still lag far behind. To bridge this gap, we introduce TRivia, a self-supervised fine-tuning method that enables pretrained VLMs to learn TR directly from unlabeled table images in the wild. Built upon Group Relative Policy Optimization, TRivia automatically identifies unlabeled samples that most effectively facilitate learning and eliminates the need for human annotations through a question-answering-based reward mechanism. An attention-guided module generates diverse questions for each table image, and the ability to interpret the recognition results and answer them correctly provides feedback to optimize the TR model. This closed-loop process allows the TR model to autonomously learn to recognize, structure, and reason over tables without labeled data. Leveraging this pipeline, we present TRivia-3B, an open-sourced, compact, and state-of-the-art TR model that surpasses existing systems (e.g., Gemini 2.5 Pro, MinerU2.5) on three popular benchmarks. Model and code are released at: https://github.com/opendatalab/TRivia",
    "translation": "标题：TRivia：面向表格识别的视觉-语言模型自监督微调方法\n\n摘要：表格识别（TR）旨在将表格图像转换为半结构化表示（如HTML或Markdown）。作为文档解析的核心组成部分，TR长期依赖监督学习，近期研究主要通过标注数据对视觉-语言模型（VLMs）进行微调。尽管VLMs已将TR性能提升至新高度，但进一步突破需要大规模标注数据，其获取成本高昂。因此，尽管专有模型持续突破性能边界，开源模型——常因资源受限而训练不足，且因隐私法规成为许多场景下的唯一可行选择——仍存在显著差距。为弥合这一鸿沟，我们提出TRivia：一种自监督微调方法，使预训练VLMs能够直接从无标注的真实场景表格图像中学习TR。该方法基于组相对策略优化构建，可自动识别最能促进学习的无标注样本，并通过基于问答的奖励机制消除对人工标注的依赖。注意力引导模块为每张表格图像生成多样化问题，而模型对识别结果的解析与正确回答能力将反馈至TR模型优化过程。这一闭环流程使TR模型能够无需标注数据即可自主学习表格的识别、结构化与推理。基于此框架，我们推出TRivia-3B——一个开源、紧凑且性能领先的TR模型，在三个主流基准测试中超越现有系统（如Gemini 2.5 Pro、MinerU2.5）。模型与代码已发布于：https://github.com/opendatalab/TRivia",
    "url": "https://huggingface.co/papers/2512.01248",
    "arxiv_url": "https://arxiv.org/abs/2512.01248"
  },
  {
    "title": "MagicQuillV2: Precise and Interactive Image Editing with Layered Visual Cues",
    "summary": "We propose MagicQuill V2, a novel system that introduces a layered composition paradigm to generative image editing, bridging the gap between the semantic power of diffusion models and the granular control of traditional graphics software. While diffusion transformers excel at holistic generation, their use of singular, monolithic prompts fails to disentangle distinct user intentions for content, position, and appearance. To overcome this, our method deconstructs creative intent into a stack of controllable visual cues: a content layer for what to create, a spatial layer for where to place it, a structural layer for how it is shaped, and a color layer for its palette. Our technical contributions include a specialized data generation pipeline for context-aware content integration, a unified control module to process all visual cues, and a fine-tuned spatial branch for precise local editing, including object removal. Extensive experiments validate that this layered approach effectively resolves the user intention gap, granting creators direct, intuitive control over the generative process.",
    "translation": "标题：MagicQuillV2：基于分层视觉线索的精确交互式图像编辑\n\n摘要：本文提出MagicQuill V2系统，该创新系统通过引入分层组合范式到生成式图像编辑领域，弥合了扩散模型的语义生成能力与传统图形软件精细化控制之间的鸿沟。尽管扩散变换器在整体图像生成方面表现出色，但其使用的单一聚合式提示词难以区分用户在内容、位置和外观等方面的不同创作意图。为解决此问题，我们的方法将创作意图解构为可控制视觉线索的层级结构：内容层定义生成对象，空间层确定对象位置，结构层控制形态轮廓，色彩层管理色调搭配。我们的技术贡献包括：用于上下文感知内容融合的专用数据生成流程、可统一处理所有视觉线索的控制模块，以及支持精确局部编辑（包括对象移除）的微调空间分支。大量实验验证表明，这种分层方法能有效解决用户意图传达的鸿沟，使创作者能够直接、直观地控制图像生成过程。",
    "url": "https://huggingface.co/papers/2512.03046",
    "arxiv_url": "https://arxiv.org/abs/2512.03046"
  },
  {
    "title": "Revisiting the Necessity of Lengthy Chain-of-Thought in Vision-centric Reasoning Generalization",
    "summary": "We study how different Chain-of-Thought (CoT) designs affect the acquisition of the generalizable visual reasoning ability in vision-language models (VLMs). While CoT data, especially long or visual CoT such as \"think with image\", has been widely used to supervise intermediate reasoning, it remains unclear why specific CoT designs help and which ones truly support generalizable reasoning. To systematically evaluate this, we focus on a controlled maze-solving benchmark where reasoning rules are fully visual, difficulty can be tuned by grid size, and all the intermediate steps can be automatically generated. Using Qwen2.5-VL-7B under a standard SFT-then-RL pipeline, we compare three representative CoT formats: Language CoT, Grounding CoT (with spatial coordinate trajectories), and Visual CoT (with image manipulations). Our experiments reveal that visual and longer CoT mainly accelerate convergence but do not lift the final performance ceiling; concise CoT containing only essential grounding steps outperforms longer traces; and, strikingly, CoT retaining only the minimal grounding results generalizes best across different maze sizes. We further validate these insights on other vision-centric tasks. These findings highlight a \"short is long\" effect and provide practical guidance for constructing more generalizable SFT datasets for visual reasoning.",
    "translation": "标题：重新审视视觉中心推理泛化中长链思维的必要性\n\n摘要：本研究探讨不同链式思维设计如何影响视觉语言模型泛化性视觉推理能力的习得。尽管链式思维数据（尤其是长链或视觉链式思维，如“基于图像的思考”）已被广泛用于监督中间推理过程，但特定链式思维设计为何有效、何种设计真正支持泛化性推理仍不明确。为系统评估该问题，我们采用可控的迷宫求解基准任务，其中推理规则完全基于视觉呈现，难度可通过网格尺寸调节，且所有中间步骤均可自动生成。在标准监督微调后强化学习的训练框架下，我们使用Qwen2.5-VL-7B模型比较了三种代表性链式思维格式：语言链式思维、具象化链式思维（含空间坐标轨迹）和视觉链式思维（含图像操作）。实验表明：视觉化与长链式思维主要加速收敛速度但未提升最终性能上限；仅包含必要具象化步骤的简洁链式思维优于长链轨迹；值得注意的是，仅保留最小必要具象化信息的链式思维在不同迷宫尺寸中展现出最佳泛化能力。我们进一步在其他视觉中心任务上验证了这些发现。这些结果揭示了“少即是多”效应，为构建更具泛化性的视觉推理监督微调数据集提供了实践指导。",
    "url": "https://huggingface.co/papers/2511.22586",
    "arxiv_url": "https://arxiv.org/abs/2511.22586"
  },
  {
    "title": "PAI-Bench: A Comprehensive Benchmark For Physical AI",
    "summary": "Physical AI aims to develop models that can perceive and predict real-world dynamics; yet, the extent to which current multi-modal large language models and video generative models support these abilities is insufficiently understood. We introduce Physical AI Bench (PAI-Bench), a unified and comprehensive benchmark that evaluates perception and prediction capabilities across video generation, conditional video generation, and video understanding, comprising 2,808 real-world cases with task-aligned metrics designed to capture physical plausibility and domain-specific reasoning. Our study provides a systematic assessment of recent models and shows that video generative models, despite strong visual fidelity, often struggle to maintain physically coherent dynamics, while multi-modal large language models exhibit limited performance in forecasting and causal interpretation. These observations suggest that current systems are still at an early stage in handling the perceptual and predictive demands of Physical AI. In summary, PAI-Bench establishes a realistic foundation for evaluating Physical AI and highlights key gaps that future systems must address.",
    "translation": "标题：PAI-Bench：面向物理人工智能的综合基准测试框架\n\n摘要：物理人工智能旨在开发能够感知并预测现实世界动态的模型；然而，当前多模态大语言模型与视频生成模型对这些能力的支持程度尚未得到充分理解。本研究提出物理人工智能基准测试框架（PAI-Bench），这是一个统一且全面的评估体系，涵盖视频生成、条件视频生成及视频理解三大任务的感知与预测能力评估。该基准包含2,808个真实场景案例，并采用任务对齐的度量标准，旨在捕捉物理合理性与领域特定推理能力。通过对近期模型的系统性评估，研究发现：视频生成模型虽具备较强的视觉保真度，但在保持物理动态连贯性方面仍存在明显不足；而多模态大语言模型在动态预测与因果解释任务中表现有限。这些发现表明，现有系统在应对物理人工智能的感知与预测需求方面仍处于早期发展阶段。总体而言，PAI-Bench为评估物理人工智能建立了现实基础，并揭示了未来系统必须解决的关键能力缺口。",
    "url": "https://huggingface.co/papers/2512.01989",
    "arxiv_url": "https://arxiv.org/abs/2512.01989"
  },
  {
    "title": "Video4Spatial: Towards Visuospatial Intelligence with Context-Guided Video Generation",
    "summary": "We investigate whether video generative models can exhibit visuospatial intelligence, a capability central to human cognition, using only visual data. To this end, we present Video4Spatial, a framework showing that video diffusion models conditioned solely on video-based scene context can perform complex spatial tasks. We validate on two tasks: scene navigation - following camera-pose instructions while remaining consistent with 3D geometry of the scene, and object grounding - which requires semantic localization, instruction following, and planning. Both tasks use video-only inputs, without auxiliary modalities such as depth or poses. With simple yet effective design choices in the framework and data curation, Video4Spatial demonstrates strong spatial understanding from video context: it plans navigation and grounds target objects end-to-end, follows camera-pose instructions while maintaining spatial consistency, and generalizes to long contexts and out-of-domain environments. Taken together, these results advance video generative models toward general visuospatial reasoning.",
    "translation": "标题：Video4Spatial：基于上下文引导视频生成的视觉空间智能探索\n\n摘要：本研究探讨视频生成模型能否仅凭视觉数据展现出人类认知核心能力之一的视觉空间智能。为此，我们提出Video4Spatial框架，证明仅以视频场景上下文为条件的视频扩散模型能够执行复杂空间任务。我们在两项任务上验证其有效性：场景导航——在遵循相机位姿指令的同时保持与场景三维几何结构的一致性；以及对象定位——该任务需要语义定位、指令跟随与路径规划能力。两项任务均仅使用视频输入，无需深度信息或位姿等辅助模态。通过框架设计与数据构建中简洁高效的技术方案，Video4Spatial展现出基于视频上下文的强大空间理解能力：能够端到端规划导航路径并定位目标对象，在遵循相机位姿指令的同时保持空间一致性，并能泛化至长序列上下文及域外环境。综合而言，这些成果推动视频生成模型向通用视觉空间推理迈出了重要一步。",
    "url": "https://huggingface.co/papers/2512.03040",
    "arxiv_url": "https://arxiv.org/abs/2512.03040"
  },
  {
    "title": "YingVideo-MV: Music-Driven Multi-Stage Video Generation",
    "summary": "While diffusion model for audio-driven avatar video generation have achieved notable process in synthesizing long sequences with natural audio-visual synchronization and identity consistency, the generation of music-performance videos with camera motions remains largely unexplored. We present YingVideo-MV, the first cascaded framework for music-driven long-video generation. Our approach integrates audio semantic analysis, an interpretable shot planning module (MV-Director), temporal-aware diffusion Transformer architectures, and long-sequence consistency modeling to enable automatic synthesis of high-quality music performance videos from audio signals. We construct a large-scale Music-in-the-Wild Dataset by collecting web data to support the achievement of diverse, high-quality results. Observing that existing long-video generation methods lack explicit camera motion control, we introduce a camera adapter module that embeds camera poses into latent noise. To enhance continulity between clips during long-sequence inference, we further propose a time-aware dynamic window range strategy that adaptively adjust denoising ranges based on audio embedding. Comprehensive benchmark tests demonstrate that YingVideo-MV achieves outstanding performance in generating coherent and expressive music videos, and enables precise music-motion-camera synchronization. More videos are available in our project page: https://giantailab.github.io/YingVideo-MV/ .",
    "translation": "标题：YingVideo-MV：音乐驱动的多阶段视频生成\n\n摘要：尽管基于扩散模型的音频驱动虚拟形象视频生成在合成具有自然音画同步与身份一致性的长序列方面已取得显著进展，但包含摄像机运动的音乐表演视频生成领域仍鲜有探索。本文提出YingVideo-MV，首个面向音乐驱动的长视频生成的级联框架。该方法整合了音频语义分析、可解释的镜头规划模块（MV-Director）、时序感知的扩散Transformer架构以及长序列一致性建模技术，实现了从音频信号自动合成高质量音乐表演视频。我们通过收集网络数据构建了大规模“野外音乐数据集”，以支持生成多样化、高质量的结果。针对现有长视频生成方法缺乏显式摄像机运动控制的问题，我们引入了摄像机适配器模块，将摄像机位姿嵌入潜在噪声空间。为提升长序列推理中片段间的连续性，进一步提出基于时序感知的动态窗口范围策略，可根据音频嵌入自适应调整去噪范围。综合基准测试表明，YingVideo-MV在生成连贯且富有表现力的音乐视频方面表现优异，并能实现精确的音乐-动作-摄像机同步。更多视频请访问项目页面：https://giantailab.github.io/YingVideo-MV/。",
    "url": "https://huggingface.co/papers/2512.02492",
    "arxiv_url": "https://arxiv.org/abs/2512.02492"
  },
  {
    "title": "SimWorld: An Open-ended Realistic Simulator for Autonomous Agents in Physical and Social Worlds",
    "summary": "While LLM/VLM-powered AI agents have advanced rapidly in math, coding, and computer use, their applications in complex physical and social environments remain challenging. Building agents that can survive and thrive in the real world (for example, by autonomously earning income or running a business) requires massive-scale interaction, reasoning, training, and evaluation across diverse embodied scenarios. However, existing world simulators for such development fall short: they often rely on limited hand-crafted environments, simulate simplified game-like physics and social rules, and lack native support for LLM/VLM agents. We introduce SimWorld, a new simulator built on Unreal Engine 5, designed for developing and evaluating LLM/VLM agents in rich, real-world-like settings. SimWorld offers three core capabilities: (1) realistic, open-ended world simulation, including accurate physical and social dynamics and language-driven procedural environment generation; (2) a rich interface for LLM/VLM agents, with multimodal world inputs and open-vocabulary actions at varying levels of abstraction; and (3) diverse and extensible physical and social reasoning scenarios that are easily customizable by users. We demonstrate SimWorld by deploying frontier LLM agents (e.g., GPT-4o, Gemini-2.5-Flash, Claude-3.5, and DeepSeek-Prover-V2) on long-horizon multi-agent delivery tasks involving strategic cooperation and competition. The results reveal distinct reasoning patterns and limitations across models. We open-source SimWorld and hope it becomes a foundational platform for advancing real-world agent intelligence across disciplines: https://simworld.org.",
    "translation": "标题：SimWorld：面向物理与社会世界中自主智能体的开放式真实模拟器\n\n摘要：尽管基于大语言模型/视觉语言模型的人工智能体在数学、编程和计算机应用领域发展迅速，但它们在复杂物理与社会环境中的应用仍面临挑战。构建能够在现实世界中生存与发展（例如通过自主创收或运营企业）的智能体，需要在多样化的具身场景中进行大规模交互、推理、训练与评估。然而，现有用于此类开发的世界模拟器存在明显不足：它们通常依赖有限的手工构建环境，模拟简化的游戏式物理与社会规则，且缺乏对大语言模型/视觉语言模型智能体的原生支持。本文提出SimWorld——一个基于虚幻引擎5构建的新型模拟器，旨在为开发与评估大语言模型/视觉语言模型智能体提供丰富、类真实世界的环境。SimWorld具备三项核心能力：（1）真实、开放的世界模拟，包括精确的物理与社会动态机制，以及基于语言的程序化环境生成；（2）面向大语言模型/视觉语言模型智能体的丰富交互接口，支持多模态世界输入和不同抽象层级的开放词汇动作；（3）多样化、可扩展的物理与社会推理场景，用户可便捷进行定制。我们通过在前沿大语言模型智能体（如GPT-4o、Gemini-2.5-Flash、Claude-3.5和DeepSeek-Prover-V2）上部署涉及战略合作与竞争的长期多智能体配送任务，展示了SimWorld的实用性。实验结果揭示了不同模型间显著的推理模式差异与局限性。我们已开源SimWorld平台，期待其成为推进跨学科现实世界智能体智能发展的基础平台：https://simworld.org。",
    "url": "https://huggingface.co/papers/2512.01078",
    "arxiv_url": "https://arxiv.org/abs/2512.01078"
  },
  {
    "title": "SwiftVLA: Unlocking Spatiotemporal Dynamics for Lightweight VLA Models at Minimal Overhead",
    "summary": "Vision-Language-Action (VLA) models built on pretrained Vision-Language Models (VLMs) show strong potential but are limited in practicality due to their large parameter counts. To mitigate this issue, using a lightweight VLM has been explored, but it compromises spatiotemporal reasoning. Although some methods suggest that incorporating additional 3D inputs can help, they usually rely on large VLMs to fuse 3D and 2D inputs and still lack temporal understanding. Therefore, we propose SwiftVLA, an architecture that enhances a compact model with 4D understanding while preserving design efficiency. Specifically, our approach features a pretrained 4D visual geometry transformer with a temporal cache that extracts 4D features from 2D images. Then, to enhance the VLM's ability to exploit both 2D images and 4D features, we introduce Fusion Tokens, a set of learnable tokens trained with a future prediction objective to generate unified representations for action generation. Finally, we introduce a mask-and-reconstruct strategy that masks 4D inputs to the VLM and trains the VLA to reconstruct them, enabling the VLM to learn effective 4D representations and allowing the 4D branch to be dropped at inference with minimal performance loss. Experiments in real and simulated environments show that SwiftVLA outperforms lightweight baselines and rivals VLAs up to 7 times larger, achieving comparable performance on edge devices while being 18 times faster and reducing memory footprint by 12 times.",
    "translation": "标题：SwiftVLA：以最小开销解锁轻量级视觉-语言-动作模型的时空动态理解\n\n摘要：基于预训练视觉-语言模型构建的视觉-语言-动作模型展现出强大潜力，但因其参数量庞大而实用性受限。为缓解此问题，已有研究探索使用轻量级视觉-语言模型，但这会牺牲时空推理能力。尽管部分方法提出引入额外三维输入可改善性能，但它们通常依赖大型视觉-语言模型融合三维与二维输入，且仍缺乏时序理解能力。为此，我们提出SwiftVLA架构，在保持设计效率的同时，为紧凑模型赋予四维理解能力。具体而言，本方法采用预训练的时序缓存四维视觉几何变换器，从二维图像中提取四维特征。随后，为增强视觉-语言模型协同利用二维图像与四维特征的能力，我们引入融合标记——一组通过未来预测目标训练的可学习标记，用于生成动作预测的统一表征。最后，我们提出掩码重构策略：通过掩码输入视觉-语言模型的四维数据并训练视觉-语言-动作模型进行重构，使视觉-语言模型能学习有效的四维表征，进而在推理阶段可舍弃四维分支且性能损失极小。真实环境与仿真实验表明，SwiftVLA性能优于轻量级基线模型，并与参数量达其7倍的大型视觉-语言-动作模型相当，在边缘设备上实现可比性能的同时，推理速度提升18倍，内存占用减少12倍。",
    "url": "https://huggingface.co/papers/2512.00903",
    "arxiv_url": "https://arxiv.org/abs/2512.00903"
  },
  {
    "title": "Ovis-Image Technical Report",
    "summary": "We introduce Ovis-Image, a 7B text-to-image model specifically optimized for high-quality text rendering, designed to operate efficiently under stringent computational constraints. Built upon our previous Ovis-U1 framework, Ovis-Image integrates a diffusion-based visual decoder with the stronger Ovis 2.5 multimodal backbone, leveraging a text-centric training pipeline that combines large-scale pre-training with carefully tailored post-training refinements. Despite its compact architecture, Ovis-Image achieves text rendering performance on par with significantly larger open models such as Qwen-Image and approaches closed-source systems like Seedream and GPT4o. Crucially, the model remains deployable on a single high-end GPU with moderate memory, narrowing the gap between frontier-level text rendering and practical deployment. Our results indicate that combining a strong multimodal backbone with a carefully designed, text-focused training recipe is sufficient to achieve reliable bilingual text rendering without resorting to oversized or proprietary models.",
    "translation": "标题：Ovis-Image技术报告\n\n摘要：本文介绍Ovis-Image，这是一个专门针对高质量文本渲染优化的70亿参数文生图模型，其设计目标是在严格的计算限制下高效运行。该模型基于我们先前提出的Ovis-U1框架构建，将基于扩散的视觉解码器与性能更强的Ovis 2.5多模态骨干网络相结合，并采用以文本为中心的训练流程——该流程融合了大规模预训练与精心设计的训练后优化阶段。尽管采用紧凑架构，Ovis-Image在文本渲染性能上已达到Qwen-Image等显著更大规模开源模型的水平，并接近Seedream、GPT4o等闭源系统。关键优势在于，该模型仅需单张高端GPU与适中显存即可部署，从而显著缩小了前沿文本渲染能力与实际应用部署之间的鸿沟。实验结果表明：通过将强大的多模态骨干网络与精心设计的文本导向训练方案相结合，无需依赖超大参数规模或专有模型，即可实现可靠的双语文本渲染能力。",
    "url": "https://huggingface.co/papers/2511.22982",
    "arxiv_url": "https://arxiv.org/abs/2511.22982"
  },
  {
    "title": "GUI Exploration Lab: Enhancing Screen Navigation in Agents via Multi-Turn Reinforcement Learning",
    "summary": "With the rapid development of Large Vision Language Models, the focus of Graphical User Interface (GUI) agent tasks shifts from single-screen tasks to complex screen navigation challenges. However, real-world GUI environments, such as PC software and mobile Apps, are often complex and proprietary, making it difficult to obtain the comprehensive environment information needed for agent training and evaluation. This limitation hinders systematic investigation and benchmarking of agent navigation capabilities. To address this limitation, we introduce GUI Exploration Lab, a simulation environment engine for GUI agent navigation research that enables flexible definition and composition of screens, icons, and navigation graphs, while providing full access to environment information for comprehensive agent training and evaluation. Through extensive experiments, we find that supervised fine-tuning enables effective memorization of fundamental knowledge, serving as a crucial foundation for subsequent training. Building on this, single-turn reinforcement learning further enhances generalization to unseen scenarios. Finally, multi-turn reinforcement learning encourages the development of exploration strategies through interactive trial and error, leading to further improvements in screen navigation performance. We validate our methods on both static and interactive benchmarks, demonstrating that our findings generalize effectively to real-world scenarios. These findings demonstrate the advantages of reinforcement learning approaches in GUI navigation and offer practical guidance for building more capable and generalizable GUI agents.",
    "translation": "标题：GUI探索实验室：通过多轮强化学习增强智能体在屏幕导航中的能力\n\n摘要：随着大规模视觉语言模型的快速发展，图形用户界面智能体任务的研究重点已从单一屏幕任务转向复杂的屏幕导航挑战。然而，现实中的图形用户界面环境（如PC软件和移动应用）通常结构复杂且具有专有性，难以获取智能体训练与评估所需的完整环境信息。这一限制阻碍了对智能体导航能力的系统性研究与基准测试。为应对此问题，我们引入了GUI探索实验室——一个专为图形用户界面智能体导航研究设计的仿真环境引擎。该引擎支持灵活定义与组合屏幕、图标及导航图结构，同时提供完整的环境信息访问权限，以实现全面的智能体训练与评估。通过大量实验，我们发现监督微调能够有效促进基础知识的记忆，为后续训练奠定关键基础。在此基础上，单轮强化学习进一步增强了模型对未见场景的泛化能力。最终，多轮强化学习通过交互式试错机制鼓励探索策略的发展，从而显著提升屏幕导航性能。我们在静态与交互式基准测试中验证了所提方法的有效性，证明研究成果能够很好地推广到实际应用场景。这些发现不仅彰显了强化学习方法在图形用户界面导航中的优势，也为构建更强大、更具泛化能力的图形用户界面智能体提供了实践指导。",
    "url": "https://huggingface.co/papers/2512.02423",
    "arxiv_url": "https://arxiv.org/abs/2512.02423"
  },
  {
    "title": "FlashVGGT: Efficient and Scalable Visual Geometry Transformers with Compressed Descriptor Attention",
    "summary": "3D reconstruction from multi-view images is a core challenge in computer vision. Recently, feed-forward methods have emerged as efficient and robust alternatives to traditional per-scene optimization techniques. Among them, state-of-the-art models like the Visual Geometry Grounding Transformer (VGGT) leverage full self-attention over all image tokens to capture global relationships. However, this approach suffers from poor scalability due to the quadratic complexity of self-attention and the large number of tokens generated in long image sequences. In this work, we introduce FlashVGGT, an efficient alternative that addresses this bottleneck through a descriptor-based attention mechanism. Instead of applying dense global attention across all tokens, FlashVGGT compresses spatial information from each frame into a compact set of descriptor tokens. Global attention is then computed as cross-attention between the full set of image tokens and this smaller descriptor set, significantly reducing computational overhead. Moreover, the compactness of the descriptors enables online inference over long sequences via a chunk-recursive mechanism that reuses cached descriptors from previous chunks. Experimental results show that FlashVGGT achieves reconstruction accuracy competitive with VGGT while reducing inference time to just 9.3% of VGGT for 1,000 images, and scaling efficiently to sequences exceeding 3,000 images. Our project page is available at https://wzpscott.github.io/flashvggt_page/.",
    "translation": "标题：FlashVGGT：基于压缩描述符注意力的高效可扩展视觉几何变换器\n\n摘要：从多视角图像进行三维重建是计算机视觉领域的核心挑战。近年来，前馈方法已成为传统逐场景优化技术的高效且鲁棒的替代方案。其中，视觉几何基础变换器（VGGT）等先进模型通过对所有图像令牌进行全局自注意力来捕捉全局关系。然而，由于自注意力的二次复杂度以及长图像序列中生成的大量令牌，该方法存在可扩展性不足的问题。本研究提出了FlashVGGT，一种基于描述符的注意力机制的高效替代方案，以解决这一瓶颈。FlashVGGT不再对所有令牌进行密集的全局注意力计算，而是将每帧图像的空间信息压缩为一组紧凑的描述符令牌。随后，通过完整图像令牌集与较小描述符集之间的交叉注意力来计算全局注意力，从而显著降低计算开销。此外，描述符的紧凑性使其能够通过分块递归机制实现长序列的在线推理，该机制可复用先前分块的缓存描述符。实验结果表明，FlashVGGT在重建精度上与VGGT相当，同时在处理1000张图像时将推理时间降至VGGT的9.3%，并能高效扩展至超过3000张图像的序列。项目页面详见：https://wzpscott.github.io/flashvggt_page/。",
    "url": "https://huggingface.co/papers/2512.01540",
    "arxiv_url": "https://arxiv.org/abs/2512.01540"
  },
  {
    "title": "BlockVid: Block Diffusion for High-Quality and Consistent Minute-Long Video Generation",
    "summary": "Generating minute-long videos is a critical step toward developing world models, providing a foundation for realistic extended scenes and advanced AI simulators. The emerging semi-autoregressive (block diffusion) paradigm integrates the strengths of diffusion and autoregressive models, enabling arbitrary-length video generation and improving inference efficiency through KV caching and parallel sampling. However, it yet faces two enduring challenges: (i) KV-cache-induced long-horizon error accumulation, and (ii) the lack of fine-grained long-video benchmarks and coherence-aware metrics. To overcome these limitations, we propose BlockVid, a novel block diffusion framework equipped with semantic-aware sparse KV cache, an effective training strategy called Block Forcing, and dedicated chunk-wise noise scheduling and shuffling to reduce error propagation and enhance temporal consistency. We further introduce LV-Bench, a fine-grained benchmark for minute-long videos, complete with new metrics evaluating long-range coherence. Extensive experiments on VBench and LV-Bench demonstrate that BlockVid consistently outperforms existing methods in generating high-quality, coherent minute-long videos. In particular, it achieves a 22.2% improvement on VDE Subject and a 19.4% improvement on VDE Clarity in LV-Bench over the state of the art approaches. Project website: https://ziplab.co/BlockVid. Inferix (Code): https://github.com/alibaba-damo-academy/Inferix.",
    "translation": "标题：BlockVid：基于块扩散的高质量、一致性分钟级视频生成方法\n\n摘要：生成长达分钟级别的视频是构建世界模型的关键一步，可为生成逼真的扩展场景和高级人工智能模拟器奠定基础。新兴的半自回归（块扩散）范式融合了扩散模型与自回归模型的优势，支持生成长度任意的视频，并通过KV缓存与并行采样提升了推理效率。然而，该方法仍面临两个长期存在的挑战：（i）由KV缓存引发的长时程误差累积问题；（ii）缺乏细粒度的长视频基准数据集以及针对连贯性评估的专用指标。为克服这些局限，本文提出BlockVid——一种新颖的块扩散框架。该框架配备了语义感知的稀疏KV缓存机制，采用名为“块强制训练”的高效训练策略，并结合专门设计的块级噪声调度与重排方法，以降低误差传播并增强时序一致性。我们进一步提出了LV-Bench，一个针对分钟级视频的细粒度基准数据集，其中包含评估长程连贯性的新指标。在VBench和LV-Bench上的大量实验表明，BlockVid在生成高质量、连贯的分钟级视频方面持续优于现有方法。具体而言，在LV-Bench评测中，相较于当前最优方法，BlockVid在VDE Subject指标上提升了22.2%，在VDE Clarity指标上提升了19.4%。项目网站：https://ziplab.co/BlockVid。代码仓库：https://github.com/alibaba-damo-academy/Inferix。",
    "url": "https://huggingface.co/papers/2511.22973",
    "arxiv_url": "https://arxiv.org/abs/2511.22973"
  },
  {
    "title": "C^2DLM: Causal Concept-Guided Diffusion Large Language Models",
    "summary": "Autoregressive (AR) language models and Diffusion Language Models (DLMs) constitute the two principal paradigms of large language models. However, both paradigms suffer from insufficient reasoning capabilities. Human reasoning inherently relies on causal knowledge and thought, which are reflected in natural language. But in the AR paradigm, language is modeled as next token prediction (a strictly left-to-right, token-by-token order), whereas natural language itself exhibits more flexible causal structures. In the DLM paradigm, the attention mechanism is fully connected, which entirely disregards causal order. To fill this gap, we propose a \\textbf{C}ausal \\textbf{C}oncept-Guided \\textbf{D}iffusion \\textbf{L}anguage \\textbf{M}odel (C^2DLM). Starting from DLM's fully connected attention, C^2DLM first obtains a concept-level causal graph from the teacher model, and then explicitly guides attention to learn causal relationships between concepts. By focusing on causal relationships and avoiding interference from difficult subgoals involving causal inversion, C^2DLM improves 12\\% with about 3.2 times training speedup in the COT-OrderPerturb task, and achieves an average gain of 1.31\\% across six downstream reasoning tasks. More details in the repository ~https://github.com/Kairong-Han/C-2-DLM{here}.",
    "translation": "标题：C^2DLM：基于因果概念引导的扩散大语言模型\n\n摘要：自回归语言模型与扩散语言模型构成了大语言模型的两大主流范式。然而，这两种范式均存在推理能力不足的问题。人类的推理本质上依赖于因果知识与思维，这些在自然语言中得以体现。但在自回归范式下，语言被建模为下一词元预测（严格的从左到右、逐词元顺序），而自然语言本身展现出更为灵活的因果结构。在扩散范式下，注意力机制采用全连接方式，完全忽略了因果顺序。为填补这一空白，本文提出一种**基于因果概念引导的扩散语言模型**。C^2DLM 从扩散模型的全连接注意力机制出发，首先从教师模型中获取概念级因果图，进而显式引导注意力学习概念间的因果关系。通过聚焦于因果关系并避免涉及因果反转的困难子目标干扰，C^2DLM 在 COT-OrderPerturb 任务中实现了 12% 的性能提升与约 3.2 倍的训练加速，并在六项下游推理任务中平均获得 1.31% 的性能增益。更多细节请参见代码仓库 ~https://github.com/Kairong-Han/C-2-DLM{此处}。",
    "url": "https://huggingface.co/papers/2511.22146",
    "arxiv_url": "https://arxiv.org/abs/2511.22146"
  },
  {
    "title": "Beyond Description: Cognitively Benchmarking Fine-Grained Action for Embodied Agents",
    "summary": "Multimodal Large Language Models (MLLMs) show promising results as decision-making engines for embodied agents operating in complex, physical environments. However, existing benchmarks often prioritize high-level planning or spatial reasoning, leaving the fine-grained action intelligence required for embodied physical interaction underexplored. To address this gap, we introduce CFG-Bench, a new benchmark designed to systematically evaluate this crucial capability. CFG-Bench consists of 1,368 curated videos paired with 19,562 three-modalities question-answer pairs targeting four cognitive abilities: 1) Physical Interaction, 2) Temporal-Causal Relation, 3) Intentional Understanding, and 4) Evaluative Judgment. Together, these dimensions provide a systematic framework for assessing a model's ability to translate visual observations into actionable knowledge, moving beyond mere surface-level recognition. Our comprehensive evaluation on CFG-Bench reveals that leading MLLMs struggle to produce detailed instructions for physical interactions and exhibit profound limitations in the higher-order reasoning of intention and evaluation. Moreover, supervised fine-tuning (SFT) on our data demonstrates that teaching an MLLMs to articulate fine-grained actions directly translates to significant performance gains on established embodied benchmarks. Our analysis highlights these limitations and offers insights for developing more capable and grounded embodied agents.",
    "translation": "标题：超越描述：具身智能体细粒度动作的认知基准测试\n\n摘要：多模态大语言模型作为在复杂物理环境中运行的具身智能体决策引擎展现出良好前景。然而，现有基准测试往往侧重于高层规划或空间推理，对具身物理交互所需的细粒度动作智能探索不足。为填补这一空白，我们提出CFG-Bench——一个旨在系统评估该关键能力的新型基准测试。该测试包含1,368个精选视频及与之匹配的19,562组三模态问答对，聚焦四大认知能力：1）物理交互，2）时序因果关联，3）意图理解，4）评估判断。这些维度共同构成了系统评估模型将视觉观察转化为可执行知识能力的框架，超越了表层识别范畴。我们在CFG-Bench上的综合评估表明：主流多模态大语言模型在生成物理交互的详细指令方面存在困难，在意图与评估的高阶推理上表现出明显局限。此外，基于本数据的监督微调实验证明，通过训练多模态大语言模型描述细粒度动作，能直接转化为现有具身基准测试上的显著性能提升。本研究的分析揭示了这些局限性，并为开发更具能力且接地气的具身智能体提供了重要洞见。",
    "url": "https://huggingface.co/papers/2511.18685",
    "arxiv_url": "https://arxiv.org/abs/2511.18685"
  },
  {
    "title": "In-Context Sync-LoRA for Portrait Video Editing",
    "summary": "Editing portrait videos is a challenging task that requires flexible yet precise control over a wide range of modifications, such as appearance changes, expression edits, or the addition of objects. The key difficulty lies in preserving the subject's original temporal behavior, demanding that every edited frame remains precisely synchronized with the corresponding source frame. We present Sync-LoRA, a method for editing portrait videos that achieves high-quality visual modifications while maintaining frame-accurate synchronization and identity consistency. Our approach uses an image-to-video diffusion model, where the edit is defined by modifying the first frame and then propagated to the entire sequence. To enable accurate synchronization, we train an in-context LoRA using paired videos that depict identical motion trajectories but differ in appearance. These pairs are automatically generated and curated through a synchronization-based filtering process that selects only the most temporally aligned examples for training. This training setup teaches the model to combine motion cues from the source video with the visual changes introduced in the edited first frame. Trained on a compact, highly curated set of synchronized human portraits, Sync-LoRA generalizes to unseen identities and diverse edits (e.g., modifying appearance, adding objects, or changing backgrounds), robustly handling variations in pose and expression. Our results demonstrate high visual fidelity and strong temporal coherence, achieving a robust balance between edit fidelity and precise motion preservation.",
    "translation": "标题：面向人像视频编辑的上下文同步低秩适应方法\n\n摘要：人像视频编辑是一项具有挑战性的任务，需要对多种修改（如外观变化、表情调整或对象添加）实现灵活而精确的控制。其核心难点在于保持主体原有的时序行为，要求每一帧编辑后的画面都能与对应源帧保持精确同步。本文提出同步低秩适应方法，该人像视频编辑方法在实现高质量视觉修改的同时，能够保持帧级精确同步与身份一致性。我们的方法基于图像到视频扩散模型，通过修改首帧定义编辑内容，并将其传播至整个序列。为实现精确同步，我们利用描绘相同运动轨迹但外观各异的配对视频训练上下文低秩适应模块。这些配对数据通过基于同步性的筛选流程自动生成与筛选，仅选择时序对齐程度最高的样本用于训练。该训练机制使模型能够将源视频的运动线索与编辑首帧引入的视觉变化相结合。通过在紧凑且精心筛选的同步人像数据集上训练，该方法能够泛化至未见过的身份特征和多样化编辑任务（如修改外观、添加对象或更换背景），并能稳健处理姿态与表情变化。实验结果表明，该方法在视觉保真度和时序连贯性方面表现优异，实现了编辑保真度与精确运动保持之间的稳健平衡。",
    "url": "https://huggingface.co/papers/2512.03013",
    "arxiv_url": "https://arxiv.org/abs/2512.03013"
  },
  {
    "title": "Benchmarking Scientific Understanding and Reasoning for Video Generation using VideoScience-Bench",
    "summary": "The next frontier for video generation lies in developing models capable of zero-shot reasoning, where understanding real-world scientific laws is crucial for accurate physical outcome modeling under diverse conditions. However, existing video benchmarks are physical commonsense-based, offering limited insight into video models' scientific reasoning capability. We introduce VideoScience-Bench, a benchmark designed to evaluate undergraduate-level scientific understanding in video models. Each prompt encodes a composite scientific scenario that requires understanding and reasoning across multiple scientific concepts to generate the correct phenomenon. The benchmark comprises 200 carefully curated prompts spanning 14 topics and 103 concepts in physics and chemistry. We conduct expert-annotated evaluations across seven state-of-the-art video models in T2V and I2V settings along five dimensions: Prompt Consistency, Phenomenon Congruency, Correct Dynamism, Immutability, and Spatio-Temporal Continuity. Using a VLM-as-a-Judge to assess video generations, we observe strong correlation with human assessments. To the best of our knowledge, VideoScience-Bench is the first benchmark to evaluate video models not only as generators but also as reasoners, requiring their generations to demonstrate scientific understanding consistent with expected physical and chemical phenomena. Our data and evaluation code are available at: https://github.com/hao-ai-lab/VideoScience{github.com/hao-ai-lab/VideoScience}.",
    "translation": "标题：基于VideoScience-Bench的视频生成科学理解与推理能力评估\n\n摘要：视频生成领域的下一个前沿在于开发具备零样本推理能力的模型，其中对现实世界科学定律的理解对于在不同条件下准确建模物理结果至关重要。然而，现有视频基准测试主要基于物理常识，对视频模型的科学推理能力评估有限。我们提出了VideoScience-Bench，这是一个专门用于评估视频模型对本科阶段科学概念理解能力的基准测试。每个提示都编码了一个复合科学场景，需要模型理解和推理多个科学概念才能生成正确的物理现象。该基准包含200个精心设计的提示，涵盖物理和化学领域的14个主题和103个概念。我们在文本到视频和图像到视频两种设置下，对七个前沿视频模型从五个维度进行了专家标注评估：提示一致性、现象符合性、动态正确性、不变性和时空连续性。通过使用视觉语言模型作为评判器评估视频生成结果，我们观察到其与人工评估结果具有强相关性。据我们所知，VideoScience-Bench是首个不仅将视频模型视为生成器，更将其作为推理器进行评估的基准测试，要求其生成内容展现出与预期物理化学现象一致的科学理解能力。我们的数据与评估代码已公开于：https://github.com/hao-ai-lab/VideoScience。",
    "url": "https://huggingface.co/papers/2512.02942",
    "arxiv_url": "https://arxiv.org/abs/2512.02942"
  },
  {
    "title": "Understanding and Harnessing Sparsity in Unified Multimodal Models",
    "summary": "Large multimodal models have achieved remarkable progress in both understanding and generation. Recent efforts pursue unified multimodal models that integrate heterogeneous components to support both capabilities within a single framework. However, such unification introduces inference inefficiencies, e.g., specific tasks or samples may not require the full knowledge or capacity of the unified model. Yet, a systematic understanding of how these inefficiencies manifest across different components remains limited. In this work, we first conduct a systematic analysis of unified multimodal model components using training-free pruning as a probing methodology, considering both depth pruning and width reduction. Our study reveals that the understanding component exhibits notable compressibility in both understanding and generation tasks, which is more pronounced in the latter. In contrast, the generation components are highly sensitive to compression, with performance deteriorating sharply even under moderate compression ratios. To address this limitation, we propose the Mixture-of-Experts (MoE) Adaptation, inspired by the dynamic activation patterns observed across different samples. This approach partitions the generation module into multiple experts and enables sparse activation to restore generation quality. We validate the effectiveness of sparse activation through expert-frozen tuning and further demonstrate that a fully trainable adaptation delivers additional gains. As a result, the adapted BAGEL model achieves performance comparable to the full model while activating only about half of its parameters. The code is released at https://github.com/Shwai-He/SparseUnifiedModel{this link}.",
    "translation": "标题：理解与利用统一多模态模型中的稀疏性\n\n摘要：大型多模态模型在理解与生成任务上均取得了显著进展。近期研究致力于构建统一多模态模型，通过整合异构组件以在单一框架内同时支持这两种能力。然而，这种统一性会引入推理效率问题，例如特定任务或样本可能无需调用统一模型的全部知识或容量。目前，对于这些效率问题在不同组件中如何体现的系统性认识仍然有限。本研究首先采用免训练剪枝作为探测方法，从深度剪枝与宽度缩减两个维度，对统一多模态模型组件进行了系统性分析。研究发现：理解组件在理解与生成任务中均表现出显著的压缩潜力，且在生成任务中更为明显；相比之下，生成组件对压缩高度敏感，即使在中等压缩比下性能也会急剧下降。为应对这一局限，我们受不同样本间动态激活模式的启发，提出了专家混合适应方法。该方法将生成模块划分为多个专家，并通过稀疏激活机制以恢复生成质量。我们通过专家冻结微调验证了稀疏激活的有效性，并进一步证明完全可训练的适应策略能带来额外增益。实验表明，经过适应的BAGEL模型在仅激活约半数参数的情况下，达到了与完整模型相当的性能。代码已发布于https://github.com/Shwai-He/SparseUnifiedModel。",
    "url": "https://huggingface.co/papers/2512.02351",
    "arxiv_url": "https://arxiv.org/abs/2512.02351"
  },
  {
    "title": "Visual Sync: Multi-Camera Synchronization via Cross-View Object Motion",
    "summary": "Today, people can easily record memorable moments, ranging from concerts, sports events, lectures, family gatherings, and birthday parties with multiple consumer cameras. However, synchronizing these cross-camera streams remains challenging. Existing methods assume controlled settings, specific targets, manual correction, or costly hardware. We present VisualSync, an optimization framework based on multi-view dynamics that aligns unposed, unsynchronized videos at millisecond accuracy. Our key insight is that any moving 3D point, when co-visible in two cameras, obeys epipolar constraints once properly synchronized. To exploit this, VisualSync leverages off-the-shelf 3D reconstruction, feature matching, and dense tracking to extract tracklets, relative poses, and cross-view correspondences. It then jointly minimizes the epipolar error to estimate each camera's time offset. Experiments on four diverse, challenging datasets show that VisualSync outperforms baseline methods, achieving an median synchronization error below 50 ms.",
    "translation": "标题：视觉同步：基于跨视角物体运动的多相机同步方法\n\n摘要：如今，人们能够轻松使用多台消费级相机记录从音乐会、体育赛事、讲座、家庭聚会到生日派对等各类值得纪念的时刻。然而，对这些跨相机视频流进行同步仍然具有挑战性。现有方法通常依赖于受控环境、特定目标、人工校正或昂贵硬件。本文提出VisualSync——一种基于多视角动态特性的优化框架，能够以毫秒级精度对齐未标定、未同步的视频。我们的核心思路是：任意在三维空间中运动的点，若同时被两台相机观测到，在正确同步后将满足极几何约束。为实现这一目标，VisualSync利用现成的三维重建、特征匹配与密集跟踪技术，提取短轨迹、相对位姿及跨视角对应关系，进而通过联合最小化极线误差来估计各相机的时间偏移量。在四个多样化、高挑战性的数据集上的实验表明，VisualSync优于现有基准方法，其中位同步误差低于50毫秒。",
    "url": "https://huggingface.co/papers/2512.02017",
    "arxiv_url": "https://arxiv.org/abs/2512.02017"
  },
  {
    "title": "Artemis: Structured Visual Reasoning for Perception Policy Learning",
    "summary": "Recent reinforcement-learning frameworks for visual perception policy have begun to incorporate intermediate reasoning chains expressed in natural language. Empirical observations indicate that such purely linguistic intermediate reasoning often reduces performance on perception tasks. We argue that the core issue lies not in reasoning per se but in the form of reasoning: while these chains perform semantic reasoning in an unstructured linguistic space, visual perception requires reasoning in a spatial and object-centric space. In response, we introduce Artemis, a perception-policy learning framework that performs structured proposal-based reasoning, where each intermediate step is represented as a (label, bounding-box) pair capturing a verifiable visual state. This design enables explicit tracking of intermediate states, direct supervision for proposal quality, and avoids ambiguity introduced by language-based reasoning. Artemis is built on Qwen2.5-VL-3B, achieves strong performance on grounding and detection task and exhibits substantial generalization to counting and geometric-perception tasks. The consistent improvements across these diverse settings confirm that aligning reasoning with spatial representations enhances perception-policy learning. Owing to its strengthened visual reasoning, Artemis also achieves competitive performance on general MLLM benchmarks, illustrating that spatially grounded reasoning provides a principled route toward scalable and general perception policies.",
    "translation": "标题：Artemis：面向感知策略学习的结构化视觉推理框架\n\n摘要：当前基于强化学习的视觉感知策略框架开始引入以自然语言表达的中间推理链。实证研究表明，这种纯语言形式的中间推理往往会降低感知任务的性能。我们认为核心问题不在于推理本身，而在于推理形式：现有推理链在非结构化的语言空间中进行语义推理，而视觉感知需要在以物体为中心的空间结构中进行推理。为此，我们提出Artemis——一种基于结构化提案推理的感知策略学习框架，其中每个中间步骤均表示为可验证视觉状态的（标签，边界框）对。该设计实现了对中间状态的显式追踪、对提案质量的直接监督，并避免了语言推理引入的歧义性。Artemis基于Qwen2.5-VL-3B构建，在指代定位与检测任务中表现优异，并在计数与几何感知任务上展现出显著的泛化能力。这些多样化场景中一致的性能提升证实了空间表征对齐的推理能有效增强感知策略学习。得益于强化的视觉推理能力，Artemis在通用多模态大模型基准测试中也展现出竞争力，表明基于空间锚定的推理为构建可扩展、泛化性强的感知策略提供了理论可行的路径。",
    "url": "https://huggingface.co/papers/2512.01988",
    "arxiv_url": "https://arxiv.org/abs/2512.01988"
  },
  {
    "title": "UnicEdit-10M: A Dataset and Benchmark Breaking the Scale-Quality Barrier via Unified Verification for Reasoning-Enriched Edits",
    "summary": "With the rapid advances of powerful multimodal models such as GPT-4o, Nano Banana, and Seedream 4.0 in Image Editing, the performance gap between closed-source and open-source models is widening, primarily due to the scarcity of large-scale, high-quality training data and comprehensive benchmarks capable of diagnosing model weaknesses across diverse editing behaviors. Existing data construction methods face a scale-quality trade-off: human annotations are high-quality but not scalable, while automated pipelines suffer from error propagation and noise. To address this, we introduce a lightweight data pipeline that replaces multi-toolchains with an end-to-end model and a unified post-verification stage. For scalable quality control, we train a 7B dual-task expert model, Qwen-Verify, for efficient failure detection and instruction recaptioning. This pipeline yields UnicEdit-10M, a 10M-scale dataset spanning diverse basic and complex editing tasks. We also propose UnicBench, a general benchmark that extends beyond basic edits to explicitly assess spatial and knowledge-driven reasoning. To enable fine-grained diagnosis, we introduce novel metrics, including Non-edit Consistency and Reasoning Accuracy. Our analysis of mainstream models on UnicBench reveals their limitations and provides clear directions for future research.",
    "translation": "标题：UnicEdit-10M：通过统一验证打破规模-质量壁垒的推理增强编辑数据集与基准\n\n摘要：随着GPT-4o、Nano Banana和Seedream 4.0等强大多模态模型在图像编辑领域的快速发展，闭源模型与开源模型之间的性能差距日益扩大，这主要源于缺乏大规模、高质量的训练数据以及能够全面诊断模型在不同编辑行为中弱点的综合性基准。现有数据构建方法面临规模与质量的权衡：人工标注质量高但难以扩展，而自动化流程则受限于错误传播和噪声问题。为解决这一挑战，我们提出一种轻量级数据构建流程，通过端到端模型和统一的后验证阶段替代多工具链。为实现可扩展的质量控制，我们训练了一个70亿参数的双任务专家模型Qwen-Verify，用于高效失败检测和指令重描述。该流程最终构建出UnicEdit-10M——一个涵盖多样化基础与复杂编辑任务的千万级规模数据集。同时，我们提出通用基准UnicBench，其评估范围超越基础编辑，可显式评估空间与知识驱动的推理能力。为支持细粒度诊断，我们引入包括非编辑一致性与推理准确度在内的新型评估指标。通过对主流模型在UnicBench上的系统性分析，我们揭示了现有模型的局限性，并为未来研究指明了明确方向。",
    "url": "https://huggingface.co/papers/2512.02790",
    "arxiv_url": "https://arxiv.org/abs/2512.02790"
  },
  {
    "title": "Shoe Style-Invariant and Ground-Aware Learning for Dense Foot Contact Estimation",
    "summary": "Foot contact plays a critical role in human interaction with the world, and thus exploring foot contact can advance our understanding of human movement and physical interaction. Despite its importance, existing methods often approximate foot contact using a zero-velocity constraint and focus on joint-level contact, failing to capture the detailed interaction between the foot and the world. Dense estimation of foot contact is crucial for accurately modeling this interaction, yet predicting dense foot contact from a single RGB image remains largely underexplored. There are two main challenges for learning dense foot contact estimation. First, shoes exhibit highly diverse appearances, making it difficult for models to generalize across different styles. Second, ground often has a monotonous appearance, making it difficult to extract informative features. To tackle these issues, we present a FEet COntact estimation (FECO) framework that learns dense foot contact with shoe style-invariant and ground-aware learning. To overcome the challenge of shoe appearance diversity, our approach incorporates shoe style adversarial training that enforces shoe style-invariant features for contact estimation. To effectively utilize ground information, we introduce a ground feature extractor that captures ground properties based on spatial context. As a result, our proposed method achieves robust foot contact estimation regardless of shoe appearance and effectively leverages ground information. Code will be released.",
    "translation": "标题：鞋型不变与地面感知的密集足部接触估计学习\n\n摘要：足部接触在人与世界的交互中起着关键作用，因此探索足部接触能够增进我们对人体运动与物理交互的理解。尽管其重要性显著，现有方法通常采用零速度约束来近似足部接触，并侧重于关节层面的接触估计，未能捕捉足部与世界之间的细节交互。足部接触的密集估计对于精确建模这种交互至关重要，然而从单张RGB图像预测密集足部接触的研究仍处于探索不足的状态。学习密集足部接触估计主要面临两大挑战：首先，鞋类外观高度多样化，导致模型难以在不同鞋型间泛化；其次，地面通常呈现单调外观，使得信息特征的提取变得困难。为解决这些问题，我们提出了一个足部接触估计框架，该框架通过鞋型不变与地面感知学习来实现密集足部接触估计。为应对鞋类外观多样性的挑战，我们的方法引入了鞋型对抗训练，以强制模型学习鞋型不变的特征用于接触估计。为有效利用地面信息，我们设计了一个地面特征提取器，基于空间上下文捕捉地面属性。实验表明，所提方法能够实现不受鞋类外观影响的鲁棒足部接触估计，并有效利用了地面信息。代码将公开发布。",
    "url": "https://huggingface.co/papers/2511.22184",
    "arxiv_url": "https://arxiv.org/abs/2511.22184"
  },
  {
    "title": "Gold-Medal-Level Olympiad Geometry Solving with Efficient Heuristic Auxiliary Constructions",
    "summary": "Automated theorem proving in Euclidean geometry, particularly for International Mathematical Olympiad (IMO) level problems, remains a major challenge and an important research focus in Artificial Intelligence. In this paper, we present a highly efficient method for geometry theorem proving that runs entirely on CPUs without relying on neural network-based inference. Our initial study shows that a simple random strategy for adding auxiliary points can achieve silver-medal level human performance on IMO. Building on this, we propose HAGeo, a Heuristic-based method for adding Auxiliary constructions in Geometric deduction that solves 28 of 30 problems on the IMO-30 benchmark, achieving gold-medal level performance and surpassing AlphaGeometry, a competitive neural network-based approach, by a notable margin. To evaluate our method and existing approaches more comprehensively, we further construct HAGeo-409, a benchmark consisting of 409 geometry problems with human-assessed difficulty levels. Compared with the widely used IMO-30, our benchmark poses greater challenges and provides a more precise evaluation, setting a higher bar for geometry theorem proving.",
    "translation": "标题：基于高效启发式辅助构造的奥数几何金牌级解题方法\n\n摘要：欧几里得几何的自动定理证明，特别是针对国际数学奥林匹克竞赛（IMO）级别的问题，仍然是人工智能领域的重大挑战和重要研究方向。本文提出一种高效的几何定理证明方法，该方法完全在CPU上运行，无需依赖基于神经网络的推理。我们的初步研究表明，采用简单的随机添加辅助点策略即可在IMO问题上达到银牌级别的人类解题水平。在此基础上，我们提出了HAGeo方法——一种基于启发式的几何推理辅助构造技术，该方法在IMO-30基准测试的30道题目中成功解答28道，达到金牌级解题水平，并以显著优势超越了基于神经网络的竞争性方法AlphaGeometry。为了更全面地评估本方法及现有技术，我们进一步构建了HAGeo-409基准测试集，该数据集包含409道经过人工难度评级的几何问题。与广泛使用的IMO-30相比，我们的基准测试集提出了更大挑战，提供了更精确的评估标准，为几何定理证明领域设立了更高的技术门槛。",
    "url": "https://huggingface.co/papers/2512.00097",
    "arxiv_url": "https://arxiv.org/abs/2512.00097"
  },
  {
    "title": "Masks Can Be Distracting: On Context Comprehension in Diffusion Language Models",
    "summary": "Masked Diffusion Language Models (MDLMs) have recently emerged as a promising alternative to Autoregressive Language Models (ARLMs), leveraging a denoising objective that, in principle, should enable more uniform context utilisation. In this work, we examine the context comprehension abilities of MDLMs and uncover two key limitations. First, despite their more global training objective and bidirectional attention mechanism, similarly to ARLMS, MDLMs exhibit a strong locality bias: performance is highly sensitive to the position of relevant information within the input, favouring local over distant context. Second, we show that appending a large number of mask tokens--required for generation--can significantly degrade context comprehension. Through systematic ablations, we find that these masks act as distractors, reducing the model's ability to process relevant information. To address this, we introduce a mask-agnostic loss function that encourages predictions to remain invariant to the number of appended masks. Fine-tuning with this objective substantially mitigates the distracting effect of masks, improving robustness of MDLMs. Overall, our findings reveal critical limitations of the current MDLM training paradigm and provide actionable insights for building diffusion-based language models with stronger context comprehension.",
    "translation": "标题：掩码可能成为干扰：论扩散语言模型中的上下文理解能力\n\n摘要：掩码扩散语言模型（MDLMs）近期作为自回归语言模型（ARLMs）的一种有前景的替代方案出现，其利用去噪目标函数，在理论上应能实现更均衡的上下文利用。本研究系统考察了MDLMs的上下文理解能力，并揭示了两项关键局限。首先，尽管MDLMs采用更具全局性的训练目标和双向注意力机制，但其与ARLMs相似地表现出强烈的局部性偏好：模型性能对输入中相关信息的位置高度敏感，更倾向于利用局部上下文而非远距离上下文。其次，我们发现生成所需的大量掩码标记会显著削弱模型的上下文理解能力。通过系统性消融实验，这些掩码被证实具有干扰作用，会降低模型处理相关信息的能力。为应对此问题，我们提出了一种掩码无关的损失函数，该函数促使模型预测结果对附加掩码数量保持稳定。基于此目标函数的微调显著缓解了掩码的干扰效应，提升了MDLMs的鲁棒性。总体而言，我们的研究揭示了当前MDLM训练范式的关键局限，并为构建具有更强上下文理解能力的扩散式语言模型提供了可操作的改进方向。",
    "url": "https://huggingface.co/papers/2511.21338",
    "arxiv_url": "https://arxiv.org/abs/2511.21338"
  },
  {
    "title": "CodeV: Code with Images for Faithful Visual Reasoning via Tool-Aware Policy Optimization",
    "summary": "Agentic vision-language models are increasingly trained to \"think with images\" by calling image operations. However, we show that high final-answer accuracy often hides unfaithful visual reasoning: models may invoke tools on irrelevant regions or ignore tool outputs entirely, yet still guess the correct answer. In this work, we first propose a faithfulness evaluation protocol that measures whether intermediate visual tool outputs (e.g., crops) actually contain the queried evidence. This reveals that recent visual agents achieve high final-answer accuracy but exhibit low rates of faithful tool-use on visual search benchmarks. We then introduce CodeV, a code-based visual agent trained with Tool-Aware Policy Optimization (TAPO). TAPO is a process-level RL framework that augments GRPO with dense rewards defined directly on visual tool inputs and outputs, rather than on chain-of-thought tokens, making supervision easier to verify and less susceptible to reward hacking. CodeV represents visual tools as executable Python code, and TAPO assigns step-wise rewards based solely on the question and tool output, encouraging both necessary and evidence-consistent tool use. In a two-stage SFT+RL pipeline, CodeV achieves competitive or superior accuracy while substantially increasing faithful tool-use rates on related visual search benchmarks. Beyond visual search, CodeV attains strong performance on a range of multimodal reasoning and math benchmarks, suggesting that explicitly supervising intermediate tool behavior is crucial for building trustworthy, agentic visual reasoning systems.",
    "translation": "标题：CodeV：基于工具感知策略优化的可信视觉推理图像编码方法\n\n摘要：具身视觉语言模型正日益通过调用图像操作来实现“基于图像的思考”。然而，我们发现最终答案的高准确率往往掩盖了不可信的视觉推理过程：模型可能在无关区域调用工具，或完全忽略工具输出，却仍能猜测出正确答案。本研究首先提出一种可信度评估协议，用于衡量中间视觉工具输出（如图像裁剪区域）是否实际包含查询所需的证据。分析表明，当前主流视觉代理模型虽然在最终答案准确率上表现优异，但在视觉搜索基准测试中展现出较低的可信工具使用率。为此，我们提出CodeV——一种基于代码的视觉代理模型，采用工具感知策略优化方法进行训练。该方法是一种过程级强化学习框架，在GRPO基础上引入直接针对视觉工具输入输出的密集奖励机制（而非思维链标记），使监督验证更易实施且能有效规避奖励操纵问题。CodeV将视觉工具定义为可执行的Python代码，其优化框架仅依据问题与工具输出分配逐步奖励，从而促进必要且符合证据一致性的工具使用。通过两阶段监督微调与强化学习训练流程，CodeV在相关视觉搜索基准测试中不仅取得具有竞争力的准确率，更显著提升了可信工具使用率。除视觉搜索任务外，CodeV在多项多模态推理与数学基准测试中均表现出色，这表明对中间工具行为的显式监督对于构建可信赖的具身视觉推理系统具有关键意义。",
    "url": "https://huggingface.co/papers/2511.19661",
    "arxiv_url": "https://arxiv.org/abs/2511.19661"
  },
  {
    "title": "BOOM: Beyond Only One Modality KIT's Multimodal Multilingual Lecture Companion",
    "summary": "The globalization of education and rapid growth of online learning have made localizing educational content a critical challenge. Lecture materials are inherently multimodal, combining spoken audio with visual slides, which requires systems capable of processing multiple input modalities. To provide an accessible and complete learning experience, translations must preserve all modalities: text for reading, slides for visual understanding, and speech for auditory learning. We present BOOM, a multimodal multilingual lecture companion that jointly translates lecture audio and slides to produce synchronized outputs across three modalities: translated text, localized slides with preserved visual elements, and synthesized speech. This end-to-end approach enables students to access lectures in their native language while aiming to preserve the original content in its entirety. Our experiments demonstrate that slide-aware transcripts also yield cascading benefits for downstream tasks such as summarization and question answering. We release our Slide Translation code at https://github.com/saikoneru/image-translator and integrate it in Lecture Translator at https://gitlab.kit.edu/kit/isl-ai4lt/lt-middleware/ltpipeline}\\footnote{All released code and models are licensed under the MIT License.",
    "translation": "标题：BOOM：超越单一模态——KIT多模态多语言讲座伴侣系统\n\n摘要：教育全球化与在线学习的快速发展使得教育内容本地化成为关键挑战。讲座材料本质上是多模态的，结合了语音音频与视觉幻灯片，这要求系统具备处理多种输入模态的能力。为提供完整且无障碍的学习体验，翻译必须保留所有模态特性：用于阅读的文本、用于视觉理解的幻灯片以及用于听觉学习的语音。本文提出BOOM多模态多语言讲座伴侣系统，通过联合翻译讲座音频与幻灯片，生成跨三种模态的同步输出：翻译文本、保留视觉元素的本地化幻灯片以及合成语音。这种端到端的解决方案使学生能够以母语访问讲座内容，同时力求完整保留原始信息。实验表明，融合幻灯片信息的转录文本还能为摘要生成和问答等下游任务带来级联效益。我们在https://github.com/saikoneru/image-translator发布了幻灯片翻译代码，并将其集成至讲座翻译系统（https://gitlab.kit.edu/kit/isl-ai4lt/lt-middleware/ltpipeline）。所有公开代码与模型均遵循MIT开源许可协议。",
    "url": "https://huggingface.co/papers/2512.02817",
    "arxiv_url": "https://arxiv.org/abs/2512.02817"
  },
  {
    "title": "Click2Graph: Interactive Panoptic Video Scene Graphs from a Single Click",
    "summary": "State-of-the-art Video Scene Graph Generation (VSGG) systems provide structured visual understanding but operate as closed, feed-forward pipelines with no ability to incorporate human guidance. In contrast, promptable segmentation models such as SAM2 enable precise user interaction but lack semantic or relational reasoning. We introduce Click2Graph, the first interactive framework for Panoptic Video Scene Graph Generation (PVSG) that unifies visual prompting with spatial, temporal, and semantic understanding. From a single user cue, such as a click or bounding box, Click2Graph segments and tracks the subject across time, autonomously discovers interacting objects, and predicts <subject, object, predicate> triplets to form a temporally consistent scene graph. Our framework introduces two key components: a Dynamic Interaction Discovery Module that generates subject-conditioned object prompts, and a Semantic Classification Head that performs joint entity and predicate reasoning. Experiments on the OpenPVSG benchmark demonstrate that Click2Graph establishes a strong foundation for user-guided PVSG, showing how human prompting can be combined with panoptic grounding and relational inference to enable controllable and interpretable video scene understanding.",
    "translation": "标题：Click2Graph：基于单次点击的交互式全景视频场景图生成\n\n摘要：当前最先进的视频场景图生成系统虽能提供结构化的视觉理解，但其作为封闭的前馈式流程运行，无法融入人工引导。相比之下，SAM2等可提示分割模型支持精确的用户交互，却缺乏语义或关系推理能力。本文提出Click2Graph，这是首个面向全景视频场景图生成的交互式框架，它将视觉提示与空间、时序及语义理解相融合。该系统仅需用户提供单次点击或边界框等简单线索，即可跨时序分割并跟踪目标主体，自主发现交互对象，进而预测〈主体，对象，谓词〉三元组以构建时序一致的场景图。本框架包含两个核心组件：动态交互发现模块（用于生成基于主体条件的对象提示）和语义分类头（用于执行实体与谓词的联合推理）。在OpenPVSG基准测试上的实验表明，Click2Graph为用户引导的全景视频场景图生成奠定了坚实基础，展示了如何将人工提示与全景定位及关系推理相结合，从而实现可控且可解释的视频场景理解。",
    "url": "https://huggingface.co/papers/2511.15948",
    "arxiv_url": "https://arxiv.org/abs/2511.15948"
  }
]