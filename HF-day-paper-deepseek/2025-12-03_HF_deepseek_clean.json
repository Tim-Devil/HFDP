[
  {
    "title": "DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models",
    "summary": "We introduce DeepSeek-V3.2, a model that harmonizes high computational efficiency with superior reasoning and agent performance. The key technical breakthroughs of DeepSeek-V3.2 are as follows: (1) DeepSeek Sparse Attention (DSA): We introduce DSA, an efficient attention mechanism that substantially reduces computational complexity while preserving model performance in long-context scenarios. (2) Scalable Reinforcement Learning Framework: By implementing a robust reinforcement learning protocol and scaling post-training compute, DeepSeek-V3.2 performs comparably to GPT-5. Notably, our high-compute variant, DeepSeek-V3.2-Speciale, surpasses GPT-5 and exhibits reasoning proficiency on par with Gemini-3.0-Pro, achieving gold-medal performance in both the 2025 International Mathematical Olympiad (IMO) and the International Olympiad in Informatics (IOI). (3) Large-Scale Agentic Task Synthesis Pipeline: To integrate reasoning into tool-use scenarios, we developed a novel synthesis pipeline that systematically generates training data at scale. This methodology facilitates scalable agentic post-training, yielding substantial improvements in generalization and instruction-following robustness within complex, interactive environments.",
    "translation": "标题：DeepSeek-V3.2：开拓开源大语言模型新前沿\n\n摘要：本文介绍DeepSeek-V3.2模型，该模型实现了高计算效率与卓越推理及智能体性能的协同优化。DeepSeek-V3.2的核心技术突破包括：（1）深度稀疏注意力机制：我们提出一种高效注意力机制，在长上下文场景中显著降低计算复杂度的同时保持模型性能。（2）可扩展强化学习框架：通过实施鲁棒的强化学习协议并扩展训练后计算规模，DeepSeek-V3.2达到与GPT-5相当的性能。特别值得注意的是，我们的高计算变体DeepSeek-V3.2-Speciale在多项指标上超越GPT-5，其推理能力与Gemini-3.0-Pro持平，并在2025年国际数学奥林匹克竞赛和国际信息学奥林匹克竞赛中均获得金牌级表现。（3）大规模智能体任务合成流程：为将推理能力融入工具使用场景，我们开发了创新的合成流程，可系统化生成大规模训练数据。该方法实现了可扩展的智能体训练后优化，在复杂交互环境中显著提升了泛化能力与指令遵循的鲁棒性。",
    "url": "https://huggingface.co/papers/2512.02556",
    "arxiv_url": "https://arxiv.org/abs/2512.02556"
  },
  {
    "title": "ToolOrchestra: Elevating Intelligence via Efficient Model and Tool Orchestration",
    "summary": "Large language models are powerful generalists, yet solving deep and complex problems such as those of the Humanity's Last Exam (HLE) remains both conceptually challenging and computationally expensive. We show that small orchestrators managing other models and a variety of tools can both push the upper bound of intelligence and improve efficiency in solving difficult agentic tasks. We introduce ToolOrchestra, a method for training small orchestrators that coordinate intelligent tools. ToolOrchestra explicitly uses reinforcement learning with outcome-, efficiency-, and user-preference-aware rewards. Using ToolOrchestra, we produce Orchestrator, an 8B model that achieves higher accuracy at lower cost than previous tool-use agents while aligning with user preferences on which tools are to be used for a given query. On HLE, Orchestrator achieves a score of 37.1%, outperforming GPT-5 (35.1%) while being 2.5x more efficient. On tau2-Bench and FRAMES, Orchestrator surpasses GPT-5 by a wide margin while using only about 30% of the cost. Extensive analysis shows that Orchestrator achieves the best trade-off between performance and cost under multiple metrics, and generalizes robustly to unseen tools. These results demonstrate that composing diverse tools with a lightweight orchestration model is both more efficient and more effective than existing methods, paving the way for practical and scalable tool-augmented reasoning systems.",
    "translation": "标题：ToolOrchestra：通过高效模型与工具编排提升智能水平\n\n摘要：大型语言模型是强大的通用型系统，但在解决如“人类终极考试”（HLE）这类深刻而复杂的问题时，仍面临概念上的挑战和高昂的计算成本。本文研究表明，通过小型编排器管理其他模型及多样化工具，既能提升智能水平的上限，也能提高解决复杂智能体任务的效率。我们提出了ToolOrchestra方法，用于训练能够协调智能工具的小型编排器。该方法明确采用强化学习，并结合了结果感知、效率感知和用户偏好感知的奖励机制。基于ToolOrchestra，我们训练出Orchestrator模型（参数量80亿），该模型在较低成本下取得了比以往工具使用智能体更高的准确率，并能根据用户偏好为给定查询选择适用工具。在HLE测试中，Orchestrator取得了37.1%的得分，优于GPT-5（35.1%），同时效率提升了2.5倍。在tau2-Bench和FRAMES基准测试中，Orchestrator以显著优势超越GPT-5，而成本仅为其约30%。深入分析表明，Orchestrator在多项指标下实现了性能与成本的最佳平衡，并对未见工具展现出强大的泛化能力。这些结果证明，通过轻量级编排模型整合多样化工具，比现有方法更高效、更有效，为实用且可扩展的工具增强推理系统开辟了道路。",
    "url": "https://huggingface.co/papers/2511.21689",
    "arxiv_url": "https://arxiv.org/abs/2511.21689"
  },
  {
    "title": "MultiShotMaster: A Controllable Multi-Shot Video Generation Framework",
    "summary": "Current video generation techniques excel at single-shot clips but struggle to produce narrative multi-shot videos, which require flexible shot arrangement, coherent narrative, and controllability beyond text prompts. To tackle these challenges, we propose MultiShotMaster, a framework for highly controllable multi-shot video generation. We extend a pretrained single-shot model by integrating two novel variants of RoPE. First, we introduce Multi-Shot Narrative RoPE, which applies explicit phase shift at shot transitions, enabling flexible shot arrangement while preserving the temporal narrative order. Second, we design Spatiotemporal Position-Aware RoPE to incorporate reference tokens and grounding signals, enabling spatiotemporal-grounded reference injection. In addition, to overcome data scarcity, we establish an automated data annotation pipeline to extract multi-shot videos, captions, cross-shot grounding signals and reference images. Our framework leverages the intrinsic architectural properties to support multi-shot video generation, featuring text-driven inter-shot consistency, customized subject with motion control, and background-driven customized scene. Both shot count and duration are flexibly configurable. Extensive experiments demonstrate the superior performance and outstanding controllability of our framework.",
    "translation": "标题：MultiShotMaster：一种可控的多镜头视频生成框架\n\n摘要：当前的视频生成技术擅长生成单镜头片段，但在生成叙事性多镜头视频方面存在困难，因为后者需要灵活的镜头编排、连贯的叙事以及超越文本提示的可控性。为应对这些挑战，我们提出了MultiShotMaster，这是一个用于高度可控的多镜头视频生成的框架。我们通过集成两种新颖的RoPE变体，扩展了一个预训练的单镜头模型。首先，我们引入了多镜头叙事RoPE，它在镜头转换处应用显式的相位偏移，从而在保持时间叙事顺序的同时实现灵活的镜头编排。其次，我们设计了时空位置感知RoPE，以融入参考标记和接地信号，实现基于时空的参考信息注入。此外，为克服数据稀缺问题，我们建立了一个自动化数据标注流程，用于提取多镜头视频、描述文本、跨镜头接地信号以及参考图像。我们的框架利用其固有的架构特性来支持多镜头视频生成，具备文本驱动的镜头间一致性、支持运动控制的定制主体以及背景驱动的定制场景等特征。镜头数量和时长均可灵活配置。大量实验证明了我们框架的卓越性能和出色的可控性。",
    "url": "https://huggingface.co/papers/2512.03041",
    "arxiv_url": "https://arxiv.org/abs/2512.03041"
  },
  {
    "title": "MG-Nav: Dual-Scale Visual Navigation via Sparse Spatial Memory",
    "summary": "We present MG-Nav (Memory-Guided Navigation), a dual-scale framework for zero-shot visual navigation that unifies global memory-guided planning with local geometry-enhanced control. At its core is the Sparse Spatial Memory Graph (SMG), a compact, region-centric memory where each node aggregates multi-view keyframe and object semantics, capturing both appearance and spatial structure while preserving viewpoint diversity. At the global level, the agent is localized on SMG and a goal-conditioned node path is planned via an image-to-instance hybrid retrieval, producing a sequence of reachable waypoints for long-horizon guidance. At the local level, a navigation foundation policy executes these waypoints in point-goal mode with obstacle-aware control, and switches to image-goal mode when navigating from the final node towards the visual target. To further enhance viewpoint alignment and goal recognition, we introduce VGGT-adapter, a lightweight geometric module built on the pre-trained VGGT model, which aligns observation and goal features in a shared 3D-aware space. MG-Nav operates global planning and local control at different frequencies, using periodic re-localization to correct errors. Experiments on HM3D Instance-Image-Goal and MP3D Image-Goal benchmarks demonstrate that MG-Nav achieves state-of-the-art zero-shot performance and remains robust under dynamic rearrangements and unseen scene conditions.",
    "translation": "标题：MG-Nav：基于稀疏空间记忆的双尺度视觉导航框架\n\n摘要：本文提出MG-Nav（记忆引导导航），一种用于零样本视觉导航的双尺度框架，该框架将全局记忆引导规划与局部几何增强控制相统一。其核心是稀疏空间记忆图（SMG），这是一种以区域为中心的紧凑记忆结构，其中每个节点聚合多视角关键帧与物体语义信息，在保持视角多样性的同时捕获外观与空间结构。在全局层面，智能体通过SMG进行定位，并基于图像-实例混合检索机制规划目标导向的节点路径，生成一系列可达航点以实现长时程导航引导。在局部层面，导航基础策略以点目标模式执行这些航点，并采用障碍物感知控制；当从最终节点向视觉目标导航时，系统切换至图像目标模式。为进一步增强视角对齐与目标识别能力，我们提出VGGT适配器——一个基于预训练VGGT模型构建的轻量化几何模块，可在共享三维感知空间中对齐观测特征与目标特征。MG-Nav以不同频率执行全局规划与局部控制，并通过周期性重定位修正误差。在HM3D实例-图像-目标与MP3D图像-目标基准测试上的实验表明，MG-Nav实现了最先进的零样本性能，并在动态场景重组与未知场景条件下保持鲁棒性。",
    "url": "https://huggingface.co/papers/2511.22609",
    "arxiv_url": "https://arxiv.org/abs/2511.22609"
  },
  {
    "title": "Skywork-R1V4: Toward Agentic Multimodal Intelligence through Interleaved Thinking with Images and DeepResearch",
    "summary": "Despite recent progress in multimodal agentic systems, existing approaches often treat image manipulation and web search as disjoint capabilities, rely heavily on costly reinforcement learning, and lack planning grounded in real tool-execution traces. To address these limitations, we present Skywork-R1V4, a 30B (A3B) parameter multimodal agentic model that unifies multimodal planning, active image manipulation (\"thinking with images\"), deep multimodal search, and, most critically, interleaved reasoning that dynamically alternates between visual operations and external knowledge retrieval. Trained solely via supervised fine-tuning on fewer than 30,000 high-quality, planning-execution-consistent trajectories and validated through stepwise consistency filtering, Skywork-R1V4 achieves state-of-the-art results across perception and multimodal search benchmarks: it scores 66.1 on MMSearch and 67.2 on FVQA, surpassing Gemini 2.5 Flash on all 11 metrics. Skywork-R1V4 exhibits emergent long-horizon reasoning at inference time, successfully orchestrating more than 10 tool calls to solve complex, multi-step tasks. Our results demonstrate that sophisticated agentic multimodal intelligence can be achieved through carefully curated supervised learning alone, without any reliance on reinforcement learning.",
    "translation": "标题：Skywork-R1V4：通过图像与深度研究的交错思考迈向具身多模态智能\n\n摘要：尽管多模态具身系统近期取得进展，但现有方法常将图像处理与网络搜索视为分离能力，严重依赖高成本的强化学习，且缺乏基于真实工具执行轨迹的规划。为应对这些局限，我们提出Skywork-R1V4——一个拥有300亿（实际激活30亿）参数的多模态具身模型。该模型统一了多模态规划、主动图像处理（“图像思考”）、深度多模态搜索，以及最关键的在视觉操作与外部知识检索间动态交替的交错推理能力。通过仅对不足3万条高质量、规划-执行一致的轨迹进行监督微调训练，并经过逐步一致性过滤验证，Skywork-R1V4在感知与多模态搜索基准测试中取得领先性能：在MMSearch上获得66.1分，在FVQA上获得67.2分，全部11项指标均超越Gemini 2.5 Flash模型。该模型在推理时展现出新兴的长程推理能力，可成功协调超过10次工具调用来解决复杂多步骤任务。我们的结果表明，无需依赖强化学习，仅通过精心构建的监督学习即可实现高度复杂的具身多模态智能。",
    "url": "https://huggingface.co/papers/2512.02395",
    "arxiv_url": "https://arxiv.org/abs/2512.02395"
  },
  {
    "title": "DualCamCtrl: Dual-Branch Diffusion Model for Geometry-Aware Camera-Controlled Video Generation",
    "summary": "This paper presents DualCamCtrl, a novel end-to-end diffusion model for camera-controlled video generation. Recent works have advanced this field by representing camera poses as ray-based conditions, yet they often lack sufficient scene understanding and geometric awareness. DualCamCtrl specifically targets this limitation by introducing a dual-branch framework that mutually generates camera-consistent RGB and depth sequences. To harmonize these two modalities, we further propose the Semantic Guided Mutual Alignment (SIGMA) mechanism, which performs RGB-depth fusion in a semantics-guided and mutually reinforced manner. These designs collectively enable DualCamCtrl to better disentangle appearance and geometry modeling, generating videos that more faithfully adhere to the specified camera trajectories. Additionally, we analyze and reveal the distinct influence of depth and camera poses across denoising stages and further demonstrate that early and late stages play complementary roles in forming global structure and refining local details. Extensive experiments demonstrate that DualCamCtrl achieves more consistent camera-controlled video generation, with over 40\\% reduction in camera motion errors compared with prior methods. Our project page: https://soyouthinkyoucantell.github.io/dualcamctrl-page/",
    "translation": "标题：DualCamCtrl：用于几何感知相机控制视频生成的双分支扩散模型\n\n摘要：本文提出DualCamCtrl，一种用于相机控制视频生成的新型端到端扩散模型。近期研究通过将相机姿态表示为基于光线的条件推动了该领域进展，但这些方法往往缺乏充分的场景理解与几何感知能力。DualCamCtrl针对这一局限性，引入能够协同生成相机一致性的RGB序列与深度序列的双分支框架。为协调这两种模态，我们进一步提出语义引导互对齐机制，以语义引导、相互增强的方式实现RGB-深度融合。这些设计使DualCamCtrl能更好解耦外观与几何建模，生成更精准遵循指定相机轨迹的视频。此外，我们分析揭示了深度信息与相机姿态在去噪各阶段的差异化影响，并论证早期与后期阶段在构建全局结构与细化局部细节方面具有互补作用。大量实验表明，DualCamCtrl实现了更一致的相机控制视频生成，相机运动误差较现有方法降低超过40%。项目页面：https://soyouthinkyoucantell.github.io/dualcamctrl-page/",
    "url": "https://huggingface.co/papers/2511.23127",
    "arxiv_url": "https://arxiv.org/abs/2511.23127"
  },
  {
    "title": "Guided Self-Evolving LLMs with Minimal Human Supervision",
    "summary": "AI self-evolution has long been envisioned as a path toward superintelligence, where models autonomously acquire, refine, and internalize knowledge from their own learning experiences. Yet in practice, unguided self-evolving systems often plateau quickly or even degrade as training progresses. These failures arise from issues such as concept drift, diversity collapse, and mis-evolution, as models reinforce their own biases and converge toward low-entropy behaviors. To enable models to self-evolve in a stable and controllable manner while minimizing reliance on human supervision, we introduce R-Few, a guided Self-Play Challenger-Solver framework that incorporates lightweight human oversight through in-context grounding and mixed training. At each iteration, the Challenger samples a small set of human-labeled examples to guide synthetic question generation, while the Solver jointly trains on human and synthetic examples under an online, difficulty-based curriculum. Across math and general reasoning benchmarks, R-Few achieves consistent and iterative improvements. For example, Qwen3-8B-Base improves by +3.0 points over R-Zero on math tasks and achieves performance on par with General-Reasoner, despite the latter being trained on 20 times more human data. Ablation studies confirm the complementary contributions of grounded challenger training and curriculum-based solver training, and further analysis shows that R-Few mitigates drift, yielding more stable and controllable co-evolutionary dynamics.",
    "translation": "标题：基于最小化人工监督的引导式自演化大语言模型\n\n摘要：人工智能的自演化长期以来被视为通往超智能的路径，即模型能够从自身学习经验中自主获取、优化并内化知识。然而在实践中，无引导的自演化系统往往在训练过程中快速陷入停滞甚至性能衰退。这类失败源于概念漂移、多样性坍缩和错误演化等问题——模型会不断强化自身偏见并收敛至低熵行为。为实现模型在稳定可控的前提下进行自演化，同时最大限度减少对人类监督的依赖，本文提出R-Few框架：一种融合轻量化人工监督的引导式自我博弈挑战者-求解器架构。该框架通过情境锚定与混合训练机制实现人类监督的嵌入。在每轮迭代中，挑战者模块采样少量人工标注示例以引导合成问题生成，而求解器模块则依据在线难度分级课程，对人工示例与合成示例进行联合训练。在数学与通用推理基准测试中，R-Few实现了持续迭代的性能提升。以Qwen3-8B-Base模型为例，在数学任务上较R-Zero提升3.0个点，其表现与通用推理模型持平，而后者训练所用的人工标注数据量是前者的20倍。消融实验证实了锚定式挑战者训练与课程化求解器训练的互补效应，进一步分析表明R-Few能有效缓解概念漂移，产生更稳定可控的协同演化动态。",
    "url": "https://huggingface.co/papers/2512.02472",
    "arxiv_url": "https://arxiv.org/abs/2512.02472"
  },
  {
    "title": "SimScale: Learning to Drive via Real-World Simulation at Scale",
    "summary": "Achieving fully autonomous driving systems requires learning rational decisions in a wide span of scenarios, including safety-critical and out-of-distribution ones. However, such cases are underrepresented in real-world corpus collected by human experts. To complement for the lack of data diversity, we introduce a novel and scalable simulation framework capable of synthesizing massive unseen states upon existing driving logs. Our pipeline utilizes advanced neural rendering with a reactive environment to generate high-fidelity multi-view observations controlled by the perturbed ego trajectory. Furthermore, we develop a pseudo-expert trajectory generation mechanism for these newly simulated states to provide action supervision. Upon the synthesized data, we find that a simple co-training strategy on both real-world and simulated samples can lead to significant improvements in both robustness and generalization for various planning methods on challenging real-world benchmarks, up to +6.8 EPDMS on navhard and +2.9 on navtest. More importantly, such policy improvement scales smoothly by increasing simulation data only, even without extra real-world data streaming in. We further reveal several crucial findings of such a sim-real learning system, which we term SimScale, including the design of pseudo-experts and the scaling properties for different policy architectures. Our simulation data and code would be released.",
    "translation": "标题：SimScale：基于大规模真实世界模拟的驾驶学习\n\n摘要：实现完全自动驾驶系统需要在广泛场景（包括安全关键场景与分布外场景）中学习理性决策。然而，人类专家采集的真实世界数据集中此类场景的代表性不足。为弥补数据多样性的缺失，我们提出一种新颖且可扩展的模拟框架，能够在现有驾驶日志基础上合成海量未见状态。该框架利用先进神经渲染技术与反应式环境，生成由扰动自车轨迹控制的高保真多视角观测数据。此外，我们为这些新模拟状态开发了伪专家轨迹生成机制以提供动作监督。基于合成数据，我们发现对真实世界样本与模拟样本采用简单的协同训练策略，可使多种规划方法在具有挑战性的真实世界基准测试中的鲁棒性与泛化能力显著提升——在navhard基准上最高提升+6.8 EPDMS，在navtest基准上提升+2.9。更重要的是，即使没有额外真实世界数据流输入，仅通过增加模拟数据即可实现策略性能的平稳提升。我们进一步揭示了此类模拟-真实学习系统（命名为SimScale）的若干关键发现，包括伪专家机制的设计方案以及不同策略架构的扩展特性。我们的模拟数据与代码将予以公开。",
    "url": "https://huggingface.co/papers/2511.23369",
    "arxiv_url": "https://arxiv.org/abs/2511.23369"
  },
  {
    "title": "InnoGym: Benchmarking the Innovation Potential of AI Agents",
    "summary": "LLMs and Agents have achieved impressive progress in code generation, mathematical reasoning, and scientific discovery. However, existing benchmarks primarily measure correctness, overlooking the diversity of methods behind solutions. True innovation depends not only on producing correct answers but also on the originality of the approach. We present InnoGym, the first benchmark and framework designed to systematically evaluate the innovation potential of AI agents. InnoGym introduces two complementary metrics: performance gain, which measures improvement over the best-known solutions, and novelty, which captures methodological differences from prior approaches. The benchmark includes 18 carefully curated tasks from real-world engineering and scientific domains, each standardized through resource filtering, evaluator validation, and solution collection. In addition, we provide iGym, a unified execution environment for reproducible and long-horizon evaluations. Extensive experiments show that while some agents produce novel approaches, their lack of robustness limits performance gains. These results highlight a key gap between creativity and effectiveness, underscoring the need for benchmarks that evaluate both.",
    "translation": "标题：InnoGym：人工智能代理创新潜能的基准测试框架\n\n摘要：大语言模型与智能代理在代码生成、数学推理及科学发现领域已取得显著进展。然而，现有基准测试主要关注结果正确性，忽视了解决方案背后方法的多样性。真正的创新不仅要求答案正确，更取决于方法的原创性。本文提出InnoGym——首个系统性评估人工智能代理创新潜能的基准测试框架。该框架引入两项互补指标：性能增益（衡量对已知最优方案的改进程度）与新颖性（量化与既有方法论的差异度）。基准测试涵盖从现实工程与科学领域中精选的18项任务，每项任务均通过资源筛选、评估验证和解决方案收集实现标准化。此外，我们同步推出iGym统一执行环境，支持可复现的长周期评估。大量实验表明，尽管部分代理能提出新颖方法，但其鲁棒性不足限制了性能提升。这些结果揭示了创造力与有效性之间的关键差距，凸显了需要同时评估两者的基准测试体系的重要性。",
    "url": "https://huggingface.co/papers/2512.01822",
    "arxiv_url": "https://arxiv.org/abs/2512.01822"
  },
  {
    "title": "ViSAudio: End-to-End Video-Driven Binaural Spatial Audio Generation",
    "summary": "Despite progress in video-to-audio generation, the field focuses predominantly on mono output, lacking spatial immersion. Existing binaural approaches remain constrained by a two-stage pipeline that first generates mono audio and then performs spatialization, often resulting in error accumulation and spatio-temporal inconsistencies. To address this limitation, we introduce the task of end-to-end binaural spatial audio generation directly from silent video. To support this task, we present the BiAudio dataset, comprising approximately 97K video-binaural audio pairs spanning diverse real-world scenes and camera rotation trajectories, constructed through a semi-automated pipeline. Furthermore, we propose ViSAudio, an end-to-end framework that employs conditional flow matching with a dual-branch audio generation architecture, where two dedicated branches model the audio latent flows. Integrated with a conditional spacetime module, it balances consistency between channels while preserving distinctive spatial characteristics, ensuring precise spatio-temporal alignment between audio and the input video. Comprehensive experiments demonstrate that ViSAudio outperforms existing state-of-the-art methods across both objective metrics and subjective evaluations, generating high-quality binaural audio with spatial immersion that adapts effectively to viewpoint changes, sound-source motion, and diverse acoustic environments. Project website: https://kszpxxzmc.github.io/ViSAudio-project.",
    "translation": "标题：ViSAudio：端到端视频驱动的双耳空间音频生成\n\n摘要：尽管视频到音频生成领域已取得进展，但现有研究主要集中于单声道输出，缺乏空间沉浸感。现有的双耳音频生成方法仍受限于两阶段流程，即首先生成单声道音频再进行空间化处理，这往往导致误差累积和时空不一致性问题。为突破这一局限，我们提出了从无声视频直接端到端生成双耳空间音频的任务。为支持该任务，我们构建了BiAudio数据集，包含约9.7万个视频-双耳音频对，涵盖多样化的真实场景与摄像机旋转轨迹，并通过半自动化流程构建。进一步，我们提出ViSAudio端到端框架，采用条件流匹配与双分支音频生成架构，其中两个独立分支分别建模音频潜在流。该框架结合条件时空模块，在保持声道间一致性的同时保留独特的空间特征，确保音频与输入视频的精确时空对齐。综合实验表明，ViSAudio在客观指标和主观评估上均优于现有先进方法，能够生成具有空间沉浸感的高质量双耳音频，并能有效适应视角变化、声源运动及多样声学环境。项目网站：https://kszpxxzmc.github.io/ViSAudio-project。",
    "url": "https://huggingface.co/papers/2512.03036",
    "arxiv_url": "https://arxiv.org/abs/2512.03036"
  },
  {
    "title": "Glance: Accelerating Diffusion Models with 1 Sample",
    "summary": "Diffusion models have achieved remarkable success in image generation, yet their deployment remains constrained by the heavy computational cost and the need for numerous inference steps. Previous efforts on fewer-step distillation attempt to skip redundant steps by training compact student models, yet they often suffer from heavy retraining costs and degraded generalization. In this work, we take a different perspective: we accelerate smartly, not evenly, applying smaller speedups to early semantic stages and larger ones to later redundant phases. We instantiate this phase-aware strategy with two experts that specialize in slow and fast denoising phases. Surprisingly, instead of investing massive effort in retraining student models, we find that simply equipping the base model with lightweight LoRA adapters achieves both efficient acceleration and strong generalization. We refer to these two adapters as Slow-LoRA and Fast-LoRA. Through extensive experiments, our method achieves up to 5 acceleration over the base model while maintaining comparable visual quality across diverse benchmarks. Remarkably, the LoRA experts are trained with only 1 samples on a single V100 within one hour, yet the resulting models generalize strongly on unseen prompts.",
    "translation": "标题：Glance：单样本加速扩散模型\n\n摘要：扩散模型在图像生成领域取得了显著成功，但其部署仍受限于高昂的计算成本和大量推理步骤的需求。先前关于减少步数的蒸馏方法试图通过训练紧凑的学生模型来跳过冗余步骤，但这些方法通常面临繁重的重新训练成本与泛化性能下降的问题。在本研究中，我们采取了一种不同的视角：我们进行智能而非均匀的加速，对早期语义阶段施加较小的加速，而对后期冗余阶段施加较大的加速。我们通过两个分别专注于慢速和快速去噪阶段的专家模型来实现这一阶段感知策略。令人惊讶的是，我们并未投入大量精力重新训练学生模型，而是发现仅需为基础模型配备轻量级的LoRA适配器，即可同时实现高效加速与强大的泛化能力。我们将这两种适配器称为Slow-LoRA与Fast-LoRA。通过大量实验验证，我们的方法在保持多种基准测试中视觉质量相当的前提下，相比基础模型实现了最高5倍的加速。值得注意的是，LoRA专家模型仅需在单个V100显卡上使用1个样本训练一小时，所得模型在未见过的提示词上仍展现出强大的泛化性能。",
    "url": "https://huggingface.co/papers/2512.02899",
    "arxiv_url": "https://arxiv.org/abs/2512.02899"
  },
  {
    "title": "WorldMM: Dynamic Multimodal Memory Agent for Long Video Reasoning",
    "summary": "Recent advances in video large language models have demonstrated strong capabilities in understanding short clips. However, scaling them to hours- or days-long videos remains highly challenging due to limited context capacity and the loss of critical visual details during abstraction. Existing memory-augmented methods mitigate this by leveraging textual summaries of video segments, yet they heavily rely on text and fail to utilize visual evidence when reasoning over complex scenes. Moreover, retrieving from fixed temporal scales further limits their flexibility in capturing events that span variable durations. To address this, we introduce WorldMM, a novel multimodal memory agent that constructs and retrieves from multiple complementary memories, encompassing both textual and visual representations. WorldMM comprises three types of memory: episodic memory indexes factual events across multiple temporal scales, semantic memory continuously updates high-level conceptual knowledge, and visual memory preserves detailed information about scenes. During inference, an adaptive retrieval agent iteratively selects the most relevant memory source and leverages multiple temporal granularities based on the query, continuing until it determines that sufficient information has been gathered. WorldMM significantly outperforms existing baselines across five long video question-answering benchmarks, achieving an average 8.4% performance gain over previous state-of-the-art methods, showing its effectiveness on long video reasoning.",
    "translation": "标题：WorldMM：面向长视频推理的动态多模态记忆智能体\n\n摘要：近期视频大语言模型的发展在短视频理解方面展现出强大能力。然而，由于上下文容量有限以及在抽象过程中关键视觉细节的丢失，将其扩展至数小时甚至数天时长的视频仍面临巨大挑战。现有基于记忆增强的方法通过利用视频片段的文本摘要缓解了这一问题，但这些方法严重依赖文本，在复杂场景推理时未能有效利用视觉证据。此外，固定时间尺度的检索机制进一步限制了其捕捉可变时长事件的灵活性。为此，我们提出WorldMM——一种新颖的多模态记忆智能体，它能够构建并检索包含文本与视觉表征的多种互补记忆。WorldMM包含三类记忆：跨多时间尺度索引事实事件的**情景记忆**、持续更新高层概念知识的**语义记忆**，以及保留场景细节信息的**视觉记忆**。在推理过程中，自适应检索智能体会根据查询内容迭代选择最相关的记忆源，并利用多时间粒度进行检索，直至确定已收集足够信息。在五个长视频问答基准测试中，WorldMM显著优于现有基线方法，较先前最优方法的平均性能提升达8.4%，充分证明了其在长视频推理任务中的有效性。",
    "url": "https://huggingface.co/papers/2512.02425",
    "arxiv_url": "https://arxiv.org/abs/2512.02425"
  },
  {
    "title": "Deep Research: A Systematic Survey",
    "summary": "Large language models (LLMs) have rapidly evolved from text generators into powerful problem solvers. Yet, many open tasks demand critical thinking, multi-source, and verifiable outputs, which are beyond single-shot prompting or standard retrieval-augmented generation. Recently, numerous studies have explored Deep Research (DR), which aims to combine the reasoning capabilities of LLMs with external tools, such as search engines, thereby empowering LLMs to act as research agents capable of completing complex, open-ended tasks. This survey presents a comprehensive and systematic overview of deep research systems, including a clear roadmap, foundational components, practical implementation techniques, important challenges, and future directions. Specifically, our main contributions are as follows: (i) we formalize a three-stage roadmap and distinguish deep research from related paradigms; (ii) we introduce four key components: query planning, information acquisition, memory management, and answer generation, each paired with fine-grained sub-taxonomies; (iii) we summarize optimization techniques, including prompting, supervised fine-tuning, and agentic reinforcement learning; and (iv) we consolidate evaluation criteria and open challenges, aiming to guide and facilitate future development. As the field of deep research continues to evolve rapidly, we are committed to continuously updating this survey to reflect the latest progress in this area.",
    "translation": "标题：深度研究：一项系统性综述\n\n摘要：大型语言模型已从文本生成工具迅速发展为强大的问题解决者。然而，许多开放性任务需要批判性思维、多源信息与可验证的输出，这超出了单次提示或标准检索增强生成的能力范围。近年来，众多研究开始探索深度研究范式，其目标是将大型语言模型的推理能力与搜索引擎等外部工具相结合，从而使大型语言模型能够作为研究智能体完成复杂、开放式的任务。本综述对深度研究系统进行了全面而系统的梳理，包括清晰的发展路线图、基础构成要素、实际实现技术、重要挑战与未来方向。具体而言，我们的主要贡献如下：（一）提出三阶段发展路线图，明确区分深度研究与相关范式；（二）系统阐述四大核心组件：查询规划、信息获取、记忆管理与答案生成，并为每个组件建立细粒度分类体系；（三）总结提示工程、监督微调与智能体强化学习等优化技术；（四）整合评估标准与开放挑战，旨在引导和促进该领域的未来发展。随着深度研究领域的持续快速演进，我们将持续更新本综述以反映该领域的最新进展。",
    "url": "https://huggingface.co/papers/2512.02038",
    "arxiv_url": "https://arxiv.org/abs/2512.02038"
  },
  {
    "title": "PixelDiT: Pixel Diffusion Transformers for Image Generation",
    "summary": "Latent-space modeling has been the standard for Diffusion Transformers (DiTs). However, it relies on a two-stage pipeline where the pretrained autoencoder introduces lossy reconstruction, leading to error accumulation while hindering joint optimization. To address these issues, we propose PixelDiT, a single-stage, end-to-end model that eliminates the need for the autoencoder and learns the diffusion process directly in the pixel space. PixelDiT adopts a fully transformer-based architecture shaped by a dual-level design: a patch-level DiT that captures global semantics and a pixel-level DiT that refines texture details, enabling efficient training of a pixel-space diffusion model while preserving fine details. Our analysis reveals that effective pixel-level token modeling is essential to the success of pixel diffusion. PixelDiT achieves 1.61 FID on ImageNet 256x256, surpassing existing pixel generative models by a large margin. We further extend PixelDiT to text-to-image generation and pretrain it at the 1024x1024 resolution in pixel space. It achieves 0.74 on GenEval and 83.5 on DPG-bench, approaching the best latent diffusion models.",
    "translation": "标题：PixelDiT：用于图像生成的像素扩散变换器\n\n摘要：潜空间建模一直是扩散变换器（DiTs）的标准范式。然而，该方法依赖于两阶段流程，其中预训练的自编码器会引入有损重构，导致误差累积并阻碍联合优化。为解决这些问题，我们提出PixelDiT——一种单阶段端到端模型，无需依赖自编码器，直接在像素空间中学习扩散过程。PixelDiT采用全变换器架构，其设计包含双重层级：捕捉全局语义的块级DiT与细化纹理细节的像素级DiT，在保持精细细节的同时实现了像素空间扩散模型的高效训练。我们的分析表明，有效的像素级令牌建模是像素扩散成功的关键。PixelDiT在ImageNet 256×256数据集上取得了1.61的FID分数，大幅超越现有像素生成模型。我们进一步将PixelDiT扩展至文本到图像生成任务，并在像素空间中以1024×1024分辨率进行预训练。该模型在GenEval评测中达到0.74分，在DPG-bench中取得83.5分，性能已接近最优潜扩散模型。",
    "url": "https://huggingface.co/papers/2511.20645",
    "arxiv_url": "https://arxiv.org/abs/2511.20645"
  },
  {
    "title": "Mixture of Horizons in Action Chunking",
    "summary": "Vision-language-action (VLA) models have shown remarkable capabilities in robotic manipulation, but their performance is sensitive to the action chunk length used during training, termed horizon. Our empirical study reveals an inherent trade-off: longer horizons provide stronger global foresight but degrade fine-grained accuracy, while shorter ones sharpen local control yet struggle on long-term tasks, implying fixed choice of single horizons being suboptimal. To mitigate the trade-off, we propose a mixture of horizons (MoH) strategy. MoH rearranges the action chunk into several segments with different horizons, processes them in parallel with a shared action transformer, and fuses outputs with a light linear gate. It has three appealing benefits. 1) MoH exploits long-term foresight and short-term precision jointly within a single model, improving both performance and generalizability to complex tasks. 2) MoH is plug-and-play for full-attention action modules with minimal training or inference overhead. 3) MoH enables dynamic inference with adaptive horizons, which selects stable actions through cross-horizon consensus, achieving 2.5times higher throughput than baselines while preserving superior performance. Extensive experiments over flow-based policies π_0, π_{0.5}, and one-step regression policy π_{reg} demonstrate that MoH yields consistent and significant gains on both simulations and real-world tasks. Notably, under mixed-task setting, π_{0.5} with MoH reaches a new state-of-the-art with 99% average success rate on LIBERO after only 30k training iterations. Project page: https://github.com/Timsty1/MixtureOfHorizons",
    "translation": "标题：动作分块中的混合视界策略\n\n摘要：视觉-语言-动作模型在机器人操控任务中展现出卓越能力，但其性能对训练时采用的动作块长度（称为视界）极为敏感。我们的实证研究揭示了一个内在权衡：较长视界能提供更强的全局预见性，但会降低细粒度动作精度；较短视界虽能提升局部控制精度，却在长期任务中表现欠佳，这意味着固定单一视界的选择具有次优性。为缓解这一矛盾，我们提出混合视界策略。该策略将动作块重组为具有不同视界的多个片段，通过共享动作变换器进行并行处理，并利用轻量级线性门融合输出结果。该方法具有三大优势：1）在单一模型内协同利用长期预见性与短期精确性，提升复杂任务中的性能表现与泛化能力；2）可作为即插即用模块适配全注意力动作架构，仅引入极小的训练与推理开销；3）支持自适应视界的动态推理机制，通过跨视界一致性筛选稳定动作，在保持卓越性能的同时实现比基线方法高2.5倍的吞吐量。基于流式策略π_0、π_0.5及单步回归策略π_reg的广泛实验表明，混合视界策略在仿真与真实场景任务中均能带来持续显著的性能提升。值得注意的是，在混合任务设定下，采用混合视界的π_0.5策略仅通过3万次训练迭代即在LIBERO基准上达到99%的平均成功率，创造了新的性能纪录。项目页面：https://github.com/Timsty1/MixtureOfHorizons",
    "url": "https://huggingface.co/papers/2511.19433",
    "arxiv_url": "https://arxiv.org/abs/2511.19433"
  },
  {
    "title": "WUSH: Near-Optimal Adaptive Transforms for LLM Quantization",
    "summary": "Quantization to low bitwidth is a standard approach for deploying large language models, however, a few extreme weights and activations stretch the dynamic range and reduce the effective resolution of the quantizer. A common mitigation approach is to apply some fixed orthogonal transforms, such as Hadamard matrices, before quantization, which typically reduces the dynamic range. Yet, these transforms ignore the statistics of the data, and their optimality is currently not understood. In this work, we derive, for the first time, closed-form optimal linear blockwise transforms for joint weight-activation quantization using standard data-free quantizers for common numerical formats. Specifically, we provide derivations of the optimal adaptive (data-aware) transforms for round-to-nearest (RTN), AbsMax-scaled block quantizers for both integer and floating-point formats. The resulting construction, which we call WUSH, combines a Hadamard backbone with a data-dependent component based on second-order moments, yielding a non-orthogonal transform that is provably optimal under mild assumptions and remains structured for efficient implementation. Preliminary experimental results show that our approach consistently improves upon the Hadamard transform for common formats.",
    "translation": "标题：WUSH：面向大语言模型量化的近最优自适应变换方法\n\n摘要：低比特量化是部署大语言模型的常用方法，但少数极端权重和激活值会扩大动态范围，降低量化器的有效分辨率。常见的缓解方法是在量化前应用固定正交变换（如哈达玛矩阵），这通常能压缩动态范围。然而，此类变换忽略了数据统计特性，其最优性尚未得到充分论证。本研究首次推导出针对通用数值格式、采用无数据标准量化器的权重-激活联合量化的闭式最优线性分块变换。具体而言，我们推导了适用于整数与浮点格式的最近舍入（RTN）和绝对值最大分块量化器的最优自适应（数据感知）变换。所得构建方法命名为WUSH，其将哈达玛矩阵主干与基于二阶矩的数据依赖组件相结合，形成一种在温和假设下可证明最优的非正交变换，同时保持结构化特性以实现高效计算。初步实验结果表明，该方法在多种常用数值格式下均能稳定超越哈达玛变换的性能。",
    "url": "https://huggingface.co/papers/2512.00956",
    "arxiv_url": "https://arxiv.org/abs/2512.00956"
  },
  {
    "title": "GoRL: An Algorithm-Agnostic Framework for Online Reinforcement Learning with Generative Policies",
    "summary": "Reinforcement learning (RL) faces a persistent tension: policies that are stable to optimize are often too simple to represent the multimodal action distributions needed for complex control. Gaussian policies provide tractable likelihoods and smooth gradients, but their unimodal form limits expressiveness. Conversely, generative policies based on diffusion or flow matching can model rich multimodal behaviors; however, in online RL, they are frequently unstable due to intractable likelihoods and noisy gradients propagating through deep sampling chains. We address this tension with a key structural principle: decoupling optimization from generation. Building on this insight, we introduce GoRL (Generative Online Reinforcement Learning), a framework that optimizes a tractable latent policy while utilizing a conditional generative decoder to synthesize actions. A two-timescale update schedule enables the latent policy to learn stably while the decoder steadily increases expressiveness, without requiring tractable action likelihoods. Across a range of continuous-control tasks, GoRL consistently outperforms both Gaussian policies and recent generative-policy baselines. Notably, on the HopperStand task, it reaches a normalized return above 870, more than 3 times that of the strongest baseline. These results demonstrate that separating optimization from generation provides a practical path to policies that are both stable and highly expressive.",
    "translation": "标题：GoRL：一种与算法无关的生成策略在线强化学习框架\n\n摘要：强化学习面临一个长期存在的矛盾：易于优化的策略往往过于简单，难以表达复杂控制所需的多模态动作分布。高斯策略提供了易于处理的似然函数和平滑梯度，但其单模态形式限制了表达能力。相反，基于扩散或流匹配的生成策略能够建模丰富的多模态行为；然而，在在线强化学习中，由于似然函数难以计算且梯度在深度采样链中传播时存在噪声，这类策略常常表现出不稳定性。我们通过一个关键的结构性原则来解决这一矛盾：将优化过程与生成过程解耦。基于这一思路，我们提出了GoRL（生成式在线强化学习）框架，该框架通过优化一个易于处理的潜变量策略，同时利用条件生成解码器来合成动作。采用双时间尺度更新机制，使得潜变量策略能够稳定学习，而解码器则逐步提升表达能力，且无需计算动作的易处理似然函数。在一系列连续控制任务中，GoRL的表现始终优于高斯策略及近期提出的生成策略基线方法。值得注意的是，在HopperStand任务中，其归一化回报超过870，达到最强基线方法的三倍以上。这些结果表明，将优化与生成分离为实现既稳定又具有高表达能力的策略提供了一条可行路径。",
    "url": "https://huggingface.co/papers/2512.02581",
    "arxiv_url": "https://arxiv.org/abs/2512.02581"
  },
  {
    "title": "CUDA-L2: Surpassing cuBLAS Performance for Matrix Multiplication through Reinforcement Learning",
    "summary": "In this paper, we propose CUDA-L2, a system that combines large language models (LLMs) and reinforcement learning (RL) to automatically optimize Half-precision General Matrix Multiply (HGEMM) CUDA kernels. Using CUDA execution speed as the RL reward, CUDA-L2 automatically optimizes HGEMM kernels across 1,000 configurations. CUDA-L2 systematically outperforms major matmul baselines to date, from the widely-used {\\it torch.matmul} to state-of-the-art Nvidia's closed-source libraries, i.e., {\\it cuBLAS}, {\\it cuBLASLt}. In offline mode, where kernels are executed consecutively without time intervals, CUDA-L2 yields +22.0\\% over {\\it torch.matmul} on average; +19.2\\% over {\\it cuBLAS} using the optimal layout configuration (normal-normal NN and transposed-normal TN); +16.8\\% over {\\it cuBLASLt-heuristic}, which queries {\\it cuBLASLt} library and selects the algorithm based on the heuristic's suggestion; and +11.4\\% over the most competitive {\\it cuBLASLt-AutoTuning} model, which selects the fastest algorithm from up to 100 candidates from {\\it cuBLASLt}'s suggestions. In server mode, where kernels are executed at random intervals simulating real-time inference, the speedups further increase to +28.7\\%, +26.0\\%, +22.4\\%, and +15.9\\% for {\\it torch.matmul}, {\\it cuBLAS}, {\\it cuBLASLt-heuristic}, and {\\it cuBLASLt-AutoTuning} respectively. CUDA-L2 shows that even the most performance-critical, heavily-optimized kernels like HGEMM can be improved through LLM-guided RL automation by systematically exploring configuration spaces at scales impractical for humans. Project and code can be found at github.com/deepreinforce-ai/CUDA-L2",
    "translation": "标题：CUDA-L2：通过强化学习超越cuBLAS性能的矩阵乘法优化系统\n\n摘要：本文提出CUDA-L2系统，该系统结合大型语言模型（LLM）与强化学习（RL），实现半精度通用矩阵乘法（HGEMM）CUDA核函数的自动优化。以CUDA执行速度为强化学习奖励，CUDA-L2在1,000种配置中自动优化HGEMM核函数。实验表明，CUDA-L2系统性地超越了当前主流矩阵乘法基准方案：从广泛使用的{\\it torch.matmul}到英伟达最新的闭源库（包括{\\it cuBLAS}和{\\it cuBLASLt}）。在离线模式下（核函数连续执行无时间间隔），CUDA-L2相比{\\it torch.matmul}平均提升22.0%；相比采用最优布局配置（正常-正常NN与转置-正常TN）的{\\it cuBLAS}提升19.2%；相比基于启发式建议从{\\it cuBLASLt}库选择算法的{\\it cuBLASLt-heuristic}提升16.8%；相比最具竞争力的{\\it cuBLASLt-AutoTuning}模型（从{\\it cuBLASLt}提供的多达100个候选算法中选择最优方案）提升11.4%。在模拟实时推理的服务器模式下（核函数随机间隔执行），加速效果进一步提升：相比{\\it torch.matmul}、{\\it cuBLAS}、{\\it cuBLASLt-heuristic}和{\\it cuBLASLt-AutoTuning}分别达到28.7%、26.0%、22.4%和15.9%的性能增益。CUDA-L2证明，即使是HGEMM这类高度优化、对性能极其敏感的核函数，也能通过LLM引导的强化学习自动化实现性能突破——该系统以人力难以企及的规模系统探索配置空间。项目与代码详见github.com/deepreinforce-ai/CUDA-L2。",
    "url": "https://huggingface.co/papers/2512.02551",
    "arxiv_url": "https://arxiv.org/abs/2512.02551"
  },
  {
    "title": "Does Hearing Help Seeing? Investigating Audio-Video Joint Denoising for Video Generation",
    "summary": "Recent audio-video generative systems suggest that coupling modalities benefits not only audio-video synchrony but also the video modality itself. We pose a fundamental question: Does audio-video joint denoising training improve video generation, even when we only care about video quality? To study this, we introduce a parameter-efficient Audio-Video Full DiT (AVFullDiT) architecture that leverages pre-trained text-to-video (T2V) and text-to-audio (T2A) modules for joint denoising. We train (i) a T2AV model with AVFullDiT and (ii) a T2V-only counterpart under identical settings. Our results provide the first systematic evidence that audio-video joint denoising can deliver more than synchrony. We observe consistent improvements on challenging subsets featuring large and object contact motions. We hypothesize that predicting audio acts as a privileged signal, encouraging the model to internalize causal relationships between visual events and their acoustic consequences (e.g., collision times impact sound), which in turn regularizes video dynamics. Our findings suggest that cross-modal co-training is a promising approach to developing stronger, more physically grounded world models. Code and dataset will be made publicly available.",
    "translation": "标题：听觉是否有助于视觉？视频生成中音视频联合去噪机制研究\n\n摘要：近期音视频生成系统的研究表明，多模态耦合不仅有助于提升音视频同步性，还能改善视频模态本身的生成质量。我们提出一个基础性问题：即使仅关注视频质量，音视频联合去噪训练是否能提升视频生成效果？为探究此问题，我们设计了一种参数高效的音视频全扩散变换器（AVFullDiT）架构，该架构利用预训练的文本到视频（T2V）与文本到音频（T2A）模块进行联合去噪训练。我们在相同实验设置下分别训练了：（1）采用AVFullDiT的T2AV模型；（2）仅使用视频模态的对照模型。实验结果首次系统性地证明，音视频联合去噪训练能带来超越同步性提升的额外增益。在包含大幅运动与物体接触动作的挑战性数据子集上，我们观察到模型性能的持续改善。我们假设音频预测作为一种特权信号，能够促使模型内化视觉事件与其声学后果之间的因果关系（例如碰撞时机对声音的影响），从而对视频动态特性产生正则化作用。本研究结果表明，跨模态协同训练是构建更强大、更符合物理规律的世界模型的有效途径。代码与数据集将公开提供。",
    "url": "https://huggingface.co/papers/2512.02457",
    "arxiv_url": "https://arxiv.org/abs/2512.02457"
  },
  {
    "title": "The Curious Case of Analogies: Investigating Analogical Reasoning in Large Language Models",
    "summary": "Analogical reasoning is at the core of human cognition, serving as an important foundation for a variety of intellectual activities. While prior work has shown that LLMs can represent task patterns and surface-level concepts, it remains unclear whether these models can encode high-level relational concepts and apply them to novel situations through structured comparisons. In this work, we explore this fundamental aspect using proportional and story analogies, and identify three key findings. First, LLMs effectively encode the underlying relationships between analogous entities; both attributive and relational information propagate through mid-upper layers in correct cases, whereas reasoning failures reflect missing relational information within these layers. Second, unlike humans, LLMs often struggle not only when relational information is missing, but also when attempting to apply it to new entities. In such cases, strategically patching hidden representations at critical token positions can facilitate information transfer to a certain extent. Lastly, successful analogical reasoning in LLMs is marked by strong structural alignment between analogous situations, whereas failures often reflect degraded or misplaced alignment. Overall, our findings reveal that LLMs exhibit emerging but limited capabilities in encoding and applying high-level relational concepts, highlighting both parallels and gaps with human cognition.",
    "translation": "标题：类比推理的奇特案例：探究大语言模型中的类比推理机制\n\n摘要：类比推理是人类认知的核心，为多种智力活动提供了重要基础。尽管已有研究表明大语言模型能够表征任务模式与表层概念，但这些模型能否编码高层次关系概念并通过结构化比较将其应用于新情境仍不明确。本研究通过比例类比与叙事类比探究这一基础问题，并发现三个关键结论：首先，大语言模型能有效编码类比实体间的深层关系——在正确推理案例中，属性信息与关系信息均通过中高层网络传播；而推理失败案例则反映出这些层级中关系信息的缺失。其次，与人类不同，大语言模型不仅在关系信息缺失时表现困难，在尝试将已有关系应用于新实体时也常遇障碍。在此类情境中，对关键标记位置的隐层表征进行策略性修补可在一定程度上促进信息迁移。最后，成功的类比推理以类比情境间强烈的结构对齐为特征，而推理失败往往伴随对齐结构的退化或错位。总体而言，本研究表明大语言模型在编码与应用高层次关系概念方面呈现出初现但有限的能力，既揭示了其与人类认知的共通之处，也凸显了二者间的显著差距。",
    "url": "https://huggingface.co/papers/2511.20344",
    "arxiv_url": "https://arxiv.org/abs/2511.20344"
  },
  {
    "title": "DiG-Flow: Discrepancy-Guided Flow Matching for Robust VLA Models",
    "summary": "Vision-Language-Action (VLA) models trained with flow matching have demonstrated impressive capabilities on robotic manipulation tasks. However, their performance often degrades under distribution shift and on complex multi-step tasks, suggesting that the learned representations may not robustly capture task-relevant semantics. We introduce DiG-Flow, a principled framework that enhances VLA robustness through geometric regularization. Our key insight is that the distributional discrepancy between observation and action embeddings provides a meaningful geometric signal: lower transport cost indicates compatible representations, while higher cost suggests potential misalignment. DiG-Flow computes a discrepancy measure between empirical distributions of observation and action embeddings, maps it to a modulation weight via a monotone function, and applies residual updates to the observation embeddings before flow matching. Crucially, this intervention operates at the representation level without modifying the flow matching path or target vector field. We provide theoretical guarantees showing that discrepancy-guided training provably decreases the training objective, and that guided inference refinement converges with contraction. Empirically, DiG-Flow integrates into existing VLA architectures with negligible overhead and consistently improves performance, with particularly pronounced gains on complex multi-step tasks and under limited training data.",
    "translation": "标题：DiG-Flow：基于差异引导流匹配的鲁棒视觉-语言-动作模型\n\n摘要：通过流匹配训练的视觉-语言-动作模型在机器人操作任务中展现出卓越的性能。然而，其表现常在分布偏移和复杂多步任务中出现退化，这表明所学表征可能未能稳健地捕捉任务相关语义。本文提出DiG-Flow，一个通过几何正则化增强VLA模型鲁棒性的原理性框架。我们的核心洞见在于：观测与动作嵌入之间的分布差异可提供有意义的几何信号——较低的传输成本表征兼容性，而较高成本则暗示潜在错位。DiG-Flow通过计算观测与动作嵌入经验分布间的差异度量，经单调函数映射为调制权重，进而在流匹配前对观测嵌入施加残差更新。关键的是，这种干预作用于表征层面，无需修改流匹配路径或目标向量场。我们提供了理论证明：差异引导训练可严格降低训练目标函数，且引导推理优化过程具有收缩收敛性。实证研究表明，DiG-Flow能以可忽略的开销集成至现有VLA架构，持续提升模型性能，在复杂多步任务及有限训练数据场景下增益尤为显著。",
    "url": "https://huggingface.co/papers/2512.01715",
    "arxiv_url": "https://arxiv.org/abs/2512.01715"
  },
  {
    "title": "RULER-Bench: Probing Rule-based Reasoning Abilities of Next-level Video Generation Models for Vision Foundation Intelligence",
    "summary": "Recent advances in video generation have enabled the synthesis of videos with strong temporal consistency and impressive visual quality, marking a crucial step toward vision foundation models. To evaluate these video generation models, existing benchmarks primarily focus on factors related to visual perception and understanding, like visual aesthetics, instruction adherence, and temporal coherence. However, the rule-based reasoning capabilities of video generation models remain largely unexplored. Although recent studies have carried out preliminary explorations into whether video models can serve as zero-shot learners, they still lack a fine-grained decomposition of reasoning capabilities and a comprehensive evaluation protocol. To address this gap, we introduce RULER-Bench, a benchmark designed to evaluate the reasoning ability of video generation models from the perspective of cognitive rules. Built upon two fundamental paradigms: text-to-video and image-to-video, RULER-Bench covers 40 representative tasks spanning six rule categories with 622 high-quality annotated instances. For the evaluation of each generated video, we construct a checklist covering four metrics and leverage GPT-o3 to assign scores to each question, achieving 85% alignment with human judgements. Extensive experiments show that the state-of-the-art model achieves only 48.87% on the rule coherence metric, highlighting significant room for improvement in the reasoning capability of next-level video models. We expect that the insight obtained from RULER-Bench will facilitate further development of reasoning-aware video generation, advancing video generation models toward vision foundation intelligence.",
    "translation": "标题：RULER-Bench：面向视觉基础智能的下一代视频生成模型规则推理能力评测基准\n\n摘要：近期视频生成技术的进展使得合成视频在时间连贯性和视觉质量方面表现突出，标志着向视觉基础模型迈出了关键一步。为评估这些视频生成模型，现有基准主要关注与视觉感知和理解相关的因素，如视觉美感、指令遵循性和时间一致性。然而，视频生成模型的规则推理能力在很大程度上尚未得到充分探索。尽管近期研究对视频模型能否作为零样本学习者进行了初步探索，但仍缺乏对推理能力的细粒度分解和全面的评估方案。为填补这一空白，我们提出了RULER-Bench，这是一个旨在从认知规则角度评估视频生成模型推理能力的基准。基于文本到视频和图像到视频两种基本范式，RULER-Bench涵盖了六大规则类别下的40个代表性任务，包含622个高质量标注实例。针对每个生成视频的评估，我们构建了涵盖四项指标的检查表，并利用GPT-4o为每个问题分配分数，实现了与人工判断85%的一致性。大量实验表明，当前最先进的模型在规则连贯性指标上仅达到48.87%，凸显了下一代视频模型在推理能力方面存在显著的提升空间。我们期望通过RULER-Bench获得的见解能够推动具备推理意识的视频生成的进一步发展，促进视频生成模型向视觉基础智能迈进。",
    "url": "https://huggingface.co/papers/2512.02622",
    "arxiv_url": "https://arxiv.org/abs/2512.02622"
  },
  {
    "title": "TRivia: Self-supervised Fine-tuning of Vision-Language Models for Table Recognition",
    "summary": "Table recognition (TR) aims to transform table images into semi-structured representations such as HTML or Markdown. As a core component of document parsing, TR has long relied on supervised learning, with recent efforts dominated by fine-tuning vision-language models (VLMs) using labeled data. While VLMs have brought TR to the next level, pushing performance further demands large-scale labeled data that is costly to obtain. Consequently, although proprietary models have continuously pushed the performance boundary, open-source models, often trained with limited resources and, in practice, the only viable option for many due to privacy regulations, still lag far behind. To bridge this gap, we introduce TRivia, a self-supervised fine-tuning method that enables pretrained VLMs to learn TR directly from unlabeled table images in the wild. Built upon Group Relative Policy Optimization, TRivia automatically identifies unlabeled samples that most effectively facilitate learning and eliminates the need for human annotations through a question-answering-based reward mechanism. An attention-guided module generates diverse questions for each table image, and the ability to interpret the recognition results and answer them correctly provides feedback to optimize the TR model. This closed-loop process allows the TR model to autonomously learn to recognize, structure, and reason over tables without labeled data. Leveraging this pipeline, we present TRivia-3B, an open-sourced, compact, and state-of-the-art TR model that surpasses existing systems (e.g., Gemini 2.5 Pro, MinerU2.5) on three popular benchmarks. Model and code are released at: https://github.com/opendatalab/TRivia",
    "translation": "标题：TRivia：基于自监督微调的视觉语言模型表格识别方法\n\n摘要：表格识别（TR）旨在将表格图像转换为HTML或Markdown等半结构化表示形式。作为文档解析的核心组成部分，表格识别长期依赖监督学习，近期研究主要通过标注数据微调视觉语言模型（VLMs）来实现突破。尽管视觉语言模型将表格识别性能提升至新高度，但进一步突破需要大规模标注数据，其获取成本高昂。因此，尽管专有模型不断突破性能边界，但受限于训练资源且因隐私法规成为实际应用中唯一可行选择的开源模型，其性能仍远落后于前沿水平。为弥合这一差距，我们提出TRivia——一种自监督微调方法，使预训练的视觉语言模型能够直接从无标注的真实场景表格图像中学习表格识别。该方法基于群体相对策略优化构建，可自动识别最能促进学习的无标注样本，并通过基于问答的奖励机制消除对人工标注的依赖。注意力引导模块为每个表格图像生成多样化问题，而模型对识别结果的解析能力与正确答案的匹配度将为优化表格识别模型提供反馈。这种闭环流程使得表格识别模型能够在无标注数据条件下，自主学习表格的识别、结构化与推理。基于此流程，我们推出TRivia-3B——一个开源、紧凑且性能领先的表格识别模型，在三个主流基准测试中超越现有系统（如Gemini 2.5 Pro、MinerU2.5）。模型与代码已发布于：https://github.com/opendatalab/TRivia",
    "url": "https://huggingface.co/papers/2512.01248",
    "arxiv_url": "https://arxiv.org/abs/2512.01248"
  },
  {
    "title": "MagicQuillV2: Precise and Interactive Image Editing with Layered Visual Cues",
    "summary": "We propose MagicQuill V2, a novel system that introduces a layered composition paradigm to generative image editing, bridging the gap between the semantic power of diffusion models and the granular control of traditional graphics software. While diffusion transformers excel at holistic generation, their use of singular, monolithic prompts fails to disentangle distinct user intentions for content, position, and appearance. To overcome this, our method deconstructs creative intent into a stack of controllable visual cues: a content layer for what to create, a spatial layer for where to place it, a structural layer for how it is shaped, and a color layer for its palette. Our technical contributions include a specialized data generation pipeline for context-aware content integration, a unified control module to process all visual cues, and a fine-tuned spatial branch for precise local editing, including object removal. Extensive experiments validate that this layered approach effectively resolves the user intention gap, granting creators direct, intuitive control over the generative process.",
    "translation": "标题：MagicQuillV2：基于分层视觉线索的精确交互式图像编辑\n\n摘要：我们提出MagicQuill V2，这是一个新颖的系统，它将分层组合范式引入生成式图像编辑，弥合了扩散模型的语义生成能力与传统图形软件的精细控制之间的鸿沟。虽然扩散变换器擅长整体生成，但其使用的单一、整体的提示词无法区分用户在内容、位置和外观方面的不同意图。为克服此限制，我们的方法将创作意图解构为一组可控的视觉线索：内容层决定创建什么，空间层决定放置位置，结构层决定形状构成，色彩层决定调色方案。我们的技术贡献包括：用于上下文感知内容集成的专用数据生成流程、处理所有视觉线索的统一控制模块，以及用于精确局部编辑（包括对象移除）的微调空间分支。大量实验验证表明，这种分层方法有效解决了用户意图鸿沟问题，使创作者能够对生成过程进行直接、直观的控制。",
    "url": "https://huggingface.co/papers/2512.03046",
    "arxiv_url": "https://arxiv.org/abs/2512.03046"
  },
  {
    "title": "Revisiting the Necessity of Lengthy Chain-of-Thought in Vision-centric Reasoning Generalization",
    "summary": "We study how different Chain-of-Thought (CoT) designs affect the acquisition of the generalizable visual reasoning ability in vision-language models (VLMs). While CoT data, especially long or visual CoT such as \"think with image\", has been widely used to supervise intermediate reasoning, it remains unclear why specific CoT designs help and which ones truly support generalizable reasoning. To systematically evaluate this, we focus on a controlled maze-solving benchmark where reasoning rules are fully visual, difficulty can be tuned by grid size, and all the intermediate steps can be automatically generated. Using Qwen2.5-VL-7B under a standard SFT-then-RL pipeline, we compare three representative CoT formats: Language CoT, Grounding CoT (with spatial coordinate trajectories), and Visual CoT (with image manipulations). Our experiments reveal that visual and longer CoT mainly accelerate convergence but do not lift the final performance ceiling; concise CoT containing only essential grounding steps outperforms longer traces; and, strikingly, CoT retaining only the minimal grounding results generalizes best across different maze sizes. We further validate these insights on other vision-centric tasks. These findings highlight a \"short is long\" effect and provide practical guidance for constructing more generalizable SFT datasets for visual reasoning.",
    "translation": "标题：重新审视视觉中心推理泛化中长链思维的必要性\n\n摘要：本研究探讨不同链式思维设计如何影响视觉语言模型获取可泛化的视觉推理能力。尽管链式思维数据（尤其是长链或视觉链式思维，如“基于图像的思考”）已被广泛用于监督中间推理过程，但特定设计为何有效、何种设计真正支持可泛化推理仍不明确。为系统评估该问题，我们采用受控迷宫求解基准任务，其中推理规则完全基于视觉，难度可通过网格尺寸调节，且所有中间步骤均可自动生成。在标准监督微调-强化学习流程下，使用Qwen2.5-VL-7B模型比较了三种代表性链式思维格式：语言链式思维、具象化链式思维（含空间坐标轨迹）和视觉链式思维（含图像操作）。实验表明：视觉化与长链式思维主要加速收敛过程，但未提升最终性能上限；仅包含必要具象化步骤的简洁链式思维优于长链追踪；值得注意的是，仅保留最简具象化信息的链式思维在不同迷宫尺寸中展现出最佳泛化能力。我们进一步在其他视觉中心任务中验证了这些发现。这些结果揭示了“短即长”效应，并为构建更具泛化能力的视觉推理监督微调数据集提供了实践指导。",
    "url": "https://huggingface.co/papers/2511.22586",
    "arxiv_url": "https://arxiv.org/abs/2511.22586"
  },
  {
    "title": "PAI-Bench: A Comprehensive Benchmark For Physical AI",
    "summary": "Physical AI aims to develop models that can perceive and predict real-world dynamics; yet, the extent to which current multi-modal large language models and video generative models support these abilities is insufficiently understood. We introduce Physical AI Bench (PAI-Bench), a unified and comprehensive benchmark that evaluates perception and prediction capabilities across video generation, conditional video generation, and video understanding, comprising 2,808 real-world cases with task-aligned metrics designed to capture physical plausibility and domain-specific reasoning. Our study provides a systematic assessment of recent models and shows that video generative models, despite strong visual fidelity, often struggle to maintain physically coherent dynamics, while multi-modal large language models exhibit limited performance in forecasting and causal interpretation. These observations suggest that current systems are still at an early stage in handling the perceptual and predictive demands of Physical AI. In summary, PAI-Bench establishes a realistic foundation for evaluating Physical AI and highlights key gaps that future systems must address.",
    "translation": "标题：PAI-Bench：面向物理人工智能的综合基准测试\n\n摘要：物理人工智能旨在开发能够感知和预测现实世界动态的模型；然而，当前多模态大语言模型与视频生成模型对这些能力的支持程度尚未得到充分理解。我们提出了物理人工智能基准测试（PAI-Bench），这是一个统一且全面的基准测试体系，用于评估模型在视频生成、条件视频生成及视频理解任务中的感知与预测能力。该基准包含2,808个真实世界案例，并采用任务对齐的评估指标，旨在捕捉物理合理性与领域特定推理能力。本研究对近期主流模型进行了系统性评估，结果表明：视频生成模型尽管在视觉保真度上表现突出，却常难以保持物理连贯的动态过程；而多模态大语言模型在动态预测与因果解释方面仍存在明显局限。这些发现表明，当前系统在应对物理人工智能的感知与预测需求方面尚处于早期发展阶段。综上所述，PAI-Bench为评估物理人工智能建立了贴近现实的基准框架，并揭示了未来系统需着力突破的关键能力缺口。",
    "url": "https://huggingface.co/papers/2512.01989",
    "arxiv_url": "https://arxiv.org/abs/2512.01989"
  },
  {
    "title": "Video4Spatial: Towards Visuospatial Intelligence with Context-Guided Video Generation",
    "summary": "We investigate whether video generative models can exhibit visuospatial intelligence, a capability central to human cognition, using only visual data. To this end, we present Video4Spatial, a framework showing that video diffusion models conditioned solely on video-based scene context can perform complex spatial tasks. We validate on two tasks: scene navigation - following camera-pose instructions while remaining consistent with 3D geometry of the scene, and object grounding - which requires semantic localization, instruction following, and planning. Both tasks use video-only inputs, without auxiliary modalities such as depth or poses. With simple yet effective design choices in the framework and data curation, Video4Spatial demonstrates strong spatial understanding from video context: it plans navigation and grounds target objects end-to-end, follows camera-pose instructions while maintaining spatial consistency, and generalizes to long contexts and out-of-domain environments. Taken together, these results advance video generative models toward general visuospatial reasoning.",
    "translation": "标题：Video4Spatial：基于上下文引导视频生成的视觉空间智能探索\n\n摘要：本研究探讨视频生成模型能否仅凭视觉数据展现出人类认知核心能力之一的视觉空间智能。为此，我们提出Video4Spatial框架，证明仅以视频场景上下文为条件的视频扩散模型能够执行复杂空间任务。我们在两项任务上进行了验证：场景导航——在遵循相机位姿指令的同时保持与场景三维几何结构的一致性；以及物体定位——需要语义定位、指令跟随与路径规划。两项任务均仅使用视频输入，无需深度或位姿等辅助模态。通过框架设计与数据构建中简洁有效的策略，Video4Spatial展现出基于视频上下文的强大空间理解能力：能够端到端规划导航路径并定位目标物体，在遵循相机位姿指令的同时保持空间一致性，并能泛化至长序列上下文及域外环境。这些成果共同推动了视频生成模型向通用视觉空间推理能力的发展。",
    "url": "https://huggingface.co/papers/2512.03040",
    "arxiv_url": "https://arxiv.org/abs/2512.03040"
  },
  {
    "title": "YingVideo-MV: Music-Driven Multi-Stage Video Generation",
    "summary": "While diffusion model for audio-driven avatar video generation have achieved notable process in synthesizing long sequences with natural audio-visual synchronization and identity consistency, the generation of music-performance videos with camera motions remains largely unexplored. We present YingVideo-MV, the first cascaded framework for music-driven long-video generation. Our approach integrates audio semantic analysis, an interpretable shot planning module (MV-Director), temporal-aware diffusion Transformer architectures, and long-sequence consistency modeling to enable automatic synthesis of high-quality music performance videos from audio signals. We construct a large-scale Music-in-the-Wild Dataset by collecting web data to support the achievement of diverse, high-quality results. Observing that existing long-video generation methods lack explicit camera motion control, we introduce a camera adapter module that embeds camera poses into latent noise. To enhance continulity between clips during long-sequence inference, we further propose a time-aware dynamic window range strategy that adaptively adjust denoising ranges based on audio embedding. Comprehensive benchmark tests demonstrate that YingVideo-MV achieves outstanding performance in generating coherent and expressive music videos, and enables precise music-motion-camera synchronization. More videos are available in our project page: https://giantailab.github.io/YingVideo-MV/ .",
    "translation": "标题：YingVideo-MV：音乐驱动的多阶段视频生成\n\n摘要：尽管基于扩散模型的音频驱动虚拟形象视频生成在合成具有自然音画同步与身份一致性的长序列方面已取得显著进展，但包含摄像机运动的音乐表演视频生成领域仍鲜有探索。本文提出YingVideo-MV——首个面向音乐驱动的长视频生成的级联框架。该方法通过整合音频语义分析、可解释的镜头规划模块（MV-Director）、时序感知的扩散Transformer架构以及长序列一致性建模，实现了从音频信号自动合成高质量音乐表演视频。我们通过收集网络数据构建了大规模真实场景音乐数据集，以支撑多样化高质量结果的生成。针对现有长视频生成方法缺乏显式摄像机运动控制的问题，我们设计了摄像机适配模块，将摄像机位姿嵌入隐空间噪声。为增强长序列推理中片段间的连续性，进一步提出基于音频嵌入自适应调整去噪范围的时序感知动态窗口策略。综合基准测试表明，YingVideo-MV在生成连贯且富有表现力的音乐视频方面表现优异，并能实现精确的音乐-动作-摄像机同步。更多视频请访问项目页面：https://giantailab.github.io/YingVideo-MV/。",
    "url": "https://huggingface.co/papers/2512.02492",
    "arxiv_url": "https://arxiv.org/abs/2512.02492"
  },
  {
    "title": "SimWorld: An Open-ended Realistic Simulator for Autonomous Agents in Physical and Social Worlds",
    "summary": "While LLM/VLM-powered AI agents have advanced rapidly in math, coding, and computer use, their applications in complex physical and social environments remain challenging. Building agents that can survive and thrive in the real world (for example, by autonomously earning income or running a business) requires massive-scale interaction, reasoning, training, and evaluation across diverse embodied scenarios. However, existing world simulators for such development fall short: they often rely on limited hand-crafted environments, simulate simplified game-like physics and social rules, and lack native support for LLM/VLM agents. We introduce SimWorld, a new simulator built on Unreal Engine 5, designed for developing and evaluating LLM/VLM agents in rich, real-world-like settings. SimWorld offers three core capabilities: (1) realistic, open-ended world simulation, including accurate physical and social dynamics and language-driven procedural environment generation; (2) a rich interface for LLM/VLM agents, with multimodal world inputs and open-vocabulary actions at varying levels of abstraction; and (3) diverse and extensible physical and social reasoning scenarios that are easily customizable by users. We demonstrate SimWorld by deploying frontier LLM agents (e.g., GPT-4o, Gemini-2.5-Flash, Claude-3.5, and DeepSeek-Prover-V2) on long-horizon multi-agent delivery tasks involving strategic cooperation and competition. The results reveal distinct reasoning patterns and limitations across models. We open-source SimWorld and hope it becomes a foundational platform for advancing real-world agent intelligence across disciplines: https://simworld.org.",
    "translation": "标题：SimWorld：面向物理与社会世界中自主智能体的开放式真实模拟器\n\n摘要：尽管基于大语言模型/视觉语言模型的人工智能体在数学、编程及计算机应用领域发展迅速，其在复杂物理与社会环境中的应用仍面临挑战。构建能够在现实世界中生存与发展（例如通过自主创收或运营企业）的智能体，需要在大规模具身化场景中进行交互、推理、训练与评估。然而，现有支持此类开发的世界模拟器存在明显不足：它们通常依赖有限的手工构建环境，模拟简化的游戏式物理与社会规则，且缺乏对大语言模型/视觉语言模型智能体的原生支持。本文提出SimWorld——一个基于虚幻引擎5构建的新型模拟器，专为在丰富、类真实世界的场景中开发与评估大语言模型/视觉语言模型智能体而设计。SimWorld具备三项核心能力：（1）真实、开放的世界模拟，包括精确的物理与社会动态机制，以及基于语言的程序化环境生成；（2）面向大语言模型/视觉语言模型智能体的丰富交互接口，支持多模态世界信息输入与多抽象层级的开放词汇动作指令；（3）多样化、可扩展的物理与社会推理场景，用户可便捷进行定制化配置。我们通过部署前沿大语言模型智能体（如GPT-4o、Gemini-2.5-Flash、Claude-3.5及DeepSeek-Prover-V2）在涉及战略合作与竞争的长期多智能体配送任务中验证了SimWorld的功能。实验结果揭示了不同模型间显著的推理模式差异与局限性。我们将SimWorld开源发布，期待其成为推动跨学科现实世界智能体智能发展的基础平台：https://simworld.org。",
    "url": "https://huggingface.co/papers/2512.01078",
    "arxiv_url": "https://arxiv.org/abs/2512.01078"
  },
  {
    "title": "SwiftVLA: Unlocking Spatiotemporal Dynamics for Lightweight VLA Models at Minimal Overhead",
    "summary": "Vision-Language-Action (VLA) models built on pretrained Vision-Language Models (VLMs) show strong potential but are limited in practicality due to their large parameter counts. To mitigate this issue, using a lightweight VLM has been explored, but it compromises spatiotemporal reasoning. Although some methods suggest that incorporating additional 3D inputs can help, they usually rely on large VLMs to fuse 3D and 2D inputs and still lack temporal understanding. Therefore, we propose SwiftVLA, an architecture that enhances a compact model with 4D understanding while preserving design efficiency. Specifically, our approach features a pretrained 4D visual geometry transformer with a temporal cache that extracts 4D features from 2D images. Then, to enhance the VLM's ability to exploit both 2D images and 4D features, we introduce Fusion Tokens, a set of learnable tokens trained with a future prediction objective to generate unified representations for action generation. Finally, we introduce a mask-and-reconstruct strategy that masks 4D inputs to the VLM and trains the VLA to reconstruct them, enabling the VLM to learn effective 4D representations and allowing the 4D branch to be dropped at inference with minimal performance loss. Experiments in real and simulated environments show that SwiftVLA outperforms lightweight baselines and rivals VLAs up to 7 times larger, achieving comparable performance on edge devices while being 18 times faster and reducing memory footprint by 12 times.",
    "translation": "标题：SwiftVLA：以最小开销解锁轻量级视觉-语言-动作模型的时空动态理解\n\n摘要：基于预训练视觉-语言模型构建的视觉-语言-动作模型展现出强大潜力，但其庞大的参数量限制了实际应用。为缓解此问题，现有研究尝试采用轻量级视觉-语言模型，但这会牺牲时空推理能力。尽管有方法提出引入额外三维输入可改善性能，但这些方案通常依赖大型视觉-语言模型融合三维与二维输入，且仍缺乏时序理解能力。为此，我们提出SwiftVLA架构，在保持设计效率的同时为紧凑模型赋予四维理解能力。具体而言，本方法采用预训练的时序缓存四维视觉几何变换器，从二维图像中提取四维特征；随后，为增强视觉-语言模型协同利用二维图像与四维特征的能力，我们引入融合令牌——一组以未来预测为目标训练的可学习令牌，用于生成动作合成的统一表征；最后，我们设计掩码重构策略：通过掩码四维输入并训练视觉-语言-动作模型进行重构，使视觉-语言模型能学习有效的四维表征，进而在推理阶段可舍弃四维分支且性能损失最小。真实环境与仿真实验表明，SwiftVLA在边缘设备上不仅超越轻量级基线模型，其性能更可媲美参数量达7倍的视觉-语言-动作模型，在实现相当性能的同时推理速度提升18倍，内存占用减少12倍。",
    "url": "https://huggingface.co/papers/2512.00903",
    "arxiv_url": "https://arxiv.org/abs/2512.00903"
  },
  {
    "title": "Ovis-Image Technical Report",
    "summary": "We introduce Ovis-Image, a 7B text-to-image model specifically optimized for high-quality text rendering, designed to operate efficiently under stringent computational constraints. Built upon our previous Ovis-U1 framework, Ovis-Image integrates a diffusion-based visual decoder with the stronger Ovis 2.5 multimodal backbone, leveraging a text-centric training pipeline that combines large-scale pre-training with carefully tailored post-training refinements. Despite its compact architecture, Ovis-Image achieves text rendering performance on par with significantly larger open models such as Qwen-Image and approaches closed-source systems like Seedream and GPT4o. Crucially, the model remains deployable on a single high-end GPU with moderate memory, narrowing the gap between frontier-level text rendering and practical deployment. Our results indicate that combining a strong multimodal backbone with a carefully designed, text-focused training recipe is sufficient to achieve reliable bilingual text rendering without resorting to oversized or proprietary models.",
    "translation": "标题：Ovis-Image技术报告\n\n摘要：本文介绍Ovis-Image——一个专门针对高质量文本渲染优化的70亿参数文生图模型，其设计目标是在严格的计算限制下实现高效运行。该模型基于我们先前提出的Ovis-U1框架构建，将基于扩散的视觉解码器与更强大的Ovis 2.5多模态骨干网络相结合，采用以文本为中心的训练流程，融合了大规模预训练与精细调整的后训练优化策略。尽管采用紧凑架构，Ovis-Image在文本渲染性能上已达到Qwen-Image等显著更大规模开源模型的水平，并接近Seedream、GPT4o等闭源系统。关键优势在于，该模型仅需单个高端GPU与适中内存即可部署，显著缩小了前沿文本渲染能力与实际应用部署之间的鸿沟。实验结果表明：通过将强大的多模态骨干网络与精心设计的文本导向训练方案相结合，无需依赖超大参数规模或专有模型，即可实现可靠的双语文本渲染能力。",
    "url": "https://huggingface.co/papers/2511.22982",
    "arxiv_url": "https://arxiv.org/abs/2511.22982"
  },
  {
    "title": "GUI Exploration Lab: Enhancing Screen Navigation in Agents via Multi-Turn Reinforcement Learning",
    "summary": "With the rapid development of Large Vision Language Models, the focus of Graphical User Interface (GUI) agent tasks shifts from single-screen tasks to complex screen navigation challenges. However, real-world GUI environments, such as PC software and mobile Apps, are often complex and proprietary, making it difficult to obtain the comprehensive environment information needed for agent training and evaluation. This limitation hinders systematic investigation and benchmarking of agent navigation capabilities. To address this limitation, we introduce GUI Exploration Lab, a simulation environment engine for GUI agent navigation research that enables flexible definition and composition of screens, icons, and navigation graphs, while providing full access to environment information for comprehensive agent training and evaluation. Through extensive experiments, we find that supervised fine-tuning enables effective memorization of fundamental knowledge, serving as a crucial foundation for subsequent training. Building on this, single-turn reinforcement learning further enhances generalization to unseen scenarios. Finally, multi-turn reinforcement learning encourages the development of exploration strategies through interactive trial and error, leading to further improvements in screen navigation performance. We validate our methods on both static and interactive benchmarks, demonstrating that our findings generalize effectively to real-world scenarios. These findings demonstrate the advantages of reinforcement learning approaches in GUI navigation and offer practical guidance for building more capable and generalizable GUI agents.",
    "translation": "标题：GUI探索实验室：通过多轮强化学习增强智能体界面导航能力\n\n摘要：随着大视觉语言模型的快速发展，图形用户界面智能体任务的重点已从单屏幕任务转向复杂的屏幕导航挑战。然而，现实中的图形用户界面环境（如电脑软件和移动应用）通常结构复杂且具有专有性，难以获取智能体训练与评估所需的完整环境信息。这一局限阻碍了对智能体导航能力的系统性研究与基准测试。为突破此限制，我们提出了GUI探索实验室——一个面向图形用户界面智能体导航研究的仿真环境引擎。该引擎支持灵活定义与组合屏幕、图标及导航图谱，同时提供完整的环境信息访问，以实现全面的智能体训练与评估。通过大量实验发现，监督微调能够有效促进基础知识的记忆，为后续训练奠定关键基础。在此基础上，单轮强化学习可进一步提升对未见过场景的泛化能力。最终，多轮强化学习通过交互式试错过程鼓励探索策略的形成，从而显著提高屏幕导航性能。我们在静态与交互式基准测试中验证了所提方法的有效性，证明研究结论能够很好地推广至实际应用场景。这些发现彰显了强化学习方法在图形用户界面导航中的优势，并为构建更具能力与泛化性的图形用户界面智能体提供了实践指导。",
    "url": "https://huggingface.co/papers/2512.02423",
    "arxiv_url": "https://arxiv.org/abs/2512.02423"
  },
  {
    "title": "FlashVGGT: Efficient and Scalable Visual Geometry Transformers with Compressed Descriptor Attention",
    "summary": "3D reconstruction from multi-view images is a core challenge in computer vision. Recently, feed-forward methods have emerged as efficient and robust alternatives to traditional per-scene optimization techniques. Among them, state-of-the-art models like the Visual Geometry Grounding Transformer (VGGT) leverage full self-attention over all image tokens to capture global relationships. However, this approach suffers from poor scalability due to the quadratic complexity of self-attention and the large number of tokens generated in long image sequences. In this work, we introduce FlashVGGT, an efficient alternative that addresses this bottleneck through a descriptor-based attention mechanism. Instead of applying dense global attention across all tokens, FlashVGGT compresses spatial information from each frame into a compact set of descriptor tokens. Global attention is then computed as cross-attention between the full set of image tokens and this smaller descriptor set, significantly reducing computational overhead. Moreover, the compactness of the descriptors enables online inference over long sequences via a chunk-recursive mechanism that reuses cached descriptors from previous chunks. Experimental results show that FlashVGGT achieves reconstruction accuracy competitive with VGGT while reducing inference time to just 9.3% of VGGT for 1,000 images, and scaling efficiently to sequences exceeding 3,000 images. Our project page is available at https://wzpscott.github.io/flashvggt_page/.",
    "translation": "标题：FlashVGGT：基于压缩描述符注意力机制的高效可扩展视觉几何变换器\n\n摘要：基于多视图图像的3D重建是计算机视觉领域的核心挑战。近年来，前馈方法已成为传统逐场景优化技术的高效且鲁棒的替代方案。其中，视觉几何基础变换器（VGGT）等先进模型通过对所有图像标记进行完全自注意力计算来捕捉全局关系。然而，由于自注意力的二次复杂度以及长图像序列生成的大量标记，该方法存在可扩展性不足的问题。本研究提出FlashVGGT，这是一种基于描述符注意力机制的高效替代方案，旨在解决这一瓶颈。FlashVGGT不再对所有标记进行密集的全局注意力计算，而是将每帧图像的空间信息压缩为一组紧凑的描述符标记。随后通过完整图像标记集与压缩描述符集之间的交叉注意力实现全局关系建模，从而显著降低计算开销。此外，描述符的紧凑性使其能够通过分块递归机制实现长序列在线推理，该机制可复用历史分块的缓存描述符。实验结果表明，FlashVGGT在重建精度上与VGGT相当，同时对1000张图像的推理时间降至VGGT的9.3%，并能高效扩展至超过3000张图像的长序列处理。项目页面详见：https://wzpscott.github.io/flashvggt_page/。",
    "url": "https://huggingface.co/papers/2512.01540",
    "arxiv_url": "https://arxiv.org/abs/2512.01540"
  },
  {
    "title": "BlockVid: Block Diffusion for High-Quality and Consistent Minute-Long Video Generation",
    "summary": "Generating minute-long videos is a critical step toward developing world models, providing a foundation for realistic extended scenes and advanced AI simulators. The emerging semi-autoregressive (block diffusion) paradigm integrates the strengths of diffusion and autoregressive models, enabling arbitrary-length video generation and improving inference efficiency through KV caching and parallel sampling. However, it yet faces two enduring challenges: (i) KV-cache-induced long-horizon error accumulation, and (ii) the lack of fine-grained long-video benchmarks and coherence-aware metrics. To overcome these limitations, we propose BlockVid, a novel block diffusion framework equipped with semantic-aware sparse KV cache, an effective training strategy called Block Forcing, and dedicated chunk-wise noise scheduling and shuffling to reduce error propagation and enhance temporal consistency. We further introduce LV-Bench, a fine-grained benchmark for minute-long videos, complete with new metrics evaluating long-range coherence. Extensive experiments on VBench and LV-Bench demonstrate that BlockVid consistently outperforms existing methods in generating high-quality, coherent minute-long videos. In particular, it achieves a 22.2% improvement on VDE Subject and a 19.4% improvement on VDE Clarity in LV-Bench over the state of the art approaches. Project website: https://ziplab.co/BlockVid. Inferix (Code): https://github.com/alibaba-damo-academy/Inferix.",
    "translation": "标题：BlockVid：基于块扩散的高质量一致性分钟级视频生成方法\n\n摘要：生成长达分钟级别的视频是构建世界模型的关键一步，可为生成逼真扩展场景和先进人工智能模拟器奠定基础。新兴的半自回归（块扩散）范式融合了扩散模型与自回归模型的优势，通过KV缓存与并行采样实现了任意长度视频生成并提升了推理效率。然而，该方法仍面临两大持续存在的挑战：（一）KV缓存引发的长时序误差累积问题；（二）缺乏细粒度长视频基准测试体系及连贯性感知评估指标。为突破这些局限，本文提出BlockVid——一种创新的块扩散框架。该框架配备语义感知稀疏KV缓存机制，采用名为“块强制训练”的有效训练策略，并设计专用的分块噪声调度与重排方案，以降低误差传播并增强时序一致性。我们进一步构建了LV-Bench细粒度分钟级视频基准测试集，配套提出评估长程连贯性的新型指标体系。在VBench与LV-Bench上的大量实验表明，BlockVid在生成高质量、高连贯性分钟级视频方面持续优于现有方法。具体而言，在LV-Bench评估中，其VDE主体指标较现有最优方法提升22.2%，VDE清晰度指标提升19.4%。项目网站：https://ziplab.co/BlockVid。代码仓库：https://github.com/alibaba-damo-academy/Inferix。",
    "url": "https://huggingface.co/papers/2511.22973",
    "arxiv_url": "https://arxiv.org/abs/2511.22973"
  },
  {
    "title": "C^2DLM: Causal Concept-Guided Diffusion Large Language Models",
    "summary": "Autoregressive (AR) language models and Diffusion Language Models (DLMs) constitute the two principal paradigms of large language models. However, both paradigms suffer from insufficient reasoning capabilities. Human reasoning inherently relies on causal knowledge and thought, which are reflected in natural language. But in the AR paradigm, language is modeled as next token prediction (a strictly left-to-right, token-by-token order), whereas natural language itself exhibits more flexible causal structures. In the DLM paradigm, the attention mechanism is fully connected, which entirely disregards causal order. To fill this gap, we propose a \\textbf{C}ausal \\textbf{C}oncept-Guided \\textbf{D}iffusion \\textbf{L}anguage \\textbf{M}odel (C^2DLM). Starting from DLM's fully connected attention, C^2DLM first obtains a concept-level causal graph from the teacher model, and then explicitly guides attention to learn causal relationships between concepts. By focusing on causal relationships and avoiding interference from difficult subgoals involving causal inversion, C^2DLM improves 12\\% with about 3.2 times training speedup in the COT-OrderPerturb task, and achieves an average gain of 1.31\\% across six downstream reasoning tasks. More details in the repository ~https://github.com/Kairong-Han/C-2-DLM{here}.",
    "translation": "标题：C^2DLM：因果概念引导的扩散大语言模型\n\n摘要：自回归语言模型与扩散语言模型构成了大语言模型的两大主要范式。然而，这两种范式均存在推理能力不足的问题。人类的推理本质上依赖于因果知识与思维，这些在自然语言中有所体现。但在自回归范式中，语言被建模为下一个词预测（严格遵循从左到右、逐词生成的顺序），而自然语言本身展现出更为灵活的因果结构。在扩散范式下，注意力机制采用全连接方式，完全忽略了因果顺序。为填补这一空白，本文提出一种**因**果**概**念引导的**扩**散**语**言**模**型（C^2DLM）。该模型从扩散语言模型的全连接注意力机制出发，首先从教师模型中获取概念级因果图，进而显式引导注意力学习概念间的因果关系。通过聚焦于因果关系，并避免涉及因果反转的困难子目标带来的干扰，C^2DLM在COT-OrderPerturb任务中实现了12%的性能提升与约3.2倍的训练加速，并在六项下游推理任务中平均获得1.31%的性能增益。更多细节请参见代码仓库：https://github.com/Kairong-Han/C-2-DLM。",
    "url": "https://huggingface.co/papers/2511.22146",
    "arxiv_url": "https://arxiv.org/abs/2511.22146"
  },
  {
    "title": "Beyond Description: Cognitively Benchmarking Fine-Grained Action for Embodied Agents",
    "summary": "Multimodal Large Language Models (MLLMs) show promising results as decision-making engines for embodied agents operating in complex, physical environments. However, existing benchmarks often prioritize high-level planning or spatial reasoning, leaving the fine-grained action intelligence required for embodied physical interaction underexplored. To address this gap, we introduce CFG-Bench, a new benchmark designed to systematically evaluate this crucial capability. CFG-Bench consists of 1,368 curated videos paired with 19,562 three-modalities question-answer pairs targeting four cognitive abilities: 1) Physical Interaction, 2) Temporal-Causal Relation, 3) Intentional Understanding, and 4) Evaluative Judgment. Together, these dimensions provide a systematic framework for assessing a model's ability to translate visual observations into actionable knowledge, moving beyond mere surface-level recognition. Our comprehensive evaluation on CFG-Bench reveals that leading MLLMs struggle to produce detailed instructions for physical interactions and exhibit profound limitations in the higher-order reasoning of intention and evaluation. Moreover, supervised fine-tuning (SFT) on our data demonstrates that teaching an MLLMs to articulate fine-grained actions directly translates to significant performance gains on established embodied benchmarks. Our analysis highlights these limitations and offers insights for developing more capable and grounded embodied agents.",
    "translation": "标题：超越描述：具身智能体细粒度动作的认知基准测试\n\n摘要：多模态大语言模型作为在复杂物理环境中运行的具身智能体的决策引擎，已展现出广阔前景。然而，现有基准测试往往侧重于高层规划或空间推理，对具身物理交互所需的细粒度动作智能探索不足。为填补这一空白，我们提出了CFG-Bench——一个旨在系统评估这一关键能力的新型基准测试。该基准包含1,368个精选视频及与之匹配的19,562组三模态问答对，聚焦四大认知能力维度：1）物理交互，2）时序因果关联，3）意图理解，4）价值判断。这些维度共同构成了评估模型将视觉观察转化为可执行知识能力的系统框架，超越了表层识别范畴。我们在CFG-Bench上的综合评估表明，当前领先的多模态大语言模型在生成物理交互的详细指令方面存在困难，在意图理解与价值判断等高阶推理能力上表现出明显局限。此外，基于本数据集的监督微调实验证明，通过训练多模态大语言模型描述细粒度动作，能直接转化为其在现有具身基准测试上的显著性能提升。本研究不仅揭示了当前模型的局限性，更为开发更具能力且贴合现实的具身智能体提供了重要洞见。",
    "url": "https://huggingface.co/papers/2511.18685",
    "arxiv_url": "https://arxiv.org/abs/2511.18685"
  },
  {
    "title": "In-Context Sync-LoRA for Portrait Video Editing",
    "summary": "Editing portrait videos is a challenging task that requires flexible yet precise control over a wide range of modifications, such as appearance changes, expression edits, or the addition of objects. The key difficulty lies in preserving the subject's original temporal behavior, demanding that every edited frame remains precisely synchronized with the corresponding source frame. We present Sync-LoRA, a method for editing portrait videos that achieves high-quality visual modifications while maintaining frame-accurate synchronization and identity consistency. Our approach uses an image-to-video diffusion model, where the edit is defined by modifying the first frame and then propagated to the entire sequence. To enable accurate synchronization, we train an in-context LoRA using paired videos that depict identical motion trajectories but differ in appearance. These pairs are automatically generated and curated through a synchronization-based filtering process that selects only the most temporally aligned examples for training. This training setup teaches the model to combine motion cues from the source video with the visual changes introduced in the edited first frame. Trained on a compact, highly curated set of synchronized human portraits, Sync-LoRA generalizes to unseen identities and diverse edits (e.g., modifying appearance, adding objects, or changing backgrounds), robustly handling variations in pose and expression. Our results demonstrate high visual fidelity and strong temporal coherence, achieving a robust balance between edit fidelity and precise motion preservation.",
    "translation": "标题：面向人像视频编辑的上下文同步LoRA方法\n\n摘要：人像视频编辑是一项具有挑战性的任务，需要对多种修改（如外观变化、表情调整或对象添加）实现灵活而精确的控制。其核心难点在于保持主体原有的时序行为，要求每一帧编辑后的画面与对应原始帧保持精确同步。本文提出Sync-LoRA方法，该人像视频编辑技术能够在实现高质量视觉修改的同时，保持帧级精确同步与身份一致性。我们的方法基于图像到视频的扩散模型，通过修改首帧定义编辑内容，并将其传播至整个序列。为实现精确同步，我们使用成对视频训练上下文LoRA模型，这些视频具有相同的运动轨迹但外观不同。这些训练对通过基于同步性的筛选流程自动生成和筛选，仅选择时序对齐程度最高的样本进行训练。该训练机制使模型能够将源视频的运动线索与编辑首帧引入的视觉变化相结合。通过在精筛选的同步人像数据集上进行训练，Sync-LoRA能够泛化至未见过的身份和多样化编辑任务（如修改外观、添加对象或更换背景），并稳健处理姿态与表情的变化。实验结果表明，该方法在视觉保真度和时序连贯性方面表现优异，实现了编辑保真度与精确运动保持之间的稳健平衡。",
    "url": "https://huggingface.co/papers/2512.03013",
    "arxiv_url": "https://arxiv.org/abs/2512.03013"
  },
  {
    "title": "Benchmarking Scientific Understanding and Reasoning for Video Generation using VideoScience-Bench",
    "summary": "The next frontier for video generation lies in developing models capable of zero-shot reasoning, where understanding real-world scientific laws is crucial for accurate physical outcome modeling under diverse conditions. However, existing video benchmarks are physical commonsense-based, offering limited insight into video models' scientific reasoning capability. We introduce VideoScience-Bench, a benchmark designed to evaluate undergraduate-level scientific understanding in video models. Each prompt encodes a composite scientific scenario that requires understanding and reasoning across multiple scientific concepts to generate the correct phenomenon. The benchmark comprises 200 carefully curated prompts spanning 14 topics and 103 concepts in physics and chemistry. We conduct expert-annotated evaluations across seven state-of-the-art video models in T2V and I2V settings along five dimensions: Prompt Consistency, Phenomenon Congruency, Correct Dynamism, Immutability, and Spatio-Temporal Continuity. Using a VLM-as-a-Judge to assess video generations, we observe strong correlation with human assessments. To the best of our knowledge, VideoScience-Bench is the first benchmark to evaluate video models not only as generators but also as reasoners, requiring their generations to demonstrate scientific understanding consistent with expected physical and chemical phenomena. Our data and evaluation code are available at: https://github.com/hao-ai-lab/VideoScience{github.com/hao-ai-lab/VideoScience}.",
    "translation": "标题：基于VideoScience-Bench的视频生成科学理解与推理能力评估\n\n摘要：视频生成领域的下一个前沿在于开发具备零样本推理能力的模型，其中理解现实世界的科学定律对于在不同条件下准确建模物理结果至关重要。然而，现有的视频评估基准多基于物理常识，对视频模型的科学推理能力评估有限。我们提出了VideoScience-Bench，这是一个专门设计用于评估视频模型对本科层次科学理解能力的基准测试。每个提示都编码了一个复合科学场景，要求模型理解并综合多个科学概念以生成正确的现象。该基准包含200个精心设计的提示，涵盖物理和化学领域的14个主题及103个核心概念。我们在文本到视频和图像到视频两种设置下，对七个前沿视频模型进行了专家标注评估，评估维度包括：提示一致性、现象符合性、动态正确性、对象恒常性以及时空连续性。通过使用视觉语言模型作为评判器对视频生成结果进行评估，我们发现其与人工评估结果具有高度相关性。据我们所知，VideoScience-Bench是首个不仅将视频模型视为生成器，更将其作为推理系统进行评估的基准，要求其生成内容展现出与预期物理化学现象相一致的科学理解能力。我们的数据与评估代码已公开于：https://github.com/hao-ai-lab/VideoScience。",
    "url": "https://huggingface.co/papers/2512.02942",
    "arxiv_url": "https://arxiv.org/abs/2512.02942"
  },
  {
    "title": "Understanding and Harnessing Sparsity in Unified Multimodal Models",
    "summary": "Large multimodal models have achieved remarkable progress in both understanding and generation. Recent efforts pursue unified multimodal models that integrate heterogeneous components to support both capabilities within a single framework. However, such unification introduces inference inefficiencies, e.g., specific tasks or samples may not require the full knowledge or capacity of the unified model. Yet, a systematic understanding of how these inefficiencies manifest across different components remains limited. In this work, we first conduct a systematic analysis of unified multimodal model components using training-free pruning as a probing methodology, considering both depth pruning and width reduction. Our study reveals that the understanding component exhibits notable compressibility in both understanding and generation tasks, which is more pronounced in the latter. In contrast, the generation components are highly sensitive to compression, with performance deteriorating sharply even under moderate compression ratios. To address this limitation, we propose the Mixture-of-Experts (MoE) Adaptation, inspired by the dynamic activation patterns observed across different samples. This approach partitions the generation module into multiple experts and enables sparse activation to restore generation quality. We validate the effectiveness of sparse activation through expert-frozen tuning and further demonstrate that a fully trainable adaptation delivers additional gains. As a result, the adapted BAGEL model achieves performance comparable to the full model while activating only about half of its parameters. The code is released at https://github.com/Shwai-He/SparseUnifiedModel{this link}.",
    "translation": "标题：理解与利用统一多模态模型中的稀疏性\n\n摘要：大型多模态模型在理解与生成任务上均取得了显著进展。近期研究致力于构建统一多模态模型，通过整合异构组件在单一框架内同时支持这两种能力。然而，这种统一性会引入推理效率问题，例如特定任务或样本可能无需调用统一模型的全部知识或容量。目前，对于这些效率问题在不同组件中如何具体体现，仍缺乏系统性的理解。本研究首先采用免训练剪枝作为探测方法，从深度剪枝与宽度缩减两个维度，对统一多模态模型组件进行了系统性分析。研究发现：理解组件在理解与生成任务中均表现出显著的压缩潜力，且在生成任务中更为明显；相比之下，生成组件对压缩高度敏感，即使在中等压缩比下性能也会急剧下降。为应对这一局限，我们受不同样本间动态激活模式的启发，提出了专家混合适配方法。该方法将生成模块划分为多个专家，并通过稀疏激活机制以恢复生成质量。我们通过专家冻结微调验证了稀疏激活的有效性，并进一步证明完全可训练的适配能带来额外性能提升。实验结果表明，适配后的BAGEL模型在仅激活约半数参数的情况下，达到了与完整模型相当的性能。代码已发布于https://github.com/Shwai-He/SparseUnifiedModel。",
    "url": "https://huggingface.co/papers/2512.02351",
    "arxiv_url": "https://arxiv.org/abs/2512.02351"
  },
  {
    "title": "Visual Sync: Multi-Camera Synchronization via Cross-View Object Motion",
    "summary": "Today, people can easily record memorable moments, ranging from concerts, sports events, lectures, family gatherings, and birthday parties with multiple consumer cameras. However, synchronizing these cross-camera streams remains challenging. Existing methods assume controlled settings, specific targets, manual correction, or costly hardware. We present VisualSync, an optimization framework based on multi-view dynamics that aligns unposed, unsynchronized videos at millisecond accuracy. Our key insight is that any moving 3D point, when co-visible in two cameras, obeys epipolar constraints once properly synchronized. To exploit this, VisualSync leverages off-the-shelf 3D reconstruction, feature matching, and dense tracking to extract tracklets, relative poses, and cross-view correspondences. It then jointly minimizes the epipolar error to estimate each camera's time offset. Experiments on four diverse, challenging datasets show that VisualSync outperforms baseline methods, achieving an median synchronization error below 50 ms.",
    "translation": "标题：视觉同步：基于跨视角物体运动的多相机同步方法\n\n摘要：如今，人们能够轻松使用多台消费级相机记录从音乐会、体育赛事、讲座、家庭聚会到生日派对等各种值得纪念的时刻。然而，同步这些跨相机视频流仍然具有挑战性。现有方法通常依赖于受控环境、特定目标、人工校正或昂贵硬件。本文提出VisualSync——一种基于多视角动态特性的优化框架，能够以毫秒级精度对齐无位姿信息且未同步的视频。我们的核心发现是：任意运动的三维点若在两个相机中同时可见，在正确同步后将满足极几何约束。基于此，VisualSync利用现成的三维重建、特征匹配与密集跟踪技术提取轨迹片段、相对位姿及跨视角对应关系，进而通过联合最小化极线误差来估计各相机的时间偏移量。在四个多样化且具有挑战性的数据集上的实验表明，VisualSync优于现有基线方法，其中位同步误差可控制在50毫秒以内。",
    "url": "https://huggingface.co/papers/2512.02017",
    "arxiv_url": "https://arxiv.org/abs/2512.02017"
  },
  {
    "title": "Artemis: Structured Visual Reasoning for Perception Policy Learning",
    "summary": "Recent reinforcement-learning frameworks for visual perception policy have begun to incorporate intermediate reasoning chains expressed in natural language. Empirical observations indicate that such purely linguistic intermediate reasoning often reduces performance on perception tasks. We argue that the core issue lies not in reasoning per se but in the form of reasoning: while these chains perform semantic reasoning in an unstructured linguistic space, visual perception requires reasoning in a spatial and object-centric space. In response, we introduce Artemis, a perception-policy learning framework that performs structured proposal-based reasoning, where each intermediate step is represented as a (label, bounding-box) pair capturing a verifiable visual state. This design enables explicit tracking of intermediate states, direct supervision for proposal quality, and avoids ambiguity introduced by language-based reasoning. Artemis is built on Qwen2.5-VL-3B, achieves strong performance on grounding and detection task and exhibits substantial generalization to counting and geometric-perception tasks. The consistent improvements across these diverse settings confirm that aligning reasoning with spatial representations enhances perception-policy learning. Owing to its strengthened visual reasoning, Artemis also achieves competitive performance on general MLLM benchmarks, illustrating that spatially grounded reasoning provides a principled route toward scalable and general perception policies.",
    "translation": "标题：Artemis：面向感知策略学习的结构化视觉推理框架\n\n摘要：近期面向视觉感知策略的强化学习框架开始引入以自然语言表达的中间推理链。实证研究表明，这种纯语言形式的中间推理往往会降低感知任务的性能。我们认为核心问题不在于推理本身，而在于推理形式：现有推理链在非结构化的语言空间中进行语义推理，而视觉感知需要在以空间和物体为中心的空间中进行推理。为此，我们提出Artemis——一个基于结构化候选框推理的感知策略学习框架，其中每个中间步骤均表示为可验证视觉状态的（标签，边界框）对。该设计实现了对中间状态的显式追踪、对候选框质量的直接监督，并避免了语言推理引入的歧义性。Artemis基于Qwen2.5-VL-3B构建，在指代定位与检测任务中表现优异，并在计数与几何感知任务上展现出显著的泛化能力。这些多样化场景中一致的性能提升证实了将推理与空间表征对齐能够增强感知策略学习。得益于其强化的视觉推理能力，Artemis在通用多模态大模型基准测试中也取得了具有竞争力的性能，这表明基于空间锚定的推理为构建可扩展、泛化性强的感知策略提供了理论可行的路径。",
    "url": "https://huggingface.co/papers/2512.01988",
    "arxiv_url": "https://arxiv.org/abs/2512.01988"
  },
  {
    "title": "UnicEdit-10M: A Dataset and Benchmark Breaking the Scale-Quality Barrier via Unified Verification for Reasoning-Enriched Edits",
    "summary": "With the rapid advances of powerful multimodal models such as GPT-4o, Nano Banana, and Seedream 4.0 in Image Editing, the performance gap between closed-source and open-source models is widening, primarily due to the scarcity of large-scale, high-quality training data and comprehensive benchmarks capable of diagnosing model weaknesses across diverse editing behaviors. Existing data construction methods face a scale-quality trade-off: human annotations are high-quality but not scalable, while automated pipelines suffer from error propagation and noise. To address this, we introduce a lightweight data pipeline that replaces multi-toolchains with an end-to-end model and a unified post-verification stage. For scalable quality control, we train a 7B dual-task expert model, Qwen-Verify, for efficient failure detection and instruction recaptioning. This pipeline yields UnicEdit-10M, a 10M-scale dataset spanning diverse basic and complex editing tasks. We also propose UnicBench, a general benchmark that extends beyond basic edits to explicitly assess spatial and knowledge-driven reasoning. To enable fine-grained diagnosis, we introduce novel metrics, including Non-edit Consistency and Reasoning Accuracy. Our analysis of mainstream models on UnicBench reveals their limitations and provides clear directions for future research.",
    "translation": "标题：UnicEdit-10M：通过统一验证打破规模-质量壁垒的推理增强编辑数据集与基准\n\n摘要：随着GPT-4o、Nano Banana、Seedream 4.0等强大多模态模型在图像编辑领域的快速发展，闭源模型与开源模型之间的性能差距日益扩大，这主要源于大规模高质量训练数据的稀缺，以及缺乏能够全面诊断模型在不同编辑行为中弱点的综合性基准。现有数据构建方法面临规模与质量的权衡：人工标注质量高但难以扩展，而自动化流程则受错误传播和噪声干扰。为解决这一问题，我们提出一种轻量级数据构建流程，以端到端模型和统一的后验阶段替代多工具链。为实现可扩展的质量控制，我们训练了一个70亿参数的双任务专家模型Qwen-Verify，用于高效失败检测和指令重描述。该流程最终构建出UnicEdit-10M——一个涵盖多样化基础与复杂编辑任务的千万级规模数据集。同时，我们提出UnicBench通用基准，其不仅涵盖基础编辑任务，更扩展至对空间推理与知识驱动推理的显式评估。为支持细粒度诊断，我们引入了包括非编辑一致性与推理准确度在内的新型评估指标。通过对主流模型在UnicBench上的分析，我们揭示了其局限性，并为未来研究指明了明确方向。",
    "url": "https://huggingface.co/papers/2512.02790",
    "arxiv_url": "https://arxiv.org/abs/2512.02790"
  },
  {
    "title": "Shoe Style-Invariant and Ground-Aware Learning for Dense Foot Contact Estimation",
    "summary": "Foot contact plays a critical role in human interaction with the world, and thus exploring foot contact can advance our understanding of human movement and physical interaction. Despite its importance, existing methods often approximate foot contact using a zero-velocity constraint and focus on joint-level contact, failing to capture the detailed interaction between the foot and the world. Dense estimation of foot contact is crucial for accurately modeling this interaction, yet predicting dense foot contact from a single RGB image remains largely underexplored. There are two main challenges for learning dense foot contact estimation. First, shoes exhibit highly diverse appearances, making it difficult for models to generalize across different styles. Second, ground often has a monotonous appearance, making it difficult to extract informative features. To tackle these issues, we present a FEet COntact estimation (FECO) framework that learns dense foot contact with shoe style-invariant and ground-aware learning. To overcome the challenge of shoe appearance diversity, our approach incorporates shoe style adversarial training that enforces shoe style-invariant features for contact estimation. To effectively utilize ground information, we introduce a ground feature extractor that captures ground properties based on spatial context. As a result, our proposed method achieves robust foot contact estimation regardless of shoe appearance and effectively leverages ground information. Code will be released.",
    "translation": "标题：鞋型不变与地面感知的密集足部接触估计学习\n\n摘要：足部接触在人与世界的交互中起着关键作用，因此探究足部接触能够深化我们对人体运动与物理交互的理解。尽管其重要性显著，现有方法通常采用零速度约束来近似足部接触，并侧重于关节层面的接触估计，未能捕捉足部与世界之间的细节交互。密集足部接触估计对于精确建模此类交互至关重要，然而从单张RGB图像预测密集足部接触的研究仍处于探索不足的状态。学习密集足部接触估计主要面临两大挑战：其一，鞋履外观高度多样化，导致模型难以在不同鞋型间泛化；其二，地面通常呈现单调外观，使得信息特征的提取较为困难。为解决这些问题，我们提出一种足部接触估计（FECO）框架，通过鞋型不变学习与地面感知学习实现密集足部接触估计。为应对鞋履外观多样性的挑战，我们的方法引入鞋型对抗训练，强制模型提取鞋型不变特征以进行接触估计。为有效利用地面信息，我们设计了地面特征提取器，基于空间上下文捕捉地面属性。实验表明，所提方法能够实现不受鞋履外观影响的鲁棒性足部接触估计，并有效利用地面信息。代码将公开发布。",
    "url": "https://huggingface.co/papers/2511.22184",
    "arxiv_url": "https://arxiv.org/abs/2511.22184"
  },
  {
    "title": "Gold-Medal-Level Olympiad Geometry Solving with Efficient Heuristic Auxiliary Constructions",
    "summary": "Automated theorem proving in Euclidean geometry, particularly for International Mathematical Olympiad (IMO) level problems, remains a major challenge and an important research focus in Artificial Intelligence. In this paper, we present a highly efficient method for geometry theorem proving that runs entirely on CPUs without relying on neural network-based inference. Our initial study shows that a simple random strategy for adding auxiliary points can achieve silver-medal level human performance on IMO. Building on this, we propose HAGeo, a Heuristic-based method for adding Auxiliary constructions in Geometric deduction that solves 28 of 30 problems on the IMO-30 benchmark, achieving gold-medal level performance and surpassing AlphaGeometry, a competitive neural network-based approach, by a notable margin. To evaluate our method and existing approaches more comprehensively, we further construct HAGeo-409, a benchmark consisting of 409 geometry problems with human-assessed difficulty levels. Compared with the widely used IMO-30, our benchmark poses greater challenges and provides a more precise evaluation, setting a higher bar for geometry theorem proving.",
    "translation": "标题：基于高效启发式辅助构造的奥林匹克几何金牌级解题方法\n\n摘要：欧几里得几何中的自动定理证明，特别是针对国际数学奥林匹克（IMO）级别的问题，仍然是人工智能领域的重大挑战和重要研究方向。本文提出一种高效的几何定理证明方法，该方法完全在CPU上运行，无需依赖基于神经网络的推理。我们的初步研究表明，采用简单的随机添加辅助点策略即可在IMO问题上达到银牌级别的人类解题水平。在此基础上，我们提出了HAGeo方法——一种基于启发式的几何演绎辅助构造技术。该方法在IMO-30基准测试中成功解决了30道题目中的28道，达到了金牌级别的解题水平，并以显著优势超越了基于神经网络的竞争性方法AlphaGeometry。为了更全面地评估本方法及现有技术，我们进一步构建了HAGeo-409基准测试集，该数据集包含409道经过人工难度评估的几何问题。与广泛使用的IMO-30相比，我们的基准测试集提出了更大挑战，提供了更精确的评估标准，为几何定理证明领域设立了更高的技术门槛。",
    "url": "https://huggingface.co/papers/2512.00097",
    "arxiv_url": "https://arxiv.org/abs/2512.00097"
  },
  {
    "title": "Masks Can Be Distracting: On Context Comprehension in Diffusion Language Models",
    "summary": "Masked Diffusion Language Models (MDLMs) have recently emerged as a promising alternative to Autoregressive Language Models (ARLMs), leveraging a denoising objective that, in principle, should enable more uniform context utilisation. In this work, we examine the context comprehension abilities of MDLMs and uncover two key limitations. First, despite their more global training objective and bidirectional attention mechanism, similarly to ARLMS, MDLMs exhibit a strong locality bias: performance is highly sensitive to the position of relevant information within the input, favouring local over distant context. Second, we show that appending a large number of mask tokens--required for generation--can significantly degrade context comprehension. Through systematic ablations, we find that these masks act as distractors, reducing the model's ability to process relevant information. To address this, we introduce a mask-agnostic loss function that encourages predictions to remain invariant to the number of appended masks. Fine-tuning with this objective substantially mitigates the distracting effect of masks, improving robustness of MDLMs. Overall, our findings reveal critical limitations of the current MDLM training paradigm and provide actionable insights for building diffusion-based language models with stronger context comprehension.",
    "translation": "标题：掩码可能造成干扰：论扩散语言模型中的上下文理解能力\n\n摘要：掩码扩散语言模型（MDLMs）近期作为自回归语言模型（ARLMs）的一种有前景的替代方案出现，其利用去噪目标在理论上应能实现更均衡的上下文利用。本研究系统考察了MDLMs的上下文理解能力，并揭示了两项关键局限。首先，尽管MDLMs采用更全局的训练目标和双向注意力机制，但其与ARLMs相似地表现出强烈的局部性偏好：模型性能对输入中相关信息的位置高度敏感，更倾向于利用局部上下文而非远距离上下文。其次，我们发现生成所需的大量掩码标记会显著削弱上下文理解能力。通过系统性消融实验，这些掩码被证实具有干扰作用，会降低模型处理相关信息的能力。为应对此问题，我们提出一种掩码无关的损失函数，该函数促使模型预测结果对附加掩码数量保持稳定。基于此目标进行微调可有效缓解掩码的干扰效应，显著提升MDLMs的鲁棒性。总体而言，本研究揭示了当前MDLM训练范式的关键局限，并为构建具有更强上下文理解能力的扩散式语言模型提供了可操作的改进方向。",
    "url": "https://huggingface.co/papers/2511.21338",
    "arxiv_url": "https://arxiv.org/abs/2511.21338"
  },
  {
    "title": "CodeV: Code with Images for Faithful Visual Reasoning via Tool-Aware Policy Optimization",
    "summary": "Agentic vision-language models are increasingly trained to \"think with images\" by calling image operations. However, we show that high final-answer accuracy often hides unfaithful visual reasoning: models may invoke tools on irrelevant regions or ignore tool outputs entirely, yet still guess the correct answer. In this work, we first propose a faithfulness evaluation protocol that measures whether intermediate visual tool outputs (e.g., crops) actually contain the queried evidence. This reveals that recent visual agents achieve high final-answer accuracy but exhibit low rates of faithful tool-use on visual search benchmarks. We then introduce CodeV, a code-based visual agent trained with Tool-Aware Policy Optimization (TAPO). TAPO is a process-level RL framework that augments GRPO with dense rewards defined directly on visual tool inputs and outputs, rather than on chain-of-thought tokens, making supervision easier to verify and less susceptible to reward hacking. CodeV represents visual tools as executable Python code, and TAPO assigns step-wise rewards based solely on the question and tool output, encouraging both necessary and evidence-consistent tool use. In a two-stage SFT+RL pipeline, CodeV achieves competitive or superior accuracy while substantially increasing faithful tool-use rates on related visual search benchmarks. Beyond visual search, CodeV attains strong performance on a range of multimodal reasoning and math benchmarks, suggesting that explicitly supervising intermediate tool behavior is crucial for building trustworthy, agentic visual reasoning systems.",
    "translation": "标题：CodeV：基于工具感知策略优化的图像代码化可信视觉推理\n\n摘要：当前代理式视觉语言模型正越来越多地通过调用图像操作来训练“基于图像的思考”能力。然而，我们发现最终答案的高准确率往往掩盖了不可信的视觉推理过程：模型可能在无关区域调用工具，或完全忽略工具输出，却仍能猜测出正确答案。本研究首先提出一种可信度评估协议，用于衡量中间视觉工具输出（如图像裁剪区域）是否真实包含查询证据。该评估显示，当前先进的视觉代理虽然在最终答案准确率上表现优异，但在视觉搜索基准测试中展现出较低的可信工具使用率。为此，我们提出CodeV——一种基于代码的视觉代理，采用工具感知策略优化（TAPO）进行训练。TAPO是一种过程级强化学习框架，它在GRPO基础上增加了直接针对视觉工具输入输出定义的密集奖励机制（而非基于思维链标记），使得监督更易于验证且不易受到奖励篡改的影响。CodeV将视觉工具表示为可执行的Python代码，TAPO仅依据问题和工具输出分配逐步奖励，从而促进必要且与证据一致的工具使用。通过两阶段监督微调与强化学习相结合的流程，CodeV在相关视觉搜索基准测试中实现了具有竞争力的准确率，同时显著提升了可信工具使用率。除视觉搜索任务外，CodeV在多种多模态推理和数学基准测试中均表现出色，这表明对中间工具行为的显式监督对于构建可信赖的代理式视觉推理系统至关重要。",
    "url": "https://huggingface.co/papers/2511.19661",
    "arxiv_url": "https://arxiv.org/abs/2511.19661"
  },
  {
    "title": "BOOM: Beyond Only One Modality KIT's Multimodal Multilingual Lecture Companion",
    "summary": "The globalization of education and rapid growth of online learning have made localizing educational content a critical challenge. Lecture materials are inherently multimodal, combining spoken audio with visual slides, which requires systems capable of processing multiple input modalities. To provide an accessible and complete learning experience, translations must preserve all modalities: text for reading, slides for visual understanding, and speech for auditory learning. We present BOOM, a multimodal multilingual lecture companion that jointly translates lecture audio and slides to produce synchronized outputs across three modalities: translated text, localized slides with preserved visual elements, and synthesized speech. This end-to-end approach enables students to access lectures in their native language while aiming to preserve the original content in its entirety. Our experiments demonstrate that slide-aware transcripts also yield cascading benefits for downstream tasks such as summarization and question answering. We release our Slide Translation code at https://github.com/saikoneru/image-translator and integrate it in Lecture Translator at https://gitlab.kit.edu/kit/isl-ai4lt/lt-middleware/ltpipeline}\\footnote{All released code and models are licensed under the MIT License.",
    "translation": "标题：BOOM：超越单一模态——KIT的多模态多语言讲座伴侣\n\n摘要：教育全球化与在线学习的迅猛发展使得教育内容本地化成为一项关键挑战。讲座材料本质上是多模态的，融合了语音音频与视觉幻灯片，这要求系统具备处理多种输入模态的能力。为提供无障碍且完整的学习体验，翻译必须保留所有模态：用于阅读的文本、用于视觉理解的幻灯片以及用于听觉学习的语音。本文提出BOOM——一种多模态多语言讲座伴侣系统，它能联合翻译讲座音频与幻灯片，生成跨三种模态的同步输出：翻译文本、保留视觉元素的本地化幻灯片以及合成语音。这种端到端的方法使学生能够以母语获取讲座内容，同时力求完整保留原始信息。实验表明，融合幻灯片信息的转录文本还能为摘要生成和问答等下游任务带来连锁增益。我们在https://github.com/saikoneru/image-translator发布了幻灯片翻译代码，并将其集成至讲座翻译系统（https://gitlab.kit.edu/kit/isl-ai4lt/lt-middleware/ltpipeline）\\footnote{所有公开代码与模型均遵循MIT许可证发布。",
    "url": "https://huggingface.co/papers/2512.02817",
    "arxiv_url": "https://arxiv.org/abs/2512.02817"
  },
  {
    "title": "Click2Graph: Interactive Panoptic Video Scene Graphs from a Single Click",
    "summary": "State-of-the-art Video Scene Graph Generation (VSGG) systems provide structured visual understanding but operate as closed, feed-forward pipelines with no ability to incorporate human guidance. In contrast, promptable segmentation models such as SAM2 enable precise user interaction but lack semantic or relational reasoning. We introduce Click2Graph, the first interactive framework for Panoptic Video Scene Graph Generation (PVSG) that unifies visual prompting with spatial, temporal, and semantic understanding. From a single user cue, such as a click or bounding box, Click2Graph segments and tracks the subject across time, autonomously discovers interacting objects, and predicts <subject, object, predicate> triplets to form a temporally consistent scene graph. Our framework introduces two key components: a Dynamic Interaction Discovery Module that generates subject-conditioned object prompts, and a Semantic Classification Head that performs joint entity and predicate reasoning. Experiments on the OpenPVSG benchmark demonstrate that Click2Graph establishes a strong foundation for user-guided PVSG, showing how human prompting can be combined with panoptic grounding and relational inference to enable controllable and interpretable video scene understanding.",
    "translation": "标题：Click2Graph：基于单次点击的交互式全景视频场景图生成\n\n摘要：当前最先进的视频场景图生成系统虽能提供结构化的视觉理解，但其作为封闭的前馈式流水线运行，无法融入人工引导。相比之下，以SAM2为代表的可提示分割模型虽支持精确的用户交互，却缺乏语义或关系推理能力。本文提出Click2Graph——首个面向全景视频场景图生成的交互式框架，实现了视觉提示与空间、时间及语义理解的统一。该系统仅需用户提供单次交互提示（如点击或边界框），即可跨时间分割并跟踪目标主体，自主发现交互对象，进而预测<主体，对象，谓词>三元组以构建时序一致的场景图。本框架包含两个核心组件：动态交互发现模块（用于生成基于主体条件的对象提示）和语义分类头（用于执行联合实体与谓词推理）。在OpenPVSG基准测试上的实验表明，Click2Graph为用户引导的全景视频场景图生成奠定了坚实基础，展示了如何将人工提示与全景定位及关系推理相结合，从而实现可控且可解释的视频场景理解。",
    "url": "https://huggingface.co/papers/2511.15948",
    "arxiv_url": "https://arxiv.org/abs/2511.15948"
  }
]