[
  {
    "title": "VIDEOP2R: Video Understanding from Perception to Reasoning",
    "summary": "Reinforcement fine-tuning (RFT), a two-stage framework consisting of supervised fine-tuning (SFT) and reinforcement learning (RL) has shown promising results on improving reasoning ability of large language models (LLMs). Yet extending RFT to large video language models (LVLMs) remains challenging. We propose VideoP2R, a novel process-aware video RFT framework that enhances video reasoning by modeling perception and reasoning as distinct processes. In the SFT stage, we develop a three-step pipeline to generate VideoP2R-CoT-162K, a high-quality, process-aware chain-of-thought (CoT) dataset for perception and reasoning. In the RL stage, we introduce a novel process-aware group relative policy optimization (PA-GRPO) algorithm that supplies separate rewards for perception and reasoning. Extensive experiments show that VideoP2R achieves state-of-the-art (SotA) performance on six out of seven video reasoning and understanding benchmarks. Ablation studies further confirm the effectiveness of our process-aware modeling and PA-GRPO and demonstrate that model's perception output is information-sufficient for downstream reasoning.",
    "translation": "标题：VIDEOP2R：从感知到推理的视频理解框架\n\n摘要：强化微调（RFT）作为一种包含监督微调（SFT）与强化学习（RL）的两阶段框架，在提升大语言模型（LLMs）推理能力方面已展现出显著成效。然而将该框架扩展至大视频语言模型（LVLMs）仍存在挑战。本文提出VideoP2R——一种创新的过程感知视频RFT框架，通过将感知与推理建模为独立过程来增强视频推理能力。在SFT阶段，我们开发了三步流水线生成VideoP2R-CoT-162K，这是一个面向感知与推理的高质量过程感知思维链数据集。在RL阶段，我们引入了创新的过程感知分组相对策略优化（PA-GRPO）算法，该算法为感知与推理过程分别提供奖励机制。大量实验表明，VideoP2R在七项视频推理与理解基准测试中有六项达到最先进性能。消融研究进一步验证了过程感知建模与PA-GRPO的有效性，并证明模型的感知输出能为下游推理提供充分信息支撑。",
    "url": "https://huggingface.co/papers/2511.11113",
    "arxiv_url": "https://arxiv.org/abs/2511.11113"
  },
  {
    "title": "Think-at-Hard: Selective Latent Iterations to Improve Reasoning Language Models",
    "summary": "Improving reasoning capabilities of Large Language Models (LLMs), especially under parameter constraints, is crucial for real-world applications. Prior work proposes recurrent transformers, which allocate a fixed number of extra iterations per token to improve generation quality. After the first, standard forward pass, instead of verbalization, last-layer hidden states are fed back as inputs for additional iterations to refine token predictions. Yet we identify a latent overthinking phenomenon: easy token predictions that are already correct after the first pass are sometimes revised into errors in additional iterations. To address this, we propose Think-at-Hard (TaH), a dynamic latent thinking method that iterates deeper only at hard tokens. It employs a lightweight neural decider to trigger latent iterations only at tokens that are likely incorrect after the standard forward pass. During latent iterations, Low-Rank Adaptation (LoRA) modules shift the LLM objective from general next-token prediction to focused hard-token refinement. We further introduce a duo-causal attention mechanism that extends attention from the token sequence dimension to an additional iteration depth dimension. This enables cross-iteration information flow while maintaining full sequential parallelism. Experiments show that TaH boosts LLM reasoning performance across five challenging benchmarks while maintaining the same parameter count. Compared with baselines that iterate twice for all output tokens, TaH delivers 8.1-11.3% accuracy gains while exempting 94% of tokens from the second iteration. Against strong single-iteration Qwen3 models finetuned with the same data, it also delivers 4.0-5.0% accuracy gains. When allowing less than 3% additional parameters from LoRA and the iteration decider, the gains increase to 8.5-12.6% and 5.3-5.4%, respectively. Our code is available at https://github.com/thu-nics/TaH.",
    "translation": "标题：[深度思考：选择性潜在迭代提升推理语言模型性能]\n\n摘要：提升大语言模型的推理能力，尤其在参数受限场景下，对实际应用至关重要。现有研究提出循环Transformer架构，通过为每个标记分配固定次数的额外迭代来优化生成质量。该方法在完成首次标准前向计算后，将末层隐藏状态而非词汇化结果作为输入进行多次迭代，以优化标记预测。然而我们发现存在潜在过度思考现象：首次计算已预测正确的简单标记，在后续迭代中可能被错误修正。为此，我们提出深度思考方法——一种动态潜在思考机制，仅对困难标记进行深度迭代。该方法采用轻量级神经决策器，仅在标准前向计算后可能出错的标记处触发潜在迭代。在潜在迭代过程中，低秩自适应模块将大语言模型目标从通用下一标记预测转向专注的困难标记优化。我们进一步提出双因果注意力机制，将注意力范围从标记序列维度扩展至迭代深度维度，在保持全序列并行性的同时实现跨迭代信息流。实验表明，深度思考方法在五大挑战性基准测试中显著提升大语言模型推理性能，且参数量保持不变。与对所有输出标记进行两次迭代的基线方法相比，本方法在免除94%标记二次迭代的同时，准确率提升8.1-11.3%。相较于使用相同数据微调的单次迭代Qwen3强基线模型，准确率提升达4.0-5.0%。当允许使用低秩自适应模块和迭代决策器带来的不足3%附加参数时，准确率增益分别扩大至8.5-12.6%和5.3-5.4%。代码已开源：https://github.com/thu-nics/TaH。",
    "url": "https://huggingface.co/papers/2511.08577",
    "arxiv_url": "https://arxiv.org/abs/2511.08577"
  },
  {
    "title": "AraLingBench A Human-Annotated Benchmark for Evaluating Arabic Linguistic Capabilities of Large Language Models",
    "summary": "We present AraLingBench: a fully human annotated benchmark for evaluating the Arabic linguistic competence of large language models (LLMs). The benchmark spans five core categories: grammar, morphology, spelling, reading comprehension, and syntax, through 150 expert-designed multiple choice questions that directly assess structural language understanding. Evaluating 35 Arabic and bilingual LLMs reveals that current models demonstrate strong surface level proficiency but struggle with deeper grammatical and syntactic reasoning. AraLingBench highlights a persistent gap between high scores on knowledge-based benchmarks and true linguistic mastery, showing that many models succeed through memorization or pattern recognition rather than authentic comprehension. By isolating and measuring fundamental linguistic skills, AraLingBench provides a diagnostic framework for developing Arabic LLMs. The full evaluation code is publicly available on GitHub.",
    "translation": "标题：AraLingBench：用于评估大语言模型阿拉伯语语言学能力的人工标注基准\n\n摘要：本文提出AraLingBench：一个全人工标注的基准测试体系，用于系统评估大语言模型（LLMs）的阿拉伯语语言学能力。该基准涵盖语法、词法、拼写、阅读理解和句法五大核心范畴，通过150道专家设计的单项选择题直接评估模型的结构化语言理解能力。对35个阿拉伯语及双语大语言模型的评估表明，当前模型虽具备较强的表层语言能力，但在深层语法和句法推理方面仍存在明显缺陷。AraLingBench揭示了基于知识的基准测试高分与真实语言掌握程度之间的持续差距，证明许多模型的成功依赖于记忆存储或模式识别而非真正的语言理解。通过分离和测量基础语言技能，AraLingBench为开发阿拉伯语大语言模型提供了诊断性框架。完整评估代码已在GitHub平台开源发布。",
    "url": "https://huggingface.co/papers/2511.14295",
    "arxiv_url": "https://arxiv.org/abs/2511.14295"
  },
  {
    "title": "A Style is Worth One Code: Unlocking Code-to-Style Image Generation with Discrete Style Space",
    "summary": "Innovative visual stylization is a cornerstone of artistic creation, yet generating novel and consistent visual styles remains a significant challenge. Existing generative approaches typically rely on lengthy textual prompts, reference images, or parameter-efficient fine-tuning to guide style-aware image generation, but often struggle with style consistency, limited creativity, and complex style representations. In this paper, we affirm that a style is worth one numerical code by introducing the novel task, code-to-style image generation, which produces images with novel, consistent visual styles conditioned solely on a numerical style code. To date, this field has only been primarily explored by the industry (e.g., Midjourney), with no open-source research from the academic community. To fill this gap, we propose CoTyle, the first open-source method for this task. Specifically, we first train a discrete style codebook from a collection of images to extract style embeddings. These embeddings serve as conditions for a text-to-image diffusion model (T2I-DM) to generate stylistic images. Subsequently, we train an autoregressive style generator on the discrete style embeddings to model their distribution, allowing the synthesis of novel style embeddings. During inference, a numerical style code is mapped to a unique style embedding by the style generator, and this embedding guides the T2I-DM to generate images in the corresponding style. Unlike existing methods, our method offers unparalleled simplicity and diversity, unlocking a vast space of reproducible styles from minimal input. Extensive experiments validate that CoTyle effectively turns a numerical code into a style controller, demonstrating a style is worth one code.",
    "translation": "标题：一码一风格：基于离散风格空间的代码到风格图像生成技术  \n\n摘要：创新性视觉风格化是艺术创作的基石，然而生成新颖且一致的视觉风格仍面临重大挑战。现有生成方法通常依赖冗长的文本提示、参考图像或参数高效微调来引导风格感知图像生成，但普遍存在风格一致性不足、创造力受限和风格表征复杂等问题。本文通过提出“代码到风格图像生成”这一创新任务，论证了“一码一风格”的核心思想——仅凭数值风格代码即可生成具有新颖且一致视觉风格的图像。迄今为止，该领域主要由业界（如Midjourney）探索，学术界尚未有开源研究成果。为填补这一空白，我们提出首个开源方法CoTyle。具体而言，我们首先从图像集合中训练离散风格码本以提取风格嵌入，这些嵌入作为文生图扩散模型的条件输入来生成风格化图像。随后，我们在离散风格嵌入上训练自回归风格生成器以建模其分布，从而实现新颖风格嵌入的合成。在推理阶段，数值风格码通过风格生成器映射为唯一风格嵌入，该嵌入引导文生图扩散模型生成对应风格的图像。与现有方法相比，本方法以极简输入解锁了海量可复现风格空间，在简洁性与多样性方面具有显著优势。大量实验验证，CoTyle能有效将数值代码转化为风格控制器，充分证明“一码一风格”的技术可行性。",
    "url": "https://huggingface.co/papers/2511.10555",
    "arxiv_url": "https://arxiv.org/abs/2511.10555"
  },
  {
    "title": "Can World Simulators Reason? Gen-ViRe: A Generative Visual Reasoning Benchmark",
    "summary": "While Chain-of-Thought (CoT) prompting enables sophisticated symbolic reasoning in LLMs, it remains confined to discrete text and cannot simulate the continuous, physics-governed dynamics of the real world. Recent video generation models have emerged as potential world simulators through Chain-of-Frames (CoF) reasoning -- materializing thought as frame-by-frame visual sequences, with each frame representing a physically-grounded reasoning step. Despite compelling demonstrations, a challenge persists: existing benchmarks, focusing on fidelity or alignment, do not assess CoF reasoning and thus cannot measure core cognitive abilities in multi-step planning, algorithmic logic, or abstract pattern extrapolation. This evaluation void prevents systematic understanding of model capabilities and principled guidance for improvement. We introduce Gen-ViRe (Generative Visual Reasoning Benchmark), a framework grounded in cognitive science and real-world AI applications, which decomposes CoF reasoning into six cognitive dimensions -- from perceptual logic to abstract planning -- and 24 subtasks. Through multi-source data curation, minimal prompting protocols, and hybrid VLM-assisted evaluation with detailed criteria, Gen-ViRe delivers the first quantitative assessment of video models as reasoners. Our experiments on SOTA systems reveal substantial discrepancies between impressive visual quality and actual reasoning depth, establishing baselines and diagnostic tools to advance genuine world simulators.",
    "translation": "标题：世界模拟器能否推理？Gen-ViRe：生成式视觉推理基准框架\n\n摘要：尽管思维链提示使大语言模型能够进行复杂符号推理，但其仍局限于离散文本范畴，无法模拟现实世界中受物理规律支配的连续动态过程。近期视频生成模型通过帧序列推理展现出作为世界模拟器的潜力——将思维具象化为逐帧视觉序列，每帧代表基于物理规律的推理步骤。尽管已有显著成果，但现有基准主要关注保真度或对齐性，尚未能评估帧序列推理能力，因而无法衡量多步规划、算法逻辑或抽象模式推演等核心认知能力。这一评估空白阻碍了对模型能力的系统性认知及改进的原则性指导。我们提出Gen-ViRe（生成式视觉推理基准），该框架植根于认知科学与现实人工智能应用，将帧序列推理解构为从感知逻辑到抽象规划的六个认知维度及24项子任务。通过多源数据策管、最小化提示协议，以及结合视觉语言模型的混合评估体系与详细标准，Gen-ViRe首次实现对视频模型推理能力的量化评估。我们在前沿系统上的实验表明，视觉质量与真实推理深度之间存在显著差距，通过建立基线标准与诊断工具，为发展真正意义上的世界模拟器提供支撑。",
    "url": "https://huggingface.co/papers/2511.13853",
    "arxiv_url": "https://arxiv.org/abs/2511.13853"
  },
  {
    "title": "MVI-Bench: A Comprehensive Benchmark for Evaluating Robustness to Misleading Visual Inputs in LVLMs",
    "summary": "Evaluating the robustness of Large Vision-Language Models (LVLMs) is essential for their continued development and responsible deployment in real-world applications. However, existing robustness benchmarks typically focus on hallucination or misleading textual inputs, while largely overlooking the equally critical challenge posed by misleading visual inputs in assessing visual understanding. To fill this important gap, we introduce MVI-Bench, the first comprehensive benchmark specially designed for evaluating how Misleading Visual Inputs undermine the robustness of LVLMs. Grounded in fundamental visual primitives, the design of MVI-Bench centers on three hierarchical levels of misleading visual inputs: Visual Concept, Visual Attribute, and Visual Relationship. Using this taxonomy, we curate six representative categories and compile 1,248 expertly annotated VQA instances. To facilitate fine-grained robustness evaluation, we further introduce MVI-Sensitivity, a novel metric that characterizes LVLM robustness at a granular level. Empirical results across 18 state-of-the-art LVLMs uncover pronounced vulnerabilities to misleading visual inputs, and our in-depth analyses on MVI-Bench provide actionable insights that can guide the development of more reliable and robust LVLMs. The benchmark and codebase can be accessed at https://github.com/chenyil6/MVI-Bench.",
    "translation": "标题：MVI-Bench：面向大视觉语言模型误导性视觉输入鲁棒性评估的综合基准\n\n摘要：评估大视觉语言模型的鲁棒性对其持续发展和在现实应用中的可靠部署至关重要。然而现有鲁棒性基准主要关注幻觉或误导性文本输入，而在评估视觉理解能力时普遍忽视了误导性视觉输入带来的同等重要挑战。为填补这一关键空白，我们推出MVI-Bench——首个专门用于评估误导性视觉输入如何影响LVLM鲁棒性的综合基准。基于基础视觉基元的设计理念，MVI-Bench围绕误导性视觉输入的三个层级构建：视觉概念、视觉属性和视觉关系。基于该分类体系，我们精选出六个代表性类别，编制了1,248个经过专家标注的视觉问答样本。为支持细粒度鲁棒性评估，我们进一步提出MVI-敏感度这一创新指标，可在微观层面表征LVLM的鲁棒性。对18个前沿LVLM的实证研究揭示了模型对误导性视觉输入存在显著脆弱性，我们通过MVI-Bench开展的深度分析为开发更可靠、鲁棒的LVLM提供了可操作的指导见解。基准与代码库可通过https://github.com/chenyil6/MVI-Bench获取。",
    "url": "https://huggingface.co/papers/2511.14159",
    "arxiv_url": "https://arxiv.org/abs/2511.14159"
  },
  {
    "title": "REVISOR: Beyond Textual Reflection, Towards Multimodal Introspective Reasoning in Long-Form Video Understanding",
    "summary": "Self-reflection mechanisms that rely on purely text-based rethinking processes perform well in most multimodal tasks. However, when directly applied to long-form video understanding scenarios, they exhibit clear limitations. The fundamental reasons for this lie in two points: (1)long-form video understanding involves richer and more dynamic visual input, meaning rethinking only the text information is insufficient and necessitates a further rethinking process specifically targeting visual information; (2) purely text-based reflection mechanisms lack cross-modal interaction capabilities, preventing them from fully integrating visual information during reflection. Motivated by these insights, we propose REVISOR (REflective VIsual Segment Oriented Reasoning), a novel framework for tool-augmented multimodal reflection. REVISOR enables MLLMs to collaboratively construct introspective reflection processes across textual and visual modalities, significantly enhancing their reasoning capability for long-form video understanding. To ensure that REVISOR can learn to accurately review video segments highly relevant to the question during reinforcement learning, we designed the Dual Attribution Decoupled Reward (DADR) mechanism. Integrated into the GRPO training strategy, this mechanism enforces causal alignment between the model's reasoning and the selected video evidence. Notably, the REVISOR framework significantly enhances long-form video understanding capability of MLLMs without requiring supplementary supervised fine-tuning or external models, achieving impressive results on four benchmarks including VideoMME, LongVideoBench, MLVU, and LVBench.",
    "translation": "标题：REVISOR：超越文本反思——面向长视频理解的多模态内省推理框架\n\n摘要：依赖纯文本再思考过程的自我反思机制在多数多模态任务中表现良好。然而当直接应用于长视频理解场景时，这些机制表现出明显局限性。其根本原因在于两点：（1）长视频理解涉及更丰富且动态变化的视觉输入，仅对文本信息进行重新思考不足以为继，必须建立专门针对视觉信息的深度反思流程；（2）纯文本反思机制缺乏跨模态交互能力，导致在反思过程中无法充分整合视觉信息。基于这些发现，我们提出REVISOR（反射式视觉片段定向推理）——一种工具增强型多模态反思新框架。该框架使多模态大语言模型能够跨文本与视觉模态协同构建内省式反思流程，显著增强其对长视频的推理能力。为确保REVISOR在强化学习中能准确审视与问题高度相关的视频片段，我们设计了双归因解耦奖励机制。该机制融入GRPO训练策略后，可强化模型推理与所选视频证据间的因果对齐。值得注意的是，REVISOR框架无需额外监督微调或外部模型辅助，即可显著提升多模态大语言模型的长视频理解能力，在VideoMME、LongVideoBench、MLVU和LVBench四个基准测试中均取得显著成效。",
    "url": "https://huggingface.co/papers/2511.13026",
    "arxiv_url": "https://arxiv.org/abs/2511.13026"
  },
  {
    "title": "OmniZip: Audio-Guided Dynamic Token Compression for Fast Omnimodal Large Language Models",
    "summary": "Omnimodal large language models (OmniLLMs) have attracted increasing research attention of late towards unified audio-video understanding, wherein processing audio-video token sequences creates a significant computational bottleneck, however. Existing token compression methods have yet to accommodate this emerging need of jointly compressing multimodal tokens. To bridge this gap, we present OmniZip, a training-free, audio-guided audio-visual token-compression framework that optimizes multimodal token representation and accelerates inference. Specifically, OmniZip first identifies salient audio tokens, then computes an audio retention score for each time group to capture information density, thereby dynamically guiding video token pruning and preserving cues from audio anchors enhanced by cross-modal similarity. For each time window, OmniZip compresses the video tokens using an interleaved spatio-temporal scheme. Extensive empirical results demonstrate the merits of OmniZip - it achieves 3.42X inference speedup and 1.4X memory reduction over other top-performing counterparts, while maintaining performance with no training.",
    "translation": "标题：OmniZip：面向快速全模态大模型的音频引导动态令牌压缩方法\n\n摘要：全模态大语言模型近期在统一音视频理解领域获得日益增长的研究关注，然而处理音视频令牌序列会形成显著的计算瓶颈。现有令牌压缩方法尚未满足这种联合压缩多模态令牌的新兴需求。为弥补这一空白，我们提出OmniZip——一种免训练的音频引导音视频令牌压缩框架，可优化多模态令牌表征并加速推理。具体而言，OmniZip首先识别显著音频令牌，随后计算每个时间组的音频保留分数以捕捉信息密度，从而动态指导视频令牌剪枝并保留通过跨模态相似性增强的音频锚点线索。针对每个时间窗口，OmniZip采用交错时空策略对视频令牌进行压缩。大量实证结果表明该方法的优势——相较于其他顶尖方案，其推理速度提升达3.42倍，内存占用降低1.4倍，且在无需训练的情况下保持性能稳定。",
    "url": "https://huggingface.co/papers/2511.14582",
    "arxiv_url": "https://arxiv.org/abs/2511.14582"
  },
  {
    "title": "ATLAS: A High-Difficulty, Multidisciplinary Benchmark for Frontier Scientific Reasoning",
    "summary": "The rapid advancement of Large Language Models (LLMs) has led to performance saturation on many established benchmarks, questioning their ability to distinguish frontier models. Concurrently, existing high-difficulty benchmarks often suffer from narrow disciplinary focus, oversimplified answer formats, and vulnerability to data contamination, creating a fidelity gap with real-world scientific inquiry. To address these challenges, we introduce ATLAS (AGI-Oriented Testbed for Logical Application in Science), a large-scale, high-difficulty, and cross-disciplinary evaluation suite composed of approximately 800 original problems. Developed by domain experts (PhD-level and above), ATLAS spans seven core scientific fields: mathematics, physics, chemistry, biology, computer science, earth science, and materials science. Its key features include: (1) High Originality and Contamination Resistance, with all questions newly created or substantially adapted to prevent test data leakage; (2) Cross-Disciplinary Focus, designed to assess models' ability to integrate knowledge and reason across scientific domains; (3) High-Fidelity Answers, prioritizing complex, open-ended answers involving multi-step reasoning and LaTeX-formatted expressions over simple multiple-choice questions; and (4) Rigorous Quality Control, employing a multi-stage process of expert peer review and adversarial testing to ensure question difficulty, scientific value, and correctness. We also propose a robust evaluation paradigm using a panel of LLM judges for automated, nuanced assessment of complex answers. Preliminary results on leading models demonstrate ATLAS's effectiveness in differentiating their advanced scientific reasoning capabilities. We plan to develop ATLAS into a long-term, open, community-driven platform to provide a reliable \"ruler\" for progress toward Artificial General Intelligence.",
    "translation": "标题：ATLAS：面向前沿科学推理的高难度跨学科基准测试  \n\n摘要：大语言模型的快速发展导致其在许多传统基准测试上出现性能饱和，难以有效区分前沿模型的真实能力。同时，现有高难度基准测试普遍存在学科覆盖狭窄、答案形式过度简化以及易受数据污染等问题，与真实科学探究场景存在显著差距。为应对这些挑战，我们推出ATLAS（面向科学逻辑应用的通用人工智能测试平台），这是一个包含约800道原创题目的大规模、高难度、跨学科评估体系。该平台由领域专家（博士及以上级别）开发，涵盖数学、物理、化学、生物、计算机科学、地球科学和材料科学七大核心科学领域，具备以下核心特征：（1）高原创性与抗污染性，所有题目均为全新创作或深度改编，有效防止测试数据泄露；（2）跨学科导向，重点评估模型整合多学科知识进行交叉推理的能力；（3）高保真答案设计，摒弃简单选择题形式，侧重需要多步推理和LaTeX公式表达的开阔式答案；（4）严格质量控制，采用多阶段专家评审与对抗测试机制，确保题目难度、科学价值与准确性。我们还提出基于大语言模型评审团的稳健评估范式，实现对复杂答案的自动化精细评估。在领先模型上的初步实验结果验证了ATLAS在区分高级科学推理能力方面的有效性。我们计划将ATLAS建设为长期开放、社区驱动的平台，为通向通用人工智能的发展进程提供可靠衡量标尺。",
    "url": "https://huggingface.co/papers/2511.14366",
    "arxiv_url": "https://arxiv.org/abs/2511.14366"
  },
  {
    "title": "Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning",
    "summary": "Large Language Models (LLMs) are increasingly being explored for building Agents capable of active environmental interaction (e.g., via tool use) to solve complex problems. Reinforcement Learning (RL) is considered a key technology with significant potential for training such Agents; however, the effective application of RL to LLM Agents is still in its nascent stages and faces considerable challenges. Currently, this emerging field lacks in-depth exploration into RL approaches specifically tailored for the LLM Agent context, alongside a scarcity of flexible and easily extensible training frameworks designed for this purpose. To help advance this area, this paper first revisits and clarifies Reinforcement Learning methodologies for LLM Agents by systematically extending the Markov Decision Process (MDP) framework to comprehensively define the key components of an LLM Agent. Secondly, we introduce Agent-R1, a modular, flexible, and user-friendly training framework for RL-based LLM Agents, designed for straightforward adaptation across diverse task scenarios and interactive environments. We conducted experiments on Multihop QA benchmark tasks, providing initial validation for the effectiveness of our proposed methods and framework.",
    "translation": "标题：Agent-R1：基于端到端强化学习的大语言模型智能体训练框架\n\n摘要：大语言模型在构建能够主动与环境交互（例如通过工具使用）以解决复杂问题的智能体方面正得到广泛探索。强化学习被认为是训练此类智能体的关键技术，具有显著潜力；然而，强化学习在大语言模型智能体中的有效应用仍处于起步阶段，并面临重大挑战。当前该新兴领域缺乏针对大语言模型智能体场景的深度强化学习方法研究，同时专门设计的灵活可扩展训练框架也较为稀缺。为推进该领域发展，本文首先通过系统扩展马尔可夫决策过程框架来明确定义大语言模型智能体的关键组件，进而重新审视和梳理适用于大语言模型智能体的强化学习方法。其次，我们提出了Agent-R1——一个模块化、灵活易用的基于强化学习的大语言模型智能体训练框架，该设计可轻松适配不同任务场景和交互环境。我们在多跳问答基准任务上进行了实验，为所提方法与框架的有效性提供了初步验证。",
    "url": "https://huggingface.co/papers/2511.14460",
    "arxiv_url": "https://arxiv.org/abs/2511.14460"
  },
  {
    "title": "Large Language Models Meet Extreme Multi-label Classification: Scaling and Multi-modal Framework",
    "summary": "Foundation models have revolutionized artificial intelligence across numerous domains, yet their transformative potential remains largely untapped in Extreme Multi-label Classification (XMC). Queries in XMC are associated with relevant labels from extremely large label spaces, where it is critical to strike a balance between efficiency and performance. Therefore, many recent approaches efficiently pose XMC as a maximum inner product search between embeddings learned from small encoder-only transformer architectures. In this paper, we address two important aspects in XMC: how to effectively harness larger decoder-only models, and how to exploit visual information while maintaining computational efficiency. We demonstrate that both play a critical role in XMC separately and can be combined for improved performance. We show that a few billion-size decoder can deliver substantial improvements while keeping computational overhead manageable. Furthermore, our Vision-enhanced eXtreme Multi-label Learning framework (ViXML) efficiently integrates foundation vision models by pooling a single embedding per image. This limits computational growth while unlocking multi-modal capabilities. Remarkably, ViXML with small encoders outperforms text-only decoder in most cases, showing that an image is worth billions of parameters. Finally, we present an extension of existing text-only datasets to exploit visual metadata and make them available for future benchmarking. Comprehensive experiments across four public text-only datasets and their corresponding image enhanced versions validate our proposals' effectiveness, surpassing previous state-of-the-art by up to +8.21\\% in P@1 on the largest dataset. ViXML's code is available at https://github.com/DiegoOrtego/vixml.",
    "translation": "标题：大语言模型与极端多标签分类的融合：扩展框架与多模态方法\n\n摘要：基础模型已在众多人工智能领域引发革命性变革，但其在极端多标签分类（XMC）领域的变革潜力仍待充分挖掘。XMC任务需要从极大规模标签空间中检索相关标签，其核心挑战在于实现效率与性能的平衡。为此，近期研究多采用基于小型编码器架构的嵌入向量最大内积搜索方法来实现高效分类。本文针对XMC领域两个关键问题展开研究：如何有效利用仅解码器架构的大型模型，以及如何在保持计算效率的同时整合视觉信息。我们证明这两个要素在XMC中各自具有重要作用，且能协同提升性能。实验表明，数十亿参数的仅解码器模型可在可控计算开销下实现显著性能提升。进一步提出的视觉增强极端多标签学习框架（ViXML）通过单图像嵌入池化技术高效集成基础视觉模型，在控制计算成本的同时解锁多模态能力。值得注意的是，采用小型编码器的ViXML框架在多数情况下优于纯文本仅解码器模型，印证了“一图抵十亿参数”的效能。此外，我们构建了现有纯文本数据集的视觉元数据增强版本以供后续基准测试。在四个公开纯文本数据集及其视觉增强版本上的综合实验验证了本方案的有效性，在最大数据集上的P@1指标较先前最优方法提升达8.21%。ViXML代码已开源：https://github.com/DiegoOrtego/vixml。",
    "url": "https://huggingface.co/papers/2511.13189",
    "arxiv_url": "https://arxiv.org/abs/2511.13189"
  },
  {
    "title": "Orion: A Unified Visual Agent for Multimodal Perception, Advanced Visual Reasoning and Execution",
    "summary": "We introduce Orion, a visual agent framework that can take in any modality and generate any modality. Using an agentic framework with multiple tool-calling capabilities, Orion is designed for visual AI tasks and achieves state-of-the-art results. Unlike traditional vision-language models that produce descriptive outputs, Orion orchestrates a suite of specialized computer vision tools, including object detection, keypoint localization, panoptic segmentation, Optical Character Recognition, and geometric analysis, to execute complex multi-step visual workflows. The system achieves competitive performance on MMMU, MMBench, DocVQA, and MMLongBench while extending monolithic vision-language models to production-grade visual intelligence. By combining neural perception with symbolic execution, Orion enables autonomous visual reasoning, marking a transition from passive visual understanding to active, tool-driven visual intelligence.",
    "translation": "标题：Orion：支持多模态感知、高级视觉推理与执行的一体化视觉智能体\n\n摘要：本文提出Orion视觉智能体框架，该框架具备多模态输入与输出能力。通过采用具有多工具调用功能的智能体架构，Orion专为视觉AI任务设计并取得了最先进的性能。与输出描述性内容的传统视觉语言模型不同，Orion通过协调包括目标检测、关键点定位、全景分割、光学字符识别和几何分析在内的专业计算机视觉工具集，执行复杂的多步骤视觉工作流。本系统在MMMU、MMBench、DocVQA和MMLongBench基准测试中达到领先水平，同时将单体视觉语言模型扩展至生产级视觉智能。通过融合神经感知与符号执行，Orion实现了自主视觉推理，标志着从被动视觉理解到工具驱动的主动视觉智能的重要转变。",
    "url": "https://huggingface.co/papers/2511.14210",
    "arxiv_url": "https://arxiv.org/abs/2511.14210"
  },
  {
    "title": "Φeat: Physically-Grounded Feature Representation",
    "summary": "Foundation models have emerged as effective backbones for many vision tasks. However, current self-supervised features entangle high-level semantics with low-level physical factors, such as geometry and illumination, hindering their use in tasks requiring explicit physical reasoning. In this paper, we introduce Φeat, a novel physically-grounded visual backbone that encourages a representation sensitive to material identity, including reflectance cues and geometric mesostructure. Our key idea is to employ a pretraining strategy that contrasts spatial crops and physical augmentations of the same material under varying shapes and lighting conditions. While similar data have been used in high-end supervised tasks such as intrinsic decomposition or material estimation, we demonstrate that a pure self-supervised training strategy, without explicit labels, already provides a strong prior for tasks requiring robust features invariant to external physical factors. We evaluate the learned representations through feature similarity analysis and material selection, showing that Φeat captures physically-grounded structure beyond semantic grouping. These findings highlight the promise of unsupervised physical feature learning as a foundation for physics-aware perception in vision and graphics. These findings highlight the promise of unsupervised physical feature learning as a foundation for physics-aware perception in vision and graphics.",
    "translation": "标题：Φeat：基于物理基础的特征表示方法\n\n摘要：基础模型已成为众多视觉任务的有效骨干网络。然而，当前自监督特征将高层语义与低层物理因素（如几何形状与光照条件）相互耦合，这阻碍了其在需要显式物理推理任务中的应用。本文提出Φeat——一种基于物理基础的新型视觉骨干网络，该网络能够促进对材料特性（包括反射率线索与几何细观结构）敏感的特征表示。我们的核心思路是采用预训练策略，通过对比不同形状和光照条件下同一材料的空间裁剪样本与物理增强样本来实现这一目标。尽管类似数据已被用于本征分解、材质估计等高端监督任务，但我们证明无需显式标注的纯自监督训练策略，已能为需要对外部物理因素保持鲁棒性的任务提供强先验知识。通过特征相似性分析与材料选择实验，我们评估了所学特征表示的性能，结果表明Φeat能够捕捉超越语义分组的物理基础结构。这些发现凸显了无监督物理特征学习作为视觉与图形领域物理感知基础的重要潜力。这些发现凸显了无监督物理特征学习作为视觉与图形领域物理感知基础的重要潜力。",
    "url": "https://huggingface.co/papers/2511.11270",
    "arxiv_url": "https://arxiv.org/abs/2511.11270"
  },
  {
    "title": "Mitigating Label Length Bias in Large Language Models",
    "summary": "Large language models (LLMs) are powerful zero- and few-shot learners. However, when predicting over a set of candidate options, LLMs suffer from label biases, and existing calibration methods overlook biases arising from multi-token class labels. We tackle an issue we call label length bias, where labels of different lengths are treated inconsistently, even after standard length normalization. To mitigate it, we propose normalized contextual calibration (NCC), an effective method that normalizes and calibrates predictions at the full-label level. NCC achieves statistically significant improvements over prior approaches across multiple datasets and models, with gains of up to 10% F1. Moreover, NCC extends bias mitigation to broader tasks such as multiple-choice question answering. Our analysis shows that, when combined with in-context learning, NCC is less sensitive to few-shot example selection, requires fewer examples for competitive performance, and produces more reliable confidence estimates. These findings highlight the importance of mitigating full-label biases to improve the performance and robustness of LLM-based methods, particularly in real-world applications where class labels naturally consist of multiple tokens.",
    "translation": "标题：缓解大语言模型中的标签长度偏差\n\n摘要：大语言模型（LLM）是强大的零样本和少样本学习器。然而，当在候选选项集合上进行预测时，LLM容易受到标签偏差的影响，而现有校准方法未能解决多标记类别标签产生的偏差。我们针对标签长度偏差问题展开研究——即使经过标准长度归一化处理，不同长度的标签仍会被不一致地处理。为缓解此问题，我们提出归一化上下文校准（NCC）方法，该方案在完整标签层面进行归一化与校准，能有效消除此类偏差。在多个数据集和模型上的实验表明，NCC相较现有方法实现了统计显著提升，F1分数最高提升达10%。此外，NCC还可扩展应用于多项选择题回答等更广泛任务的偏差缓解。分析表明，当与上下文学习结合时，NCC对少样本示例选择的敏感性更低，用更少示例即可获得竞争优势，并能生成更可靠的可信度估计。这些发现凸显了消除完整标签偏差对于提升基于LLM的方法性能与鲁棒性的重要性，尤其在现实应用场景中，类别标签天然由多标记构成的情况下更具价值。",
    "url": "https://huggingface.co/papers/2511.14385",
    "arxiv_url": "https://arxiv.org/abs/2511.14385"
  },
  {
    "title": "Agent READMEs: An Empirical Study of Context Files for Agentic Coding",
    "summary": "Agentic coding tools receive goals written in natural language as input, break them down into specific tasks, and write or execute the actual code with minimal human intervention. Central to this process are agent context files (\"READMEs for agents\") that provide persistent, project-level instructions. In this paper, we conduct the first large-scale empirical study of 2,303 agent context files from 1,925 repositories to characterize their structure, maintenance, and content. We find that these files are not static documentation but complex, difficult-to-read artifacts that evolve like configuration code, maintained through frequent, small additions. Our content analysis of 16 instruction types shows that developers prioritize functional context, such as build and run commands (62.3%), implementation details (69.9%), and architecture (67.7%). We also identify a significant gap: non-functional requirements like security (14.5%) and performance (14.5%) are rarely specified. These findings indicate that while developers use context files to make agents functional, they provide few guardrails to ensure that agent-written code is secure or performant, highlighting the need for improved tooling and practices.",
    "translation": "标题：智能体自述文件：关于自主编码上下文本文件的实证研究\n\n摘要：自主编码工具接收以自然语言编写的目标作为输入，将其分解为具体任务，并在最少人工干预的情况下编写或执行实际代码。该过程的核心是智能体上下文本文件（即“面向智能体的自述文件”），这些文件提供持久性的项目级指导。本文通过对来自1,925个代码库的2,303个智能体上下文本文件进行首次大规模实证研究，系统分析了其结构特征、维护模式和内容构成。研究发现这些文件并非静态文档，而是类似配置代码般持续演化的复杂产物，通过频繁的小规模增补进行维护，且具有较高的阅读难度。对16类指令类型的内容分析表明，开发者优先关注功能性上下文，包括构建与运行命令（62.3%）、实现细节（69.9%）和系统架构（67.7%）。同时研究发现存在显著缺陷：非功能性需求如安全性（14.5%）与性能指标（14.5%）鲜少被明确规范。这些发现表明，虽然开发者通过上下文本文件使智能体具备基础功能，但缺乏确保智能体生成代码安全性与性能的防护机制，凸显出改进开发工具与实践方法的迫切需求。",
    "url": "https://huggingface.co/papers/2511.12884",
    "arxiv_url": "https://arxiv.org/abs/2511.12884"
  },
  {
    "title": "Proactive Hearing Assistants that Isolate Egocentric Conversations",
    "summary": "We introduce proactive hearing assistants that automatically identify and separate the wearer's conversation partners, without requiring explicit prompts. Our system operates on egocentric binaural audio and uses the wearer's self-speech as an anchor, leveraging turn-taking behavior and dialogue dynamics to infer conversational partners and suppress others. To enable real-time, on-device operation, we propose a dual-model architecture: a lightweight streaming model runs every 12.5 ms for low-latency extraction of the conversation partners, while a slower model runs less frequently to capture longer-range conversational dynamics. Results on real-world 2- and 3-speaker conversation test sets, collected with binaural egocentric hardware from 11 participants totaling 6.8 hours, show generalization in identifying and isolating conversational partners in multi-conversation settings. Our work marks a step toward hearing assistants that adapt proactively to conversational dynamics and engagement. More information can be found on our website: https://proactivehearing.cs.washington.edu/",
    "translation": "标题：主动式听觉助手：自我中心对话的隔离技术\n\n摘要：本文提出一种主动式听觉助手，能够在不需明确提示的情况下自动识别并分离佩戴者的对话对象。该系统基于自我中心双耳音频工作，以佩戴者自身语音为锚点，利用对话轮换行为与交流动态推断对话对象并抑制其他声音。为实现实时设备端运行，我们提出双模型架构：轻量级流式模型每12.5毫秒运行一次以实现低延迟的对话对象提取，而运行频率较低的慢速模型则用于捕捉长程对话动态。通过使用双耳自我中心硬件采集11位参与者总计6.8小时的真实场景数据，在2-3人对话测试集上的结果表明，该系统在多对话场景中具有识别与隔离对话对象的泛化能力。本研究成果标志着听觉助手向主动适应对话动态与参与模式迈出了重要一步。更多信息请访问我们的网站：https://proactivehearing.cs.washington.edu/",
    "url": "https://huggingface.co/papers/2511.11473",
    "arxiv_url": "https://arxiv.org/abs/2511.11473"
  },
  {
    "title": "A Brain Wave Encodes a Thousand Tokens: Modeling Inter-Cortical Neural Interactions for Effective EEG-based Emotion Recognition",
    "summary": "Human emotions are difficult to convey through words and are often abstracted in the process; however, electroencephalogram (EEG) signals can offer a more direct lens into emotional brain activity. Recent studies show that deep learning models can process these signals to perform emotion recognition with high accuracy. However, many existing approaches overlook the dynamic interplay between distinct brain regions, which can be crucial to understanding how emotions unfold and evolve over time, potentially aiding in more accurate emotion recognition. To address this, we propose RBTransformer, a Transformer-based neural network architecture that models inter-cortical neural dynamics of the brain in latent space to better capture structured neural interactions for effective EEG-based emotion recognition. First, the EEG signals are converted into Band Differential Entropy (BDE) tokens, which are then passed through Electrode Identity embeddings to retain spatial provenance. These tokens are processed through successive inter-cortical multi-head attention blocks that construct an electrode x electrode attention matrix, allowing the model to learn the inter-cortical neural dependencies. The resulting features are then passed through a classification head to obtain the final prediction. We conducted extensive experiments, specifically under subject-dependent settings, on the SEED, DEAP, and DREAMER datasets, over all three dimensions, Valence, Arousal, and Dominance (for DEAP and DREAMER), under both binary and multi-class classification settings. The results demonstrate that the proposed RBTransformer outperforms all previous state-of-the-art methods across all three datasets, over all three dimensions under both classification settings. The source code is available at: https://github.com/nnilayy/RBTransformer.",
    "translation": "标题：一波脑电编码千种表征：基于皮层间神经交互建模的高效脑电情绪识别研究\n\n摘要：人类情绪难以通过语言完整传达，且常在表达过程中被抽象化；而脑电图信号能为情绪相关的脑部活动提供更直接的观测窗口。近期研究表明，深度学习模型可通过处理这些信号实现高精度情绪识别。然而，现有方法大多忽视了不同脑区之间的动态交互作用——这种交互对于理解情绪随时间展开与演变的规律至关重要，并可能提升情绪识别的准确性。为此，我们提出RBTransformer模型，该基于Transformer的神经网络架构在隐空间中对大脑皮层间神经动力学进行建模，以更好地捕捉结构化神经交互，从而实现高效的基于脑电的情绪识别。该方法首先将脑电信号转换为频带差分熵表征，再通过电极身份嵌入保留空间溯源信息。这些表征经由连续的皮层间多头注意力模块处理，构建电极×电极注意力矩阵，使模型能够学习皮层间神经依赖关系。最终生成的特征通过分类头获得预测结果。我们在SEED、DEAP和DREAMER数据集上开展了全面实验，特别在受试者依赖设定下，针对效价、唤醒度和优势度三个维度（DEAP和DREAMER数据集），分别进行二元与多分类验证。实验结果表明，在两种分类设定下，所提出的RBTransformer在所有数据集的三个维度上均超越现有最优方法。源代码详见：https://github.com/nnilayy/RBTransformer。",
    "url": "https://huggingface.co/papers/2511.13954",
    "arxiv_url": "https://arxiv.org/abs/2511.13954"
  },
  {
    "title": "Error-Driven Scene Editing for 3D Grounding in Large Language Models",
    "summary": "Despite recent progress in 3D-LLMs, they remain limited in accurately grounding language to visual and spatial elements in 3D environments. This limitation stems in part from training data that focuses on language reasoning rather than spatial understanding due to scarce 3D resources, leaving inherent grounding biases unresolved. To address this, we propose 3D scene editing as a key mechanism to generate precise visual counterfactuals that mitigate these biases through fine-grained spatial manipulation, without requiring costly scene reconstruction or large-scale 3D data collection. Furthermore, to make these edits targeted and directly address the specific weaknesses of the model, we introduce DEER-3D, an error-driven framework following a structured \"Decompose, Diagnostic Evaluation, Edit, and Re-train\" workflow, rather than broadly or randomly augmenting data as in conventional approaches. Specifically, upon identifying a grounding failure of the 3D-LLM, our framework first diagnoses the exact predicate-level error (e.g., attribute or spatial relation). It then executes minimal, predicate-aligned 3D scene edits, such as recoloring or repositioning, to produce targeted counterfactual supervision for iterative model fine-tuning, significantly enhancing grounding accuracy. We evaluate our editing pipeline across multiple benchmarks for 3D grounding and scene understanding tasks, consistently demonstrating improvements across all evaluated datasets through iterative refinement. DEER-3D underscores the effectiveness of targeted, error-driven scene editing in bridging linguistic reasoning capabilities with spatial grounding in 3D LLMs.",
    "translation": "标题：面向大语言模型三维定位的误差驱动场景编辑方法\n\n摘要：尽管三维大语言模型近期取得进展，其在三维环境中实现语言与视觉空间元素的精准定位仍存在局限。该问题部分源于训练数据因三维资源稀缺而侧重于语言推理而非空间理解，导致固有的定位偏差未能解决。为此，我们提出以三维场景编辑为核心机制，通过细粒度空间操作生成精确的视觉反事实样本以消除偏差，无需昂贵场景重建或大规模三维数据采集。为进一步实现精准编辑并针对性修正模型缺陷，我们提出DEER-3D误差驱动框架，采用“分解-诊断评估-编辑-再训练”的结构化流程，而非传统方法中广泛或随机的数据增强。具体而言，当检测到三维大语言模型定位失败时，本框架首先诊断谓词层级的精确错误（如属性或空间关系错误），随后执行最小化的谓词对齐三维场景编辑（如重新着色或位移），为迭代式模型微调提供定向反事实监督，显著提升定位精度。我们在多个三维定位与场景理解任务的基准测试中评估编辑流程，通过迭代优化在所有数据集上均取得稳定提升。DEER-3D验证了定向误差驱动场景编辑在弥合三维大语言模型语言推理与空间定位能力方面的有效性。",
    "url": "https://huggingface.co/papers/2511.14086",
    "arxiv_url": "https://arxiv.org/abs/2511.14086"
  },
  {
    "title": "LLM-Powered Fully Automated Chaos Engineering: Towards Enabling Anyone to Build Resilient Software Systems at Low Cost",
    "summary": "Chaos Engineering (CE) is an engineering technique aimed at improving the resilience of distributed systems. It involves intentionally injecting faults into a system to test its resilience, uncover weaknesses, and address them before they cause failures in production. Recent CE tools automate the execution of predefined CE experiments. However, planning such experiments and improving the system based on the experimental results still remain manual. These processes are labor-intensive and require multi-domain expertise. To address these challenges and enable anyone to build resilient systems at low cost, this paper proposes ChaosEater, a system that automates the entire CE cycle with Large Language Models (LLMs). It predefines an agentic workflow according to a systematic CE cycle and assigns subdivided processes within the workflow to LLMs. ChaosEater targets CE for software systems built on Kubernetes. Therefore, the LLMs in ChaosEater complete CE cycles through software engineering tasks, including requirement definition, code generation, testing, and debugging. We evaluate ChaosEater through case studies on small- and large-scale Kubernetes systems. The results demonstrate that it consistently completes reasonable CE cycles with significantly low time and monetary costs. Its cycles are also qualitatively validated by human engineers and LLMs.",
    "translation": "标题：基于大语言模型的全自动混沌工程：助力低成本构建高韧性软件系统\n\n摘要：混沌工程是一种旨在提升分布式系统韧性的工程技术，其通过主动向系统注入故障来测试韧性、发现潜在缺陷，并在生产环境故障发生前进行修复。现有混沌工程工具已能自动化执行预定义的实验方案，但实验规划与基于结果的系统改进仍依赖人工操作。这些过程不仅劳动密集，还需要跨领域专业知识。为突破这些限制，实现低成本普及化系统韧性构建，本文提出ChaosEater系统，利用大语言模型实现全流程自动化的混沌工程。该系统依据系统化混沌工程周期预设智能体工作流，并将流程中的细分任务分配给大语言模型。ChaosEater专注于基于Kubernetes的软件系统混沌工程，其大语言模型通过需求定义、代码生成、测试调试等软件工程任务完成完整周期。我们通过不同规模的Kubernetes系统案例进行评估，结果表明该系统能以极低的时间和经济成本持续完成合理的混沌工程周期，其输出成果同时获得了人类工程师与大语言模型的质性验证。",
    "url": "https://huggingface.co/papers/2511.07865",
    "arxiv_url": "https://arxiv.org/abs/2511.07865"
  },
  {
    "title": "TopoPerception: A Shortcut-Free Evaluation of Global Visual Perception in Large Vision-Language Models",
    "summary": "Large Vision-Language Models (LVLMs) typically align visual features from an encoder with a pre-trained Large Language Model (LLM). However, this makes the visual perception module a bottleneck, which constrains the overall capabilities of LVLMs. Conventional evaluation benchmarks, while rich in visual semantics, often contain unavoidable local shortcuts that can lead to an overestimation of models' perceptual abilities. Here, we introduce TopoPerception, a benchmark that leverages topological properties to rigorously evaluate the global visual perception capabilities of LVLMs across various granularities. Since topology depends on the global structure of an image and is invariant to local features, TopoPerception enables a shortcut-free assessment of global perception, fundamentally distinguishing it from semantically rich tasks. We evaluate state-of-the-art models on TopoPerception and find that even at the coarsest perceptual granularity, all models perform no better than random chance, indicating a profound inability to perceive global visual features. Notably, a consistent trend emerge within model families: more powerful models with stronger reasoning capabilities exhibit lower accuracy. This suggests that merely scaling up models is insufficient to address this deficit and may even exacerbate it. Progress may require new training paradigms or architectures. TopoPerception not only exposes a critical bottleneck in current LVLMs but also offers a lens and direction for improving their global visual perception. The data and code are publicly available at: https://github.com/Wenhao-Zhou/TopoPerception.",
    "translation": "标题：TopoPerception：大视觉语言模型中全局视觉感知的无捷径评估\n\n摘要：大视觉语言模型通常将视觉编码器提取的特征与预训练大语言模型对齐。然而这种做法使视觉感知模块成为性能瓶颈，制约了模型的整体能力。传统评估基准虽富含视觉语义信息，但常存在不可避免的局部捷径，可能导致高估模型的感知能力。本文提出TopoPerception基准，利用拓扑特性在多粒度层面严格评估大视觉语言模型的全局视觉感知能力。由于拓扑结构依赖于图像全局特征且对局部变化保持恒定，该基准能实现无捷径的全局感知评估，与语义密集型任务形成本质区别。我们在TopoPerception上评估前沿模型发现：即使在最粗粒度层面，所有模型表现均不优于随机猜测，揭示其全局视觉感知能力存在严重缺陷。值得注意的是，模型家族内部呈现一致趋势：推理能力越强的模型准确率反而越低。这表明单纯扩大模型规模不仅无法弥补该缺陷，甚至可能加剧问题，突破瓶颈需探索新型训练范式或架构。TopoPerception不仅揭示了当前大视觉语言模型的关键瓶颈，更为提升其全局视觉感知能力提供了观测视角与改进方向。数据与代码已公开于：https://github.com/Wenhao-Zhou/TopoPerception。",
    "url": "https://huggingface.co/papers/2511.11831",
    "arxiv_url": "https://arxiv.org/abs/2511.11831"
  }
]