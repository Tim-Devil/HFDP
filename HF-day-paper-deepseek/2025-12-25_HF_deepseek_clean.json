[
  {
    "title": "TurboDiffusion: Accelerating Video Diffusion Models by 100-200 Times",
    "summary": "We introduce TurboDiffusion, a video generation acceleration framework that can speed up end-to-end diffusion generation by 100-200x while maintaining video quality. TurboDiffusion mainly relies on several components for acceleration: (1) Attention acceleration: TurboDiffusion uses low-bit SageAttention and trainable Sparse-Linear Attention (SLA) to speed up attention computation. (2) Step distillation: TurboDiffusion adopts rCM for efficient step distillation. (3) W8A8 quantization: TurboDiffusion quantizes model parameters and activations to 8 bits to accelerate linear layers and compress the model. In addition, TurboDiffusion incorporates several other engineering optimizations.\n  We conduct experiments on the Wan2.2-I2V-14B-720P, Wan2.1-T2V-1.3B-480P, Wan2.1-T2V-14B-720P, and Wan2.1-T2V-14B-480P models. Experimental results show that TurboDiffusion achieves 100-200x speedup for video generation even on a single RTX 5090 GPU, while maintaining comparable video quality. The GitHub repository, which includes model checkpoints and easy-to-use code, is available at https://github.com/thu-ml/TurboDiffusion.",
    "translation": "标题：TurboDiffusion：将视频扩散模型加速100-200倍\n\n摘要：本文提出TurboDiffusion，一种视频生成加速框架，能够在保持视频质量的同时将端到端扩散生成过程加速100-200倍。TurboDiffusion主要依赖以下组件实现加速：（1）注意力加速：采用低比特SageAttention与可训练稀疏线性注意力（SLA）来加速注意力计算；（2）步数蒸馏：通过改进的rCM方法实现高效的步数蒸馏；（3）W8A8量化：将模型参数和激活值量化为8位，以加速线性层计算并压缩模型规模。此外，TurboDiffusion还融合了多项工程优化技术。我们在Wan2.2-I2V-14B-720P、Wan2.1-T2V-1.3B-480P、Wan2.1-T2V-14B-720P及Wan2.1-T2V-14B-480P模型上进行了实验验证。结果表明，即使在单张RTX 5090 GPU上，TurboDiffusion仍能实现100-200倍的视频生成加速，同时保持可比的视频生成质量。相关GitHub仓库已开源，包含模型检查点与易用代码，访问地址为：https://github.com/thu-ml/TurboDiffusion。",
    "url": "https://huggingface.co/papers/2512.16093",
    "arxiv_url": "https://arxiv.org/abs/2512.16093"
  },
  {
    "title": "Learning to Reason in 4D: Dynamic Spatial Understanding for Vision Language Models",
    "summary": "Vision-language models (VLM) excel at general understanding yet remain weak at dynamic spatial reasoning (DSR), i.e., reasoning about the evolvement of object geometry and relationship in 3D space over time, largely due to the scarcity of scalable 4D-aware training resources. To bridge this gap across aspects of dataset, benchmark and model, we introduce DSR Suite. First, we propose an automated pipeline that generates multiple-choice question-answer pairs from in-the-wild videos for DSR. By leveraging modern vision foundation models, the pipeline extracts rich geometric and motion information, including camera poses, local point clouds, object masks, orientations, and 3D trajectories. These geometric cues enable the construction of DSR-Train for learning and further human-refined DSR-Bench for evaluation. Compared with previous works, our data emphasize (i) in-the-wild video sources, (ii) object- and scene-level 3D requirements, (iii) viewpoint transformations, (iv) multi-object interactions, and (v) fine-grained, procedural answers. Beyond data, we propose a lightweight Geometry Selection Module (GSM) to seamlessly integrate geometric priors into VLMs, which condenses question semantics and extracts question-relevant knowledge from pretrained 4D reconstruction priors into a compact set of geometry tokens. This targeted extraction avoids overwhelming the model with irrelevant knowledge. Experiments show that integrating DSR-Train and GSM into Qwen2.5-VL-7B significantly enhances its dynamic spatial reasoning capability, while maintaining accuracy on general video understanding benchmarks.",
    "translation": "标题：面向视觉语言模型的四维推理学习：动态空间理解研究\n\n摘要：视觉语言模型在通用理解任务中表现出色，但在动态空间推理方面仍存在明显不足——即对三维空间中物体几何属性与相互关系随时间演变过程的推理能力较弱，这主要源于可扩展的四维感知训练资源的匮乏。为从数据集、基准测试和模型三个维度系统性地弥合这一差距，我们提出了动态空间推理套件。首先，我们设计了一种自动化流程，能够从真实场景视频中生成针对动态空间推理的多选题问答对。该流程通过整合现代视觉基础模型，提取丰富的几何与运动信息，包括相机位姿、局部点云、物体掩码、空间朝向以及三维运动轨迹。这些几何线索既支撑了用于模型训练的DSR-Train数据集构建，也形成了经人工精修的评估基准DSR-Bench。与已有研究相比，我们的数据强调以下特性：（1）真实场景视频源；（2）物体与场景层级的三维空间约束；（3）视角变换；（4）多物体交互；（5）细粒度、过程化的答案设计。在数据资源之外，我们提出了一种轻量化的几何选择模块，该模块能够将几何先验知识无缝集成至视觉语言模型中。其核心机制是通过压缩问题语义，从预训练的四维重建先验中提取与问题相关的知识，并将其编码为紧凑的几何标记集合。这种定向提取策略有效避免了无关知识对模型造成的干扰。实验表明，将DSR-Train数据集与几何选择模块集成至Qwen2.5-VL-7B模型后，其动态空间推理能力得到显著提升，同时在通用视频理解基准测试中保持了原有的性能水平。",
    "url": "https://huggingface.co/papers/2512.20557",
    "arxiv_url": "https://arxiv.org/abs/2512.20557"
  },
  {
    "title": "DreaMontage: Arbitrary Frame-Guided One-Shot Video Generation",
    "summary": "The \"one-shot\" technique represents a distinct and sophisticated aesthetic in filmmaking. However, its practical realization is often hindered by prohibitive costs and complex real-world constraints. Although emerging video generation models offer a virtual alternative, existing approaches typically rely on naive clip concatenation, which frequently fails to maintain visual smoothness and temporal coherence. In this paper, we introduce DreaMontage, a comprehensive framework designed for arbitrary frame-guided generation, capable of synthesizing seamless, expressive, and long-duration one-shot videos from diverse user-provided inputs. To achieve this, we address the challenge through three primary dimensions. (i) We integrate a lightweight intermediate-conditioning mechanism into the DiT architecture. By employing an Adaptive Tuning strategy that effectively leverages base training data, we unlock robust arbitrary-frame control capabilities. (ii) To enhance visual fidelity and cinematic expressiveness, we curate a high-quality dataset and implement a Visual Expression SFT stage. In addressing critical issues such as subject motion rationality and transition smoothness, we apply a Tailored DPO scheme, which significantly improves the success rate and usability of the generated content. (iii) To facilitate the production of extended sequences, we design a Segment-wise Auto-Regressive (SAR) inference strategy that operates in a memory-efficient manner. Extensive experiments demonstrate that our approach achieves visually striking and seamlessly coherent one-shot effects while maintaining computational efficiency, empowering users to transform fragmented visual materials into vivid, cohesive one-shot cinematic experiences.",
    "translation": "标题：DreaMontage：基于任意帧引导的单镜头视频生成框架\n\n摘要：\"单镜头\"技术代表了电影制作中一种独特而精妙的美学风格。然而，其实践应用常受限于高昂成本与复杂的现实条件制约。尽管新兴的视频生成模型提供了虚拟化解决方案，但现有方法通常依赖于简单的片段拼接，往往难以保持视觉流畅性与时序连贯性。本文提出DreaMontage——一个专为任意帧引导生成设计的综合框架，能够基于用户提供的多样化输入，合成无缝、富有表现力且时长长的单镜头视频。为实现这一目标，我们通过三个核心维度解决技术挑战：（一）在DiT架构中集成轻量级中间条件调节机制。通过采用能有效利用基础训练数据的自适应调优策略，我们实现了鲁棒的任意帧控制能力。（二）为提升视觉保真度与电影表现力，我们构建了高质量数据集并实施视觉表达监督微调阶段。针对主体运动合理性与转场平滑性等关键问题，我们采用定制化的直接偏好优化方案，显著提升了生成内容的成功率与可用性。（三）为支持长序列生成，我们设计了分段自回归推理策略，以内存高效的方式实现扩展生成。大量实验表明，我们的方法在保持计算效率的同时，能够实现视觉惊艳且无缝连贯的单镜头效果，使用户能够将碎片化的视觉素材转化为生动、连贯的单镜头电影体验。",
    "url": "https://huggingface.co/papers/2512.21252",
    "arxiv_url": "https://arxiv.org/abs/2512.21252"
  },
  {
    "title": "T2AV-Compass: Towards Unified Evaluation for Text-to-Audio-Video Generation",
    "summary": "Text-to-Audio-Video (T2AV) generation aims to synthesize temporally coherent video and semantically synchronized audio from natural language, yet its evaluation remains fragmented, often relying on unimodal metrics or narrowly scoped benchmarks that fail to capture cross-modal alignment, instruction following, and perceptual realism under complex prompts. To address this limitation, we present T2AV-Compass, a unified benchmark for comprehensive evaluation of T2AV systems, consisting of 500 diverse and complex prompts constructed via a taxonomy-driven pipeline to ensure semantic richness and physical plausibility. Besides, T2AV-Compass introduces a dual-level evaluation framework that integrates objective signal-level metrics for video quality, audio quality, and cross-modal alignment with a subjective MLLM-as-a-Judge protocol for instruction following and realism assessment. Extensive evaluation of 11 representative T2AVsystems reveals that even the strongest models fall substantially short of human-level realism and cross-modal consistency, with persistent failures in audio realism, fine-grained synchronization, instruction following, etc. These results indicate significant improvement room for future models and highlight the value of T2AV-Compass as a challenging and diagnostic testbed for advancing text-to-audio-video generation.",
    "translation": "标题：T2AV-Compass：迈向文本-音频-视频生成的统一评估\n\n摘要：文本-音频-视频（T2AV）生成旨在从自然语言合成时序连贯的视频与语义同步的音频，然而其评估仍处于碎片化状态，常依赖单模态指标或范围狭窄的基准，难以捕捉复杂提示下的跨模态对齐、指令跟随及感知真实性。为应对此局限，本文提出T2AV-Compass——一个用于全面评估T2AV系统的统一基准。该基准包含500个多样化且复杂的提示，通过分类学驱动的流程构建，以确保语义丰富性与物理合理性。同时，T2AV-Compass引入双层评估框架，整合了针对视频质量、音频质量及跨模态对齐的客观信号级指标，以及用于指令跟随和真实性评估的主观“大语言模型即评判者”协议。对11个代表性T2AV系统的广泛评估表明，即使最强模型仍远未达到人类水平的真实性与跨模态一致性，在音频真实性、细粒度同步、指令跟随等方面存在持续缺陷。这些结果揭示了未来模型的显著改进空间，并凸显了T2AV-Compass作为推动文本-音频-视频生成发展的挑战性诊断测试平台的价值。",
    "url": "https://huggingface.co/papers/2512.21094",
    "arxiv_url": "https://arxiv.org/abs/2512.21094"
  },
  {
    "title": "Beyond Memorization: A Multi-Modal Ordinal Regression Benchmark to Expose Popularity Bias in Vision-Language Models",
    "summary": "We expose a significant popularity bias in state-of-the-art vision-language models (VLMs), which achieve up to 34% higher accuracy on famous buildings compared to ordinary ones, indicating a reliance on memorization over generalizable understanding. To systematically investigate this, we introduce the largest open benchmark for this task: the YearGuessr dataset, a collection of 55,546 building images with multi-modal attributes from 157 countries, annotated with continuous ordinal labels of their construction year (1001-2024), GPS data, and page-view counts as a proxy for popularity. Using this dataset, we frame the construction year prediction task as ordinal regression and introduce popularity-aware interval accuracy metrics to quantify this bias. Our resulting benchmark of 30+ models, including our YearCLIP model, confirms that VLMs excel on popular, memorized items but struggle significantly with unrecognized subjects, exposing a critical flaw in their reasoning capabilities. Project page: https://sytwu.github.io/BeyondMemo/",
    "translation": "标题：超越记忆：揭示视觉语言模型中流行度偏差的多模态序数回归基准\n\n摘要：我们在前沿视觉语言模型中发现显著的流行度偏差，这些模型在著名建筑上的准确率比普通建筑高出34%，表明其依赖记忆而非可泛化的理解能力。为系统研究此问题，我们构建了该任务规模最大的开放基准：YearGuessr数据集，包含来自157个国家的55,546张建筑图像，每张图像均标注了连续序数标签（建造年份1001-2024年）、GPS数据，并以页面浏览量作为流行度代理指标。基于该数据集，我们将建造年份预测任务构建为序数回归问题，并提出融合流行度感知的区间准确率度量方法以量化此类偏差。通过对30余个模型（包括我们提出的YearCLIP模型）的基准测试证实，视觉语言模型擅长处理流行且被记忆的实体，而对非知名对象的识别能力显著不足，这揭示了其推理能力存在根本缺陷。项目页面：https://sytwu.github.io/BeyondMemo/",
    "url": "https://huggingface.co/papers/2512.21337",
    "arxiv_url": "https://arxiv.org/abs/2512.21337"
  },
  {
    "title": "Nemotron 3 Nano: Open, Efficient Mixture-of-Experts Hybrid Mamba-Transformer Model for Agentic Reasoning",
    "summary": "We present Nemotron 3 Nano 30B-A3B, a Mixture-of-Experts hybrid Mamba-Transformer language model. Nemotron 3 Nano was pretrained on 25 trillion text tokens, including more than 3 trillion new unique tokens over Nemotron 2, followed by supervised fine tuning and large-scale RL on diverse environments. Nemotron 3 Nano achieves better accuracy than our previous generation Nemotron 2 Nano while activating less than half of the parameters per forward pass. It achieves up to 3.3x higher inference throughput than similarly-sized open models like GPT-OSS-20B and Qwen3-30B-A3B-Thinking-2507, while also being more accurate on popular benchmarks. Nemotron 3 Nano demonstrates enhanced agentic, reasoning, and chat abilities and supports context lengths up to 1M tokens. We release both our pretrained Nemotron 3 Nano 30B-A3B Base and post-trained Nemotron 3 Nano 30B-A3B checkpoints on Hugging Face.",
    "translation": "标题：Nemotron 3 Nano：面向智能体推理的开放高效混合专家Mamba-Transformer模型\n\n摘要：本文介绍Nemotron 3 Nano 30B-A3B模型，这是一种采用混合专家架构的Mamba-Transformer混合语言模型。该模型在25万亿文本标记（其中包含超过3万亿相较于Nemotron 2新增的独特标记）上进行预训练，随后在多类环境中进行监督微调与大规模强化学习。Nemotron 3 Nano在每次前向传播中激活的参数量不足半数的情况下，仍实现了比前代Nemotron 2 Nano更优的准确率。与GPT-OSS-20B、Qwen3-30B-A3B-Thinking-2507等规模相近的开源模型相比，其推理吞吐量最高可提升3.3倍，同时在主流基准测试中表现更精准。Nemotron 3 Nano展现出增强的智能体交互、推理及对话能力，并支持高达100万标记的上下文长度。我们已在Hugging Face平台发布预训练版本Nemotron 3 Nano 30B-A3B Base与训练后版本Nemotron 3 Nano 30B-A3B的模型检查点。",
    "url": "https://huggingface.co/papers/2512.20848",
    "arxiv_url": "https://arxiv.org/abs/2512.20848"
  },
  {
    "title": "HiStream: Efficient High-Resolution Video Generation via Redundancy-Eliminated Streaming",
    "summary": "High-resolution video generation, while crucial for digital media and film, is computationally bottlenecked by the quadratic complexity of diffusion models, making practical inference infeasible. To address this, we introduce HiStream, an efficient autoregressive framework that systematically reduces redundancy across three axes: i) Spatial Compression: denoising at low resolution before refining at high resolution with cached features; ii) Temporal Compression: a chunk-by-chunk strategy with a fixed-size anchor cache, ensuring stable inference speed; and iii) Timestep Compression: applying fewer denoising steps to subsequent, cache-conditioned chunks. On 1080p benchmarks, our primary HiStream model (i+ii) achieves state-of-the-art visual quality while demonstrating up to 76.2x faster denoising compared to the Wan2.1 baseline and negligible quality loss. Our faster variant, HiStream+, applies all three optimizations (i+ii+iii), achieving a 107.5x acceleration over the baseline, offering a compelling trade-off between speed and quality, thereby making high-resolution video generation both practical and scalable.",
    "translation": "标题：HiStream：通过冗余消除流式处理实现高效高分辨率视频生成\n\n摘要：高分辨率视频生成对数字媒体和电影制作至关重要，但扩散模型的二次计算复杂度导致其成为计算瓶颈，使得实际推理难以实现。为解决这一问题，我们提出了HiStream——一种高效的自回归框架，该系统性地从三个维度消除冗余：i) 空间压缩：在低分辨率下进行去噪，再利用缓存特征进行高分辨率细化；ii) 时间压缩：采用固定尺寸锚点缓存的逐块处理策略，确保稳定的推理速度；iii) 时间步压缩：对后续基于缓存条件生成的视频块应用更少的去噪步骤。在1080p基准测试中，我们的核心HiStream模型（i+ii）在实现最先进视觉质量的同时，相比Wan2.1基线模型去噪速度提升最高达76.2倍，且质量损失可忽略不计。我们的加速变体HiStream+整合全部三项优化（i+ii+iii），相比基线实现107.5倍加速，在速度与质量间取得了卓越的平衡，从而使高分辨率视频生成兼具实用性与可扩展性。",
    "url": "https://huggingface.co/papers/2512.21338",
    "arxiv_url": "https://arxiv.org/abs/2512.21338"
  },
  {
    "title": "NVIDIA Nemotron 3: Efficient and Open Intelligence",
    "summary": "We introduce the Nemotron 3 family of models - Nano, Super, and Ultra. These models deliver strong agentic, reasoning, and conversational capabilities. The Nemotron 3 family uses a Mixture-of-Experts hybrid Mamba-Transformer architecture to provide best-in-class throughput and context lengths of up to 1M tokens. Super and Ultra models are trained with NVFP4 and incorporate LatentMoE, a novel approach that improves model quality. The two larger models also include MTP layers for faster text generation. All Nemotron 3 models are post-trained using multi-environment reinforcement learning enabling reasoning, multi-step tool use, and support granular reasoning budget control. Nano, the smallest model, outperforms comparable models in accuracy while remaining extremely cost-efficient for inference. Super is optimized for collaborative agents and high-volume workloads such as IT ticket automation. Ultra, the largest model, provides state-of-the-art accuracy and reasoning performance. Nano is released together with its technical report and this white paper, while Super and Ultra will follow in the coming months. We will openly release the model weights, pre- and post-training software, recipes, and all data for which we hold redistribution rights.",
    "translation": "标题：NVIDIA Nemotron 3：高效开放的智能模型\n\n摘要：我们推出Nemotron 3系列模型——Nano、Super和Ultra。这些模型具备强大的智能体交互、推理和对话能力。该系列采用混合专家（Mixture-of-Experts）与Mamba-Transformer融合架构，实现了业界领先的吞吐量，并支持高达100万标记的上下文长度。Super和Ultra模型使用NVFP4精度进行训练，并引入创新性方法LatentMoE以提升模型质量。两款较大模型还搭载了MTP层以加速文本生成。所有Nemotron 3模型均通过多环境强化学习进行后训练，使其具备复杂推理、多步骤工具调用能力，并支持细粒度推理资源控制。最小模型Nano在保持极高推理成本效益的同时，其准确率优于同类模型；Super专为协作智能体和高负载场景（如IT工单自动化）优化；最大模型Ultra则提供了最先进的准确率与推理性能。Nano模型已与其技术报告及本白皮书同步发布，Super和Ultra模型将于未来数月内陆续推出。我们将公开模型权重、训练前后软件工具、训练方案以及所有具备再分发权的数据。",
    "url": "https://huggingface.co/papers/2512.20856",
    "arxiv_url": "https://arxiv.org/abs/2512.20856"
  },
  {
    "title": "TokSuite: Measuring the Impact of Tokenizer Choice on Language Model Behavior",
    "summary": "Tokenizers provide the fundamental basis through which text is represented and processed by language models (LMs). Despite the importance of tokenization, its role in LM performance and behavior is poorly understood due to the challenge of measuring the impact of tokenization in isolation. To address this need, we present TokSuite, a collection of models and a benchmark that supports research into tokenization's influence on LMs. Specifically, we train fourteen models that use different tokenizers but are otherwise identical using the same architecture, dataset, training budget, and initialization. Additionally, we curate and release a new benchmark that specifically measures model performance subject to real-world perturbations that are likely to influence tokenization. Together, TokSuite allows robust decoupling of the influence of a model's tokenizer, supporting a series of novel findings that elucidate the respective benefits and shortcomings of a wide range of popular tokenizers.",
    "translation": "标题：TokSuite：衡量分词器选择对语言模型行为的影响\n\n摘要：分词器为语言模型（LMs）处理与表示文本提供了基础支撑。尽管分词过程至关重要，但由于难以单独衡量分词选择的影响，其在语言模型性能与行为中的作用尚未得到充分理解。为应对这一需求，我们提出了TokSuite——一个支持分词对语言模型影响研究的模型集合与基准测试框架。具体而言，我们训练了十四种采用不同分词器但其他条件完全一致的模型，这些模型在架构、数据集、训练预算和初始化方式上均保持一致。此外，我们构建并发布了一个新的基准测试集，专门用于衡量模型在可能影响分词效果的真实场景扰动下的性能表现。TokSuite通过系统化设计实现了模型分词器影响的稳健解耦分析，进而支持了一系列创新发现，揭示了多种主流分词器各自的优势与局限。",
    "url": "https://huggingface.co/papers/2512.20757",
    "arxiv_url": "https://arxiv.org/abs/2512.20757"
  },
  {
    "title": "Learning from Next-Frame Prediction: Autoregressive Video Modeling Encodes Effective Representations",
    "summary": "Recent advances in pretraining general foundation models have significantly improved performance across diverse downstream tasks. While autoregressive (AR) generative models like GPT have revolutionized NLP, most visual generative pretraining methods still rely on BERT-style masked modeling, which often disregards the temporal information essential for video analysis. The few existing autoregressive visual pretraining methods suffer from issues such as inaccurate semantic localization and poor generation quality, leading to poor semantics. In this work, we propose NExT-Vid, a novel autoregressive visual generative pretraining framework that utilizes masked next-frame prediction to jointly model images and videos. NExT-Vid introduces a context-isolated autoregressive predictor to decouple semantic representation from target decoding, and a conditioned flow-matching decoder to enhance generation quality and diversity. Through context-isolated flow-matching pretraining, our approach achieves strong representations. Extensive experiments on large-scale pretrained models demonstrate that our proposed method consistently outperforms previous generative pretraining methods for visual representation learning via attentive probing in downstream classification.",
    "translation": "标题：基于下一帧预测的学习：自回归视频建模编码有效表征\n\n摘要：通用基础模型预训练的最新进展显著提升了各类下游任务的性能。尽管如GPT等自回归生成模型已在自然语言处理领域引发革命，但大多数视觉生成式预训练方法仍依赖BERT风格的掩码建模，这种方法常忽略视频分析所必需的时间信息。现有的少数自回归视觉预训练方法存在语义定位不准、生成质量欠佳等问题，导致语义表征能力不足。本研究提出NExT-Vid——一种新颖的自回归视觉生成式预训练框架，通过掩码下一帧预测实现对图像与视频的联合建模。该框架引入上下文隔离的自回归预测器以解耦语义表征与目标解码过程，并采用条件流匹配解码器以提升生成质量与多样性。通过上下文隔离的流匹配预训练，本方法获得了强大的表征能力。基于大规模预训练模型的广泛实验表明，通过下游分类任务中的注意力探针评估，我们提出的方法在视觉表征学习方面持续优于以往的生成式预训练方法。",
    "url": "https://huggingface.co/papers/2512.21004",
    "arxiv_url": "https://arxiv.org/abs/2512.21004"
  },
  {
    "title": "From Word to World: Can Large Language Models be Implicit Text-based World Models?",
    "summary": "Agentic reinforcement learning increasingly relies on experience-driven scaling, yet real-world environments remain non-adaptive, limited in coverage, and difficult to scale. World models offer a potential way to improve learning efficiency through simulated experience, but it remains unclear whether large language models can reliably serve this role and under what conditions they meaningfully benefit agents. We study these questions in text-based environments, which provide a controlled setting to reinterpret language modeling as next-state prediction under interaction. We introduce a three-level framework for evaluating LLM-based world models: (i) fidelity and consistency, (ii) scalability and robustness, and (iii) agent utility. Across five representative environments, we find that sufficiently trained world models maintain coherent latent state, scale predictably with data and model size, and improve agent performance via action verification, synthetic trajectory generation, and warm-starting reinforcement learning. Meanwhile, these gains depend critically on behavioral coverage and environment complexity, delineating clear boundry on when world modeling effectively supports agent learning.",
    "translation": "标题：从词语到世界：大型语言模型能否成为隐式的基于文本的世界模型？\n\n摘要：智能体强化学习日益依赖经验驱动的规模化扩展，然而现实世界环境仍存在非适应性、覆盖范围有限和难以扩展的问题。世界模型通过模拟经验提供了提升学习效率的潜在路径，但大型语言模型能否可靠承担这一角色，以及在何种条件下能实质性地提升智能体性能，目前尚未明确。本研究在基于文本的环境中探讨这些问题——该环境为将语言建模重新诠释为交互情境下的状态预测提供了受控实验场。我们提出三层评估框架用于检验基于大型语言模型的世界模型：（一）保真度与一致性，（二）可扩展性与鲁棒性，（三）智能体效用。在五个典型环境中的实验表明，经过充分训练的世界模型能够保持连贯的潜在状态，其性能随数据量与模型规模呈现可预测的扩展，并能通过动作验证、合成轨迹生成以及强化学习预热启动等方式提升智能体表现。同时，这些增益效果严格依赖于行为覆盖度与环境复杂度，由此明确了世界模型有效支持智能体学习的边界条件。",
    "url": "https://huggingface.co/papers/2512.18832",
    "arxiv_url": "https://arxiv.org/abs/2512.18832"
  },
  {
    "title": "DramaBench: A Six-Dimensional Evaluation Framework for Drama Script Continuation",
    "summary": "Drama script continuation requires models to maintain character consistency, advance plot coherently, and preserve dramatic structurecapabilities that existing benchmarks fail to evaluate comprehensively. We present DramaBench, the first large-scale benchmark for evaluating drama script continuation across six independent dimensions: Format Standards, Narrative Efficiency, Character Consistency, Emotional Depth, Logic Consistency, and Conflict Handling. Our framework combines rulebased analysis with LLM-based labeling and statistical metrics, ensuring objective and reproducible evaluation. We conduct comprehensive evaluation of 8 state-of-the-art language models on 1,103 scripts (8,824 evaluations total), with rigorous statistical significance testing (252 pairwise comparisons, 65.9% significant) and human validation (188 scripts, substantial agreement on 3/5 dimensions). Our ablation studies confirm all six dimensions capture independent quality aspects (mean | r | = 0.020). DramaBench provides actionable, dimensionspecific feedback for model improvement and establishes a rigorous standard for creative writing evaluation.",
    "translation": "标题：DramaBench：面向剧本续写的六维评估框架\n\n摘要：剧本续写任务要求模型保持角色一致性、推进情节连贯性并保留戏剧结构，而现有基准测试未能全面评估这些能力。本文提出DramaBench——首个面向剧本续写任务的大规模评估基准，涵盖六个独立维度：格式规范、叙事效率、角色一致性、情感深度、逻辑一致性与冲突处理。本框架结合基于规则的分析、大语言模型标注与统计度量，确保评估过程的客观性与可复现性。我们在1,103个剧本样本（总计8,824次评估）上对8个前沿语言模型进行综合评估，采用严格的统计显著性检验（252组配对比较，65.9%具统计显著性）及人工验证（188个剧本，其中3/5维度达到显著一致性）。消融实验证实六个维度均捕捉独立的文本质量特征（平均|r|=0.020）。DramaBench可为模型改进提供可操作的维度特异性反馈，并为创造性写作评估建立严谨标准。",
    "url": "https://huggingface.co/papers/2512.19012",
    "arxiv_url": "https://arxiv.org/abs/2512.19012"
  },
  {
    "title": "SWE-EVO: Benchmarking Coding Agents in Long-Horizon Software Evolution Scenarios",
    "summary": "Existing benchmarks for AI coding agents focus on isolated, single-issue tasks such as fixing a bug or implementing a small feature. However, real-world software engineering is fundamentally a long-horizon endeavor: developers must interpret high-level requirements, plan coordinated changes across many files, and evolve codebases over multiple iterations while preserving existing functionality. We introduce SWE-EVO, a benchmark that evaluates agents on this long-horizon software evolution challenge. Constructed from release notes and version histories of seven mature open-source Python projects, Tool comprises 48 evolution tasks that require agents to implement multi-step modifications spanning an average of 21 files, validated against comprehensive test suites averaging 874 tests per instance. Experiments with state-of-the-art models reveal a striking capability gap: even GPT-5 with OpenHands achieves only a 21 percent resolution rate on Tool, compared to 65 percent on the single-issue SWE-Bench Verified. This demonstrates that current agents struggle with sustained, multi-file reasoning. We also propose Fix Rate, a fine-grained metric that captures partial progress toward solving these complex, long-horizon tasks.",
    "translation": "标题：SWE-EVO：在长周期软件演化场景中对编码智能体进行基准测试\n\n摘要：现有针对AI编码智能体的基准测试主要集中于孤立、单一问题的任务，例如修复错误或实现小型功能。然而，现实世界的软件工程本质上是一项长周期任务：开发者必须理解高层次需求，规划跨多个文件的协调变更，并在保持现有功能的同时通过多次迭代演进代码库。我们提出了SWE-EVO，这是一个用于评估智能体在长周期软件演化挑战中表现的基准测试。该基准基于七个成熟开源Python项目的发布说明和版本历史构建，包含48个演化任务，要求智能体实现平均涉及21个文件的多步骤修改，并通过平均每个实例包含874个测试的全面测试套件进行验证。对前沿模型的实验揭示了一个显著的能力差距：即使是配备OpenHands的GPT-5，在SWE-EVO上的解决率也仅为21%，而在单一问题基准SWE-Bench Verified上则达到65%。这表明当前智能体在持续、多文件的推理方面存在困难。我们还提出了修复率这一细粒度指标，用于捕捉解决这些复杂长周期任务过程中的部分进展。",
    "url": "https://huggingface.co/papers/2512.18470",
    "arxiv_url": "https://arxiv.org/abs/2512.18470"
  },
  {
    "title": "Streaming Video Instruction Tuning",
    "summary": "We present Streamo, a real-time streaming video LLM that serves as a general-purpose interactive assistant. Unlike existing online video models that focus narrowly on question answering or captioning, Streamo performs a broad spectrum of streaming video tasks, including real-time narration, action understanding, event captioning, temporal event grounding, and time-sensitive question answering. To develop such versatility, we construct Streamo-Instruct-465K, a large-scale instruction-following dataset tailored for streaming video understanding. The dataset covers diverse temporal contexts and multi-task supervision, enabling unified training across heterogeneous streaming tasks. After training end-to-end on the instruction-following dataset through a streamlined pipeline, Streamo exhibits strong temporal reasoning, responsive interaction, and broad generalization across a variety of streaming benchmarks. Extensive experiments show that Streamo bridges the gap between offline video perception models and real-time multimodal assistants, making a step toward unified, intelligent video understanding in continuous video streams.",
    "translation": "标题：流式视频指令调优\n\n摘要：本文提出Streamo——一个作为通用交互助手的实时流式视频大语言模型。与现有专注于问答或描述等单一功能的在线视频模型不同，Streamo能够执行广泛的流式视频任务，包括实时叙述、动作理解、事件描述、时序事件定位及时间敏感型问答。为实现这种多功能性，我们构建了Streamo-Instruct-465K——一个专为流式视频理解定制的大规模指令遵循数据集。该数据集涵盖多样化时序语境与多任务监督信号，支持跨异构流式任务的统一训练。通过端到端的精简训练流程，Streamo在指令数据集上训练后展现出强大的时序推理能力、即时响应特性以及在多种流式基准测试中的广泛泛化性能。大量实验表明，Streamo成功弥合了离线视频感知模型与实时多模态助手之间的鸿沟，为连续视频流中的统一智能视频理解迈出了关键一步。",
    "url": "https://huggingface.co/papers/2512.21334",
    "arxiv_url": "https://arxiv.org/abs/2512.21334"
  },
  {
    "title": "Multi-hop Reasoning via Early Knowledge Alignment",
    "summary": "Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for Large Language Models (LLMs) to address knowledge-intensive queries requiring domain-specific or up-to-date information. To handle complex multi-hop questions that are challenging for single-step retrieval, iterative RAG approaches incorporating reinforcement learning have been proposed. However, existing iterative RAG systems typically plan to decompose questions without leveraging information about the available retrieval corpus, leading to inefficient retrieval and reasoning chains that cascade into suboptimal performance. In this paper, we introduce Early Knowledge Alignment (EKA), a simple but effective module that aligns LLMs with retrieval set before planning in iterative RAG systems with contextually relevant retrieved knowledge. Extensive experiments on six standard RAG datasets demonstrate that by establishing a stronger reasoning foundation, EKA significantly improves retrieval precision, reduces cascading errors, and enhances both performance and efficiency. Our analysis from an entropy perspective demonstrate that incorporating early knowledge reduces unnecessary exploration during the reasoning process, enabling the model to focus more effectively on relevant information subsets. Moreover, EKA proves effective as a versatile, training-free inference strategy that scales seamlessly to large models. Generalization tests across diverse datasets and retrieval corpora confirm the robustness of our approach. Overall, EKA advances the state-of-the-art in iterative RAG systems while illuminating the critical interplay between structured reasoning and efficient exploration in reinforcement learning-augmented frameworks. The code is released at https://github.com/yxzwang/EarlyKnowledgeAlignment{Github}.",
    "translation": "标题：基于早期知识对齐的多跳推理方法\n\n摘要：检索增强生成技术已成为大语言模型处理需要领域专业知识或最新信息的密集知识型查询的重要范式。为应对单步检索难以处理的复杂多跳问题，研究者提出了结合强化学习的迭代式检索增强生成方法。然而，现有迭代系统通常在规划问题分解时未能充分利用检索语料库的元信息，导致检索效率低下，推理链中的误差累积最终影响整体性能。本文提出早期知识对齐模块——一种简单而有效的解决方案，通过在迭代系统中引入上下文相关的检索知识，使大语言模型在规划阶段前与检索集合实现对齐。在六个标准检索增强生成数据集上的实验表明，该模块通过建立更坚实的推理基础，显著提升了检索精度，减少了误差传递，同时改善了模型性能与效率。从信息熵视角的分析证明，早期知识的引入能够减少推理过程中不必要的探索，使模型更聚焦于相关信息的子集。此外，该模块作为无需训练的通用推理策略，可无缝扩展至大规模模型。跨数据集与检索语料的泛化测试验证了方法的鲁棒性。总体而言，早期知识对齐模块推动了迭代式检索增强生成技术的发展，同时揭示了强化学习增强框架中结构化推理与高效探索的关键交互机制。代码已发布于 https://github.com/yxzwang/EarlyKnowledgeAlignment{Github}。",
    "url": "https://huggingface.co/papers/2512.20144",
    "arxiv_url": "https://arxiv.org/abs/2512.20144"
  },
  {
    "title": "LLM Swiss Round: Aggregating Multi-Benchmark Performance via Competitive Swiss-System Dynamics",
    "summary": "The rapid proliferation of Large Language Models (LLMs) and diverse specialized benchmarks necessitates a shift from fragmented, task-specific metrics to a holistic, competitive ranking system that effectively aggregates performance across multiple ability dimensions. Primarily using static scoring, current evaluation methods are fundamentally limited. They struggle to determine the proper mix ratio across diverse benchmarks, and critically, they fail to capture a model's dynamic competitive fitness or its vulnerability when confronted with sequential, high-stakes tasks. To address this, we introduce the novel Competitive Swiss-System Dynamics (CSD) framework. CSD simulates a multi-round, sequential contest where models are dynamically paired across a curated sequence of benchmarks based on their accumulated win-loss record. And Monte Carlo Simulation (N=100,000 iterations) is used to approximate the statistically robust Expected Win Score (E[S_m]), which eliminates the noise of random pairing and early-round luck. Furthermore, we implement a Failure Sensitivity Analysis by parameterizing the per-round elimination quantity (T_k), which allows us to profile models based on their risk appetite--distinguishing between robust generalists and aggressive specialists. We demonstrate that CSD provides a more nuanced and context-aware ranking than traditional aggregate scoring and static pairwise models, representing a vital step towards risk-informed, next-generation LLM evaluation.",
    "translation": "标题：LLM瑞士轮：基于竞争性瑞士制动态的多基准性能聚合\n\n摘要：大语言模型（LLM）的快速涌现与多样化专业基准的激增，要求评估体系从碎片化的任务特定指标转向能够有效聚合多维度能力的整体性竞争排名系统。当前评估方法主要依赖静态评分，存在根本性局限：既难以确定跨异质基准的合理混合比例，更无法捕捉模型在连续高压力任务环境中的动态竞争适应性及其脆弱性。为此，我们提出创新的竞争性瑞士制动态框架。该框架通过模拟多轮次序列化竞赛，使模型根据累积胜负记录在精心设计的基准序列中动态配对竞技。我们采用蒙特卡洛模拟进行十万次迭代，以计算统计稳健的期望胜率得分，从而消除随机配对与早期轮次运气带来的噪声干扰。此外，我们通过参数化每轮淘汰数量实施失败敏感性分析，从而根据模型的风险偏好进行画像区分——识别稳健的通才型模型与激进的专才型模型。实验表明，相较于传统聚合评分与静态配对模型，竞争性瑞士制动态框架能提供更精细且情境感知的排名结果，标志着向风险感知的下一代LLM评估迈出关键一步。",
    "url": "https://huggingface.co/papers/2512.21010",
    "arxiv_url": "https://arxiv.org/abs/2512.21010"
  },
  {
    "title": "PhononBench:A Large-Scale Phonon-Based Benchmark for Dynamical Stability in Crystal Generation",
    "summary": "In this work, we introduce PhononBench, the first large-scale benchmark for dynamical stability in AI-generated crystals. Leveraging the recently developed MatterSim interatomic potential, which achieves DFT-level accuracy in phonon predictions across more than 10,000 materials, PhononBench enables efficient large-scale phonon calculations and dynamical-stability analysis for 108,843 crystal structures generated by six leading crystal generation models. PhononBench reveals a widespread limitation of current generative models in ensuring dynamical stability: the average dynamical-stability rate across all generated structures is only 25.83%, with the top-performing model, MatterGen, reaching just 41.0%. Further case studies show that in property-targeted generation-illustrated here by band-gap conditioning with MatterGen--the dynamical-stability rate remains as low as 23.5% even at the optimal band-gap condition of 0.5 eV. In space-group-controlled generation, higher-symmetry crystals exhibit better stability (e.g., cubic systems achieve rates up to 49.2%), yet the average stability across all controlled generations is still only 34.4%. An important additional outcome of this study is the identification of 28,119 crystal structures that are phonon-stable across the entire Brillouin zone, providing a substantial pool of reliable candidates for future materials exploration. By establishing the first large-scale dynamical-stability benchmark, this work systematically highlights the current limitations of crystal generation models and offers essential evaluation criteria and guidance for their future development toward the design and discovery of physically viable materials. All model-generated crystal structures, phonon calculation results, and the high-throughput evaluation workflows developed in PhononBench will be openly released at https://github.com/xqh19970407/PhononBench",
    "translation": "标题：PhononBench：面向晶体生成动态稳定性评估的大规模声子基准测试框架\n\n摘要：本研究提出了PhononBench——首个面向人工智能生成晶体的动态稳定性评估大规模基准测试框架。基于近期开发的MatterSim原子间势函数（在超过10,000种材料的声子预测中达到密度泛函理论精度），该框架对六种主流晶体生成模型产生的108,843个晶体结构实现了高效的大规模声子计算与动态稳定性分析。PhononBench揭示了当前生成模型在保障动态稳定性方面存在的普遍局限：所有生成结构的平均动态稳定率仅为25.83%，其中表现最优的MatterGen模型也仅达到41.0%。进一步的案例研究表明，在面向特定物性的生成任务中（以MatterGen的带隙条件生成为例），即使在0.5 eV的最优带隙条件下，动态稳定率仍低至23.5%。在空间群受控的生成任务中，高对称性晶体表现出更好的稳定性（如立方晶系稳定率可达49.2%），但所有受控生成的平均稳定率仍仅为34.4%。本研究的重要附加成果是识别出28,119个在全布里渊区内声子稳定的晶体结构，为未来材料探索提供了可靠的候选材料库。通过建立首个大规模动态稳定性基准，本工作系统揭示了当前晶体生成模型的局限性，并为推动其朝着设计物理可行材料的方向发展提供了关键评估标准与指导。所有模型生成的晶体结构、声子计算结果及PhononBench开发的高通量评估工作流将在https://github.com/xqh19970407/PhononBench 公开释放。",
    "url": "https://huggingface.co/papers/2512.21227",
    "arxiv_url": "https://arxiv.org/abs/2512.21227"
  }
]