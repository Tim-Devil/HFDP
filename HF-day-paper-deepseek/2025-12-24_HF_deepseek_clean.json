[
  {
    "title": "SemanticGen: Video Generation in Semantic Space",
    "summary": "State-of-the-art video generative models typically learn the distribution of video latents in the VAE space and map them to pixels using a VAE decoder. While this approach can generate high-quality videos, it suffers from slow convergence and is computationally expensive when generating long videos. In this paper, we introduce SemanticGen, a novel solution to address these limitations by generating videos in the semantic space. Our main insight is that, due to the inherent redundancy in videos, the generation process should begin in a compact, high-level semantic space for global planning, followed by the addition of high-frequency details, rather than directly modeling a vast set of low-level video tokens using bi-directional attention. SemanticGen adopts a two-stage generation process. In the first stage, a diffusion model generates compact semantic video features, which define the global layout of the video. In the second stage, another diffusion model generates VAE latents conditioned on these semantic features to produce the final output. We observe that generation in the semantic space leads to faster convergence compared to the VAE latent space. Our method is also effective and computationally efficient when extended to long video generation. Extensive experiments demonstrate that SemanticGen produces high-quality videos and outperforms state-of-the-art approaches and strong baselines.",
    "translation": "标题：SemanticGen：语义空间中的视频生成\n\n摘要：当前最先进的视频生成模型通常在变分自编码器（VAE）空间中学习视频潜在特征的分布，并通过VAE解码器将其映射为像素。尽管这种方法能够生成高质量视频，但其收敛速度较慢，且在生成长视频时计算成本高昂。本文提出SemanticGen，通过在语义空间中生成视频来解决这些局限性。我们的核心观点是：由于视频本身存在固有冗余性，生成过程应当从紧凑的高层语义空间开始进行全局规划，再逐步添加高频细节，而非直接使用双向注意力机制对大量低层视频标记进行建模。SemanticGen采用两阶段生成流程：第一阶段通过扩散模型生成紧凑的语义视频特征，这些特征定义了视频的全局布局；第二阶段由另一个扩散模型基于这些语义特征生成VAE潜在特征，最终合成输出视频。实验表明，与在VAE潜在空间中生成相比，语义空间生成能实现更快的收敛速度。该方法在扩展至长视频生成时仍保持高效性与计算效率。大量实验证明，SemanticGen能够生成高质量视频，其性能优于当前最先进的方法和强基线模型。",
    "url": "https://huggingface.co/papers/2512.20619",
    "arxiv_url": "https://arxiv.org/abs/2512.20619"
  },
  {
    "title": "Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies",
    "summary": "Existing reinforcement learning (RL) approaches treat large language models (LLMs) as a single unified policy, overlooking their internal mechanisms. Understanding how policy evolves across layers and modules is therefore crucial for enabling more targeted optimization and raveling out complex reasoning mechanisms. In this paper, we decompose the language model policy by leveraging the intrinsic split of the Transformer residual stream and the equivalence between the composition of hidden states with the unembedding matrix and the resulting samplable policy. This decomposition reveals Internal Layer Policies, corresponding to contributions from individual layers, and Internal Modular Policies, which align with the self-attention and feed-forward network (FFN) components within each layer. By analyzing the entropy of internal policy, we find that: (a) Early layers keep high entropy for exploration, top layers converge to near-zero entropy for refinement, with convergence patterns varying across model series. (b) LLama's prediction space rapidly converges in the final layer, whereas Qwen-series models, especially Qwen3, exhibit a more human-like, progressively structured reasoning pattern. Motivated by these findings, we propose Bottom-up Policy Optimization (BuPO), a novel RL paradigm that directly optimizes the internal layer policy during early training. By aligning training objective at lower layer, BuPO reconstructs foundational reasoning capabilities and achieves superior performance. Extensive experiments on complex reasoning benchmarks demonstrates the effectiveness of our method. Our code is available at https://github.com/Trae1ounG/BuPO.",
    "translation": "标题：自底向上策略优化：语言模型策略中隐含的内部策略\n\n摘要：现有强化学习方法将大语言模型视为单一整体策略，忽视了其内部机制。理解策略在不同层级与模块间的演化过程，对于实现更具针对性的优化及揭示复杂推理机制至关重要。本文通过利用Transformer残差流的固有分割特性，以及隐藏状态与解嵌入矩阵的组合等效于可采样策略的性质，对语言模型策略进行分解。该分解揭示了对应单层贡献的**内部层级策略**，以及对应每层中自注意力与前馈网络组件的**内部模块化策略**。通过分析内部策略的熵值变化，我们发现：（1）早期层保持高熵以支持探索，顶层则收敛至接近零熵以实现精细化调整，且收敛模式在不同模型系列中存在差异；（2）Llama模型的预测空间在最终层快速收敛，而Qwen系列模型（尤其是Qwen3）展现出更接近人类的渐进结构化推理模式。基于这些发现，我们提出**自底向上策略优化**——一种在早期训练阶段直接优化内部层级策略的新型强化学习范式。该方法通过在底层对齐训练目标，重构基础推理能力，从而获得更优性能。在复杂推理基准测试上的大量实验验证了本方法的有效性。代码已开源：https://github.com/Trae1ounG/BuPO。",
    "url": "https://huggingface.co/papers/2512.19673",
    "arxiv_url": "https://arxiv.org/abs/2512.19673"
  },
  {
    "title": "LongVideoAgent: Multi-Agent Reasoning with Long Videos",
    "summary": "Recent advances in multimodal LLMs and systems that use tools for long-video QA point to the promise of reasoning over hour-long episodes. However, many methods still compress content into lossy summaries or rely on limited toolsets, weakening temporal grounding and missing fine-grained cues. We propose a multi-agent framework in which a master LLM coordinates a grounding agent to localize question-relevant segments and a vision agent to extract targeted textual observations. The master agent plans with a step limit, and is trained with reinforcement learning to encourage concise, correct, and efficient multi-agent cooperation. This design helps the master agent focus on relevant clips via grounding, complements subtitles with visual detail, and yields interpretable trajectories. On our proposed LongTVQA and LongTVQA+ which are episode-level datasets aggregated from TVQA/TVQA+, our multi-agent system significantly outperforms strong non-agent baselines. Experiments also show reinforcement learning further strengthens reasoning and planning for the trained agent. Code and data will be shared at https://longvideoagent.github.io/.",
    "translation": "标题：LongVideoAgent：基于多智能体推理的长视频理解框架\n\n摘要：近年来，多模态大语言模型及基于工具的长视频问答系统的发展，为长达数小时的视频内容推理提供了新的可能。然而，现有方法仍多将视频内容压缩为有损摘要，或依赖有限工具集，导致时序定位能力弱化及细粒度信息丢失。本文提出一种多智能体框架：主控大语言模型协调定位智能体以确定问题相关片段，并调度视觉智能体提取针对性文本化视觉观测。主控智能体在步数限制下进行规划，并通过强化学习训练以促进简洁、准确且高效的多智能体协作。该设计使主控智能体能够借助定位机制聚焦相关片段，通过视觉细节补充字幕信息，并生成可解释的推理轨迹。在我们基于TVQA/TVQA+构建的剧集级数据集LongTVQA与LongTVQA+上，本多智能体系统显著优于强力的非智能体基线模型。实验同时表明，强化学习能进一步强化已训练智能体的推理与规划能力。代码与数据将在https://longvideoagent.github.io/公开。",
    "url": "https://huggingface.co/papers/2512.20618",
    "arxiv_url": "https://arxiv.org/abs/2512.20618"
  },
  {
    "title": "SpatialTree: How Spatial Abilities Branch Out in MLLMs",
    "summary": "Cognitive science suggests that spatial ability develops progressively-from perception to reasoning and interaction. Yet in multimodal LLMs (MLLMs), this hierarchy remains poorly understood, as most studies focus on a narrow set of tasks. We introduce SpatialTree, a cognitive-science-inspired hierarchy that organizes spatial abilities into four levels: low-level perception (L1), mental mapping (L2), simulation (L3), and agentic competence (L4). Based on this taxonomy, we construct the first capability-centric hierarchical benchmark, thoroughly evaluating mainstream MLLMs across 27 sub-abilities. The evaluation results reveal a clear structure: L1 skills are largely orthogonal, whereas higher-level skills are strongly correlated, indicating increasing interdependency. Through targeted supervised fine-tuning, we uncover a surprising transfer dynamic-negative transfer within L1, but strong cross-level transfer from low- to high-level abilities with notable synergy. Finally, we explore how to improve the entire hierarchy. We find that naive RL that encourages extensive \"thinking\" is unreliable: it helps complex reasoning but hurts intuitive perception. We propose a simple auto-think strategy that suppresses unnecessary deliberation, enabling RL to consistently improve performance across all levels. By building SpatialTree, we provide a proof-of-concept framework for understanding and systematically scaling spatial abilities in MLLMs.",
    "translation": "标题：SpatialTree：空间能力在多模态大语言模型中的层级化发展探究\n\n摘要：认知科学研究表明，空间能力的发展遵循从感知到推理再到交互的渐进过程。然而，在多模态大语言模型（MLLMs）中，这种层级结构尚未得到充分理解，现有研究多局限于有限的任务类型。本文提出SpatialTree——一个受认知科学启发的层级化框架，将空间能力划分为四个递进层级：基础感知（L1）、心理映射（L2）、动态模拟（L3）与具身交互（L4）。基于此分类体系，我们构建了首个以能力为中心的层级化基准测试，系统评估了主流MLLMs在27项子能力上的表现。评估结果揭示了清晰的能力结构：L1层技能呈现相对独立性，而高层级技能则表现出强相关性，表明能力间的相互依赖性随层级提升而增强。通过定向监督微调实验，我们发现了有趣的迁移规律：L1层存在负迁移现象，而从低层级到高层级则呈现显著的跨级正向迁移与协同效应。最后，我们探索了全层级能力的优化路径。研究发现，简单鼓励\"深度思考\"的强化学习策略并不可靠：虽能提升复杂推理能力，却会损害直觉感知表现。为此，我们提出一种自适应思考抑制策略，通过避免冗余计算，使强化学习能够在所有层级实现稳定性能提升。SpatialTree的建立为理解并系统化扩展MLLMs的空间能力提供了概念验证框架。",
    "url": "https://huggingface.co/papers/2512.20617",
    "arxiv_url": "https://arxiv.org/abs/2512.20617"
  },
  {
    "title": "MemEvolve: Meta-Evolution of Agent Memory Systems",
    "summary": "Self-evolving memory systems are unprecedentedly reshaping the evolutionary paradigm of large language model (LLM)-based agents. Prior work has predominantly relied on manually engineered memory architectures to store trajectories, distill experience, and synthesize reusable tools, enabling agents to evolve on the fly within environment interactions. However, this paradigm is fundamentally constrained by the staticity of the memory system itself: while memory facilitates agent-level evolving, the underlying memory architecture cannot be meta-adapted to diverse task contexts. To address this gap, we propose MemEvolve, a meta-evolutionary framework that jointly evolves agents' experiential knowledge and their memory architecture, allowing agent systems not only to accumulate experience but also to progressively refine how they learn from it. To ground MemEvolve in prior research and foster openness in future self-evolving systems, we introduce EvolveLab, a unified self-evolving memory codebase that distills twelve representative memory systems into a modular design space (encode, store, retrieve, manage), providing both a standardized implementation substrate and a fair experimental arena. Extensive evaluations on four challenging agentic benchmarks demonstrate that MemEvolve achieves (I) substantial performance gains, improving frameworks such as SmolAgent and Flash-Searcher by up to 17.06%; and (II) strong cross-task and cross-LLM generalization, designing memory architectures that transfer effectively across diverse benchmarks and backbone models.",
    "translation": "标题：MemEvolve：智能体记忆系统的元演化\n\n摘要：自演化记忆系统正以前所未有的方式重塑基于大语言模型（LLM）的智能体演化范式。先前研究主要依赖人工设计的记忆架构来存储轨迹、提炼经验并合成可重用工具，使智能体能够在环境交互中实时演化。然而，该范式本质上受限于记忆系统自身的静态性：虽然记忆促进了智能体层级的演化，但其底层记忆架构无法针对多样化任务情境进行元适应。为弥补这一局限，本文提出MemEvolve——一种元演化框架，通过联合演化智能体的经验知识及其记忆架构，使智能体系统不仅能积累经验，还能持续优化其经验学习机制。为将MemEvolve根植于现有研究并促进未来自演化系统的开放性，我们同步推出EvolveLab：一个统一的自演化记忆代码库，将十二种代表性记忆系统提炼为模块化设计空间（编码、存储、检索、管理），既提供标准化实现基础，也构建了公平的实验平台。在四项具有挑战性的智能体基准测试中的广泛实验表明，MemEvolve实现了：（I）显著的性能提升，将SmolAgent、Flash-Searcher等框架的性能最高提升17.06%；（II）强大的跨任务与跨LLM泛化能力，其设计的记忆架构能够有效迁移至不同基准测试与骨干模型。",
    "url": "https://huggingface.co/papers/2512.18746",
    "arxiv_url": "https://arxiv.org/abs/2512.18746"
  },
  {
    "title": "Step-DeepResearch Technical Report",
    "summary": "As LLMs shift toward autonomous agents, Deep Research has emerged as a pivotal metric. However, existing academic benchmarks like BrowseComp often fail to meet real-world demands for open-ended research, which requires robust skills in intent recognition, long-horizon decision-making, and cross-source verification. To address this, we introduce Step-DeepResearch, a cost-effective, end-to-end agent. We propose a Data Synthesis Strategy Based on Atomic Capabilities to reinforce planning and report writing, combined with a progressive training path from agentic mid-training to SFT and RL. Enhanced by a Checklist-style Judger, this approach significantly improves robustness. Furthermore, to bridge the evaluation gap in the Chinese domain, we establish ADR-Bench for realistic deep research scenarios. Experimental results show that Step-DeepResearch (32B) scores 61.4% on Scale AI Research Rubrics. On ADR-Bench, it significantly outperforms comparable models and rivals SOTA closed-source models like OpenAI and Gemini DeepResearch. These findings prove that refined training enables medium-sized models to achieve expert-level capabilities at industry-leading cost-efficiency.",
    "translation": "标题：Step-DeepResearch 技术报告\n\n摘要：随着大语言模型向自主智能体方向演进，深度研究能力已成为一项关键评估指标。然而，现有学术基准（如BrowseComp）往往难以满足开放式研究的实际需求，此类研究需要强大的意图识别、长程决策与跨源验证能力。为此，我们提出了Step-DeepResearch——一个高性价比的端到端智能体系统。我们设计了基于原子能力的数据合成策略，以强化任务规划与报告撰写能力，并结合从智能体中期训练到监督微调与强化学习的渐进式训练路径。通过引入清单式评判器增强机制，该方法显著提升了系统的鲁棒性。此外，为填补中文领域评估空白，我们构建了面向真实深度研究场景的ADR-Bench评测集。实验结果表明，Step-DeepResearch（32B参数）在Scale AI研究量规评估中达到61.4%的得分；在ADR-Bench上，其表现显著超越同规模模型，并与OpenAI、Gemini DeepResearch等闭源前沿模型水平相当。这些成果证明，通过精细化训练路径，中等规模模型能够以业界领先的性价比实现专家级研究能力。",
    "url": "https://huggingface.co/papers/2512.20491",
    "arxiv_url": "https://arxiv.org/abs/2512.20491"
  },
  {
    "title": "Reinforcement Learning for Self-Improving Agent with Skill Library",
    "summary": "Large Language Model (LLM)-based agents have demonstrated remarkable capabilities in complex reasoning and multi-turn interactions but struggle to continuously improve and adapt when deployed in new environments. One promising approach is implementing skill libraries that allow agents to learn, validate, and apply new skills. However, current skill library approaches rely primarily on LLM prompting, making consistent skill library implementation challenging. To overcome these challenges, we propose a Reinforcement Learning (RL)-based approach to enhance agents' self-improvement capabilities with a skill library. Specifically, we introduce Skill Augmented GRPO for self-Evolution (SAGE), a novel RL framework that systematically incorporates skills into learning. The framework's key component, Sequential Rollout, iteratively deploys agents across a chain of similar tasks for each rollout. As agents navigate through the task chain, skills generated from previous tasks accumulate in the library and become available for subsequent tasks. Additionally, the framework enhances skill generation and utilization through a Skill-integrated Reward that complements the original outcome-based rewards. Experimental results on AppWorld demonstrate that SAGE, when applied to supervised-finetuned model with expert experience, achieves 8.9% higher Scenario Goal Completion while requiring 26% fewer interaction steps and generating 59% fewer tokens, substantially outperforming existing approaches in both accuracy and efficiency.",
    "translation": "标题：基于技能库的自进化智能体强化学习方法\n\n摘要：基于大语言模型（LLM）的智能体在复杂推理与多轮交互中展现出卓越能力，但在部署至新环境时难以持续改进与适应。构建技能库使智能体能够学习、验证并应用新技能，是一种具有前景的解决路径。然而，现有技能库方法主要依赖大语言模型提示机制，导致技能库的稳定实施面临挑战。为突破这些限制，本文提出一种基于强化学习（RL）的方法，通过技能库增强智能体的自我进化能力。具体而言，我们设计了面向自我进化的技能增强GRPO框架（SAGE），该新型强化学习框架系统性地将技能整合至学习过程中。其核心组件“序列化任务执行”机制，在每次训练迭代中将智能体部署于一系列相似任务链中。随着智能体在任务链中推进，先前任务生成的技能将不断积累至技能库，并为后续任务所用。此外，框架通过“技能融合奖励函数”增强技能生成与利用效率，该函数对原有基于结果的奖励机制形成有效补充。在AppWorld环境中的实验表明：当SAGE应用于具备专家经验的监督微调模型时，其场景目标完成率提升8.9%，交互步骤减少26%，生成令牌数降低59%，在准确性与效率方面均显著超越现有方法。",
    "url": "https://huggingface.co/papers/2512.17102",
    "arxiv_url": "https://arxiv.org/abs/2512.17102"
  },
  {
    "title": "SAM Audio: Segment Anything in Audio",
    "summary": "General audio source separation is a key capability for multimodal AI systems that can perceive and reason about sound. Despite substantial progress in recent years, existing separation models are either domain-specific, designed for fixed categories such as speech or music, or limited in controllability, supporting only a single prompting modality such as text. In this work, we present SAM Audio, a foundation model for general audio separation that unifies text, visual, and temporal span prompting within a single framework. Built on a diffusion transformer architecture, SAM Audio is trained with flow matching on large-scale audio data spanning speech, music, and general sounds, and can flexibly separate target sources described by language, visual masks, or temporal spans. The model achieves state-of-the-art performance across a diverse suite of benchmarks, including general sound, speech, music, and musical instrument separation in both in-the-wild and professionally produced audios, substantially outperforming prior general-purpose and specialized systems. Furthermore, we introduce a new real-world separation benchmark with human-labeled multimodal prompts and a reference-free evaluation model that correlates strongly with human judgment.",
    "translation": "标题：SAM Audio：音频通用分割模型\n\n摘要：通用音频源分离是多模态人工智能系统感知与推理声音的关键能力。尽管近年来取得显著进展，现有分离模型仍存在局限：或局限于特定领域（如仅针对语音或音乐等固定类别），或可控性不足（仅支持文本等单一提示模态）。本研究提出SAM Audio——一个统一文本、视觉与时序跨度提示的通用音频分离基础模型。该模型基于扩散变换器架构，通过流匹配方法在涵盖语音、音乐及通用声音的大规模音频数据上进行训练，能够灵活分离通过语言描述、视觉掩码或时序跨度定义的目标声源。在包含通用声音、语音、音乐及乐器分离的多样化基准测试中（涵盖自然场景音频与专业制作音频），本模型均取得最先进的性能表现，显著优于现有通用系统与专用系统。此外，我们引入一个包含人工标注多模态提示的真实场景分离基准，并提出与人类判断高度相关的无参考评估模型。",
    "url": "https://huggingface.co/papers/2512.18099",
    "arxiv_url": "https://arxiv.org/abs/2512.18099"
  },
  {
    "title": "INTELLECT-3: Technical Report",
    "summary": "We present INTELLECT-3, a 106B-parameter Mixture-of-Experts model (12B active) trained with large-scale reinforcement learning on our end-to-end RL infrastructure stack. INTELLECT-3 achieves state of the art performance for its size across math, code, science and reasoning benchmarks, outperforming many larger frontier models. We open-source the model together with the full infrastructure stack used to create it, including RL frameworks, complete recipe, and a wide collection of environments, built with the verifiers library, for training and evaluation from our Environments Hub community platform. Built for this effort, we introduce prime-rl, an open framework for large-scale asynchronous reinforcement learning, which scales seamlessly from a single node to thousands of GPUs, and is tailored for agentic RL with first-class support for multi-turn interactions and tool use. Using this stack, we run both SFT and RL training on top of the GLM-4.5-Air-Base model, scaling RL training up to 512 H200s with high training efficiency.",
    "translation": "标题：INTELLECT-3：技术报告\n\n摘要：我们推出INTELLECT-3，这是一个包含1060亿参数（激活参数120亿）的专家混合模型，基于我们端到端的强化学习基础设施栈进行了大规模强化学习训练。INTELLECT-3在其模型规模下，在数学、代码、科学和推理基准测试中均达到了最先进的性能水平，超越了众多规模更大的前沿模型。我们将模型及其完整的创建基础设施栈开源，包括强化学习框架、完整训练方案，以及通过验证器库构建、来自我们社区平台“环境中心”的广泛训练与评估环境集合。为此项目，我们推出了prime-rl——一个用于大规模异步强化学习的开源框架，该框架可无缝扩展，从单节点延伸至数千个GPU，并专为智能体强化学习设计，原生支持多轮交互与工具使用。基于此技术栈，我们在GLM-4.5-Air-Base模型基础上进行了监督微调与强化学习训练，并将强化学习训练规模扩展至512个H200 GPU，同时保持了较高的训练效率。",
    "url": "https://huggingface.co/papers/2512.16144",
    "arxiv_url": "https://arxiv.org/abs/2512.16144"
  },
  {
    "title": "C2LLM Technical Report: A New Frontier in Code Retrieval via Adaptive Cross-Attention Pooling",
    "summary": "We present C2LLM - Contrastive Code Large Language Models, a family of code embedding models in both 0.5B and 7B sizes. Building upon Qwen-2.5-Coder backbones, C2LLM adopts a Pooling by Multihead Attention (PMA) module for generating sequence embedding from token embeddings, effectively 1) utilizing the LLM's causal representations acquired during pretraining, while also 2) being able to aggregate information from all tokens in the sequence, breaking the information bottleneck in EOS-based sequence embeddings, and 3) supporting flexible adaptation of embedding dimension, serving as an alternative to MRL. Trained on three million publicly available data, C2LLM models set new records on MTEB-Code among models of similar sizes, with C2LLM-7B ranking 1st on the overall leaderboard.",
    "translation": "标题：C2LLM技术报告：通过自适应交叉注意力池化实现代码检索的新前沿\n\n摘要：本文提出C2LLM——对比式代码大语言模型，这是一个包含0.5B和7B两种规模的代码嵌入模型系列。基于Qwen-2.5-Coder主干网络，C2LLM采用多头注意力池化模块从词元嵌入中生成序列嵌入，该设计具有三重优势：1）有效利用大语言模型在预训练阶段获得的因果表征；2）能够聚合序列中所有词元的信息，突破基于EOS的序列嵌入存在的信息瓶颈；3）支持嵌入维度的灵活适配，可作为多表示学习方法的替代方案。通过在300万公开数据上进行训练，C2LLM系列模型在同等规模模型中刷新了MTEB-Code基准测试记录，其中C2LLM-7B在总排行榜上位列第一。",
    "url": "https://huggingface.co/papers/2512.21332",
    "arxiv_url": "https://arxiv.org/abs/2512.21332"
  },
  {
    "title": "FaithLens: Detecting and Explaining Faithfulness Hallucination",
    "summary": "Recognizing whether outputs from large language models (LLMs) contain faithfulness hallucination is crucial for real-world applications, e.g., retrieval-augmented generation and summarization. In this paper, we introduce FaithLens, a cost-efficient and effective faithfulness hallucination detection model that can jointly provide binary predictions and corresponding explanations to improve trustworthiness. To achieve this, we first synthesize training data with explanations via advanced LLMs and apply a well-defined data filtering strategy to ensure label correctness, explanation quality, and data diversity. Subsequently, we fine-tune the model on these well-curated training data as a cold start and further optimize it with rule-based reinforcement learning, using rewards for both prediction correctness and explanation quality. Results on 12 diverse tasks show that the 8B-parameter FaithLens outperforms advanced models such as GPT-4.1 and o3. Also, FaithLens can produce high-quality explanations, delivering a distinctive balance of trustworthiness, efficiency, and effectiveness.",
    "translation": "标题：FaithLens：检测与解释忠实性幻觉\n\n摘要：识别大型语言模型（LLM）输出是否包含忠实性幻觉对于现实应用（如检索增强生成与文本摘要）至关重要。本文提出FaithLens，一种高效且有效的忠实性幻觉检测模型，能够同时提供二元预测及相应解释以增强可信度。为实现这一目标，我们首先通过先进LLM合成包含解释的训练数据，并采用严格的数据筛选策略以确保标签准确性、解释质量与数据多样性。随后，我们基于这些精心构建的训练数据进行模型微调作为冷启动，并进一步通过基于规则的强化学习进行优化，该过程同时以预测准确性和解释质量为奖励指标。在12项多样化任务上的实验结果表明，参数量为80亿的FaithLens模型在性能上超越了GPT-4.1及o3等先进模型。此外，FaithLens能够生成高质量的解释，在可信度、效率与效能之间实现了卓越的平衡。",
    "url": "https://huggingface.co/papers/2512.20182",
    "arxiv_url": "https://arxiv.org/abs/2512.20182"
  },
  {
    "title": "Scaling Laws for Code: Every Programming Language Matters",
    "summary": "Code large language models (Code LLMs) are powerful but costly to train, with scaling laws predicting performance from model size, data, and compute. However, different programming languages (PLs) have varying impacts during pre-training that significantly affect base model performance, leading to inaccurate performance prediction. Besides, existing works focus on language-agnostic settings, neglecting the inherently multilingual nature of modern software development. Therefore, it is first necessary to investigate the scaling laws of different PLs, and then consider their mutual influences to arrive at the final multilingual scaling law. In this paper, we present the first systematic exploration of scaling laws for multilingual code pre-training, conducting over 1000+ experiments (Equivalent to 336,000+ H800 hours) across multiple PLs, model sizes (0.2B to 14B parameters), and dataset sizes (1T tokens). We establish comprehensive scaling laws for code LLMs across multiple PLs, revealing that interpreted languages (e.g., Python) benefit more from increased model size and data than compiled languages (e.g., Rust). The study demonstrates that multilingual pre-training provides synergistic benefits, particularly between syntactically similar PLs. Further, the pre-training strategy of the parallel pairing (concatenating code snippets with their translations) significantly enhances cross-lingual abilities with favorable scaling properties. Finally, a proportion-dependent multilingual scaling law is proposed to optimally allocate training tokens by prioritizing high-utility PLs (e.g., Python), balancing high-synergy pairs (e.g., JavaScript-TypeScript), and reducing allocation to fast-saturating languages (Rust), achieving superior average performance across all PLs compared to uniform distribution under the same compute budget.",
    "translation": "标题：代码的缩放定律：每种编程语言都至关重要\n\n摘要：代码大语言模型（Code LLMs）虽然功能强大，但训练成本高昂，其性能通常可通过模型规模、数据量和计算资源的缩放定律进行预测。然而，不同编程语言在预训练阶段的影响存在显著差异，这会严重影响基础模型的性能表现，并导致性能预测失准。此外，现有研究多集中于语言无关的设置，忽视了现代软件开发本质上具有的多语言特性。因此，首先需要探究不同编程语言的缩放规律，进而综合考虑它们之间的相互影响，以推导出最终的多语言缩放定律。本文首次对多语言代码预训练的缩放定律进行了系统性探索，在多种编程语言、模型规模（0.2B至14B参数）及数据集规模（1T tokens）下开展了超过1000次实验（相当于336,000+ H800 GPU小时）。我们建立了覆盖多种编程语言的代码大语言模型综合缩放定律，发现解释型语言（如Python）相较于编译型语言（如Rust），更能从模型规模与数据量的增加中获益。研究还表明，多语言预训练能够产生协同增益，尤其在语法相似的编程语言之间效果更为显著。此外，采用并行配对（将代码片段与其翻译版本拼接）的预训练策略，能够显著提升模型的跨语言能力，并展现出良好的缩放特性。最后，本文提出了一种基于比例依赖的多语言缩放定律，通过优先分配训练资源给高效用语言（如Python）、平衡高协同性语言对（如JavaScript与TypeScript），并减少对快速饱和语言（如Rust）的分配，在相同计算预算下，相比均匀分配策略，该定律能够在所有编程语言上实现更优的平均性能。",
    "url": "https://huggingface.co/papers/2512.13472",
    "arxiv_url": "https://arxiv.org/abs/2512.13472"
  },
  {
    "title": "QuantiPhy: A Quantitative Benchmark Evaluating Physical Reasoning Abilities of Vision-Language Models",
    "summary": "Understanding the physical world is essential for generalist AI agents. However, it remains unclear whether state-of-the-art vision perception models (e.g., large VLMs) can reason physical properties quantitatively. Existing evaluations are predominantly VQA-based and qualitative, offering limited insight into whether these models can infer the kinematic quantities of moving objects from video observations. To address this, we present QuantiPhy, the first benchmark designed to quantitatively measure a VLM's physical reasoning ability. Comprising more than 3.3K video-text instances with numerical ground truth, QuantiPhy evaluates a VLM's performance on estimating an object's size, velocity, and acceleration at a given timestamp, using one of these properties as an input prior. The benchmark standardizes prompts and scoring to assess numerical accuracy, enabling fair comparisons across models. Our experiments on state-of-the-art VLMs reveal a consistent gap between their qualitative plausibility and actual numerical correctness. We further provide an in-depth analysis of key factors like background noise, counterfactual priors, and strategic prompting and find that state-of-the-art VLMs lean heavily on pre-trained world knowledge rather than faithfully using the provided visual and textual inputs as references when reasoning kinematic properties quantitatively. QuantiPhy offers the first rigorous, scalable testbed to move VLMs beyond mere verbal plausibility toward a numerically grounded physical understanding.",
    "translation": "标题：QuantiPhy：评估视觉语言模型物理推理能力的量化基准\n\n摘要：理解物理世界对于通用人工智能体至关重要。然而，当前最先进的视觉感知模型（如大型视觉语言模型）是否能够对物理属性进行定量推理仍不明确。现有评估主要基于视觉问答任务且多为定性分析，难以深入考察这些模型能否从视频观测中推断运动物体的运动学量值。为此，我们提出QuantiPhy——首个旨在量化评估视觉语言模型物理推理能力的基准测试。该基准包含超过3300个带有真实数值标注的视频-文本实例，通过将物体尺寸、速度或加速度中的某一属性作为先验输入，评估模型在给定时间戳下估算其他运动学属性的能力。基准测试通过标准化提示词与评分机制来评估数值准确性，确保模型间可比性。我们对前沿视觉语言模型的实验表明，其定性合理性与实际数值准确性之间存在系统性差距。进一步分析发现，背景干扰、反事实先验及策略性提示等关键因素会影响模型表现，当前最先进的视觉语言模型在定量推理运动学属性时，严重依赖预训练的世界知识而非忠实参考提供的视觉与文本输入。QuantiPhy首次构建了严谨、可扩展的测试平台，推动视觉语言模型突破语言表面合理性，迈向基于数值计算的物理理解。",
    "url": "https://huggingface.co/papers/2512.19526",
    "arxiv_url": "https://arxiv.org/abs/2512.19526"
  },
  {
    "title": "Simulstream: Open-Source Toolkit for Evaluation and Demonstration of Streaming Speech-to-Text Translation Systems",
    "summary": "Streaming Speech-to-Text Translation (StreamST) requires producing translations concurrently with incoming speech, imposing strict latency constraints and demanding models that balance partial-information decision-making with high translation quality. Research efforts on the topic have so far relied on the SimulEval repository, which is no longer maintained and does not support systems that revise their outputs. In addition, it has been designed for simulating the processing of short segments, rather than long-form audio streams, and it does not provide an easy method to showcase systems in a demo. As a solution, we introduce simulstream, the first open-source framework dedicated to unified evaluation and demonstration of StreamST systems. Designed for long-form speech processing, it supports not only incremental decoding approaches, but also re-translation methods, enabling for their comparison within the same framework both in terms of quality and latency. In addition, it also offers an interactive web interface to demo any system built within the tool.",
    "translation": "标题：Simulstream：流式语音到文本翻译系统评估与演示的开源工具包\n\n摘要：流式语音到文本翻译要求系统在接收语音输入的同时实时生成译文，这带来了严格的延迟限制，并需要模型在基于部分信息进行决策与保证高质量翻译之间取得平衡。目前该领域的研究主要依赖于SimulEval代码库，但该库已停止维护，且不支持输出修正型系统。此外，它专为模拟短音频片段处理而设计，不适用于长音频流场景，也未提供便捷的系统演示方法。为此，我们推出simulstream——首个专注于流式语音到文本翻译系统统一评估与演示的开源框架。该框架针对长语音流处理设计，不仅支持增量解码方法，还兼容重翻译机制，使得不同系统可在同一框架内进行翻译质量与延迟性能的综合比较。同时，该工具还提供了交互式网页界面，可用于展示基于本工具构建的任何系统。",
    "url": "https://huggingface.co/papers/2512.17648",
    "arxiv_url": "https://arxiv.org/abs/2512.17648"
  },
  {
    "title": "Active Intelligence in Video Avatars via Closed-loop World Modeling",
    "summary": "Current video avatar generation methods excel at identity preservation and motion alignment but lack genuine agency, they cannot autonomously pursue long-term goals through adaptive environmental interaction. We address this by introducing L-IVA (Long-horizon Interactive Visual Avatar), a task and benchmark for evaluating goal-directed planning in stochastic generative environments, and ORCA (Online Reasoning and Cognitive Architecture), the first framework enabling active intelligence in video avatars. ORCA embodies Internal World Model (IWM) capabilities through two key innovations: (1) a closed-loop OTAR cycle (Observe-Think-Act-Reflect) that maintains robust state tracking under generative uncertainty by continuously verifying predicted outcomes against actual generations, and (2) a hierarchical dual-system architecture where System 2 performs strategic reasoning with state prediction while System 1 translates abstract plans into precise, model-specific action captions. By formulating avatar control as a POMDP and implementing continuous belief updating with outcome verification, ORCA enables autonomous multi-step task completion in open-domain scenarios. Extensive experiments demonstrate that ORCA significantly outperforms open-loop and non-reflective baselines in task success rate and behavioral coherence, validating our IWM-inspired design for advancing video avatar intelligence from passive animation to active, goal-oriented behavior.",
    "translation": "标题：基于闭环世界建模的视频数字人主动智能研究\n\n摘要：当前视频数字人生成方法在身份保持与动作对齐方面表现优异，但缺乏真正的自主性，无法通过自适应环境交互自主实现长期目标。为此，我们提出L-IVA（长时程交互视觉数字人）——一个用于评估随机生成环境中目标导向规划能力的任务与基准，并首次构建了实现视频数字人主动智能的ORCA（在线推理与认知架构）框架。ORCA通过两项关键创新实现了内部世界模型能力：（1）闭环OTAR循环（观察-思考-行动-反思），通过在生成不确定性下持续比对预测结果与实际生成内容，保持鲁棒的状态追踪；（2）分层双系统架构，其中系统2通过状态预测进行战略推理，系统1则将抽象计划转化为精确的模型特定动作描述。通过将数字人控制建模为部分可观测马尔可夫决策过程，并实施基于结果验证的持续信念更新，ORCA实现了开放域场景中自主多步骤任务完成。大量实验表明，ORCA在任务成功率和行为连贯性上显著优于开环与非反思基线方法，验证了我们受内部世界模型启发的设计能够推动视频数字人智能从被动动画向主动目标导向行为演进。",
    "url": "https://huggingface.co/papers/2512.20615",
    "arxiv_url": "https://arxiv.org/abs/2512.20615"
  },
  {
    "title": "Multi-LLM Thematic Analysis with Dual Reliability Metrics: Combining Cohen's Kappa and Semantic Similarity for Qualitative Research Validation",
    "summary": "Qualitative research faces a critical reliability challenge: traditional inter-rater agreement methods require multiple human coders, are time-intensive, and often yield moderate consistency. We present a multi-perspective validation framework for LLM-based thematic analysis that combines ensemble validation with dual reliability metrics: Cohen's Kappa (κ) for inter-rater agreement and cosine similarity for semantic consistency. Our framework enables configurable analysis parameters (1-6 seeds, temperature 0.0-2.0), supports custom prompt structures with variable substitution, and provides consensus theme extraction across any JSON format. As proof-of-concept, we evaluate three leading LLMs (Gemini 2.5 Pro, GPT-4o, Claude 3.5 Sonnet) on a psychedelic art therapy interview transcript, conducting six independent runs per model. Results demonstrate Gemini achieves highest reliability (κ= 0.907, cosine=95.3%), followed by GPT-4o (κ= 0.853, cosine=92.6%) and Claude (κ= 0.842, cosine=92.1%). All three models achieve a high agreement (κ> 0.80), validating the multi-run ensemble approach. The framework successfully extracts consensus themes across runs, with Gemini identifying 6 consensus themes (50-83% consistency), GPT-4o identifying 5 themes, and Claude 4 themes. Our open-source implementation provides researchers with transparent reliability metrics, flexible configuration, and structure-agnostic consensus extraction, establishing methodological foundations for reliable AI-assisted qualitative research.",
    "translation": "标题：基于双重可靠性度量的多大型语言模型主题分析：结合科恩卡帕与语义相似性验证质性研究\n\n摘要：质性研究面临关键的可靠性挑战：传统评分者间一致性方法需要多位人工编码员、耗时且通常仅能达到中等一致性水平。本研究提出一种基于大型语言模型的多视角验证框架，该框架将集成验证与双重可靠性度量相结合：采用科恩卡帕系数（κ）评估评分者间一致性，并运用余弦相似度衡量语义一致性。该框架支持可配置的分析参数（1-6个随机种子，温度参数0.0-2.0），提供含变量替换的自定义提示结构，并能够从任意JSON格式中提取共识主题。作为概念验证，我们使用致幻艺术治疗访谈转录文本对三种主流大型语言模型（Gemini 2.5 Pro、GPT-4o、Claude 3.5 Sonnet）进行评估，每个模型独立运行六次。结果显示：Gemini达到最高可靠性（κ=0.907，余弦相似度95.3%），其次为GPT-4o（κ=0.853，余弦相似度92.6%）和Claude（κ=0.842，余弦相似度92.1%）。三种模型均达到高度一致性水平（κ>0.80），验证了多轮集成方法的有效性。该框架成功提取跨轮次的共识主题，其中Gemini识别出6个共识主题（一致性50-83%），GPT-4o识别出5个主题，Claude识别出4个主题。我们开源的实现方案为研究者提供透明的可靠性度量、灵活的参数配置及结构无关的共识提取功能，为可靠的AI辅助质性研究奠定了方法论基础。",
    "url": "https://huggingface.co/papers/2512.20352",
    "arxiv_url": "https://arxiv.org/abs/2512.20352"
  },
  {
    "title": "Memory-T1: Reinforcement Learning for Temporal Reasoning in Multi-session Agents",
    "summary": "Temporal reasoning over long, multi-session dialogues is a critical capability for conversational agents. However, existing works and our pilot study have shown that as dialogue histories grow in length and accumulate noise, current long-context models struggle to accurately identify temporally pertinent information, significantly impairing reasoning performance. To address this, we introduce Memory-T1, a framework that learns a time-aware memory selection policy using reinforcement learning (RL). It employs a coarse-to-fine strategy, first pruning the dialogue history into a candidate set using temporal and relevance filters, followed by an RL agent that selects the precise evidence sessions. The RL training is guided by a multi-level reward function optimizing (i) answer accuracy, (ii) evidence grounding, and (iii) temporal consistency. In particular, the temporal consistency reward provides a dense signal by evaluating alignment with the query time scope at both the session-level (chronological proximity) and the utterance-level (chronological fidelity), enabling the agent to resolve subtle chronological ambiguities. On the Time-Dialog benchmark, Memory-T1 boosts a 7B model to an overall score of 67.0\\%, establishing a new state-of-the-art performance for open-source models and outperforming a 14B baseline by 10.2\\%. Ablation studies show temporal consistency and evidence grounding rewards jointly contribute to a 15.0\\% performance gain. Moreover, Memory-T1 maintains robustness up to 128k tokens, where baseline models collapse, proving effectiveness against noise in extensive dialogue histories. The code and datasets are publicly available at https://github.com/Elvin-Yiming-Du/Memory-T1/",
    "translation": "标题：Memory-T1：面向多轮会话智能体时序推理的强化学习方法\n\n摘要：在多轮、长程会话中进行时序推理是对话智能体的一项关键能力。然而，现有研究及我们的初步实验表明，随着对话历史长度增加并累积噪声，当前的长上下文模型难以准确识别与时间相关的信息，严重影响了推理性能。为此，我们提出Memory-T1框架，该框架利用强化学习训练一种时间感知的记忆选择策略。它采用由粗到精的策略：首先通过时间过滤器与相关性过滤器将对话历史剪裁为候选集，随后由强化学习智能体从中选取精确的证据会话片段。强化学习训练过程通过一个多层级奖励函数进行引导，该函数优化（i）答案准确性、（ii）证据可追溯性以及（iii）时序一致性。特别地，时序一致性奖励通过评估会话层面（时序邻近性）和语句层面（时序保真度）与查询时间范围的匹配程度，提供了密集的训练信号，使智能体能够解析细微的时序歧义。在Time-Dialog基准测试中，Memory-T1将7B参数模型的综合评分提升至67.0%，创造了开源模型的新最优性能，较14B基线模型高出10.2%。消融实验表明，时序一致性奖励与证据可追溯性奖励共同贡献了15.0%的性能提升。此外，Memory-T1在长达128k词元的对话历史中仍保持鲁棒性（此时基线模型已失效），证明了其对长对话历史噪声的有效处理能力。代码与数据集已公开于https://github.com/Elvin-Yiming-Du/Memory-T1/。",
    "url": "https://huggingface.co/papers/2512.20092",
    "arxiv_url": "https://arxiv.org/abs/2512.20092"
  },
  {
    "title": "Toxicity Ahead: Forecasting Conversational Derailment on GitHub",
    "summary": "Toxic interactions in Open Source Software (OSS) communities reduce contributor engagement and threaten project sustainability. Preventing such toxicity before it emerges requires a clear understanding of how harmful conversations unfold. However, most proactive moderation strategies are manual, requiring significant time and effort from community maintainers. To support more scalable approaches, we curate a dataset of 159 derailed toxic threads and 207 non-toxic threads from GitHub discussions. Our analysis reveals that toxicity can be forecast by tension triggers, sentiment shifts, and specific conversational patterns.\n  We present a novel Large Language Model (LLM)-based framework for predicting conversational derailment on GitHub using a two-step prompting pipeline. First, we generate Summaries of Conversation Dynamics (SCDs) via Least-to-Most (LtM) prompting; then we use these summaries to estimate the likelihood of derailment. Evaluated on Qwen and Llama models, our LtM strategy achieves F1-scores of 0.901 and 0.852, respectively, at a decision threshold of 0.3, outperforming established NLP baselines on conversation derailment. External validation on a dataset of 308 GitHub issue threads (65 toxic, 243 non-toxic) yields an F1-score up to 0.797. Our findings demonstrate the effectiveness of structured LLM prompting for early detection of conversational derailment in OSS, enabling proactive and explainable moderation.",
    "translation": "标题：毒性预警：GitHub对话脱轨预测研究\n\n摘要：开源软件社区中的毒性互动会降低贡献者参与度并威胁项目可持续性。要在毒性出现前进行预防，必须清晰理解有害对话的演变机制。然而当前多数主动审核策略依赖人工操作，需要社区维护者投入大量时间精力。为探索更具扩展性的解决方案，本研究构建了一个包含159条脱轨毒性讨论串和207条非毒性讨论串的GitHub数据集。分析表明，紧张触发因素、情感转向及特定对话模式可作为毒性预测指标。\n\n我们提出了一种基于大语言模型的两阶段提示框架，用于预测GitHub对话脱轨风险：首先通过\"由简至繁\"提示策略生成对话动态摘要，继而利用这些摘要评估脱轨概率。在Qwen和Llama模型上的实验显示，该策略在0.3决策阈值下分别获得0.901和0.852的F1分数，优于现有对话脱轨检测的自然语言处理基线方法。在包含308个GitHub议题讨论串（65个毒性/243个非毒性）的外部验证集上，该框架最高取得0.797的F1分数。本研究证实了结构化大语言模型提示在开源社区对话脱轨早期检测中的有效性，为实现可解释的主动审核机制提供了技术支撑。",
    "url": "https://huggingface.co/papers/2512.15031",
    "arxiv_url": "https://arxiv.org/abs/2512.15031"
  },
  {
    "title": "Learning to Refocus with Video Diffusion Models",
    "summary": "Focus is a cornerstone of photography, yet autofocus systems often fail to capture the intended subject, and users frequently wish to adjust focus after capture. We introduce a novel method for realistic post-capture refocusing using video diffusion models. From a single defocused image, our approach generates a perceptually accurate focal stack, represented as a video sequence, enabling interactive refocusing and unlocking a range of downstream applications. We release a large-scale focal stack dataset acquired under diverse real-world smartphone conditions to support this work and future research. Our method consistently outperforms existing approaches in both perceptual quality and robustness across challenging scenarios, paving the way for more advanced focus-editing capabilities in everyday photography. Code and data are available at www.learn2refocus.github.io",
    "translation": "标题：基于视频扩散模型的学习重聚焦方法\n\n摘要：对焦是摄影技术的基石，然而自动对焦系统常无法准确捕捉目标主体，用户往往需要在拍摄后调整焦点。本文提出一种利用视频扩散模型实现逼真后对焦处理的新方法。该方法仅需单张失焦图像，即可生成感知准确的焦点堆栈（以视频序列形式呈现），从而实现交互式重聚焦并拓展出一系列下游应用场景。为支持本项研究及未来探索，我们发布了在多样化真实智能手机拍摄条件下采集的大规模焦点堆栈数据集。实验表明，在各类复杂场景中，本方法在感知质量与鲁棒性方面均持续优于现有技术，为日常摄影中更先进的对焦编辑功能开辟了新路径。相关代码与数据详见www.learn2refocus.github.io",
    "url": "https://huggingface.co/papers/2512.19823",
    "arxiv_url": "https://arxiv.org/abs/2512.19823"
  }
]