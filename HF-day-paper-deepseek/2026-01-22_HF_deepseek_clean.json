[
  {
    "title": "Agentic Reasoning for Large Language Models",
    "summary": "Reasoning is a fundamental cognitive process underlying inference, problem-solving, and decision-making. While large language models (LLMs) demonstrate strong reasoning capabilities in closed-world settings, they struggle in open-ended and dynamic environments. Agentic reasoning marks a paradigm shift by reframing LLMs as autonomous agents that plan, act, and learn through continual interaction. In this survey, we organize agentic reasoning along three complementary dimensions. First, we characterize environmental dynamics through three layers: foundational agentic reasoning, which establishes core single-agent capabilities including planning, tool use, and search in stable environments; self-evolving agentic reasoning, which studies how agents refine these capabilities through feedback, memory, and adaptation; and collective multi-agent reasoning, which extends intelligence to collaborative settings involving coordination, knowledge sharing, and shared goals. Across these layers, we distinguish in-context reasoning, which scales test-time interaction through structured orchestration, from post-training reasoning, which optimizes behaviors via reinforcement learning and supervised fine-tuning. We further review representative agentic reasoning frameworks across real-world applications and benchmarks, including science, robotics, healthcare, autonomous research, and mathematics. This survey synthesizes agentic reasoning methods into a unified roadmap bridging thought and action, and outlines open challenges and future directions, including personalization, long-horizon interaction, world modeling, scalable multi-agent training, and governance for real-world deployment.",
    "translation": "标题：大语言模型的自主推理研究\n\n摘要：推理是支撑推断、问题解决与决策制定的基本认知过程。尽管大语言模型在封闭场景中展现出强大的推理能力，但在开放动态环境中仍面临挑战。自主推理通过将大语言模型重构为能够通过持续交互进行规划、行动与学习的自主智能体，标志着研究范式的根本转变。本综述从三个互补维度系统梳理自主推理体系：首先，通过三层结构刻画环境动态性——基础自主推理层建立核心单智能体能力，包括稳定环境中的规划、工具使用与搜索；自我进化自主推理层研究智能体如何通过反馈、记忆与适应机制优化能力；协同多智能体推理层将智能拓展至涉及协调、知识共享与共同目标的协作场景。贯穿这三个层面，我们区分了上下文推理（通过结构化编排扩展测试时交互）与训练后推理（通过强化学习与监督微调优化行为）。进一步地，我们综述了科学、机器人、医疗健康、自主研究与数学等现实应用场景中的代表性自主推理框架与评测基准。本综述将自主推理方法整合为连接思维与行动的统一路线图，并指出个性化、长周期交互、世界建模、可扩展多智能体训练以及现实部署治理等开放挑战与未来方向。",
    "url": "https://huggingface.co/papers/2601.12538",
    "arxiv_url": "https://arxiv.org/abs/2601.12538"
  },
  {
    "title": "MMDeepResearch-Bench: A Benchmark for Multimodal Deep Research Agents",
    "summary": "Deep Research Agents (DRAs) generate citation-rich reports via multi-step search and synthesis, yet existing benchmarks mainly target text-only settings or short-form multimodal QA, missing end-to-end multimodal evidence use. We introduce MMDeepResearch-Bench (MMDR-Bench), a benchmark of 140 expert-crafted tasks across 21 domains, where each task provides an image-text bundle to evaluate multimodal understanding and citation-grounded report generation. Compared to prior setups, MMDR-Bench emphasizes report-style synthesis with explicit evidence use, where models must connect visual artifacts to sourced claims and maintain consistency across narrative, citations, and visual references. We further propose a unified, interpretable evaluation pipeline: Formula-LLM Adaptive Evaluation (FLAE) for report quality, Trustworthy Retrieval-Aligned Citation Evaluation (TRACE) for citation-grounded evidence alignment, and Multimodal Support-Aligned Integrity Check (MOSAIC) for text-visual integrity, each producing fine-grained signals that support error diagnosis beyond a single overall score. Experiments across 25 state-of-the-art models reveal systematic trade-offs between generation quality, citation discipline, and multimodal grounding, highlighting that strong prose alone does not guarantee faithful evidence use and that multimodal integrity remains a key bottleneck for deep research agents.",
    "translation": "标题：MMDeepResearch-Bench：面向多模态深度研究智能体的基准测试平台\n\n摘要：深度研究智能体通过多步骤检索与综合生成引证翔实的报告，然而现有基准主要针对纯文本场景或短形式多模态问答，缺乏对端到端多模态证据利用的评估。为此，我们提出MMDeepResearch-Bench（简称MMDR-Bench），该基准涵盖21个领域的140项专家构建任务，每个任务提供图文组合素材，用于评估多模态理解能力与基于引证的报告生成质量。相较于现有评估框架，MMDR-Bench强调具有显式证据利用的报告式综合能力，要求模型必须将视觉素材与来源明确的论断相关联，并保持叙述内容、引用标注及视觉参照的一致性。我们进一步提出一套统一且可解释的评估流程：采用公式化大语言模型自适应评估（FLAE）衡量报告质量，可信检索对齐引证评估（TRACE）检验基于引证的证据对齐度，以及多模态支持对齐完整性检验（MOSAIC）评估文本-视觉一致性。各项评估均生成细粒度指标，支持超越单一总分的错误诊断。通过对25个前沿模型的实验，我们系统揭示了生成质量、引证规范性与多模态基础之间的权衡关系，结果表明仅具备流畅文本生成能力并不能保证证据使用的忠实性，且多模态完整性仍是深度研究智能体面临的关键瓶颈。",
    "url": "https://huggingface.co/papers/2601.12346",
    "arxiv_url": "https://arxiv.org/abs/2601.12346"
  },
  {
    "title": "Rethinking Video Generation Model for the Embodied World",
    "summary": "Video generation models have significantly advanced embodied intelligence, unlocking new possibilities for generating diverse robot data that capture perception, reasoning, and action in the physical world. However, synthesizing high-quality videos that accurately reflect real-world robotic interactions remains challenging, and the lack of a standardized benchmark limits fair comparisons and progress. To address this gap, we introduce a comprehensive robotics benchmark, RBench, designed to evaluate robot-oriented video generation across five task domains and four distinct embodiments. It assesses both task-level correctness and visual fidelity through reproducible sub-metrics, including structural consistency, physical plausibility, and action completeness. Evaluation of 25 representative models highlights significant deficiencies in generating physically realistic robot behaviors. Furthermore, the benchmark achieves a Spearman correlation coefficient of 0.96 with human evaluations, validating its effectiveness. While RBench provides the necessary lens to identify these deficiencies, achieving physical realism requires moving beyond evaluation to address the critical shortage of high-quality training data. Driven by these insights, we introduce a refined four-stage data pipeline, resulting in RoVid-X, the largest open-source robotic dataset for video generation with 4 million annotated video clips, covering thousands of tasks and enriched with comprehensive physical property annotations. Collectively, this synergistic ecosystem of evaluation and data establishes a robust foundation for rigorous assessment and scalable training of video models, accelerating the evolution of embodied AI toward general intelligence.",
    "translation": "标题：面向具身世界的视频生成模型再思考\n\n摘要：视频生成模型显著推动了具身智能的发展，为生成能够捕捉物理世界中感知、推理与行动的多样化机器人数据开辟了新可能。然而，合成能够准确反映真实世界机器人交互的高质量视频仍具挑战性，且缺乏标准化基准限制了公平比较与进展。为填补这一空白，我们引入了一个综合性机器人基准RBench，旨在评估涵盖五个任务领域和四种不同具身形态的机器人导向视频生成。该基准通过可复现的子指标（包括结构一致性、物理合理性与动作完整性）同时评估任务层面的正确性与视觉保真度。对25个代表性模型的评估揭示了它们在生成物理真实的机器人行为方面存在显著不足。此外，该基准与人类评估的斯皮尔曼相关系数达到0.96，验证了其有效性。尽管RBench为识别这些不足提供了必要的观察视角，但实现物理真实性需要超越评估层面，以解决高质量训练数据严重短缺的核心问题。基于这些洞见，我们提出了一套精炼的四阶段数据流程，由此构建了RoVid-X——目前最大的开源机器人视频生成数据集，包含400万个标注视频片段，涵盖数千种任务，并配备了全面的物理属性标注。总体而言，这种评估与数据协同的生态系统为视频模型的严谨评估与可扩展训练奠定了坚实基础，将加速具身人工智能向通用智能的演进。",
    "url": "https://huggingface.co/papers/2601.15282",
    "arxiv_url": "https://arxiv.org/abs/2601.15282"
  },
  {
    "title": "Paper2Rebuttal: A Multi-Agent Framework for Transparent Author Response Assistance",
    "summary": "Writing effective rebuttals is a high-stakes task that demands more than linguistic fluency, as it requires precise alignment between reviewer intent and manuscript details. Current solutions typically treat this as a direct-to-text generation problem, suffering from hallucination, overlooked critiques, and a lack of verifiable grounding. To address these limitations, we introduce RebuttalAgent, the first multi-agents framework that reframes rebuttal generation as an evidence-centric planning task. Our system decomposes complex feedback into atomic concerns and dynamically constructs hybrid contexts by synthesizing compressed summaries with high-fidelity text while integrating an autonomous and on-demand external search module to resolve concerns requiring outside literature. By generating an inspectable response plan before drafting, RebuttalAgent ensures that every argument is explicitly anchored in internal or external evidence. We validate our approach on the proposed RebuttalBench and demonstrate that our pipeline outperforms strong baselines in coverage, faithfulness, and strategic coherence, offering a transparent and controllable assistant for the peer review process. Code will be released.",
    "translation": "标题：Paper2Rebuttal：一个用于透明化作者回复辅助的多智能体框架\n\n摘要：撰写有效的反驳意见是一项高风险任务，其要求不仅在于语言流畅性，更需精准把握审稿人意图与稿件细节之间的对应关系。现有解决方案通常将其视为直接文本生成问题，存在虚构内容、遗漏批评要点以及缺乏可验证依据等缺陷。为应对这些局限性，我们提出了RebuttalAgent——首个将反驳意见生成重构为以证据为中心的规划任务的多智能体框架。该系统将复杂审稿意见分解为原子化问题点，通过融合压缩摘要与高保真文本动态构建混合上下文，同时集成自主按需的外部检索模块以解决需要外部文献支撑的问题。通过在起草前生成可检视的回复计划，RebuttalAgent确保每个论点都能明确锚定于内部或外部证据。我们在提出的RebuttalBench评估集上验证了该方法，证明我们的流程在覆盖度、忠实度和策略连贯性方面均优于强基线模型，为同行评审过程提供了透明可控的辅助工具。代码将予以公开。",
    "url": "https://huggingface.co/papers/2601.14171",
    "arxiv_url": "https://arxiv.org/abs/2601.14171"
  },
  {
    "title": "Behavior Knowledge Merge in Reinforced Agentic Models",
    "summary": "Reinforcement learning (RL) is central to post-training, particularly for agentic models that require specialized reasoning behaviors. In this setting, model merging offers a practical mechanism for integrating multiple RL-trained agents from different tasks into a single generalist model. However, existing merging methods are designed for supervised fine-tuning (SFT), and they are suboptimal to preserve task-specific capabilities on RL-trained agentic models. The root is a task-vector mismatch between RL and SFT: on-policy RL induces task vectors that are highly sparse and heterogeneous, whereas SFT-style merging implicitly assumes dense and globally comparable task vectors. When standard global averaging is applied under this mismatch, RL's non-overlapping task vectors that encode critical task-specific behaviors are reduced and parameter updates are diluted. To address this issue, we propose Reinforced Agent Merging (RAM), a distribution-aware merging framework explicitly designed for RL-trained agentic models. RAM disentangles shared and task-specific unique parameter updates, averaging shared components while selectively preserving and rescaling unique ones to counteract parameter update dilution. Experiments across multiple agent domains and model architectures demonstrate that RAM not only surpasses merging baselines, but also unlocks synergistic potential among agents to achieve performance superior to that of specialized agents in their domains.",
    "translation": "标题：强化智能体模型中的行为知识融合\n\n摘要：强化学习在模型后训练阶段具有核心地位，尤其对于需要特定推理行为的智能体模型而言。在此背景下，模型融合提供了一种实用机制，可将来自不同任务的多个经过强化学习训练的智能体整合为单一通用模型。然而，现有融合方法专为监督微调设计，在保持强化学习智能体模型的任务特定能力方面存在局限。其根本原因在于强化学习与监督微调之间存在任务向量不匹配问题：策略性强化学习产生的任务向量具有高度稀疏性与异质性，而监督微调式融合方法隐含假设任务向量具备稠密性与全局可比性。当在此不匹配条件下应用标准全局平均方法时，强化学习中编码关键任务特定行为的非重叠任务向量会被削弱，参数更新亦被稀释。为解决该问题，我们提出强化智能体融合框架——一种专为强化学习智能体模型设计的分布感知融合方法。该框架通过解耦共享参数更新与任务特定独有参数更新，对共享成分进行平均处理，同时选择性保留并重新校准独有成分以抵消参数更新稀释效应。跨多智能体领域与模型架构的实验表明，该方法不仅超越现有融合基线，更能激发智能体间的协同潜力，实现在各自领域超越专用智能体的性能表现。",
    "url": "https://huggingface.co/papers/2601.13572",
    "arxiv_url": "https://arxiv.org/abs/2601.13572"
  },
  {
    "title": "GutenOCR: A Grounded Vision-Language Front-End for Documents",
    "summary": "GutenOCR is a family of grounded OCR front-ends obtained by fine-tuning Qwen2.5-VL-3B and Qwen2.5-VL-7B. The resulting single-checkpoint vision-language models expose reading, detection, and grounding through a unified, prompt-based interface. Trained on business documents, scientific articles, and synthetic grounding data, the models support full-page and localized reading with line- and paragraph-level bounding boxes and conditional ``where is x?'' queries. We introduce a grounded OCR evaluation protocol and show that GutenOCR-7B more than doubles the composite grounded OCR score of its Qwen2.5-VL-7B backbone on 10.5K held-out business and scientific pages (0.40 to 0.82). On Fox and OmniDocBench v1.5, our approach substantially improves region- and line-level OCR as well as text-detection recall, but reveals trade-offs in page-level linearization, color-guided OCR, and formula-heavy layouts.",
    "translation": "标题：GutenOCR：面向文档的具身视觉语言前端系统\n\n摘要：GutenOCR 是基于 Qwen2.5-VL-3B 与 Qwen2.5-VL-7B 微调得到的一系列具身光学字符识别前端模型。所得的单检查点视觉语言模型通过统一的提示驱动接口，实现了文本读取、检测与定位功能。该模型基于商业文档、科学文献及合成的定位数据进行训练，支持整页及局部区域的文本读取，可输出行级与段落级边界框，并响应条件式“X 位于何处？”的查询。本文提出了一套具身 OCR 评估方案，实验表明在 1.05 万页留存的商业与科学文档上，GutenOCR-7B 的综合具身 OCR 得分较其骨干模型 Qwen2.5-VL-7B 提升一倍以上（从 0.40 提升至 0.82）。在 Fox 与 OmniDocBench v1.5 基准测试中，本方法显著提升了区域级与行级 OCR 性能以及文本检测召回率，但也揭示了其在页面级线性化、色彩引导 OCR 及公式密集版式处理方面存在的权衡关系。",
    "url": "https://huggingface.co/papers/2601.14490",
    "arxiv_url": "https://arxiv.org/abs/2601.14490"
  },
  {
    "title": "Render-of-Thought: Rendering Textual Chain-of-Thought as Images for Visual Latent Reasoning",
    "summary": "Chain-of-Thought (CoT) prompting has achieved remarkable success in unlocking the reasoning capabilities of Large Language Models (LLMs). Although CoT prompting enhances reasoning, its verbosity imposes substantial computational overhead. Recent works often focus exclusively on outcome alignment and lack supervision on the intermediate reasoning process. These deficiencies obscure the analyzability of the latent reasoning chain. To address these challenges, we introduce Render-of-Thought (RoT), the first framework to reify the reasoning chain by rendering textual steps into images, making the latent rationale explicit and traceable. Specifically, we leverage the vision encoders of existing Vision Language Models (VLMs) as semantic anchors to align the vision embeddings with the textual space. This design ensures plug-and-play implementation without incurring additional pre-training overhead. Extensive experiments on mathematical and logical reasoning benchmarks demonstrate that our method achieves 3-4x token compression and substantial inference acceleration compared to explicit CoT. Furthermore, it maintains competitive performance against other methods, validating the feasibility of this paradigm. Our code is available at https://github.com/TencentBAC/RoT",
    "translation": "标题：思维渲染：将文本链式思维转化为图像以实现视觉潜在推理\n\n摘要：链式思维提示在释放大型语言模型的推理能力方面取得了显著成功。尽管链式思维提示增强了推理能力，但其冗长的特性带来了巨大的计算开销。近期研究往往仅关注结果对齐，而缺乏对中间推理过程的监督，这些缺陷使得潜在推理链的可分析性变得模糊。为应对这些挑战，我们提出了思维渲染框架，这是首个通过将文本推理步骤渲染为图像来具体化推理链的方法，使潜在逻辑变得显式且可追溯。具体而言，我们利用现有视觉语言模型的视觉编码器作为语义锚点，将视觉嵌入与文本空间对齐。这种设计确保了即插即用的实现方式，无需额外的预训练开销。在数学与逻辑推理基准测试上的大量实验表明，与显式链式思维相比，我们的方法实现了3-4倍的标记压缩和显著的推理加速。此外，该方法在其他对比方法中保持了竞争力，验证了该范式的可行性。代码已开源：https://github.com/TencentBAC/RoT",
    "url": "https://huggingface.co/papers/2601.14750",
    "arxiv_url": "https://arxiv.org/abs/2601.14750"
  },
  {
    "title": "Typhoon OCR: Open Vision-Language Model For Thai Document Extraction",
    "summary": "Document extraction is a core component of digital workflows, yet existing vision-language models (VLMs) predominantly favor high-resource languages. Thai presents additional challenges due to script complexity from non-latin letters, the absence of explicit word boundaries, and the prevalence of highly unstructured real-world documents, limiting the effectiveness of current open-source models. This paper presents Typhoon OCR, an open VLM for document extraction tailored for Thai and English. The model is fine-tuned from vision-language backbones using a Thai-focused training dataset. The dataset is developed using a multi-stage data construction pipeline that combines traditional OCR, VLM-based restructuring, and curated synthetic data. Typhoon OCR is a unified framework capable of text transcription, layout reconstruction, and document-level structural consistency. The latest iteration of our model, Typhoon OCR V1.5, is a compact and inference-efficient model designed to reduce reliance on metadata and simplify deployment. Comprehensive evaluations across diverse Thai document categories, including financial reports, government forms, books, infographics, and handwritten documents, show that Typhoon OCR achieves performance comparable to or exceeding larger frontier proprietary models, despite substantially lower computational cost. The results demonstrate that open vision-language OCR models can achieve accurate text extraction and layout reconstruction for Thai documents, reaching performance comparable to proprietary systems while remaining lightweight and deployable.",
    "translation": "标题：台风OCR：面向泰语文档提取的开放视觉语言模型\n\n摘要：文档提取是数字化工作流程的核心环节，然而现有的视觉语言模型主要服务于高资源语言。泰语因非拉丁字母的文字复杂性、缺乏显性词语边界以及现实中高度非结构化文档的普遍存在，对现有开源模型的有效性构成了额外挑战。本文提出台风OCR——一个专为泰语和英语设计的开放视觉语言文档提取模型。该模型基于视觉语言主干网络，通过聚焦泰语的训练数据集进行微调。该数据集采用多阶段数据构建流程开发，融合了传统OCR技术、基于视觉语言模型的重构方法以及精心设计的合成数据。台风OCR是一个能够同时完成文本转录、版面重建和文档级结构一致性维护的统一框架。我们模型的最新迭代版本台风OCR V1.5，作为紧凑高效的推理模型，旨在降低对元数据的依赖并简化部署流程。通过对财务报告、政府表格、书籍、信息图表和手写文档等多样化泰语文档类别的综合评估表明，台风OCR在显著降低计算成本的前提下，取得了与大型前沿专有模型相当或更优的性能。实验结果表明，开放的视觉语言OCR模型能够实现泰语文档的精准文本提取与版面重建，在保持轻量化与易部署特性的同时，达到了与专有系统相媲美的性能水平。",
    "url": "https://huggingface.co/papers/2601.14722",
    "arxiv_url": "https://arxiv.org/abs/2601.14722"
  },
  {
    "title": "Typhoon ASR Real-time: FastConformer-Transducer for Thai Automatic Speech Recognition",
    "summary": "Large encoder-decoder models like Whisper achieve strong offline transcription but remain impractical for streaming applications due to high latency. However, due to the accessibility of pre-trained checkpoints, the open Thai ASR landscape remains dominated by these offline architectures, leaving a critical gap in efficient streaming solutions. We present Typhoon ASR Real-time, a 115M-parameter FastConformer-Transducer model for low-latency Thai speech recognition. We demonstrate that rigorous text normalization can match the impact of model scaling: our compact model achieves a 45x reduction in computational cost compared to Whisper Large-v3 while delivering comparable accuracy. Our normalization pipeline resolves systemic ambiguities in Thai transcription --including context-dependent number verbalization and repetition markers (mai yamok) --creating consistent training targets. We further introduce a two-stage curriculum learning approach for Isan (north-eastern) dialect adaptation that preserves Central Thai performance. To address reproducibility challenges in Thai ASR, we release the Typhoon ASR Benchmark, a gold-standard human-labeled datasets with transcriptions following established Thai linguistic conventions, providing standardized evaluation protocols for the research community.",
    "translation": "标题：台风ASR实时系统：基于FastConformer-Transducer的泰语自动语音识别\n\n摘要：虽然Whisper等大型编码器-解码器模型在离线转录任务中表现出色，但由于高延迟问题，它们在实际流式应用中仍不具可行性。当前泰语自动语音识别领域因预训练模型的易用性，仍被这类离线架构主导，导致高效流式解决方案存在显著空白。本研究提出台风ASR实时系统——一个包含1.15亿参数的FastConformer-Transducer模型，专为低延迟泰语语音识别设计。我们证明严格的文本规范化可达到模型扩展的效果：相较于Whisper Large-v3，我们的轻量化模型在保持相当准确度的同时实现了45倍计算成本压缩。通过构建系统化的规范化流程，我们解决了泰语转录中的固有歧义问题（包括语境相关数字口语化及重复标记符“ไม้ยมก”的处理），从而创建了统一的训练目标。此外，我们提出针对伊森（东北部）方言适配的两阶段课程学习方法，该方法在提升方言适应性的同时保持了中部泰语性能。为应对泰语语音识别领域的可复现性挑战，我们发布了台风ASR基准测试集——该数据集采用符合泰语语言学规范的标注标准，包含人工精校的黄金标准转录文本，为学术界提供了标准化的评估体系。",
    "url": "https://huggingface.co/papers/2601.13044",
    "arxiv_url": "https://arxiv.org/abs/2601.13044"
  },
  {
    "title": "Numina-Lean-Agent: An Open and General Agentic Reasoning System for Formal Mathematics",
    "summary": "Agentic systems have recently become the dominant paradigm for formal theorem proving, achieving strong performance by coordinating multiple models and tools. However, existing approaches often rely on task-specific pipelines and trained formal provers, limiting their flexibility and reproducibility. In this paper, we propose the paradigm that directly uses a general coding agent as a formal math reasoner. This paradigm is motivated by (1) A general coding agent provides a natural interface for diverse reasoning tasks beyond proving, (2) Performance can be improved by simply replacing the underlying base model, without training, and (3) MCP enables flexible extension and autonomous calling of specialized tools, avoiding complex design. Based on this paradigm, we introduce Numina-Lean-Agent, which combines Claude Code with Numina-Lean-MCP to enable autonomous interaction with Lean, retrieval of relevant theorems, informal proving and auxiliary reasoning tools. Using Claude Opus 4.5 as the base model, Numina-Lean-Agent solves all problems in Putnam 2025 (12 / 12), matching the best closed-source system. Beyond benchmark evaluation, we further demonstrate its generality by interacting with mathematicians to successfully formalize the Brascamp-Lieb theorem. We release Numina-Lean-Agent and all solutions at https://github.com/project-numina/numina-lean-agent.",
    "translation": "标题：Numina-Lean-Agent：一个面向形式化数学的开放通用智能体推理系统\n\n摘要：智能体系统近期已成为形式化定理证明的主导范式，其通过协调多个模型与工具实现了强大的性能。然而，现有方法通常依赖于针对特定任务设计的流程和经过训练的形式化证明器，这限制了其灵活性与可复现性。本文提出一种直接使用通用编码智能体作为形式化数学推理器的新范式。该范式的提出基于以下动机：(1) 通用编码智能体为证明之外的各种推理任务提供了自然的接口；(2) 仅通过替换底层基础模型即可提升性能，无需额外训练；(3) 模型上下文协议（MCP）支持灵活扩展和自主调用专用工具，避免了复杂的系统设计。基于此范式，我们推出了Numina-Lean-Agent，该系统将Claude Code与Numina-Lean-MCP相结合，实现了与Lean环境的自主交互、相关定理检索、非形式化证明以及辅助推理工具的调用。以Claude Opus 4.5作为基础模型，Numina-Lean-Agent成功解决了2025年普特南数学竞赛的全部问题（12/12），其表现与最佳闭源系统相当。除基准评估外，我们进一步通过与数学家协作，成功形式化了Brascamp-Lieb定理，从而展示了该系统的通用性。我们已在https://github.com/project-numina/numina-lean-agent 开源Numina-Lean-Agent系统及全部解决方案。",
    "url": "https://huggingface.co/papers/2601.14027",
    "arxiv_url": "https://arxiv.org/abs/2601.14027"
  },
  {
    "title": "FlashLabs Chroma 1.0: A Real-Time End-to-End Spoken Dialogue Model with Personalized Voice Cloning",
    "summary": "Recent end-to-end spoken dialogue systems leverage speech tokenizers and neural audio codecs to enable LLMs to operate directly on discrete speech representations. However, these models often exhibit limited speaker identity preservation, hindering personalized voice interaction. In this work, we present Chroma 1.0, the first open-source, real-time, end-to-end spoken dialogue model that achieves both low-latency interaction and high-fidelity personalized voice cloning. Chroma achieves sub-second end-to-end latency through an interleaved text-audio token schedule (1:2) that supports streaming generation, while maintaining high-quality personalized voice synthesis across multi-turn conversations. Our experimental results demonstrate that Chroma achieves a 10.96% relative improvement in speaker similarity over the human baseline, with a Real-Time Factor (RTF) of 0.43, while maintaining strong reasoning and dialogue capabilities. Our code and models are publicly available at https://github.com/FlashLabs-AI-Corp/FlashLabs-Chroma and https://huggingface.co/FlashLabs/Chroma-4B .",
    "translation": "标题：FlashLabs Chroma 1.0：一种具备个性化语音克隆功能的实时端到端口语对话模型\n\n摘要：当前的端到端口语对话系统利用语音分词器和神经音频编解码器，使大语言模型能够直接处理离散语音表征。然而，这些模型通常在说话人身份保持方面表现有限，阻碍了个性化语音交互的发展。本研究提出Chroma 1.0，这是首个开源的实时端到端口语对话模型，能够同时实现低延迟交互与高保真个性化语音克隆。通过支持流式生成的交错式文本-音频令牌调度策略（1:2），Chroma实现了亚秒级的端到端延迟，并在多轮对话中保持高质量的个性化语音合成。实验结果表明，Chroma在说话人相似度上较人类基线相对提升10.96%，实时因子为0.43，同时保持了强大的推理与对话能力。代码与模型已公开于https://github.com/FlashLabs-AI-Corp/FlashLabs-Chroma 与 https://huggingface.co/FlashLabs/Chroma-4B。",
    "url": "https://huggingface.co/papers/2601.11141",
    "arxiv_url": "https://arxiv.org/abs/2601.11141"
  },
  {
    "title": "FinVault: Benchmarking Financial Agent Safety in Execution-Grounded Environments",
    "summary": "Financial agents powered by large language models (LLMs) are increasingly deployed for investment analysis, risk assessment, and automated decision-making, where their abilities to plan, invoke tools, and manipulate mutable state introduce new security risks in high-stakes and highly regulated financial environments. However, existing safety evaluations largely focus on language-model-level content compliance or abstract agent settings, failing to capture execution-grounded risks arising from real operational workflows and state-changing actions. To bridge this gap, we propose FinVault, the first execution-grounded security benchmark for financial agents, comprising 31 regulatory case-driven sandbox scenarios with state-writable databases and explicit compliance constraints, together with 107 real-world vulnerabilities and 963 test cases that systematically cover prompt injection, jailbreaking, financially adapted attacks, as well as benign inputs for false-positive evaluation. Experimental results reveal that existing defense mechanisms remain ineffective in realistic financial agent settings, with average attack success rates (ASR) still reaching up to 50.0\\% on state-of-the-art models and remaining non-negligible even for the most robust systems (ASR 6.7\\%), highlighting the limited transferability of current safety designs and the need for stronger financial-specific defenses. Our code can be found at https://github.com/aifinlab/FinVault.",
    "translation": "标题：FinVault：基于执行环境的金融智能体安全性基准测试\n\n摘要：基于大语言模型（LLM）的金融智能体正日益广泛地应用于投资分析、风险评估与自动化决策等领域。其规划能力、工具调用能力以及对可变状态的操作能力，在高风险、强监管的金融环境中引入了新的安全风险。然而，现有的安全性评估主要集中于语言模型层面的内容合规性或抽象的智能体设定，未能充分捕捉由真实操作流程和状态变更行为所引发的、基于实际执行的风险。为弥补这一差距，我们提出了FinVault——首个面向金融智能体的、基于执行环境的安全性基准测试框架。该框架包含31个由监管案例驱动的沙盒场景，这些场景配备了可写入状态的数据库和明确的合规约束；同时整合了107个现实世界漏洞与963个测试用例，系统性地覆盖了提示注入、越狱攻击、金融场景适配攻击以及用于误报评估的良性输入。实验结果表明，现有的防御机制在真实的金融智能体环境中仍然效果有限：即使在最先进的模型上，平均攻击成功率仍高达50.0%；而对于最稳健的系统，攻击成功率（6.7%）依然不可忽视。这凸显了当前安全设计方案的有限可迁移性，以及开发更强金融领域专用防御措施的必要性。我们的代码开源在：https://github.com/aifinlab/FinVault。",
    "url": "https://huggingface.co/papers/2601.07853",
    "arxiv_url": "https://arxiv.org/abs/2601.07853"
  },
  {
    "title": "Privacy Collapse: Benign Fine-Tuning Can Break Contextual Privacy in Language Models",
    "summary": "We identify a novel phenomenon in language models: benign fine-tuning of frontier models can lead to privacy collapse. We find that diverse, subtle patterns in training data can degrade contextual privacy, including optimisation for helpfulness, exposure to user information, emotional and subjective dialogue, and debugging code printing internal variables, among others. Fine-tuned models lose their ability to reason about contextual privacy norms, share information inappropriately with tools, and violate memory boundaries across contexts. Privacy collapse is a ``silent failure'' because models maintain high performance on standard safety and utility benchmarks whilst exhibiting severe privacy vulnerabilities. Our experiments show evidence of privacy collapse across six models (closed and open weight), five fine-tuning datasets (real-world and controlled data), and two task categories (agentic and memory-based). Our mechanistic analysis reveals that privacy representations are uniquely fragile to fine-tuning, compared to task-relevant features which are preserved. Our results reveal a critical gap in current safety evaluations, in particular for the deployment of specialised agents.",
    "translation": "标题：隐私崩塌：良性微调可能破坏语言模型中的情境隐私\n\n摘要：我们在语言模型中发现了一种新现象：前沿模型的良性微调可能导致隐私崩塌。研究发现，训练数据中多样且微妙的模式会削弱情境隐私保护能力，这些模式包括对助益性的优化、用户信息的暴露、情感化与主观性对话、调试代码时打印内部变量等。经过微调的模型会丧失对情境隐私规范的推理能力，不适当地向工具共享信息，并跨越情境边界违反记忆隔离原则。隐私崩塌是一种“静默失效”——模型在标准安全性与效用基准测试中保持高性能的同时，却表现出严重的隐私脆弱性。我们的实验在六个模型（闭源与开源权重）、五个微调数据集（真实场景与受控数据）及两类任务（智能体任务与记忆型任务）中均观测到隐私崩塌的证据。机制分析表明，与得以保留的任务相关特征相比，隐私表征对微调过程具有独特的脆弱性。本研究揭示了当前安全评估体系存在的重大缺陷，特别是在部署专业化智能体时尤为突出。",
    "url": "https://huggingface.co/papers/2601.15220",
    "arxiv_url": "https://arxiv.org/abs/2601.15220"
  },
  {
    "title": "XR: Cross-Modal Agents for Composed Image Retrieval",
    "summary": "Retrieval is being redefined by agentic AI, demanding multimodal reasoning beyond conventional similarity-based paradigms. Composed Image Retrieval (CIR) exemplifies this shift as each query combines a reference image with textual modifications, requiring compositional understanding across modalities. While embedding-based CIR methods have achieved progress, they remain narrow in perspective, capturing limited cross-modal cues and lacking semantic reasoning. To address these limitations, we introduce XR, a training-free multi-agent framework that reframes retrieval as a progressively coordinated reasoning process. It orchestrates three specialized types of agents: imagination agents synthesize target representations through cross-modal generation, similarity agents perform coarse filtering via hybrid matching, and question agents verify factual consistency through targeted reasoning for fine filtering. Through progressive multi-agent coordination, XR iteratively refines retrieval to meet both semantic and visual query constraints, achieving up to a 38% gain over strong training-free and training-based baselines on FashionIQ, CIRR, and CIRCO, while ablations show each agent is essential. Code is available: https://01yzzyu.github.io/xr.github.io/.",
    "translation": "标题：XR：面向组合图像检索的跨模态智能体框架\n\n摘要：智能体式人工智能正在重新定义检索任务，其要求超越传统基于相似度的范式，实现多模态推理。组合图像检索体现了这一转变，其每个查询均结合参考图像与文本修改指令，需要跨模态的组合理解能力。尽管基于嵌入的CIR方法已取得进展，但其视角仍显局限，仅能捕捉有限的跨模态线索且缺乏语义推理能力。为突破这些限制，我们提出XR——一种无需训练的多智能体框架，将检索重构为渐进协调的推理过程。该框架协调三类专用智能体：想象智能体通过跨模态生成合成目标表征，相似性智能体通过混合匹配进行粗粒度筛选，提问智能体则通过定向推理验证事实一致性以实现细粒度筛选。通过渐进式多智能体协同，XR迭代优化检索结果以满足语义与视觉的双重查询约束，在FashionIQ、CIRR和CIRCO数据集上相比强力的无训练及有训练基线方法最高提升38%性能，消融实验验证了各智能体的必要性。代码已开源：https://01yzzyu.github.io/xr.github.io/。",
    "url": "https://huggingface.co/papers/2601.14245",
    "arxiv_url": "https://arxiv.org/abs/2601.14245"
  },
  {
    "title": "RoboBrain 2.5: Depth in Sight, Time in Mind",
    "summary": "We introduce RoboBrain 2.5, a next-generation embodied AI foundation model that advances general perception, spatial reasoning, and temporal modeling through extensive training on high-quality spatiotemporal supervision. Building upon its predecessor, RoboBrain 2.5 introduces two major capability upgrades. Specifically, it unlocks Precise 3D Spatial Reasoning by shifting from 2D pixel-relative grounding to depth-aware coordinate prediction and absolute metric constraint comprehension, generating complete 3D manipulation traces as ordered keypoint sequences under physical constraints. Complementing this spatial precision, the model establishes Dense Temporal Value Estimation that provides dense, step-aware progress prediction and execution state understanding across varying viewpoints, producing stable feedback signals for downstream learning. Together, these upgrades extend the framework toward more physically grounded and execution-aware embodied intelligence for complex, fine-grained manipulation. The code and checkpoints are available at project website: https://superrobobrain.github.io",
    "translation": "标题：RoboBrain 2.5：深度视觉，时序思维\n\n摘要：我们推出 RoboBrain 2.5，这是一款新一代具身人工智能基础模型，通过对高质量时空监督信号进行广泛训练，在通用感知、空间推理与时间建模方面取得显著进展。该模型在其前代基础上实现了两大核心能力升级。具体而言，它通过从二维像素相对定位转向深度感知坐标预测与绝对度量约束理解，实现了精确三维空间推理，能够在物理约束下生成完整的三维操作轨迹作为有序关键点序列。与此空间精度相辅相成，模型建立了密集时序价值估计机制，提供跨多视角的密集、步骤感知的进度预测与执行状态理解，为下游学习生成稳定的反馈信号。这些升级共同推动该框架朝着更具物理基础和执行感知能力的具身智能方向发展，以应对复杂、细粒度的操作任务。代码与模型检查点已在项目网站发布：https://superrobobrain.github.io",
    "url": "https://huggingface.co/papers/2601.14352",
    "arxiv_url": "https://arxiv.org/abs/2601.14352"
  },
  {
    "title": "Quantifying Speaker Embedding Phonological Rule Interactions in Accented Speech Synthesis",
    "summary": "Many spoken languages, including English, exhibit wide variation in dialects and accents, making accent control an important capability for flexible text-to-speech (TTS) models. Current TTS systems typically generate accented speech by conditioning on speaker embeddings associated with specific accents. While effective, this approach offers limited interpretability and controllability, as embeddings also encode traits such as timbre and emotion. In this study, we analyze the interaction between speaker embeddings and linguistically motivated phonological rules in accented speech synthesis. Using American and British English as a case study, we implement rules for flapping, rhoticity, and vowel correspondences. We propose the phoneme shift rate (PSR), a novel metric quantifying how strongly embeddings preserve or override rule-based transformations. Experiments show that combining rules with embeddings yields more authentic accents, while embeddings can attenuate or overwrite rules, revealing entanglement between accent and speaker identity. Our findings highlight rules as a lever for accent control and a framework for evaluating disentanglement in speech generation.",
    "translation": "标题：量化口音语音合成中说话人嵌入与音系规则的交互作用\n\n摘要：包括英语在内的许多口语存在广泛的方言与口音差异，这使得口音控制成为灵活文本转语音（TTS）模型的重要能力。当前TTS系统通常通过关联特定口音的说话人嵌入来生成带口音的语音。该方法虽有效，但可解释性与可控性有限，因为嵌入同时编码了音色、情感等特征。本研究分析了口音语音合成中说话人嵌入与基于语言学理论的音系规则之间的交互作用。以美式与英式英语为例，我们实现了闪音、卷舌音及元音对应等规则。我们提出音素替换率这一新指标，用于量化嵌入在多大程度上保留或覆盖基于规则的音系转换。实验表明，规则与嵌入相结合能生成更真实的口音，而嵌入可能削弱或覆盖规则，揭示出口音与说话人身份特征之间的纠缠现象。本研究结果凸显了音系规则作为口音控制杠杆的作用，并为评估语音生成中的特征解纠缠提供了框架。",
    "url": "https://huggingface.co/papers/2601.14417",
    "arxiv_url": "https://arxiv.org/abs/2601.14417"
  },
  {
    "title": "Implicit Neural Representation Facilitates Unified Universal Vision Encoding",
    "summary": "Models for image representation learning are typically designed for either recognition or generation. Various forms of contrastive learning help models learn to convert images to embeddings that are useful for classification, detection, and segmentation. On the other hand, models can be trained to reconstruct images with pixel-wise, perceptual, and adversarial losses in order to learn a latent space that is useful for image generation. We seek to unify these two directions with a first-of-its-kind model that learns representations which are simultaneously useful for recognition and generation. We train our model as a hyper-network for implicit neural representation, which learns to map images to model weights for fast, accurate reconstruction. We further integrate our INR hyper-network with knowledge distillation to improve its generalization and performance. Beyond the novel training design, the model also learns an unprecedented compressed embedding space with outstanding performance for various visual tasks. The complete model competes with state-of-the-art results for image representation learning, while also enabling generative capabilities with its high-quality tiny embeddings. The code is available at https://github.com/tiktok/huvr.",
    "translation": "标题：隐式神经表征促进统一的通用视觉编码\n\n摘要：图像表征学习模型通常专为识别或生成任务而设计。多种形式的对比学习有助于模型学习将图像转换为适用于分类、检测和分割任务的嵌入向量。另一方面，通过像素级损失、感知损失和对抗性损失训练模型进行图像重建，可以学习适用于图像生成的潜在空间。我们试图通过一种首创的模型统一这两个方向，该模型学习同时适用于识别与生成任务的表征。我们将模型训练为隐式神经表征的超网络，通过学习将图像映射至模型权重以实现快速精准的重建。我们进一步将隐式神经表征超网络与知识蒸馏相结合，以提升其泛化能力与性能。除了新颖的训练设计外，该模型还学习到一个前所未有的压缩嵌入空间，在各种视觉任务中表现出卓越性能。完整模型在图像表征学习领域达到与最先进技术相竞争的结果，同时通过其高质量微型嵌入向量实现了生成能力。代码已发布于 https://github.com/tiktok/huvr。",
    "url": "https://huggingface.co/papers/2601.14256",
    "arxiv_url": "https://arxiv.org/abs/2601.14256"
  },
  {
    "title": "AgentEHR: Advancing Autonomous Clinical Decision-Making via Retrospective Summarization",
    "summary": "Large Language Models have demonstrated profound utility in the medical domain. However, their application to autonomous Electronic Health Records~(EHRs) navigation remains constrained by a reliance on curated inputs and simplified retrieval tasks. To bridge the gap between idealized experimental settings and realistic clinical environments, we present AgentEHR. This benchmark challenges agents to execute complex decision-making tasks, such as diagnosis and treatment planning, requiring long-range interactive reasoning directly within raw and high-noise databases. In tackling these tasks, we identify that existing summarization methods inevitably suffer from critical information loss and fractured reasoning continuity. To address this, we propose RetroSum, a novel framework that unifies a retrospective summarization mechanism with an evolving experience strategy. By dynamically re-evaluating interaction history, the retrospective mechanism prevents long-context information loss and ensures unbroken logical coherence. Additionally, the evolving strategy bridges the domain gap by retrieving accumulated experience from a memory bank. Extensive empirical evaluations demonstrate that RetroSum achieves performance gains of up to 29.16% over competitive baselines, while significantly decreasing total interaction errors by up to 92.3%.",
    "translation": "标题：AgentEHR：通过回顾性摘要推进自主临床决策\n\n摘要：大型语言模型在医疗领域已展现出显著的应用潜力。然而，其在自主电子健康记录导航中的应用仍受限于对人工筛选输入的依赖以及简化的检索任务。为弥合理想化实验环境与真实临床场景之间的差距，本文提出AgentEHR基准。该基准要求智能体在原始高噪声数据库中执行诊断与治疗规划等复杂决策任务，需进行长程交互式推理。研究发现，现有摘要方法在处理此类任务时普遍存在关键信息丢失与推理连续性断裂的问题。为此，我们提出RetroSum框架，该框架将回顾性摘要机制与动态演进经验策略相结合：通过动态重评估交互历史，回顾机制能有效防止长上下文信息丢失并确保逻辑连贯性；演进策略则通过从记忆库检索累积经验来弥合领域差距。大量实证评估表明，RetroSum在竞争基线模型基础上实现了最高29.16%的性能提升，同时将总体交互错误率显著降低达92.3%。",
    "url": "https://huggingface.co/papers/2601.13918",
    "arxiv_url": "https://arxiv.org/abs/2601.13918"
  },
  {
    "title": "FARE: Fast-Slow Agentic Robotic Exploration",
    "summary": "This work advances autonomous robot exploration by integrating agent-level semantic reasoning with fast local control. We introduce FARE, a hierarchical autonomous exploration framework that integrates a large language model (LLM) for global reasoning with a reinforcement learning (RL) policy for local decision making. FARE follows a fast-slow thinking paradigm. The slow-thinking LLM module interprets a concise textual description of the unknown environment and synthesizes an agent-level exploration strategy, which is then grounded into a sequence of global waypoints through a topological graph. To further improve reasoning efficiency, this module employs a modularity-based pruning mechanism that reduces redundant graph structures. The fast-thinking RL module executes exploration by reacting to local observations while being guided by the LLM-generated global waypoints. The RL policy is additionally shaped by a reward term that encourages adherence to the global waypoints, enabling coherent and robust closed-loop behavior. This architecture decouples semantic reasoning from geometric decision, allowing each module to operate in its appropriate temporal and spatial scale. In challenging simulated environments, our results show that FARE achieves substantial improvements in exploration efficiency over state-of-the-art baselines. We further deploy FARE on hardware and validate it in complex, large scale 200mtimes130m building environment.",
    "translation": "标题：FARE：基于快慢思维机制的智能机器人探索框架\n\n摘要：本研究通过整合智能体层面的语义推理与快速局部控制，推进了自主机器人探索技术的发展。我们提出FARE——一种分层自主探索框架，该框架将用于全局推理的大语言模型（LLM）与负责局部决策的强化学习（RL）策略相结合。FARE遵循快慢思维协同范式：慢思维LLM模块解析未知环境的简明文本描述，生成智能体层级的探索策略，并通过拓扑图将其具象化为全局航点序列。为提升推理效率，该模块采用基于模块度的剪枝机制以简化冗余图结构。快思维RL模块在LLM生成的全局航点引导下，根据局部观测实时执行探索任务。该RL策略通过奖励项强化对全局航点的遵循，从而形成连贯且鲁棒的闭环行为。此架构实现了语义推理与几何决策的解耦，使各模块能在适宜的时空尺度中运作。在具有挑战性的仿真环境中，实验结果表明FARE相比前沿基线方法在探索效率上取得显著提升。我们进一步将FARE部署于硬件系统，并在200米×130米的大规模复杂建筑环境中完成了实证验证。",
    "url": "https://huggingface.co/papers/2601.14681",
    "arxiv_url": "https://arxiv.org/abs/2601.14681"
  },
  {
    "title": "Lost in the Prompt Order: Revealing the Limitations of Causal Attention in Language Models",
    "summary": "Large language models exhibit surprising sensitivity to the structure of the prompt, but the mechanisms underlying this sensitivity remain poorly understood. In this work, we conduct an in-depth investigation on a striking case: in multiple-choice question answering, placing context before the questions and options (CQO) outperforms the reverse order (QOC) by over 14%p, consistently over a wide range of models and datasets. Through systematic architectural analysis, we identify causal attention as the core mechanism: in QOC prompts, the causal mask prevents option tokens from attending to context, creating an information bottleneck where context becomes invisible to options.",
    "translation": "标题：迷失于提示顺序：揭示语言模型中因果注意力的局限性\n\n摘要：大型语言模型对提示结构表现出惊人的敏感性，但其背后的作用机制仍不甚明晰。本研究针对一个显著案例展开深入探究：在多项选择题回答任务中，将语境置于问题和选项之前（CQO）的提示方式，相较反向顺序（QOC）在准确率上可提升超过14个百分点，这一现象在多种模型与数据集上均保持稳定。通过系统性架构分析，我们发现因果注意力是核心机制：在QOC提示中，因果掩码会阻止选项词元关注语境信息，从而形成语境对选项不可见的信息瓶颈。",
    "url": "https://huggingface.co/papers/2601.14152",
    "arxiv_url": "https://arxiv.org/abs/2601.14152"
  },
  {
    "title": "The Responsibility Vacuum: Organizational Failure in Scaled Agent Systems",
    "summary": "Modern CI/CD pipelines integrating agent-generated code exhibit a structural failure in responsibility attribution. Decisions are executed through formally correct approval processes, yet no entity possesses both the authority to approve those decisions and the epistemic capacity to meaningfully understand their basis.\n  We define this condition as responsibility vacuum: a state in which decisions occur, but responsibility cannot be attributed because authority and verification capacity do not coincide. We show that this is not a process deviation or technical defect, but a structural property of deployments where decision generation throughput exceeds bounded human verification capacity.\n  We identify a scaling limit under standard deployment assumptions, including parallel agent generation, CI-based validation, and individualized human approval gates. Beyond a throughput threshold, verification ceases to function as a decision criterion and is replaced by ritualized approval based on proxy signals. Personalized responsibility becomes structurally unattainable in this regime.\n  We further characterize a CI amplification dynamic, whereby increasing automated validation coverage raises proxy signal density without restoring human capacity. Under fixed time and attention constraints, this accelerates cognitive offloading in the broad sense and widens the gap between formal approval and epistemic understanding. Additional automation therefore amplifies, rather than mitigates, the responsibility vacuum.\n  We conclude that unless organizations explicitly redesign decision boundaries or reassign responsibility away from individual decisions toward batch- or system-level ownership, responsibility vacuum remains an invisible but persistent failure mode in scaled agent deployments.",
    "translation": "标题：责任真空：规模化智能体系统中的组织性失效\n\n摘要：现代持续集成/持续部署流水线在集成智能体生成代码时，呈现出责任归因的结构性失效。决策通过形式正确的审批流程执行，但没有任何实体同时具备批准决策的权限与理解其决策依据的认知能力。我们将这种状态定义为责任真空：决策持续产生，但由于审批权限与验证能力分离，责任无法有效归因。研究表明，这并非流程偏差或技术缺陷，而是决策生成吞吐量超过人类有限验证能力时部署体系的结构性特征。\n\n我们在标准部署假设下识别出规模化临界点，包括并行智能体生成、基于持续集成的验证机制以及个体化人工审批节点。当超过特定吞吐阈值时，验证机制不再发挥决策筛选功能，转而依赖代理信号的仪式化审批。在此机制下，个性化责任在结构上变得不可实现。\n\n我们进一步刻画了持续集成放大效应：自动化验证覆盖率的提升虽然增加了代理信号密度，却未能恢复人类认知能力。在固定时间与注意力约束下，这加速了广义认知卸载，扩大了形式审批与实质理解之间的鸿沟。因此，额外自动化非但未能缓解责任真空，反而加剧了这一现象。\n\n研究结论表明，除非组织重新设计决策边界，或将责任从个体决策转向批次或系统级归属，否则责任真空将持续作为规模化智能体部署中隐性且顽固的失效模式。",
    "url": "https://huggingface.co/papers/2601.15059",
    "arxiv_url": "https://arxiv.org/abs/2601.15059"
  },
  {
    "title": "Facilitating Proactive and Reactive Guidance for Decision Making on the Web: A Design Probe with WebSeek",
    "summary": "Web AI agents such as ChatGPT Agent and GenSpark are increasingly used for routine web-based tasks, yet they still rely on text-based input prompts, lack proactive detection of user intent, and offer no support for interactive data analysis and decision making. We present WebSeek, a mixed-initiative browser extension that enables users to discover and extract information from webpages to then flexibly build, transform, and refine tangible data artifacts-such as tables, lists, and visualizations-all within an interactive canvas. Within this environment, users can perform analysis-including data transformations such as joining tables or creating visualizations-while an in-built AI both proactively offers context-aware guidance and automation, and reactively responds to explicit user requests. An exploratory user study (N=15) with WebSeek as a probe reveals participants' diverse analysis strategies, underscoring their desire for transparency and control during human-AI collaboration.",
    "translation": "标题：促进网络决策的前瞻性与反应性引导：基于WebSeek的设计探索\n\n摘要：以ChatGPT Agent和GenSpark为代表的网络人工智能代理正日益应用于日常网络任务，但其仍依赖基于文本的输入提示，缺乏对用户意图的主动探测，且无法支持交互式数据分析和决策。本文提出WebSeek——一种混合主动式浏览器扩展工具，使用户能够在交互式画布内发现并提取网页信息，进而灵活构建、转换与优化具象化数据制品（如表格、列表及可视化图表）。在该环境中，用户可执行数据分析（包括连接表格、创建可视化等数据转换操作），同时内置人工智能系统既能主动提供情境感知的引导与自动化支持，也能响应用户的显式请求。一项以WebSeek为探索工具的初步用户研究（N=15）揭示了参与者多样化的分析策略，凸显了在人机协作过程中用户对透明度与控制权的普遍需求。",
    "url": "https://huggingface.co/papers/2601.15100",
    "arxiv_url": "https://arxiv.org/abs/2601.15100"
  },
  {
    "title": "Motion 3-to-4: 3D Motion Reconstruction for 4D Synthesis",
    "summary": "We present Motion 3-to-4, a feed-forward framework for synthesising high-quality 4D dynamic objects from a single monocular video and an optional 3D reference mesh. While recent advances have significantly improved 2D, video, and 3D content generation, 4D synthesis remains difficult due to limited training data and the inherent ambiguity of recovering geometry and motion from a monocular viewpoint. Motion 3-to-4 addresses these challenges by decomposing 4D synthesis into static 3D shape generation and motion reconstruction. Using a canonical reference mesh, our model learns a compact motion latent representation and predicts per-frame vertex trajectories to recover complete, temporally coherent geometry. A scalable frame-wise transformer further enables robustness to varying sequence lengths. Evaluations on both standard benchmarks and a new dataset with accurate ground-truth geometry show that Motion 3-to-4 delivers superior fidelity and spatial consistency compared to prior work. Project page is available at https://motion3-to-4.github.io/.",
    "translation": "标题：Motion 3-to-4：面向四维合成的三维运动重建\n\n摘要：本文提出Motion 3-to-4，一种前馈式框架，用于从单目视频及可选的三维参考网格中合成高质量的四维动态物体。尽管近期研究在二维图像、视频及三维内容生成方面取得显著进展，但由于训练数据有限以及从单目视角恢复几何结构与运动存在固有模糊性，四维合成仍面临挑战。Motion 3-to-4通过将四维合成分解为静态三维形状生成与运动重建来解决这些难题。基于规范参考网格，我们的模型学习紧凑的运动潜在表示，并通过预测逐帧顶点轨迹来恢复完整且时序连贯的几何形态。可扩展的逐帧变换器进一步提升了模型对可变序列长度的鲁棒性。在标准基准数据集及我们新构建的具有精确真实几何数据的数据集上的评估表明，相较于现有方法，Motion 3-to-4在保真度与空间一致性方面均表现出更优性能。项目页面详见 https://motion3-to-4.github.io/。",
    "url": "https://huggingface.co/papers/2601.14253",
    "arxiv_url": "https://arxiv.org/abs/2601.14253"
  },
  {
    "title": "sangkuriang: A pseudo-spectral Python library for Korteweg-de Vries soliton simulation",
    "summary": "The Korteweg-de Vries (KdV) equation serves as a foundational model in nonlinear wave physics, describing the balance between dispersive spreading and nonlinear steepening that gives rise to solitons. This article introduces sangkuriang, an open-source Python library for solving this equation using Fourier pseudo-spectral spatial discretization coupled with adaptive high-order time integration. The implementation leverages just-in-time (JIT) compilation for computational efficiency while maintaining accessibility for instructional purposes. Validation encompasses progressively complex scenarios including isolated soliton propagation, symmetric two-wave configurations, overtaking collisions between waves of differing amplitudes, and three-body interactions. Conservation of the classical invariants is monitored throughout, with deviations remaining small across all test cases. Measured soliton velocities conform closely to theoretical predictions based on the amplitude-velocity relationship characteristic of integrable systems. Complementary diagnostics drawn from information theory and recurrence analysis confirm that computed solutions preserve the regular phase-space structure expected for completely integrable dynamics. The solver outputs data in standard scientific formats compatible with common analysis tools and generates visualizations of spatiotemporal wave evolution. By combining numerical accuracy with practical accessibility on modest computational resources, sangkuriang offers a platform suitable for both classroom demonstrations of nonlinear wave phenomena and exploratory research into soliton dynamics.",
    "translation": "标题：sangkuriang：用于Korteweg-de Vries孤子模拟的伪谱Python库\n\n摘要：Korteweg-de Vries（KdV）方程作为非线性波物理的基础模型，描述了色散展宽与非线性陡化之间的平衡，这种平衡导致了孤子的产生。本文介绍sangkuriang——一个开源的Python库，该库采用傅里叶伪谱空间离散化结合自适应高阶时间积分方法来求解此方程。该实现利用即时（JIT）编译技术以提高计算效率，同时保持了教学用途的易用性。验证涵盖了逐步复杂的场景，包括孤立孤子传播、对称双波结构、不同振幅波之间的超越碰撞以及三体相互作用。在整个过程中监测了经典不变量的守恒性，所有测试案例中的偏差均保持较小。实测的孤子速度与基于可积系统特有的振幅-速度关系的理论预测高度吻合。从信息论和递归分析中提取的补充诊断证实，计算得到的解保持了完全可积动力学所预期的规则相空间结构。该求解器以兼容常见分析工具的标准科学格式输出数据，并生成时空波演化的可视化结果。通过在适度计算资源上结合数值精度与实际可操作性，sangkuriang为非线性波现象的课堂演示和孤子动力学的探索性研究提供了一个合适的平台。",
    "url": "https://huggingface.co/papers/2601.12029",
    "arxiv_url": "https://arxiv.org/abs/2601.12029"
  },
  {
    "title": "Show me the evidence: Evaluating the role of evidence and natural language explanations in AI-supported fact-checking",
    "summary": "Although much research has focused on AI explanations to support decisions in complex information-seeking tasks such as fact-checking, the role of evidence is surprisingly under-researched. In our study, we systematically varied explanation type, AI prediction certainty, and correctness of AI system advice for non-expert participants, who evaluated the veracity of claims and AI system predictions. Participants were provided the option of easily inspecting the underlying evidence. We found that participants consistently relied on evidence to validate AI claims across all experimental conditions. When participants were presented with natural language explanations, evidence was used less frequently although they relied on it when these explanations seemed insufficient or flawed. Qualitative data suggests that participants attempted to infer evidence source reliability, despite source identities being deliberately omitted. Our results demonstrate that evidence is a key ingredient in how people evaluate the reliability of information presented by an AI system and, in combination with natural language explanations, offers valuable support for decision-making. Further research is urgently needed to understand how evidence ought to be presented and how people engage with it in practice.",
    "translation": "标题：请出示证据：评估证据与自然语言解释在AI辅助事实核查中的作用\n\n摘要：尽管大量研究聚焦于AI解释如何支持复杂信息检索任务（如事实核查）中的决策，但证据的作用却鲜有研究。本研究系统性地调整了解释类型、AI预测确定性及AI系统建议的正确性，并邀请非专业参与者评估陈述与AI系统预测的真实性。参与者可选择便捷地查看底层证据。研究发现，在所有实验条件下，参与者始终依赖证据来验证AI的论断。当参与者获得自然语言解释时，证据使用频率降低，但当这些解释显得不足或有缺陷时，他们仍会依赖证据。定性数据表明，尽管实验刻意隐去了来源身份，参与者仍试图推断证据来源的可靠性。研究结果证明，证据是人们评估AI系统所提供信息可靠性的关键要素，且与自然语言解释相结合，能为决策提供有力支持。当前亟需进一步研究证据应如何呈现，以及人们在实践中如何有效利用证据。",
    "url": "https://huggingface.co/papers/2601.11387",
    "arxiv_url": "https://arxiv.org/abs/2601.11387"
  },
  {
    "title": "CURE-Med: Curriculum-Informed Reinforcement Learning for Multilingual Medical Reasoning",
    "summary": "While large language models (LLMs) have shown to perform well on monolingual mathematical and commonsense reasoning, they remain unreliable for multilingual medical reasoning applications, hindering their deployment in multilingual healthcare settings. We address this by first introducing CUREMED-BENCH, a high-quality multilingual medical reasoning dataset with open-ended reasoning queries with a single verifiable answer, spanning thirteen languages, including underrepresented languages such as Amharic, Yoruba, and Swahili. Building on this dataset, we propose CURE-MED, a curriculum-informed reinforcement learning framework that integrates code-switching-aware supervised fine-tuning and Group Relative Policy Optimization to jointly improve logical correctness and language stability. Across thirteen languages, our approach consistently outperforms strong baselines and scales effectively, achieving 85.21% language consistency and 54.35% logical correctness at 7B parameters, and 94.96% language consistency and 70.04% logical correctness at 32B parameters. These results support reliable and equitable multilingual medical reasoning in LLMs. The code and dataset are available at https://cure-med.github.io/",
    "translation": "标题：CURE-Med：基于课程学习的多语言医学推理强化学习框架\n\n摘要：尽管大语言模型在单语数学推理和常识推理任务中表现优异，但在多语言医学推理应用中仍存在可靠性不足的问题，这阻碍了其在多语言医疗场景中的实际部署。为解决这一挑战，本研究首先构建了CUREMED-BENCH数据集——一个涵盖十三种语言（包括阿姆哈拉语、约鲁巴语和斯瓦希里语等资源稀缺语言）的高质量多语言医学推理数据集，其特点为开放式推理查询与单一可验证答案。基于该数据集，我们提出CURE-Med框架，该框架采用课程化强化学习策略，融合代码切换感知的监督微调与组相对策略优化方法，以协同提升逻辑正确性与语言稳定性。在十三种语言的测试中，该方法持续超越现有基线模型并展现良好的扩展性：在70亿参数规模下实现85.21%的语言一致性与54.35%的逻辑正确率，在320亿参数规模下达到94.96%的语言一致性与70.04%的逻辑正确率。这些成果为大语言模型实现可靠且公平的多语言医学推理提供了有效支撑。代码与数据集已公开于https://cure-med.github.io/。",
    "url": "https://huggingface.co/papers/2601.13262",
    "arxiv_url": "https://arxiv.org/abs/2601.13262"
  }
]