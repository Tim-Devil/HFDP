[
  {
    "title": "P1: Mastering Physics Olympiads with Reinforcement Learning",
    "summary": "Recent progress in large language models (LLMs) has moved the frontier from puzzle-solving to science-grade reasoning-the kind needed to tackle problems whose answers must stand against nature, not merely fit a rubric. Physics is the sharpest test of this shift, which binds symbols to reality in a fundamental way, serving as the cornerstone of most modern technologies. In this work, we manage to advance physics research by developing large language models with exceptional physics reasoning capabilities, especially excel at solving Olympiad-level physics problems. We introduce P1, a family of open-source physics reasoning models trained entirely through reinforcement learning (RL). Among them, P1-235B-A22B is the first open-source model with Gold-medal performance at the latest International Physics Olympiad (IPhO 2025), and wins 12 gold medals out of 13 international/regional physics competitions in 2024/2025. P1-30B-A3B also surpasses almost all other open-source models on IPhO 2025, getting a silver medal. Further equipped with an agentic framework PhysicsMinions, P1-235B-A22B+PhysicsMinions achieves overall No.1 on IPhO 2025, and obtains the highest average score over the 13 physics competitions. Besides physics, P1 models also present great performance on other reasoning tasks like math and coding, showing the great generalibility of P1 series.",
    "translation": "标题：P1：基于强化学习的物理奥林匹克竞赛解题系统\n\n摘要：大语言模型的最新进展已将其能力边界从谜题求解推进至科学级推理——这类推理要求答案必须经得起自然规律的检验，而非仅符合评分标准。物理学作为检验这一转变的关键领域，以根本性的方式将符号与现实相联结，构成了大多数现代技术的基石。本研究通过开发具备卓越物理推理能力的大语言模型，成功推动了物理研究进展，特别是在解决奥林匹克级别物理问题方面表现突出。我们提出P1系列模型——完全基于强化学习训练的开源物理推理模型族。其中P1-235B-A22B是首个在国际物理奥林匹克竞赛（IPhO 2025）中达到金牌水平的开源模型，并在2024/2025年度的13项国际/地区物理竞赛中斩获12枚金牌。P1-30B-A3B同样在IPhO 2025中超越绝大多数开源模型，获得银牌成绩。进一步结合智能体框架PhysicsMinions后，P1-235B-A22B+PhysicsMinions在IPhO 2025中荣膺总排名第一，并在13项物理竞赛中取得最高平均分。除物理领域外，P1系列模型在数学与编程等推理任务中也展现出卓越性能，体现了该模型家族强大的泛化能力。",
    "url": "https://huggingface.co/papers/2511.13612",
    "arxiv_url": "https://arxiv.org/abs/2511.13612"
  },
  {
    "title": "MiroThinker: Pushing the Performance Boundaries of Open-Source Research Agents via Model, Context, and Interactive Scaling",
    "summary": "We present MiroThinker v1.0, an open-source research agent designed to advance tool-augmented reasoning and information-seeking capabilities. Unlike previous agents that only scale up model size or context length, MiroThinker explores interaction scaling at the model level, systematically training the model to handle deeper and more frequent agent-environment interactions as a third dimension of performance improvement. Unlike LLM test-time scaling, which operates in isolation and risks degradation with longer reasoning chains, interactive scaling leverages environment feedback and external information acquisition to correct errors and refine trajectories. Through reinforcement learning, the model achieves efficient interaction scaling: with a 256K context window, it can perform up to 600 tool calls per task, enabling sustained multi-turn reasoning and complex real-world research workflows. Across four representative benchmarks-GAIA, HLE, BrowseComp, and BrowseComp-ZH-the 72B variant achieves up to 81.9%, 37.7%, 47.1%, and 55.6% accuracy respectively, surpassing previous open-source agents and approaching commercial counterparts such as GPT-5-high. Our analysis reveals that MiroThinker benefits from interactive scaling consistently: research performance improves predictably as the model engages in deeper and more frequent agent-environment interactions, demonstrating that interaction depth exhibits scaling behaviors analogous to model size and context length. These findings establish interaction scaling as a third critical dimension for building next-generation open research agents, complementing model capacity and context windows.",
    "translation": "标题：MiroThinker：通过模型、上下文与交互式扩展突破开源研究智能体的性能边界\n\n摘要：本文推出MiroThinker v1.0开源研究智能体，旨在推进工具增强推理与信息检索能力的发展。与仅扩大模型规模或上下文长度的传统方案不同，MiroThinker开创性地在模型层面实施交互式扩展，通过系统化训练使模型能够处理更深层次、更高频次的智能体-环境交互，将其作为性能提升的第三维度。相较于孤立运行且长推理链存在性能退化风险的LLM测试时扩展，交互式扩展通过环境反馈与外部信息获取实现错误修正与路径优化。借助强化学习，该模型实现了高效的交互扩展：在256K上下文窗口支持下，单个任务可执行高达600次工具调用，支撑持续多轮推理与复杂现实研究流程。在GAIA、HLE、BrowseComp及BrowseComp-ZH四个代表性基准测试中，72B参数变体分别取得81.9%、37.7%、47.1%与55.6%的准确率，超越既有开源智能体并逼近GPT-5-high等商业系统。分析表明，MiroThinker持续受益于交互式扩展：随着智能体-环境交互深度与频次提升，研究性能呈现可预测的增长，证明交互深度具有与模型规模、上下文长度类似的扩展规律。这些发现确立了交互式扩展作为构建下一代开源研究智能体的第三关键维度，与模型能力及上下文窗口形成有效互补。",
    "url": "https://huggingface.co/papers/2511.11793",
    "arxiv_url": "https://arxiv.org/abs/2511.11793"
  },
  {
    "title": "Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data",
    "summary": "We present Uni-MoE 2.0 from the Lychee family. As a fully open-source omnimodal large model (OLM), it substantially advances Lychee's Uni-MoE series in language-centric multimodal understanding, reasoning, and generating. Based on the Qwen2.5-7B dense architecture, we build Uni-MoE-2.0-Omni from scratch through three core contributions: dynamic-capacity Mixture-of-Experts (MoE) design, a progressive training strategy enhanced with an iterative reinforcement strategy, and a carefully curated multimodal data matching technique. It is capable of omnimodal understanding, as well as generating images, text, and speech. Architecturally, our new MoE framework balances computational efficiency and capability for 10 cross-modal inputs using shared, routed, and null experts, while our Omni-Modality 3D RoPE ensures spatio-temporal cross-modality alignment in the self-attention layer. For training, following cross-modal pretraining, we use a progressive supervised fine-tuning strategy that activates modality-specific experts and is enhanced by balanced data composition and an iterative GSPO-DPO method to stabilise RL training and improve reasoning. Data-wise, the base model, trained on approximately 75B tokens of open-source multimodal data, is equipped with special speech and image generation tokens, allowing it to learn these generative tasks by conditioning its outputs on linguistic cues. Extensive evaluation across 85 benchmarks demonstrates that our model achieves SOTA or highly competitive performance against leading OLMs, surpassing Qwen2.5-Omni (trained with 1.2T tokens) on over 50 of 76 benchmarks. Key strengths include video understanding (+7% avg. of 8), omnimodallity understanding (+7% avg. of 4), and audiovisual reasoning (+4%). It also advances long-form speech processing (reducing WER by 4.2%) and leads in low-level image processing and controllable generation across 5 metrics.",
    "translation": "标题：Uni-MoE-2.0-Omni：基于先进混合专家架构、训练策略与数据技术的语言核心型全模态大模型扩展\n\n摘要：我们推出荔枝家族系列新品Uni-MoE 2.0。作为完全开源的全模态大模型，该模型在以语言为核心的多模态理解、推理与生成能力上显著推进了荔枝Uni-MoE系列的发展。基于Qwen2.5-7B稠密架构，我们通过三大核心创新实现从零构建：动态容量混合专家设计、结合迭代强化策略的渐进式训练方法，以及精心构建的多模态数据匹配技术。该模型具备全模态理解能力，并能生成图像、文本与语音。在架构层面，新型混合专家框架通过共享专家、路由专家与空置专家的协同机制，在十种跨模态输入场景中实现计算效率与模型能力的平衡，而全模态三维旋转位置编码技术则确保自注意力层的时空跨模态对齐。训练策略上，在跨模态预训练后采用渐进式监督微调，通过激活模态专属专家并结合均衡数据组合与迭代式GSPO-DPO方法，有效稳定强化学习训练过程并提升推理能力。数据层面，基于约750亿token开源多模态数据训练的基础模型，通过特殊语音与图像生成标记的引入，实现了基于语言线索的条件化生成学习。在85个基准测试的广泛评估中，本模型在76个基准测试中超过50项表现优于采用1.2万亿token训练的Qwen2.5-Omni，在领先全模态大模型中达到顶尖或极具竞争力的性能。核心优势体现在视频理解（8项测试平均提升7%）、全模态理解（4项测试平均提升7%）和视听推理（提升4%）领域，同时显著推进长语音处理（词错误率降低4.2%），并在底层图像处理与可控生成的5项指标中保持领先。",
    "url": "https://huggingface.co/papers/2511.12609",
    "arxiv_url": "https://arxiv.org/abs/2511.12609"
  },
  {
    "title": "Souper-Model: How Simple Arithmetic Unlocks State-of-the-Art LLM Performance",
    "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse domains, but their training remains resource- and time-intensive, requiring massive compute power and careful orchestration of training procedures. Model souping-the practice of averaging weights from multiple models of the same architecture-has emerged as a promising pre- and post-training technique that can enhance performance without expensive retraining. In this paper, we introduce Soup Of Category Experts (SoCE), a principled approach for model souping that utilizes benchmark composition to identify optimal model candidates and applies non-uniform weighted averaging to maximize performance. Contrary to previous uniform-averaging approaches, our method leverages the observation that benchmark categories often exhibit low inter-correlations in model performance. SoCE identifies \"expert\" models for each weakly-correlated category cluster and combines them using optimized weighted averaging rather than uniform weights. We demonstrate that the proposed method improves performance and robustness across multiple domains, including multilingual capabilities, tool calling, and math and achieves state-of-the-art results on the Berkeley Function Calling Leaderboard.",
    "translation": "标题：Souper-Model：简单算术如何实现最先进大语言模型性能\n\n摘要：大语言模型（LLMs）已在多个领域展现出卓越能力，但其训练过程仍需要耗费大量资源和时间，不仅需要强大的计算能力，还需精心设计训练流程。模型融合——即对相同架构的多个模型权重进行平均——已成为一种颇具前景的训练前/后优化技术，无需昂贵重训练即可提升模型性能。本文提出类别专家融合法（SoCE），该方法通过基准测试构成分析筛选最优模型候选集，并采用非均匀加权平均来最大化模型性能。与传统均匀平均方法不同，我们基于模型在不同基准类别间表现相关性较低的观测，为每个弱相关类别集群识别“专家”模型，并通过优化加权而非均匀权重进行组合。实验证明，该方法在多语言能力、工具调用、数学推理等多个领域均能提升模型性能与鲁棒性，并在伯克利函数调用排行榜上取得了最先进的成绩。",
    "url": "https://huggingface.co/papers/2511.13254",
    "arxiv_url": "https://arxiv.org/abs/2511.13254"
  },
  {
    "title": "Part-X-MLLM: Part-aware 3D Multimodal Large Language Model",
    "summary": "We introduce Part-X-MLLM, a native 3D multimodal large language model that unifies diverse 3D tasks by formulating them as programs in a structured, executable grammar. Given an RGB point cloud and a natural language prompt, our model autoregressively generates a single, coherent token sequence encoding part-level bounding boxes, semantic descriptions, and edit commands. This structured output serves as a versatile interface to drive downstream geometry-aware modules for part-based generation and editing. By decoupling the symbolic planning from the geometric synthesis, our approach allows any compatible geometry engine to be controlled through a single, language-native frontend. We pre-train a dual-encoder architecture to disentangle structure from semantics and instruction-tune the model on a large-scale, part-centric dataset. Experiments demonstrate that our model excels at producing high-quality, structured plans, enabling state-of-the-art performance in grounded Q\\&A, compositional generation, and localized editing through one unified interface. Project page: https://chunshi.wang/Part-X-MLLM/",
    "translation": "标题：Part-X-MLLM：面向部件感知的三维多模态大语言模型\n\n摘要：本文提出Part-X-MLLM——一种原生三维多模态大语言模型，通过将多样化三维任务构建为结构化可执行语法中的程序，实现了任务统一。给定RGB点云和自然语言提示，本模型能自回归生成单一连贯的标记序列，该序列编码了部件级边界框、语义描述及编辑指令。这种结构化输出作为多功能接口，可驱动下游几何感知模块实现基于部件的生成与编辑。通过将符号规划与几何合成解耦，我们的方法允许任何兼容的几何引擎通过单一语言原生前端进行控制。我们预训练了双编码器架构以实现结构与语义解耦，并在大规模以部件为中心的数据集上对模型进行指令微调。实验表明，该模型能出色生成高质量结构化方案，通过统一接口在具身问答、组合生成及局部化编辑任务中实现最先进性能。项目页面：https://chunshi.wang/Part-X-MLLM/",
    "url": "https://huggingface.co/papers/2511.13647",
    "arxiv_url": "https://arxiv.org/abs/2511.13647"
  },
  {
    "title": "MMaDA-Parallel: Multimodal Large Diffusion Language Models for Thinking-Aware Editing and Generation",
    "summary": "While thinking-aware generation aims to improve performance on complex tasks, we identify a critical failure mode where existing sequential, autoregressive approaches can paradoxically degrade performance due to error propagation. To systematically analyze this issue, we propose ParaBench, a new benchmark designed to evaluate both text and image output modalities. Our analysis using ParaBench reveals that this performance degradation is strongly correlated with poor alignment between the generated reasoning and the final image. To resolve this, we propose a parallel multimodal diffusion framework, MMaDA-Parallel, that enables continuous, bidirectional interaction between text and images throughout the entire denoising trajectory. MMaDA-Parallel is trained with supervised finetuning and then further optimized by Parallel Reinforcement Learning (ParaRL), a novel strategy that applies semantic rewards along the trajectory to enforce cross-modal consistency. Experiments validate that our model significantly improves cross-modal alignment and semantic consistency, achieving a 6.9\\% improvement in Output Alignment on ParaBench compared to the state-of-the-art model, Bagel, establishing a more robust paradigm for thinking-aware image synthesis. Our code is open-sourced at https://github.com/tyfeld/MMaDA-Parallel",
    "translation": "标题：MMaDA-并行：面向思维感知编辑与生成的多模态扩散大语言模型\n\n摘要：尽管思维感知生成技术旨在提升复杂任务的处理性能，但我们发现现有自回归序列方法存在关键缺陷——错误传播效应反而可能导致性能退化。为系统分析该问题，我们提出ParaBench基准测试，专门用于评估文本与图像双模态输出。基于ParaBench的分析表明，这种性能退化与生成推理过程和最终图像间的对齐偏差密切相关。为此，我们提出并行多模态扩散框架MMaDA-Parallel，通过在完整去噪轨迹中实现文本与图像的持续双向交互来解决问题。该框架首先通过监督微调进行训练，继而采用新型并行强化学习策略（ParaRL）进行优化——该策略沿去噪轨迹施加语义奖励以强化跨模态一致性。实验验证表明，我们的模型显著改善了跨模态对齐与语义连贯性，在ParaBench基准上相较最先进的Bagel模型实现了输出对齐指标6.9%的提升，为思维感知图像合成建立了更稳健的范式。代码已开源于：https://github.com/tyfeld/MMaDA-Parallel",
    "url": "https://huggingface.co/papers/2511.09611",
    "arxiv_url": "https://arxiv.org/abs/2511.09611"
  },
  {
    "title": "GroupRank: A Groupwise Reranking Paradigm Driven by Reinforcement Learning",
    "summary": "Large Language Models have shown strong potential as rerankers to enhance the overall performance of RAG systems. However, existing reranking paradigms are constrained by a core theoretical and practical dilemma: Pointwise methods, while simple and highly flexible, evaluate documents independently, making them prone to the Ranking Myopia Trap, overlooking the relative importance between documents. In contrast, Listwise methods can perceive the global ranking context, but suffer from inherent List Rigidity, leading to severe scalability and flexibility issues when handling large candidate sets. To address these challenges, we propose Groupwise, a novel reranking paradigm. In this approach, the query and a group of candidate documents are jointly fed into the model, which performs within-group comparisons to assign individual relevance scores to each document. This design retains the flexibility of Pointwise methods while enabling the comparative capability of Listwise methods. We further adopt GRPO for model training, equipped with a heterogeneous reward function that integrates ranking metrics with a distributional reward aimed at aligning score distributions across groups. To overcome the bottleneck caused by the scarcity of high quality labeled data, we further propose an innovative pipeline for synthesizing high quality retrieval and ranking data. The resulting data can be leveraged not only for training the reranker but also for training the retriever. Extensive experiments validate the effectiveness of our approach. On two reasoning intensive retrieval benchmarks, BRIGHT and R2MED.",
    "translation": "标题：GroupRank：一种强化学习驱动的分组重排序范式\n\n摘要：大语言模型展现出作为重排序器的强大潜力，能够有效提升RAG系统的整体性能。然而现有重排序范式面临核心理论与实践的困境：点式排序方法虽然简单灵活，但独立评估文档的特性使其容易陷入\"排序短视陷阱\"，忽略文档间的相对重要性；而列式排序方法虽能感知全局排序上下文，却存在固有的\"列表刚性\"缺陷，在处理大规模候选集时面临严重的可扩展性与灵活性挑战。为突破这些限制，我们提出分组式排序这一新型重排序范式。该方法将查询与候选文档组共同输入模型，通过组内比较机制为每个文档分配独立相关性分数。这种设计既保留了点式方法的灵活性，又兼具列式方法的比较能力。我们采用GRPO进行模型训练，并配置融合排序指标与分布奖励的异构奖励函数，以实现跨组分数分布的对齐。针对高质量标注数据稀缺的瓶颈，我们进一步提出创新性的检索与排序数据合成流程，所生成数据不仅可用于训练重排序器，也能用于训练检索器。大量实验验证了我们方法的有效性：在BRIGHT和R2MED两个推理密集型检索基准测试中均取得显著成效。",
    "url": "https://huggingface.co/papers/2511.11653",
    "arxiv_url": "https://arxiv.org/abs/2511.11653"
  },
  {
    "title": "TiViBench: Benchmarking Think-in-Video Reasoning for Video Generative Models",
    "summary": "The rapid evolution of video generative models has shifted their focus from producing visually plausible outputs to tackling tasks requiring physical plausibility and logical consistency. However, despite recent breakthroughs such as Veo 3's chain-of-frames reasoning, it remains unclear whether these models can exhibit reasoning capabilities similar to large language models (LLMs). Existing benchmarks predominantly evaluate visual fidelity and temporal coherence, failing to capture higher-order reasoning abilities. To bridge this gap, we propose TiViBench, a hierarchical benchmark specifically designed to evaluate the reasoning capabilities of image-to-video (I2V) generation models. TiViBench systematically assesses reasoning across four dimensions: i) Structural Reasoning & Search, ii) Spatial & Visual Pattern Reasoning, iii) Symbolic & Logical Reasoning, and iv) Action Planning & Task Execution, spanning 24 diverse task scenarios across 3 difficulty levels. Through extensive evaluations, we show that commercial models (e.g., Sora 2, Veo 3.1) demonstrate stronger reasoning potential, while open-source models reveal untapped potential that remains hindered by limited training scale and data diversity. To further unlock this potential, we introduce VideoTPO, a simple yet effective test-time strategy inspired by preference optimization. By performing LLM self-analysis on generated candidates to identify strengths and weaknesses, VideoTPO significantly enhances reasoning performance without requiring additional training, data, or reward models. Together, TiViBench and VideoTPO pave the way for evaluating and advancing reasoning in video generation models, setting a foundation for future research in this emerging field.",
    "translation": "标题：TiViBench：面向视频生成模型的视觉思维推理基准测试框架\n\n摘要：视频生成模型的快速发展已使其重点从产生视觉合理的输出转向处理需要物理合理性和逻辑一致性的任务。然而，尽管近期出现了如Veo 3的帧序列推理等突破性进展，这些模型是否具备与大型语言模型（LLM）相似的推理能力仍不明确。现有基准测试主要评估视觉保真度和时序连贯性，未能捕捉高阶推理能力。为填补这一空白，我们提出TiViBench——一个专门用于评估图像到视频（I2V）生成模型推理能力的分层基准框架。TiViBench系统性地从四个维度评估推理能力：i）结构推理与搜索，ii）空间与视觉模式推理，iii）符号与逻辑推理，以及iv）行动规划与任务执行，涵盖3个难度级别下的24种多样化任务场景。通过广泛评估，我们发现商业模型（如Sora 2、Veo 3.1）展现出更强的推理潜力，而开源模型虽存在未开发潜力，但仍受限于训练规模和数据多样性的不足。为进一步释放这种潜力，我们提出VideoTPO——一种受偏好优化启发的简易高效测试时策略。该方法通过对生成候选结果进行LLM自分析来识别优劣，无需额外训练、数据或奖励模型即可显著提升推理性能。TiViBench与VideoTPO共同为视频生成模型的推理能力评估与推进铺平道路，为这一新兴领域的未来研究奠定基础。",
    "url": "https://huggingface.co/papers/2511.13704",
    "arxiv_url": "https://arxiv.org/abs/2511.13704"
  },
  {
    "title": "PhysX-Anything: Simulation-Ready Physical 3D Assets from Single Image",
    "summary": "3D modeling is shifting from static visual representations toward physical, articulated assets that can be directly used in simulation and interaction. However, most existing 3D generation methods overlook key physical and articulation properties, thereby limiting their utility in embodied AI. To bridge this gap, we introduce PhysX-Anything, the first simulation-ready physical 3D generative framework that, given a single in-the-wild image, produces high-quality sim-ready 3D assets with explicit geometry, articulation, and physical attributes. Specifically, we propose the first VLM-based physical 3D generative model, along with a new 3D representation that efficiently tokenizes geometry. It reduces the number of tokens by 193x, enabling explicit geometry learning within standard VLM token budgets without introducing any special tokens during fine-tuning and significantly improving generative quality. In addition, to overcome the limited diversity of existing physical 3D datasets, we construct a new dataset, PhysX-Mobility, which expands the object categories in prior physical 3D datasets by over 2x and includes more than 2K common real-world objects with rich physical annotations. Extensive experiments on PhysX-Mobility and in-the-wild images demonstrate that PhysX-Anything delivers strong generative performance and robust generalization. Furthermore, simulation-based experiments in a MuJoCo-style environment validate that our sim-ready assets can be directly used for contact-rich robotic policy learning. We believe PhysX-Anything can substantially empower a broad range of downstream applications, especially in embodied AI and physics-based simulation.",
    "translation": "标题：PhysX-Anything：基于单张图像生成仿真就绪的物理三维资产\n\n摘要：三维建模正从静态视觉表示转向可直接用于仿真与交互的物理化可动资产。然而现有三维生成方法大多忽略了关键的物理属性与可动特性，限制了其在具身智能中的应用。为弥补这一差距，我们提出PhysX-Anything——首个仿真就绪的物理三维生成框架，仅需输入单张真实场景图像，即可生成具有显式几何结构、可动属性与物理特征的高质量仿真就绪三维资产。具体而言，我们提出了首个基于视觉语言模型的物理三维生成方法，并创新性地设计了一种高效表征几何结构的三维表示方法。该方法将表征所需标记数量降低193倍，在标准视觉语言模型标记预算内实现显式几何学习，无需在微调阶段引入特殊标记即可显著提升生成质量。此外，为克服现有物理三维数据集多样性不足的问题，我们构建了PhysX-Mobility数据集，将原有物理三维数据集的物体类别扩展2倍以上，涵盖2,000余个具有丰富物理标注的常见现实物体。在PhysX-Mobility数据集与真实场景图像上的大量实验表明，PhysX-Anything具有卓越的生成性能与鲁棒泛化能力。在MuJoCo风格环境中开展的仿真实验进一步验证了本方法生成的仿真就绪资产可直接用于接触密集型的机器人策略学习。我们相信PhysX-Anything将有力推动具身智能与物理仿真等下游应用的全面发展。",
    "url": "https://huggingface.co/papers/2511.13648",
    "arxiv_url": "https://arxiv.org/abs/2511.13648"
  },
  {
    "title": "UFO^3: Weaving the Digital Agent Galaxy",
    "summary": "Large language model (LLM)-powered agents are transforming digital devices from passive tools into proactive intelligent collaborators. However, most existing frameworks remain confined to a single OS or device, making cross-device workflows brittle and largely manual. We present UFO^3, a system that unifies heterogeneous endpoints, desktops, servers, mobile devices, and edge, into a single orchestration fabric. UFO^3 models each user request as a mutable TaskConstellation: a distributed DAG of atomic subtasks (TaskStars) with explicit control and data dependencies (TaskStarLines). The TaskConstellation continuously evolves as results stream in from distributed devices, enabling asynchronous execution, adaptive recovery, and dynamic optimization. A Constellation Orchestrator} executes tasks safely and asynchronously while applying dynamic DAG updates, and the Agent Interaction Protocol (AIP) provides persistent, low-latency channels for reliable task dispatch and result streaming. These designs dissolve the traditional boundaries between devices and platforms, allowing agents to collaborate seamlessly and amplify their collective intelligence.\n  We evaluate UFO^3 on NebulaBench, a benchmark of 55 cross-device tasks across 5 machines and 10 categories. UFO^3 achieves 83.3% subtask completion, 70.9% task success, exposes parallelism with an average width of 1.72, and reduces end-to-end latency by 31% relative to a sequential baseline. Fault-injection experiments demonstrate graceful degradation and recovery under transient and permanent agent failures. These results show that UFO^3 achieves accurate, efficient, and resilient task orchestration across heterogeneous devices, uniting isolated agents into a coherent, adaptive computing fabric that extends across the landscape of ubiquitous computing.",
    "translation": "标题：UFO^3：编织智能体数字星系\n\n摘要：基于大语言模型的智能体正在将数字设备从被动工具转变为主动智能协作者。然而现有框架大多局限于单一操作系统或设备，导致跨设备工作流脆弱且高度依赖人工操作。我们提出UFO^3系统，该系统将异构终端设备（包括桌面端、服务器、移动设备与边缘节点）统一整合为协同编排架构。UFO^3将用户请求建模为可演化的任务星群：通过具有显式控制流与数据依赖关系的原子化子任务构成分布式有向无环图。随着分布式设备持续返回执行结果，任务星群持续动态演化，支持异步执行、自适应恢复与动态优化。星群编排器在实施动态图更新的同时保障任务安全异步执行，而智能体交互协议则通过持久化低延迟通道实现可靠的任务调度与结果流式传输。这些设计打破了设备与平台间的传统壁垒，使智能体能够无缝协作并增强集体智能。\n\n我们在包含5台设备、10个类别、55项跨设备任务的NebulaBench基准测试中评估UFO^3。实验表明：该系统实现83.3%的子任务完成率与70.9%的整体任务成功率，平均并行宽度达1.72，相较顺序执行基线端到端延迟降低31%。故障注入实验证明系统在瞬态与永久性智能体故障场景下均能实现优雅降级与恢复。这些结果表明，UFO^3在异构设备间实现了精准、高效、鲁棒的任务编排，将孤立智能体整合为贯穿普适计算场景的连贯自适应计算架构。",
    "url": "https://huggingface.co/papers/2511.11332",
    "arxiv_url": "https://arxiv.org/abs/2511.11332"
  },
  {
    "title": "Evolve the Method, Not the Prompts: Evolutionary Synthesis of Jailbreak Attacks on LLMs",
    "summary": "Automated red teaming frameworks for Large Language Models (LLMs) have become increasingly sophisticated, yet they share a fundamental limitation: their jailbreak logic is confined to selecting, combining, or refining pre-existing attack strategies. This binds their creativity and leaves them unable to autonomously invent entirely new attack mechanisms. To overcome this gap, we introduce EvoSynth, an autonomous framework that shifts the paradigm from attack planning to the evolutionary synthesis of jailbreak methods. Instead of refining prompts, EvoSynth employs a multi-agent system to autonomously engineer, evolve, and execute novel, code-based attack algorithms. Crucially, it features a code-level self-correction loop, allowing it to iteratively rewrite its own attack logic in response to failure. Through extensive experiments, we demonstrate that EvoSynth not only establishes a new state-of-the-art by achieving an 85.5\\% Attack Success Rate (ASR) against highly robust models like Claude-Sonnet-4.5, but also generates attacks that are significantly more diverse than those from existing methods. We release our framework to facilitate future research in this new direction of evolutionary synthesis of jailbreak methods. Code is available at: https://github.com/dongdongunique/EvoSynth.",
    "translation": "标题：进化方法而非提示：大语言模型越狱攻击的演化合成\n\n摘要：大语言模型自动红队测试框架日益精进，但其仍存在根本性局限：越狱逻辑始终受限于选择、组合或优化既有攻击策略。这种束缚限制了系统创造性，使其无法自主发明全新攻击机制。为突破此限制，我们提出EvoSynth框架，将范式从攻击规划转向越狱方法的演化合成。该框架采用多智能体系统自主设计、进化并执行基于代码的新型攻击算法，其核心突破在于引入代码级自修正循环机制，使系统能够根据失败反馈迭代重写自身攻击逻辑。大量实验表明，EvoSynth不仅在对Claude-Sonnet-4.5等高鲁棒性模型的测试中达到85.5%的攻击成功率，创下最新技术标杆，其生成的攻击方法多样性也显著超越现有技术。我们公开此框架以促进越狱方法演化合成这一新方向的研究。代码地址：https://github.com/dongdongunique/EvoSynth。",
    "url": "https://huggingface.co/papers/2511.12710",
    "arxiv_url": "https://arxiv.org/abs/2511.12710"
  },
  {
    "title": "Back to Basics: Let Denoising Generative Models Denoise",
    "summary": "Today's denoising diffusion models do not \"denoise\" in the classical sense, i.e., they do not directly predict clean images. Rather, the neural networks predict noise or a noised quantity. In this paper, we suggest that predicting clean data and predicting noised quantities are fundamentally different. According to the manifold assumption, natural data should lie on a low-dimensional manifold, whereas noised quantities do not. With this assumption, we advocate for models that directly predict clean data, which allows apparently under-capacity networks to operate effectively in very high-dimensional spaces. We show that simple, large-patch Transformers on pixels can be strong generative models: using no tokenizer, no pre-training, and no extra loss. Our approach is conceptually nothing more than \"Just image Transformers\", or JiT, as we call it. We report competitive results using JiT with large patch sizes of 16 and 32 on ImageNet at resolutions of 256 and 512, where predicting high-dimensional noised quantities can fail catastrophically. With our networks mapping back to the basics of the manifold, our research goes back to basics and pursues a self-contained paradigm for Transformer-based diffusion on raw natural data.",
    "translation": "标题：回归本质：让去噪生成模型真正实现去噪\n\n摘要：当今的去噪扩散模型并非以经典意义实现\"去噪\"，即不直接预测干净图像。相反，神经网络预测的是噪声或含噪量值。本文提出，预测干净数据与预测含噪量值存在本质区别。根据流形假设，自然数据应位于低维流形上，而含噪量值则不然。基于此假设，我们主张采用直接预测干净数据的模型，这使得表观容量不足的网络能在高维空间中有效运作。我们证明：无需标记器、预训练或额外损失的简单大尺寸块Transformer在像素层面即可成为强效生成模型。该方法在概念上仅体现为\"纯图像Transformer\"（简称JiT）。我们在ImageNet数据集256×256和512×512分辨率上使用16×16和32×32大尺寸块进行实验，结果表明JiT能取得具有竞争力的效果——而在相同设置下预测高维含噪量值可能导致灾难性失败。通过让网络回归流形本质，我们的研究既回归基础理论，又致力于在原始自然数据上构建自包含的基于Transformer的扩散范式。",
    "url": "https://huggingface.co/papers/2511.13720",
    "arxiv_url": "https://arxiv.org/abs/2511.13720"
  },
  {
    "title": "OlmoEarth: Stable Latent Image Modeling for Multimodal Earth Observation",
    "summary": "Earth observation data presents a unique challenge: it is spatial like images, sequential like video or text, and highly multimodal. We present OlmoEarth: a multimodal, spatio-temporal foundation model that employs a novel self-supervised learning formulation, masking strategy, and loss all designed for the Earth observation domain. OlmoEarth achieves state-of-the-art performance compared to 12 other foundation models across a variety of research benchmarks and real-world tasks from external partners. When evaluating embeddings OlmoEarth achieves the best performance on 15 out of 24 tasks, and with full fine-tuning it is the best on 19 of 29 tasks. We deploy OlmoEarth as the backbone of an end-to-end platform for data collection, labeling, training, and inference of Earth observation models. The OlmoEarth Platform puts frontier foundation models and powerful data management tools into the hands of non-profits and NGOs working to solve the world's biggest problems. OlmoEarth source code, training data, and pre-trained weights are available at https://github.com/allenai/olmoearth_pretrain{https://github.com/allenai/olmoearth_pretrain}.",
    "translation": "标题：OlmoEarth：面向多模态地球观测的稳定潜在图像建模\n\n摘要：地球观测数据呈现出独特的挑战性：既具备图像的空间特性，又具有视频或文本的时序特征，且呈现高度多模态特性。本文提出OlmoEarth——一个多模态时空基础模型，采用专为地球观测领域设计的新型自监督学习框架、掩码策略与损失函数。在各类研究基准测试和来自外部合作伙伴的实际任务中，OlmoEarth相较于其他12种基础模型实现了最先进的性能表现。在嵌入向量评估中，OlmoEarth在24项任务中的15项取得最佳性能；经过完整微调后，其在29项任务中的19项表现最优。我们将OlmoEarth部署为端到端地球观测模型平台的核心架构，该平台涵盖数据采集、标注、训练及推理全流程。OlmoEarth平台将前沿基础模型与强大数据管理工具赋能致力于解决全球重大问题的非营利组织与非政府机构。OlmoEarth的源代码、训练数据与预训练权重已发布于https://github.com/allenai/olmoearth_pretrain。",
    "url": "https://huggingface.co/papers/2511.13655",
    "arxiv_url": "https://arxiv.org/abs/2511.13655"
  },
  {
    "title": "Live-SWE-agent: Can Software Engineering Agents Self-Evolve on the Fly?",
    "summary": "Large Language Models (LLMs) are reshaping almost all industries, including software engineering. In recent years, a number of LLM agents have been proposed to solve real-world software problems. Such software agents are typically equipped with a suite of coding tools and can autonomously decide the next actions to form complete trajectories to solve end-to-end software tasks. While promising, they typically require dedicated design and may still be suboptimal, since it can be extremely challenging and costly to exhaust the entire agent scaffold design space. Recognizing that software agents are inherently software themselves that can be further refined/modified, researchers have proposed a number of self-improving software agents recently, including the Darwin-Gödel Machine (DGM). Meanwhile, such self-improving agents require costly offline training on specific benchmarks and may not generalize well across different LLMs or benchmarks. In this paper, we propose Live-SWE-agent, the first live software agent that can autonomously and continuously evolve itself on-the-fly during runtime when solving real-world software problems. More specifically, Live-SWE-agent starts with the most basic agent scaffold with only access to bash tools (e.g., mini-SWE-agent), and autonomously evolves its own scaffold implementation while solving real-world software problems. Our evaluation on the widely studied SWE-bench Verified benchmark shows that Live-SWE-agent can achieve an impressive solve rate of 75.4% without test-time scaling, outperforming all existing open-source software agents and approaching the performance of the best proprietary solution. Moreover, Live-SWE-agent outperforms state-of-the-art manually crafted software agents on the recent SWE-Bench Pro benchmark, achieving the best-known solve rate of 45.8%.",
    "translation": "标题：Live-SWE-agent：软件工程智能体能否实现实时自主演进？\n\n摘要：大型语言模型正在重塑包括软件工程在内的几乎所有行业领域。近年来，已有诸多基于大语言模型的智能体被提出以解决现实世界中的软件问题。此类软件智能体通常配备一套编码工具，能够自主决策后续操作以形成完整任务轨迹，从而解决端到端的软件任务。尽管前景可观，这些智能体往往需要专门设计且仍可能非最优解，因为穷尽整个智能体框架设计空间极具挑战且成本高昂。鉴于软件智能体本质上是可被进一步优化/修改的软件实体，研究者近期提出了若干自改进型软件智能体，包括达尔文-哥德尔机。然而，这类自改进智能体需要在特定基准测试上进行昂贵的离线训练，且在不同大语言模型或基准测试间可能泛化能力有限。本文提出Live-SWE-agent——首个能在解决现实软件问题过程中实时自主持续演进的在线软件智能体。具体而言，该智能体从仅配备基础bash工具的最简框架起步，在解决实际软件问题时自主演进其框架实现。在广泛研究的SWE-bench Verified基准测试中，我们的评估表明Live-SWE-agent无需测试时扩展即可实现75.4%的惊人解决率，超越所有现有开源软件智能体，接近最佳专有解决方案的性能。此外，Live-SWE-agent在最新的SWE-Bench Pro基准测试中优于最先进的人工设计软件智能体，以45.8%的解决率创下当前最佳纪录。",
    "url": "https://huggingface.co/papers/2511.13646",
    "arxiv_url": "https://arxiv.org/abs/2511.13646"
  },
  {
    "title": "Genomic Next-Token Predictors are In-Context Learners",
    "summary": "In-context learning (ICL) -- the capacity of a model to infer and apply abstract patterns from examples provided within its input -- has been extensively studied in large language models trained for next-token prediction on human text. In fact, prior work often attributes this emergent behavior to distinctive statistical properties in human language. This raises a fundamental question: can ICL arise organically in other sequence domains purely through large-scale predictive training?\n  To explore this, we turn to genomic sequences, an alternative symbolic domain rich in statistical structure. Specifically, we study the Evo2 genomic model, trained predominantly on next-nucleotide (A/T/C/G) prediction, at a scale comparable to mid-sized LLMs. We develop a controlled experimental framework comprising symbolic reasoning tasks instantiated in both linguistic and genomic forms, enabling direct comparison of ICL across genomic and linguistic models. Our results show that genomic models, like their linguistic counterparts, exhibit log-linear gains in pattern induction as the number of in-context demonstrations increases. To the best of our knowledge, this is the first evidence of organically emergent ICL in genomic sequences, supporting the hypothesis that ICL arises as a consequence of large-scale predictive modeling over rich data. These findings extend emergent meta-learning beyond language, pointing toward a unified, modality-agnostic view of in-context learning.",
    "translation": "标题：基因组下一标记预测器具备上下文学习能力\n\n摘要：上下文学习（ICL）——即模型从输入中提供的示例推断并应用抽象模式的能力——已在基于人类文本进行下一标记预测训练的大语言模型中得到广泛研究。事实上，先前研究常将这种涌现行为归因于人类语言独特的统计特性。这引发了一个根本性问题：在其他序列领域，ICL能否纯粹通过大规模预测训练自然涌现？为探索此问题，我们转向基因组序列这一富含统计结构的替代符号领域。具体而言，我们研究了主要基于下一核苷酸（A/T/C/G）预测训练的Evo2基因组模型，其训练规模与中型LLM相当。我们开发了受控实验框架，包含以语言形式和基因组形式实例化的符号推理任务，实现了基因组模型与语言模型在ICL能力上的直接比较。研究结果表明，与语言模型类似，基因组模型在模式归纳能力上随着上下文示例数量的增加呈现对数线性增长。据我们所知，这是首次在基因组序列中发现自然涌现的ICL能力，支持了“ICL是丰富数据大规模预测建模的产物”这一假说。这些发现将涌现元学习拓展至语言领域之外，为构建统一、模态无关的上下文学习理论指明了方向。",
    "url": "https://huggingface.co/papers/2511.12797",
    "arxiv_url": "https://arxiv.org/abs/2511.12797"
  },
  {
    "title": "WebCoach: Self-Evolving Web Agents with Cross-Session Memory Guidance",
    "summary": "Multimodal LLM-powered agents have recently demonstrated impressive capabilities in web navigation, enabling agents to complete complex browsing tasks across diverse domains. However, current agents struggle with repetitive errors and lack the ability to learn from past experiences across sessions, limiting their long-term robustness and sample efficiency. We introduce WebCoach, a model-agnostic self-evolving framework that equips web browsing agents with persistent cross-session memory, enabling improved long-term planning, reflection, and continual learning without retraining. WebCoach consists of three key components: (1) a WebCondenser, which standardizes raw navigation logs into concise summaries; (2) an External Memory Store, which organizes complete trajectories as episodic experiences; and (3) a Coach, which retrieves relevant experiences based on similarity and recency, and decides whether to inject task-specific advice into the agent via runtime hooks. This design empowers web agents to access long-term memory beyond their native context window, improving robustness in complex browsing tasks. Moreover, WebCoach achieves self-evolution by continuously curating episodic memory from new navigation trajectories, enabling agents to improve over time without retraining. Evaluations on the WebVoyager benchmark demonstrate that WebCoach consistently improves the performance of browser-use agents across three different LLM backbones. With a 38B model, it increases task success rates from 47% to 61% while reducing or maintaining the average number of steps. Notably, smaller base models with WebCoach achieve performance comparable to the same web agent using GPT-4o.",
    "translation": "标题：WebCoach：具备跨会话记忆引导的自演进网络智能体\n\n摘要：多模态大语言模型驱动的智能体近期在网络浏览任务中展现出卓越能力，能够完成跨领域的复杂浏览任务。然而，现有智能体存在重复性错误问题，且缺乏跨会话经验学习能力，限制了其长期鲁棒性与样本效率。本文提出WebCoach——一个模型无关的自演进框架，通过赋予网络浏览智能体持续性的跨会话记忆，实现无需重新训练即可提升长期规划、反思与持续学习能力。该框架包含三个核心组件：（1）WebCondenser：将原始导航日志标准化为精简摘要；（2）外部记忆库：将完整操作轨迹组织为情景化经验；（3）教练模块：基于相似度与时效性检索相关经验，并通过运行时钩子决策是否向智能体注入任务特定建议。该设计使网络智能体能够突破原生上下文窗口限制访问长期记忆，显著提升复杂浏览任务的鲁棒性。此外，WebCoach通过持续整合新导航轨迹至情景记忆库实现自我演进，使智能体无需重训练即可持续优化。在WebVoyager基准测试中，WebCoach在三种不同大语言模型基座上均能稳定提升浏览器使用智能体的性能。使用38B参数模型时，任务成功率从47%提升至61%，同时减少或维持平均操作步数。值得注意的是，搭载WebCoach的较小基座模型可实现与使用GPT-4o的同等网络智能体相媲美的性能。",
    "url": "https://huggingface.co/papers/2511.12997",
    "arxiv_url": "https://arxiv.org/abs/2511.12997"
  },
  {
    "title": "Assessing LLMs for Serendipity Discovery in Knowledge Graphs: A Case for Drug Repurposing",
    "summary": "Large Language Models (LLMs) have greatly advanced knowledge graph question answering (KGQA), yet existing systems are typically optimized for returning highly relevant but predictable answers. A missing yet desired capacity is to exploit LLMs to suggest surprise and novel (\"serendipitious\") answers. In this paper, we formally define the serendipity-aware KGQA task and propose the SerenQA framework to evaluate LLMs' ability to uncover unexpected insights in scientific KGQA tasks. SerenQA includes a rigorous serendipity metric based on relevance, novelty, and surprise, along with an expert-annotated benchmark derived from the Clinical Knowledge Graph, focused on drug repurposing. Additionally, it features a structured evaluation pipeline encompassing three subtasks: knowledge retrieval, subgraph reasoning, and serendipity exploration. Our experiments reveal that while state-of-the-art LLMs perform well on retrieval, they still struggle to identify genuinely surprising and valuable discoveries, underscoring a significant room for future improvements. Our curated resources and extended version are released at: https://cwru-db-group.github.io/serenQA.",
    "translation": "标题：评估大语言模型在知识图谱中的偶然性发现能力：以药物重定位为例\n\n摘要：大语言模型显著推动了知识图谱问答系统的发展，然而现有系统通常针对返回高度相关但可预测的答案进行优化。目前尚缺乏利用大语言模型提供具有惊喜感和新颖性（\"偶然性\"）答案的能力。本文正式定义了偶然性感知的知识图谱问答任务，并提出SerenQA框架来评估大语言模型在科学知识图谱问答任务中发现意外洞见的能力。该框架包含基于相关性、新颖性和惊喜度的严谨偶然性度量指标，以及源自临床知识图谱并聚焦药物重定位领域的专家标注基准。此外，该框架还设计了包含三个子任务的结构化评估流程：知识检索、子图推理和偶然性探索。实验结果表明，虽然最先进的大语言模型在检索任务上表现良好，但在识别真正具有惊喜度和价值的发现方面仍存在困难，这凸显了未来改进的重要空间。我们整理的资源及扩展版本发布于：https://cwru-db-group.github.io/serenQA。",
    "url": "https://huggingface.co/papers/2511.12472",
    "arxiv_url": "https://arxiv.org/abs/2511.12472"
  },
  {
    "title": "Test-Time Spectrum-Aware Latent Steering for Zero-Shot Generalization in Vision-Language Models",
    "summary": "Vision-Language Models (VLMs) excel at zero-shot inference but often degrade under test-time domain shifts. For this reason, episodic test-time adaptation strategies have recently emerged as powerful techniques for adapting VLMs to a single unlabeled image. However, existing adaptation strategies, such as test-time prompt tuning, typically require backpropagating through large encoder weights or altering core model components. In this work, we introduce Spectrum-Aware Test-Time Steering (STS), a lightweight adaptation framework that extracts a spectral subspace from the textual embeddings to define principal semantic directions and learns to steer latent representations in a spectrum-aware manner by adapting a small number of per-sample shift parameters to minimize entropy across augmented views. STS operates entirely at inference in the latent space, without backpropagation through or modification of the frozen encoders. Building on standard evaluation protocols, our comprehensive experiments demonstrate that STS largely surpasses or compares favorably against state-of-the-art test-time adaptation methods, while introducing only a handful of additional parameters and achieving inference speeds up to 8x faster with a 12x smaller memory footprint than conventional test-time prompt tuning. The code is available at https://github.com/kdafnis/STS.",
    "translation": "标题：测试时频谱感知潜空间导向在视觉语言模型零样本泛化中的应用\n\n摘要：视觉语言模型在零样本推理方面表现卓越，但常因测试时域偏移而性能下降。为此，近期出现的临时测试时自适应策略已成为将视觉语言模型适配至单张未标注图像的有效技术。然而，现有适配策略（如测试时提示调优）通常需要对大型编码器权重进行反向传播或改变核心模型组件。本研究提出频谱感知测试时导向（STS）——一种轻量级适配框架，该框架从文本嵌入中提取频谱子空间以定义主要语义方向，并通过调整少量样本级偏移参数以最小化增强视图间的信息熵，从而实现频谱感知的潜表征导向。STS完全在推理阶段于潜空间内运行，无需对冻结编码器进行反向传播或结构修改。基于标准评估协议的综合实验表明，STS在多数情况下显著超越或优于最先进的测试时自适应方法，同时仅引入少量额外参数，推理速度提升最高达8倍，内存占用较传统测试时提示调优减少12倍。代码已发布于https://github.com/kdafnis/STS。",
    "url": "https://huggingface.co/papers/2511.09809",
    "arxiv_url": "https://arxiv.org/abs/2511.09809"
  },
  {
    "title": "UnSAMv2: Self-Supervised Learning Enables Segment Anything at Any Granularity",
    "summary": "The Segment Anything Model (SAM) family has become a widely adopted vision foundation model, but its ability to control segmentation granularity remains limited. Users often need to refine results manually - by adding more prompts or selecting from pre-generated masks - to achieve the desired level of detail. This process can be ambiguous, as the same prompt may correspond to several plausible masks, and collecting dense annotations across all granularities is prohibitively expensive, making supervised solutions infeasible. To address this limitation, we introduce UnSAMv2, which enables segment anything at any granularity without human annotations. UnSAMv2 extends the divide-and-conquer strategy of UnSAM by discovering abundant mask-granularity pairs and introducing a novel granularity control embedding that enables precise, continuous control over segmentation scale. Remarkably, with only 6K unlabeled images and 0.02% additional parameters, UnSAMv2 substantially enhances SAM-2, achieving segment anything at any granularity across interactive, whole-image, and video segmentation tasks. Evaluated on over 11 benchmarks, UnSAMv2 improves NoC_{90} (5.69 rightarrow 4.75), 1-IoU (58.0 rightarrow 73.1), and AR_{1000} (49.6 rightarrow 68.3), showing that small amounts of unlabeled data with a granularity-aware self-supervised learning method can unlock the potential of vision foundation models.",
    "translation": "标题：UnSAMv2：自监督学习实现任意粒度图像分割\n\n摘要：Segment Anything Model（SAM）系列已成为广泛采用的视觉基础模型，但其控制分割粒度的能力仍存在局限。用户常需通过添加更多提示或从预生成掩码中筛选来手动优化结果，以获得理想细节层级。这一过程存在不确定性，因为相同提示可能对应多个合理掩码，且在所有粒度级别收集密集标注的成本过高，使得监督式解决方案难以实施。为解决此局限，我们提出UnSAMv2，无需人工标注即可实现任意粒度级别的图像分割。该模型扩展了UnSAM的分治策略，通过发掘丰富的掩码-粒度配对数据，并引入新型粒度控制嵌入机制，实现对分割尺度的精准连续控制。值得注意的是，仅使用6千张无标注图像和0.02%的附加参数，UnSAMv2就显著增强了SAM-2模型，在交互式分割、全图分割和视频分割任务中实现任意粒度分割。经超过11个基准测试验证，UnSAMv2将NoC_{90}指标从5.69提升至4.75，1-IoU从58.0提升至73.1，AR_{1000}从49.6提升至68.3，证明结合粒度感知自监督学习方法的小规模无标注数据能够释放视觉基础模型的潜力。",
    "url": "https://huggingface.co/papers/2511.13714",
    "arxiv_url": "https://arxiv.org/abs/2511.13714"
  },
  {
    "title": "MicroVQA++: High-Quality Microscopy Reasoning Dataset with Weakly Supervised Graphs for Multimodal Large Language Model",
    "summary": "Multimodal Large Language Models are increasingly applied to biomedical imaging, yet scientific reasoning for microscopy remains limited by the scarcity of large-scale, high-quality training data. We introduce MicroVQA++, a three-stage, large-scale and high-quality microscopy VQA corpus derived from the BIOMEDICA archive. Stage one bootstraps supervision from expert-validated figure-caption pairs sourced from peer-reviewed articles. Stage two applies HiCQA-Graph, a novel heterogeneous graph over images, captions, and QAs that fuses NLI-based textual entailment, CLIP-based vision-language alignment, and agent signals to identify and filter inconsistent samples. Stage three uses a MultiModal Large Language Model (MLLM) agent to generate multiple-choice questions (MCQ) followed by human screening. The resulting release comprises a large training split and a human-checked test split whose Bloom's level hard-sample distribution exceeds the MicroVQA benchmark. Our work delivers (i) a quality-controlled dataset that couples expert literature with graph-based filtering and human refinement; (ii) HiCQA-Graph, the first graph that jointly models (image, caption, QA) for cross-modal consistency filtering; (iii) evidence that careful data construction enables 4B-scale MLLMs to reach competitive microscopy reasoning performance (e.g., GPT-5) and achieve state-of-the-art performance among open-source MLLMs. Code and dataset will be released after the review process concludes.",
    "translation": "标题：MicroVQA++：面向多模态大语言模型的高质量显微图像推理数据集与弱监督图结构\n\n摘要：多模态大语言模型在生物医学成像中的应用日益广泛，然而显微图像的科学推理能力仍受限于大规模高质量训练数据的稀缺。我们提出MicroVQA++——一个源自BIOMEDICA档案的三阶段、大规模高质量显微视觉问答语料库。第一阶段通过经专家验证的论文图表-标题对实现监督引导；第二阶段应用HiCQA-Graph新型异质图结构，融合基于自然语言推理的文本蕴含分析、基于CLIP的视觉-语言对齐以及智能体信号，实现对图像、标题与问答三元组的跨模态一致性筛选；第三阶段采用多模态大语言模型智能体生成多项选择题，并经过人工筛查。最终发布的数据集包含大规模训练集和经人工校验的测试集，其布鲁姆分类学难度样本分布超越MicroVQA基准。本研究贡献包括：（1）通过文献专家知识与图结构过滤及人工精修相结合的质量控制数据集；（2）首个联合建模图像-标题-问答三元组以实现跨模态一致性过滤的HiCQA-Graph图结构；（3）实证表明精细的数据构建能使40亿参数级多模态大语言模型达到与GPT-5相当的显微图像推理性能，并在开源模型中实现最优效果。代码与数据集将在评审结束后公开。",
    "url": "https://huggingface.co/papers/2511.11407",
    "arxiv_url": "https://arxiv.org/abs/2511.11407"
  },
  {
    "title": "Dynamic Reflections: Probing Video Representations with Text Alignment",
    "summary": "The alignment of representations from different modalities has recently been shown to provide insights on the structural similarities and downstream capabilities of different encoders across diverse data types. While significant progress has been made in aligning images with text, the temporal nature of video data remains largely unexplored in this context. In this work, we conduct the first comprehensive study of video-text representation alignment, probing the capabilities of modern video and language encoders. Our findings reveal several key insights. First, we demonstrate that cross-modal alignment highly depends on the richness of both visual (static images vs. multi-frame videos) and text (single caption vs. a collection) data provided at test time, especially when using state-of-the-art video encoders. We propose parametric test-time scaling laws that capture this behavior and show remarkable predictive power against empirical observations. Secondly, we investigate the correlation between semantic alignment and performance on both semantic and non-semantic downstream tasks, providing initial evidence that strong alignment against text encoders may be linked to general-purpose video representation and understanding. Finally, we correlate temporal reasoning with cross-modal alignment providing a challenging test-bed for vision and language models. Overall, our work introduces video-text alignment as an informative zero-shot way to probe the representation power of different encoders for spatio-temporal data. Project page can be found at https://video-prh.github.io/",
    "translation": "标题：动态反思：通过文本对齐探究视频表征能力\n\n摘要：多模态表征对齐技术近年来为揭示不同编码器在跨数据类型中的结构相似性与下游任务能力提供了新的视角。尽管图像与文本对齐已取得显著进展，但视频数据的时间特性在此研究框架下仍待深入探索。本研究首次对视频-文本表征对齐展开系统性分析，深入探究现代视频与语言编码器的能力。我们的研究获得以下关键发现：首先，实验证明跨模态对齐效果高度依赖于测试时提供的视觉数据（静态图像与多帧视频）与文本数据（单一描述与集合描述）的丰富程度，这一特性在使用前沿视频编码器时尤为显著。我们提出参数化测试时扩展定律来刻画该现象，该定律在实证观察中展现出卓越的预测能力。其次，我们探究了语义对齐与语义/非语义下游任务性能间的关联性，初步证据表明与文本编码器的强对齐可能关联着通用视频表征与理解能力。最后，我们将时序推理能力与跨模态对齐建立关联，为视觉语言模型提供了具有挑战性的测试基准。总体而言，本研究提出视频-文本对齐作为一种零样本评估方法，为探究时空数据编码器的表征能力提供了有效途径。项目页面详见：https://video-prh.github.io/",
    "url": "https://huggingface.co/papers/2511.02767",
    "arxiv_url": "https://arxiv.org/abs/2511.02767"
  },
  {
    "title": "LoCoBench-Agent: An Interactive Benchmark for LLM Agents in Long-Context Software Engineering",
    "summary": "As large language models (LLMs) evolve into sophisticated autonomous agents capable of complex software development tasks, evaluating their real-world capabilities becomes critical. While existing benchmarks like LoCoBench~qiu2025locobench assess long-context code understanding, they focus on single-turn evaluation and cannot capture the multi-turn interactive nature, tool usage patterns, and adaptive reasoning required by real-world coding agents. We introduce LoCoBench-Agent, a comprehensive evaluation framework specifically designed to assess LLM agents in realistic, long-context software engineering workflows. Our framework extends LoCoBench's 8,000 scenarios into interactive agent environments, enabling systematic evaluation of multi-turn conversations, tool usage efficiency, error recovery, and architectural consistency across extended development sessions. We also introduce an evaluation methodology with 9 metrics across comprehension and efficiency dimensions. Our framework provides agents with 8 specialized tools (file operations, search, code analysis) and evaluates them across context lengths ranging from 10K to 1M tokens, enabling precise assessment of long-context performance. Through systematic evaluation of state-of-the-art models, we reveal several key findings: (1) agents exhibit remarkable long-context robustness; (2) comprehension-efficiency trade-off exists with negative correlation, where thorough exploration increases comprehension but reduces efficiency; and (3) conversation efficiency varies dramatically across models, with strategic tool usage patterns differentiating high-performing agents. As the first long-context LLM agent benchmark for software engineering, LoCoBench-Agent establishes a rigorous foundation for measuring agent capabilities, identifying performance gaps, and advancing autonomous software development at scale.",
    "translation": "标题：LoCoBench-Agent：长上下文软件工程中LLM智能体的交互式基准测试框架\n\n摘要：随着大语言模型（LLM）逐渐发展成为能够执行复杂软件开发任务的自主智能体，评估其实际能力变得至关重要。现有基准测试（如LoCoBench~qiu2025locobench）虽能评估长上下文代码理解能力，但其采用单轮评估模式，无法捕捉现实编码智能体所需的多轮交互特性、工具使用模式和自适应推理能力。我们提出LoCoBench-Agent——一个专门用于评估LLM智能体在真实长上下文软件工程工作流中表现的综合性评估框架。该框架将LoCoBench的8,000个场景扩展为交互式智能体环境，支持系统化评估多轮对话、工具使用效率、错误恢复能力以及跨时段开发中的架构一致性。我们还提出了包含理解度与效率维度共9项指标的评估方法。本框架为智能体提供8种专业工具（文件操作、搜索、代码分析等），并在10K至1M标记的上下文长度范围内进行评估，实现对长上下文性能的精准度量。通过对前沿模型的系统化评估，我们获得三项关键发现：（1）智能体展现出卓越的长上下文鲁棒性；（2）存在理解度与效率的负相关权衡，即深入探索会提升理解度但降低效率；（3）不同模型的对话效率差异显著，战略性的工具使用模式是高性能智能体的关键区分特征。作为软件工程领域首个长上下文LLM智能体基准测试框架，LoCoBench-Agent为衡量智能体能力、识别性能差距及推进规模化自主软件开发奠定了严谨基础。",
    "url": "https://huggingface.co/papers/2511.13998",
    "arxiv_url": "https://arxiv.org/abs/2511.13998"
  },
  {
    "title": "SafeGRPO: Self-Rewarded Multimodal Safety Alignment via Rule-Governed Policy Optimization",
    "summary": "Multimodal large language models (MLLMs) have demonstrated impressive reasoning and instruction-following capabilities, yet their expanded modality space introduces new compositional safety risks that emerge from complex text-image interactions. Such cross-modal couplings can produce unsafe semantics even when individual inputs are benign, exposing the fragile safety awareness of current MLLMs. While recent works enhance safety by guiding models to reason about potential risks, unregulated reasoning traces may compromise alignment; although Group Relative Policy Optimization (GRPO) offers self-rewarded refinement without human supervision, it lacks verifiable signals for reasoning safety. To address this, we propose SafeGRPO a self-rewarded multimodal safety alignment framework that integrates rule-governed reward construction into GRPO, enabling interpretable and verifiable optimization of reasoning safety. Built upon the constructed SafeTag-VL-3K dataset with explicit visual, textual, and combined safety tags, SafeGRPO performs step-guided safety thinking to enforce structured reasoning and behavior alignment, substantially improving multimodal safety awareness, compositional robustness, and reasoning stability across diverse benchmarks without sacrificing general capabilities.",
    "translation": "标题：SafeGRPO：基于规则约束策略优化的自奖励多模态安全对齐方法\n\n摘要：多模态大语言模型虽展现出卓越的推理与指令跟随能力，但其扩展的模态空间引发了由复杂图文交互产生的新型组合式安全风险。这种跨模态耦合即使在单模态输入无害时也可能生成不安全语义，暴露出现有模型脆弱的安全认知。现有研究虽通过引导模型进行风险推理来增强安全性，但未受约束的推理轨迹可能破坏对齐效果；尽管群体相对策略优化（GRPO）提供了无需人工监督的自奖励优化机制，其缺乏可验证的推理安全保障。为此，我们提出SafeGRPO框架，将规则约束的奖励构建机制融入GRPO，实现可解释、可验证的推理安全优化。基于构建的包含显式视觉、文本及组合安全标签的SafeTag-VL-3K数据集，SafeGRPO通过步骤引导的安全思维机制强化结构化推理与行为对齐，在保持通用能力的同时，显著提升了多模态安全认知、组合鲁棒性及跨基准推理稳定性。",
    "url": "https://huggingface.co/papers/2511.12982",
    "arxiv_url": "https://arxiv.org/abs/2511.12982"
  },
  {
    "title": "AI-Salesman: Towards Reliable Large Language Model Driven Telemarketing",
    "summary": "Goal-driven persuasive dialogue, exemplified by applications like telemarketing, requires sophisticated multi-turn planning and strict factual faithfulness, which remains a significant challenge for even state-of-the-art Large Language Models (LLMs). A lack of task-specific data often limits previous works, and direct LLM application suffers from strategic brittleness and factual hallucination. In this paper, we first construct and release TeleSalesCorpus, the first real-world-grounded dialogue dataset for this domain. We then propose AI-Salesman, a novel framework featuring a dual-stage architecture. For the training stage, we design a Bayesian-supervised reinforcement learning algorithm that learns robust sales strategies from noisy dialogues. For the inference stage, we introduce the Dynamic Outline-Guided Agent (DOGA), which leverages a pre-built script library to provide dynamic, turn-by-turn strategic guidance. Moreover, we design a comprehensive evaluation framework that combines fine-grained metrics for key sales skills with the LLM-as-a-Judge paradigm. Experimental results demonstrate that our proposed AI-Salesman significantly outperforms baseline models in both automatic metrics and comprehensive human evaluations, showcasing its effectiveness in complex persuasive scenarios.",
    "translation": "标题：AI-Salesman：构建可信赖大语言模型驱动的电话营销系统\n\n摘要：以电话营销为代表的目标驱动型说服性对话，需要复杂的多轮策略规划和严格的事实准确性，这对当前最先进的大语言模型仍构成重大挑战。既往研究常受限于领域特定数据的缺乏，而直接应用大语言模型则存在策略脆弱性和事实幻觉问题。本文首先构建并发布了业界首个基于真实场景的对话数据集TeleSalesCorpus。继而提出创新框架AI-Salesman，其采用双阶段架构：在训练阶段设计贝叶斯监督强化学习算法，从含噪声的对话数据中学习鲁棒的销售策略；在推理阶段引入动态大纲引导智能体（DOGA），通过预构建的话术库提供动态的逐轮策略指导。此外，我们设计了结合细粒度销售技能指标与“LLM即评判官”范式的综合评估体系。实验结果表明，所提出的AI-Salesman框架在自动评估指标和综合人工评估中均显著优于基线模型，展现了其在复杂说服场景中的卓越效能。",
    "url": "https://huggingface.co/papers/2511.12133",
    "arxiv_url": "https://arxiv.org/abs/2511.12133"
  },
  {
    "title": "Instella: Fully Open Language Models with Stellar Performance",
    "summary": "Large language models (LLMs) have demonstrated remarkable performance across a wide range of tasks, yet the majority of high-performing models remain closed-source or partially open, limiting transparency and reproducibility. In this work, we introduce Instella, a family of fully open three billion parameter language models trained entirely on openly available data and codebase. Powered by AMD Instinct MI300X GPUs, Instella is developed through large-scale pre-training, general-purpose instruction tuning, and alignment with human preferences. Despite using substantially fewer pre-training tokens than many contemporaries, Instella achieves state-of-the-art results among fully open models and is competitive with leading open-weight models of comparable size. We further release two specialized variants: Instella-Long, capable of handling context lengths up to 128K tokens, and Instella-Math, a reasoning-focused model enhanced through supervised fine-tuning and reinforcement learning on mathematical tasks. Together, these contributions establish Instella as a transparent, performant, and versatile alternative for the community, advancing the goal of open and reproducible language modeling research.",
    "translation": "标题：Instella：具备卓越性能的完全开放语言模型\n\n摘要：大语言模型已在广泛任务中展现出卓越性能，然而多数高性能模型仍保持闭源或部分开放，限制了研究的透明度与可复现性。本研究推出Instella系列——完全开放的三十亿参数语言模型，其训练全程基于公开可获取的数据与代码库。依托AMD Instinct MI300X GPU的算力支持，该模型通过大规模预训练、通用指令微调及人类偏好对齐三个阶段完成开发。尽管使用的预训练标记数量显著少于同期多数模型，Instella在完全开放模型中取得了领先性能，并与同类规模的顶尖开放权重模型表现相当。我们同时发布两个专业变体：支持128K标记上下文长度的Instella-Long，以及通过监督微调与数学任务强化学习增强的推理专用模型Instella-Math。这些成果共同确立了Instella作为透明、高效、多功能的开源替代方案，有力推进了开放可复现语言模型研究的发展目标。",
    "url": "https://huggingface.co/papers/2511.10628",
    "arxiv_url": "https://arxiv.org/abs/2511.10628"
  },
  {
    "title": "A Decentralized Retrieval Augmented Generation System with Source Reliabilities Secured on Blockchain",
    "summary": "Existing retrieval-augmented generation (RAG) systems typically use a centralized architecture, causing a high cost of data collection, integration, and management, as well as privacy concerns. There is a great need for a decentralized RAG system that enables foundation models to utilize information directly from data owners who maintain full control over their sources. However, decentralization brings a challenge: the numerous independent data sources vary significantly in reliability, which can diminish retrieval accuracy and response quality. To address this, our decentralized RAG system has a novel reliability scoring mechanism that dynamically evaluates each source based on the quality of responses it contributes to generate and prioritizes high-quality sources during retrieval. To ensure transparency and trust, the scoring process is securely managed through blockchain-based smart contracts, creating verifiable and tamper-proof reliability records without relying on a central authority. We evaluate our decentralized system with two Llama models (3B and 8B) in two simulated environments where six data sources have different levels of reliability. Our system achieves a +10.7\\% performance improvement over its centralized counterpart in the real world-like unreliable data environments. Notably, it approaches the upper-bound performance of centralized systems under ideally reliable data environments. The decentralized infrastructure enables secure and trustworthy scoring management, achieving approximately 56\\% marginal cost savings through batched update operations. Our code and system are open-sourced at github.com/yining610/Reliable-dRAG.",
    "translation": "标题：一种基于区块链保障来源可靠性的去中心化检索增强生成系统\n\n摘要：现有检索增强生成系统通常采用集中式架构，导致数据收集、集成和管理成本高昂，并引发隐私担忧。业界亟需一种去中心化的RAG系统，使基础模型能够直接利用数据所有者维护的信息源，同时确保数据所有者对其来源保持完全控制。然而去中心化架构带来新的挑战：众多独立数据源的可靠性差异显著，可能降低检索精度和响应质量。为此，我们提出的去中心化RAG系统创新性地设计了可靠性评分机制，该机制基于各数据源在生成响应过程中的贡献质量进行动态评估，并在检索阶段优先选择高质量数据源。为确保透明度和可信度，评分过程通过基于区块链的智能合约进行安全管理，无需依赖中心化机构即可创建可验证且防篡改的可靠性记录。我们使用两个Llama模型（3B和8B）在两种模拟环境中对系统进行评估，其中六个数据源具有不同等级的可靠性。在真实世界类不可靠数据环境中，本系统相较集中式系统实现了10.7%的性能提升。值得注意的是，在理想可靠数据环境下，本系统性能可逼近集中式系统的理论上限。去中心化基础设施实现了安全可信的评分管理，通过批量更新操作节省约56%的边际成本。我们的代码与系统已在github.com/yining610/Reliable-dRAG开源。",
    "url": "https://huggingface.co/papers/2511.07577",
    "arxiv_url": "https://arxiv.org/abs/2511.07577"
  },
  {
    "title": "NORA-1.5: A Vision-Language-Action Model Trained using World Model- and Action-based Preference Rewards",
    "summary": "Vision--language--action (VLA) models have recently shown promising performance on a variety of embodied tasks, yet they still fall short in reliability and generalization, especially when deployed across different embodiments or real-world environments. In this work, we introduce NORA-1.5, a VLA model built from the pre-trained NORA backbone by adding to it a flow-matching-based action expert. This architectural enhancement alone yields substantial performance gains, enabling NORA-1.5 to outperform NORA and several state-of-the-art VLA models across both simulated and real-world benchmarks. To further improve robustness and task success, we develop a set of reward models for post-training VLA policies. Our rewards combine (i) an action-conditioned world model (WM) that evaluates whether generated actions lead toward the desired goal, and (ii) a deviation-from-ground-truth heuristic that distinguishes good actions from poor ones. Using these reward signals, we construct preference datasets and adapt NORA-1.5 to target embodiments through direct preference optimization (DPO). Extensive evaluations show that reward-driven post-training consistently improves performance in both simulation and real-robot settings, demonstrating significant VLA model-reliability gains through simple yet effective reward models. Our findings highlight NORA-1.5 and reward-guided post-training as a viable path toward more dependable embodied agents suitable for real-world deployment.",
    "translation": "标题：NORA-1.5：基于世界模型与动作偏好奖励训练的视觉-语言-动作模型\n\n摘要：视觉-语言-动作模型近期在各类具身任务中展现出良好性能，但其可靠性与泛化能力仍存在不足，尤其是在跨具身系统或真实环境部署时。本研究提出NORA-1.5模型，该模型基于预训练的NORA主干网络，通过引入基于流匹配的动作专家模块实现架构升级。仅此架构改进即带来显著性能提升，使NORA-1.5在仿真与真实场景基准测试中均超越原始NORA模型及多种先进视觉-语言-动作模型。为增强模型鲁棒性与任务成功率，我们开发了一套用于后训练视觉-语言-动作策略的奖励模型。该奖励体系融合了（i）动作条件世界模型——评估生成动作是否导向预期目标，以及（ii）基于真值偏差的启发式准则——区分优质与劣质动作。利用这些奖励信号，我们构建偏好数据集并通过直接偏好优化方法使NORA-1.5适配目标具身系统。大量实验表明，基于奖励的后训练能持续提升模型在仿真与真实机器人环境中的表现，通过简洁高效的奖励模型实现了视觉-语言-动作模型可靠性的显著突破。我们的研究证实NORA-1.5模型与奖励引导的后训练机制共同构成了开发适用于真实场景的可靠具身智能体的有效路径。",
    "url": "https://huggingface.co/papers/2511.14659",
    "arxiv_url": "https://arxiv.org/abs/2511.14659"
  },
  {
    "title": "OpenUS: A Fully Open-Source Foundation Model for Ultrasound Image Analysis via Self-Adaptive Masked Contrastive Learning",
    "summary": "Ultrasound (US) is one of the most widely used medical imaging modalities, thanks to its low cost, portability, real-time feedback, and absence of ionizing radiation. However, US image interpretation remains highly operator-dependent and varies significantly across anatomical regions, acquisition protocols, and device types. These variations, along with unique challenges such as speckle, low contrast, and limited standardized annotations, hinder the development of generalizable, label-efficient ultrasound AI models. In this paper, we propose OpenUS, the first reproducible, open-source ultrasound foundation model built on a large collection of public data. OpenUS employs a vision Mamba backbone, capturing both local and global long-range dependencies across the image. To extract rich features during pre-training, we introduce a novel self-adaptive masking framework that combines contrastive learning with masked image modeling. This strategy integrates the teacher's attention map with student reconstruction loss, adaptively refining clinically-relevant masking to enhance pre-training effectiveness. OpenUS also applies a dynamic learning schedule to progressively adjust the difficulty of the pre-training process. To develop the foundation model, we compile the largest to-date public ultrasound dataset comprising over 308K images from 42 publicly available datasets, covering diverse anatomical regions, institutions, imaging devices, and disease types. Our pre-trained OpenUS model can be easily adapted to specific downstream tasks by serving as a backbone for label-efficient fine-tuning. Code is available at https://github.com/XZheng0427/OpenUS.",
    "translation": "标题：OpenUS：基于自适应掩码对比学习的全开源超声图像分析基础模型\n\n摘要：超声成像因其低成本、便携性、实时反馈和无电离辐射等优势，成为应用最广泛的医学影像模态之一。然而，超声图像解读仍高度依赖操作者经验，且受解剖区域、采集协议和设备类型的影响存在显著差异。这些变异因素，连同散斑噪声、低对比度和标准化标注稀缺等特有挑战，严重制约了泛化性强、标注效率高的超声AI模型的开发。本文提出OpenUS——首个基于大规模公共数据集构建的可复现开源超声基础模型。该模型采用视觉Mamba架构，能够同时捕捉图像中的局部特征与全局长程依赖关系。为在预训练阶段提取丰富特征，我们创新性地提出了结合对比学习与掩码图像建模的自适应掩码框架。该策略将教师模型的注意力图与学生模型的重建损失相融合，通过自适应优化临床相关掩码区域来提升预训练效能。OpenUS还采用动态学习调度机制，逐步调整预训练任务的难度层级。为构建基础模型，我们整合了迄今最大的公共超声数据集，涵盖42个公开数据源的逾30.8万张图像，涉及多解剖部位、多医疗机构、多成像设备及多疾病类型。预训练完成的OpenUS模型可作为高效标注微调任务的骨干网络，快速适配特定下游应用。代码已开源：https://github.com/XZheng0427/OpenUS。",
    "url": "https://huggingface.co/papers/2511.11510",
    "arxiv_url": "https://arxiv.org/abs/2511.11510"
  }
]