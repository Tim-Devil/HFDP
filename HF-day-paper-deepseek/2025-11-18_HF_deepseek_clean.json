[
  {
    "title": "P1: Mastering Physics Olympiads with Reinforcement Learning",
    "summary": "Recent progress in large language models (LLMs) has moved the frontier from puzzle-solving to science-grade reasoning-the kind needed to tackle problems whose answers must stand against nature, not merely fit a rubric. Physics is the sharpest test of this shift, which binds symbols to reality in a fundamental way, serving as the cornerstone of most modern technologies. In this work, we manage to advance physics research by developing large language models with exceptional physics reasoning capabilities, especially excel at solving Olympiad-level physics problems. We introduce P1, a family of open-source physics reasoning models trained entirely through reinforcement learning (RL). Among them, P1-235B-A22B is the first open-source model with Gold-medal performance at the latest International Physics Olympiad (IPhO 2025), and wins 12 gold medals out of 13 international/regional physics competitions in 2024/2025. P1-30B-A3B also surpasses almost all other open-source models on IPhO 2025, getting a silver medal. Further equipped with an agentic framework PhysicsMinions, P1-235B-A22B+PhysicsMinions achieves overall No.1 on IPhO 2025, and obtains the highest average score over the 13 physics competitions. Besides physics, P1 models also present great performance on other reasoning tasks like math and coding, showing the great generalibility of P1 series.",
    "translation": "标题：P1：基于强化学习的物理奥林匹克竞赛解题系统  \n\n摘要：大型语言模型（LLM）的最新进展已将其能力边界从解决谜题拓展至科学级推理——这类推理要求答案必须经得起自然规律的检验，而非仅符合评分标准。物理学作为这一转变的终极试金石，以根本性的方式将符号与现实相联结，成为大多数现代技术的基石。本研究通过开发具备卓越物理推理能力的大型语言模型推动物理研究进展，尤其擅长解决奥林匹克竞赛级别的物理问题。我们提出P1系列模型——完全通过强化学习（RL）训练的开源物理推理模型家族。其中P1-235B-A22B成为首个在最新国际物理奥林匹克竞赛（IPhO 2025）中达到金牌水平的开源模型，并在2024/2025年度的13项国际/地区物理竞赛中斩获12枚金牌。P1-30B-A3B同样在IPhO 2025中超越几乎所有其他开源模型，获得银牌成绩。进一步结合智能体框架PhysicsMinions后，P1-235B-A22B+PhysicsMinions在IPhO 2025实现总排名第一，并在13项物理竞赛中取得最高平均分。除物理领域外，P1系列模型在数学、编程等推理任务中也展现出卓越性能，证明了该系列模型的强泛化能力。",
    "url": "https://huggingface.co/papers/2511.13612",
    "arxiv_url": "https://arxiv.org/abs/2511.13612"
  },
  {
    "title": "MiroThinker: Pushing the Performance Boundaries of Open-Source Research Agents via Model, Context, and Interactive Scaling",
    "summary": "We present MiroThinker v1.0, an open-source research agent designed to advance tool-augmented reasoning and information-seeking capabilities. Unlike previous agents that only scale up model size or context length, MiroThinker explores interaction scaling at the model level, systematically training the model to handle deeper and more frequent agent-environment interactions as a third dimension of performance improvement. Unlike LLM test-time scaling, which operates in isolation and risks degradation with longer reasoning chains, interactive scaling leverages environment feedback and external information acquisition to correct errors and refine trajectories. Through reinforcement learning, the model achieves efficient interaction scaling: with a 256K context window, it can perform up to 600 tool calls per task, enabling sustained multi-turn reasoning and complex real-world research workflows. Across four representative benchmarks-GAIA, HLE, BrowseComp, and BrowseComp-ZH-the 72B variant achieves up to 81.9%, 37.7%, 47.1%, and 55.6% accuracy respectively, surpassing previous open-source agents and approaching commercial counterparts such as GPT-5-high. Our analysis reveals that MiroThinker benefits from interactive scaling consistently: research performance improves predictably as the model engages in deeper and more frequent agent-environment interactions, demonstrating that interaction depth exhibits scaling behaviors analogous to model size and context length. These findings establish interaction scaling as a third critical dimension for building next-generation open research agents, complementing model capacity and context windows.",
    "translation": "标题：MiroThinker：通过模型规模、上下文长度与交互深度的协同扩展突破开源研究智能体性能边界\n\n摘要：本文推出开源研究智能体MiroThinker v1.0，旨在推进工具增强推理与信息检索能力的发展。与先前仅扩大模型规模或上下文长度的智能体不同，MiroThinker开创性地在模型层面探索交互深度扩展，将其作为性能提升的第三维度，通过系统化训练使模型能够处理更深层次、更频繁的智能体-环境交互。相较于孤立运行且长推理链存在性能衰减风险的大语言模型测试时扩展，交互式扩展通过环境反馈与外部信息获取实现错误修正与路径优化。借助强化学习，该模型实现了高效的交互扩展：在256K上下文窗口支持下，单任务可执行高达600次工具调用，支撑持续多轮推理与复杂现实研究流程。在GAIA、HLE、BrowseComp及BrowseComp-ZH四个代表性基准测试中，72B参数版本分别取得81.9%、37.7%、47.1%和55.6%的准确率，超越既往开源智能体并逼近GPT-5-high等商业系统。我们的分析表明，MiroThinker始终受益于交互扩展：随着智能体-环境交互深度与频次的增加，研究性能呈现可预测的提升，证明交互深度具有与模型规模、上下文长度类似的扩展规律。这些发现确立了交互扩展作为构建下一代开源研究智能体的第三关键维度，与模型容量和上下文窗口形成有效互补。",
    "url": "https://huggingface.co/papers/2511.11793",
    "arxiv_url": "https://arxiv.org/abs/2511.11793"
  },
  {
    "title": "Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data",
    "summary": "We present Uni-MoE 2.0 from the Lychee family. As a fully open-source omnimodal large model (OLM), it substantially advances Lychee's Uni-MoE series in language-centric multimodal understanding, reasoning, and generating. Based on the Qwen2.5-7B dense architecture, we build Uni-MoE-2.0-Omni from scratch through three core contributions: dynamic-capacity Mixture-of-Experts (MoE) design, a progressive training strategy enhanced with an iterative reinforcement strategy, and a carefully curated multimodal data matching technique. It is capable of omnimodal understanding, as well as generating images, text, and speech. Architecturally, our new MoE framework balances computational efficiency and capability for 10 cross-modal inputs using shared, routed, and null experts, while our Omni-Modality 3D RoPE ensures spatio-temporal cross-modality alignment in the self-attention layer. For training, following cross-modal pretraining, we use a progressive supervised fine-tuning strategy that activates modality-specific experts and is enhanced by balanced data composition and an iterative GSPO-DPO method to stabilise RL training and improve reasoning. Data-wise, the base model, trained on approximately 75B tokens of open-source multimodal data, is equipped with special speech and image generation tokens, allowing it to learn these generative tasks by conditioning its outputs on linguistic cues. Extensive evaluation across 85 benchmarks demonstrates that our model achieves SOTA or highly competitive performance against leading OLMs, surpassing Qwen2.5-Omni (trained with 1.2T tokens) on over 50 of 76 benchmarks. Key strengths include video understanding (+7% avg. of 8), omnimodallity understanding (+7% avg. of 4), and audiovisual reasoning (+4%). It also advances long-form speech processing (reducing WER by 4.2%) and leads in low-level image processing and controllable generation across 5 metrics.",
    "translation": "标题：Uni-MoE-2.0-Omni：基于先进混合专家系统、训练策略与数据技术的以语言为中心的全模态大模型扩展\n\n摘要：我们推出荔枝家族系列的全新开源全模态大模型Uni-MoE 2.0。该模型在以语言为核心的多模态理解、推理与生成领域显著推进了荔枝Uni-MoE系列的技术水平。基于Qwen2.5-7B稠密架构，我们通过三大核心创新实现模型从零构建：动态容量混合专家框架设计、结合迭代强化策略的渐进式训练方法，以及精心构建的多模态数据匹配技术。该模型具备全模态理解能力，并可实现图像、文本与语音的跨模态生成。在架构层面，新型混合专家框架通过共享专家、路由专家与空置专家的协同机制，在十种跨模态输入场景中实现计算效率与模型能力的平衡；同时我们提出的全模态三维旋转位置编码技术，有效保障自注意力层中的时空跨模态对齐。训练策略上，在完成跨模态预训练后，我们采用渐进式监督微调方案，通过激活模态特定专家配合均衡数据组合，并结合迭代式GSPO-DPO方法以增强推理能力并稳定强化学习训练过程。数据层面，基于约750亿token开源多模态数据训练的基础模型，通过嵌入专用语音与图像生成标记，实现了基于语言线索的条件化生成能力。在涵盖85个基准测试的全面评估中，本模型在76项测试中超过50项表现优于采用1.2万亿token训练的Qwen2.5-Omni模型，在领先全模态大模型中达到顶尖或极具竞争力的性能水平。核心优势体现在视频理解（8项测试平均提升7%）、全模态理解（4项测试平均提升7%）和视听推理（提升4%）领域，同时显著推进长语音处理（词错误率降低4.2%），并在底层图像处理与可控生成的5项指标中保持领先地位。",
    "url": "https://huggingface.co/papers/2511.12609",
    "arxiv_url": "https://arxiv.org/abs/2511.12609"
  },
  {
    "title": "Souper-Model: How Simple Arithmetic Unlocks State-of-the-Art LLM Performance",
    "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse domains, but their training remains resource- and time-intensive, requiring massive compute power and careful orchestration of training procedures. Model souping-the practice of averaging weights from multiple models of the same architecture-has emerged as a promising pre- and post-training technique that can enhance performance without expensive retraining. In this paper, we introduce Soup Of Category Experts (SoCE), a principled approach for model souping that utilizes benchmark composition to identify optimal model candidates and applies non-uniform weighted averaging to maximize performance. Contrary to previous uniform-averaging approaches, our method leverages the observation that benchmark categories often exhibit low inter-correlations in model performance. SoCE identifies \"expert\" models for each weakly-correlated category cluster and combines them using optimized weighted averaging rather than uniform weights. We demonstrate that the proposed method improves performance and robustness across multiple domains, including multilingual capabilities, tool calling, and math and achieves state-of-the-art results on the Berkeley Function Calling Leaderboard.",
    "translation": "标题：Souper-Model：简单算术如何实现前沿大语言模型性能突破\n\n摘要：大语言模型（LLM）已在多个领域展现出卓越能力，但其训练过程仍需要消耗大量资源和时间，不仅依赖巨额算力支撑，还需精心设计训练流程。模型参数融合技术——即对同架构多个模型的权重参数进行平均处理——已成为一种具有潜力的训练前/后优化方法，可在避免昂贵重复训练的前提下提升模型性能。本文提出类别专家融合模型（SoCE），该方法通过基准测试构成分析确定最优模型候选集，并采用非均匀加权平均策略实现性能最大化。与传统均匀平均方法不同，我们基于模型在不同测试类别中表现相关性较低这一观测结果，为每个弱相关类别集群筛选“专家”模型，并通过优化加权而非均匀权重进行模型融合。实验证明，该方法在多语言能力、工具调用及数学推理等多个领域均能提升模型性能与鲁棒性，并在伯克利函数调用排行榜上取得了领先成果。",
    "url": "https://huggingface.co/papers/2511.13254",
    "arxiv_url": "https://arxiv.org/abs/2511.13254"
  },
  {
    "title": "Part-X-MLLM: Part-aware 3D Multimodal Large Language Model",
    "summary": "We introduce Part-X-MLLM, a native 3D multimodal large language model that unifies diverse 3D tasks by formulating them as programs in a structured, executable grammar. Given an RGB point cloud and a natural language prompt, our model autoregressively generates a single, coherent token sequence encoding part-level bounding boxes, semantic descriptions, and edit commands. This structured output serves as a versatile interface to drive downstream geometry-aware modules for part-based generation and editing. By decoupling the symbolic planning from the geometric synthesis, our approach allows any compatible geometry engine to be controlled through a single, language-native frontend. We pre-train a dual-encoder architecture to disentangle structure from semantics and instruction-tune the model on a large-scale, part-centric dataset. Experiments demonstrate that our model excels at producing high-quality, structured plans, enabling state-of-the-art performance in grounded Q\\&A, compositional generation, and localized editing through one unified interface. Project page: https://chunshi.wang/Part-X-MLLM/",
    "translation": "标题：Part-X-MLLM：面向部件感知的三维多模态大语言模型\n\n摘要：本文提出Part-X-MLLM——一种原生三维多模态大语言模型，通过将多样化三维任务表述为结构化可执行语法中的程序来实现任务统一。给定RGB点云和自然语言提示，本模型能以自回归方式生成单一连贯的标记序列，该序列编码了部件级边界框、语义描述及编辑指令。这种结构化输出可作为驱动下游几何感知模块的通用接口，实现基于部件的生成与编辑。通过将符号规划与几何合成解耦，我们的方法使得任何兼容的几何引擎都能通过单一语言原生前端进行控制。我们预训练了双编码器架构以实现结构与语义解耦，并在大规模部件中心数据集上对模型进行指令微调。实验表明，本模型在生成高质量结构化规划方面表现卓越，通过统一接口在具身问答、组合生成及局部编辑任务中实现了最先进的性能。项目页面：https://chunshi.wang/Part-X-MLLM/",
    "url": "https://huggingface.co/papers/2511.13647",
    "arxiv_url": "https://arxiv.org/abs/2511.13647"
  },
  {
    "title": "MMaDA-Parallel: Multimodal Large Diffusion Language Models for Thinking-Aware Editing and Generation",
    "summary": "While thinking-aware generation aims to improve performance on complex tasks, we identify a critical failure mode where existing sequential, autoregressive approaches can paradoxically degrade performance due to error propagation. To systematically analyze this issue, we propose ParaBench, a new benchmark designed to evaluate both text and image output modalities. Our analysis using ParaBench reveals that this performance degradation is strongly correlated with poor alignment between the generated reasoning and the final image. To resolve this, we propose a parallel multimodal diffusion framework, MMaDA-Parallel, that enables continuous, bidirectional interaction between text and images throughout the entire denoising trajectory. MMaDA-Parallel is trained with supervised finetuning and then further optimized by Parallel Reinforcement Learning (ParaRL), a novel strategy that applies semantic rewards along the trajectory to enforce cross-modal consistency. Experiments validate that our model significantly improves cross-modal alignment and semantic consistency, achieving a 6.9\\% improvement in Output Alignment on ParaBench compared to the state-of-the-art model, Bagel, establishing a more robust paradigm for thinking-aware image synthesis. Our code is open-sourced at https://github.com/tyfeld/MMaDA-Parallel",
    "translation": "标题：MMaDA-并行：面向思维感知编辑与生成的多模态扩散大语言模型\n\n摘要：尽管思维感知生成技术旨在提升复杂任务的处理性能，但我们发现现有序列自回归方法存在关键失效模式——由于错误传播效应，反而可能导致性能退化。为系统分析该问题，我们提出ParaBench基准测试集，专门用于评估文本与图像双模态输出。基于ParaBench的分析表明，这种性能退化与生成推理过程和最终图像之间的对齐偏差高度相关。为此，我们提出并行多模态扩散框架MMaDA-Parallel，该框架能在整个去噪轨迹中实现文本与图像的持续双向交互。该模型首先通过监督微调进行训练，随后采用新型并行强化学习策略（ParaRL）进行优化，该策略通过沿轨迹施加语义奖励来增强跨模态一致性。实验验证表明，我们的模型显著改善了跨模态对齐与语义一致性，在ParaBench基准上相较最先进的Bagel模型实现了输出对齐指标6.9%的提升，为思维感知图像合成建立了更稳健的范式。代码已开源于：https://github.com/tyfeld/MMaDA-Parallel",
    "url": "https://huggingface.co/papers/2511.09611",
    "arxiv_url": "https://arxiv.org/abs/2511.09611"
  },
  {
    "title": "GroupRank: A Groupwise Reranking Paradigm Driven by Reinforcement Learning",
    "summary": "Large Language Models have shown strong potential as rerankers to enhance the overall performance of RAG systems. However, existing reranking paradigms are constrained by a core theoretical and practical dilemma: Pointwise methods, while simple and highly flexible, evaluate documents independently, making them prone to the Ranking Myopia Trap, overlooking the relative importance between documents. In contrast, Listwise methods can perceive the global ranking context, but suffer from inherent List Rigidity, leading to severe scalability and flexibility issues when handling large candidate sets. To address these challenges, we propose Groupwise, a novel reranking paradigm. In this approach, the query and a group of candidate documents are jointly fed into the model, which performs within-group comparisons to assign individual relevance scores to each document. This design retains the flexibility of Pointwise methods while enabling the comparative capability of Listwise methods. We further adopt GRPO for model training, equipped with a heterogeneous reward function that integrates ranking metrics with a distributional reward aimed at aligning score distributions across groups. To overcome the bottleneck caused by the scarcity of high quality labeled data, we further propose an innovative pipeline for synthesizing high quality retrieval and ranking data. The resulting data can be leveraged not only for training the reranker but also for training the retriever. Extensive experiments validate the effectiveness of our approach. On two reasoning intensive retrieval benchmarks, BRIGHT and R2MED.",
    "translation": "标题：GroupRank：一种强化学习驱动的分组重排序范式\n\n摘要：大语言模型作为重排序器已展现出增强RAG系统整体性能的强大潜力。然而，现有重排序范式受限于核心的理论与实践困境：点式排序方法虽然简单灵活，但独立评估文档的特性使其容易陷入\"排序近视陷阱\"，忽略文档间的相对重要性；而列式排序方法虽能感知全局排序上下文，却存在固有的\"列表刚性缺陷\"，在处理大规模候选集时面临严重的可扩展性与灵活性挑战。为此，我们提出分组排序这一新型重排序范式。该方法将查询与候选文档组共同输入模型，通过组内比较为每个文档分配独立相关性分数。该设计既保留了点式方法的灵活性，又实现了列式方法的对比能力。我们采用GRPO进行模型训练，并配备融合排序指标与分布奖励的异构奖励函数，以协调跨组分数分布。针对高质量标注数据稀缺的瓶颈，我们进一步提出合成高质量检索与排序数据的创新流程，所得数据不仅可用于训练重排序器，还可用于训练检索器。在BRIGHT和R2MED两个推理密集型检索基准上的大量实验验证了我们方法的有效性。",
    "url": "https://huggingface.co/papers/2511.11653",
    "arxiv_url": "https://arxiv.org/abs/2511.11653"
  },
  {
    "title": "TiViBench: Benchmarking Think-in-Video Reasoning for Video Generative Models",
    "summary": "The rapid evolution of video generative models has shifted their focus from producing visually plausible outputs to tackling tasks requiring physical plausibility and logical consistency. However, despite recent breakthroughs such as Veo 3's chain-of-frames reasoning, it remains unclear whether these models can exhibit reasoning capabilities similar to large language models (LLMs). Existing benchmarks predominantly evaluate visual fidelity and temporal coherence, failing to capture higher-order reasoning abilities. To bridge this gap, we propose TiViBench, a hierarchical benchmark specifically designed to evaluate the reasoning capabilities of image-to-video (I2V) generation models. TiViBench systematically assesses reasoning across four dimensions: i) Structural Reasoning & Search, ii) Spatial & Visual Pattern Reasoning, iii) Symbolic & Logical Reasoning, and iv) Action Planning & Task Execution, spanning 24 diverse task scenarios across 3 difficulty levels. Through extensive evaluations, we show that commercial models (e.g., Sora 2, Veo 3.1) demonstrate stronger reasoning potential, while open-source models reveal untapped potential that remains hindered by limited training scale and data diversity. To further unlock this potential, we introduce VideoTPO, a simple yet effective test-time strategy inspired by preference optimization. By performing LLM self-analysis on generated candidates to identify strengths and weaknesses, VideoTPO significantly enhances reasoning performance without requiring additional training, data, or reward models. Together, TiViBench and VideoTPO pave the way for evaluating and advancing reasoning in video generation models, setting a foundation for future research in this emerging field.",
    "translation": "标题：TiViBench：视频生成模型中的视觉推理能力基准测试框架\n\n摘要：视频生成模型的快速发展已使其重点从产生视觉合理的输出转向处理需要物理合理性和逻辑一致性的任务。然而，尽管近期出现了如Veo 3的帧序列推理等突破性进展，这些模型是否能展现类似大语言模型的推理能力仍不明确。现有基准主要评估视觉保真度和时序连贯性，未能捕捉高阶推理能力。为填补这一空白，我们提出TiViBench——一个专门用于评估图像到视频生成模型推理能力的分层基准框架。该基准系统地从四个维度评估推理能力：i) 结构推理与检索，ii) 空间与视觉模式推理，iii) 符号与逻辑推理，iv) 行动规划与任务执行，涵盖3个难度级别下的24种任务场景。通过广泛评估，我们发现商业模型（如Sora 2、Veo 3.1）展现出更强的推理潜力，而开源模型虽存在未开发潜力，但仍受限于训练规模和数据多样性的不足。为释放这种潜力，我们提出VideoTPO——一种受偏好优化启发的简易有效测试时策略。通过对生成候选结果进行LLM自分析以识别优劣，VideoTPO在无需额外训练、数据或奖励模型的情况下显著提升推理性能。TiViBench与VideoTPO共同为评估和推进视频生成模型的推理能力开辟道路，为这一新兴领域的未来研究奠定基础。",
    "url": "https://huggingface.co/papers/2511.13704",
    "arxiv_url": "https://arxiv.org/abs/2511.13704"
  },
  {
    "title": "PhysX-Anything: Simulation-Ready Physical 3D Assets from Single Image",
    "summary": "3D modeling is shifting from static visual representations toward physical, articulated assets that can be directly used in simulation and interaction. However, most existing 3D generation methods overlook key physical and articulation properties, thereby limiting their utility in embodied AI. To bridge this gap, we introduce PhysX-Anything, the first simulation-ready physical 3D generative framework that, given a single in-the-wild image, produces high-quality sim-ready 3D assets with explicit geometry, articulation, and physical attributes. Specifically, we propose the first VLM-based physical 3D generative model, along with a new 3D representation that efficiently tokenizes geometry. It reduces the number of tokens by 193x, enabling explicit geometry learning within standard VLM token budgets without introducing any special tokens during fine-tuning and significantly improving generative quality. In addition, to overcome the limited diversity of existing physical 3D datasets, we construct a new dataset, PhysX-Mobility, which expands the object categories in prior physical 3D datasets by over 2x and includes more than 2K common real-world objects with rich physical annotations. Extensive experiments on PhysX-Mobility and in-the-wild images demonstrate that PhysX-Anything delivers strong generative performance and robust generalization. Furthermore, simulation-based experiments in a MuJoCo-style environment validate that our sim-ready assets can be directly used for contact-rich robotic policy learning. We believe PhysX-Anything can substantially empower a broad range of downstream applications, especially in embodied AI and physics-based simulation.",
    "translation": "标题：PhysX-Anything：基于单图像的仿真就绪物理三维资产生成框架\n\n摘要：三维建模正从静态视觉表示转向可直接用于仿真与交互的物理化可动资产。然而现有三维生成方法大多忽略关键物理属性与可动结构，限制了其在具身智能中的应用。为此我们提出PhysX-Anything——首个仿真就绪的物理三维生成框架，仅需单张真实场景图像即可生成具有显式几何、可动结构与物理属性的高质量仿真就绪三维资产。具体而言，我们提出首个基于视觉语言模型的物理三维生成方法，并创新性地设计了一种高效几何表征的token化方案，将token数量降低193倍，在标准VLM token预算内实现显式几何学习，且无需微调阶段引入特殊token，显著提升生成质量。此外，为克服现有物理三维数据集多样性不足的问题，我们构建了PhysX-Mobility数据集，将现有物理三维数据集的物体类别扩展2倍以上，包含2000余个常见现实物体并配备丰富物理标注。在PhysX-Mobility与真实场景图像上的大量实验表明，PhysX-Anything具有优异的生成性能与鲁棒泛化能力。基于MuJoCo仿真环境的实验进一步验证，本方法生成的仿真就绪资产可直接用于接触密集型机器人策略学习。我们相信PhysX-Anything将有力推动具身智能与物理仿真等下游应用的发展。",
    "url": "https://huggingface.co/papers/2511.13648",
    "arxiv_url": "https://arxiv.org/abs/2511.13648"
  },
  {
    "title": "UFO^3: Weaving the Digital Agent Galaxy",
    "summary": "Large language model (LLM)-powered agents are transforming digital devices from passive tools into proactive intelligent collaborators. However, most existing frameworks remain confined to a single OS or device, making cross-device workflows brittle and largely manual. We present UFO^3, a system that unifies heterogeneous endpoints, desktops, servers, mobile devices, and edge, into a single orchestration fabric. UFO^3 models each user request as a mutable TaskConstellation: a distributed DAG of atomic subtasks (TaskStars) with explicit control and data dependencies (TaskStarLines). The TaskConstellation continuously evolves as results stream in from distributed devices, enabling asynchronous execution, adaptive recovery, and dynamic optimization. A Constellation Orchestrator} executes tasks safely and asynchronously while applying dynamic DAG updates, and the Agent Interaction Protocol (AIP) provides persistent, low-latency channels for reliable task dispatch and result streaming. These designs dissolve the traditional boundaries between devices and platforms, allowing agents to collaborate seamlessly and amplify their collective intelligence.\n  We evaluate UFO^3 on NebulaBench, a benchmark of 55 cross-device tasks across 5 machines and 10 categories. UFO^3 achieves 83.3% subtask completion, 70.9% task success, exposes parallelism with an average width of 1.72, and reduces end-to-end latency by 31% relative to a sequential baseline. Fault-injection experiments demonstrate graceful degradation and recovery under transient and permanent agent failures. These results show that UFO^3 achieves accurate, efficient, and resilient task orchestration across heterogeneous devices, uniting isolated agents into a coherent, adaptive computing fabric that extends across the landscape of ubiquitous computing.",
    "translation": "标题：UFO^3：编织智能体数字星系\n\n摘要：基于大语言模型的智能体正将数字设备从被动工具转变为主动智能协作者。然而现有框架大多局限于单一操作系统或设备，导致跨设备工作流脆弱且高度依赖人工操作。我们提出UFO^3系统，该系统将异构终端、桌面端、服务器、移动设备与边缘计算节点统一整合为协同编排架构。UFO^3将用户请求建模为可演化的任务星云：即由具有显式控制流与数据依赖关系的原子化子任务构成分布式有向无环图。随着分布式设备持续返回执行结果，任务星云持续动态演化，支持异步执行、自适应恢复与动态优化。星云编排器在实施动态DAG更新的同时保障任务安全异步执行，而智能体交互协议则通过持久化低延迟通道实现可靠任务调度与结果流式传输。这些设计打破了设备与平台间的传统壁垒，使智能体能够无缝协作并增强集体智能。\n\n我们在包含5台设备、10个类别、55项跨设备任务的NebulaBench基准测试中评估UFO^3。实验表明：该系统实现83.3%的子任务完成率与70.9%的整体任务成功率，平均并行宽度达1.72，较顺序执行基线降低端到端延迟31%。故障注入实验证明系统在瞬态与永久性智能体故障场景下均能实现优雅降级与恢复。这些结果表明，UFO^3通过跨异构设备实现精准、高效、鲁棒的任务编排，将孤立智能体整合为贯穿普适计算场景的协同自适应计算架构。",
    "url": "https://huggingface.co/papers/2511.11332",
    "arxiv_url": "https://arxiv.org/abs/2511.11332"
  },
  {
    "title": "Evolve the Method, Not the Prompts: Evolutionary Synthesis of Jailbreak Attacks on LLMs",
    "summary": "Automated red teaming frameworks for Large Language Models (LLMs) have become increasingly sophisticated, yet they share a fundamental limitation: their jailbreak logic is confined to selecting, combining, or refining pre-existing attack strategies. This binds their creativity and leaves them unable to autonomously invent entirely new attack mechanisms. To overcome this gap, we introduce EvoSynth, an autonomous framework that shifts the paradigm from attack planning to the evolutionary synthesis of jailbreak methods. Instead of refining prompts, EvoSynth employs a multi-agent system to autonomously engineer, evolve, and execute novel, code-based attack algorithms. Crucially, it features a code-level self-correction loop, allowing it to iteratively rewrite its own attack logic in response to failure. Through extensive experiments, we demonstrate that EvoSynth not only establishes a new state-of-the-art by achieving an 85.5\\% Attack Success Rate (ASR) against highly robust models like Claude-Sonnet-4.5, but also generates attacks that are significantly more diverse than those from existing methods. We release our framework to facilitate future research in this new direction of evolutionary synthesis of jailbreak methods. Code is available at: https://github.com/dongdongunique/EvoSynth.",
    "translation": "标题：进化方法而非提示：大语言模型越狱攻击的演化合成\n\n摘要：针对大语言模型的自动化红队测试框架日益精密，但其仍存在根本性局限：越狱逻辑始终局限于选择、组合或优化既有攻击策略。这种约束限制了系统创造性，使其无法自主发明全新攻击机制。为突破此局限，我们提出EvoSynth框架，将范式从攻击规划转向越狱方法的演化合成。该框架采用多智能体系统自主设计、进化并执行基于代码的新型攻击算法，其核心特性在于具备代码级自我修正循环，能够根据失败反馈迭代重写攻击逻辑。通过大量实验验证，EvoSynth不仅在对Claude-Sonnet-4.5等高鲁棒性模型的测试中达到85.5%的攻击成功率，创下最新技术水准，且生成的攻击方式比现有方法具有显著更高的多样性。我们公开此框架以促进越狱方法演化合成这一新方向的研究。代码地址：https://github.com/dongdongunique/EvoSynth。",
    "url": "https://huggingface.co/papers/2511.12710",
    "arxiv_url": "https://arxiv.org/abs/2511.12710"
  },
  {
    "title": "Back to Basics: Let Denoising Generative Models Denoise",
    "summary": "Today's denoising diffusion models do not \"denoise\" in the classical sense, i.e., they do not directly predict clean images. Rather, the neural networks predict noise or a noised quantity. In this paper, we suggest that predicting clean data and predicting noised quantities are fundamentally different. According to the manifold assumption, natural data should lie on a low-dimensional manifold, whereas noised quantities do not. With this assumption, we advocate for models that directly predict clean data, which allows apparently under-capacity networks to operate effectively in very high-dimensional spaces. We show that simple, large-patch Transformers on pixels can be strong generative models: using no tokenizer, no pre-training, and no extra loss. Our approach is conceptually nothing more than \"Just image Transformers\", or JiT, as we call it. We report competitive results using JiT with large patch sizes of 16 and 32 on ImageNet at resolutions of 256 and 512, where predicting high-dimensional noised quantities can fail catastrophically. With our networks mapping back to the basics of the manifold, our research goes back to basics and pursues a self-contained paradigm for Transformer-based diffusion on raw natural data.",
    "translation": "标题：回归本源：让去噪生成模型真正执行去噪任务\n\n摘要：当今的去噪扩散模型并非经典意义上的\"去噪\"，即不直接预测洁净图像。相反，神经网络预测的是噪声或含噪量值。本文提出，预测洁净数据与预测含噪量值存在本质区别。根据流形假设，自然数据应位于低维流形上，而含噪量值则不然。基于此假设，我们主张采用直接预测洁净数据的模型，这使得表观容量不足的网络能在高维空间中有效运作。我们证明：无需标记器、预训练或额外损失的简单大尺寸图像块Transformer即可成为强大的生成模型。该方法在概念上仅是\"纯图像Transformer\"（简称JiT）。我们在ImageNet数据集256×256和512×512分辨率上使用16×16和32×32大尺寸图像块的JiT模型取得了具有竞争力的结果——而在相同场景下，预测高维含噪量值可能导致灾难性失败。通过使网络回归流形的基本原理，我们的研究不仅回归技术本源，更致力于构建基于原始自然数据的Transformer扩散模型的自包含范式。",
    "url": "https://huggingface.co/papers/2511.13720",
    "arxiv_url": "https://arxiv.org/abs/2511.13720"
  },
  {
    "title": "OlmoEarth: Stable Latent Image Modeling for Multimodal Earth Observation",
    "summary": "Earth observation data presents a unique challenge: it is spatial like images, sequential like video or text, and highly multimodal. We present OlmoEarth: a multimodal, spatio-temporal foundation model that employs a novel self-supervised learning formulation, masking strategy, and loss all designed for the Earth observation domain. OlmoEarth achieves state-of-the-art performance compared to 12 other foundation models across a variety of research benchmarks and real-world tasks from external partners. When evaluating embeddings OlmoEarth achieves the best performance on 15 out of 24 tasks, and with full fine-tuning it is the best on 19 of 29 tasks. We deploy OlmoEarth as the backbone of an end-to-end platform for data collection, labeling, training, and inference of Earth observation models. The OlmoEarth Platform puts frontier foundation models and powerful data management tools into the hands of non-profits and NGOs working to solve the world's biggest problems. OlmoEarth source code, training data, and pre-trained weights are available at https://github.com/allenai/olmoearth_pretrain{https://github.com/allenai/olmoearth_pretrain}.",
    "translation": "标题：OlmoEarth：面向多模态地球观测的稳定隐空间图像建模\n\n摘要：地球观测数据具有独特的挑战性：既具备图像的空间特性，又具有视频或文本的时序特征，且呈现高度多模态特性。本文提出OlmoEarth——一个多模态时空基础模型，采用专为地球观测领域设计的新型自监督学习框架、掩码策略与损失函数。在各类研究基准测试和来自外部合作伙伴的实际任务中，OlmoEarth相较于其他12种基础模型实现了最先进的性能表现。在嵌入向量评估中，OlmoEarth在24项任务中的15项取得最佳性能；经过全参数微调后，其在29项任务中的19项表现最优。我们将OlmoEarth部署为端到端地球观测模型平台的核心架构，该平台涵盖数据采集、标注、训练和推理全流程。OlmoEarth平台将前沿基础模型与强大数据管理工具赋能给致力于解决全球重大问题的非营利组织与非政府机构。OlmoEarth源代码、训练数据与预训练权重已在https://github.com/allenai/olmoearth_pretrain 开源。",
    "url": "https://huggingface.co/papers/2511.13655",
    "arxiv_url": "https://arxiv.org/abs/2511.13655"
  },
  {
    "title": "Live-SWE-agent: Can Software Engineering Agents Self-Evolve on the Fly?",
    "summary": "Large Language Models (LLMs) are reshaping almost all industries, including software engineering. In recent years, a number of LLM agents have been proposed to solve real-world software problems. Such software agents are typically equipped with a suite of coding tools and can autonomously decide the next actions to form complete trajectories to solve end-to-end software tasks. While promising, they typically require dedicated design and may still be suboptimal, since it can be extremely challenging and costly to exhaust the entire agent scaffold design space. Recognizing that software agents are inherently software themselves that can be further refined/modified, researchers have proposed a number of self-improving software agents recently, including the Darwin-Gödel Machine (DGM). Meanwhile, such self-improving agents require costly offline training on specific benchmarks and may not generalize well across different LLMs or benchmarks. In this paper, we propose Live-SWE-agent, the first live software agent that can autonomously and continuously evolve itself on-the-fly during runtime when solving real-world software problems. More specifically, Live-SWE-agent starts with the most basic agent scaffold with only access to bash tools (e.g., mini-SWE-agent), and autonomously evolves its own scaffold implementation while solving real-world software problems. Our evaluation on the widely studied SWE-bench Verified benchmark shows that Live-SWE-agent can achieve an impressive solve rate of 75.4% without test-time scaling, outperforming all existing open-source software agents and approaching the performance of the best proprietary solution. Moreover, Live-SWE-agent outperforms state-of-the-art manually crafted software agents on the recent SWE-Bench Pro benchmark, achieving the best-known solve rate of 45.8%.",
    "translation": "标题：Live-SWE-agent：软件工程智能体能否实现实时自主演进？\n\n摘要：大型语言模型正在重塑包括软件工程在内的几乎所有行业领域。近年来，已有诸多基于大语言模型的智能体被提出以解决现实世界中的软件问题。这类软件智能体通常配备一套编程工具集，能够自主决策后续操作以形成完整任务轨迹，从而解决端到端的软件任务。尽管前景可观，此类智能体往往需要专门设计且仍可能非最优解，因为穷尽整个智能体框架设计空间极具挑战且成本高昂。鉴于软件智能体本质上是可被进一步优化/修改的软件实体，研究者近期提出了若干自演进软件智能体，包括达尔文-哥德尔机。然而，这类自演进智能体需要在特定基准测试上进行昂贵的离线训练，且在不同大语言模型或基准测试间泛化能力有限。本文提出Live-SWE-agent——首个能在解决现实软件问题时于运行时自主持续实时演进的活体软件智能体。具体而言，Live-SWE-agent从仅配备基础bash工具的最简智能体框架起步，在解决实际软件问题的过程中自主演进其框架实现。在广泛研究的SWE-bench Verified基准测试中，我们的评估表明Live-SWE-agent无需测试时扩展即可达到75.4%的惊人解决率，超越所有现有开源软件智能体，逼近最佳专有解决方案的性能。此外，Live-SWE-agent在最新的SWE-Bench Pro基准测试中优于最先进的人工设计软件智能体，以45.8%的解决率创下当前最佳纪录。",
    "url": "https://huggingface.co/papers/2511.13646",
    "arxiv_url": "https://arxiv.org/abs/2511.13646"
  },
  {
    "title": "Genomic Next-Token Predictors are In-Context Learners",
    "summary": "In-context learning (ICL) -- the capacity of a model to infer and apply abstract patterns from examples provided within its input -- has been extensively studied in large language models trained for next-token prediction on human text. In fact, prior work often attributes this emergent behavior to distinctive statistical properties in human language. This raises a fundamental question: can ICL arise organically in other sequence domains purely through large-scale predictive training?\n  To explore this, we turn to genomic sequences, an alternative symbolic domain rich in statistical structure. Specifically, we study the Evo2 genomic model, trained predominantly on next-nucleotide (A/T/C/G) prediction, at a scale comparable to mid-sized LLMs. We develop a controlled experimental framework comprising symbolic reasoning tasks instantiated in both linguistic and genomic forms, enabling direct comparison of ICL across genomic and linguistic models. Our results show that genomic models, like their linguistic counterparts, exhibit log-linear gains in pattern induction as the number of in-context demonstrations increases. To the best of our knowledge, this is the first evidence of organically emergent ICL in genomic sequences, supporting the hypothesis that ICL arises as a consequence of large-scale predictive modeling over rich data. These findings extend emergent meta-learning beyond language, pointing toward a unified, modality-agnostic view of in-context learning.",
    "translation": "标题：基因组下一标记预测器具备上下文学习能力\n\n摘要：上下文学习（ICL）——即模型从其输入提供的示例中推断并应用抽象模式的能力——已在基于人类文本进行下一标记预测训练的大语言模型中得到广泛研究。事实上，先前研究常将这种涌现行为归因于人类语言独特的统计特性。这引发了一个根本性问题：在其他序列领域中，ICL能否纯粹通过大规模预测训练自然涌现？为探索此问题，我们转向基因组序列这一富含统计结构的替代符号领域。具体而言，我们研究了主要在下一核苷酸（A/T/C/G）预测任务上训练的Evo2基因组模型，其训练规模与中型LLM相当。我们开发了一个受控实验框架，包含以语言形式和基因组形式实例化的符号推理任务，从而实现对基因组模型与语言模型上下文学习的直接比较。研究结果表明，基因组模型与语言模型类似，随着上下文示例数量的增加，在模式归纳能力上呈现对数线性增益。据我们所知，这是首次在基因组序列中发现自然涌现的上下文学习能力，支持了“ICL是丰富数据大规模预测建模的产物”这一假说。这些发现将涌现元学习拓展至语言领域之外，为构建统一、模态无关的上下文学习理论指明了方向。",
    "url": "https://huggingface.co/papers/2511.12797",
    "arxiv_url": "https://arxiv.org/abs/2511.12797"
  },
  {
    "title": "WebCoach: Self-Evolving Web Agents with Cross-Session Memory Guidance",
    "summary": "Multimodal LLM-powered agents have recently demonstrated impressive capabilities in web navigation, enabling agents to complete complex browsing tasks across diverse domains. However, current agents struggle with repetitive errors and lack the ability to learn from past experiences across sessions, limiting their long-term robustness and sample efficiency. We introduce WebCoach, a model-agnostic self-evolving framework that equips web browsing agents with persistent cross-session memory, enabling improved long-term planning, reflection, and continual learning without retraining. WebCoach consists of three key components: (1) a WebCondenser, which standardizes raw navigation logs into concise summaries; (2) an External Memory Store, which organizes complete trajectories as episodic experiences; and (3) a Coach, which retrieves relevant experiences based on similarity and recency, and decides whether to inject task-specific advice into the agent via runtime hooks. This design empowers web agents to access long-term memory beyond their native context window, improving robustness in complex browsing tasks. Moreover, WebCoach achieves self-evolution by continuously curating episodic memory from new navigation trajectories, enabling agents to improve over time without retraining. Evaluations on the WebVoyager benchmark demonstrate that WebCoach consistently improves the performance of browser-use agents across three different LLM backbones. With a 38B model, it increases task success rates from 47% to 61% while reducing or maintaining the average number of steps. Notably, smaller base models with WebCoach achieve performance comparable to the same web agent using GPT-4o.",
    "translation": "标题：WebCoach：具备跨会话记忆引导的自演进网络智能体\n\n摘要：多模态大语言模型驱动的智能体近期在网络导航任务中展现出卓越能力，能够完成跨领域的复杂浏览任务。然而，现有智能体仍面临重复性错误问题，且缺乏跨会话经验学习能力，限制了其长期鲁棒性与样本效率。我们提出WebCoach——一个与模型无关的自演进框架，通过赋予网络浏览智能体持续性的跨会话记忆，实现无需重新训练即可提升长期规划、反思与持续学习能力。该框架包含三个核心组件：（1）WebCondenser：将原始导航日志标准化为精简摘要；（2）外部记忆库：将完整操作轨迹组织为情景化经验；（3）教练模块：基于相似度与时效性检索相关经验，并通过运行时钩子决策是否向智能体注入任务特定建议。该设计使网络智能体能够突破自身上下文窗口限制访问长期记忆，显著增强复杂浏览任务中的鲁棒性。此外，WebCoach通过持续整合新导航轨迹来优化情景记忆，实现无需重新训练的持续性能提升。在WebVoyager基准测试中，WebCoach在三种不同大语言模型基座上均能稳定提升浏览器使用智能体的性能。使用38B参数模型时，任务成功率从47%提升至61%，同时保持或减少了平均操作步数。值得注意的是，搭载WebCoach的较小基座模型可实现与使用GPT-4o的同等网络智能体相媲美的性能。",
    "url": "https://huggingface.co/papers/2511.12997",
    "arxiv_url": "https://arxiv.org/abs/2511.12997"
  },
  {
    "title": "Assessing LLMs for Serendipity Discovery in Knowledge Graphs: A Case for Drug Repurposing",
    "summary": "Large Language Models (LLMs) have greatly advanced knowledge graph question answering (KGQA), yet existing systems are typically optimized for returning highly relevant but predictable answers. A missing yet desired capacity is to exploit LLMs to suggest surprise and novel (\"serendipitious\") answers. In this paper, we formally define the serendipity-aware KGQA task and propose the SerenQA framework to evaluate LLMs' ability to uncover unexpected insights in scientific KGQA tasks. SerenQA includes a rigorous serendipity metric based on relevance, novelty, and surprise, along with an expert-annotated benchmark derived from the Clinical Knowledge Graph, focused on drug repurposing. Additionally, it features a structured evaluation pipeline encompassing three subtasks: knowledge retrieval, subgraph reasoning, and serendipity exploration. Our experiments reveal that while state-of-the-art LLMs perform well on retrieval, they still struggle to identify genuinely surprising and valuable discoveries, underscoring a significant room for future improvements. Our curated resources and extended version are released at: https://cwru-db-group.github.io/serenQA.",
    "translation": "标题：评估大语言模型在知识图谱中的偶然性发现能力：以药物重定位为例\n\n摘要：大语言模型显著推动了知识图谱问答系统的发展，然而现有系统通常针对返回高度相关但可预测的答案进行优化。当前系统缺失但亟需的能力是利用大语言模型提供令人惊喜的新颖（\"偶然性\"）答案。本文正式定义了偶然性感知的知识图谱问答任务，并提出SerenQA框架来评估大语言模型在科学知识图谱问答中发现意外洞见的能力。该框架包含基于相关性、新颖性和惊喜度的严谨偶然性度量指标，以及源自临床知识图谱的专家标注基准数据集，重点关注药物重定位领域。此外，该框架设计了包含三个子任务的结构化评估流程：知识检索、子图推理和偶然性探索。实验结果表明，尽管最先进的大语言模型在检索任务上表现良好，但在识别真正具有惊喜价值的新发现方面仍存在困难，这凸显了未来改进的重要空间。我们整理的资源及扩展版本已发布于：https://cwru-db-group.github.io/serenQA。",
    "url": "https://huggingface.co/papers/2511.12472",
    "arxiv_url": "https://arxiv.org/abs/2511.12472"
  },
  {
    "title": "Test-Time Spectrum-Aware Latent Steering for Zero-Shot Generalization in Vision-Language Models",
    "summary": "Vision-Language Models (VLMs) excel at zero-shot inference but often degrade under test-time domain shifts. For this reason, episodic test-time adaptation strategies have recently emerged as powerful techniques for adapting VLMs to a single unlabeled image. However, existing adaptation strategies, such as test-time prompt tuning, typically require backpropagating through large encoder weights or altering core model components. In this work, we introduce Spectrum-Aware Test-Time Steering (STS), a lightweight adaptation framework that extracts a spectral subspace from the textual embeddings to define principal semantic directions and learns to steer latent representations in a spectrum-aware manner by adapting a small number of per-sample shift parameters to minimize entropy across augmented views. STS operates entirely at inference in the latent space, without backpropagation through or modification of the frozen encoders. Building on standard evaluation protocols, our comprehensive experiments demonstrate that STS largely surpasses or compares favorably against state-of-the-art test-time adaptation methods, while introducing only a handful of additional parameters and achieving inference speeds up to 8x faster with a 12x smaller memory footprint than conventional test-time prompt tuning. The code is available at https://github.com/kdafnis/STS.",
    "translation": "标题：面向视觉语言模型零样本泛化的测试时频谱感知隐空间调控\n\n摘要：视觉语言模型在零样本推理任务中表现卓越，但常因测试时域偏移而性能衰退。为此，基于情景式测试时适应的策略近期成为将视觉语言模型适配至单张未标注图像的有效手段。然而现有适配方法（如测试时提示调优）通常需要对大型编码器权重进行反向传播，或需改动核心模型结构。本研究提出频谱感知测试时调控框架，该轻量化适配框架通过以下方式实现：从文本嵌入中提取频谱子空间以定义核心语义方向，采用频谱感知方式学习隐表征的调控策略，通过适配少量样本级偏移参数来最小化多增强视图下的信息熵。该框架完全在隐空间内进行推理，无需对冻结编码器执行反向传播或结构修改。基于标准评估协议的综合性实验表明，本方法在多数情况下显著超越或优于最先进的测试时适配方法，同时仅引入少量额外参数，推理速度较传统测试时提示调优提升最高达8倍，内存占用减少12倍。代码已开源：https://github.com/kdafnis/STS。",
    "url": "https://huggingface.co/papers/2511.09809",
    "arxiv_url": "https://arxiv.org/abs/2511.09809"
  },
  {
    "title": "UnSAMv2: Self-Supervised Learning Enables Segment Anything at Any Granularity",
    "summary": "The Segment Anything Model (SAM) family has become a widely adopted vision foundation model, but its ability to control segmentation granularity remains limited. Users often need to refine results manually - by adding more prompts or selecting from pre-generated masks - to achieve the desired level of detail. This process can be ambiguous, as the same prompt may correspond to several plausible masks, and collecting dense annotations across all granularities is prohibitively expensive, making supervised solutions infeasible. To address this limitation, we introduce UnSAMv2, which enables segment anything at any granularity without human annotations. UnSAMv2 extends the divide-and-conquer strategy of UnSAM by discovering abundant mask-granularity pairs and introducing a novel granularity control embedding that enables precise, continuous control over segmentation scale. Remarkably, with only 6K unlabeled images and 0.02% additional parameters, UnSAMv2 substantially enhances SAM-2, achieving segment anything at any granularity across interactive, whole-image, and video segmentation tasks. Evaluated on over 11 benchmarks, UnSAMv2 improves NoC_{90} (5.69 rightarrow 4.75), 1-IoU (58.0 rightarrow 73.1), and AR_{1000} (49.6 rightarrow 68.3), showing that small amounts of unlabeled data with a granularity-aware self-supervised learning method can unlock the potential of vision foundation models.",
    "translation": "标题：UnSAMv2：自监督学习实现任意粒度图像分割\n\n摘要：Segment Anything Model（SAM）系列已成为广泛采用的视觉基础模型，但其分割粒度控制能力仍存在局限。用户常需通过添加更多提示或从预生成掩码中手动筛选来优化结果，以实现所需细节层级。这一过程存在模糊性——相同提示可能对应多个合理掩码，且全粒度密集标注成本极高，使监督式解决方案难以实施。为突破此限制，我们提出UnSAMv2，无需人工标注即可实现任意粒度图像分割。该模型延续UnSAM的分治策略，通过发掘海量掩码-粒度配对数据，引入新型粒度控制嵌入机制，实现对分割尺度的精准连续调控。值得注意的是，仅使用6千张无标注图像和0.02%的附加参数，UnSAMv2即显著增强SAM-2模型，在交互式分割、全图像分割及视频分割任务中实现全粒度分割能力。经11余项基准测试验证，UnSAMv2将NoC_{90}指标从5.69提升至4.75，1-IoU从58.0优化至73.1，AR_{1000}从49.6提升至68.3，证明结合粒度感知自监督方法的少量无标注数据即可释放视觉基础模型的潜在能力。",
    "url": "https://huggingface.co/papers/2511.13714",
    "arxiv_url": "https://arxiv.org/abs/2511.13714"
  },
  {
    "title": "MicroVQA++: High-Quality Microscopy Reasoning Dataset with Weakly Supervised Graphs for Multimodal Large Language Model",
    "summary": "Multimodal Large Language Models are increasingly applied to biomedical imaging, yet scientific reasoning for microscopy remains limited by the scarcity of large-scale, high-quality training data. We introduce MicroVQA++, a three-stage, large-scale and high-quality microscopy VQA corpus derived from the BIOMEDICA archive. Stage one bootstraps supervision from expert-validated figure-caption pairs sourced from peer-reviewed articles. Stage two applies HiCQA-Graph, a novel heterogeneous graph over images, captions, and QAs that fuses NLI-based textual entailment, CLIP-based vision-language alignment, and agent signals to identify and filter inconsistent samples. Stage three uses a MultiModal Large Language Model (MLLM) agent to generate multiple-choice questions (MCQ) followed by human screening. The resulting release comprises a large training split and a human-checked test split whose Bloom's level hard-sample distribution exceeds the MicroVQA benchmark. Our work delivers (i) a quality-controlled dataset that couples expert literature with graph-based filtering and human refinement; (ii) HiCQA-Graph, the first graph that jointly models (image, caption, QA) for cross-modal consistency filtering; (iii) evidence that careful data construction enables 4B-scale MLLMs to reach competitive microscopy reasoning performance (e.g., GPT-5) and achieve state-of-the-art performance among open-source MLLMs. Code and dataset will be released after the review process concludes.",
    "translation": "标题：MicroVQA++：面向多模态大语言模型的高质量显微图像推理数据集与弱监督图结构\n\n摘要：多模态大语言模型在生物医学成像领域的应用日益广泛，但显微图像的科学推理仍受限于大规模高质量训练数据的稀缺。我们提出MicroVQA++——一个基于BIOMEDICA档案构建的三阶段、大规模高质量显微视觉问答语料库。第一阶段通过经专家验证的同行评审期刊图注对实现监督信号的初始引导；第二阶段应用HiCQA-Graph新型异质图结构，该图融合基于自然语言推理的文本蕴含关系、基于CLIP的视觉语言对齐以及智能体信号，对图像、图注和问答三元组进行跨模态一致性筛选；第三阶段采用多模态大语言模型智能体生成多项选择题，并经过人工筛查验证。最终发布的数据集包含大规模训练集和经人工校验的测试集，其布鲁姆认知层级难度样本分布超越MicroVQA基准。本研究的贡献在于：（1）构建了融合专家文献知识、图结构过滤与人工精校的质量可控数据集；（2）首创联合建模（图像、图注、问答）三元组的HiCQA-Graph跨模态一致性过滤机制；（3）通过实验证明精细的数据构建能使40亿参数级多模态大语言模型达到与GPT-5相当的显微图像推理性能，并在开源模型中实现最先进水平。代码与数据集将在评审结束后公开发布。",
    "url": "https://huggingface.co/papers/2511.11407",
    "arxiv_url": "https://arxiv.org/abs/2511.11407"
  },
  {
    "title": "Dynamic Reflections: Probing Video Representations with Text Alignment",
    "summary": "The alignment of representations from different modalities has recently been shown to provide insights on the structural similarities and downstream capabilities of different encoders across diverse data types. While significant progress has been made in aligning images with text, the temporal nature of video data remains largely unexplored in this context. In this work, we conduct the first comprehensive study of video-text representation alignment, probing the capabilities of modern video and language encoders. Our findings reveal several key insights. First, we demonstrate that cross-modal alignment highly depends on the richness of both visual (static images vs. multi-frame videos) and text (single caption vs. a collection) data provided at test time, especially when using state-of-the-art video encoders. We propose parametric test-time scaling laws that capture this behavior and show remarkable predictive power against empirical observations. Secondly, we investigate the correlation between semantic alignment and performance on both semantic and non-semantic downstream tasks, providing initial evidence that strong alignment against text encoders may be linked to general-purpose video representation and understanding. Finally, we correlate temporal reasoning with cross-modal alignment providing a challenging test-bed for vision and language models. Overall, our work introduces video-text alignment as an informative zero-shot way to probe the representation power of different encoders for spatio-temporal data. Project page can be found at https://video-prh.github.io/",
    "translation": "标题：动态反思：通过文本对齐探究视频表征能力\n\n摘要：近年来研究表明，跨模态表征对齐能够揭示不同编码器在多种数据类型上的结构相似性与下游任务处理能力。尽管图像与文本对齐已取得显著进展，但视频数据的时间特性在此背景下仍待深入探索。本研究首次对视频-文本表征对齐展开系统性研究，深入探究现代视频与语言编码器的能力。我们的研究获得若干重要发现：首先，我们证明跨模态对齐高度依赖于测试时提供的视觉数据（静态图像与多帧视频）和文本数据（单一描述与集合描述）的丰富程度，这一特性在使用前沿视频编码器时尤为显著。我们提出参数化测试时缩放定律来刻画这一现象，该定律在实证观察中展现出卓越的预测能力。其次，我们探究了语义对齐与语义/非语义下游任务性能之间的关联，初步证据表明与文本编码器的强对齐可能关联着通用视频表征与理解能力。最后，我们将时序推理与跨模态对齐建立关联，为视觉语言模型提供了具有挑战性的测试平台。总体而言，本研究提出视频-文本对齐作为一种零样本评估方法，为探究时空数据编码器的表征能力提供了新的研究视角。项目页面详见：https://video-prh.github.io/",
    "url": "https://huggingface.co/papers/2511.02767",
    "arxiv_url": "https://arxiv.org/abs/2511.02767"
  },
  {
    "title": "LoCoBench-Agent: An Interactive Benchmark for LLM Agents in Long-Context Software Engineering",
    "summary": "As large language models (LLMs) evolve into sophisticated autonomous agents capable of complex software development tasks, evaluating their real-world capabilities becomes critical. While existing benchmarks like LoCoBench~qiu2025locobench assess long-context code understanding, they focus on single-turn evaluation and cannot capture the multi-turn interactive nature, tool usage patterns, and adaptive reasoning required by real-world coding agents. We introduce LoCoBench-Agent, a comprehensive evaluation framework specifically designed to assess LLM agents in realistic, long-context software engineering workflows. Our framework extends LoCoBench's 8,000 scenarios into interactive agent environments, enabling systematic evaluation of multi-turn conversations, tool usage efficiency, error recovery, and architectural consistency across extended development sessions. We also introduce an evaluation methodology with 9 metrics across comprehension and efficiency dimensions. Our framework provides agents with 8 specialized tools (file operations, search, code analysis) and evaluates them across context lengths ranging from 10K to 1M tokens, enabling precise assessment of long-context performance. Through systematic evaluation of state-of-the-art models, we reveal several key findings: (1) agents exhibit remarkable long-context robustness; (2) comprehension-efficiency trade-off exists with negative correlation, where thorough exploration increases comprehension but reduces efficiency; and (3) conversation efficiency varies dramatically across models, with strategic tool usage patterns differentiating high-performing agents. As the first long-context LLM agent benchmark for software engineering, LoCoBench-Agent establishes a rigorous foundation for measuring agent capabilities, identifying performance gaps, and advancing autonomous software development at scale.",
    "translation": "标题：LoCoBench-Agent：面向长上下文软件工程场景的大语言模型智能体交互式评测基准\n\n摘要：随着大语言模型（LLM）逐步发展为能够执行复杂软件开发任务的自主智能体，对其实际能力的评估变得至关重要。现有基准（如LoCoBench~qiu2025locobench）虽能评估长上下文代码理解能力，但仅关注单轮测试，无法捕捉现实编码智能体所需的多轮交互特性、工具使用模式和自适应推理能力。我们提出LoCoBench-Agent——专为评估长上下文软件工程工作流中LLM智能体表现而构建的综合评测框架。该框架将LoCoBench的8,000个场景扩展为交互式智能体环境，支持系统化评估多轮对话、工具使用效率、错误恢复能力以及跨时段开发的架构一致性。我们同时提出包含理解度与效率维度共9项指标的评测方法，为智能体提供8种专业工具（文件操作、搜索、代码分析等），并在10K至1M令牌的上下文长度范围内进行评测，实现对长上下文性能的精准评估。通过对前沿模型的系统化评测，我们获得三项关键发现：（1）智能体展现出显著的长上下文鲁棒性；（2）理解度与效率存在负相关的权衡关系，深入探索会提升理解度但降低效率；（3）不同模型的对话效率差异显著，战略性的工具使用模式是高性能智能体的关键区分特征。作为首个面向软件工程的长上下文LLM智能体基准，LoCoBench-Agent为衡量智能体能力、识别性能差距及推进规模化自主软件开发建立了严谨的评估基础。",
    "url": "https://huggingface.co/papers/2511.13998",
    "arxiv_url": "https://arxiv.org/abs/2511.13998"
  },
  {
    "title": "SafeGRPO: Self-Rewarded Multimodal Safety Alignment via Rule-Governed Policy Optimization",
    "summary": "Multimodal large language models (MLLMs) have demonstrated impressive reasoning and instruction-following capabilities, yet their expanded modality space introduces new compositional safety risks that emerge from complex text-image interactions. Such cross-modal couplings can produce unsafe semantics even when individual inputs are benign, exposing the fragile safety awareness of current MLLMs. While recent works enhance safety by guiding models to reason about potential risks, unregulated reasoning traces may compromise alignment; although Group Relative Policy Optimization (GRPO) offers self-rewarded refinement without human supervision, it lacks verifiable signals for reasoning safety. To address this, we propose SafeGRPO a self-rewarded multimodal safety alignment framework that integrates rule-governed reward construction into GRPO, enabling interpretable and verifiable optimization of reasoning safety. Built upon the constructed SafeTag-VL-3K dataset with explicit visual, textual, and combined safety tags, SafeGRPO performs step-guided safety thinking to enforce structured reasoning and behavior alignment, substantially improving multimodal safety awareness, compositional robustness, and reasoning stability across diverse benchmarks without sacrificing general capabilities.",
    "translation": "标题：SafeGRPO：基于规则约束策略优化的自奖励多模态安全对齐方法\n\n摘要：多模态大语言模型虽展现出卓越的推理与指令跟随能力，但其扩展的模态空间引发了由复杂图文交互产生的新型组合式安全风险。这种跨模态耦合即使在输入内容各自无害时仍可能生成不安全语义，暴露出当前模型脆弱的安全认知。现有研究通过引导模型进行风险推理来增强安全性，但未受约束的推理轨迹可能破坏对齐效果；尽管组相对策略优化（GRPO）提供了无需人工监督的自奖励优化机制，但其缺乏可验证的推理安全信号。为此，我们提出SafeGRPO框架，将规则约束的奖励构建机制融入GRPO，实现可解释、可验证的推理安全优化。基于构建的包含显式视觉、文本及组合安全标签的SafeTag-VL-3K数据集，SafeGRPO通过步骤引导的安全思维机制强化结构化推理与行为对齐，在保持通用能力的同时，显著提升了多模态安全认知、组合鲁棒性及跨基准推理稳定性。",
    "url": "https://huggingface.co/papers/2511.12982",
    "arxiv_url": "https://arxiv.org/abs/2511.12982"
  },
  {
    "title": "AI-Salesman: Towards Reliable Large Language Model Driven Telemarketing",
    "summary": "Goal-driven persuasive dialogue, exemplified by applications like telemarketing, requires sophisticated multi-turn planning and strict factual faithfulness, which remains a significant challenge for even state-of-the-art Large Language Models (LLMs). A lack of task-specific data often limits previous works, and direct LLM application suffers from strategic brittleness and factual hallucination. In this paper, we first construct and release TeleSalesCorpus, the first real-world-grounded dialogue dataset for this domain. We then propose AI-Salesman, a novel framework featuring a dual-stage architecture. For the training stage, we design a Bayesian-supervised reinforcement learning algorithm that learns robust sales strategies from noisy dialogues. For the inference stage, we introduce the Dynamic Outline-Guided Agent (DOGA), which leverages a pre-built script library to provide dynamic, turn-by-turn strategic guidance. Moreover, we design a comprehensive evaluation framework that combines fine-grained metrics for key sales skills with the LLM-as-a-Judge paradigm. Experimental results demonstrate that our proposed AI-Salesman significantly outperforms baseline models in both automatic metrics and comprehensive human evaluations, showcasing its effectiveness in complex persuasive scenarios.",
    "translation": "标题：AI-Salesman：构建可信赖的大语言模型驱动电话营销系统\n\n摘要：以电话营销为代表的目标驱动型说服性对话，需要复杂的多轮规划与严格的事实准确性，这对当前最先进的大语言模型仍构成重大挑战。既往研究常受限于领域特定数据的缺乏，而直接应用大语言模型则存在策略脆弱性和事实幻觉问题。本文首先构建并发布了TeleSalesCorpus——该领域首个基于真实场景的对话数据集。继而提出具有双阶段架构的创新框架AI-Salesman：在训练阶段，我们设计了贝叶斯监督强化学习算法，从含噪声的对话数据中学习稳健的销售策略；在推理阶段，我们引入动态大纲引导智能体（DOGA），通过预设脚本库提供动态的逐轮策略指导。此外，我们构建了结合细粒度销售技能指标与“LLM即评判官”范式的综合评估框架。实验结果表明，我们提出的AI-Salesman在自动评估指标和综合人工评估中均显著优于基线模型，展现了其在复杂说服场景中的卓越效能。",
    "url": "https://huggingface.co/papers/2511.12133",
    "arxiv_url": "https://arxiv.org/abs/2511.12133"
  },
  {
    "title": "Instella: Fully Open Language Models with Stellar Performance",
    "summary": "Large language models (LLMs) have demonstrated remarkable performance across a wide range of tasks, yet the majority of high-performing models remain closed-source or partially open, limiting transparency and reproducibility. In this work, we introduce Instella, a family of fully open three billion parameter language models trained entirely on openly available data and codebase. Powered by AMD Instinct MI300X GPUs, Instella is developed through large-scale pre-training, general-purpose instruction tuning, and alignment with human preferences. Despite using substantially fewer pre-training tokens than many contemporaries, Instella achieves state-of-the-art results among fully open models and is competitive with leading open-weight models of comparable size. We further release two specialized variants: Instella-Long, capable of handling context lengths up to 128K tokens, and Instella-Math, a reasoning-focused model enhanced through supervised fine-tuning and reinforcement learning on mathematical tasks. Together, these contributions establish Instella as a transparent, performant, and versatile alternative for the community, advancing the goal of open and reproducible language modeling research.",
    "translation": "标题：Instella：具备卓越性能的完全开放语言模型\n\n摘要：大语言模型已在广泛任务中展现出卓越性能，然而多数高性能模型仍保持闭源或部分开放，限制了研究的透明度与可复现性。本研究推出Instella系列——完全开放的三十亿参数语言模型，其训练全程基于公开可获取的数据与代码库。依托AMD Instinct MI300X GPU的算力支持，该模型通过大规模预训练、通用指令微调及人类偏好对齐三个阶段构建。尽管预训练词元数量显著少于同期多数模型，Instella在完全开放模型中取得了领先性能，并与同类规模的顶尖开放权重模型表现相当。我们同时发布两个专项优化版本：支持128K词元上下文长度的Instella-Long，以及通过监督微调与数学任务强化学习增强的推理专用模型Instella-Math。这些成果共同确立了Instella作为透明、高效、多功能的开源替代方案，有力推进了开放可复现语言模型研究的发展目标。",
    "url": "https://huggingface.co/papers/2511.10628",
    "arxiv_url": "https://arxiv.org/abs/2511.10628"
  },
  {
    "title": "A Decentralized Retrieval Augmented Generation System with Source Reliabilities Secured on Blockchain",
    "summary": "Existing retrieval-augmented generation (RAG) systems typically use a centralized architecture, causing a high cost of data collection, integration, and management, as well as privacy concerns. There is a great need for a decentralized RAG system that enables foundation models to utilize information directly from data owners who maintain full control over their sources. However, decentralization brings a challenge: the numerous independent data sources vary significantly in reliability, which can diminish retrieval accuracy and response quality. To address this, our decentralized RAG system has a novel reliability scoring mechanism that dynamically evaluates each source based on the quality of responses it contributes to generate and prioritizes high-quality sources during retrieval. To ensure transparency and trust, the scoring process is securely managed through blockchain-based smart contracts, creating verifiable and tamper-proof reliability records without relying on a central authority. We evaluate our decentralized system with two Llama models (3B and 8B) in two simulated environments where six data sources have different levels of reliability. Our system achieves a +10.7\\% performance improvement over its centralized counterpart in the real world-like unreliable data environments. Notably, it approaches the upper-bound performance of centralized systems under ideally reliable data environments. The decentralized infrastructure enables secure and trustworthy scoring management, achieving approximately 56\\% marginal cost savings through batched update operations. Our code and system are open-sourced at github.com/yining610/Reliable-dRAG.",
    "translation": "标题：基于区块链可信源可靠性的去中心化检索增强生成系统  \n\n摘要：现有的检索增强生成（RAG）系统通常采用集中式架构，导致数据收集、整合与管理成本高昂，并引发隐私担忧。亟需一种去中心化RAG系统，使基础模型能够直接利用数据所有者完全掌控的源信息。然而，去中心化架构带来关键挑战：大量独立数据源的可靠性差异显著，可能降低检索精度与响应质量。为此，我们提出一种新型去中心化RAG系统，其创新性可靠性评分机制能根据各数据源对生成响应的质量贡献进行动态评估，并在检索过程中优先调用高质量源。为确保透明度与可信度，评分过程通过基于区块链的智能合约进行安全管理，建立可验证且防篡改的可靠性记录，无需依赖中心化机构。我们使用两个Llama模型（3B与8B参数）在两种模拟环境中进行系统评估，其中六个数据源具有不同可靠性等级。在类真实世界的不可靠数据环境中，本系统性能较集中式基准提升10.7%。值得注意的是，在理想可靠数据环境下，其性能可逼近集中式系统的理论上限。该去中心化基础设施通过批处理更新操作实现安全可信的评分管理，边际成本降低约56%。代码与系统已在github.com/yining610/Reliable-dRAG开源。",
    "url": "https://huggingface.co/papers/2511.07577",
    "arxiv_url": "https://arxiv.org/abs/2511.07577"
  },
  {
    "title": "NORA-1.5: A Vision-Language-Action Model Trained using World Model- and Action-based Preference Rewards",
    "summary": "Vision--language--action (VLA) models have recently shown promising performance on a variety of embodied tasks, yet they still fall short in reliability and generalization, especially when deployed across different embodiments or real-world environments. In this work, we introduce NORA-1.5, a VLA model built from the pre-trained NORA backbone by adding to it a flow-matching-based action expert. This architectural enhancement alone yields substantial performance gains, enabling NORA-1.5 to outperform NORA and several state-of-the-art VLA models across both simulated and real-world benchmarks. To further improve robustness and task success, we develop a set of reward models for post-training VLA policies. Our rewards combine (i) an action-conditioned world model (WM) that evaluates whether generated actions lead toward the desired goal, and (ii) a deviation-from-ground-truth heuristic that distinguishes good actions from poor ones. Using these reward signals, we construct preference datasets and adapt NORA-1.5 to target embodiments through direct preference optimization (DPO). Extensive evaluations show that reward-driven post-training consistently improves performance in both simulation and real-robot settings, demonstrating significant VLA model-reliability gains through simple yet effective reward models. Our findings highlight NORA-1.5 and reward-guided post-training as a viable path toward more dependable embodied agents suitable for real-world deployment.",
    "translation": "标题：NORA-1.5：基于世界模型与动作偏好奖励训练的视觉-语言-动作模型\n\n摘要：视觉-语言-动作模型近期在各类具身任务中展现出良好性能，但其可靠性与泛化能力仍存在不足，尤其在跨智能体部署或真实环境应用时更为明显。本研究提出NORA-1.5模型，该模型基于预训练的NORA主干网络，通过引入基于流匹配的动作专家模块实现架构升级。仅此结构改进即带来显著性能提升，使NORA-1.5在仿真与真实环境测试中均超越原始NORA模型及多项前沿VLA模型。为进一步增强鲁棒性与任务成功率，我们开发了一套用于后训练VLA策略的奖励模型。该奖励体系融合了：（i）动作条件世界模型——评估生成动作是否导向预期目标；（ii）真实动作偏差启发式规则——区分优质与劣质动作。利用这些奖励信号，我们构建偏好数据集并通过直接偏好优化方法使NORA-1.5适配目标智能体。大量实验表明，基于奖励的后训练能持续提升模型在仿真与真实机器人环境中的表现，通过简洁高效的奖励模型实现了VLA模型可靠性的显著进步。我们的研究成果印证了NORA-1.5模型与奖励引导后训练机制可作为开发适用于真实场景的可靠具身智能体的有效路径。",
    "url": "https://huggingface.co/papers/2511.14659",
    "arxiv_url": "https://arxiv.org/abs/2511.14659"
  },
  {
    "title": "OpenUS: A Fully Open-Source Foundation Model for Ultrasound Image Analysis via Self-Adaptive Masked Contrastive Learning",
    "summary": "Ultrasound (US) is one of the most widely used medical imaging modalities, thanks to its low cost, portability, real-time feedback, and absence of ionizing radiation. However, US image interpretation remains highly operator-dependent and varies significantly across anatomical regions, acquisition protocols, and device types. These variations, along with unique challenges such as speckle, low contrast, and limited standardized annotations, hinder the development of generalizable, label-efficient ultrasound AI models. In this paper, we propose OpenUS, the first reproducible, open-source ultrasound foundation model built on a large collection of public data. OpenUS employs a vision Mamba backbone, capturing both local and global long-range dependencies across the image. To extract rich features during pre-training, we introduce a novel self-adaptive masking framework that combines contrastive learning with masked image modeling. This strategy integrates the teacher's attention map with student reconstruction loss, adaptively refining clinically-relevant masking to enhance pre-training effectiveness. OpenUS also applies a dynamic learning schedule to progressively adjust the difficulty of the pre-training process. To develop the foundation model, we compile the largest to-date public ultrasound dataset comprising over 308K images from 42 publicly available datasets, covering diverse anatomical regions, institutions, imaging devices, and disease types. Our pre-trained OpenUS model can be easily adapted to specific downstream tasks by serving as a backbone for label-efficient fine-tuning. Code is available at https://github.com/XZheng0427/OpenUS.",
    "translation": "标题：OpenUS：基于自适应掩码对比学习的全开源超声图像分析基础模型\n\n摘要：超声成像因其低成本、便携性、实时反馈和无电离辐射等优势，已成为应用最广泛的医学影像模态之一。然而，超声图像解读仍高度依赖操作者经验，且受解剖区域、采集协议和设备类型的影响存在显著差异。这些变异因素，连同斑点噪声、低对比度和标准化标注稀缺等特有挑战，严重制约了泛化性强、标注高效的超声AI模型的开发。本文提出OpenUS——首个基于大规模公共数据构建的可复现开源超声基础模型。该模型采用视觉Mamba主干网络，能够同时捕捉图像中的局部特征与全局长程依赖关系。为在预训练阶段提取丰富特征，我们创新性地提出了结合对比学习与掩码图像建模的自适应掩码框架。该策略将教师网络的注意力图与学生网络的重构损失相融合，通过自适应优化临床相关区域的掩码机制来提升预训练效能。OpenUS还采用动态学习调度策略，逐步调整预训练任务的难度级别。为构建基础模型，我们整合了迄今最大的公共超声数据集，涵盖42个公开数据源的逾30.8万张图像，涉及多解剖部位、医疗机构、成像设备及疾病类型。预训练完成的OpenUS模型可作为骨干网络，通过标注高效的微调快速适配特定下游任务。代码已开源：https://github.com/XZheng0427/OpenUS。",
    "url": "https://huggingface.co/papers/2511.11510",
    "arxiv_url": "https://arxiv.org/abs/2511.11510"
  }
]