[
  {
    "title": "Urban Socio-Semantic Segmentation with Vision-Language Reasoning",
    "summary": "As hubs of human activity, urban surfaces consist of a wealth of semantic entities. Segmenting these various entities from satellite imagery is crucial for a range of downstream applications. Current advanced segmentation models can reliably segment entities defined by physical attributes (e.g., buildings, water bodies) but still struggle with socially defined categories (e.g., schools, parks). In this work, we achieve socio-semantic segmentation by vision-language model reasoning. To facilitate this, we introduce the Urban Socio-Semantic Segmentation dataset named SocioSeg, a new resource comprising satellite imagery, digital maps, and pixel-level labels of social semantic entities organized in a hierarchical structure. Additionally, we propose a novel vision-language reasoning framework called SocioReasoner that simulates the human process of identifying and annotating social semantic entities via cross-modal recognition and multi-stage reasoning. We employ reinforcement learning to optimize this non-differentiable process and elicit the reasoning capabilities of the vision-language model. Experiments demonstrate our approach's gains over state-of-the-art models and strong zero-shot generalization. Our dataset and code are available in https://github.com/AMAP-ML/SocioReasoner.",
    "translation": "标题：基于视觉-语言推理的城市社会语义分割\n\n摘要：作为人类活动的枢纽，城市地表包含丰富的语义实体。从卫星图像中分割这些多样化的实体对于一系列下游应用至关重要。当前先进的语义分割模型能够可靠地分割由物理属性定义的实体（如建筑物、水体），但在处理社会属性定义的类别（如学校、公园）时仍面临挑战。本研究通过视觉-语言模型的推理能力实现社会语义分割。为此，我们构建了名为SocioSeg的城市社会语义分割数据集，该数据集包含卫星影像、数字地图以及按层级结构组织的社会语义实体像素级标注。此外，我们提出了一种新颖的视觉-语言推理框架SocioReasoner，该框架通过跨模态识别与多阶段推理模拟人类识别与标注社会语义实体的认知过程。我们采用强化学习优化这一不可微分的推理流程，从而激发视觉-语言模型的深层推理能力。实验表明，该方法在性能上超越现有先进模型，并展现出强大的零样本泛化能力。数据集与代码已公开于https://github.com/AMAP-ML/SocioReasoner。",
    "url": "https://huggingface.co/papers/2601.10477",
    "arxiv_url": "https://arxiv.org/abs/2601.10477"
  },
  {
    "title": "STEP3-VL-10B Technical Report",
    "summary": "We present STEP3-VL-10B, a lightweight open-source foundation model designed to redefine the trade-off between compact efficiency and frontier-level multimodal intelligence. STEP3-VL-10B is realized through two strategic shifts: first, a unified, fully unfrozen pre-training strategy on 1.2T multimodal tokens that integrates a language-aligned Perception Encoder with a Qwen3-8B decoder to establish intrinsic vision-language synergy; and second, a scaled post-training pipeline featuring over 1k iterations of reinforcement learning. Crucially, we implement Parallel Coordinated Reasoning (PaCoRe) to scale test-time compute, allocating resources to scalable perceptual reasoning that explores and synthesizes diverse visual hypotheses. Consequently, despite its compact 10B footprint, STEP3-VL-10B rivals or surpasses models 10times-20times larger (e.g., GLM-4.6V-106B, Qwen3-VL-235B) and top-tier proprietary flagships like Gemini 2.5 Pro and Seed-1.5-VL. Delivering best-in-class performance, it records 92.2% on MMBench and 80.11% on MMMU, while excelling in complex reasoning with 94.43% on AIME2025 and 75.95% on MathVision. We release the full model suite to provide the community with a powerful, efficient, and reproducible baseline.",
    "translation": "标题：STEP3-VL-10B技术报告\n\n摘要：本文介绍STEP3-VL-10B——一个旨在重新定义紧凑效率与前沿多模态智能之间权衡的轻量级开源基础模型。该模型通过两项战略转型实现：首先，采用基于1.2万亿多模态标记的统一全解冻预训练策略，将语言对齐的感知编码器与Qwen3-8B解码器融合，建立内在的视觉-语言协同机制；其次，构建包含超千轮强化学习的规模化后训练流程。我们创新性地引入并行协调推理（PaCoRe）机制扩展测试时计算资源，将算力动态分配至可扩展的感知推理过程，实现对多样化视觉假设的探索与综合。实验表明，尽管仅具有100亿参数的紧凑架构，STEP3-VL-10B在多项基准测试中媲美或超越规模达其10-20倍的模型（如GLM-4.6V-106B、Qwen3-VL-235B），并与Gemini 2.5 Pro、Seed-1.5-VL等顶尖专有模型旗鼓相当。该模型取得业界领先性能：在MMBench达到92.2%，MMMU获得80.11%，同时在复杂推理任务中表现卓越，AIME2025得分94.43%，MathVision得分75.95%。我们完整开源模型套件，为学界提供强大、高效且可复现的基准系统。",
    "url": "https://huggingface.co/papers/2601.09668",
    "arxiv_url": "https://arxiv.org/abs/2601.09668"
  },
  {
    "title": "Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in LLMs",
    "summary": "Reinforcement learning (RL) has become a central paradigm for post-training large language models (LLMs), particularly for complex reasoning tasks, yet it often suffers from exploration collapse: policies prematurely concentrate on a small set of dominant reasoning patterns, improving pass@1 while limiting rollout-level diversity and gains in pass@k. We argue that this failure stems from regularizing local token behavior rather than diversity over sets of solutions. To address this, we propose Uniqueness-Aware Reinforcement Learning, a rollout-level objective that explicitly rewards correct solutions that exhibit rare high-level strategies. Our method uses an LLM-based judge to cluster rollouts for the same problem according to their high-level solution strategies, ignoring superficial variations, and reweights policy advantages inversely with cluster size. As a result, correct but novel strategies receive higher rewards than redundant ones. Across mathematics, physics, and medical reasoning benchmarks, our approach consistently improves pass@k across large sampling budgets and increases the area under the pass@k curve (AUC@K) without sacrificing pass@1, while sustaining exploration and uncovering more diverse solution strategies at scale.",
    "translation": "标题：奖励稀缺性：面向大语言模型创造性问题求解的独特性感知强化学习\n\n摘要：强化学习已成为大语言模型后训练的核心范式，尤其在复杂推理任务中，但该方法常受探索坍缩问题困扰：策略过早集中于少数主导推理模式，虽能提升单次通过率，却限制了轨迹层面的多样性及k次通过率的增益。我们认为这一缺陷源于对局部词元行为的常规化约束，而非对解决方案集合多样性的关注。为此，我们提出独特性感知强化学习方法，通过轨迹层目标函数显式奖励那些采用罕见高层策略的正确解决方案。该方法基于大语言模型的评判器，将同一问题的求解轨迹按其高层策略进行聚类（忽略表面差异），并依据聚类规模对策略优势进行反向加权。由此，正确但新颖的策略将比冗余策略获得更高奖励。在数学、物理和医学推理基准测试中，本方法在大规模采样预算下持续提升k次通过率，在保持单次通过率的同时提高k次通过率曲线下面积，并能维持探索过程，系统性地发掘更多样化的求解策略。",
    "url": "https://huggingface.co/papers/2601.08763",
    "arxiv_url": "https://arxiv.org/abs/2601.08763"
  },
  {
    "title": "Collaborative Multi-Agent Test-Time Reinforcement Learning for Reasoning",
    "summary": "Multi-agent systems have evolved into practical LLM-driven collaborators for many applications, gaining robustness from diversity and cross-checking. However, multi-agent RL (MARL) training is resource-intensive and unstable: co-adapting teammates induce non-stationarity, and rewards are often sparse and high-variance. Therefore, we introduce Multi-Agent Test-Time Reinforcement Learning (MATTRL), a framework that injects structured textual experience into multi-agent deliberation at inference time. MATTRL forms a multi-expert team of specialists for multi-turn discussions, retrieves and integrates test-time experiences, and reaches consensus for final decision-making. We also study credit assignment for constructing a turn-level experience pool, then reinjecting it into the dialogue. Across challenging benchmarks in medicine, math, and education, MATTRL improves accuracy by an average of 3.67\\% over a multi-agent baseline, and by 8.67\\% over comparable single-agent baselines. Ablation studies examine different credit-assignment schemes and provide a detailed comparison of how they affect training outcomes. MATTRL offers a stable, effective and efficient path to distribution-shift-robust multi-agent reasoning without tuning.",
    "translation": "标题：面向推理任务的协作式多智能体测试时强化学习\n\n摘要：多智能体系统已发展成为众多实际应用中由大语言模型驱动的实用协作框架，其通过多样性与交叉验证获得鲁棒性。然而，多智能体强化学习训练过程资源消耗大且不稳定：智能体间的协同适应会引发非平稳性问题，且奖励信号通常稀疏且具有高方差特性。为此，我们提出多智能体测试时强化学习框架，该框架在推理阶段将结构化文本经验注入多智能体协商过程。该框架构建由专业智能体组成的多专家团队进行多轮讨论，检索并整合测试时经验，最终通过共识机制形成决策。我们还研究了用于构建轮次级经验池的信用分配机制，并将其重新注入对话流程。在医学、数学与教育领域的多个挑战性基准测试中，该框架相较于多智能体基线模型平均准确率提升3.67%，较可比单智能体基线提升8.67%。消融实验检验了不同信用分配方案，并详细比较了其对训练结果的影响。该框架为无需调参即可实现分布偏移鲁棒的多智能体推理提供了一条稳定、高效且可靠的路径。",
    "url": "https://huggingface.co/papers/2601.09667",
    "arxiv_url": "https://arxiv.org/abs/2601.09667"
  },
  {
    "title": "VIBE: Visual Instruction Based Editor",
    "summary": "Instruction-based image editing is among the fastest developing areas in generative AI. Over the past year, the field has reached a new level, with dozens of open-source models released alongside highly capable commercial systems. However, only a limited number of open-source approaches currently achieve real-world quality. In addition, diffusion backbones, the dominant choice for these pipelines, are often large and computationally expensive for many deployments and research settings, with widely used variants typically containing 6B to 20B parameters. This paper presents a compact, high-throughput instruction-based image editing pipeline that uses a modern 2B-parameter Qwen3-VL model to guide the editing process and the 1.6B-parameter diffusion model Sana1.5 for image generation. Our design decisions across architecture, data processing, training configuration, and evaluation target low-cost inference and strict source consistency while maintaining high quality across the major edit categories feasible at this scale. Evaluated on the ImgEdit and GEdit benchmarks, the proposed method matches or exceeds the performance of substantially heavier baselines, including models with several times as many parameters and higher inference cost, and is particularly strong on edits that require preserving the input image, such as an attribute adjustment, object removal, background edits, and targeted replacement. The model fits within 24 GB of GPU memory and generates edited images at up to 2K resolution in approximately 4 seconds on an NVIDIA H100 in BF16, without additional inference optimizations or distillation.",
    "translation": "标题：VIBE：基于视觉指令的编辑器\n\n摘要：基于指令的图像编辑是生成式人工智能领域发展最快的方向之一。过去一年中，该领域已达到新高度，数十个开源模型与高性能商业系统相继发布。然而，目前仅有有限数量的开源方法能够实现实际应用级别的质量。此外，作为此类流程主流选择的扩散模型主干网络通常体量庞大、计算成本高昂，广泛使用的变体通常包含60亿至200亿参数，这对许多部署和研究场景构成挑战。本文提出一种紧凑型、高吞吐的基于指令图像编辑流程，该流程采用现代化的20亿参数Qwen3-VL模型指导编辑过程，并利用16亿参数扩散模型Sana1.5进行图像生成。我们在架构设计、数据处理、训练配置和评估目标等方面均以低成本推理与严格源图像一致性为核心考量，同时在此规模可行的主要编辑类别中保持高质量输出。通过在ImgEdit和GEdit基准测试中的评估，所提方法的性能达到或显著超越了参数规模数倍、推理成本更高的基线模型，在需要保持输入图像特性的编辑任务（如属性调整、对象移除、背景编辑和定向替换）上表现尤为突出。该模型可在24GB GPU内存内运行，在未进行额外推理优化或蒸馏的情况下，于NVIDIA H100显卡上以BF16精度生成最高2K分辨率的编辑图像，耗时约4秒。",
    "url": "https://huggingface.co/papers/2601.02242",
    "arxiv_url": "https://arxiv.org/abs/2601.02242"
  },
  {
    "title": "Beyond Static Tools: Test-Time Tool Evolution for Scientific Reasoning",
    "summary": "The central challenge of AI for Science is not reasoning alone, but the ability to create computational methods in an open-ended scientific world. Existing LLM-based agents rely on static, pre-defined tool libraries, a paradigm that fundamentally fails in scientific domains where tools are sparse, heterogeneous, and intrinsically incomplete. In this paper, we propose Test-Time Tool Evolution (TTE), a new paradigm that enables agents to synthesize, verify, and evolve executable tools during inference. By transforming tools from fixed resources into problem-driven artifacts, TTE overcomes the rigidity and long-tail limitations of static tool libraries. To facilitate rigorous evaluation, we introduce SciEvo, a benchmark comprising 1,590 scientific reasoning tasks supported by 925 automatically evolved tools. Extensive experiments show that TTE achieves state-of-the-art performance in both accuracy and tool efficiency, while enabling effective cross-domain adaptation of computational tools. The code and benchmark have been released at https://github.com/lujiaxuan0520/Test-Time-Tool-Evol.",
    "translation": "标题：超越静态工具：面向科学推理的测试时工具演化\n\n摘要：科学人工智能的核心挑战不仅在于推理本身，更在于在开放的科学世界中创建计算方法的能力。现有基于大语言模型的智能体依赖于静态的、预定义的工具库，这种范式在工具稀缺、异构且本质上不完整的科学领域存在根本性缺陷。本文提出测试时工具演化这一新范式，使智能体能够在推理过程中合成、验证并演化可执行工具。通过将工具从固定资源转化为问题驱动的产物，该范式克服了静态工具库的僵化性与长尾局限性。为支持严谨评估，我们构建了SciEvo基准数据集，包含1,590项科学推理任务及支撑其实现的925个自动演化工具。大量实验表明，该范式在准确率与工具效率方面均达到最先进水平，同时实现了计算工具的有效跨领域适配。代码与基准数据集已发布于https://github.com/lujiaxuan0520/Test-Time-Tool-Evol。",
    "url": "https://huggingface.co/papers/2601.07641",
    "arxiv_url": "https://arxiv.org/abs/2601.07641"
  },
  {
    "title": "DanQing: An Up-to-Date Large-Scale Chinese Vision-Language Pre-training Dataset",
    "summary": "Vision-Language Pre-training (VLP) models demonstrate strong performance across various downstream tasks by learning from large-scale image-text pairs through contrastive pretraining. The release of extensive English image-text datasets (e.g., COYO-700M and LAION-400M) has enabled widespread adoption of models such as CLIP and SigLIP in tasks including cross-modal retrieval and image captioning. However, the advancement of Chinese vision-language pretraining has substantially lagged behind, due to the scarcity of high-quality Chinese image-text data. To address this gap, we develop a comprehensive pipeline for constructing a high-quality Chinese cross-modal dataset. As a result, we propose DanQing, which contains 100 million image-text pairs collected from Common Crawl. Different from existing datasets, DanQing is curated through a more rigorous selection process, yielding superior data quality. Moreover, DanQing is primarily built from 2024-2025 web data, enabling models to better capture evolving semantic trends and thus offering greater practical utility. We compare DanQing with existing datasets by continual pre-training of the SigLIP2 model. Experimental results show that DanQing consistently achieves superior performance across a range of Chinese downstream tasks, including zero-shot classification, cross-modal retrieval, and LMM-based evaluations. To facilitate further research in Chinese vision-language pre-training, we will open-source the DanQing dataset under the Creative Common CC-BY 4.0 license.",
    "translation": "标题：丹青：一个前沿的大规模中文视觉-语言预训练数据集\n\n摘要：视觉-语言预训练模型通过对比预训练从大规模图文对中学习，在多种下游任务中展现出强大性能。大规模英文图文数据集（如COYO-700M和LAION-400M）的发布，使得CLIP、SigLIP等模型在跨模态检索、图像描述生成等任务中得到广泛应用。然而，由于高质量中文图文数据的稀缺，中文视觉-语言预训练的发展明显滞后。为填补这一空白，我们开发了一套构建高质量中文跨模态数据集的完整流程。基于此，我们提出了包含1亿个从Common Crawl收集的图文对的“丹青”数据集。与现有数据集不同，丹青通过更严格的筛选流程进行构建，数据质量显著提升。此外，丹青主要基于2024-2025年的网络数据构建，能使模型更好地捕捉语义演变趋势，从而具备更强的实际应用价值。我们通过持续预训练SigLIP2模型，将丹青与现有数据集进行了对比实验。结果表明，在包括零样本分类、跨模态检索以及基于大语言模型的评估等一系列中文下游任务中，丹青均能取得更优的性能。为促进中文视觉-语言预训练的进一步研究，我们将在知识共享CC-BY 4.0协议下开源丹青数据集。",
    "url": "https://huggingface.co/papers/2601.10305",
    "arxiv_url": "https://arxiv.org/abs/2601.10305"
  },
  {
    "title": "Toward Ultra-Long-Horizon Agentic Science: Cognitive Accumulation for Machine Learning Engineering",
    "summary": "The advancement of artificial intelligence toward agentic science is currently bottlenecked by the challenge of ultra-long-horizon autonomy, the ability to sustain strategic coherence and iterative correction over experimental cycles spanning days or weeks. While Large Language Models (LLMs) have demonstrated prowess in short-horizon reasoning, they are easily overwhelmed by execution details in the high-dimensional, delayed-feedback environments of real-world research, failing to consolidate sparse feedback into coherent long-term guidance. Here, we present ML-Master 2.0, an autonomous agent that masters ultra-long-horizon machine learning engineering (MLE) which is a representative microcosm of scientific discovery. By reframing context management as a process of cognitive accumulation, our approach introduces Hierarchical Cognitive Caching (HCC), a multi-tiered architecture inspired by computer systems that enables the structural differentiation of experience over time. By dynamically distilling transient execution traces into stable knowledge and cross-task wisdom, HCC allows agents to decouple immediate execution from long-term experimental strategy, effectively overcoming the scaling limits of static context windows. In evaluations on OpenAI's MLE-Bench under 24-hour budgets, ML-Master 2.0 achieves a state-of-the-art medal rate of 56.44%. Our findings demonstrate that ultra-long-horizon autonomy provides a scalable blueprint for AI capable of autonomous exploration beyond human-precedent complexities.",
    "translation": "标题：迈向超长程自主科学：面向机器学习工程的认知累积框架\n\n摘要：人工智能向自主科学的发展目前受限于超长程自主能力挑战——即在跨越数日或数周的实验周期中保持战略连贯性与迭代修正的能力。尽管大语言模型已在短程推理中展现卓越能力，但在现实研究的高维延迟反馈环境中，它们易被执行细节淹没，难以将稀疏反馈整合为连贯的长期指导。本文提出ML-Master 2.0自主智能体，该系统在作为科学发现典型微观范式的超长程机器学习工程任务中实现突破。通过将情境管理重构为认知累积过程，我们引入受计算机系统启发的分层认知缓存架构，该多层级设计实现了经验随时间推移的结构化区分。通过动态将瞬态执行轨迹提炼为稳定知识与跨任务智慧，该架构使智能体能够解耦即时执行与长期实验策略，有效克服静态上下文窗口的扩展限制。在OpenAI MLE-Bench平台24小时预算评估中，ML-Master 2.0获得56.44%的领先奖牌率。研究结果表明，超长程自主能力为人工智能超越人类既有复杂度的自主探索提供了可扩展的蓝图。",
    "url": "https://huggingface.co/papers/2601.10402",
    "arxiv_url": "https://arxiv.org/abs/2601.10402"
  },
  {
    "title": "CoF-T2I: Video Models as Pure Visual Reasoners for Text-to-Image Generation",
    "summary": "Recent video generation models have revealed the emergence of Chain-of-Frame (CoF) reasoning, enabling frame-by-frame visual inference. With this capability, video models have been successfully applied to various visual tasks (e.g., maze solving, visual puzzles). However, their potential to enhance text-to-image (T2I) generation remains largely unexplored due to the absence of a clearly defined visual reasoning starting point and interpretable intermediate states in the T2I generation process. To bridge this gap, we propose CoF-T2I, a model that integrates CoF reasoning into T2I generation via progressive visual refinement, where intermediate frames act as explicit reasoning steps and the final frame is taken as output. To establish such an explicit generation process, we curate CoF-Evol-Instruct, a dataset of CoF trajectories that model the generation process from semantics to aesthetics. To further improve quality and avoid motion artifacts, we enable independent encoding operation for each frame. Experiments show that CoF-T2I significantly outperforms the base video model and achieves competitive performance on challenging benchmarks, reaching 0.86 on GenEval and 7.468 on Imagine-Bench. These results indicate the substantial promise of video models for advancing high-quality text-to-image generation.",
    "translation": "标题：CoF-T2I：作为纯视觉推理器的视频模型用于文本到图像生成\n\n摘要：近期视频生成模型揭示了帧间推理链能力的涌现，实现了逐帧视觉推断。凭借这一能力，视频模型已成功应用于多种视觉任务（如迷宫求解、视觉谜题）。然而，由于文本到图像生成过程中缺乏明确定义的视觉推理起点与可解释的中间状态，其在增强文本到图像生成方面的潜力尚未得到充分探索。为弥补这一空白，我们提出CoF-T2I模型，该模型通过渐进式视觉优化将帧间推理链整合至文本到图像生成过程，其中中间帧作为显式推理步骤，最终帧作为输出结果。为构建此类显式生成流程，我们构建了CoF-Evol-Instruct数据集，该数据集包含建模从语义到美学生成过程的帧间推理轨迹。为进一步提升生成质量并避免运动伪影，我们实现了对每帧的独立编码操作。实验表明，CoF-T2I显著优于基础视频模型，并在具有挑战性的基准测试中取得具有竞争力的性能表现——在GenEval上达到0.86分，在Imagine-Bench上达到7.468分。这些结果证明了视频模型在推进高质量文本到图像生成方面的巨大潜力。",
    "url": "https://huggingface.co/papers/2601.10061",
    "arxiv_url": "https://arxiv.org/abs/2601.10061"
  },
  {
    "title": "Think-Then-Generate: Reasoning-Aware Text-to-Image Diffusion with LLM Encoders",
    "summary": "Recent progress in text-to-image (T2I) diffusion models (DMs) has enabled high-quality visual synthesis from diverse textual prompts. Yet, most existing T2I DMs, even those equipped with large language model (LLM)-based text encoders, remain text-pixel mappers -- they employ LLMs merely as text encoders, without leveraging their inherent reasoning capabilities to infer what should be visually depicted given the textual prompt. To move beyond such literal generation, we propose the think-then-generate (T2G) paradigm, where the LLM-based text encoder is encouraged to reason about and rewrite raw user prompts; the states of the rewritten prompts then serve as diffusion conditioning. To achieve this, we first activate the think-then-rewrite pattern of the LLM encoder with a lightweight supervised fine-tuning process. Subsequently, the LLM encoder and diffusion backbone are co-optimized to ensure faithful reasoning about the context and accurate rendering of the semantics via Dual-GRPO. In particular, the text encoder is reinforced using image-grounded rewards to infer and recall world knowledge, while the diffusion backbone is pushed to produce semantically consistent and visually coherent images. Experiments show substantial improvements in factual consistency, semantic alignment, and visual realism across reasoning-based image generation and editing benchmarks, achieving 0.79 on WISE score, nearly on par with GPT-4. Our results constitute a promising step toward next-generation unified models with reasoning, expression, and demonstration capacities.",
    "translation": "标题：先思后成：基于大语言模型编码器的推理感知文本到图像扩散方法\n\n摘要：文本到图像扩散模型的最新进展已能根据多样化文本提示生成高质量视觉内容。然而，现有大多数文本到图像扩散模型（即使配备基于大语言模型的文本编码器）仍停留在文本-像素映射阶段——它们仅将大语言模型用作文本编码器，未能利用其内在推理能力来推断文本提示对应的视觉呈现内容。为突破这种字面生成模式，本文提出“先思后成”范式，通过激励基于大语言模型的文本编码器对原始用户提示进行推理与重写，并将重写后的提示状态作为扩散条件。为实现这一目标，我们首先通过轻量级监督微调激活大语言模型编码器的“先思后写”模式，随后通过双重生成式强化策略优化协同优化大语言模型编码器与扩散主干网络，确保对上下文的忠实推理和语义的精确呈现。具体而言，文本编码器通过基于图像的奖励机制强化其世界知识的推断与调用能力，而扩散主干网络则被优化以生成语义一致且视觉连贯的图像。实验表明，该方法在基于推理的图像生成与编辑基准测试中，在事实一致性、语义对齐和视觉真实性方面均取得显著提升，WISE分数达到0.79，与GPT-4表现近乎持平。本研究为构建兼具推理、表达与演示能力的下一代统一模型迈出了重要一步。",
    "url": "https://huggingface.co/papers/2601.10332",
    "arxiv_url": "https://arxiv.org/abs/2601.10332"
  },
  {
    "title": "Alterbute: Editing Intrinsic Attributes of Objects in Images",
    "summary": "We introduce Alterbute, a diffusion-based method for editing an object's intrinsic attributes in an image. We allow changing color, texture, material, and even the shape of an object, while preserving its perceived identity and scene context. Existing approaches either rely on unsupervised priors that often fail to preserve identity or use overly restrictive supervision that prevents meaningful intrinsic variations. Our method relies on: (i) a relaxed training objective that allows the model to change both intrinsic and extrinsic attributes conditioned on an identity reference image, a textual prompt describing the target intrinsic attributes, and a background image and object mask defining the extrinsic context. At inference, we restrict extrinsic changes by reusing the original background and object mask, thereby ensuring that only the desired intrinsic attributes are altered; (ii) Visual Named Entities (VNEs) - fine-grained visual identity categories (e.g., ''Porsche 911 Carrera'') that group objects sharing identity-defining features while allowing variation in intrinsic attributes. We use a vision-language model to automatically extract VNE labels and intrinsic attribute descriptions from a large public image dataset, enabling scalable, identity-preserving supervision. Alterbute outperforms existing methods on identity-preserving object intrinsic attribute editing.",
    "translation": "标题：Alterbute：图像中对象固有属性的编辑方法\n\n摘要：本文提出Alterbute，一种基于扩散模型的图像中对象固有属性编辑方法。该方法支持修改对象的颜色、纹理、材质甚至形状，同时保持其感知身份与场景上下文。现有方法要么依赖无监督先验（往往难以保持身份一致性），要么采用过度严格的监督机制（限制了有意义的固有属性变化）。我们的方法基于两个核心设计：（一）采用宽松的训练目标，使模型能够根据身份参考图像、描述目标固有属性的文本提示、以及定义外部背景的背景图像和对象掩码，同时修改对象的固有属性与外部属性。在推理阶段，通过复用原始背景和对象掩码来限制外部变化，从而确保仅目标固有属性被修改；（二）引入视觉命名实体——一种细粒度的视觉身份类别（例如“保时捷911卡雷拉”），将具有身份定义特征但固有属性可变的对象进行分组。我们利用视觉语言模型从大规模公共图像数据集中自动提取VNE标签和固有属性描述，实现了可扩展且保持身份一致性的监督训练。实验表明，Alterbute在保持身份一致性的对象固有属性编辑任务上优于现有方法。",
    "url": "https://huggingface.co/papers/2601.10714",
    "arxiv_url": "https://arxiv.org/abs/2601.10714"
  },
  {
    "title": "MatchTIR: Fine-Grained Supervision for Tool-Integrated Reasoning via Bipartite Matching",
    "summary": "Tool-Integrated Reasoning (TIR) empowers large language models (LLMs) to tackle complex tasks by interleaving reasoning steps with external tool interactions. However, existing reinforcement learning methods typically rely on outcome- or trajectory-level rewards, assigning uniform advantages to all steps within a trajectory. This coarse-grained credit assignment fails to distinguish effective tool calls from redundant or erroneous ones, particularly in long-horizon multi-turn scenarios. To address this, we propose MatchTIR, a framework that introduces fine-grained supervision via bipartite matching-based turn-level reward assignment and dual-level advantage estimation. Specifically, we formulate credit assignment as a bipartite matching problem between predicted and ground-truth traces, utilizing two assignment strategies to derive dense turn-level rewards. Furthermore, to balance local step precision with global task success, we introduce a dual-level advantage estimation scheme that integrates turn-level and trajectory-level signals, assigning distinct advantage values to individual interaction turns. Extensive experiments on three benchmarks demonstrate the superiority of MatchTIR. Notably, our 4B model surpasses the majority of 8B competitors, particularly in long-horizon and multi-turn tasks. Our codes are available at https://github.com/quchangle1/MatchTIR.",
    "translation": "标题：MatchTIR：通过二分图匹配实现工具集成推理的细粒度监督\n\n摘要：工具集成推理通过将推理步骤与外部工具调用交错进行，赋能大语言模型处理复杂任务。然而，现有的强化学习方法通常依赖于结果级或轨迹级奖励，对轨迹中的所有步骤赋予统一的优势值。这种粗粒度的信用分配无法区分有效工具调用与冗余或错误调用，尤其在长视野、多轮次的场景中。为此，我们提出MatchTIR框架，该框架通过基于二分图匹配的轮次级奖励分配和双层优势估计，引入了细粒度的监督机制。具体而言，我们将信用分配问题形式化为预测轨迹与真实轨迹之间的二分图匹配问题，并利用两种分配策略来生成密集的轮次级奖励。此外，为平衡局部步骤精度与全局任务成功率，我们引入了一种双层优势估计方案，该方案整合了轮次级与轨迹级信号，为每个交互轮次分配不同的优势值。在三个基准测试上进行的大量实验证明了MatchTIR的优越性。值得注意的是，我们的40亿参数模型在多数任务上超越了80亿参数的竞争模型，尤其在长视野和多轮次任务中表现突出。代码已开源：https://github.com/quchangle1/MatchTIR。",
    "url": "https://huggingface.co/papers/2601.10712",
    "arxiv_url": "https://arxiv.org/abs/2601.10712"
  },
  {
    "title": "ToolSafe: Enhancing Tool Invocation Safety of LLM-based agents via Proactive Step-level Guardrail and Feedback",
    "summary": "While LLM-based agents can interact with environments via invoking external tools, their expanded capabilities also amplify security risks. Monitoring step-level tool invocation behaviors in real time and proactively intervening before unsafe execution is critical for agent deployment, yet remains under-explored. In this work, we first construct TS-Bench, a novel benchmark for step-level tool invocation safety detection in LLM agents. We then develop a guardrail model, TS-Guard, using multi-task reinforcement learning. The model proactively detects unsafe tool invocation actions before execution by reasoning over the interaction history. It assesses request harmfulness and action-attack correlations, producing interpretable and generalizable safety judgments and feedback. Furthermore, we introduce TS-Flow, a guardrail-feedback-driven reasoning framework for LLM agents, which reduces harmful tool invocations of ReAct-style agents by 65 percent on average and improves benign task completion by approximately 10 percent under prompt injection attacks.",
    "translation": "标题：ToolSafe：通过主动式步骤级护栏与反馈增强基于大语言模型的智能体工具调用安全性\n\n摘要：尽管基于大语言模型的智能体能够通过调用外部工具与环境交互，但其扩展的能力也同时放大了安全风险。实时监控步骤级工具调用行为并在不安全执行前主动干预，对于智能体部署至关重要，然而相关研究仍显不足。本研究首先构建了TS-Bench——一个面向大语言模型智能体步骤级工具调用安全检测的新型基准测试集。随后，我们通过多任务强化学习开发了护栏模型TS-Guard。该模型通过分析交互历史记录，在执行前主动检测不安全的工具调用行为，评估请求的危害性及行为与攻击的关联度，并生成可解释、可泛化的安全判断与反馈。此外，我们提出了TS-Flow——一种基于护栏-反馈驱动的智能体推理框架，该框架在提示注入攻击场景下，平均将ReAct式智能体的有害工具调用减少65%，并将良性任务完成率提升约10%。",
    "url": "https://huggingface.co/papers/2601.10156",
    "arxiv_url": "https://arxiv.org/abs/2601.10156"
  },
  {
    "title": "A Safety Report on GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5",
    "summary": "The rapid evolution of Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) has produced substantial gains in reasoning, perception, and generative capability across language and vision. However, whether these advances yield commensurate improvements in safety remains unclear, in part due to fragmented evaluation practices limited to single modalities or threat models. In this report, we present an integrated safety evaluation of 7 frontier models: GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5. We evaluate each model across language, vision-language, and image generation settings using a unified protocol that integrates benchmark evaluation, adversarial evaluation, multilingual evaluation, and compliance evaluation. Aggregating our evaluations into safety leaderboards and model safety profiles across multiple evaluation modes reveals a sharply heterogeneous safety landscape. While GPT-5.2 demonstrates consistently strong and balanced safety performance across evaluations, other models exhibit pronounced trade-offs among benchmark safety, adversarial alignment, multilingual generalization, and regulatory compliance. Both language and vision-language modalities show significant vulnerability under adversarial evaluation, with all models degrading substantially despite strong results on standard benchmarks. Text-to-image models achieve relatively stronger alignment in regulated visual risk categories, yet remain brittle under adversarial or semantically ambiguous prompts. Overall, these results show that safety in frontier models is inherently multidimensional--shaped by modality, language, and evaluation scheme, underscoring the need for standardized safety evaluations to accurately assess real-world risk and guide responsible model development and deployment.",
    "translation": "标题：GPT-5.2、Gemini 3 Pro、Qwen3-VL、Doubao 1.8、Grok 4.1 Fast、Nano Banana Pro与Seedream 4.5安全评估报告\n\n摘要：大语言模型与多模态大语言模型的快速发展，显著提升了语言与视觉领域的推理、感知及生成能力。然而，这些技术进步是否带来相应的安全性提升尚不明确，部分原因在于现有评估实践局限于单一模态或威胁模型，呈现碎片化特征。本报告对GPT-5.2、Gemini 3 Pro、Qwen3-VL、Doubao 1.8、Grok 4.1 Fast、Nano Banana Pro及Seedream 4.5等7个前沿模型开展综合性安全评估。我们采用统一评估框架——整合基准测试、对抗性评估、多语言评估与合规性评估——在纯语言、视觉-语言及图像生成三种场景中对各模型进行系统评测。通过将多维度评估结果聚合为安全性能排行榜与模型安全画像，研究揭示出高度异质化的安全格局：GPT-5.2在所有评估中均展现出持续稳定且均衡的安全性能，而其他模型则在基准安全性、对抗对齐性、多语言泛化能力及法规遵从性方面呈现显著权衡关系。尽管各模型在标准基准测试中表现良好，但在对抗性评估下，语言与视觉-语言模态均显现出显著脆弱性，所有模型性能均出现大幅下降。文生图模型在受监管视觉风险类别中表现出相对更强的对齐能力，但在对抗性提示或语义模糊提示下仍显脆弱。总体而言，研究结果表明前沿模型的安全性本质上是多维度的——其表现受模态特性、语言类型及评估方案共同塑造，这凸显了建立标准化安全评估体系的必要性，以准确评估现实风险并引导负责任的模型开发与部署。",
    "url": "https://huggingface.co/papers/2601.10527",
    "arxiv_url": "https://arxiv.org/abs/2601.10527"
  },
  {
    "title": "Molmo2: Open Weights and Data for Vision-Language Models with Video Understanding and Grounding",
    "summary": "Today's strongest video-language models (VLMs) remain proprietary. The strongest open-weight models either rely on synthetic data from proprietary VLMs, effectively distilling from them, or do not disclose their training data or recipe. As a result, the open-source community lacks the foundations needed to improve on the state-of-the-art video (and image) language models. Crucially, many downstream applications require more than just high-level video understanding; they require grounding -- either by pointing or by tracking in pixels. Even proprietary models lack this capability. We present Molmo2, a new family of VLMs that are state-of-the-art among open-source models and demonstrate exceptional new capabilities in point-driven grounding in single image, multi-image, and video tasks. Our key contribution is a collection of 7 new video datasets and 2 multi-image datasets, including a dataset of highly detailed video captions for pre-training, a free-form video Q&A dataset for fine-tuning, a new object tracking dataset with complex queries, and an innovative new video pointing dataset, all collected without the use of closed VLMs. We also present a training recipe for this data utilizing an efficient packing and message-tree encoding scheme, and show bi-directional attention on vision tokens and a novel token-weight strategy improves performance. Our best-in-class 8B model outperforms others in the class of open weight and data models on short videos, counting, and captioning, and is competitive on long-videos. On video-grounding Molmo2 significantly outperforms existing open-weight models like Qwen3-VL (35.5 vs 29.6 accuracy on video counting) and surpasses proprietary models like Gemini 3 Pro on some tasks (38.4 vs 20.0 F1 on video pointing and 56.2 vs 41.1 J&F on video tracking).",
    "translation": "标题：Molmo2：具备视频理解与定位能力的视觉语言模型开源权重与数据集\n\n摘要：当前最强大的视频语言模型（VLMs）仍属于闭源系统。最优秀的开源权重模型要么依赖闭源VLMs生成的合成数据进行知识蒸馏，要么未公开其训练数据与方法。这导致开源社区缺乏改进前沿视频（及图像）语言模型所需的基础资源。尤为关键的是，许多下游应用不仅需要高层次视频理解能力，更需具备像素级定位（指向或追踪）功能，而现有闭源模型同样欠缺此能力。本文提出Molmo2系列VLMs，该系列在开源模型中达到领先水平，并在单图像、多图像及视频任务的指向驱动定位中展现出卓越的新能力。我们的核心贡献在于构建了7个新型视频数据集与2个多图像数据集，包括：用于预训练的高细节视频描述数据集、用于微调的自由形式视频问答数据集、包含复杂查询的新型目标追踪数据集，以及创新的视频指向数据集——所有数据均未使用闭源VLMs生成。同时，我们提出了基于高效数据打包与消息树编码方案的训练方法，并证明视觉令牌的双向注意力机制与新颖的令牌加权策略能有效提升性能。我们8B规模的顶尖模型在短视频理解、计数与描述任务上超越同类开源权重与数据模型，在长视频任务中表现相当，在视频定位任务中显著优于Qwen3-VL等现有开源模型（视频计数准确率35.5对29.6），并在部分任务上超越Gemini 3 Pro等闭源模型（视频指向F1分数38.4对20.0，视频追踪J&F分数56.2对41.1）。",
    "url": "https://huggingface.co/papers/2601.10611",
    "arxiv_url": "https://arxiv.org/abs/2601.10611"
  },
  {
    "title": "FlowAct-R1: Towards Interactive Humanoid Video Generation",
    "summary": "Interactive humanoid video generation aims to synthesize lifelike visual agents that can engage with humans through continuous and responsive video. Despite recent advances in video synthesis, existing methods often grapple with the trade-off between high-fidelity synthesis and real-time interaction requirements. In this paper, we propose FlowAct-R1, a framework specifically designed for real-time interactive humanoid video generation. Built upon a MMDiT architecture, FlowAct-R1 enables the streaming synthesis of video with arbitrary durations while maintaining low-latency responsiveness. We introduce a chunkwise diffusion forcing strategy, complemented by a novel self-forcing variant, to alleviate error accumulation and ensure long-term temporal consistency during continuous interaction. By leveraging efficient distillation and system-level optimizations, our framework achieves a stable 25fps at 480p resolution with a time-to-first-frame (TTFF) of only around 1.5 seconds. The proposed method provides holistic and fine-grained full-body control, enabling the agent to transition naturally between diverse behavioral states in interactive scenarios. Experimental results demonstrate that FlowAct-R1 achieves exceptional behavioral vividness and perceptual realism, while maintaining robust generalization across diverse character styles.",
    "translation": "标题：FlowAct-R1：迈向交互式人形视频生成\n\n摘要：交互式人形视频生成旨在合成能够通过连续且响应式视频与人类互通的逼真视觉智能体。尽管视频合成领域近期取得了进展，现有方法仍常面临高保真合成与实时交互需求之间的权衡。本文提出FlowAct-R1，一个专为实时交互式人形视频生成设计的框架。该框架基于MMDiT架构构建，能够实现任意时长视频的流式合成，同时保持低延迟响应。我们引入了一种分块扩散强制策略，并结合新颖的自强制变体，以减轻误差累积并确保连续交互过程中的长期时序一致性。通过高效蒸馏与系统级优化，本框架在480p分辨率下实现了稳定的25fps生成速度，首帧生成时间仅约1.5秒。所提方法提供整体且细粒度的全身控制，使智能体能在交互场景中自然过渡于多种行为状态。实验结果表明，FlowAct-R1在保持跨角色风格鲁棒泛化能力的同时，实现了卓越的行为生动性与感知真实感。",
    "url": "https://huggingface.co/papers/2601.10103",
    "arxiv_url": "https://arxiv.org/abs/2601.10103"
  },
  {
    "title": "Transition Matching Distillation for Fast Video Generation",
    "summary": "Large video diffusion and flow models have achieved remarkable success in high-quality video generation, but their use in real-time interactive applications remains limited due to their inefficient multi-step sampling process. In this work, we present Transition Matching Distillation (TMD), a novel framework for distilling video diffusion models into efficient few-step generators. The central idea of TMD is to match the multi-step denoising trajectory of a diffusion model with a few-step probability transition process, where each transition is modeled as a lightweight conditional flow. To enable efficient distillation, we decompose the original diffusion backbone into two components: (1) a main backbone, comprising the majority of early layers, that extracts semantic representations at each outer transition step; and (2) a flow head, consisting of the last few layers, that leverages these representations to perform multiple inner flow updates. Given a pretrained video diffusion model, we first introduce a flow head to the model, and adapt it into a conditional flow map. We then apply distribution matching distillation to the student model with flow head rollout in each transition step. Extensive experiments on distilling Wan2.1 1.3B and 14B text-to-video models demonstrate that TMD provides a flexible and strong trade-off between generation speed and visual quality. In particular, TMD outperforms existing distilled models under comparable inference costs in terms of visual fidelity and prompt adherence. Project page: https://research.nvidia.com/labs/genair/tmd",
    "translation": "标题：快速视频生成的过渡匹配蒸馏方法\n\n摘要：大型视频扩散与流模型已在高质量视频生成领域取得显著成功，但由于其低效的多步采样过程，在实时交互应用中的使用仍受限。本研究提出过渡匹配蒸馏（TMD），一种将视频扩散模型蒸馏为高效少步生成器的新框架。TMD的核心思想是将扩散模型的多步去噪轨迹与少步概率转移过程相匹配，其中每个转移步骤被建模为轻量级条件流。为实现高效蒸馏，我们将原始扩散主干网络分解为两个部分：（1）主主干网络（包含早期多数层），用于在外部每个转移步骤提取语义表征；（2）流头部网络（由最后若干层构成），利用这些表征执行多次内部流更新。给定预训练视频扩散模型，我们首先为其引入流头部结构，并将其适配为条件流映射。随后通过在每个转移步骤中展开流头部，对包含流头部的学生模型实施分布匹配蒸馏。基于Wan2.1 1.3B和14B文本到视频模型的蒸馏实验表明，TMD在生成速度与视觉质量之间实现了灵活而优异的平衡。特别值得注意的是，在可比较的推理成本下，TMD在视觉保真度与提示遵循度方面均优于现有蒸馏模型。项目页面：https://research.nvidia.com/labs/genair/tmd",
    "url": "https://huggingface.co/papers/2601.09881",
    "arxiv_url": "https://arxiv.org/abs/2601.09881"
  },
  {
    "title": "PACEvolve: Enabling Long-Horizon Progress-Aware Consistent Evolution",
    "summary": "Large Language Models (LLMs) have emerged as powerful operators for evolutionary search, yet the design of efficient search scaffolds remains ad hoc. While promising, current LLM-in-the-loop systems lack a systematic approach to managing the evolutionary process. We identify three distinct failure modes: Context Pollution, where experiment history biases future candidate generation; Mode Collapse, where agents stagnate in local minima due to poor exploration-exploitation balance; and Weak Collaboration, where rigid crossover strategies fail to leverage parallel search trajectories effectively. We introduce Progress-Aware Consistent Evolution (PACEvolve), a framework designed to robustly govern the agent's context and search dynamics, to address these challenges. PACEvolve combines hierarchical context management (HCM) with pruning to address context pollution; momentum-based backtracking (MBB) to escape local minima; and a self-adaptive sampling policy that unifies backtracking and crossover for dynamic search coordination (CE), allowing agents to balance internal refinement with cross-trajectory collaboration. We demonstrate that PACEvolve provides a systematic path to consistent, long-horizon self-improvement, achieving state-of-the-art results on LLM-SR and KernelBench, while discovering solutions surpassing the record on Modded NanoGPT.",
    "translation": "标题：PACEvolve：实现长周期进度感知一致性进化的框架\n\n摘要：大语言模型已成为进化搜索的强大操作工具，但高效搜索框架的设计仍缺乏系统性。尽管现有的大语言模型循环系统展现出潜力，但其在管理进化过程方面缺乏系统化方法。我们识别出三种典型的失效模式：上下文污染（实验历史偏差影响后续候选生成）、模式崩溃（因探索-利用平衡不佳导致智能体陷入局部最优）以及弱协作（僵化的交叉策略无法有效利用并行搜索轨迹）。为应对这些挑战，我们提出进度感知一致性进化框架，该框架通过分层上下文管理结合剪枝机制解决上下文污染问题；采用动量回溯机制逃离局部最优；并引入自适应采样策略，将回溯与交叉统一为动态搜索协调机制，使智能体能够平衡内部优化与跨轨迹协作。实验表明，该框架为实现持续长周期自我进化提供了系统化路径，在LLM-SR和KernelBench基准测试中达到最优性能，并在Modded NanoGPT任务中发现了超越现有记录的解决方案。",
    "url": "https://huggingface.co/papers/2601.10657",
    "arxiv_url": "https://arxiv.org/abs/2601.10657"
  },
  {
    "title": "Action100M: A Large-scale Video Action Dataset",
    "summary": "Inferring physical actions from visual observations is a fundamental capability for advancing machine intelligence in the physical world. Achieving this requires large-scale, open-vocabulary video action datasets that span broad domains. We introduce Action100M, a large-scale dataset constructed from 1.2M Internet instructional videos (14.6 years of duration), yielding O(100 million) temporally localized segments with open-vocabulary action supervision and rich captions. Action100M is generated by a fully automated pipeline that (i) performs hierarchical temporal segmentation using V-JEPA 2 embeddings, (ii) produces multi-level frame and segment captions organized as a Tree-of-Captions, and (iii) aggregates evidence with a reasoning model (GPT-OSS-120B) under a multi-round Self-Refine procedure to output structured annotations (brief/detailed action, actor, brief/detailed caption). Training VL-JEPA on Action100M demonstrates consistent data-scaling improvements and strong zero-shot performance across diverse action recognition benchmarks, establishing Action100M as a new foundation for scalable research in video understanding and world modeling.",
    "translation": "标题：Action100M：一个大规模视频动作数据集\n\n摘要：从视觉观察中推断物理动作是推动机器智能在物理世界中发展的核心能力。实现这一目标需要覆盖广泛领域的大规模、开放词汇的视频动作数据集。我们提出了Action100M，这是一个基于120万个互联网教学视频（总时长约14.6年）构建的大规模数据集，生成了约1亿个具有开放词汇动作标注和丰富文本描述的时间定位片段。Action100M通过全自动流程生成，该流程（i）利用V-JEPA 2嵌入进行分层时间分割，（ii）生成组织为“描述树”的多层级帧与片段描述，以及（iii）在多轮自我优化过程中，通过推理模型（GPT-OSS-120B）聚合证据，输出结构化标注（简洁/详细动作、执行者、简洁/详细描述）。在Action100M上训练VL-JEPA模型显示出持续的数据规模扩展效益，并在多种动作识别基准测试中实现了强大的零样本性能，从而确立了Action100M作为视频理解与世界建模可扩展研究的新基础。",
    "url": "https://huggingface.co/papers/2601.10592",
    "arxiv_url": "https://arxiv.org/abs/2601.10592"
  },
  {
    "title": "M^4olGen: Multi-Agent, Multi-Stage Molecular Generation under Precise Multi-Property Constraints",
    "summary": "Generating molecules that satisfy precise numeric constraints over multiple physicochemical properties is critical and challenging. Although large language models (LLMs) are expressive, they struggle with precise multi-objective control and numeric reasoning without external structure and feedback. We introduce M olGen, a fragment-level, retrieval-augmented, two-stage framework for molecule generation under multi-property constraints. Stage I : Prototype generation: a multi-agent reasoner performs retrieval-anchored, fragment-level edits to produce a candidate near the feasible region. Stage II : RL-based fine-grained optimization: a fragment-level optimizer trained with Group Relative Policy Optimization (GRPO) applies one- or multi-hop refinements to explicitly minimize the property errors toward our target while regulating edit complexity and deviation from the prototype. A large, automatically curated dataset with reasoning chains of fragment edits and measured property deltas underpins both stages, enabling deterministic, reproducible supervision and controllable multi-hop reasoning. Unlike prior work, our framework better reasons about molecules by leveraging fragments and supports controllable refinement toward numeric targets. Experiments on generation under two sets of property constraints (QED, LogP, Molecular Weight and HOMO, LUMO) show consistent gains in validity and precise satisfaction of multi-property targets, outperforming strong LLMs and graph-based algorithms.",
    "translation": "标题：M^4olGen：精确多属性约束下的多智能体、多阶段分子生成\n\n摘要：生成满足多种物理化学性质精确数值约束的分子至关重要且具有挑战性。尽管大语言模型（LLMs）表达能力强，但在缺乏外部结构和反馈的情况下，难以实现精确的多目标控制和数值推理。我们提出了M^4olGen，一个基于片段、检索增强的两阶段框架，用于多属性约束下的分子生成。第一阶段：原型生成：一个多智能体推理器执行检索锚定的片段级编辑，生成接近可行区域的候选分子。第二阶段：基于强化学习的细粒度优化：一个采用组相对策略优化（GRPO）训练的片段级优化器，通过单跳或多跳精炼，在调控编辑复杂度和原型偏离度的同时，显式最小化属性误差以逼近目标值。一个自动构建的大规模数据集支撑了两个阶段，该数据集包含片段编辑的推理链和测量的属性变化，从而实现了确定性的、可复现的监督以及可控的多跳推理。与先前工作不同，我们的框架通过利用片段更好地对分子进行推理，并支持向数值目标的可控精炼。在两组属性约束（QED、LogP、分子量以及HOMO、LUMO）下的生成实验表明，该框架在有效性和精确满足多属性目标方面持续提升，性能优于强大的LLMs和基于图的算法。\n\n请按照以下格式返回：\n标题：[中文标题]\n摘要：[中文摘要]",
    "url": "https://huggingface.co/papers/2601.10131",
    "arxiv_url": "https://arxiv.org/abs/2601.10131"
  },
  {
    "title": "HeartMuLa: A Family of Open Sourced Music Foundation Models",
    "summary": "We present a family of open-source Music Foundation Models designed to advance large-scale music understanding and generation across diverse tasks and modalities. Our framework consists of four major components: (1) HeartCLAP, an audio-text alignment model; (2) HeartTranscriptor, a robust lyric recognition model optimized for real-world music scenarios; and (3) HeartCodec, a low-frame-rate (12.5 Hz) yet high-fidelity music codec tokenizer that captures long-range musical structure while preserving fine-grained acoustic details and enabling efficient autoregressive modeling; (4) HeartMuLa, an LLM-based song generation model capable of synthesizing high-fidelity music under rich, user-controllable conditions (e.g., textual style descriptions, lyrics, and reference audio). In addition, it provides two specialized modes: (i) fine-grained musical attribute control, which allows users to specify the style of different song sections (e.g., intro, verse, chorus) using natural language prompts; and (ii) short, engaging music generation, which is suitable as background music for short videos. Lastly, HeartMuLa improves significantly when scaled to 7B parameters. For the first time, we show that a Suno-level, commercial-grade system can be reproduced using academic-scale data and GPU resources. We expect these foundation models to serve as strong baselines for future research and to facilitate practical applications in multimodal content production.",
    "translation": "标题：HeartMuLa：开源音乐基础模型系列\n\n摘要：本文提出一系列开源音乐基础模型，旨在推动跨任务与多模态的大规模音乐理解与生成。该框架包含四大核心组件：（1）HeartCLAP——音频-文本对齐模型；（2）HeartTranscriptor——面向真实音乐场景优化的鲁棒歌词识别模型；（3）HeartCodec——低帧率（12.5 Hz）高保真音乐编解码分词器，在捕捉长程音乐结构的同时保留细粒度声学细节，并支持高效自回归建模；（4）HeartMuLa——基于大语言模型的歌曲生成模型，能够在丰富用户可控条件下（如文本风格描述、歌词及参考音频）合成高保真音乐。此外，该模型提供两种专项生成模式：（i）细粒度音乐属性控制，允许用户通过自然语言指令指定歌曲段落（如前奏、主歌、副歌）的风格；（ii）简短动感音乐生成，适用于短视频背景音乐场景。实验表明，当模型参数量扩展至70亿时，HeartMuLa性能显著提升。本研究首次证明，利用学术规模数据与GPU资源即可复现达到Suno级别的商用系统水准。我们期望该系列基础模型能为未来研究提供坚实基线，并推动多模态内容生产的实际应用。",
    "url": "https://huggingface.co/papers/2601.10547",
    "arxiv_url": "https://arxiv.org/abs/2601.10547"
  },
  {
    "title": "Inference-time Physics Alignment of Video Generative Models with Latent World Models",
    "summary": "State-of-the-art video generative models produce promising visual content yet often violate basic physics principles, limiting their utility. While some attribute this deficiency to insufficient physics understanding from pre-training, we find that the shortfall in physics plausibility also stems from suboptimal inference strategies. We therefore introduce WMReward and treat improving physics plausibility of video generation as an inference-time alignment problem. In particular, we leverage the strong physics prior of a latent world model (here, VJEPA-2) as a reward to search and steer multiple candidate denoising trajectories, enabling scaling test-time compute for better generation performance. Empirically, our approach substantially improves physics plausibility across image-conditioned, multiframe-conditioned, and text-conditioned generation settings, with validation from human preference study. Notably, in the ICCV 2025 Perception Test PhysicsIQ Challenge, we achieve a final score of 62.64%, winning first place and outperforming the previous state of the art by 7.42%. Our work demonstrates the viability of using latent world models to improve physics plausibility of video generation, beyond this specific instantiation or parameterization.",
    "translation": "标题：视频生成模型的推理时物理对齐与潜在世界模型\n\n摘要：当前最先进的视频生成模型能够生成视觉效果出色的内容，但常常违反基本物理原理，这限制了其实用性。尽管有观点认为这一缺陷源于预训练阶段对物理规律的理解不足，但我们发现物理合理性的不足也源于次优的推理策略。因此，我们提出了WMReward方法，将提升视频生成的物理合理性视为推理时的对齐问题。具体而言，我们利用潜在世界模型（此处为VJEPA-2）的强物理先验作为奖励，通过搜索和引导多条候选去噪轨迹，实现了测试时计算资源的灵活扩展以提升生成性能。实验表明，我们的方法在图像条件生成、多帧条件生成和文本条件生成等多种设置下均显著提升了物理合理性，并通过人工偏好研究验证了其有效性。值得注意的是，在ICCV 2025感知测试物理智商挑战赛中，我们以62.64%的最终得分获得第一名，较先前最优方法提升了7.42%。我们的工作证明了利用潜在世界模型提升视频生成物理合理性的可行性，其价值不仅限于特定的模型实例或参数化方案。",
    "url": "https://huggingface.co/papers/2601.10553",
    "arxiv_url": "https://arxiv.org/abs/2601.10553"
  },
  {
    "title": "EvasionBench: Detecting Evasive Answers in Financial Q&A via Multi-Model Consensus and LLM-as-Judge",
    "summary": "Detecting evasive answers in earnings calls is critical for financial transparency, yet progress is hindered by the lack of large-scale benchmarks. We introduce EvasionBench, comprising 30,000 training samples and 1,000 human-annotated test samples (Cohen's Kappa 0.835) across three evasion levels. Our key contribution is a multi-model annotation framework leveraging a core insight: disagreement between frontier LLMs signals hard examples most valuable for training. We mine boundary cases where two strong annotators conflict, using a judge to resolve labels. This approach outperforms single-model distillation by 2.4 percent, with judge-resolved samples improving generalization despite higher training loss (0.421 vs 0.393) - evidence that disagreement mining acts as implicit regularization. Our trained model Eva-4B (4B parameters) achieves 81.3 percent accuracy, outperforming its base by 25 percentage points and approaching frontier LLM performance at a fraction of inference cost.",
    "translation": "标题：EvasionBench：基于多模型共识与LLM裁判机制的金融问答规避性回答检测\n\n摘要：在财报电话会议中检测规避性回答对提升金融透明度至关重要，但大规模基准数据的缺乏制约了该领域进展。本文提出EvasionBench数据集，包含按三种规避程度划分的30,000个训练样本和1,000个人工标注测试样本（科恩卡帕系数0.835）。我们的核心贡献在于提出一种多模型标注框架，其关键洞见是：前沿大语言模型之间的分歧信号能识别出最具训练价值的困难样本。我们通过挖掘两个强标注模型产生冲突的边界案例，并引入裁判模型进行标签裁决。该方法相比单模型蒸馏策略提升2.4%的性能，且经裁判裁决的样本虽带来更高训练损失（0.421对比0.393），却显著提升了模型泛化能力——这证明分歧挖掘机制发挥了隐式正则化作用。最终训练的Eva-4B模型（40亿参数）达到81.3%的准确率，较基础模型提升25个百分点，并以极低的推理成本逼近前沿大语言模型性能。",
    "url": "https://huggingface.co/papers/2601.09142",
    "arxiv_url": "https://arxiv.org/abs/2601.09142"
  },
  {
    "title": "TAG-MoE: Task-Aware Gating for Unified Generative Mixture-of-Experts",
    "summary": "Unified image generation and editing models suffer from severe task interference in dense diffusion transformers architectures, where a shared parameter space must compromise between conflicting objectives (e.g., local editing v.s. subject-driven generation). While the sparse Mixture-of-Experts (MoE) paradigm is a promising solution, its gating networks remain task-agnostic, operating based on local features, unaware of global task intent. This task-agnostic nature prevents meaningful specialization and fails to resolve the underlying task interference. In this paper, we propose a novel framework to inject semantic intent into MoE routing. We introduce a Hierarchical Task Semantic Annotation scheme to create structured task descriptors (e.g., scope, type, preservation). We then design Predictive Alignment Regularization to align internal routing decisions with the task's high-level semantics. This regularization evolves the gating network from a task-agnostic executor to a dispatch center. Our model effectively mitigates task interference, outperforming dense baselines in fidelity and quality, and our analysis shows that experts naturally develop clear and semantically correlated specializations.",
    "translation": "标题：TAG-MoE：面向统一生成式专家混合模型的任务感知门控机制\n\n摘要：在密集扩散Transformer架构中，统一的图像生成与编辑模型存在严重的任务干扰问题，其共享参数空间必须在相互冲突的目标（如局部编辑与主体驱动生成）之间做出妥协。尽管稀疏专家混合（MoE）范式是一种颇具前景的解决方案，但其门控网络仍保持任务无关性，仅基于局部特征进行操作，无法感知全局任务意图。这种任务无关特性阻碍了有意义的专业化分工，未能从根本上解决任务干扰问题。本文提出一种新颖框架，将语义意图注入MoE路由机制。我们引入分层任务语义标注方案以构建结构化任务描述符（如作用范围、任务类型、保留要求），进而设计预测对齐正则化方法，使内部路由决策与任务高层语义保持一致。该正则化使门控网络从任务无关执行器演进为智能调度中心。实验表明，本模型能有效缓解任务干扰，在生成保真度与质量方面超越密集基线模型，分析结果证实各专家能自然形成清晰且语义关联的专业化能力。",
    "url": "https://huggingface.co/papers/2601.08881",
    "arxiv_url": "https://arxiv.org/abs/2601.08881"
  },
  {
    "title": "PRL: Process Reward Learning Improves LLMs' Reasoning Ability and Broadens the Reasoning Boundary",
    "summary": "Improving the reasoning abilities of Large Language Models (LLMs) has been a continuous topic recently. But most relevant works are based on outcome rewards at the trajectory level, missing fine-grained supervision during the reasoning process. Other existing training frameworks that try to combine process signals together to optimize LLMs also rely heavily on tedious additional steps like MCTS, training a separate reward model, etc., doing harm to the training efficiency. Moreover, the intuition behind the process signals design lacks rigorous theoretical support, leaving the understanding of the optimization mechanism opaque. In this paper, we propose Process Reward Learning (PRL), which decomposes the entropy regularized reinforcement learning objective into intermediate steps, with rigorous process rewards that could be assigned to models accordingly. Starting from theoretical motivation, we derive the formulation of PRL that is essentially equivalent to the objective of reward maximization plus a KL-divergence penalty term between the policy model and a reference model. However, PRL could turn the outcome reward into process supervision signals, which helps better guide the exploration during RL optimization. From our experiment results, we demonstrate that PRL not only improves the average performance for LLMs' reasoning ability measured by average @ n, but also broadens the reasoning boundary by improving the pass @ n metric. Extensive experiments show the effectiveness of PRL could be verified and generalized.",
    "translation": "标题：PRL：过程奖励学习提升大语言模型推理能力并拓展推理边界\n\n摘要：提升大语言模型的推理能力是当前持续关注的研究课题。然而，现有研究大多基于轨迹层面的结果奖励，缺乏对推理过程的细粒度监督。其他尝试融合过程信号以优化大语言模型的训练框架，往往严重依赖蒙特卡洛树搜索、训练独立奖励模型等繁琐附加步骤，损害了训练效率。此外，过程信号设计背后的直觉缺乏严格的理论支撑，导致优化机制的理解仍不透明。本文提出过程奖励学习方法，该方法将熵正则化强化学习目标分解为中间步骤，并基于严格推导的过程奖励对模型进行相应分配。我们从理论动机出发，推导出与“奖励最大化目标加策略模型与参考模型间KL散度惩罚项”本质等价的过程奖励学习形式化表达。该方法能够将结果奖励转化为过程监督信号，从而在强化学习优化过程中更好地引导探索。实验结果表明，过程奖励学习不仅通过平均@n指标提升了大语言模型推理能力的整体表现，还通过提高通过@n指标拓展了推理边界。大量实验验证了该方法的有效性及其良好的泛化能力。",
    "url": "https://huggingface.co/papers/2601.10201",
    "arxiv_url": "https://arxiv.org/abs/2601.10201"
  },
  {
    "title": "LaViT: Aligning Latent Visual Thoughts for Multi-modal Reasoning",
    "summary": "Current multimodal latent reasoning often relies on external supervision (e.g., auxiliary images), ignoring intrinsic visual attention dynamics. In this work, we identify a critical Perception Gap in distillation: student models frequently mimic a teacher's textual output while attending to fundamentally divergent visual regions, effectively relying on language priors rather than grounded perception. To bridge this, we propose LaViT, a framework that aligns latent visual thoughts rather than static embeddings. LaViT compels the student to autoregressively reconstruct the teacher's visual semantics and attention trajectories prior to text generation, employing a curriculum sensory gating mechanism to prevent shortcut learning. Extensive experiments show that LaViT significantly enhances visual grounding, achieving up to +16.9% gains on complex reasoning tasks and enabling a compact 3B model to outperform larger open-source variants and proprietary models like GPT-4o.",
    "translation": "标题：LaViT：面向多模态推理的潜在视觉思维对齐方法\n\n摘要：当前多模态潜在推理方法通常依赖外部监督（如辅助图像），忽略了内在的视觉注意力动态机制。本研究揭示了知识蒸馏中的关键感知鸿沟：学生模型往往在关注完全不同的视觉区域时模仿教师的文本输出，实质上依赖于语言先验而非基于感知的推理。为弥合这一鸿沟，我们提出LaViT框架，该框架通过对齐潜在视觉思维而非静态嵌入来实现多模态对齐。LaViT强制学生在文本生成前自回归地重构教师的视觉语义与注意力轨迹，并采用课程式感知门控机制以避免捷径学习。大量实验表明，LaViT显著增强了视觉基础推理能力，在复杂推理任务上实现了最高达16.9%的性能提升，使一个紧凑的30亿参数模型能够超越更大规模的开源变体及GPT-4o等专有模型。",
    "url": "https://huggingface.co/papers/2601.10129",
    "arxiv_url": "https://arxiv.org/abs/2601.10129"
  },
  {
    "title": "LSRIF: Logic-Structured Reinforcement Learning for Instruction Following",
    "summary": "Instruction-following is critical for large language models, but real-world instructions often contain logical structures such as sequential dependencies and conditional branching. Existing methods typically construct datasets with parallel constraints and optimize average rewards, ignoring logical dependencies and yielding noisy signals. We propose a logic-structured training framework LSRIF that explicitly models instruction logic. We first construct a dataset LSRInstruct with constraint structures such as parallel, sequential, and conditional types, and then design structure-aware rewarding method LSRIF including average aggregation for parallel structures, failure-penalty propagation for sequential structures, and selective rewards for conditional branches. Experiments show LSRIF brings significant improvements in instruction-following (in-domain and out-of-domain) and general reasoning. Analysis reveals that learning with explicit logic structures brings parameter updates in attention layers and sharpens token-level attention to constraints and logical operators.",
    "translation": "标题：LSRIF：面向指令跟随的逻辑结构化强化学习\n\n摘要：指令跟随能力对大语言模型至关重要，但现实场景中的指令常包含顺序依赖与条件分支等逻辑结构。现有方法通常构建具有并行约束的数据集并优化平均奖励，忽视了逻辑依赖性，导致训练信号存在噪声。本文提出一种显式建模指令逻辑的训练框架LSRIF。我们首先构建包含并行、顺序、条件三类约束结构的LSRInstruct数据集，进而设计结构感知的奖励方法LSRIF：对并行结构采用平均聚合奖励，对顺序结构实施失败惩罚传递机制，对条件分支设计选择性奖励。实验表明，LSRIF在指令跟随（领域内/外）和通用推理任务上均带来显著提升。分析发现，通过显式逻辑结构学习能够驱动注意力层的参数更新，并增强模型对约束条件与逻辑运算符的词级关注度。",
    "url": "https://huggingface.co/papers/2601.06431",
    "arxiv_url": "https://arxiv.org/abs/2601.06431"
  },
  {
    "title": "Patient-Similarity Cohort Reasoning in Clinical Text-to-SQL",
    "summary": "Real-world clinical text-to-SQL requires reasoning over heterogeneous EHR tables, temporal windows, and patient-similarity cohorts to produce executable queries. We introduce CLINSQL, a benchmark of 633 expert-annotated tasks on MIMIC-IV v3.1 that demands multi-table joins, clinically meaningful filters, and executable SQL. Solving CLINSQL entails navigating schema metadata and clinical coding systems, handling long contexts, and composing multi-step queries beyond traditional text-to-SQL. We evaluate 22 proprietary and open-source models under Chain-of-Thought self-refinement and use rubric-based SQL analysis with execution checks that prioritize critical clinical requirements. Despite recent advances, performance remains far from clinical reliability: on the test set, GPT-5-mini attains 74.7% execution score, DeepSeek-R1 leads open-source at 69.2% and Gemini-2.5-Pro drops from 85.5% on Easy to 67.2% on Hard. Progress on CLINSQL marks tangible advances toward clinically reliable text-to-SQL for real-world EHR analytics.",
    "translation": "标题：临床文本到SQL中的患者相似性队列推理\n\n摘要：现实世界的临床文本到SQL任务需要对异构电子健康记录表、时间窗口和患者相似性队列进行推理，以生成可执行的查询。我们提出了CLINSQL基准，该基准基于MIMIC-IV v3.1数据集构建，包含633项专家标注的任务，要求进行多表连接、具有临床意义的筛选并生成可执行的SQL语句。解决CLINSQL任务需要理解数据库模式元数据和临床编码系统、处理长上下文信息，并构建超越传统文本到SQL的多步骤查询。我们在思维链自优化框架下评估了22个专有和开源模型，并采用基于量规的SQL分析与执行检查方法，优先考虑关键的临床需求。尽管近期技术有所进展，但模型性能仍远未达到临床可靠性标准：在测试集上，GPT-5-mini的执行准确率为74.7%，DeepSeek-R1以69.2%的成绩领先开源模型，而Gemini-2.5-Pro在简单任务上虽达到85.5%的准确率，但在困难任务上则降至67.2%。CLINSQL基准的进展标志着面向真实世界电子健康记录分析的临床可靠文本到SQL技术取得了实质性进步。",
    "url": "https://huggingface.co/papers/2601.09876",
    "arxiv_url": "https://arxiv.org/abs/2601.09876"
  },
  {
    "title": "Agent Skills in the Wild: An Empirical Study of Security Vulnerabilities at Scale",
    "summary": "The rise of AI agent frameworks has introduced agent skills, modular packages containing instructions and executable code that dynamically extend agent capabilities. While this architecture enables powerful customization, skills execute with implicit trust and minimal vetting, creating a significant yet uncharacterized attack surface. We conduct the first large-scale empirical security analysis of this emerging ecosystem, collecting 42,447 skills from two major marketplaces and systematically analyzing 31,132 using SkillScan, a multi-stage detection framework integrating static analysis with LLM-based semantic classification. Our findings reveal pervasive security risks: 26.1% of skills contain at least one vulnerability, spanning 14 distinct patterns across four categories: prompt injection, data exfiltration, privilege escalation, and supply chain risks. Data exfiltration (13.3%) and privilege escalation (11.8%) are most prevalent, while 5.2% of skills exhibit high-severity patterns strongly suggesting malicious intent. We find that skills bundling executable scripts are 2.12x more likely to contain vulnerabilities than instruction-only skills (OR=2.12, p<0.001). Our contributions include: (1) a grounded vulnerability taxonomy derived from 8,126 vulnerable skills, (2) a validated detection methodology achieving 86.7% precision and 82.5% recall, and (3) an open dataset and detection toolkit to support future research. These results demonstrate an urgent need for capability-based permission systems and mandatory security vetting before this attack vector is further exploited.",
    "translation": "标题：智能体技能在真实环境中的安全风险：大规模安全漏洞实证研究\n\n摘要：人工智能智能体框架的兴起催生了智能体技能——一种包含指令与可执行代码的模块化封装包，能够动态扩展智能体能力。尽管这种架构实现了强大的定制功能，但技能通常在隐式信任且缺乏严格审查的环境下执行，从而形成了一个显著但尚未被充分认知的攻击面。本研究首次对这一新兴生态系统开展大规模安全实证分析：从两大主流市场收集42,447个技能样本，并运用SkillScan多阶段检测框架（整合静态分析与基于大语言模型的语义分类技术）对31,132个技能进行系统性检测。研究发现普遍存在的安全风险：26.1%的技能至少存在一种漏洞，这些漏洞涵盖提示词注入、数据窃取、权限提升和供应链风险四大类共14种具体模式。其中数据窃取（13.3%）与权限提升（11.8%）最为普遍，5.2%的技能表现出强烈暗示恶意意图的高危模式。研究进一步发现：封装可执行脚本的技能存在漏洞的概率是纯指令型技能的2.12倍（OR=2.12, p<0.001）。本研究的贡献包括：（1）基于8,126个漏洞技能构建的实证漏洞分类体系；（2）达到86.7%精确率与82.5%召回率的可验证检测方法；（3）为后续研究提供的开源数据集与检测工具包。这些结果表明，在此攻击向量被进一步利用之前，亟需建立基于能力的权限控制系统和强制性的安全审查机制。",
    "url": "https://huggingface.co/papers/2601.10338",
    "arxiv_url": "https://arxiv.org/abs/2601.10338"
  },
  {
    "title": "Deriving Character Logic from Storyline as Codified Decision Trees",
    "summary": "Role-playing (RP) agents rely on behavioral profiles to act consistently across diverse narrative contexts, yet existing profiles are largely unstructured, non-executable, and weakly validated, leading to brittle agent behavior. We propose Codified Decision Trees (CDT), a data-driven framework that induces an executable and interpretable decision structure from large-scale narrative data. CDT represents behavioral profiles as a tree of conditional rules, where internal nodes correspond to validated scene conditions and leaves encode grounded behavioral statements, enabling deterministic retrieval of context-appropriate rules at execution time. The tree is learned by iteratively inducing candidate scene-action rules, validating them against data, and refining them through hierarchical specialization, yielding profiles that support transparent inspection and principled updates. Across multiple benchmarks, CDT substantially outperforms human-written profiles and prior profile induction methods on 85 characters across 16 artifacts, indicating that codified and validated behavioral representations lead to more reliable agent grounding.",
    "translation": "标题：从情节中推导角色逻辑：基于编码决策树的方法\n\n摘要：角色扮演智能体依赖行为配置文件在不同叙事情境中保持行为一致性，但现有配置文件大多为非结构化、不可执行且验证薄弱，导致智能体行为脆弱。我们提出编码决策树（CDT）这一数据驱动框架，能够从大规模叙事数据中推导出可执行且可解释的决策结构。CDT将行为配置文件表示为条件规则树，其中内部节点对应经过验证的场景条件，叶节点编码具体行为陈述，从而在执行时实现情境适配规则的确定性检索。该决策树通过迭代推导候选场景-行动规则、依据数据进行验证，并通过层次化特化进行优化而习得，最终形成支持透明检视与原则性更新的配置文件。在涵盖16个叙事作品的85个角色的多项基准测试中，CDT显著优于人工编写的配置文件及先前的配置文件推导方法，表明经过编码与验证的行为表征能够为智能体提供更可靠的行为基础。",
    "url": "https://huggingface.co/papers/2601.10080",
    "arxiv_url": "https://arxiv.org/abs/2601.10080"
  },
  {
    "title": "V-DPM: 4D Video Reconstruction with Dynamic Point Maps",
    "summary": "Powerful 3D representations such as DUSt3R invariant point maps, which encode 3D shape and camera parameters, have significantly advanced feed forward 3D reconstruction. While point maps assume static scenes, Dynamic Point Maps (DPMs) extend this concept to dynamic 3D content by additionally representing scene motion. However, existing DPMs are limited to image pairs and, like DUSt3R, require post processing via optimization when more than two views are involved. We argue that DPMs are more useful when applied to videos and introduce V-DPM to demonstrate this. First, we show how to formulate DPMs for video input in a way that maximizes representational power, facilitates neural prediction, and enables reuse of pretrained models. Second, we implement these ideas on top of VGGT, a recent and powerful 3D reconstructor. Although VGGT was trained on static scenes, we show that a modest amount of synthetic data is sufficient to adapt it into an effective V-DPM predictor. Our approach achieves state of the art performance in 3D and 4D reconstruction for dynamic scenes. In particular, unlike recent dynamic extensions of VGGT such as P3, DPMs recover not only dynamic depth but also the full 3D motion of every point in the scene.",
    "translation": "标题：V-DPM：基于动态点图映射的四维视频重建\n\n摘要：诸如DUSt3R不变点图映射等强大的三维表示方法，通过编码三维形状与相机参数，显著推进了前馈式三维重建技术的发展。传统点图映射通常假设场景为静态，而动态点图映射（DPMs）通过额外表征场景运动，将这一概念扩展至动态三维内容。然而，现有DPMs仅适用于图像对，且与DUSt3R类似，在处理超过两个视角时仍需通过优化进行后处理。我们认为DPMs在视频应用中更具实用价值，并由此提出V-DPM予以验证。首先，我们阐释如何构建适用于视频输入的DPMs表示框架，以最大化表征能力、促进神经预测并实现预训练模型复用。其次，我们在近期强大的三维重建框架VGGT基础上实现了该框架。尽管VGGT原基于静态场景训练，但我们证明仅需适量合成数据即可将其有效适配为V-DPM预测器。该方法在动态场景的三维与四维重建任务中达到业界最优性能。特别值得注意的是，相较于VGGT的近期动态扩展方法（如P3），DPMs不仅能重建动态深度，还能完整恢复场景中每个点的三维运动轨迹。",
    "url": "https://huggingface.co/papers/2601.09499",
    "arxiv_url": "https://arxiv.org/abs/2601.09499"
  },
  {
    "title": "RigMo: Unifying Rig and Motion Learning for Generative Animation",
    "summary": "Despite significant progress in 4D generation, rig and motion, the core structural and dynamic components of animation are typically modeled as separate problems. Existing pipelines rely on ground-truth skeletons and skinning weights for motion generation and treat auto-rigging as an independent process, undermining scalability and interpretability. We present RigMo, a unified generative framework that jointly learns rig and motion directly from raw mesh sequences, without any human-provided rig annotations. RigMo encodes per-vertex deformations into two compact latent spaces: a rig latent that decodes into explicit Gaussian bones and skinning weights, and a motion latent that produces time-varying SE(3) transformations. Together, these outputs define an animatable mesh with explicit structure and coherent motion, enabling feed-forward rig and motion inference for deformable objects. Beyond unified rig-motion discovery, we introduce a Motion-DiT model operating in RigMo's latent space and demonstrate that these structure-aware latents can naturally support downstream motion generation tasks. Experiments on DeformingThings4D, Objaverse-XL, and TrueBones demonstrate that RigMo learns smooth, interpretable, and physically plausible rigs, while achieving superior reconstruction and category-level generalization compared to existing auto-rigging and deformation baselines. RigMo establishes a new paradigm for unified, structure-aware, and scalable dynamic 3D modeling.",
    "translation": "标题：RigMo：统一骨骼与运动学习的生成式动画框架\n\n摘要：尽管四维生成领域已取得显著进展，但作为动画核心结构与动态组件的骨骼与运动通常被建模为独立问题。现有流程依赖真实骨骼与蒙皮权重进行运动生成，并将自动骨骼绑定视为独立过程，这限制了方法的可扩展性与可解释性。本文提出RigMo——一个统一的生成式框架，能够直接从原始网格序列中联合学习骨骼与运动，无需任何人工标注的骨骼信息。RigMo将逐顶点变形编码至两个紧凑的潜在空间：骨骼潜在空间（解码为显式高斯骨骼与蒙皮权重）和运动潜在空间（生成随时间变化的SE(3)变换）。这些输出共同定义了具有显式结构与连贯运动的可动画网格，实现了可变形物体的前馈式骨骼与运动推断。除统一发现骨骼-运动关系外，我们进一步提出在RigMo潜在空间中运行的Motion-DiT模型，证明这种结构感知的潜在表示能自然支持下游运动生成任务。在DeformingThings4D、Objaverse-XL和TrueBones数据集上的实验表明，RigMo能够学习平滑、可解释且物理合理的骨骼系统，同时在重建效果与类别级泛化能力上优于现有自动骨骼绑定与变形基线方法。RigMo为统一、结构感知且可扩展的动态三维建模建立了新范式。",
    "url": "https://huggingface.co/papers/2601.06378",
    "arxiv_url": "https://arxiv.org/abs/2601.06378"
  },
  {
    "title": "CaMeLs Can Use Computers Too: System-level Security for Computer Use Agents",
    "summary": "AI agents are vulnerable to prompt injection attacks, where malicious content hijacks agent behavior to steal credentials or cause financial loss. The only known robust defense is architectural isolation that strictly separates trusted task planning from untrusted environment observations. However, applying this design to Computer Use Agents (CUAs) -- systems that automate tasks by viewing screens and executing actions -- presents a fundamental challenge: current agents require continuous observation of UI state to determine each action, conflicting with the isolation required for security. We resolve this tension by demonstrating that UI workflows, while dynamic, are structurally predictable. We introduce Single-Shot Planning for CUAs, where a trusted planner generates a complete execution graph with conditional branches before any observation of potentially malicious content, providing provable control flow integrity guarantees against arbitrary instruction injections. Although this architectural isolation successfully prevents instruction injections, we show that additional measures are needed to prevent Branch Steering attacks, which manipulate UI elements to trigger unintended valid paths within the plan. We evaluate our design on OSWorld, and retain up to 57% of the performance of frontier models while improving performance for smaller open-source models by up to 19%, demonstrating that rigorous security and utility can coexist in CUAs.",
    "translation": "标题：CaMeLs亦能驾驭计算机：计算机使用代理的系统级安全\n\n摘要：人工智能代理易受提示注入攻击，恶意内容可通过劫持代理行为窃取凭证或造成经济损失。目前唯一已知的稳健防御方案是采用架构隔离机制，将可信任务规划与不可信环境观察严格分离。然而，将该设计应用于计算机使用代理（CUAs）——即通过观察屏幕状态并执行操作实现任务自动化的系统——面临根本性挑战：现有代理需要持续观察用户界面状态以确定每个操作步骤，这与安全所需的隔离要求相冲突。我们通过论证用户界面工作流虽具有动态性但结构可预测的特性，解决了这一矛盾。本文提出面向CUAs的单次规划方法，由可信规划器在观察任何潜在恶意内容前，生成包含条件分支的完整执行图，从而为任意指令注入攻击提供可验证的控制流完整性保障。尽管架构隔离能有效防御指令注入，但我们发现仍需额外措施防范分支导向攻击——此类攻击通过操纵界面元素触发计划中非预期的有效路径。我们在OSWorld环境中评估该设计，在保持前沿模型57%性能的同时，将小型开源模型的性能提升最高达19%，证明CUAs能够实现严格安全性与实用性的共存。",
    "url": "https://huggingface.co/papers/2601.09923",
    "arxiv_url": "https://arxiv.org/abs/2601.09923"
  },
  {
    "title": "WildRayZer: Self-supervised Large View Synthesis in Dynamic Environments",
    "summary": "We present WildRayZer, a self-supervised framework for novel view synthesis (NVS) in dynamic environments where both the camera and objects move. Dynamic content breaks the multi-view consistency that static NVS models rely on, leading to ghosting, hallucinated geometry, and unstable pose estimation. WildRayZer addresses this by performing an analysis-by-synthesis test: a camera-only static renderer explains rigid structure, and its residuals reveal transient regions. From these residuals, we construct pseudo motion masks, distill a motion estimator, and use it to mask input tokens and gate loss gradients so supervision focuses on cross-view background completion. To enable large-scale training and evaluation, we curate Dynamic RealEstate10K (D-RE10K), a real-world dataset of 15K casually captured dynamic sequences, and D-RE10K-iPhone, a paired transient and clean benchmark for sparse-view transient-aware NVS. Experiments show that WildRayZer consistently outperforms optimization-based and feed-forward baselines in both transient-region removal and full-frame NVS quality with a single feed-forward pass.",
    "translation": "标题：WildRayZer：动态环境中自监督的大视角合成\n\n摘要：本文提出WildRayZer，一种用于相机与物体均发生运动的动态环境中的新视角合成自监督框架。动态内容会破坏静态新视角合成模型所依赖的多视角一致性，导致重影、几何结构失真及姿态估计不稳定等问题。WildRayZer通过执行分析-合成测试解决这一难题：首先通过仅考虑相机运动的静态渲染器解析刚性结构，其残差图可揭示瞬变区域。基于这些残差，我们构建伪运动掩码，蒸馏出运动估计器，并利用该估计器对输入标记进行掩码处理及损失梯度门控，从而使监督学习聚焦于跨视角背景补全任务。为支持大规模训练与评估，我们构建了Dynamic RealEstate10K（D-RE10K）数据集——包含1.5万条自然采集动态序列的真实场景数据集，以及D-RE10K-iPhone配对数据集——专为稀疏视角瞬变感知新视角合成设计的瞬变/洁净场景基准测试集。实验表明，WildRayZer在单次前向传播中，无论是瞬变区域消除还是全帧新视角合成质量，均持续优于基于优化的基线方法与前馈基线模型。",
    "url": "https://huggingface.co/papers/2601.10716",
    "arxiv_url": "https://arxiv.org/abs/2601.10716"
  },
  {
    "title": "VQ-Seg: Vector-Quantized Token Perturbation for Semi-Supervised Medical Image Segmentation",
    "summary": "Consistency learning with feature perturbation is a widely used strategy in semi-supervised medical image segmentation. However, many existing perturbation methods rely on dropout, and thus require a careful manual tuning of the dropout rate, which is a sensitive hyperparameter and often difficult to optimize and may lead to suboptimal regularization. To overcome this limitation, we propose VQ-Seg, the first approach to employ vector quantization (VQ) to discretize the feature space and introduce a novel and controllable Quantized Perturbation Module (QPM) that replaces dropout. Our QPM perturbs discrete representations by shuffling the spatial locations of codebook indices, enabling effective and controllable regularization. To mitigate potential information loss caused by quantization, we design a dual-branch architecture where the post-quantization feature space is shared by both image reconstruction and segmentation tasks. Moreover, we introduce a Post-VQ Feature Adapter (PFA) to incorporate guidance from a foundation model (FM), supplementing the high-level semantic information lost during quantization. Furthermore, we collect a large-scale Lung Cancer (LC) dataset comprising 828 CT scans annotated for central-type lung carcinoma. Extensive experiments on the LC dataset and other public benchmarks demonstrate the effectiveness of our method, which outperforms state-of-the-art approaches. Code available at: https://github.com/script-Yang/VQ-Seg.",
    "translation": "标题：VQ-Seg：基于向量量化标记扰动的半监督医学图像分割方法\n\n摘要：基于特征扰动的一致性学习是半监督医学图像分割中广泛采用的策略。然而，现有许多扰动方法依赖于随机失活技术，需要人工精细调整失活率——该超参数敏感且难以优化，易导致正则化效果欠佳。为克服这一局限，本文提出VQ-Seg方法，首次引入向量量化技术离散化特征空间，并设计了一种可替代随机失活的新型可控量化扰动模块。该模块通过重排码本索引的空间位置实现对离散表示的扰动，从而达成高效可控的正则化效果。为缓解量化可能造成的信息损失，我们设计了双分支架构，使图像重建与分割任务共享量化后的特征空间。此外，我们提出后量化特征适配器，通过引入基础模型的语义指导来补充量化过程中损失的高层语义信息。本研究还构建了包含828例中央型肺癌标注CT扫描的大规模肺癌数据集。在肺癌数据集及其他公开基准上的大量实验表明，本方法优于当前最优方案，验证了其有效性。代码已开源：https://github.com/script-Yang/VQ-Seg。",
    "url": "https://huggingface.co/papers/2601.10124",
    "arxiv_url": "https://arxiv.org/abs/2601.10124"
  },
  {
    "title": "Enhancing Sentiment Classification and Irony Detection in Large Language Models through Advanced Prompt Engineering Techniques",
    "summary": "This study investigates the use of prompt engineering to enhance large language models (LLMs), specifically GPT-4o-mini and gemini-1.5-flash, in sentiment analysis tasks. It evaluates advanced prompting techniques like few-shot learning, chain-of-thought prompting, and self-consistency against a baseline. Key tasks include sentiment classification, aspect-based sentiment analysis, and detecting subtle nuances such as irony. The research details the theoretical background, datasets, and methods used, assessing performance of LLMs as measured by accuracy, recall, precision, and F1 score. Findings reveal that advanced prompting significantly improves sentiment analysis, with the few-shot approach excelling in GPT-4o-mini and chain-of-thought prompting boosting irony detection in gemini-1.5-flash by up to 46%. Thus, while advanced prompting techniques overall improve performance, the fact that few-shot prompting works best for GPT-4o-mini and chain-of-thought excels in gemini-1.5-flash for irony detection suggests that prompting strategies must be tailored to both the model and the task. This highlights the importance of aligning prompt design with both the LLM's architecture and the semantic complexity of the task.",
    "translation": "标题：通过先进提示工程技术增强大语言模型的情感分类与反讽检测能力\n\n摘要：本研究探讨如何运用提示工程技术提升大语言模型（特别是GPT-4o-mini与gemini-1.5-flash）在情感分析任务中的表现。通过对比基线方法，系统评估了少样本学习、思维链提示及自我一致性等先进提示技术的效果。核心任务涵盖情感分类、基于方面的情感分析以及反讽等微妙语义的检测。研究详细阐述了理论背景、数据集与实验方法，并基于准确率、召回率、精确率和F1分数评估大语言模型的性能。结果表明，先进提示技术能显著改善情感分析效果：少样本提示在GPT-4o-mini中表现最优，而思维链提示使gemini-1.5-flash的反讽检测性能提升高达46%。由此可见，尽管先进提示技术整体提升模型性能，但针对不同模型与任务需采用差异化策略——少样本提示最适合GPT-4o-mini，而思维链提示在gemini-1.5-flash的反讽检测中更具优势。这凸显了提示设计必须同时适配大语言模型架构与任务语义复杂性的重要意义。",
    "url": "https://huggingface.co/papers/2601.08302",
    "arxiv_url": "https://arxiv.org/abs/2601.08302"
  },
  {
    "title": "Demystifying the Slash Pattern in Attention: The Role of RoPE",
    "summary": "Large Language Models (LLMs) often exhibit slash attention patterns, where attention scores concentrate along the Δ-th sub-diagonal for some offset Δ. These patterns play a key role in passing information across tokens. But why do they emerge? In this paper, we demystify the emergence of these Slash-Dominant Heads (SDHs) from both empirical and theoretical perspectives. First, by analyzing open-source LLMs, we find that SDHs are intrinsic to models and generalize to out-of-distribution prompts. To explain the intrinsic emergence, we analyze the queries, keys, and Rotary Position Embedding (RoPE), which jointly determine attention scores. Our empirical analysis reveals two characteristic conditions of SDHs: (1) Queries and keys are almost rank-one, and (2) RoPE is dominated by medium- and high-frequency components. Under these conditions, queries and keys are nearly identical across tokens, and interactions between medium- and high-frequency components of RoPE give rise to SDHs. Beyond empirical evidence, we theoretically show that these conditions are sufficient to ensure the emergence of SDHs by formalizing them as our modeling assumptions. Particularly, we analyze the training dynamics of a shallow Transformer equipped with RoPE under these conditions, and prove that models trained via gradient descent exhibit SDHs. The SDHs generalize to out-of-distribution prompts.",
    "translation": "标题：揭秘注意力机制中的斜线模式：RoPE的作用\n\n摘要：大型语言模型（LLM）常表现出斜线注意力模式，即注意力分数集中在某个偏移量Δ对应的第Δ条次对角线上。这些模式在跨词元传递信息中起着关键作用。但为何会出现这种模式？本文从实证与理论双重视角揭示了斜线主导注意力头（SDH）的生成机制。首先，通过对开源LLM的分析，我们发现SDH是模型固有的特性，并能泛化至分布外提示。为解释其固有生成机制，我们分析了共同决定注意力分数的查询向量、键向量及旋转位置编码（RoPE）。实证分析揭示了SDH的两个特征条件：（1）查询向量与键向量近似秩为一；（2）RoPE由中高频分量主导。在此条件下，跨词元的查询向量与键向量近乎相同，而RoPE中高频分量间的相互作用催生了SDH。除实证证据外，我们通过将上述条件形式化为建模假设，从理论上证明这些条件足以保证SDH的生成。特别地，我们分析了在此条件下配备RoPE的浅层Transformer的训练动态，并证明通过梯度下降训练的模型会呈现SDH特性，且该特性可泛化至分布外提示。",
    "url": "https://huggingface.co/papers/2601.08297",
    "arxiv_url": "https://arxiv.org/abs/2601.08297"
  },
  {
    "title": "Memory Bank Compression for Continual Adaptation of Large Language Models",
    "summary": "Large Language Models (LLMs) have become a mainstay for many everyday applications. However, as data evolve their knowledge quickly becomes outdated. Continual learning aims to update LLMs with new information without erasing previously acquired knowledge. Although methods such as full fine-tuning can incorporate new data, they are computationally expensive and prone to catastrophic forgetting, where prior knowledge is overwritten. Memory-augmented approaches address this by equipping LLMs with a memory bank, that is an external memory module which stores information for future use. However, these methods face a critical limitation, in particular, the memory bank constantly grows in the real-world scenario when large-scale data streams arrive. In this paper, we propose MBC, a model that compresses the memory bank through a codebook optimization strategy during online adaptation learning. To ensure stable learning, we also introduce an online resetting mechanism that prevents codebook collapse. In addition, we employ Key-Value Low-Rank Adaptation in the attention layers of the LLM, enabling efficient utilization of the compressed memory representations. Experiments with benchmark question-answering datasets demonstrate that MBC reduces the memory bank size to 0.3% when compared against the most competitive baseline, while maintaining high retention accuracy during online adaptation learning. Our code is publicly available at https://github.com/Thomkat/MBC.",
    "translation": "标题：面向大语言模型持续适应的记忆库压缩方法\n\n摘要：大语言模型已成为众多日常应用的核心技术。然而，随着数据动态演进，其知识体系会迅速过时。持续学习旨在通过新信息更新大语言模型，同时避免遗忘已掌握的知识。尽管全参数微调等方法能够整合新数据，但其计算成本高昂且易引发灾难性遗忘——即旧有知识被覆盖的问题。基于记忆增强的方法通过为大语言模型配备记忆库（即存储信息以供后续调用的外部记忆模块）来解决这一挑战。然而，这些方法面临关键局限：在现实场景中，当大规模数据流持续涌入时，记忆库会不断膨胀。本文提出MBC模型，该模型通过在线适应学习过程中的码本优化策略实现记忆库压缩。为确保学习稳定性，我们同时引入在线重置机制以防止码本坍缩。此外，我们在大语言模型的注意力层采用键值低秩适应技术，从而实现对压缩记忆表征的高效利用。基于基准问答数据集的实验表明：相较于最具竞争力的基线方法，MBC可将记忆库规模压缩至原大小的0.3%，同时在在线适应学习中保持高记忆保持准确率。代码已公开于https://github.com/Thomkat/MBC。",
    "url": "https://huggingface.co/papers/2601.00756",
    "arxiv_url": "https://arxiv.org/abs/2601.00756"
  }
]